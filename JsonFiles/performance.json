{"count":25568,"tag":"performance","questions":[{"tags":["performance","cakephp","godaddy","cakephp-2.0"],"answer_count":5,"favorite_count":2,"up_vote_count":5,"down_vote_count":0,"view_count":1231,"score":5,"question_id":7937168,"title":"Incredibly slow load times using CakePHP on GoDaddy hosting","body":"<p>I have multiple sites on the same hosting account, and they all run really fast.  But now I've started my first CakePHP (2.0 stable) site hosted on this account, and it runs CRAZY-slow.</p>\n\n<p>I'm talking 6.5 to 12 seconds to just load a static/empty 20KB homepage.</p>\n\n<p>The only modifications I did were tweaking the .htaccess files per instructions here: <a href=\"http://book.cakephp.org/2.0/en/installation/advanced-installation.html?highlight=godaddy\">http://book.cakephp.org/2.0/en/installation/advanced-installation.html?highlight=godaddy</a> (htaccess code below) by adding <code>Rewrite Base /</code> (which got the site to work in the first place).</p>\n\n<p>GoDaddy tech guys spent 20 minutes while on the phone with me running lots of tests and assured me the server itself and the database are both running fine/fast (which I tend to believe since my other sites on the same server are coming up really quickly).</p>\n\n<p>/mysite/.htaccess</p>\n\n<pre><code>&lt;IfModule mod_rewrite.c&gt;\nRewriteEngine on\nRewriteBase /\nRewriteRule    ^$ app/webroot/    [L]\nRewriteRule    (.*) app/webroot/$1 [L]\n&lt;/IfModule&gt;\n</code></pre>\n\n<p>/mysite/app/.htaccess</p>\n\n<pre><code>&lt;IfModule mod_rewrite.c&gt;\nRewriteEngine on\nRewriteBase /\nRewriteRule    ^$    webroot/    [L]\nRewriteRule    (.*) webroot/$1    [L]\n&lt;/IfModule&gt;\n</code></pre>\n\n<p>/mysite/app/webroot/.htaccess</p>\n\n<pre><code>&lt;IfModule mod_rewrite.c&gt;\nRewriteEngine On\nRewriteBase /\nRewriteCond %{REQUEST_FILENAME} !-d\nRewriteCond %{REQUEST_FILENAME} !-f\nRewriteRule ^(.*)$ index.php/$1 [QSA,L]\n&lt;/IfModule&gt;\n</code></pre>\n"},{"tags":["iphone","ios","performance","core-graphics","quartz-2d"],"answer_count":2,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":58,"score":6,"question_id":13277031,"title":"Core Graphics Performance on iOS","body":"<h1>Summary</h1>\n\n<p>I'm working on a fairly straightforward 2D tower defense game for iOS.</p>\n\n<p>So far, I've been using Core Graphics exclusively to handle rendering. There are no image files in the app at all (yet). I've been experiencing some significant performance issues doing relatively simple drawing, and I'm looking for ideas as to how I can fix this, short of moving to OpenGL.</p>\n\n<h1>Game Setup</h1>\n\n<p>At a high level, I have a Board class, which is a subclass of <code>UIView</code>, to represent the game board. All other objects in the game (towers, creeps, weapons, explosions, etc) are also subclasses of <code>UIView</code>, and are added as subviews to the Board when they are created.</p>\n\n<p>I keep game state totally separate from view properties within the objects, and each object's state is updated in the main game loop (fired by an <code>NSTimer</code> at 60-240 Hz, depending on the game speed setting). The game is totally playable without ever drawing, updating, or animating the views.</p>\n\n<p>I handle view updates using a <code>CADisplayLink</code> timer at the native refresh rate (60 Hz), which calls <code>setNeedsDisplay</code> on the board objects that need to have their view properties updated based on changes in the game state. All the objects on the board override <code>drawRect:</code> to paint some pretty simple 2D shapes within their frame. So when a weapon, for example, is animated, it will redraw itself based on the weapon's new state.</p>\n\n<h1>Performance Issues</h1>\n\n<p>Testing on an iPhone 5, with about 2 dozen total game objects on the board, the frame rate drops significantly below 60 FPS (the target frame rate), usually into the 10-20 FPS range. With more action on the screen, it goes downhill from here. And on an iPhone 4, things are even worse.</p>\n\n<p>Using Instruments I've determined that only roughly 5% of the CPU time is being spent on actually updating the game state -- the vast majority of it is going towards rendering. Specifically, the <code>CGContextDrawPath</code> function (which from my understanding is where the rasterization of vector paths is done) is taking an enormous amount of CPU time. See the Instruments screenshot at the bottom for more details.</p>\n\n<p>From some research on StackOverflow and other sites, it seems as though Core Graphics just isn't up to the task for what I need. Apparently, stroking vector paths is extremely expensive (especially when drawing things that aren't opaque and have some alpha value &lt; 1.0). I'm almost certain OpenGL would solve my problems, but it's pretty low level and I'm not really excited to have to use it -- it doesn't seem like it should be necessary for what I'm doing here.</p>\n\n<h1>The Question</h1>\n\n<p><strong>Are there any optimizations I should be looking at to try to get a smooth 60 FPS out of Core Graphics?</strong> </p>\n\n<h2>Some Ideas...</h2>\n\n<p>Someone suggested that I consider drawing all my objects onto a single <code>CALayer</code> instead of having each object on its own <code>CALayer</code>, but I'm not convinced that this would help based on what Instruments is showing. </p>\n\n<p>Personally, I have a theory that using <code>CGAffineTransforms</code> to do my animation (i.e. draw the object's shape(s) in <code>drawRect:</code> once, then do transforms to move/rotate/resize its layer in subsequent frames) would solve my problem, since those are based directly on OpenGL. But I don't think it would be any easier to do that than just use OpenGL outright.</p>\n\n<h2>Sample Code</h2>\n\n<p>To give you a feel for the level of drawing I'm doing, here's an example of the <code>drawRect:</code> implementation for one of my weapon objects (a \"beam\" fired from a tower).</p>\n\n<p><em>Note: this beam can be \"retargeted\" and it crosses the entire board, so for simplicity its frame is the same dimensions as the board. However most other objects on the board have their frame set to the smallest circumscribed rectangle possible.</em></p>\n\n<pre><code>- (void)drawRect:(CGRect)rect\n{\n    CGContextRef c = UIGraphicsGetCurrentContext();\n\n    // Draw beam\n    CGContextSetStrokeColorWithColor(c, [UIColor greenColor].CGColor);\n    CGContextSetLineWidth(c, self.width);\n    CGContextMoveToPoint(c, self.origin.x, self.origin.y);\n    CGPoint vector = [TDBoard vectorFromPoint:self.origin toPoint:self.destination];\n    double magnitude = sqrt(pow(self.board.frame.size.width, 2) + pow(self.board.frame.size.height, 2));\n    CGContextAddLineToPoint(c, self.origin.x+magnitude*vector.x, self.origin.y+magnitude*vector.y);\n    CGContextStrokePath(c);\n\n}\n</code></pre>\n\n<h2>Instruments Run</h2>\n\n<p>Here's a look at Instruments after letting the game run for a while:</p>\n\n<p><em><strong>The <code>TDGreenBeam</code> class has the exact <code>drawRect:</code> implementation shown above in the Sample Code section.</em></strong></p>\n\n<p><strong><a href=\"http://cl.ly/image/2b1y3512341d\" rel=\"nofollow\">Full Size Screenshot</a></strong>\n<img src=\"http://i.stack.imgur.com/qSw3j.png\" alt=\"Instruments run of the game, with the heaviest stack trace expanded.\"></p>\n"},{"tags":["javascript","performance","extjs3"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":7,"score":0,"question_id":13281517,"title":"How to tuning performance with extjs 3.x in large web application?","body":"<p>I want to seek advice about ExtJS 3.x performance tuning. Like layout, code structure, reduce http request, reusability etc...</p>\n\n<p>Thanks a lot.</p>\n"},{"tags":["sql","ruby-on-rails","database","performance","postgresql"],"answer_count":2,"favorite_count":6,"up_vote_count":14,"down_vote_count":0,"view_count":919,"score":14,"question_id":9407442,"title":"Optimise PostgreSQL for fast testing","body":"<p>I am switching to PostgreSQL from SQLite for a typical Rails application.</p>\n\n<p>The problem is that running specs became slow with PG.<br>\nOn SQLite it took ~34 seconds, on PG it's ~76 seconds which is <strong>more than 2x slower</strong>.</p>\n\n<p>So now I want to apply some techniques to <strong>bring the performance of the specs on par with SQLite</strong> with no code modifications (ideally just by setting the connection options, which is probably not possible).</p>\n\n<p>Couple of obvious things from top of my head are:</p>\n\n<ul>\n<li>RAM Disk (good setup with RSpec on OSX would be good to see)</li>\n<li>Unlogged tables (can it be applied on the whole database so I don't have change all the scripts?)</li>\n</ul>\n\n<p>As you may have understood I don't care about reliability and the rest (the DB is just a throwaway thingy here).<br>\nI need to get the most out of the PG and make it <strong>as fast as it can possibly be</strong>.</p>\n\n<p><strong>Best answer</strong> would ideally describe the <em>tricks</em> for doing just that, setup and the drawbacks of those tricks.</p>\n\n<p><strong>UPDATE:</strong> <code>fsync = off</code> + <code>full_page_writes = off</code> only decreased time to ~65 seconds (~-16 secs). Good start, but far from the target of 34.</p>\n\n<p><strong>UPDATE 2:</strong> I <a href=\"https://gist.github.com/1573414\">tried to use RAM disk</a> but the performance gain was within an error margin. So doesn't seem to be worth it.</p>\n\n<p><strong>UPDATE 3:*</strong>\nI found the biggest bottleneck and now my specs run as fast as the SQLite ones.</p>\n\n<p>The issue was the database cleanup that did the <strong>truncation</strong>. Apparently SQLite is way too fast there.</p>\n\n<p>To \"fix\" it I open a <strong>transaction</strong> before each test and roll it back at the end.</p>\n\n<p>Some numbers for ~700 tests.</p>\n\n<ul>\n<li>Truncation: SQLite - 34s, PG - 76s.</li>\n<li>Transaction: SQLite - 17s, PG - 18s.</li>\n</ul>\n\n<p>2x speed increase for SQLite.\n4x speed increase for PG.</p>\n"},{"tags":["database","performance","csv","migration","redis"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":8,"score":0,"question_id":13280993,"title":"How to move this data to redis for speedy access?","body":"<p>I've purchased a MaxMind.com's geoIP country database. They have a neat little tutorial about accessing it <a href=\"http://dev.maxmind.com/geoip/csv\" rel=\"nofollow\">here</a>.</p>\n\n<p>Here's a snippet of code that would be used with MySQL:</p>\n\n<pre><code>SELECT ip_country\nFROM geoip\nWHERE\nINET_ATON('174.36.207.186') BETWEEN begin_ip_num AND end_ip_num\nLIMIT 1\n</code></pre>\n\n<p>So, there's two columns, begin_ip_num and end_ip_num - determining the country of origin is as easy as finding a value between the two.</p>\n\n<p>Say I want to have this same data in Redis for quick access. What do I do? Will I have to run through the list and generate every possible ip thereby increasing the size of the instance DRAMATICALLY, or is there a clever way to do the same thing BETWEEN does?</p>\n"},{"tags":["sql","sql-server","performance","query","tsql"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":16,"score":0,"question_id":13280548,"title":"Performance issue in retrieving multiple output of the same column with different values from a single table","body":"<p>is there anyway to get the following results from query without joining the same table three times (or) without reading the same \"wordlocation\" table three times (or more if there are more words)? If there are three or more words, it takes about over a minute for the results to be returned.</p>\n\n<p>Currently \"wordlocation\" table has three rows being (\"bookid\",\"wordid\",\"location\") and it currently has 917802 rows.</p>\n\n<p>What I am trying to do is </p>\n\n<ol>\n<li>retrieve the \"bookid\" that contains all the words specified in the query by \"wordid\".</li>\n<li>sum word count of all words (from the query) from each book</li>\n<li>minimum values of each word location, e.g. (min(w0.location), min (w1.location)</li>\n</ol>\n\n<p>I have tried commenting out count(w0.wordid) and min(location) calculations to see whether they are affecting the performance but this is not the case. Joining the same table multiple time was the case.</p>\n\n<p><img src=\"http://i.stack.imgur.com/SYLB7.jpg\" alt=\"enter image description here\"></p>\n\n<p>(this is the same code as the above image)   </p>\n\n<pre><code>select \n    w0.bookid, \n    count(w0.wordid) as wcount, \n    abs(min(w0.location) + min(w1.location) + min(w2.location)) as wordlocation, \n    (abs(min(w0.location) - min(w1.location)) + abs(min(w1.location) - min(w2.location))) as distance \n    from \n    wordlocation as w0 \n    inner join \n    wordlocation as w1 on w0.bookid = w1.bookid \n    join \n    wordlocation as w2 on w1.bookid = w2.bookid \n    where \n    w0.wordid =3 \n    and \n    w1.wordid =52 \n    and \n    w2.wordid =42\n    group by w0.bookid \n    order by wcount desc;\n</code></pre>\n\n<p>This is the result that I am looking for, and which I got from running the above query, but it takes too long if I specify more than 3 words, e.g. (w0 = 3, w1 = 52 , w2 = 42, w3 = 71)</p>\n\n<p><img src=\"http://i.stack.imgur.com/08dD5.jpg\" alt=\"enter image description here\"></p>\n"},{"tags":["javascript","performance","function","function-pointers"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":25,"score":0,"question_id":13280443,"title":"Implications of declaring a function within another function","body":"<p>Given the following two:</p>\n\n<p><strong>Scenario 1</strong></p>\n\n<pre><code>function inner() {\n  // a bunch of code that does stuff\n}\nfunction outer() {\n  inner();\n}\nfor(var i = 0; i &lt; 10000; i++) {\n  outer();\n}\n</code></pre>\n\n<p><strong>Scenario 2</strong></p>\n\n<pre><code>function outer() {\n  function inner() {\n    // a bunch of code that does stuff\n  }\n  inner();\n}\nfor(var i = 0; i &lt; 10000; i++) {\n  outer();\n}\n</code></pre>\n\n<p>Behavior is identical in both cases, no doubt. But what's the difference under the hood? How much extra work, if any, is the interpreter doing in scenario 2? Is the memory affected. Or, say, if the body of <code>inner()</code> gets longer, would that increase the effect on performance?</p>\n\n<p>Please don't bother asking \"why would you want to do that\", because my question is not about a practical issue. Just trying to get a deeper understanding of how JS function are parsed and represented. Thanks!</p>\n"},{"tags":["performance","64bit","firebird","firebird2.5"],"answer_count":5,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":725,"score":3,"question_id":2283497,"title":"Huge page buffer vs. multiple simultaneous processes","body":"<p>One of our customer has a 35 Gb database with average active connections count about 70-80. Some tables in database have more than 10M records per table.</p>\n\n<p>Now they have bought new server: 4 * 6 Core = 24 Cores CPU, 48 Gb RAM, 2 RAID controllers 256 Mb cache, with 8 SAS 15K HDD on each.</p>\n\n<p>64bit OS.</p>\n\n<p>I'm wondering, what would be a fastest configuration:</p>\n\n<p>1) FB 2.5 SuperServer with huge buffer 8192 * 3500000 pages = 29 Gb</p>\n\n<p>or</p>\n\n<p>2) FB 2.5 Classic with small buffer of 1000 pages.</p>\n\n<p>Maybe some one has tested such case before and will save me days of work :)</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["mysql","performance","replication"],"answer_count":2,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":2706,"score":4,"question_id":2832912,"title":"How can \"set timestamp\" be a slow query?","body":"<p>My slow query log is full of entries like the following</p>\n\n<p># Query_time: 1.016361  Lock_time: 0.000000 Rows_sent: 0  Rows_examined: 0\nSET timestamp=1273826821;\nCOMMIT;</p>\n\n<p>I guess the set timestamp command is issued by replication but I don't understand how set timestamp can take over a second. Any ideas?</p>\n"},{"tags":["javascript","performance","for-loop","while-loop","three.js"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":26,"score":0,"question_id":13279912,"title":"three.js why does it use for loops instead of while","body":"<p>I see in three.js that there is the common code feature in many languages:</p>\n\n<pre><code>for ( var i = 0, l = something.length; i &lt; l; i++ ) {\n    do some stuff over i\n}\n</code></pre>\n\n<p>but I read that in javascript performance can be better by using:</p>\n\n<pre><code>var i = something.length;\nwhile(i--){\n    do some stuff over i\n}\n</code></pre>\n\n<p>Does this actually improve any performance significantly? is there a reason to prefer one over the other?</p>\n"},{"tags":["performance","cryptography","rsa","public-key-encryption","factorization"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":84,"score":2,"question_id":12637582,"title":"why is integer factorization a non-polynomial time?","body":"<p>I am just a beginner of computer science. I learned something about running time but I can't be sure what I understood is right. So please help me. </p>\n\n<p>So integer factorization is currently not a polynomial time problem but primality test is. Assume the number to be checked is n. If we run a program just to decide whether every number from 1 to sqrt(n) can divide n, and if the answer is yes, then store the number. I think this program is polynomial time, isn't it? </p>\n\n<p>One possible way that I am wrong would be a factorization program should find all primes, instead of the first prime discovered. So maybe this is the reason why. </p>\n\n<p>However, in public key cryptography, finding a prime factor of a large number is essential to attack the cryptography. Since usually a large number (public key) is only the product of two primes, finding one prime means finding the other. This should be polynomial time. So why is it difficult or impossible to attack? </p>\n"},{"tags":["c#","performance","asp.net-4.0","code-readability","boolean-operations"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":31,"score":1,"question_id":13279096,"title":"how to decide on a bool operators usage, performance issues vs readability","body":"<p>Having even more than two options to choose from, leads me to question, which one to choose, if  the result / outcome is same.</p>\n\n<p>in <code>.NET</code> <code>C#</code>  the following conditions are asking same question using different operators, so the question is , what experienced developers use, i tend to assume that ready made operators like <code>Equals</code> would go through more processing actions.</p>\n\n<p>When and why would you choose <code>!</code> over <code>Equals</code>, and both over 'traditional'  <code>==</code> ?</p>\n\n<pre><code>//bool\nif (!Page.IsPostBack) \n{\n    //bool\n    if (NotAuthorized().Equals(false)) \n    {            \n        AppsCtrls.DDLs_Init();\n\n        //bool\n        if (CurrSeSn.Raised(Flag.MainDataSet_IsPopulated) == false)\n        {\n            initALLDataSet(AllDataStColsSelectionMod.doneViaSP);\n        }\n\n        custid = RConv.Str2int(Request.QueryString[\"custid\"]);\n        username = GetTableData.AsString(\"name\", \"tblCustomers\", \"custid\", custid);\n    }\n}\n</code></pre>\n"},{"tags":["java","performance","date","comparison","deprecated"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":27,"score":3,"question_id":13279701,"title":"Most efficient way of checking if Date object and Calendar object are in the same month","body":"<p>I am working on a project that will run many thousands of comparisons between dates to see if they are in the same month, and I am wondering what the most efficient way of doing it would be.  </p>\n\n<p>This isn't exactly what my code looks like, but here's the gist:</p>\n\n<pre><code>List&lt;Date&gt; dates = getABunchOfDates();\nCalendar month = Calendar.getInstance();\nfor(int i = 0; i &lt; numMonths; i++) \n{\n    for(Date date : dates)\n    {\n        if(sameMonth(month, date)\n            .. doSomething\n    }\n    month.add(Calendar.MONTH, -1);\n}\n</code></pre>\n\n<p>Creating a new <code>Calendar</code> object for every date seems like a pretty hefty overhead when this comparison will happen thousands of times, soI kind of want to cheat a bit and use the deprecated method <code>Date.getMonth()</code> and <code>Date.getYear()</code></p>\n\n<pre><code>public static boolean sameMonth(Calendar month, Date date)\n{\n    return month.get(Calendar.YEAR) == date.getYear() &amp;&amp; month.get(Calendar.MONTH) == date.getMonth();\n}\n</code></pre>\n\n<p>I'm pretty close to just using this method, since it seems to be the fastest, but <strong>is there a faster way?</strong>  And <strong>is this a foolish way</strong>, since the <code>Date</code> methods are deprecated?  Note: This project will always run with Java 7</p>\n"},{"tags":["c#","wpf","performance","master-detail"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":2,"view_count":37,"score":-2,"question_id":13279655,"title":"How to make my WPF application as FAST as Outlook","body":"<p>The commons WPF applications take some time for loading medium complex views, once the view is loaded it works fine. For example in a Master - Detail view, if the Detail view is very complex and use different DataTemplates take some seconds (2-3 seconds) for load the view.</p>\n\n<p>When i open the Outlook application, for instance, it renders complex views and it is relative much more fast.</p>\n\n<p>Is there a way for increase the performance of my WPF application? Maybe a way for not loading the template's data every time that change the \"master\" item, and load it only one time in the app time live?</p>\n\n<p>i will appreciate any suggestion. </p>\n"},{"tags":["performance","variables","loops","initialization","declaration"],"answer_count":17,"favorite_count":19,"up_vote_count":69,"down_vote_count":1,"view_count":25283,"score":68,"question_id":407255,"title":"Difference between declaring variables before or in loop?","body":"<p>I have always wondered if, in general, declaring a throw-away variable before a loop, as opposed to repeatedly inside the loop, makes any (performance) difference? \nA (quite pointless example) in Java:</p>\n\n<p>a) declaration before loop:</p>\n\n<pre><code>double intermediateResult;\nfor(int i=0;i&lt;1000;i++){\n    intermediateResult = i;\n    System.out.println(intermediateResult);\n}\n</code></pre>\n\n<p>b) declaration (repeatedly) inside loop:</p>\n\n<pre><code>for(int i=0;i&lt;1000;i++){\n    double intermediateResult = i;\n    System.out.println(intermediateResult);\n}\n</code></pre>\n\n<p>Which one is better, a or b? </p>\n\n<p>I suspect that repeated variable declaration (example b) creates more overhead <em>in theory</em>, but that compilers are smart enough so that it doesn't matter. Example b has the advantage of being more compact and limiting the scope of the variable to where it is used. Still, I tend to code according example a...</p>\n\n<p>Edit: I am especially interested in the Java case.</p>\n"},{"tags":["javascript","arrays","performance","algorithm","multidimensional-array"],"answer_count":5,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":104,"score":7,"question_id":13275839,"title":"Fastest way to reset a multidimensional array?","body":"<p>Say I have a two dimensional array: <code>vectors[x][y]</code>, and the initial array structure looks like this:</p>\n\n<pre><code>vectors = [    \n [0, 0, 0, 0, 0,],\n [0, 0, 0, 0, 0,],\n [0, 0, 0, 0, 0,],\n [0, 0, 0, 0, 0,],\n [0, 0, 0, 0, 0,]\n]\n</code></pre>\n\n<p>After some calculations, the data in the array is randomized. What is the fastest way and most efficient way to return the array to it's initial state?</p>\n\n<p>I know that I could just hardcode the above zeroed array and set vectors equal to it again, but I also know that an algorithm such as:</p>\n\n<pre><code>for (var x = 0; x &lt; vectors.length; x++) {\n    for (var y = 0; y &lt; vectors[x].length; y++) {\n        vectors[x][y] = 0;\n    }\n\n}\n</code></pre>\n\n<p>is O(x * y).</p>\n\n<p>So which is the better way? And is there a better, even faster/more efficient way to solve this?</p>\n\n<p>And for the general case of zeroing a multi-dimensional array of any length, which is the best way? (I'm working in JavaScript if it matters)</p>\n"},{"tags":["ruby-on-rails-3","performance","activerecord"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":9,"score":0,"question_id":13271561,"title":"ActiveRecord: Is it possible to get the number of DB queries executed in addition to total time in the Rails log?","body":"<p>For every request, I get this in the logs:</p>\n\n<pre><code>Completed 200 OK in 854ms (Views: 1.0ms | ActiveRecord: 17.0ms)\n</code></pre>\n\n<p>Is it possible to get it to also include the number of queries?<br>\nSomething like:</p>\n\n<pre><code>Completed 200 OK in 854ms (Views: 1.0ms | ActiveRecord: 17.0ms | Queries: 10)\n</code></pre>\n\n<p>Ideally, I'd like all the \"cached\" ones to show up in that count too. Ie, even if the \"cache\" is saving me from \"N+1\" queries from hitting the DB, I still want to know I have a problem.</p>\n\n<p>I'm fine with monkeypatching / manually editing something, since I really want this just for my dev box. </p>\n\n<p>(If this can be made civilizedly so I can have it in production, that's even better, but if not, I'm fine with just having a manually modified Rails in my own machine)</p>\n\n<p>Thanks!<br>\nDaniel</p>\n"},{"tags":["performance","wordpress","time","blogs"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":6,"score":0,"question_id":13274366,"title":"wordpress blog excerpt prestation","body":"<p>I don't know how this page of my web site, takes much time to load:\nmichelepierri.it/blog \nIn this page there are post excerpts of blog.</p>\n\n<p>Instead other pages like home page, take less time to load.</p>\n\n<p>What can be causing this?\nThanks a lot.</p>\n\n<p>Plugins I use:</p>\n\n<pre><code>Advanced Code Editor\nAll in One SEO Pack\nBetter Related Content\ncbnet Twitter Widget\nCloudFlare\nContact Form 7\nDefault Thumbnail Plus\nDeveloper Formatter\nDisqus Comment System\nFancybox\nFast Secure Contact Form\nFeedBurner FeedSmith Extend\nGoogle Analytics\nGoogle XML Sitemaps\nlorem shortcode\nNextScripts: Social Networks Auto-Poster\nOfficial StatCounter Plugin\nPingler\nReally Simple CAPTCHA\nShareaholic | email, bookmark, share buttons\nSimple Skype Status\nSingle Category Permalink\nSkype Online Status\nSocial Metrics\nSyntaxHighlighter Plus\nTransposh Filtro per Traduzioni\nTrash Manager\nW3 Total Cache\nWP-Cumulus\nWP-o-Matic\nWP Facebook Open Graph protocol\nWP Minify\nWP to Twitter\nYoutube shortcode\n</code></pre>\n"},{"tags":["performance","numpy","scipy","sparse-matrix"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":21,"score":0,"question_id":13272453,"title":"Multiplying Numpy/Scipy Sparse and Dense Matrices Efficiently","body":"<p>I'm working to implement the following equation:</p>\n\n<pre><code>Xu = (Y.T * C * Y) ^ -1\n</code></pre>\n\n<p>Y is a (n x f) matrix and C is (n x n) diagonal one; n is about 300k and f will vary between 100 and 200. As part of an optimization process this equation will be used almost 100 million times so it has to be processed really fast.</p>\n\n<p>Y is initialized by:</p>\n\n<pre><code>from numpy import array, random as rd\nf = 100\nn = 300000\ny = array([rd.random(f) for i in range(n)])\n</code></pre>\n\n<p>Resulting:</p>\n\n<pre><code>&gt;&gt;&gt; y.shape\n&gt;&gt;&gt; (300000L, 100L)\n</code></pre>\n\n<p>C is a very sparse matrix and only a few numbers out of the 300k on the diagonal will be different than 0. Since Numpy's diagonal functions creates dense matrices, I created C as a sparse csr matrix. But when trying to solve the first part of the equation:</p>\n\n<pre><code>r = dot(C, Y)\n</code></pre>\n\n<p>I had to restart my computer as it crashed down trying to make the operation (I researched why but couldn't figure out why this happens).</p>\n\n<p>Then I tried:</p>\n\n<pre><code>r = C * Y\n</code></pre>\n\n<p>and it did work. But it took <strong>53 ms</strong> to do so. </p>\n\n<p>As another option I tried also:</p>\n\n<pre><code>from scipy.linalg import fblas as FB\nr = FB.dgemm(alpha=1., a=C, b=Y)\n</code></pre>\n\n<p>but I discovered dgemm function accepts only dense matrices.</p>\n\n<p>Since C has few elements I also tried to multiply the columns of Y by the scalars in C, like so:</p>\n\n<pre><code>Y.T[0] *= 2\n</code></pre>\n\n<p>but it took <strong>3 ms</strong> per element in C.</p>\n\n<p>I decided then trying to convert Y to csr_matrix and make the same operation:</p>\n\n<pre><code>r = C * Ysparse   \n</code></pre>\n\n<p>and this approach took <strong>1.38 ms</strong>. But this solution is somewhat \"tricky\" since I'm using a sparse matrix to store a dense one, I wonder how efficient this really is. And another problem to this approach is that the inverse operation in the end only accepts dense matrices as input so some time has to be lost transforming the sparse into dense.</p>\n\n<p>Is there some way of multiplying the sparse C and the dense Y without having to turn Y into sparse and improve performance? If somehow C could be represented as diagonal dense without consuming tons of memory maybe this would lead to very efficient performance but I don't know if this is possible.</p>\n\n<p>I appreciate your help!</p>\n"},{"tags":["performance","oracle","jpa","eclipselink","jpql"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":105,"score":0,"question_id":12641891,"title":"Fetch dates (by month or year) stored in TIMESTAMP using JPA","body":"<p>I am facing a problem and I would like you to help me.</p>\n\n<p>It turns out I have one table in my Oracle 11g database where I store failures of one electronic device. The table definition is following:</p>\n\n<pre><code>CREATE TABLE failure\n( failure_id NUMERIC NOT NULL\n, fecha TIMESTAMP NOT NULL\n, module_id NUMERIC NOT NULL\n, code NUMERIC\n, PRIMARY KEY(failure_id)\n);\n</code></pre>\n\n<p><em>Where 'fecha' means 'date'.</em></p>\n\n<p>I need to fetch failures by YEAR or by MONTH for one specific module but I can't. My ORM maps the TIMESTAMP type to java.sql.Date but I don't know how to compare the month in the JPQL sentence. I have tried to use ORACLE functions with native queries but I front with another issue: to cast the results.\nI am using JPA 2.0 with Eclipselink 2.3.2.</p>\n\n<p><strong>My doubts are:</strong></p>\n\n<p>Can I use Oracle functions with this version of Eclipselink library? My experience say no.</p>\n\n<pre><code>Query query = entityManager.createQuery(\"SELECT f FROM Failure f \"\n            + \"WHERE EXTRACT(YEAR FROM f.fecha) = ?1 \"\n            + \"AND f.moduleId.moduleId = ?2\");\n    query.setParameter(1, year);\n    query.setParameter(2, idModule);\n</code></pre>\n\n<p>I get this error: <code>Unexpected token [(]</code></p>\n\n<p>Can I use Eclipselink functions? My experience say no.</p>\n\n<pre><code>Query query = entityManager.createQuery(\"SELECT f FROM Failure f \"\n            + \"WHERE EXTRACT('YEAR', f.fecha) = ?1 \"\n            + \"AND f.moduleId.moduleId = ?2\");\n    query.setParameter(1, year);\n    query.setParameter(2, idModule);\n</code></pre>\n\n<p>Same error.</p>\n\n<p>Do you know a simple way to fetch this data using only one query?\nI know I can fetch one module and then check failures with loops but I think it is not the best performing solution.</p>\n\n<p>Thanks.</p>\n\n<p><strong>My sources:</strong></p>\n\n<ul>\n<li>Eclipselink JPA functions <a href=\"http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Basic_JPA_Development/Querying/Support_for_Native_Database_Functions#Functions\" rel=\"nofollow\">link</a></li>\n<li>Eclipselink Query Enhancements <a href=\"http://wiki.eclipse.org/EclipseLink/Release/2.1.0/JPAQueryEnhancements\" rel=\"nofollow\">link</a></li>\n</ul>\n"},{"tags":["linux","performance","node.js","iis"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":35,"score":2,"question_id":13271057,"title":"node.js vs IIS 7.5 performance","body":"<h2>The setup:</h2>\n\n<p>2 machines on EC2 of type m3.xlarge.</p>\n\n<p>First one with ubuntu server.</p>\n\n<p>Second one win2008r2.</p>\n\n<p>node.js on ubuntu using the basic example to return a string response to any request.</p>\n\n<p>asp.net httphandler to return the same response.</p>\n\n<p>using <a href=\"https://github.com/newsapps/beeswithmachineguns\" rel=\"nofollow\">https://github.com/newsapps/beeswithmachineguns</a> \nI used 10 machines to execute 200000 with concurrency of 2000 (200 per machine)\nI ran the benchmark and got:</p>\n\n<h2>NodeJS:</h2>\n\n<pre><code> Complete requests:         200000\n\n Requests per second:       5605.170000 [#/sec] (mean)\n\n Time per request:          358.071900 [ms] (mean)\n\n 50% response time:         31.000000 [ms] (mean)\n\n 90% response time:         239.300000 [ms] (mean)\n</code></pre>\n\n<h2>IIS:</h2>\n\n<pre><code> Complete requests:         200000\n\n Requests per second:       9263.810000 [#/sec] (mean)\n\n Time per request:          215.992900 [ms] (mean)\n\n 50% response time:         214.000000 [ms] (mean)\n\n 90% response time:         244.000000 [ms] (mean)\n</code></pre>\n\n<p>The nodeJS code is:</p>\n\n<pre><code>http.createServer(function (request, response) {\n  response.writeHead(200, {'Content-Type': 'text/plain'});\n  response.end('Some response\\n');\n}).listen(80);\n</code></pre>\n\n<p>The httphandler code is:</p>\n\n<pre><code>context.Response.Write(\"Some response\\n\" + Guid.NewGuid().ToString(\"N\"));\n</code></pre>\n\n<p>I thought node js will be much faster, did I do something wrong?</p>\n\n<h2>EDIT:</h2>\n\n<p>after using the cluster module I got 16685 request per second from the node js\nI\"m going to bring up the strongest EC2 instances and check on them</p>\n"},{"tags":["wpf","performance","image","decode","bitmapimage"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":55,"score":2,"question_id":13054909,"title":"BitmapImage decoding speed performance wpf","body":"<p>I have 5 images all the same pixel height and pixel width (2481 * 3508 for that matter). But, one is gif, one jpeg, one png and one bmp. Now I render them into a BitmapSource with (1) two thirds of the original pixel height for DecodePixelHeight and (2) original pixel height for DecodePixelHeight. </p>\n\n<p>First scenario: </p>\n\n<pre><code>bitmapImage.BeginInit();\nbitmapImage.CreateOptions = BitmapCreateOptions.IgnoreColorProfile;\nbitmapImage.CacheOption = BitmapCacheOption.OnLoad;\nbitmapImage.DecodePixelHeight = 2/3 * originalHeight;\nbitmapImage.StreamSource = streamWithTheFile;\nbitmapImage.EndInit();\nbitmapImage.Freeze();\n</code></pre>\n\n<p>BMP and Jpeg are equally slow. Png and Gif need less than half the time. Why?</p>\n\n<p>Second scenario:</p>\n\n<pre><code>bitmapImage.BeginInit();\nbitmapImage.CreateOptions = BitmapCreateOptions.IgnoreColorProfile;\nbitmapImage.CacheOption = BitmapCacheOption.OnLoad;\nbitmapImage.StreamSource = streamWithTheFile;\nbitmapImage.EndInit();\nbitmapImage.Freeze();\n</code></pre>\n\n<p>Png half of the time needed before. Jpeg and BMP one 5th of the time needed before. Gif the same time as before.</p>\n\n<p>According to <a href=\"http://msdn.microsoft.com/en-us/library/system.windows.media.imaging.bitmapimage.decodepixelheight.aspx\" rel=\"nofollow\">documentation</a> I would have assumed that Png and Jpeg performance would somehow be more independent of actual decode size than the other formats.  What could be the reason, that it is not?</p>\n"},{"tags":["windows","performance","drupal","localhost","easyphp"],"answer_count":4,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":116,"score":2,"question_id":13148645,"title":"SLOW Drupal over localhost | windows7 EasyPHP 64x","body":"<p><br>\nI am running Drupal 7.16 on my laptop(Windows7 x64 with 6gb RAM over EasyPHP12.1).<br>\nFor now the drupal running very slow!</p>\n\n<p>I already try to:</p>\n\n<ol>\n<li>Increase php.ini <code>realpath_cache_size</code> to <code>24M</code></li>\n<li>Change my.ini <code>innodb_flush_log_at_trx_commit</code> to <code>0</code></li>\n<li>Change the <code>hosts</code> file to resolve ipv6 bug..</li>\n<li>Try another wamp solution</li>\n<li>It seems that when I run simple query(<code>SELECT uid FROM users</code>) the phpMyAdmin return a quick respond(0.0009s)..</li>\n<li>Another drupal clean installation load also slow...</li>\n</ol>\n\n<p>Thanks,<br>\n~Almog</p>\n\n<p>* <em>I also tried UniformServer and it still slow, and changing the my.ini follwing the posts over here(stackexchange websites) and follwing drupal.org</em><br>\n** <em>It's seems that wordpress load fast, so it seems that the problem is with the drupal only?</em></p>\n"},{"tags":["unit-testing","performance","mstest"],"answer_count":6,"favorite_count":1,"up_vote_count":8,"down_vote_count":0,"view_count":776,"score":8,"question_id":3824762,"title":"How slow is too slow for unit tests?","body":"<p>Michael Feathers, in <em>Working Effectively With Legacy Code</em>, on pages 13-14 mentions:</p>\n\n<blockquote>\n  <p>A unit test that takes 1/10th of a\n  second to run is a slow unit test...\n  If [unit tests] don't run fast, they\n  aren't unit tests.</p>\n</blockquote>\n\n<p>I can understand why 1/10th a second is too slow if one has 30,000 tests, as it would take close to an hour to run. However, does this mean 1/11th of a second is any better? No, not really (as it's only 5 minutes faster). So a hard fast rule probably isn't perfect.</p>\n\n<p>Thus when considering how slow is too slow for a unit tests, perhaps I should rephrase the question. <strong>How long is too long for a developer to wait for the unit test suite to complete?</strong></p>\n\n<p>To give an example of test speeds. Take a look at several MSTest unit test duration timings:</p>\n\n<pre><code>0.2637638 seconds\n0.0589954\n0.0272193\n0.0209824\n0.0199389\n0.0088322\n0.0033815\n0.0028137\n0.0027601\n0.0008775\n0.0008171\n0.0007351\n0.0007147\n0.0005898\n0.0004937\n0.0004624\n0.00045\n0.0004397\n0.0004385\n0.0004376\n0.0003329\n</code></pre>\n\n<p>The average for all 21 of these unit tests comes to 0.019785 seconds. Note the slowest test is due to it using Microsoft Moles to mock/isolate the file system. </p>\n\n<p>So with this example, if my unit test suite grows to 10,000 tests, it <em>could</em> take over 3 minutes to run. </p>\n"},{"tags":["performance","mongomapper"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":8,"score":0,"question_id":13272557,"title":"Mongo Mapper taking too long for query","body":"<p>I'm using Mongo Mapper and it is taking 1.1 second to retrieve only 15 objects.\nThe document has more than 200.000 rows.\nFor my concern mongo should be fast, right? please help me .\nps: the document has indexes as well.</p>\n\n<pre><code>class Mention\ninclude MongoMapper::Document\n\ndef all (options = {})\n      mentions = Mention.paginate({\n         :order    =&gt; \"data_captura desc,\n         :per_page =&gt; 15, \n         :page     =&gt; (options[:pagination_page].to_i || 1),\n      })\n    end\nend\n</code></pre>\n"},{"tags":["performance","entity-framework","generics","dbcontext"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":20,"score":1,"question_id":13270418,"title":"Generics around Entityframework DbContext causes performance degradation?","body":"<p>I wrote a simple import/export application that transforms data from source->destination using EntityFramework and AutoMapper. It basically:</p>\n\n<ol>\n<li><strong>selects</strong> <code>batchSize</code> of records from the source table</li>\n<li>'<strong>maps</strong>' data from source->destination entity</li>\n<li><strong>add</strong> new destination entities to destination table and <strong>saves</strong> context</li>\n</ol>\n\n<p>I move around <em>500k records in under 5 minutes</em>. After I <em>refactored</em> the code <em>using generics</em> the performance <em>drops drastically to 250 records in 5 minutes.</em></p>\n\n<p><strong>Are my delegates that return <code>DbSet&lt;T&gt;</code> properties on the <code>DbContext</code> causing these problems? Or is something else going on?</strong></p>\n\n<h1>Fast non-generic code:</h1>\n\n<pre><code>public class Importer\n{        \n    public void ImportAddress()\n    {\n        const int batchSize = 50;\n        int done = 0;\n        var src = new SourceDbContext();\n\n        var count = src.Addresses.Count();\n\n        while (done &lt; count)\n        {\n            using (var dest = new DestinationDbContext())\n            {\n                var list = src.Addresses.OrderBy(x =&gt; x.AddressId).Skip(done).Take(batchSize).ToList();\n                list.ForEach(x =&gt; dest.Address.Add(Mapper.Map&lt;Addresses, Address&gt;(x)));\n\n                done += batchSize;\n\n                dest.SaveChanges();\n            }\n        }\n\n        src.Dispose();\n    }\n}\n</code></pre>\n\n<h1>(Very) slow generic code:</h1>\n\n<pre><code>public class Importer&lt;TSourceContext, TDestinationContext&gt;\n    where TSourceContext : DbContext\n    where TDestinationContext : DbContext\n{\n    public void Import&lt;TSourceEntity, TSourceOrder, TDestinationEntity&gt;(Func&lt;TSourceContext, DbSet&lt;TSourceEntity&gt;&gt; getSourceSet, Func&lt;TDestinationContext, DbSet&lt;TDestinationEntity&gt;&gt; getDestinationSet, Func&lt;TSourceEntity, TSourceOrder&gt; getOrderBy) \n        where TSourceEntity : class\n        where TDestinationEntity : class\n    {\n        const int batchSize = 50;\n        int done = 0;\n        var ctx = Activator.CreateInstance&lt;TSourceContext&gt;();\n        //Does this getSourceSet delegate cause problems perhaps?\n\n        //Added this\n        var set = getSourceSet(ctx);\n\n        var count = set.Count(); \n\n        while (done &lt; count)\n        {\n            using (var dctx = Activator.CreateInstance&lt;TDestinationContext&gt;())\n            {\n                var list = set.OrderBy(getOrderBy).Skip(done).Take(batchSize).ToList(); \n                //Or is the db-side paging mechanism broken by the getSourceSet delegate?\n                //Added this\n                var destSet = getDestinationSet(dctx);\n                list.ForEach(x =&gt; destSet.Add(Mapper.Map&lt;TSourceEntity, TDestinationEntity&gt;(x)));\n\n                done += batchSize;\n                dctx.SaveChanges();\n            }\n        }\n\n        ctx.Dispose();\n    }\n}\n</code></pre>\n"},{"tags":["windows","performance","driver","wdm"],"answer_count":2,"favorite_count":2,"up_vote_count":6,"down_vote_count":0,"view_count":254,"score":6,"question_id":8627590,"title":"Will moving code into kernel space give more precise timing?","body":"<p>Background information:</p>\n\n<p>I presently have a hardware device that connects to the USB port.  The hardware device is responsible sending out precise periodic messages onto various networks that it, in turn, connects too.  Inside the hardware device I have a couple Microchip dsPICs.  There are two modes of operation.  </p>\n\n<p>One scenario is where send simple \"jobs\" down to the dsPICs that, in turn, can send out the precise messages with .001ms accuracy.  This architecture is not ideal for more complex messaging where we need to send a periodic packet that changes based on events going on within the PC application.  So we have a second mode of operation where our PC application will send the periodic messages and the dsPICs simply convert and transmit in response.  All this, by the way, is transparent to the end user of our software.  Our hardware device is a test tool used in the automotive field.</p>\n\n<p>Currently, we use a USB to serial chip from FTDI and the FTDI Windows drivers to interface the hardware to our PC software.</p>\n\n<p>The problem is that in mode two where we send messages from the PC, the best we are able to achieve is around 1ms on average hardware range.  We are subjected to Windows kernel pre-emption.  I've tried a number of \"tricks\" to improve things such as:</p>\n\n<ol>\n<li>Making sure our reader &amp; writer threads live on seperate CPU affinities when possible.</li>\n<li>Increasing the thread priority of the writer while reducing that of the reader.</li>\n<li>Informing the user to turn off screen saver and other applications when using our software.</li>\n<li>Replacing createthread calls with CreateTimerQueueTimer calls.</li>\n</ol>\n\n<p>All our software is written in C/C++.  I'm very familiar and comfortable with advanced Windows programming; such as IO Completions, Overlapped I/O, lockless thread queues (really a design strategy), sockets, threads, semaphores, etc...</p>\n\n<p>However, I know nothing about Windows driver development.  I've read through a few papers on KMDF vs. UDMF vs. WDM.  </p>\n\n<p>I'm hoping a seasoned Windows kernel mode driver developer will respond here... </p>\n\n<p>The next rev. of our hardware has the option to replace the FTDI chip and use either the dsPIC's USB interface or, possibly, port the open source Linux FTDI stuff to Windows and continue to use the FTDI chip within our custom driver.  I think by going to a kernel mode driver on the PC side, I can establish a kernel driver that can send out periodic messages at more precise intervals without preemption and/or possibly taking advantage of DMA.</p>\n\n<p>We have a competitor in our business who I think does exactly something similar with their tools.  As far as I know, user space applications can not schedule a thread any better than 1ms.  We currently use timeGetTime in a thread.  I've experiemented with timer queues (via CreateTimerQueueTimer) with no real improvement.</p>\n\n<p>Is a WDM the correct approach to achieve more precise timing?</p>\n\n<p>Our competitor some how is achieveing very precise timing from Windows driven signals to their hardware and they do load a kernel driver (.sys) and their device runs over USB2.0 as does ours.</p>\n\n<p>If WDM is the way to go, can I get some advise on what kernel functions I should be studying for setting up the timings?\nThanks for reading</p>\n"},{"tags":["c#","performance"],"answer_count":6,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":1395,"score":4,"question_id":2443164,"title":"Does variable name length matter for performance C#?","body":"<p>I've been wondering if using long descriptive variable names in WinForms C# matters for performance? I'm asking this question since in AutoIt v3 (interpreted language) it was brought up that having variables with short names like <code>aa</code> instead of <code>veryLongVariableName</code> is much much faster (when program is bigger then 5 liner). I'm wondering if it's the same in C#?</p>\n"},{"tags":["sql","performance"],"answer_count":3,"favorite_count":1,"up_vote_count":0,"down_vote_count":1,"view_count":38,"score":-1,"question_id":13270502,"title":"sql - what is the difference between these two queries?","body":"<pre><code>select a.Name from rat. Assessment a where a.AssessmentId = '3'\nselect a.Name from rat. Assessment a where a.AssessmentId = 3\n</code></pre>\n\n<p>whats the difference performance wise ?\nis it slow for the first one ?</p>\n\n<p>How can i know if sql server has casted the the column or value to match the the column type or value type ?</p>\n\n<p>I saw the execution plan. cant understand much.</p>\n"},{"tags":["java","performance","garbage-collection","java1.4"],"answer_count":3,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":3319,"score":3,"question_id":9005632,"title":"UseConcMarkSweepGC vs UseParallelGC","body":"<p>I'm currently having problems with very long garbage collection times. please see the followig. My current setup is that I'm using a -Xms1g and -Xmx3g. my application is using java 1.4.2. I don't have any garbage collection flags set. by the looks of it, 3gb is not enough and I really have a lot of objects to garbage collect.</p>\n\n<p>question:</p>\n\n<p>should I change my garbage collection algorithm?\nwhat should i use? is it better to use <code>-XX:+UseParallelGC or -XX:+UseConcMarkSweepGC</code></p>\n\n<p>or should i use this combination</p>\n\n<pre><code>-XX:+UseParNewGC -XX:+UseConcMarkSweepGC\n</code></pre>\n\n<p>the ones occupying the memory are largely reports data and not cache data. also, the machine has 16gb memory and I plan to increase the heap to 8gb. </p>\n\n<p>What are the difference between the two options as I still find it hard to understand.\nthe machine has multiple processors. I can take hits of up to 5 seconds but 30 to 70 seconds is really hard.</p>\n\n<p>Thanks for the help.</p>\n\n<pre><code>  Line 151493: [14/Jan/2012:11:47:48] WARNING ( 8710): CORE3283: stderr: [GC 1632936K-&gt;1020739K(2050552K), 1.2462436 secs]\n    Line 157710: [14/Jan/2012:11:53:38] WARNING ( 8710): CORE3283: stderr: [GC 1670531K-&gt;1058755K(2050552K), 1.1555375 secs]\n    Line 163840: [14/Jan/2012:12:00:42] WARNING ( 8710): CORE3283: stderr: [GC 1708547K-&gt;1097282K(2050552K), 1.1503118 secs]\n    Line 169811: [14/Jan/2012:12:08:02] WARNING ( 8710): CORE3283: stderr: [GC 1747074K-&gt;1133764K(2050552K), 1.1017273 secs]\n    Line 175879: [14/Jan/2012:12:14:18] WARNING ( 8710): CORE3283: stderr: [GC 1783556K-&gt;1173103K(2050552K), 1.2060946 secs]\n    Line 176606: [14/Jan/2012:12:15:42] WARNING ( 8710): CORE3283: stderr: [Full GC 1265571K-&gt;1124875K(2050552K), 25.0670316 secs]\n    Line 184755: [14/Jan/2012:12:25:53] WARNING ( 8710): CORE3283: stderr: [GC 2007435K-&gt;1176457K(2784880K), 1.2483770 secs]\n    Line 193087: [14/Jan/2012:12:37:09] WARNING ( 8710): CORE3283: stderr: [GC 2059017K-&gt;1224285K(2784880K), 1.4739291 secs]\n    Line 201377: [14/Jan/2012:12:51:08] WARNING ( 8710): CORE3283: stderr: [Full GC 2106845K-&gt;1215242K(2784880K), 30.4016208 secs]\n\n\nxaa:1: [11/Oct/2011:16:00:28] WARNING (17125): CORE3283: stderr: [Full GC 3114936K-&gt;2985477K(3114944K), 53.0468651 secs] --&gt; garbage collection occurring too often as noticed in the time. garbage being collected is quite low and if you would notice is quite close the the heap size. during the 53 seconds, this is equivalent to a pause.\nxaa:2087: [11/Oct/2011:16:01:35] WARNING (17125): CORE3283: stderr: [Full GC 3114943K-&gt;2991338K(3114944K), 58.3776291 secs]\nxaa:3897: [11/Oct/2011:16:02:33] WARNING (17125): CORE3283: stderr: [Full GC 3114940K-&gt;2997077K(3114944K), 55.3197974 secs]\nxaa:5597: [11/Oct/2011:16:03:00] WARNING (17125): CORE3283: stderr: [Full GC[Unloading class sun.reflect.GeneratedConstructorAccessor119]\nxaa:7936: [11/Oct/2011:16:04:36] WARNING (17125): CORE3283: stderr: [Full GC 3114938K-&gt;3004947K(3114944K), 55.5269911 secs]\nxaa:9070: [11/Oct/2011:16:05:53] WARNING (17125): CORE3283: stderr: [Full GC 3114937K-&gt;3012793K(3114944K), 70.6993328 secs]\n</code></pre>\n"},{"tags":["php","xml","json","performance"],"answer_count":7,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":5173,"score":4,"question_id":993282,"title":"PHP: is JSON or XML parser faster?","body":"<p>I'm building classes that interface with the Twitter API, and I'm wondering whether PHP's built-in XML or JSON parser is faster? Twitter will send me the same data in either format, so  PHP performance will determine my choice. I'm using php_apc, so you can disregard parse time and assume I'm running off bytecode.</p>\n\n<p>Thanks!</p>\n\n<p>more: I'm just looking to get associative arrays from the data. I'm not doing tree walking, node iteration or anything too complex. The format will always be the same. (I hope!)</p>\n"},{"tags":["performance","postgresql"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":27,"score":1,"question_id":13266869,"title":"Best config options for PostgreSQL (development machine)","body":"<p>My question is simple: I'm using PostgreSQL 9.1 (will use 9.2 some time later) on my development machine (Linux amd64, quad core, 8GB ram). I've already turned fsync off for speed (because I don't care if my computer crashes, I can rebuild the schema and data at anytime), but I'm a bit confused about tuning the other factory defaults. I want it to be as fast as possible.</p>\n\n<p>Distribution: Linux Mint Debian Edition (latest update pack)</p>\n"},{"tags":["performance","jmeter"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":13199383,"title":"JMeter different results during replay of test","body":"<p>I have a strange problem with JMeter.<br>\nI've made recording of some sort of web application without any problems. Problem appears during playback of test.\nFor some reason I receive different results during playback than during recording.</p>\n\n<p>When I compare Http Request made during recording and playback I don't see a single difference (except for some security token which I'm extracting from earlier requests and passing as parameter).\nTo be more exact during recording I receive a response with a big body (>5kB), and during playback body of response is empty. Response code is 200 (OK).\nThis body contains crucial data from database, so I'm afraid that measurement made by this JMeter script will not reflect actual behavior of application, simply I will not measure what I really need.</p>\n\n<p>Now my questions: </p>\n\n<ol>\n<li>is there some tool or JMeter plug-in which will allow more effectively see contents of HTTP requests and its responses? It would be great If I could compare of requests made during recording and playback. So far I used two listeners: \"View Results Tree\". I've sandwiched between them to compare request from recording and playback.</li>\n<li>is there some known bug in JMeter which could explain the difference? For example something related to recording process?</li>\n</ol>\n\n<p>Here is example of request:</p>\n\n<pre><code>POST http://10.133.27.81:8080/c/portal/render_portlet\n\nPOST data:\np_l_id=69210&amp;p_p_id=blank_WAR_Blank_INSTANCE_iNM3&amp;p_p_action=0&amp;p_p_state=normal&amp;p_p_mode=view&amp;p_p_col_id=column-2&amp;p_p_col_pos=1&amp;p_p_col_count=2\n\n[no cookies]\n\nRequest Headers:\nConnection: keep-alive\nContent-Type: application/x-www-form-urlencoded\nAccept-Language: pl\nAccept: */*\nUser-Agent: Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)\ncsrf_token: 1GXK-0QD7-GFPJ-JLDG-JP2G-J390-BFLG-7LL7\nPragma: no-cache\nMethod: POST /c/portal/render_portlet HTTP/1.1\nX-Requested-With: OWASP CSRFGuard Project\nReferer: http://10.133.27.81:8080/group/bou\nAccept-Encoding: gzip, deflate\nContent-Length: 143\nHost: 10.133.27.81:8080\n</code></pre>\n\n<hr>\n\n<p><strong>Update</strong>: to make sure which headers or parameters are constant I made 4 recordings of same test case during different sessions and compared them, so I'm quite sure that only <code>csrf_token</code> has to be field with value fetched from other request. I've added debug sampler to verify that this value is fetched properly.</p>\n\n<hr>\n\n<p><strong>Update 2</strong>: Problem found.<br>\nThere where two problems:</p>\n\n<ol>\n<li>There is a bug in JMeter when you do a search (Ctrl-F) it searches the whole project except for <code>HTTP Header Menager</code>s and my request contained <code>csrf_token</code> inside of header (I detected that before posting this question). Making a search in xml using text editor was good workaround for that.</li>\n<li>when I try to find source of problems, before I've found problem number one, I've added a new problem by removing a <code>HTTP Cookie Manager</code> (I'm blaming myself and IE for this).</li>\n</ol>\n\n<p>Generally changing Internet Explorer to FireFox with HttpFox add-on help to spot the problem.</p>\n\n<p>Thanks everyone for support.</p>\n\n<p>Marek</p>\n"},{"tags":["performance",".htaccess","optimization","cdn"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":9,"score":0,"question_id":13269144,"title":"Using a not real CDN for images is still useful?","body":"<p>I have a site with heavy traffic and I've read that moving images on another server could improve performances, best if this second server is on a CDN.</p>\n\n<p>www.mysite.com ---> web site\nimages.mysite.com ---> server for images</p>\n\n<p>if images aren't really on a different server but the images.mysite.com is just a subdirectory on  the same server, same web site, but handled only with htaccess redirects... is this still useful to split requests to server and improve speed for browser?</p>\n"},{"tags":["c++","python","performance","list","boost-python"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":49,"score":2,"question_id":12929196,"title":"How to speed up Boost::Python::Extract when passing a list from python to a C++ vector","body":"<p>I'm new to boost.python and have made a simple function for passing a list from python to a C++ vector:</p>\n\n<pre><code>void SetXValues(boost::python::list xl){\n    int n = len((xl));\n    xvals.resize(n);\n    for(unsigned int i=0; i&lt;n; i++){\n    xvals[i] = boost::python::extract&lt;double&gt;((xl)[i]);\n    }\n}\n</code></pre>\n\n<p>xvals is an C++ STL vector. This function works and I can load the python list into C++ but it seems extremely slow. </p>\n\n<p>A small speed test I did was to write a binning algorithm in C++ and in pure Python. The results show that the C++ method is only 5x faster when the time to pass the data from Python is included but of course the binning algorithm alone is considerably faster (74x).</p>\n\n<p>So is there any way to improve the function above to make it more efficient? </p>\n"},{"tags":["javascript","ajax","performance","google-chrome","heap"],"answer_count":0,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":60,"score":5,"question_id":13240866,"title":"JavaScript Heap Size and Chrome Increasing","body":"<p>I have a single web page application that is all JavaScript.  I noticed the JavaScript heap size goes up on each AJAX call that returns a new view.  Is there something I should be doing to clean up the older views?</p>\n"},{"tags":["performance","r","join","merge","data.table"],"answer_count":3,"favorite_count":22,"up_vote_count":28,"down_vote_count":0,"view_count":3663,"score":28,"question_id":4322219,"title":"What's the fastest way to merge/join data.frames in R?","body":"<p>For example (not sure if most representative example though):</p>\n\n<pre><code>N &lt;- 1e6\nd1 &lt;- data.frame(x=sample(N,N), y1=rnorm(N))\nd2 &lt;- data.frame(x=sample(N,N), y2=rnorm(N))\n</code></pre>\n\n<p>This is what I've got so far:</p>\n\n<pre><code>d &lt;- merge(d1,d2)\n# 7.6 sec\n\nlibrary(plyr)\nd &lt;- join(d1,d2)\n# 2.9 sec\n\nlibrary(data.table)\ndt1 &lt;- data.table(d1, key=\"x\")\ndt2 &lt;- data.table(d2, key=\"x\")\nd &lt;- data.frame( dt1[dt2,list(x,y1,y2=dt2$y2)] )\n# 4.9 sec\n\nlibrary(sqldf)\nsqldf()\nsqldf(\"create index ix1 on d1(x)\")\nsqldf(\"create index ix2 on d2(x)\")\nd &lt;- sqldf(\"select * from d1 inner join d2 on d1.x=d2.x\")\nsqldf()\n# 17.4 sec\n</code></pre>\n"},{"tags":["java","performance","iteration"],"answer_count":8,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":6098,"score":4,"question_id":1006395,"title":"Fastest way to iterate in Java","body":"<p>In Java, is it faster to iterate through an array the old-fashioned way,</p>\n\n<pre><code>for (int i = 0; i &lt; a.length; i++)\n    f(a[i]);\n</code></pre>\n\n<p>Or using the more concise form,</p>\n\n<pre><code>for (Foo foo : a)\n    f(foo);\n</code></pre>\n\n<p>For an ArrayList, is the answer the same?</p>\n\n<p>Of course for the vast bulk of application code, the answer is it makes no discernible difference so the more concise form should be used for readability. However the context I'm looking at is heavy duty technical computation, with operations that must be performed billions of times, so even a tiny speed difference could end up being significant.</p>\n"},{"tags":["c++","c","performance","gcc","sse"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":71,"score":1,"question_id":13267354,"title":"segmentation fault in SSE when -O3 is on","body":"<p>I found a very strange behaviour when using gcc's <strong><em>-O3</em></strong> or <strong><em>-O2</em></strong>  option.</p>\n\n<p>When my program is running under debug mode (<strong><em>-g</em></strong>), it is fine; but it raises a segmentation fault when I turned on <strong><em>-O3</em></strong> or <strong><em>-O2</em></strong>.</p>\n\n<p>The segmentation happens when it is running a function with <strong>SSE2</strong> macro inside; like</p>\n\n<pre><code>_m128i polynomial = _mm_set1_epi8(0x1d)\n</code></pre>\n\n<p>This is only part of the code.</p>\n\n<p>I think I have already eliminate the situation of address alignment on 16 bytes. It's so wired that the -g mode and the <strong><em>-O2</em></strong> or <strong><em>-O3</em></strong> mode behaves differently.</p>\n\n<p>Actually, I am not sure the bug is related to SSE2 or not.</p>\n\n<p>I am using <strong><em>gcc 4.4.3</em></strong>.</p>\n\n<p><strong>Have you encountered the same problem?</strong></p>\n\n<p><strong>Or can you gave me some suggestion on how to deal with it?</strong></p>\n"},{"tags":["android","performance","debugging","sha256"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":12,"score":0,"question_id":13267405,"title":"Spongycastle SHA256 update function is too slow in Debug session","body":"<p>I am trying to verify the signature of a file in our apk package. The file is about 1.5MB and \nI am first reading the file into a byte array and then start the signature verfication with the following code:</p>\n\n<pre class=\"lang-java prettyprint-override\"><code>// verify signature\nSignature sig = Signature.getInstance(\"SHA256withRSA\", new BouncyCastleProvider());\nsig.initVerify(pubKey);\nsig.update(dataBytes);\nreturn sig.verify(sigBytes); \n</code></pre>\n\n<p>When I run my app on the device normally then it takes about 2.5 seconds to pass the signature verification, but in debug session, it takes about 2.5 minutes. I checked where it does take so long to perform, and I saw that update function of SHA256Digest class uses this time to calculate.</p>\n\n<p>Did anyone have a similar problem, or any idea what might cause this performance issue during debug session.</p>\n"},{"tags":["python","performance","timer","pypy"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":27,"score":1,"question_id":13267316,"title":"Python: process average timing, first one/two are much slower","body":"<p>I'm writing some program with sorting algorithms comparsion using Python. I want to measure average sorting time. I have a problem with first measurement.</p>\n\n<p>This:</p>\n\n<pre><code>for i in xrange(self.repeats):\n    # random list generator\n    data_orig = [random.randint(0, self.size - 1) for x in xrange(self.size)]\n\n    sorter = self.class_()\n    data = data_orig[:]\n    debug(\"%s for data size: %d, try #%d\" % (sorter.__class__.__name__, self.size, i+1))\n    t1 = time.clock()\n    sorter.sort(data)\n    t2 = time.clock()\n    debug(\"Took: %0.4fms, shifts: %d, comparisons: %d\" % ((t2-t1)*1000.0, sorter.shifts, sorter.comps))\n</code></pre>\n\n<p><code>class_</code> is a reference to InsertionSort class.\nFor size = 1000 and 5 repeats I get following results:</p>\n\n<pre><code>InsertionSort for data size: 1000, try #1\nTook: 39.5341ms, shifts: 254340, comparisons: 255331\nInsertionSort for data size: 1000, try #2\nTook: 6.0765ms, shifts: 250778, comparisons: 251772\nInsertionSort for data size: 1000, try #3\nTook: 6.9946ms, shifts: 254189, comparisons: 255180\nInsertionSort for data size: 1000, try #4\nTook: 6.7421ms, shifts: 252162, comparisons: 253156\nInsertionSort for data size: 1000, try #5\nTook: 5.9584ms, shifts: 241412, comparisons: 242404\n</code></pre>\n\n<p>For every sorting algorithm and every time I run program first result is bigger than the others. I run it with PyPy (with Python it seems OK, but it's MUCH slower).</p>\n\n<p>I know I can simply ommit first results but this solution doesn't satisfies me :-)</p>\n\n<p>Any ideas?</p>\n"},{"tags":["performance","opencv","cpu","environment","mingw32"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":55,"score":0,"question_id":13241691,"title":"Slow CPU + Windows Explorer + Everything when programming in C++ using MinGW compiler and Code::Blocks IDE","body":"<p>When I was working with OpenCV, I added the build/install folder (i.e. where I had my mingw32-make and mingw32-install files created) to PATH variable. However, this made my PC quite slow after I did a few builds on my CodeBlocks IDE. I didn't forget to deallocate memory or anything like that which might cause trouble. Even a simple hello world program takes ages to run after a few builds. I am using i7 with 8GB of RAM and L3 cache which, I believe, is good enough for any development. After I removed the OpenCV directory from my PATH, it was okay, but not permanently. What could the problem be?</p>\n\n<p>I also checked my environment variables and there is no garbage variable in there! I only have MinGW32 compiler i.e. the bin folder in PATH. But it is necessary (I think!).</p>\n\n<p>I am not sure if it has something to do with CodeBlocks itself! I have another PC where I use Visual Studio 2010 Professional and it doesn't have the same problem.</p>\n\n<p>I have manage to isolate the problem around the usage of MinGW compiler. The reason is that the problem occurs only when I am using it with an IDE (tried with Eclipse CDT, Dev CPP, and CodeBlocks). I got a stable release of MinGW downloaded from sourceforge.net which shouldn't have any major memory management issue with Windows Platform. Actually, the problem arises if I use it at all (doesn't start immediately, but after I have run my simply program a few times!).</p>\n\n<p>My MinGW compiler is from <a href=\"http://sourceforge.net/projects/mingw/files/Installer/mingw-get-inst/mingw-get-inst-20120426/\" rel=\"nofollow\">http://sourceforge.net/projects/mingw/files/Installer/mingw-get-inst/mingw-get-inst-20120426/</a></p>\n\n<p>I couldn't find a very straightforward answer to this, so I am assuming that this is an unusual problem? If it is a repeated question by any chance which I have missed, please post a link to the comment and I will accept the answer. Ta</p>\n"},{"tags":["java","performance","math","sqrt"],"answer_count":3,"favorite_count":1,"up_vote_count":2,"down_vote_count":1,"view_count":95,"score":1,"question_id":13263948,"title":"Fast sqrt in Java at the expense of accuracy","body":"<p>I am looking for a fast square root implementation in Java for double values in the input range of [0, 2*10^12]. For any value in this range, the precision should be upto 5 decimal places. In other words, the result can differ from the <code>Math.sqrt()</code> method after 5 decimal places. However, this method needs to be much faster than <code>Math.sqrt()</code>.</p>\n\n<p>Any ideas? Thanks!</p>\n"},{"tags":["c#",".net","winforms","performance","logging"],"answer_count":7,"favorite_count":4,"up_vote_count":1,"down_vote_count":0,"view_count":6020,"score":1,"question_id":5057567,"title":"How to do logging in c#?","body":"<p>I would like to implement logging in my application but would rather not use any outside frameworks like log4net.</p>\n\n<p>So I would like to do something like Dos's echo to file. What is the most effective way to do it?</p>\n\n<p>Is there a way to log unhandled exceptions logged without using an outside framework?</p>\n"},{"tags":["php","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":36,"score":-1,"question_id":13265093,"title":"Using too many if blocks is good ? what are possible alternatives?","body":"<p>I have been designing a shopping website lately in PHP. Now in Checkout page i have to check many times if the user is logged in. i.e If the user is logged in <em>hide the Signin div</em> and <em>show the Shipping div</em>.</p>\n\n<pre><code>&lt;section class=\"panels &lt;?php if(!isset($_SESSION['username'])) echo\"unactivepanel\"; else echo\"activepanel\";?&gt;\" id=\"Sign In\"&gt;\n        &lt;div class=\"wrapper1\" &gt;\n            &lt;ul&gt;\n            &lt;li&gt;&lt;input type=\"email\" placeholder=\"Enter your email address\" required/&gt;&lt;/li&gt;\n            &lt;li class=\"inf\"&gt;(will only be used for sending you order information.)&lt;/li&gt;\n            &lt;fieldset&gt;&lt;legend class=\"orb\"&gt; OR&lt;/legend&gt;\n            &lt;li&gt;&lt;input type=\"button\" value=\"Sign In to booksmore\" class=\"sb si llb\" /&gt;&lt;/li&gt;&lt;/fieldset&gt;\n            &lt;fieldset&gt;&lt;legend class=\"orb\"&gt; OR&lt;/legend&gt;\n            &lt;li&gt;Sign In using any of following service:&lt;/li&gt;\n            &lt;li&gt;\n                &lt;div class=\"box1 fc\"&gt;Facebook&lt;/div&gt;\n                &lt;div class=\"box1 tw\"&gt;Twitter&lt;/div&gt;\n                &lt;div class=\"box1 gg\"&gt;Google&lt;/div&gt;\n                &lt;div class=\"box1 oi\"&gt;Open Id&lt;/div&gt;\n            &lt;/li&gt;&lt;/fieldset&gt;\n\n            &lt;/ul&gt;\n        &lt;/div&gt;\n    &lt;/section&gt;\n    &lt;section class=\"panels &lt;?php if(!isset($_SESSION['username'])) echo\"unactivepanel\"; else echo\"activepanel\";?&gt;unactivepanel\" id=\"Shipping\"&gt;&lt;/section&gt;\n    &lt;section class=\"panels unactivepanel\" id=\"Confirm\"&gt;&lt;/section&gt;\n</code></pre>\n\n<p>What i have studied is using too many <strong>if</strong> block slows down the executions speed, So am I right using too many <strong>if</strong> blocks ? if I am not coding it right(<em>what i feel right now</em>) then what may be the possible alternatives to <strong>if</strong> blocks ? (I was thinking of using <strong>switch</strong> block.)</p>\n"},{"tags":["java","performance","swing"],"answer_count":2,"favorite_count":0,"up_vote_count":9,"down_vote_count":0,"view_count":2940,"score":9,"question_id":646089,"title":"Java Robot createScreenCapture performance","body":"<p>I need to grab a series of screenshots and concatenate them into a movie. I'm trying to use the java Robot class to capture the screen. </p>\n\n<p>But the createScreenCapture() method takes more than 1 second on my machine. I can't even get 1 fps. Is there a way to speed it up? Or is there any other API?</p>\n\n<p>Edit: It is allocating a buffered image.</p>\n\n<p>BufferedImage image = robot.createScreenCapture(screen);\n//Save the screenshot as a jpg<br />\nFile file = new File(\"images/screen\"+ index + \".jpg\");<br />\nImageIO.write(image, \"jpg\", file);\nindex++;</p>\n\n<p>Writing it to the jpg file takes about 200 ms where as getting BufferedImage takes about 1400ms.</p>\n"},{"tags":["performance","algorithm","statistics","traffic"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":20,"score":0,"question_id":13264027,"title":"Negotiate traffic speed for mobile applications - algorithm?","body":"<p>we have 2 devices - iPad A &amp; iPad B. Both use the same application that is designed to send audio/video data. </p>\n\n<p>Sometimes devices are located far away from each other and traffic goes through internet instead of local network.</p>\n\n<p>In that case, we can't guarantee the same network speed on both ends. One device may send 512Kb/s another one 5 times less.</p>\n\n<p>Question is -</p>\n\n<p>Is it possible to measure traffic between both devices in real time and scale up/down when needed. For example if I know that device A has only 256kb/s incoming connection at this given moment, then device B should scale down from 1mb/s to 256kb/s automatically</p>\n\n<p>Right now the solution we have implemented is to detect the traffic/speed degradation by recognizing lost packets. But it's not perfect.</p>\n\n<p>Maybe you have read something or have an idea in mind?</p>\n\n<p>thanks in Advance,\nDmitry</p>\n"},{"tags":["asp.net","performance","keep-alive","yslow"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":5,"score":0,"question_id":13263834,"title":"How can I enable \"keep-alive\" if I'm with a web farm/cluster as a webhost, and they don't honor it?","body":"<p>I'm with a webhost for my company's website that has 47 pages (asp.net 4/vb).  All speed tests, Google, Yslow, Gtmetrix, etc., want me to <strong>enable keep-alive</strong>.  I have already set http header requests to expire after 30 minutes via IIS7.</p>\n\n<p>I submitted a ticket to my webhost, and they say they don't offer keep-alive.  Does that just mean I'm stuck if I stay with this webhost, or is there something I can do on my own, regardless of my webhost?  Thanks for any guidance!</p>\n"},{"tags":["performance","file","caching","disk"],"answer_count":5,"favorite_count":5,"up_vote_count":21,"down_vote_count":0,"view_count":7117,"score":21,"question_id":478340,"title":"Clear file cache to repeat performance testing","body":"<p>What tools are available to either completely clear, or selectively remove cached information about file and directory contents?</p>\n\n<p>The application that I'm developing is a specialised compression utility, and is expected to do a lot of work reading and writing files that the operating system hasn't touched recently, and whose disk blocks are unlikely to be cached.</p>\n\n<p>I wish to remove the variability I see in IO time when I repeat the task of profiling different strategies for doing the file processing work.</p>\n\n<p>I'm primarily interested in solutions for Windows XP, as that is my main development machine, but I can also test using linux, and so am interested in answers for that environment too.</p>\n\n<p>I tried SysInternals <a href=\"http://technet.microsoft.com/en-us/sysinternals/bb897561.aspx\" rel=\"nofollow\">CacheSet</a>, but clicking \"Clear\" doesn't result in a measurable increase (restoration to timing after a cold-boot) in the time to re-read files I've just read a few times.</p>\n"},{"tags":["c++","performance","visual-c++","dll","random"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":66,"score":0,"question_id":13262498,"title":"Combining rand() and srand() for better performance","body":"<p>So, I am making a DLL that I want to have a function to generate random numbers. I was wondering which of these options is more efficient (performance wise).</p>\n\n<p>This one is just making a function in a DLL that allows me to get a random number.</p>\n\n<pre><code>int getRand(unsigned int seed) {\n    int rNum;  // Random Number.\n\n    srand(seed);\n    rNum = (rand() % // Whatever I need here.\n}\n</code></pre>\n\n<p>Or, would just using <code>srand(time(nullptr))</code> and <code>rand()</code> in the application be better in performance?</p>\n\n<p>Thanks,</p>\n\n<p>Johnny P.</p>\n"},{"tags":["c++","performance","visual-c++","atomic","compare-and-swap"],"answer_count":1,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":77,"score":3,"question_id":13213113,"title":"Should std::atomic<int*>::load be doing a compare-and-swap loop?","body":"<p><strong>Summary</strong>: I had expected that <code>std::atomic&lt;int*&gt;::load</code> with <code>std::memory_order_relaxed</code> would be close to the performance of just loading a pointer directly, at least when the loaded value rarely changes. I saw far worse performance for the atomic load than a normal load on Visual Studio C++ 2012, so I decided to investigate. It turns out that the atomic load is implemented as a <a href=\"http://en.wikipedia.org/wiki/Compare-and-swap\" rel=\"nofollow\">compare-and-swap</a> loop, which I suspect is not the fastest possible implementation.</p>\n\n<p><strong>Question</strong>: Is there some reason that <code>std::atomic&lt;int*&gt;::load</code> needs to do a compare-and-swap loop?</p>\n\n<p><strong>Background</strong>: I believe that MSVC++ 2012 is doing a compare-and-swap loop on atomic load of a pointer based on this test program:</p>\n\n<pre><code>#include &lt;atomic&gt;\n#include &lt;iostream&gt;\n\ntemplate&lt;class T&gt;\n__declspec(noinline) T loadRelaxed(const std::atomic&lt;T&gt;&amp; t) {\n  return t.load(std::memory_order_relaxed);\n}\n\nint main() {\n  int i = 42;\n  char c = 42;\n  std::atomic&lt;int*&gt; ptr(&amp;i);\n  std::atomic&lt;int&gt; integer;\n  std::atomic&lt;char&gt; character;\n  std::cout\n    &lt;&lt; *loadRelaxed(ptr) &lt;&lt; ' '\n    &lt;&lt; loadRelaxed(integer) &lt;&lt; ' '\n    &lt;&lt; loadRelaxed(character) &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>\n\n<p>I'm using a <code>__declspec(noinline)</code> function in order to isolate the assembly instructions related to the atomic load. I made a new MSVC++ 2012 project, added an x64 platform, selected the release configuration, ran the program in the debugger and looked at the disassembly. Turns out that both <code>std::atomic&lt;char&gt;</code> and <code>std::atomic&lt;int&gt;</code> parameters end up giving the same call to <code>loadRelaxed&lt;int&gt;</code> - this must be something the optimizer did. Here is the disassembly of the two loadRelaxed instantiations that get called:</p>\n\n<p><strong><code>loadRelaxed&lt;int * __ptr64&gt;</code></strong></p>\n\n<pre><code>000000013F4B1790  prefetchw   [rcx]  \n000000013F4B1793  mov         rax,qword ptr [rcx]  \n000000013F4B1796  mov         rdx,rax  \n000000013F4B1799  lock cmpxchg qword ptr [rcx],rdx  \n000000013F4B179E  jne         loadRelaxed&lt;int * __ptr64&gt;+6h (013F4B1796h)  \n</code></pre>\n\n<p><strong><code>loadRelaxed&lt;int&gt;</code></strong></p>\n\n<pre><code>000000013F3F1940  prefetchw   [rcx]  \n000000013F3F1943  mov         eax,dword ptr [rcx]  \n000000013F3F1945  mov         edx,eax  \n000000013F3F1947  lock cmpxchg dword ptr [rcx],edx  \n000000013F3F194B  jne         loadRelaxed&lt;int&gt;+5h (013F3F1945h)  \n</code></pre>\n\n<p>The instruction <code>lock cmpxchg</code> is atomic <a href=\"http://en.wikipedia.org/wiki/Compare-and-swap\" rel=\"nofollow\">compare-and-swap</a> and we see here that the code for atomically loading a <code>char</code>, an <code>int</code> or an <code>int*</code> is a compare-and-swap loop. I also built this code for 32-bit x86 and that implementation is still based on <code>lock cmpxchg</code>.</p>\n\n<p><strong>Question</strong>: Is there some reason that <code>std::atomic&lt;int*&gt;::load</code> needs to do a compare-and-swap loop?</p>\n"},{"tags":["c++","performance","design-patterns","data-structures","iterator"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":93,"score":0,"question_id":13219821,"title":"How to access and manage block stored data","body":"<p><strong>What I want to do:</strong> I need to store cell data in block-wise form, that is</p>\n\n<p>*cell_member1[cell0] .. cell_member1[cellN]  ...  cell_memberM[cell0] .. cell_memberM[cellN]*</p>\n\n<p>Then I need to access this data efficiently and, if possible, using a nice syntax. It would be great if I could define the data to be stored easily, i.e. by defining an object with members as the data that I want to store and passing it to some \"magic\" that does everything for me.</p>\n\n<p><strong>How I'm currently doing it:</strong> I have a container of the form: </p>\n\n<pre><code>template&lt;class T&gt; struct Container {\n  char* data;\n  Container(const int n) {\n    data = new char[n*T::spaceRequirements()]; //&lt; Data stored \"block-wise\"\n    new(data) typename T::Flags[n]; //&lt; Flags stored \"cell-wise\"\n  }\n  /// Destructor ommited for briefness.\n};\n</code></pre>\n\n<p>in which I store data for some cells of type T. I need some flags per cell and right now I'm using std::bitset to store them which means that I need to store this bitsets in cell-wise form: </p>\n\n<p>*cell_member1[cell0] ... cell_memberM[cell0] ... cell_member1[cellN] .. cell_memberM[cellN]*</p>\n\n<p>I am describing how much data per cell needs to be stored in the following class, which also provides access to the data:</p>\n\n<pre><code>template&lt;int nd&gt; struct CellAccessor {\n  /// Cell flags are stored cell-wise:\n  typedef std::bitset&lt;64&gt; Flags;\n  enum { DELETE = 0, ///&lt; Cell marked for deletion\n         REFINE = 1 ///&lt; Cell marked for refinement\n         //...\n  }; ///&lt; Enum for the flags.\n  static inline Flags&amp; flags(const int cellId) {\n    return *reinterpret_cast&lt;Flags*&gt;(data + sizeof(Flags)*cellId); }\n  template&lt;int pId&gt; static inline Flags::reference flags(const int cellId) {\n    return flags(cellId)[pId]; } //&lt; Cell-wise access to the properties\n\n  /// The rest of the data is stored block-wise:\n  static inline int&amp; order(const int cellId) { ///&lt; One int field.\n    return *reinterpret_cast&lt;int*&gt;\n        (data + maxNoCells*sizeof(Flags) + sizeof(int)*cellId);}\n\n  /// Coordinate vector with nd components:\n  static inline double&amp; coordinates(const int cellId, const int i) {\n    return *reinterpret_cast&lt;double*&gt;\n        (data + maxNoCells*(sizeof(Flags)+sizeof(int))\n         + maxNoCells*i*sizeof(double) + sizeof(double)*cellId); }\n  template&lt;int i&gt; static inline double&amp; coordinates(const int cellId) {\n    return *reinterpret_cast&lt;double*&gt;\n        (data +maxNoCells*(sizeof(Flags)+sizeof(int)+i*sizeof(double))\n         + sizeof(double)*cellId); }\n\n  /// Total amount of memory to allocate per cell: (used by Container)\n  static inline int spaceRequirements() { return\n        sizeof(Flags) // Flags\n        + sizeof(int) // order\n        + nd*sizeof(double) // coordinates\n        ;}\n\n  /// Constructor gets pointer to the beginning of the container \n  /// and the offset for the member variables:\n  CellAccessor(char* d, int n){data = d; maxNoCells = n;}\n private:\n  static char* data;  ///&lt; Pointer to the beginning of the container.\n  static int maxNoCells;  ///&lt; Cell offset for the member variables.\n};\ntemplate&lt;int nd&gt; char* CellAccessor&lt;nd&gt;::data = nullptr;\ntemplate&lt;int nd&gt; int CellAccessor&lt;nd&gt;::maxNoCells = 0;\n</code></pre>\n\n<p>And I use it like this:</p>\n\n<pre><code>int main() {\n  int maxNoCells = 10000;   ///&lt; Maximum number of cells (=cell offset).\n  typedef CellAccessor&lt;2&gt; A;\n  Container&lt; A &gt; cellData(maxNoCells);  ///&lt; Allocate cell data.\n  A cells(cellData.data,maxNoCells);  ///&lt; Provides access to cell data.\n\n  for(int i = 0; i &lt; maxNoCells; ++i){\n    cells.flags&lt;A::DELETE&gt;(i) = i%2==0 ? true : false;\n    cells.flags&lt;A::REFINE&gt;(i) = i%2==0 ? false : true;\n    cells.coordinates(i,0) = i;\n    cells.coordinates&lt;1&gt;(i) = -((double)i);\n    cells.order(i) = 2;\n  }\n}\n</code></pre>\n\n<p><strong>Pros:</strong> </p>\n\n<ul>\n<li><p>The data is in block-wise form, which is what I needed.</p></li>\n<li><p>The syntax is ok.</p></li>\n</ul>\n\n<p><strong>Problems:</strong> </p>\n\n<ul>\n<li><p>My classes are doing too much: providing access to the data for the users, providing how much data needs to be stored for the containers, providing how the data should be moved/copied/swaped for my data structures (which are trees...)... </p></li>\n<li><p>I can't use STL algorithms without iterators. I've implemented iterators by making them store the cell index and reimplementing the CellAccessor class inside them (bad! DRY!). </p></li>\n<li><p>Bitset is still being stored in cell-wise form. I could re-implement bitset for my block-wise data structure...</p></li>\n<li><p>data and maxNoCells are static variables, but I could make them normal member variables if required.</p></li>\n</ul>\n\n<p><strong>Question:</strong> is there any efficient way to store \"objects\" (or what we conceptually understand by objects) in block-wise form and access them as if they were stored in a std container such as vector?</p>\n"},{"tags":["c++","performance","boost","c++11","iterator"],"answer_count":1,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":192,"score":5,"question_id":11281298,"title":"Boost any_range performance: std::prev(iterator) versus --iterator","body":"<p>I've recently begun to prefer the free functions <code>std::next</code> and <code>std::prev</code> to explicitly copying and incrementing/decrementing iterators. Now, I am seeing weird behavior in a pretty specific case, and I would appreciate any help demystifying it.</p>\n\n<p>I have an interpolation/extrapolation function operating on a <code>boost::any_range</code> of some <code>X_type</code>. The full definition of the range type is:</p>\n\n<pre><code>boost::any_range &lt;\n    const X_type,\n    boost::random_access_traversal_tag,\n    const X_type,\n    std::ptrdiff_t\n&gt;\n</code></pre>\n\n<p>The <code>any_range</code>, in this particular case, is assigned from an <code>iterator_range</code> holding two pointers to <code>const X_type</code>, which serves as an <code>X_type</code> view of about half of the <code>data()</code> area of a <code>vector&lt;char&gt;</code>.</p>\n\n<p>Compiling my application in MSVC 2010, everything works just fine.\nCompiling the same code in MinGW g++ 4.7.0, it seemed to hang in one particular location, which I've then narrowed down to this (slightly abbreviated):</p>\n\n<pre><code>// Previously ensured conditions:\n// 1) xrange is nonempty;\n// 2) yrange is the same size as xrange.\n\nauto x_equal_or_greater =\n    std::lower_bound(std::begin(xrange),std::end(xrange),xval);\n\nif (x_equal_or_greater == std::end(xrange))\n{\n    return *yit_from_xit(std::prev(x_equal_or_greater),xrange,yrange);\n}\n</code></pre>\n\n<p>Stepping through the code in gdb, I found out it wasn't getting stuck, just taking a very long time to return from the single <code>std::prev</code> call - which in libstdc++ is implemented in terms of <code>std::advance</code> and ultimately the <code>+=</code> operator.</p>\n\n<p>By merely replacing the <code>return</code> line with:</p>\n\n<pre><code>auto xprev=x_equal_or_greater;\n--xprev;\nreturn *yit_from_xit(xprev,xrange,yrange);\n</code></pre>\n\n<p>Performance is great again, and there's virtually no delay.</p>\n\n<p>I am aware of the overhead of using type-erased iterators (those of <code>any_range</code>), but even so, are the two cases above really supposed to carry such different costs? Or am I doing something wrong?</p>\n"},{"tags":["vb.net","performance","vb6"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":47,"score":1,"question_id":13258103,"title":"API call in VB.NET much slower than in VB6","body":"<p>Can somebody explain how it can be that the same API call returns so much quicker with VB6 than with VB.NET?</p>\n\n<p>Here is my VB6 code:</p>\n\n<pre><code>Public Declare Function GetWindowTextLength Lib \"user32\" Alias \"GetWindowTextLengthA\" (ByVal hWnd As Long) As Long\nPublic Declare Function GetWindowText Lib \"user32\" Alias \"GetWindowTextA\" (ByVal hWnd As Long, ByVal lpString As String, ByVal cch As Long) As Long\n\n\nPublic Function GetWindowTextEx(ByVal uHwnd As Long) As String\n\nDim lLen&amp;\nlLen = GetWindowTextLength(uHwnd) + 1\n\nDim sTemp$\nsTemp = Space(lLen)\n\nlLen = GetWindowText(uHwnd, sTemp, lLen)\n\nDim sRes$\nsRes = Left(sTemp, lLen)\n\nGetWindowTextEx = sRes\n\nEnd Function\n</code></pre>\n\n<p>And here is my VB.NET code:</p>\n\n<pre><code>Private Declare Function GetWindowText Lib \"user32\" Alias \"GetWindowTextA\" (ByVal hwnd As Integer, ByVal lpWindowText As String, ByVal cch As Integer) As Integer\n\n    Dim sText As String = Space(Int16.MaxValue)\n    GetWindowText(hwnd, sText, Int16.MaxValue)\n</code></pre>\n\n<p>I ran each version 1000 times. </p>\n\n<p>The VB6 version needed 2.04893359351538 ms.\nThe VB.NET version needed 372.1322491699365 ms.</p>\n\n<p>Both Release and Debug version are about the same.</p>\n\n<p>What is happening here?</p>\n"},{"tags":["database","performance","sqlalchemy","uuid","sharding"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":43,"score":4,"question_id":13167799,"title":"SQLAlchemy, UUIDs, Sharding, and AUTO_INCREMENT primary key... how to get them to work together?","body":"<p>I have a question pertaining to SQLAlchemy, database sharding, and UUIDs for you fine folks. </p>\n\n<p>I'm currently using MySQL in which I have a table of the form:</p>\n\n<pre><code>CREATE TABLE foo (\n    added_id INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    id BINARY(16) NOT NULL,\n    ... other stuff ...\n    UNIQUE KEY(id)\n);\n</code></pre>\n\n<p>A little background on this table. I never care about the 'added_id', I'm only using to ensure that the inserted items are clustered together on disk (since the B-Tree used to index the table in MySQL uses the primary key as the cluster index). The 'id' column contains the binary representation of a UUID -- this is the column I actually care about and all other things reference this ID. Again, I don't want the UUID to be the primary key, since the UUID is random and thus making the B-Tree created to index the table have horrible IO characteristics (at least that is what has been said elsewhere). Also, although UUID1 includes the timestamp to ensure that IDs are generated in \"sequential\" order, the inclusion of the MAC address in the ID makes it something I'd rather avoid. Thus, I'd like to use UUID4s.</p>\n\n<p>Ok, now moving on to the SQLAlchemy part. In SQLAlchemy one can define a model using their ORM for the above table by doing something like:</p>\n\n<pre><code># The SQL Alchemy ORM base class\nBase = declerative_base()\n\n# The model for table 'foo'\nclass Foo(Base):\n    __table__ = 'foo'\n    add_id = Column(Integer, primary_key=True, nullable=False)\n    id = Column(Binary, index=True, unique=True, nullable=False)\n    ...\n</code></pre>\n\n<p>Again, this is basically the same as the SQL above. </p>\n\n<p>And now to the question. Let's say that this database is going to be sharded (horizontally partitioned) into 2 (or more) separate databases. Now, (assuming no deletions) each of these databases will have records with added_id of 1, 2, 3, etc in table foo. Since SQLAlchemy uses a session to manage the objects that are being worked on such that each object is identified only by its primary key, it seems like it would be possible to have the situation where I could end trying to access two Foo objects from the two shards with the same added_id resulting in some conflict in the managed session. </p>\n\n<p>Has anyone run in to this issue? What have you done to solve it? Or, more than likely, am I missing something from the SQLAlchemy documentation that ensures that this cannot happen. However, looking at the sharding example provided with the SQLAlchemy download (examples/sharding/attribute_shard.py) they seem to side-step this issue by designating one of the database shards as an ID generator... creating an implicit bottle neck as all INSERTS  have to go against that single database to get an ID. (They also mention using UUIDs, but apparently that causes the performance issue for the indexes.)</p>\n\n<p>Alternatively, is there a way to set the UUID as the primary key and have the data be clustered on disk using the added_id? If it's not possible in MySQL is it possible in another DB like Postgres?</p>\n\n<p>Thanks in advance for any and all input!</p>\n"},{"tags":["performance","cdn"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":8,"score":0,"question_id":13260464,"title":"Is it possible to implement a Content Delivery Network (CDN) if I'm with a cluster web host?","body":"<p>I'm with a webhost, a web farm or cluster, I guess you could say.  I have a 47 page company website, and all speed tests suggest I use a CDN.</p>\n\n<p>I've googled and SE's this to no end, but still don't understand how to implement a content delivery network.  Are they suggesting I order a subdomain and put all my .css, .js, and image files in that subdomain?  Or are they suggesting that instead of downloading jquery 1.7, I just link to malsup's jquery?  But then what would I do for images and .css?</p>\n\n<p>Just kinda confused here; any help in this regard would be truly appreciated!</p>\n"},{"tags":["c++","optimization","performance","post-increment","pre-increment"],"answer_count":14,"favorite_count":45,"up_vote_count":87,"down_vote_count":1,"view_count":9652,"score":86,"question_id":24901,"title":"Is there a performance difference between i++ and ++i in C++?","body":"<p>We looked at this answer for C in this question:</p>\n\n<p><a href=\"http://stackoverflow.com/questions/24886/is-there-a-performance-difference-between-i-and-i-in-c\" rel=\"nofollow\">http://stackoverflow.com/questions/24886/is-there-a-performance-difference-between-i-and-i-in-c</a></p>\n\n<p>What's the answer for C++?</p>\n"},{"tags":["performance","knockout.js","tuning","knockout-mapping-plugin"],"answer_count":3,"favorite_count":9,"up_vote_count":21,"down_vote_count":0,"view_count":1124,"score":21,"question_id":9927213,"title":"Performance tuning a knockout application - guidelines for improving response times","body":"<p>I have a large, complex page that relies heavily on knockout.js. Performance is starting to become an issue but examining the call stack and trying to find the bottlenecks is a real challenge. </p>\n\n<p>I noticed in another question ( <a href=\"http://stackoverflow.com/questions/9916843/knockout-js-understanding-foreach-and-with\">Knockout.js -- understanding foreach and with</a> ) that the accepted answer has the comment:</p>\n\n<blockquote>\n  <p>...and I suggest not using <code>with</code> where high performance is necessary\n  because of the overhead...</p>\n</blockquote>\n\n<p>Assuming the statement is true, this is really useful stuff to know and I have not found a source for such performance tips.</p>\n\n<p>Therefore, my question is: </p>\n\n<p><strong>Are there general guidelines / top tips that I can apply to help the performance of my application before I get deep into classic performance tuning.</strong></p>\n"},{"tags":["android","json","performance","parsing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":36,"score":0,"question_id":13259000,"title":"Json parsing strategy in Android","body":"<p>I have a large json file (~3.5 MB), with (~140) complex objects, and the max depth in the object graph is about 4-5. I use Gson to parse it, but it's really slow. I've tried some way to parse it (like mixed parsing or using stream to parse), but I couldn't increase performance.</p>\n\n<p>I checked Memory Analizer, it kill memory (70-80%), if I only parse the base Id of the objects. While parsing there are 400-500k object in memory (mostly string and char).</p>\n\n<p>Would be parsing more efficient if the object graph wouldn't be so deep? Do you have a good idea how could be better? I tried other libs too (like Jackson), but performance wasn't better.</p>\n"},{"tags":["vb.net","performance","vbscript"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":17,"score":1,"question_id":13243111,"title":"HTTPRequest performance VBScript vs. VB.NET","body":"<p>I have an application that uses a VBScript file within operation to make HTTP requests and I would like to preferably move this to a VB.NET solution entirely. </p>\n\n<p>I know there are some differences in the VBScript HTTP requests and the VB.NET ones, but what I really want to know is which one will be faster? After some tests with VB.NET I'm not so sure it will be as fast as my VBScript solution. Does anyone have experience with this and is there even a noticeable change in performance?</p>\n\n<p>Thanks</p>\n"},{"tags":["sql","database","performance","query","filter"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":22,"score":0,"question_id":13257155,"title":"Architecture for a fast screener","body":"<p>I'm building a screener that should be able to search through a table about 50 columns wide and 7000 rows long really fast.</p>\n\n<p>Each row is composed of the following columns.</p>\n\n<p>primary_key, quantity1, quantity2, quantity3...quantity50.</p>\n\n<p>All quantities are essentially floats or integers. Hence a typical screener would look like this.</p>\n\n<pre><code>Get all rows which have quantity1 &gt; x and quantity2 &lt; y and quantity3 &gt;= z.\n</code></pre>\n\n<p>Indexing all columns should lead to really fast search times however some of the columns will be updating in realtime. Indexing everything obviously leads to very low insert/update times.</p>\n\n<p>A portion of the columns are fairly static though. Hence an idea was to segregate the data into two tables, one containing all columns that are static while the other containing data that is dynamic. Any screener would then be applied to both tables based on the actual query. And the results combined in the end.</p>\n\n<p>I am currently planning on using a MySQL engine, most probably INNoDB. However I'm looking to get much faster response times. An implementation of the same problem on a certain site was very snappy. Regardless of the query size, i was getting the results within 500 ms. Wondering what other options are available out there to implement this function.</p>\n\n<p>Any thoughts/hints towards a potential solution would be greatly appreciated.</p>\n"},{"tags":["ios","performance","mvc","key-value-observing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":19,"score":0,"question_id":13256093,"title":"Design assistance for MVC and KVO","body":"<p>I have data that is persisted on the device that needs to sync with my my server, in terms of design am I on the right track?</p>\n\n<p>I have a model with a method to return my local objects, let's call it <code>[model objects]';</code> </p>\n\n<p>Within this method it returns the local objects immediately but I'd also like to check if the server needs to update any of these objects so in the background thread I run a method that uses an <code>async NSUrlConnection</code> that will return the JSON of each object that needs to be updated in which I will save that to core data. In the Table View Controller I would set a KVO to observe when the values changed for the <code>updatedObjects</code> then reload the objects displayed in the <code>objectTableView</code>.</p>\n\n<p><strong>Question: Is this the correct way to handle this? Am I missing something? Can I improve this somehow?</strong></p>\n\n<p>I am also thinking of disabling the UITableViewCells of the objects that are being updated and showing a <code>UIActivityIndicator</code> during the downloading and saving stages but I'm not sure if this will cause any race conditions in the UX.</p>\n\n<p>Hopefully this was explained well enough for you to understand, if you have any questions I will try to reply immediately.</p>\n"},{"tags":["c++","performance","exception","try-catch"],"answer_count":5,"favorite_count":2,"up_vote_count":8,"down_vote_count":0,"view_count":4457,"score":8,"question_id":3730654,"title":"What's better to use, a __try/__except block or a try / catch block?","body":"<p>I'm wondering which is the better way to catch exceptions that I throw: is it a __try / __except block or a try / catch block?  </p>\n\n<p>I'm writing in C++ and the program will only be used on Windows, so portability is not an issue.  </p>\n\n<p>Thanks!</p>\n"},{"tags":["android","performance","opengl-es","gingerbread"],"answer_count":0,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":32,"score":3,"question_id":13256091,"title":"OpenGL bad performance in Android Gingerbread","body":"<p>I'm working in a videogame for Android using OpenGL 1.0.</p>\n\n<p>I followed this tutorial: \n<a href=\"http://www.javacodegeeks.com/2011/06/android-game-development-tutorials.html\" rel=\"nofollow\">http://www.javacodegeeks.com/2011/06/android-game-development-tutorials.html</a></p>\n\n<p>I have created a little demo that works very well and fluid (limited to 30 FPS) in my current mobile: HTC Sensation (Android 4.0.3 Ice Cream Sandwich).</p>\n\n<p>Also, I tested my game in my friend's mobiles with the next results:</p>\n\n<ul>\n<li>Galaxy S2 = perfect! (30FPS)</li>\n<li>Galaxy Note = perfect! (30FPS)</li>\n<li>HTC One S = perfect! (30FPS)</li>\n<li>Sony Xperia Tipo = perfect! (30FPS)</li>\n<li>Samsung Galaxy Ace GT-S5830 = very bad performance >_&lt;' (&lt;10FPS and tested in two mobiles)</li>\n</ul>\n\n<p>If you compare the specs between Xperia Tipo and Samsung Ace you can see that they are almost the same mobile.</p>\n\n<p>XPERIA TIPO:</p>\n\n<ul>\n<li>320 x 480 resolution</li>\n<li>512MB RAM</li>\n<li>Qualcomm MSM7227A 800MHz, GPU Adreno 200</li>\n</ul>\n\n<p>SAMSUNG GALAXY ACE:</p>\n\n<ul>\n<li>320 x 480 resolution</li>\n<li>278MB RAM</li>\n<li>Qualcomm MSM7227 800 MHz, GPU Adreno 200</li>\n</ul>\n\n<p>OK, the RAM is different, but my game is very simple, does not consume more than 200MB! But the performance is totally different. No sense!</p>\n\n<p>However... only one thing is different! Samsung Galaxy Ace uses Android 2.3.X (Gingerbread) and Xperia Tipo uses Android 4.0.3 (ICS).</p>\n\n<p>So, my spearhead is targetting that the problem is with Android 2.3.X - Gingerbread or Galaxy Ace is a trash mobile.</p>\n\n<p>But... I forced my friends to install the AndEngine Examples and test it, with the result that the Nexus ParticleSystem test works very well and fluid.</p>\n\n<p>I'm totally lost! Why this different in performance? What I'm doing wrong?</p>\n\n<p>Some extra info:</p>\n\n<ol>\n<li>I follow all the steps from that tutorial.</li>\n<li>I use GL10 (OpenGL 1.0).</li>\n<li>No shaders.</li>\n<li>No delta time for lost FPS (I do not think this thing is going to solve my problem).</li>\n<li>No native code, just Java.</li>\n<li>SystemClock.uptimemillis() and sleep() to control FPS.</li>\n<li>MediaPlayer to play one MIDI song (no more sounds).</li>\n<li>Textures with a lot of alpha (2D game, is neccesary for sprites).</li>\n<li>minSDKversion=8</li>\n</ol>\n\n<p>If you need more information, please tell me.</p>\n"},{"tags":["performance","haskell","project-euler","micro-optimization"],"answer_count":1,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":136,"score":6,"question_id":13252992,"title":"How do I optimize a loop which can be fully strict","body":"<p>I'm trying to write a brute-force solution to <a href=\"http://projecteuler.net/problem=145\">Project Euler Problem #145</a>, and I cannot get my solution to run in less than about 1 minute 30 secs.</p>\n\n<p><em>(I'm aware there are various short-cuts and even paper-and-pencil solutions; for the purpose of this question I'm not considering those).</em></p>\n\n<p>In the best version I've come up with so far, profiling shows that the majority of the time is spent in <code>foldDigits</code>. This function need not be lazy at all, and to my mind ought to be optimized to a simple loop. As you can see I've attempted to make various bits of the program strict.</p>\n\n<p>So my question is: <strong>without changing the overall algorithm, is there some way to bring the execution time of this program down to the sub-minute mark?</strong></p>\n\n<p>(Or if not, is there a way to see that the code of <code>foldDigits</code> is as optimized as possible?)</p>\n\n<pre><code>-- ghc -O3 -threaded Euler-145.hs &amp;&amp; Euler-145.exe +RTS -N4\n\n{-# LANGUAGE BangPatterns #-}\n\nimport Control.Parallel.Strategies\n\nfoldDigits :: (a -&gt; Int -&gt; a) -&gt; a -&gt; Int -&gt; a\nfoldDigits f !acc !n\n    | n &lt; 10    = i\n    | otherwise = foldDigits f i d\n  where (d, m) = n `quotRem` 10\n        !i     = f acc m\n\nreverseNumber :: Int -&gt; Int\nreverseNumber !n\n    = foldDigits accumulate 0 n\n  where accumulate !v !d = v * 10 + d\n\nallDigitsOdd :: Int -&gt; Bool\nallDigitsOdd n\n    = foldDigits andOdd True n\n  where andOdd !a d = a &amp;&amp; isOdd d\n        isOdd !x    = x `rem` 2 /= 0\n\nisReversible :: Int -&gt; Bool\nisReversible n\n    = notDivisibleByTen n &amp;&amp; allDigitsOdd (n + rn)\n  where rn                   = reverseNumber n\n        notDivisibleByTen !x = x `rem` 10 /= 0\n\ncountRange acc start end\n    | start &gt; end = acc\n    | otherwise   = countRange (acc + v) (start + 1) end\n  where v = if isReversible start then 1 else 0\n\nmain\n    = print $ sum $ parMap rseq cr ranges\n  where max       = 1000000000\n        qmax      = max `div` 4\n        ranges    = [(1, qmax), (qmax, qmax * 2), (qmax * 2, qmax * 3), (qmax * 3, max)]\n        cr (s, e) = countRange 0 s e\n</code></pre>\n"},{"tags":["android","performance","touch","velocity","acceleration"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":320,"score":0,"question_id":10155907,"title":"speed or acceleration of motion event android","body":"<p>Is it possible to get the speed or velocity or acceleration of touch event in android with the existing api? I have gone through MotionEvent class and none of the fields in that class seem to retrieve information that i need. Any help would be greatly appreciated</p>\n"},{"tags":["database","performance","algorithm","caching","graph"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":69,"score":3,"question_id":11216410,"title":"Caching Strategy for a Deep, Traversable Egocentric Graph","body":"<p><strong>Firstly, let me explain what I'm building:</strong></p>\n\n<ul>\n<li><p>I have a D3.js Force Layout graph which is rooted at the center, and has a bunch of nodes spread around it. The center node is an Entity of some sort, the nodes around it are other Entities which are somehow related to the root. The edges are the actual relations (i.e. how the two are related).</p></li>\n<li><p>The outer nodes can be clicked to center the target Entity and load its relations</p></li>\n<li><p>This graph is \"Egocentric\" in the sense that every time a node is clicked, it becomes the center, and only relations <em>directly</em> involved with itself are displayed.</p></li>\n</ul>\n\n<p><strong>My Setup, in case any of it matters:</strong></p>\n\n<ul>\n<li><p>I'm serving an API through Node.js, which translates requests into queries to a CouchDB server with huge data sets.</p></li>\n<li><p>D3.js is used for layout, and aside from jQuery and Bootstrap, I'm not using any other client-side libraries. If any would help with this caching task, I'm open to suggestions :)</p></li>\n</ul>\n\n<p><strong>My Ideas:</strong></p>\n\n<ul>\n<li><p>I could easily grab a few levels of the graph each time (recurse through the process of listing and expanding children a few times) but since clicking on any given node loads completely unrelated data, it is not guaranteed to yield a high percentage of the similar data as was loaded for the root. This seems like a complete waste, and actually a step in the <em>opposite</em> direction -- I'd end up doing <em>more</em> processing this way!</p></li>\n<li><p>I can easily maintain a hash table of Entities that have already been retrieved, and check the list before requesting data for that entity from the server. I'll probably end up doing this regardless of the cache strategy I implement, since it's a really simple way of reducing queries.</p></li>\n</ul>\n\n<p><strong>Now, how do you suggest I cache this data?</strong></p>\n\n<p>Is there any super-effective strategy you can think of for doing this kind of caching? Both server-and-client-side options are greatly welcomed. A <em>ton</em> of data is involved in this process, and any reduction of querying/processing puts me miles ahead of the game.</p>\n\n<p>Thanks!</p>\n"},{"tags":["java","performance","arraylist","hashtable"],"answer_count":6,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":658,"score":3,"question_id":8512356,"title":"Java Iterated HashTable vs ArrayList speed","body":"<p>I am writing a simple 3D SW rendering engine. I have a default <code>ArrayList&lt;Object3D&gt;</code> containing the whole scene. Now, I want to be able to add, remove and select objects by name, like 3D editors do (because its MUCH more simple than mouse select, but still looking good in homework :) ).</p>\n\n<p>So, the first thing I thought is to have <code>Hashtable</code> for name and index to scene <code>ArrayList</code>. But, then I thought I could just simply save the scene using <code>Hashtable</code> directly, and go through it to render using iterator. </p>\n\n<p>So I want to ask, in a 3D engine, what is speed-preferable? Because I will for-loop the scene many times per second, compared to selecting object. Is <code>ArrayList</code> any faster than <code>iterated Hashtable</code>? Thanks.</p>\n"},{"tags":["c#","performance","encryption","aes","bouncycastle"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":29,"score":2,"question_id":13248587,"title":"How can I improve cyphering speed of the Bouncy Castle API?","body":"<p>I'm developing a software in C#, until now I was using the cryptographic library included in .NET (especially <code>AesCryptoServiceProvider</code>), but now I've special needs and I need to move to the Bouncy Castle API's. I've done some tests and I found out that these API are slower in comparison of what it's included in the .NET framework. Any idea on how to improve their performance?\nThis is the cipher that I'm using:\n<code>IBufferedCipher cipher = new CtsBlockCipher(new CbcBlockCipher(new AesFastEngine()));</code></p>\n\n<p>To be more specific, I need to move to the Bouncy Castle API's because I need both input and output file with the same length and the .NET <code>RijndaelManaged</code> (the only class that can assure that kind of behavior) is way slower than <code>AesCryptoServiceProvider</code>.</p>\n\n<p>To encrypt a file of 1.7MB with <code>AesCryptoServiceProvider</code> it takes about 40ms in my machine and about 170ms with Bouncy Castle. It doesn't look like much, but I need to use this software in a server with hundreds of requests per minutes...</p>\n\n<p>Thank you very much!!</p>\n"},{"tags":["android","performance","listview"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":27,"score":0,"question_id":13250307,"title":"Android: avoid unwanted calls from getView() in an adapter","body":"<p>I searched for hours but didn't find a suitable solution for me. \nWhat I want to do: in my ListFragment I use the onListItemClick(...) method to handle the click events. Here, I change the background of the row item. But unfortunately every time the onListItemClick(...) is called, also the getView() from the adapter is called and updates all 8 visible row items. That takes to much time: 0.5 seconds. Because the row layout is pretty complex (2 Images, 8 TextViews). \nSo I want to update only the row which is clicked. I want to use <a href=\"http://stackoverflow.com/questions/2123083/android-listview-refresh-single-row/9987714#9987714\">this solution</a> but that has no effect, when the other 7 row items are updated anyways. \nI already followed <a href=\"http://lucasr.org/2012/04/05/performance-tips-for-androids-listview/\" rel=\"nofollow\">these advices</a> to speed the list up, but it's still to slow. </p>\n\n<p>Any help, ideas and thoughts are appreciated. :)\nThank you!</p>\n\n<p>[EDIT]</p>\n\n<p>Thanks to CommensWare for giving me some new ideas. What I did now was to check what traceview says. And the result is, that the delay is devided in two parts. The first 300ms of the delay takes the \"FastXMLSerialzier.escapeAndAppendString()\" with over 22.000 calls. That seems a lot! In the second half, many, maybe all, onMeasure()-methods of the views and layouts are called. </p>\n\n<p>What I tried:\nI filled every textview with static dummy values in the adapter and excluded the part with loading the images. It changes nothing, traceview shows the same picture. \nIn the second try, I looked at the LinearLayout of my list item and replaced every \"wrap_content\" I found with \"match_parent\" - nothing. Still the same. </p>\n\n<p>I am still open for your thoughts and hints. :)</p>\n"},{"tags":["sql","sql-server","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":54,"score":2,"question_id":13253522,"title":"Fast SQL Server search on 40M text records","body":"<p>I have a SQL Server 2005 database with a table containing 40 million records. Each record contains a column that stores a comma separated list of keywords. Each keyword is a combination of letters and numbers. The keywords are up to 7 characters long and on average there are 15 keywords per record. The keywords are not unique across rows.</p>\n\n<p>I want to search on full or part of the keyword.</p>\n\n<p>I've created Full text index which shows 328,245,708 unique key count. The search efficiency is fine for queries of 4 or more characters (around <strong>100ms</strong> on the test machine), but too slow for queries that have 3 or less characters (up to <strong>3s</strong> on the test machine).</p>\n\n<p>I've been trying both <code>CONTAINSTABLE</code> and <code>CONTAINS</code> queries of a sort <code>'[query]*'</code> with similar result.</p>\n\n<p>I believe the performance of the short queries is slower, because short words repeat across different records more frequently.</p>\n\n<p>Sorting the results is not crucial, and I've been trying to return <code>TOP X</code> results sorted on Rank from <code>CONTAINSTABLE</code>. This doesn't provide the desired performance.</p>\n\n<p>How can I make this search faster for short queries?</p>\n"},{"tags":["python","performance","path","directory"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":35,"score":0,"question_id":13252980,"title":"Common pathname manipulations to save in a specific directory a file","body":"<p>I am writing a function where I read <code>inFile</code> in order to split it into two files (outFile1,outFile2).</p>\n\n<p>What I want is if <code>outFile1</code> and/or <code>outFile2</code> are specified without pathname directory (ex: <code>outFile1=\"result1.txt\"</code> and <code>outFile2=\"result2.txt\"</code>) both files are saved in the same directory as <code>inFile</code> (ex: inFile=\"C:\\mydata\\myfile.txt\"). If the  pathname directory for the output files is present, I wish to save the results in that directory.  </p>\n\n<p>when I don't report the the <code>outFile</code> pathname directory, the files are saved in the same directory as my python script.</p>\n\n<pre><code>def LAS2LASDivide(inFile,outFile1,outFile2,Parse,NumVal):\n    inFile_path, inFile_name_ext = os.path.split(os.path.abspath(inFile))\n    outFile1_path, outFile1_name_ext = os.path.split(os.path.abspath(outFile1))\n    outFile2_path, outFile2_name_ext = os.path.split(os.path.abspath(outFile2))\n    outFile1_name = os.path.splitext(outFile1_name_ext)[0]\n    outFile2_name = os.path.splitext(outFile2_name_ext)[0]\n</code></pre>\n\n<p>example</p>\n\n<pre><code>inFile=\"C:\\\\mydoc\\\\Area_18.las\"\noutFile1=\"Area_18_overlap.las\"\noutFile2=\"Area_18_clean.las\"\n\ninFile_path, inFile_name_ext = os.path.split(os.path.abspath(inFile))\ninFile_path, inFile_name_ext\n('C:\\\\mydoc', 'Area_18.las')\n\noutFile1_path, outFile1_name_ext = os.path.split(os.path.abspath(outFile1))\noutFile1_path, outFile1_name_ext \n('C:\\\\Program Files\\\\PyScripter', 'Area_18_overlap.las')\n</code></pre>\n\n<p>this is all my code (tested) modify with the suggestion of mgilson</p>\n\n<pre><code>import os\nfrom os import path\nfrom liblas import file as lasfile\n\n\ndef LAS2LASDivide(inFile,outFile1,outFile2,Parse,NumVal):\n    inFile_path, inFile_name_ext = os.path.split(os.path.abspath(inFile))\n    outFile1_path, outFile1_name_ext = os.path.split(os.path.abspath(outFile1))\n    outFile2_path, outFile2_name_ext = os.path.split(os.path.abspath(outFile2))\n    outFile1_name = os.path.splitext(outFile1_name_ext)[0]\n    outFile2_name = os.path.splitext(outFile2_name_ext)[0]\n    if outFile1_name != outFile2_name:\n        # function pesudo_switch\n        def pseudo_switch(x):\n            return {\n                \"i\": p.intensity,\n                \"r\": p.return_number,\n                \"n\": p.number_of_returns,\n                \"s\": p.scan_direction,\n                \"e\": p.flightline_edge,\n                \"c\": p.classification,\n                \"a\": p.scan_angle,\n            }[x]\n        h = lasfile.File(inFile,None,'r').header\n        # change the software id to libLAS\n        h.software_id = \"alessandro.montaghi@gmail.com\"\n        if not os.path.split(outFile1)[0]:\n            file_out1 = lasfile.File(os.path.abspath(\"{0}\\\\{1}.las\".format(inFile_path,outFile1_name)),mode='w',header= h)\n        else:\n            file_out1 = lasfile.File(os.path.abspath(\"{0}\\\\{1}.las\".format(outFile1_path,outFile1_name)),mode='w',header= h)\n        if not os.path.split(outFile2)[0]:\n            file_out2 = lasfile.File(os.path.abspath(\"{0}\\\\{1}.las\".format(inFile_path,outFile2_name)),mode='w',header= h)\n        else:\n            file_out2 = lasfile.File(os.path.abspath(\"{0}\\\\{1}.las\".format(outFile2_path,outFile2_name)),mode='w',header= h)\n        for p in lasfile.File(inFile,None,'r'):\n            if pseudo_switch(Parse) == int(NumVal):\n                file_out1.write(p)\n            elif pseudo_switch(Parse) != int(NumVal):\n                file_out2.write(p)\n        file_out1.close()\n        file_out2.close()\n    else:\n        print \"outFile1 and outFile2 cannot have the same name\"\n</code></pre>\n"},{"tags":["c#","performance","linq"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":58,"score":1,"question_id":13253561,"title":"Improving performance of LINQ query with IEqualityComparer","body":"<p>I'm using <code>IEqualityComparer</code> to match \"near duplicates\" in a database using LINQ to Entities.</p>\n\n<p>With a record set of around 40,000, this query is taking around 15 seconds to complete and I wondered if there were any structural changes that could be made to the code below.</p>\n\n<p><strong>My public method</strong></p>\n\n<pre><code>public List&lt;LeadGridViewModel&gt; AllHighlightingDuplicates(int company)\n        {\n\n            var results = AllLeads(company)\n                  .GroupBy(c =&gt; c, new CompanyNameIgnoringSpaces())\n                  .Select(g =&gt; new LeadGridViewModel\n                  {\n                      LeadId = g.First().LeadId,\n                      Qty = g.Count(),\n                      CompanyName = g.Key.CompanyName\n                  }).OrderByDescending(x =&gt; x.Qty).ToList();\n\n            return results;\n\n        }\n</code></pre>\n\n<p><strong>Private method to grab the leads</strong></p>\n\n<pre><code>private char[] delimiters = new[] { ' ', '-', '*', '&amp;', '!' };\nprivate IEnumerable&lt;LeadGridViewModel&gt; AllLeads(int company)\n        {\n            var items = (from t1 in db.Leads\n                          where\n                              t1.Company_ID == company\n                          select new LeadGridViewModel\n                          {\n                              LeadId = t1.Lead_ID,\n                              CompanyName = t1.Company_Name,\n                          }).ToList();\n\n\n            foreach (var x in items)\n                x.CompanyNameStripped = string.Join(\"\", (x.CompanyName ?? String.Empty).Split(delimiters));\n\n            return items;\n        }\n</code></pre>\n\n<p><strong>My IEqualityComparer</strong></p>\n\n<pre><code> public class CompanyNameIgnoringSpaces : IEqualityComparer&lt;LeadGridViewModel&gt;\n    {\n        public bool Equals(LeadGridViewModel x, LeadGridViewModel y)\n        {\n            var delimiters = new[] {' ', '-', '*', '&amp;', '!'};\n            return delimiters.Aggregate(x.CompanyName ?? String.Empty, (c1, c2) =&gt; c1.Replace(c2, '\\0')) \n                == delimiters.Aggregate(y.CompanyName ?? String.Empty, (c1, c2) =&gt; c1.Replace(c2, '\\0'));\n        }\n\n        public int GetHashCode(LeadGridViewModel obj)\n        {\n            var delimiters = new[] {' ', '-', '*', '&amp;', '!'};\n            return delimiters.Aggregate(obj.CompanyName ?? String.Empty, (c1, c2) =&gt; c1.Replace(c2, '\\0')).GetHashCode();\n        }\n    }\n</code></pre>\n"},{"tags":["eclipse","performance","slowness"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":29,"score":0,"question_id":13253363,"title":"Eclipse slowing down computer?","body":"<p>I feel like when I use Eclipse (especially for big projects) and even after I close it, the performance of my computer slows down permanently. It happened to my old laptop and I see it's happening with my new one. Is that possible and what can I do about it?</p>\n\n<p>(btw im using a Dell Inspiron Intel core i3, CPU 1.40 Ghz,RAM 6GB)</p>\n"},{"tags":["performance","drools"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":23,"score":0,"question_id":13251664,"title":"Drools decision tables performance","body":"<p>I'm using Drools decision tables. I have an .XSL file with condition columns, an action column, and several rows. Everything is working fine, but I'm worried about performance if the decision table start growing.\nDoes anybody know if there is some kind of limitations in decision tables? or something to keep in mind about the number of rows/columns that could affect the performance?\nThe application needs to be always available, and decision tables are taking care of a critical part of my application, and thats why I'm worried.\nThanks.</p>\n"},{"tags":["c#",".net","sql","performance","ado.net"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":13252966,"title":"Fastest Get data from remote server","body":"<p>I'm creating a windows application in which I need to get data using ado.net/(Or any other way using C# if any ). From one table. The database table apparently has around <code>100000</code> records and it takes forever to download.</p>\n\n<p>Is there any faster way where I could get data into faster? </p>\n\n<p>I tried the <code>DataReader</code> but still isn't fast enough. </p>\n"},{"tags":["database","performance","plsql","cursor"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":16,"score":0,"question_id":13251545,"title":"PLSQL block takes very long time to declare a cursor","body":"<p>I'm executing a PL/SQL blocks that updates some rows from an explicit cursor. The code to declare the cursor is the following:    </p>\n\n<pre><code>cursor DUP_SUBJECTS is  \n  select * \n    from ODS_SUBJECT_D  \n   WHERE SUBJECT_COD = ANY (SELECT SUBJECT_COD  \n                              FROM ODS_SUBJECT_D  \n                             WHERE END_DATE = TO_DATE ('31-12-9999','DD-MM-YYYY')   \n                             GROUP BY SUBJECT_COD, ROW_TYPE_DE  \n                            HAVING COUNT(*) &gt; 1)  \n   ORDER BY SUBJECT_COD, START_DATE; \n</code></pre>\n\n<p>The first statement in the body is a <code>DBMS_OUTPUT.PUT_LINE</code> in order to notify when it starts to executing the block. The query in the above script returns 20000 rows out of 2900000 rows in the table. It seems to take very long time in the declaration block of the script (after 30 minutes it does not print the message yet).<br>\nAny suggestion to optimize the performances of the script?  </p>\n\n<p>Thanks,<br>\nAntonio</p>\n"},{"tags":["python","performance","csv","data","statistics"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":7,"view_count":54,"score":-6,"question_id":13251754,"title":"python generate unique counter from csv file","body":"<p>I have a huge csv (tab) file that is about 1 GB of student data for overall grades.<br/><br/>\n(section) Example:</p>\n\n<pre><code>Mandy 1Q 2011 82 English\nCarey 4Q 2010 70 Math\nFreeman 2Q 2012 52 Music\nDarrick 4Q 2010 82 Science\nOsvaldo 4Q 2012 59 Science\nOsvaldo 4Q 2012 79 Science\nJudy 1Q 2012 89 Science\nEmilia 2Q 2011 31 Music\nEmilia 2Q 2011 31 Music\nCordia 1Q 2011 79 Science\nCelestine 1Q 2012 62 English\nCelestine 1Q 2012 62 English\nOpal 4Q 2012 93 Math\nKarlene 1Q 2011 87 English\nKellie 2Q 2012 44 Science\n</code></pre>\n\n<p>I'm looking for:<br/>\nStudent Average, Max, Min (whole, Qtr, Yr) (by class type)<br/>\nAverage per class type (Whole, Qtr, Yr)<br/></p>\n\n<p>Thanks for any help</p>\n"},{"tags":["sql","performance","query","postgresql","postgresql-performance"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":75,"score":3,"question_id":13234812,"title":"Improving query speed: simple SELECT in big postgres table","body":"<p>I'm having trouble regarding speed in a SELECT query on a Postgres database.</p>\n\n<p>I have a table with two integer columns as key: (int1,int2)\nThis table has around 70 million rows.</p>\n\n<p>I need to make two kind of simple SELECT queries in this environment:</p>\n\n<pre><code>SELECT * FROM table WHERE int1=X;\nSELECT * FROM table WHERE int2=X;\n</code></pre>\n\n<p>These two selects returns around 10.000 rows each out of these 70 million. For this to work as fast as possible I thought on using two HASH indexes, one for each column. Unfortunately the results are not that good:</p>\n\n<pre><code>                                                               QUERY PLAN                                                               \n----------------------------------------------------------------------------------------------------------------------------------------\n Bitmap Heap Scan on lec_sim  (cost=232.21..25054.38 rows=6565 width=36) (actual time=14.759..23339.545 rows=7871 loops=1)\n   Recheck Cond: (lec2_id = 11782)\n   -&gt;  Bitmap Index Scan on lec_sim_lec2_hash_ind  (cost=0.00..230.56 rows=6565 width=0) (actual time=13.495..13.495 rows=7871 loops=1)\n         Index Cond: (lec2_id = 11782)\n Total runtime: 23342.534 ms\n(5 rows)\n</code></pre>\n\n<p>This is an EXPLAIN ANALYZE example of one of these queries. It is taking around 23 seconds. My expectations are to get this information in less than a second.</p>\n\n<p>These are some parameters of the postgres db config:</p>\n\n<pre><code>work_mem = 128MB\nshared_buffers = 2GB\nmaintenance_work_mem = 512MB\nfsync = off\nsynchronous_commit = off\neffective_cache_size = 4GB\n</code></pre>\n\n<p>Any help, comment or thought would be really appreciated.</p>\n\n<p>Thank you in advance.</p>\n"},{"tags":["python","performance","numpy","cython"],"answer_count":1,"favorite_count":0,"up_vote_count":8,"down_vote_count":0,"view_count":81,"score":8,"question_id":13241724,"title":"Assigning values to array slices is slow","body":"<p>I'm trying to optimize a Python algorithm by implementing it in Cython. My question is regarding a certain performance bottleneck that exists in the following code:</p>\n\n<pre><code>@cython.boundscheck(False) # turn off bounds-checking for entire function\ndef anglesToRGB( np.ndarray[double, ndim=2] y, np.ndarray[double, ndim=2] x ):\n\ncdef double angle\ncdef double Hp\ncdef double C\ncdef double X\ncdef np.ndarray[double, ndim=3] res = np.zeros([y.shape[0], y.shape[1], 3], dtype=np.float64)\n\nfor i in xrange(y.shape[0]):\n    for j in xrange(y.shape[1]):\n        angle = atan2( y[i,j], x[i,j] )*180.0/PI+180\n\n        C = sqrt(pow(y[i,j],2)+pow(x[i,j],2))/360.0 #Chroma\n        Hp = angle/60.0\n        X = C*(1-fabs( Hp%2-1))\n\n        C *= 255\n        X *= 255\n\n        if (0. &lt;= Hp &lt; 1.):\n            res[i,j,:] = [C,X,0]\n        elif (1. &lt;= Hp &lt; 2.):\n            res[i,j,:] = [X,C,0]\n        elif (2. &lt;= Hp &lt; 3.):\n            res[i,j,:] = [0,C,X]\n        elif (3. &lt;= Hp &lt; 4.):\n            res[i,j,:] = [0,X,C]\n        elif (4. &lt;= Hp &lt; 5.):\n            res[i,j,:] = [X,C,C]\n        else:\n            res[i,j,:] = [C,0,X]\n\nreturn res\n</code></pre>\n\n<p>I've identified the major bottleneck to be when i assign a list of values to a slice of the res array, \nlike with</p>\n\n<pre><code>res[i,j,:] = [C,X,0]\n</code></pre>\n\n<p>However, if i change the assignment to</p>\n\n<pre><code>res[i,j,0] = C\nres[i,j,1] = X\nres[i,j,2] = 0\n</code></pre>\n\n<p>Then the code runs orders of magnitude faster.\nTo me this is strange because surely the Cython compiler should be smart enough to do this for me? Or do i need to provide it with some hints first?\nI should note that changing the slicing to 0:3 instead of : and making the list of values a numpy array doesn't improve the performance.</p>\n\n<p>What i'd like to know is why this operation is killing performance so badly and if there's any way to solve it without having to sacrifice the convenient list and slice notation.</p>\n\n<p>Best regards</p>\n"},{"tags":["python","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":2,"view_count":75,"score":0,"question_id":13112527,"title":"Python performance issues","body":"<p>I built a simple gravity simulation shown here:</p>\n\n<p><img src=\"http://i.stack.imgur.com/8YzWt.jpg\" alt=\"enter image description here\"></p>\n\n<p>At first, I used a variable timestep, by calculating how much it takes for one update, and applying that to the next one - standard stuff.</p>\n\n<p>However, this method reduces determinism, and results can vary from simulation to simulation - something I definitely do not want.</p>\n\n<p>I figured that I should use a fixed time step (10 seconds), but then I noticed that the simulation would speed up and down in regular intervals (of around 1 second).</p>\n\n<p>Why is that happening? Does it have to do anything with Python itself?</p>\n\n<p><br /></p>\n\n<h1>The code</h1>\n\n<p>The main loop:</p>\n\n<pre><code>while run:\n    uni.update(10)\n\n    for event in pygame.event.get():\n        if event.type == QUIT:\n            run = False\n</code></pre>\n\n<p>The update method:</p>\n\n<pre><code>def update (self, dt):\n    self.time += dt\n\n    for b1, b2 in combinations(self.bodies.values(), 2):\n        fg = self.Fg(b1, b2)\n\n        if b1.position.x &gt; b2.position.x:\n            b1.force.x -= fg.x\n            b2.force.x += fg.x\n        else:\n            b1.force.x += fg.x\n            b2.force.x -= fg.x\n\n\n        if b1.position.y &gt; b2.position.y:\n            b1.force.y -= fg.y\n            b2.force.y += fg.y\n        else:\n            b1.force.y += fg.y\n            b2.force.y -= fg.y\n\n\n    for b in self.bodies.itervalues():\n        ax = b.force.x/b.m\n        ay = b.force.y/b.m\n\n        b.position.x += b.velocity.x*dt\n        b.position.y += b.velocity.y*dt\n\n        nvx = ax*dt\n        nvy = ay*dt\n\n        b.position.x += 0.5*nvx*dt\n        b.position.y += 0.5*nvy*dt\n\n        b.velocity.x += nvx\n        b.velocity.y += nvy\n\n        b.force.x = 0\n        b.force.y = 0\n</code></pre>\n"},{"tags":["performance","search","microsoft","full-text-search","fast-esp"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":319,"score":0,"question_id":4979894,"title":"Need information for FAST Search Server 2010 for Internet Sites","body":"<p>This is NOT the FAST Search for SharePoint product. I am looking for information regarding the FAST product that can be installed without SharePoint.</p>\n\n<p>I've Googled and searched but can't find any more information other than the following blog - <a href=\"http://consultingblogs.emc.com/manjunathasubbarya/archive/2010/12/05/fsis-fast-search-for-internet-sites.aspx\" rel=\"nofollow\">http://consultingblogs.emc.com/manjunathasubbarya/archive/2010/12/05/fsis-fast-search-for-internet-sites.aspx</a></p>\n\n<p>I am curious if there are SMEs supporting installation and configuration. If there is any information on Microsoft's website? If anyone knows if FAST is supported outside of SharePoint.</p>\n"},{"tags":["performance","sql-server-2008-r2","certification"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":16,"score":-1,"question_id":13251086,"title":"sql server 2008r2 multi column clustered index vs covering index","body":"<p>suppose you have this table, note Bookid is nonclustered. I want to choose C, but D seems also makes sense (in this particular scenario). </p>\n\n<p>The column descriptions: Column, data type, primary key, indexed, index type\n<img src=\"http://i.stack.imgur.com/1eA50.jpg\" alt=\"enter image description here\"></p>\n"},{"tags":["performance","algorithm","language-agnostic","unix","pi"],"answer_count":22,"favorite_count":41,"up_vote_count":112,"down_vote_count":6,"view_count":13450,"score":106,"question_id":19,"title":"Fastest way to get value of pi","body":"<p>Solutions welcome in any language. :-) I'm looking for the fastest way to obtain the value of pi, as a personal challenge. More specifically I'm using ways that don't involve using <code>#define</code>d constants like <code>M_PI</code>, or hard-coding the number in.</p>\n\n<p>The program below tests the various ways I know of. The inline assembly version is, in theory, the fastest option, though clearly not portable; I've included it as a baseline to compare the other versions against. In my tests, with built-ins, the <code>4 * atan(1)</code> version is fastest on GCC 4.2, because it auto-folds the <code>atan(1)</code> into a constant. With <code>-fno-builtin</code> specified, the <code>atan2(0, -1)</code> version is fastest.</p>\n\n<p>Here's the main testing program (<code>pitimes.c</code>):</p>\n\n<pre><code>#include &lt;math.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;time.h&gt;\n\n#define ITERS 10000000\n#define TESTWITH(x) {                                                       \\\n    diff = 0.0;                                                             \\\n    time1 = clock();                                                        \\\n    for (i = 0; i &lt; ITERS; ++i)                                             \\\n        diff += (x) - M_PI;                                                 \\\n    time2 = clock();                                                        \\\n    printf(\"%s\\t=&gt; %e, time =&gt; %f\\n\", #x, diff, diffclock(time2, time1));   \\\n}\n\nstatic inline double\ndiffclock(clock_t time1, clock_t time0)\n{\n    return (double) (time1 - time0) / CLOCKS_PER_SEC;\n}\n\nint\nmain()\n{\n    int i;\n    clock_t time1, time2;\n    double diff;\n\n    /* Warmup. The atan2 case catches GCC's atan folding (which would\n     * optimise the ``4 * atan(1) - M_PI'' to a no-op), if -fno-builtin\n     * is not used. */\n    TESTWITH(4 * atan(1))\n    TESTWITH(4 * atan2(1, 1))\n\n#if defined(__GNUC__) &amp;&amp; (defined(__i386__) || defined(__amd64__))\n    extern double fldpi();\n    TESTWITH(fldpi())\n#endif\n\n    /* Actual tests start here. */\n    TESTWITH(atan2(0, -1))\n    TESTWITH(acos(-1))\n    TESTWITH(2 * asin(1))\n    TESTWITH(4 * atan2(1, 1))\n    TESTWITH(4 * atan(1))\n\n    return 0;\n}\n</code></pre>\n\n<p>And the inline assembly stuff (<code>fldpi.c</code>), noting that it will only work for x86 and x64 systems:</p>\n\n<pre><code>double\nfldpi()\n{\n    double pi;\n    asm(\"fldpi\" : \"=t\" (pi));\n    return pi;\n}\n</code></pre>\n\n<p>And a build script that builds all the configurations I'm testing (<code>build.sh</code>):</p>\n\n<pre><code>#!/bin/sh\ngcc -O3 -Wall -c           -m32 -o fldpi-32.o fldpi.c\ngcc -O3 -Wall -c           -m64 -o fldpi-64.o fldpi.c\n\ngcc -O3 -Wall -ffast-math  -m32 -o pitimes1-32 pitimes.c fldpi-32.o\ngcc -O3 -Wall              -m32 -o pitimes2-32 pitimes.c fldpi-32.o -lm\ngcc -O3 -Wall -fno-builtin -m32 -o pitimes3-32 pitimes.c fldpi-32.o -lm\ngcc -O3 -Wall -ffast-math  -m64 -o pitimes1-64 pitimes.c fldpi-64.o -lm\ngcc -O3 -Wall              -m64 -o pitimes2-64 pitimes.c fldpi-64.o -lm\ngcc -O3 -Wall -fno-builtin -m64 -o pitimes3-64 pitimes.c fldpi-64.o -lm\n</code></pre>\n\n<p>Apart from testing between various compiler flags (I've compared 32-bit against 64-bit too, because the optimisations are different), I've also tried switching the order of the tests around. The <code>atan2(0, -1)</code> version still comes out top every time, though.</p>\n\n<p>I'm keen to hear what results you have, as well as improvements to the testing process. :-)</p>\n"},{"tags":[".net","performance","optimization","logging"],"answer_count":5,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":254,"score":1,"question_id":2684961,"title":"Minimize performance impact of event logging?","body":"<p>I have a static logging function that uses StringBuilder to concatenate a bunch of query parameters before sending the string to a log.  This process can get moderately long, as we may have ~10 parameters (.Append calls) and end up being ~200 chars long.</p>\n\n<p>I want to minimize the performance impact of the logging function. (This logging function may be called multiple times per web request, and we measure the processing time for each web request)</p>\n\n<p>How/Should/Can I build a \"pool\" of StringBuilders to improve performance?</p>\n\n<p>I can also do all this logging asynchronously, right?  How should I do that?</p>\n"},{"tags":["asp.net","performance","iis7","umbraco","cpu-usage"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":34,"score":1,"question_id":13249718,"title":"Server CPU stays on 100% when e-commerce site runs","body":"<p>I have developed an e-commerce website on .NET umbraco. After deploying it i have made a load test with 100 users (<a href=\"http://loadimpact.com/\" rel=\"nofollow\">loadimapct</a>).When ever more than 30 users tried to access the site,it freezes and CPU usage stays at 100% till the test is over. The site is accessing an indexing engine (Fact-Finder) internally which is deployed on a Tomcat Server.\nHere is a snap shot of the condition.</p>\n\n<p><a href=\"http://my.jetscreenshot.com/7391/20121106-xhtd-222kb.jpg\" rel=\"nofollow\">Snapshot</a></p>\n\n<p>I have checked the site with profilers but there is no heavy process running that i can see.\ncan any one give me some possible reasons for this ?</p>\n"},{"tags":["asp.net-mvc","performance","entity-framework","iis","entity-framework-5"],"answer_count":0,"favorite_count":2,"up_vote_count":5,"down_vote_count":0,"view_count":64,"score":5,"question_id":13250679,"title":"How to \"warm-up\" Entity Framework? When does it get \"cold\"?","body":"<p>No, the answer to my second question is not the winter.</p>\n\n<p><strong>Preface:</strong></p>\n\n<p>I've been doing a lot of research on Entity Framework recently and something that keeps bothering me is its performance when the queries are not warmed-up, so called cold queries. </p>\n\n<p>I went through the <a href=\"http://msdn.microsoft.com/en-us/data/hh949853.aspx\">performance considerations</a> article for Entity Framework 5.0. The authors introduced the concept of <strong>Warm</strong> and <strong>Cold</strong> queries and how they differ, which I also noticed myself without knowing of their existence. Here it's probably worth to mention I only have six month of experience behind my back.</p>\n\n<p>Now I know what topics I can research into additionally if I want to understand the framework better in terms of performance. Unfortunately most of the information on the Internet is outdated or bloated with subjectivity, hence my inability to find any additional information on the <em>Warm</em> vs <em>Cold</em> queries topic.</p>\n\n<p>Basically what I've noticed so far is that whenever I have to recompile or the recycling hits in, my initial queries are getting very slow. Any subsequent data read is fast (<em>subjective</em>), as expected.</p>\n\n<p>We'll be migrating to Windows Server 2012, IIS8 and SQL Server 2012 and as a Junior I actually won myself the opportunity to test them before the rest. I'm very happy they introduced a warming-up module that will get my application ready for that first request. However, I'm not sure how to proceed with warming up my Entity Framework.</p>\n\n<p>What I already know is worth doing:</p>\n\n<ul>\n<li>Generate my Views in advance as suggested.</li>\n<li>Eventually move my models into a separate assembly.</li>\n</ul>\n\n<p>What I consider doing, by going with common sense, <strong>probably wrong approach</strong>:</p>\n\n<ul>\n<li>Doing dummy data reads at Application Start in order to warm things\nup, generate and validate the models.</li>\n</ul>\n\n<p><strong>Questions:</strong> </p>\n\n<ul>\n<li>What would be the best approach to have high availability on my Entity Framework at anytime?</li>\n<li>In what cases does the Entity Framework gets \"cold\" again? (Recompilation, Recycling, IIS Restart etc.)</li>\n</ul>\n"},{"tags":["performance","drools"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":1267,"score":2,"question_id":7743892,"title":"How to improve drools performance?","body":"<p>Is there a way to log / monitor the time taken for rule in a Drools rule set?</p>\n\n<p>Is there a way to make sure that one rule is not executed more than once(It seems to be happening in my case)</p>\n\n<p>What are the general guidelines on improving Drools performance?</p>\n\n<p>Currently I am using a one single DRL file with 100 odd rules. </p>\n\n<p>Any additiional information you need will be provided.</p>\n\n<p>Thanks,\nAbdul</p>\n"},{"tags":["javascript","performance","google-chrome-devtools"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":25,"score":0,"question_id":13247386,"title":"Page resources takes long to load","body":"<p>I am building a little web-based inventory application but I keep having trouble that the page <a href=\"http://tinyurl.com/csxc7e4\" rel=\"nofollow\">link</a> takes far too long to load.</p>\n\n<p>There are some resources that takes always five seconds to load, in the developer tools is see for example that that the file query.placeholderfix.js is five seconds waiting and then loads right away.</p>\n\n<p>If I remove that file there will be a other file that is just waiting five seconds and then load almost directly. </p>\n\n<p>The server which host this website have more than enough resources available (it is my own private server and is doing mostly nothing) so I don’t see where the problem my lay. </p>\n\n<p><strong>Update</strong></p>\n\n<p>I have changed some value's in the Apache configuration and it is now working perfectly. </p>\n\n<pre><code>    KeepAliveTimeout 2\n\n    &lt;IfModule mpm_prefork_module&gt;\n         StartServers      5\n         MinSpareServers   5\n         MaxSpareServers   10\n         MaxClients        25\n         MaxRequestsPerChild 800\n    &lt;/IfModule&gt;\n</code></pre>\n"},{"tags":["javascript","performance","html5","html5-canvas","webrtc"],"answer_count":0,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":31,"score":0,"question_id":13212382,"title":"WebRTC performance - very high cpu load","body":"<p>working on a motion detector <a href=\"https://github.com/alonisser/WebcamSwiper\" rel=\"nofollow\">js library</a> built with WebRTC + canvas. \nwhen I run the app I immediatly get very high cpu usage.\nI optimized the loops etc, but the basic problem seems to be accessing the camera eg WebRTC.</p>\n\n<p><strong>Is there a way to make WebRTC behave better?</strong> another configuration? something I'm missing?\nor What am I doing wrong?</p>\n\n<p>or maybe some js memory leak I'm handling wrong?</p>\n\n<p>you can check another demo <a href=\"http://swiper.4p-tech.co.il\" rel=\"nofollow\">here</a> with the same lib (simpler and mine)</p>\n\n<p>and a different one using WebRTC and with same problem <a href=\"http://enotionz.github.com/jscii/\" rel=\"nofollow\">here</a></p>\n"},{"tags":["sql","sql-server","performance","nested-sets","hierarchyid"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":114,"score":4,"question_id":12568201,"title":"SQL Server Nested set vs Hierarchyid performance","body":"<p>I have a hierarchical data. The most common queries will be \"get parent branch for node\" and \"get subtree of node\". Updates and inserts are not likely to occur often. I am choosing between nested sets and hierarchyid. As far as I am concerned, search on nested set should be pretty fast on indexed columns, however, I have no clue about internal implementation of hierarchyid. What should I use in order to achieve highest performance possible?</p>\n"},{"tags":["asp.net","performance","yslow"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":9,"score":0,"question_id":13244410,"title":"Will including these javascript files in the header eliminate Etag errors, and other SEO errors from YSlow?","body":"<p>I have been working on performance lately, and recently ran a YSlow test on my company's asp.net website.  I have already set up an IIS7 rule to set expiration dates to 30 days.  After researching, I think Yahoo suggests you add tags like this to your head section:</p>\n\n<pre><code>&lt;script src=\"http://yui.yahooapis.com/2.9.0/build/yahoo/yahoo-min.js\" &gt;&lt;/script&gt;\n&lt;script src=\"http://yui.yahooapis.com/2.9.0/build/get/get-min.js\" &gt;&lt;/script&gt;\n&lt;script src=\"http://yui.yahooapis.com/2.9.0/build/yahoo-dom-event/yahoo-dom-event.js\"&gt; \n&lt;/script&gt;\n&lt;script src=\"http://yui.yahooapis.com/2.9.0/build/imageloader/imageloader-min.js\"&gt;\n&lt;/script&gt;\n&lt;script src=\"http://yui.yahooapis.com/2.9.0/build/cookie/cookie-min.js\"&gt;&lt;/script&gt;\n</code></pre>\n\n<p>If you see this link, <a href=\"http://www.google.com/search?hl=en&amp;as_q=&amp;as_epq=build/yahoo/yahoo-min.js&amp;as_oq=&amp;as_eq=&amp;as_nlo=&amp;as_nhi=&amp;lr=&amp;cr=&amp;as_qdr=all&amp;as_sitesearch=&amp;as_occt=any&amp;safe=images&amp;tbs=&amp;as_filetype=&amp;as_rights=\" rel=\"nofollow\">Yahoo's recommendations on how to improve SEO</a>, it seems as if yahoo suggests putting these .js files in your header.</p>\n\n<p>Are these javascript files from yahoo something you should include in your head section?  Or are they just unnecessary external .js files that will slow your site down?  I just don't understand yahoo's logic behind this.  Any guidance to clear this up would be greatly appreciated!</p>\n"},{"tags":["performance","distributed","hadoop","shared-nothing"],"answer_count":7,"favorite_count":4,"up_vote_count":9,"down_vote_count":0,"view_count":2868,"score":9,"question_id":17721,"title":"Experience with Hadoop?","body":"<p>Have any of you tried Hadoop? Can it be used without the distributed filesystem that goes with it, in a Share-nothing architecture? Would that make sense?</p>\n\n<p>I'm also interested into any performance results you have...</p>\n"},{"tags":["c#",".net","performance","linq"],"answer_count":5,"favorite_count":1,"up_vote_count":7,"down_vote_count":0,"view_count":229,"score":7,"question_id":8595258,"title":"Why the order of LINQ to objects methods counts","body":"<p>I read <a href=\"http://stackoverflow.com/questions/7499384/does-the-order-of-linq-functions-matter\">this</a> question's answers that explain the order of the LINQ to objects methods makes a difference. My question is why?</p>\n\n<p>If I write a LINQ to SQL query, it doesn't matter the order of the LINQ methods-<code>projections</code> for example:</p>\n\n<pre><code>session.Query&lt;Person&gt;().OrderBy(x =&gt; x.Id)\n                       .Where(x =&gt; x.Name == \"gdoron\")\n                       .ToList();\n</code></pre>\n\n<p>The expression tree will be transformed to a rational SQL like this:</p>\n\n<pre><code>  SELECT   * \n  FROM     Persons\n  WHERE    Name = 'gdoron'\n  ORDER BY Id; \n</code></pre>\n\n<p>When I Run the query, SQL query will built according to the expression tree no matter how weird the order of the methods.<br>\nWhy it doesn't work the same with <code>LINQ to objects</code>?<br>\nwhen I enumerate an <em>IQueryable</em> all the projections can be placed in a rational order(e.g. Order By after Where) just like the Data Base optimizer does.</p>\n"},{"tags":["c++","performance"],"answer_count":3,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":128,"score":4,"question_id":13243274,"title":"Why is accumulate faster than a simple for cycle?","body":"<p>I was testing algorithms and run into this weird behavior, when <code>std::accumulate</code> is faster than a simple <code>for</code> cycle.</p>\n\n<p>Looking at the generated assembler I'm not much wiser :-) It seems that the <code>for</code> cycle is optimized into MMX instructions, while accumulate expands into a loop.</p>\n\n<p>This is the code. The behavior manifests with <code>-O3</code> optimization level, gcc 4.7.1</p>\n\n<pre><code>#include &lt;vector&gt;                                                                                                                                                                                                                                                              \n#include &lt;chrono&gt;                                                                                                                                                                                                                                                              \n#include &lt;iostream&gt;                                                                                                                                                                                                                                                            \n#include &lt;random&gt;                                                                                                                                                                                                                                                              \n#include &lt;algorithm&gt;                                                                                                                                                                                                                                                           \nusing namespace std;                                                                                                                                                                                                                                                           \n\nint main()                                                                                                                                                                                                                                                                     \n{                                                                                                                                                                                                                                                                              \n    const size_t vsize = 100*1000*1000;                                                                                                                                                                                                                                        \n\n    vector&lt;int&gt; x;\n    x.reserve(vsize);\n\n    mt19937 rng;\n    rng.seed(chrono::system_clock::to_time_t(chrono::system_clock::now()));\n\n    uniform_int_distribution&lt;uint32_t&gt; dist(0,10);\n\n    for (size_t i = 0; i &lt; vsize; i++)\n    {\n        x.push_back(dist(rng));\n    }\n\n    long long tmp = 0;\n    for (size_t i = 0; i &lt; vsize; i++)\n    {\n        tmp += x[i];\n    }\n\n    cout &lt;&lt; \"dry run \" &lt;&lt; tmp &lt;&lt; endl;\n\n    auto start = chrono::high_resolution_clock::now();\n    long long suma = accumulate(x.begin(),x.end(),0);\n    auto end = chrono::high_resolution_clock::now();\n\n    cout &lt;&lt; \"Accumulate runtime \" &lt;&lt; chrono::duration_cast&lt;chrono::nanoseconds&gt;(end-start).count() &lt;&lt; \" - \" &lt;&lt; suma &lt;&lt; endl;\n\n    start = chrono::high_resolution_clock::now();\n    suma = 0;\n    for (size_t i = 0; i &lt; vsize; i++)\n    {\n        suma += x[i];\n    }\n    end = chrono::high_resolution_clock::now();\n\n    cout &lt;&lt; \"Manual sum runtime \" &lt;&lt; chrono::duration_cast&lt;chrono::nanoseconds&gt;(end-start).count() &lt;&lt; \" - \" &lt;&lt; suma &lt;&lt;  endl;\n\n    return 0;\n}\n</code></pre>\n"},{"tags":["performance","language-agnostic","compiler"],"answer_count":30,"favorite_count":22,"up_vote_count":41,"down_vote_count":13,"view_count":4332,"score":28,"question_id":405770,"title":"Why are compilers so stupid?","body":"<p>I always wonder why compilers can't figure out simple things that are obvious to the human eye. They do lots of simple optimizations, but never something even a little bit complex. For example, this code takes about 6 seconds on my computer to print the value zero (using java 1.6):</p>\n\n<pre><code>int x = 0;\nfor (int i = 0; i &lt; 100 * 1000 * 1000 * 1000; ++i) {\n    x += x + x + x + x + x;\n}\nSystem.out.println(x);\n</code></pre>\n\n<p>It is totally obvious that x is never changed so no matter how often you add 0 to itself it stays zero. So the compiler could in theory replace this with System.out.println(0).</p>\n\n<p>Or even better, this takes 23 seconds:</p>\n\n<pre><code>public int slow() {\n    String s = \"x\";\n    for (int i = 0; i &lt; 100000; ++i) {\n        s += \"x\";\n    }\n    return 10;\n}\n</code></pre>\n\n<p>First the compiler could notice that I am actually creating a string s of 100000 \"x\" so it could automatically use s StringBuilder instead, or even better directly replace it with the resulting string as it is always the same. Second, It does not recognize that I do not actually use the string at all, so the whole loop could be discarded!</p>\n\n<p>Why, after so much manpower is going into fast compilers, are they still so relatively dumb?</p>\n\n<p><strong>EDIT</strong>: Of course these are stupid examples that should never be used anywhere. But whenever I have to rewrite a beautiful and very readable code into something unreadable so that the compiler is happy and produces fast code, I wonder why compilers or some other automated tool can't do this work for me.</p>\n"},{"tags":["ruby-on-rails","ruby-on-rails-3","performance","time-series"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":13,"score":0,"question_id":13243137,"title":"Efficiently plot database activity as a time series","body":"<p>I am working on a system where 200,000+ records have been created in the past year and I need to plot their creation on a time series with various added filters. At this present, this requires performing lots of <code>count</code> queries (30 for each month plotted). How should these dates be stored for maximum speed?</p>\n\n<p>One idea: store the most commonly-visualized data in a number of serialized fields containing <code>count</code>s for each day over the past month. Update each day with <code>cron</code> and serve up as necessary. (Where should these be stored - some new database table or a separate file accessible by Heroku cron?)</p>\n"},{"tags":["mysql","performance","table","sql-tuning"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":38,"score":0,"question_id":13241558,"title":"Slow mysql query on large table with GROUP BY, AVG, etc","body":"<p>I have a query on a large table(over 2 million rows) which takes ~10 seconds to complete. Is there any way to optimize it?  The query is as below:</p>\n\n<pre><code>SELECT\n    DATE_FORMAT(date0, '%Y-%m' ) AS Yr_Mo, \n    DATE_FORMAT(date0, '%p' ) AS AM_PM,\n    province AS Province,\n    SUM( IF( top_ads + left_ads =0, 1, 0 ) ) AS pagesWithRightAdsOnly, \n    AVG( top_ads ) AS top_ads, \n    AVG( left_ads ) AS left_ads, \n    AVG( right_ads ) AS right_ads\nFROM ad_counts\nGROUP BY Yr_Mo, AM_PM, Province\n</code></pre>\n\n<p>The table 'ad_counts':</p>\n\n<pre><code>date0 (timestamp)\nprovince(varchar)\nkeyword_id\nnumber_ads(int)\ntop_ads (int)\nleft_ads (int)\nright_ads (int)\n</code></pre>\n\n<p>Index on date0, but date0 is not unique.</p>\n\n<p>Any thoughts?</p>\n\n<p>Edit:\nEXPLAIN:</p>\n\n<pre>\nid  select_type table   type    possible_keys   key key_len ref rows    Extra\n1   SIMPLE  baidu_pro_ad_counts ALL NULL    NULL    NULL    NULL    2160752 Using temporary; Using filesort\n</pre>\n"},{"tags":["performance","magento","cart"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":22,"score":1,"question_id":13241954,"title":"magento - slow add to cart - high number of queries","body":"<p>we are running an Enterprise instance and see some strange behaviour on add to cart where a transaction trace shows inventory &amp; status being checked multiple times - even for a small'ish basket (e.g. 20 items).  We run new-relic and can see hundreds of calls in the trace and while they respond quickly they add up given they are called so many times.  Attached shows a sample trace for a simple cart add.  We are running nginx, varnish, apc as core config.  Any ideas/help greatly appreciated.  Anyone seen similar before?</p>\n\n<p>load time: 14,871   </p>\n\n<p>98.82% Mage_Core_Controller_Varien_Action::dispatch</p>\n\n<p>6.13%</p>\n\n<p>156 fast method calls        2.050s 1.0 0.01%\ncatalogrule_product_price - SELECT   2.050s 1.0\ncatalog_product_entity_group_price - SELECT 2.054  s 0.0\ncatalog_product_flat_1 - SELECT      2.059  s\ncataloginventory_stock_status - SELECT       2.061  s\ncatalog_product_flat_1 - SELECT      2.073  s\ncataloginventory_stock_status - SELECT       2.082  s\ncatalog_product_flat_1 - SELECT      2.087  s\ncataloginventory_stock_status - SELECT       2.097  s\ncataloginventory_stock_status - SELECT       2.104  s\nand so on for hundreds of calls..</p>\n"},{"tags":["ios","performance","maps","mkmapview"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":37,"score":1,"question_id":13239838,"title":"iOS6 Maps performance issue","body":"<p>On iPhone4s/iPad2, apps using a fullsize MKMapView exhibit a <strong>much worse scrolling performance</strong> than in the stock Maps app. Especially on fast scroll gestures it lags very bad. </p>\n\n<p>It seems that loading the vectors/tiles needs much time. When I test it with satellite maps it runs smoothly (so issue just comes up in vector based map).</p>\n\n<p><strong>I verified this issue:</strong></p>\n\n<ul>\n<li>in own apps and Instagram (Map feature)</li>\n<li>on iPhone4s and iPad2</li>\n</ul>\n\n<p>On my iPad there was iOS 5 still running. I checked in iOS5 my App's map + Instagram's map and both were smoothly. After that I updated it to 6.0.1 and both maps of the apps were lagging.</p>\n\n<p><strong>How to reproduce?</strong></p>\n\n<ul>\n<li>create a view based iPhone Application in Xcode </li>\n<li>add a mapview to the generated xib (fullscreen)</li>\n<li>launch it on your device an scroll very fast</li>\n</ul>\n\n<p><strong>How I did work around it:</strong></p>\n\n<p>I set the map to satellite type and overlay it with OSM vectors (http://wiki.openstreetmap.org/wiki/OSM_in_MapKit). That runs smoothly but needs double traffic (for satellite pictures and osm vectors)</p>\n\n<p><strong>I don't like this workaround, so does anybody noticed this issue?</strong></p>\n\n<p>Update: I filled a bug with the Apple Bug reporter - ID:<strong>12638328</strong> feel free to participate</p>\n"},{"tags":["performance","algorithm","data-structures","stack"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":65,"score":2,"question_id":13223574,"title":"Retrieving the Min element in a stack in O(1) Time","body":"<p>The reason I'm asking this question is because I cannot see why the way I think cannot be applied to this particular quesiton</p>\n\n<p><i>\"How would you design a stack which, \nin addition to push and pop, also has a function min which returns the minimum element? Push, pop and min should all operate in O(1) time</i>\"</p>\n\n<p><b>My basic solution: </b>Wouldn't it be possible if we had a variable in <b>stack</b> class, that whenever we were pushing an item to stack we would check if it is <b>smaller</b> than our <b>min</b> variable. If it is assign the value to the min, if not ignore.</p>\n\n<p>You would still get the O(1) as the min fucntion would be;</p>\n\n<pre><code>int getMinimum(){\n  return min;\n}\n</code></pre>\n\n<p>Why this solution is never mentioned, or what is the fault with the way I think?</p>\n"},{"tags":["mysql","crash","load","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":228,"score":1,"question_id":859072,"title":"Mysql Connections topping out within seconds","body":"<p>i have aweird problem that has only just started happening.</p>\n\n<p>i have a small cluster (one web and one db) setup and i host a rather popular group of4 -5 sites that allow users to dynamicly create their own mobile chat communitys automaticly. each site gets its own mysql db createdand populated automaticly.</p>\n\n<p>this is all fine, </p>\n\n<p>but in the last 24hours weird things have begun happening,\npreviously i had the sql max_connections set to 500 and this was perfectly adaqute for the demand but now even when i set the connection to 4000+ they are all maxxed out within 5-10 minutes, and mysql processlist shows thousands of unauthenticated user connections sitting at login status,</p>\n\n<p>i have gone through the sites and all their mysql configs are fine so i cant see what the issue is. </p>\n\n<p>server specs below</p>\n\n<p>db server:</p>\n\n<ul>\n<li>dual amd opteron 246</li>\n<li>8GB ram</li>\n<li>120gb hd(64gb free)</li>\n<li>33gb swap (rarly used but their for emergencys)</li>\n<li>centos 5 64bit.</li>\n<li>direct 100mbit lan to web serv</li>\n</ul>\n\n<p>only mysql,ssh and webmin running, no other apps installed</p>\n\n<p>web server:</p>\n\n<ul>\n<li>amd athlon 64 3800+</li>\n<li>plesk 9.2.1 </li>\n<li>4gb rram</li>\n<li>2x120gb hds</li>\n</ul>\n\n<p>apache status onthe web server only shows 120ish http connections butthe sql keeps climbing </p>\n"},{"tags":["javascript","regex","performance"],"answer_count":2,"favorite_count":1,"up_vote_count":8,"down_vote_count":0,"view_count":299,"score":8,"question_id":9750338,"title":"Dynamic vs Inline RegExp performance in JavaScript","body":"<p>I stumbled upon that performance test, saying that RegExps in JavaScript are not necessarily slow: <a href=\"http://jsperf.com/regexp-indexof-perf\">http://jsperf.com/regexp-indexof-perf</a></p>\n\n<p>There's one thing i didn't get though: two cases involve something that i believed to be exactly the same:</p>\n\n<pre><code>RegExp('(?:^| )foo(?: |$)').test(node.className);\n</code></pre>\n\n<p>And</p>\n\n<pre><code>/(?:^| )foo(?: |$)/.test(node.className);\n</code></pre>\n\n<p>In my mind, those two lines were <strong>exactly</strong> the same, the second one being some kind of shorthand to create a RegExp object. Still, it's <strong>twice faster</strong> than the first.</p>\n\n<p>Those cases are called \"dynamic regexp\" and \"inline regexp\".</p>\n\n<p>Could someone help me understand the difference (and the performance gap) between these two?</p>\n"},{"tags":["performance","matlab","signal-processing","time-series","vectorization"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":48,"score":1,"question_id":13238926,"title":"how to find difference between two vectors as a function of time-lag in MATLAB","body":"<p>First time posting a question here! Will happily take any advice<code>||</code>criticism I can get.</p>\n\n<p>We have two vectors: <code>v1</code> and <code>v2</code>. Assume <code>length(v1)</code> >> <code>length(v2)</code>. I move a window of size <code>length(v2)</code> along the vector <code>v1</code>. At each lag index, from the windowed portion of <code>v1</code> I subtract <code>v2</code>. I then sum the terms of the resulting vector and return this sum for every lag index along the length of vector <code>v1</code>. For simplicity, let's ignore the edge cases.</p>\n\n<p>I've done this with a <code>for</code> loop, but the length of my vectors is on the order of 10^9 [and larger], and even though the calculation is simple, it appears to take a long time to just iterate through the whole thing.</p>\n\n<p>Any ideas? I suspect there is a function that does something like this, but I have had no luck finding it.</p>\n"},{"tags":["performance","matlab"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":31,"score":3,"question_id":13237377,"title":"Why mutating cell array in a class property is so slow?","body":"<p>For example, I have the following class:</p>\n\n<pre><code>classdef testclass &lt; handle\n    properties\n            buckets\n    end\n    methods\n        function tc = testclass(sz)\n            tc.buckets = cell(1, sz);\n        end\n        function put(tc,k)\n            tc.buckets{k}{1} = 1;\n        end\n    end\nend\n</code></pre>\n\n<p>And the following example loop, where I compare performance with a plain cell array:</p>\n\n<pre><code>tm = @java.lang.System.currentTimeMillis;\n\nfor N=[100 200 400 1000 2000 4000 8000]\n    tc = testclass(N);\n    Tstart = tm();\n    for k=1:N\n        tc.put(k);\n    end\n    Tend = tm();\n    fprintf(1, 'filling hash class (size %d): %d ms\\n', N, Tend - Tstart);\n\n    arr = cell(1,N);\n    Tstart = tm();\n    for k=1:N\n        arr{k}{1} = 1;\n    end\n    Tend = tm();\n    fprintf(1, 'filling cell array (size %d): %d ms\\n', N, Tend - Tstart);\nend\n</code></pre>\n\n<p>The output is:</p>\n\n<pre><code>filling hash class (size 100): 8 ms\nfilling cell array (size 100): 0 ms\nfilling hash class (size 200): 9 ms\nfilling cell array (size 200): 0 ms\nfilling hash class (size 400): 24 ms\nfilling cell array (size 400): 1 ms\nfilling hash class (size 1000): 108 ms\nfilling cell array (size 1000): 2 ms\nfilling hash class (size 2000): 370 ms\nfilling cell array (size 2000): 5 ms\nfilling hash class (size 4000): 1396 ms\nfilling cell array (size 4000): 10 ms\nfilling hash class (size 8000): 5961 ms\nfilling cell array (size 8000): 21 ms\n</code></pre>\n\n<p>As you can see, plain cell array exhibits \"linear\" performance (which is to be expected), but array wrapped in a class gives horrible quadratic performance.</p>\n\n<p>I tested this on Matlab 2008a and Matlab 2010a.</p>\n\n<p>What causes this? And how can I work around it?</p>\n"},{"tags":["performance","windows-phone-7","asynchronous","outofmemoryexception","loadimage"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":18,"score":0,"question_id":13237048,"title":"LowProfileImageLoader get OutOfMemoryException Windows Phone","body":"<p>I am using LowProfileImageLoader  to load images async without block UI.</p>\n\n<p>You can get it from link: <a href=\"http://blogs.msdn.com/b/delay/archive/2011/03/03/quot-your-feedback-is-important-to-us-please-stay-on-the-line-quot-improving-windows-phone-7-application-performance-is-even-easier-with-these-lowprofileimageloader-and-deferredloadlistbox-updates.aspx\" rel=\"nofollow\">LowProfileImageLoader </a></p>\n\n<p>Everything is OK if DataSource of List has many different links.</p>\n\n<p><strong>But now, I want to test with DataSource with all same URL so In file TwitterService.cs, I edit followed:</strong></p>\n\n<pre><code>private static void HandleGetFollowersResponse(IAsyncResult result)\n        {\n            var state = (GetFollowersState)result.AsyncState;\n//#if DEBUG\n            try\n            {\n//#endif\n                //using (var response = state.Request.EndGetResponse(result))\n                //{\n                //    using (var stream = response.GetResponseStream())\n                //    {\n                //        var document = XDocument.Load(stream);\n                //        Deployment.Current.Dispatcher.BeginInvoke(() =&gt;\n                //        {\n                //            foreach (var user in document.Element(\"users_list\").Element(\"users\").Elements(\"user\"))\n                //            {\n                //                state.Collection.Add(new TwitterUser(user.Element(\"screen_name\").Value, new Uri(user.Element(\"profile_image_url\").Value)));\n                //            }\n                //        });\n                //        var nextCursor = document.Element(\"users_list\").Element(\"next_cursor\").Value;\n                //        if (\"0\" == nextCursor)\n                //        {\n                //            // Load completed\n                //            if (null != state.OnFollowersLoaded)\n                //            {\n                //                Deployment.Current.Dispatcher.BeginInvoke(() =&gt; state.OnFollowersLoaded());\n                //            }\n                //        }\n                //        else\n                //        {\n                //            // Load the next set\n                //            MakeGetFollowersRequest(state.ScreenName, nextCursor, state.Collection, state.OnFollowersLoaded);\n                //        }\n                //    }\n                //}\n\n                throw new WebException();\n//#if DEBUG\n            }\n            catch (WebException)\n            {\n                 //No network access; create some fake users for debugging purposes\n                for (var i = 0; i &lt; 200; i++)\n                {\n                    state.Collection.Add(new TwitterUser(\"Fake User \" + i, new Uri(\"http://4.bp.blogspot.com/-O-6vxSiyvbk/UClib6CiR0I/AAAAAAAAQaI/5Fr1dI-kQBI/s1600/Flowers+beauty+desktop+wallpapers.+(1).jpg\")));\n                }\n                if (null != state.OnFollowersLoaded)\n                {\n                    Deployment.Current.Dispatcher.BeginInvoke(() =&gt; state.OnFollowersLoaded());\n                }\n            }\n//#endif\n        }\n</code></pre>\n\n<p><strong>It will create dataSource of List with 200 item that has same URL to test.</strong></p>\n\n<p><em><strong>In LowProfileImageLoader.cs, I log the currentUsedMemory of app:</em></strong></p>\n\n<pre><code>private static void WorkerThreadProc(object unused)\n    {\n            // Process pending completions\n                    if (0 &lt; pendingCompletions.Count)\n                    {\n                        // Get the Dispatcher and process everything that needs to happen on the UI thread in one batch\n                        Deployment.Current.Dispatcher.BeginInvoke(() =&gt;\n                        {\n\n                            while (0 &lt; pendingCompletions.Count)\n                            {\n\n                                Logger.printUsedMemory();\n                                }\n                            }\n                        }\n\n    }\n</code></pre>\n\n<p>Logger.printUsedMemory() is helper function to help me log current used memory of my app.\nNothing edit anymore in code.</p>\n\n<p>But when run my app, the result is strange:\n<strong>I get OutOfMemoryException.</strong></p>\n\n<p><em><strong>Text log in Output windows (used memory in bytes) bellow:</em></strong></p>\n\n<pre><code> Used memory: 21966848\n Used memory: 25051136\n Used memory: 28442624\n Used memory: 32673792\n Used memory: 35512320\n Used memory: 39079936\n Used memory: 43364352\n Used memory: 46571520\n Used memory: 49815552\n Used memory: 53497856\n Used memory: 52514816\n Used memory: 55902208\n Used memory: 60452864\n Used memory: 62001152\n Used memory: 65503232 ~ 65.5mb\n Used memory: 69005312 ~ 69mb\n</code></pre>\n\n<p>Text log show that <strong>after loading image memory increase ~ 3mb</strong> although <strong>image size (from URL) only 120kb.</strong></p>\n\n<p>Why does the OutofMemoryException throws ?\nWhy garbage collection not call and my memory increased steady ?</p>\n\n<p>Any help is very apprepriate.\nThank in advance.</p>\n"},{"tags":["visual-studio-2010","performance","internet"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":70,"score":0,"question_id":13183772,"title":"VS2010 slowed to crawl when Hurricane Sandy killed internet connection","body":"<p>I'm in the greater New York area, and hurricane sandy has left us with power, but has killed our Internet connection. (I'm typing this on my ipad at a local McDonalds)</p>\n\n<p>This has dramatically slowed VS2010 when starting a debug session within the IDE. There now seems to be a long pause...ten seconds for each dll...for every dll referenced in the project.  Now it takes several minutes every time we run our project.  </p>\n\n<p>The project itself has absolutely no Internet functionality...seems to be strictly related to VS 2010. </p>\n\n<p>Disabling MS Symbol Server had no effect, and I could not find anything else in VS settings that would need the Internet.</p>\n\n<p><strong>Edit</strong></p>\n\n<p>We now have internet again, and startup times are back to around 20 seconds.</p>\n\n<p>Previously, I also tried disabling Symbol Server again, and this time it did have a major effect - reduced startup time from 4 minutes to 1 minute.  Don't know why I did not see the same result the first time I tried.</p>\n\n<p>Some testing with Win7 Resource Monitor indicates that VS is trying to talk to 224.0.0.251 and 252...Multicast DNS address and Link-local Multicast Name Resolution address respectively). </p>\n\n<p>Also 239.255.255.250 (Simple Service Discovery Protocol).  Got the definitions by looking up the IP addresses on Wikipedia, but that does not really help me.</p>\n\n<p>Any suggestions on how to track this down?</p>\n"},{"tags":["c#","asp.net","asp.net-mvc","asp.net-mvc-3","performance"],"answer_count":0,"favorite_count":1,"up_vote_count":1,"down_vote_count":1,"view_count":56,"score":0,"question_id":13238988,"title":"MVC3 App High Memory Usage","body":"<p>I have an MVC App that when deployed to the test server uses quite a bit of memory.  The Worker Process for the app pool that is running under is right around 1GB of memory usage. What is the best way to debug something like this?  I am not currently explicitly calling Garbage Collection because I have been told it is not necessary in MVC3.   </p>\n"},{"tags":["performance","cuda"],"answer_count":0,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":80,"score":6,"question_id":13236408,"title":"break overhead vs control flag","body":"<p>I was using a naive prime generated function. This code takes about, 5.25 seconds to generate 10k prime numbers (device_primes[0] holds the number primes already found, the remaining position the prime numbers found).</p>\n\n<pre><code>_global__ void getPrimes(int *device_primes,int n)\n{ \n    int c = 0;\n    int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n    int num = thread_id+2;\n\n    if (thread_id == 0) device_primes[0] = 1;\n    __syncthreads();\n\n    while(device_primes[0] &lt; n)\n    {\n        for (c = 2; c &lt;= num - 1; c++)\n        { \n            if (num % c == 0) //not prime\n            {\n                break;\n            }\n        }\n        if (c == num) //prime\n        {\n            int pos = atomicAdd(&amp;device_primes[0],1);\n            device_primes[pos] = num;\n        }\n        num += blockDim.x * gridDim.x; // Next number for this thread       \n    }\n}\n</code></pre>\n\n<p>I was just starting to optimize the code, and i made the follow modification, instead of :</p>\n\n<pre><code>for (c = 2; c &lt;= num - 1; c++)\n{ \n    if (num % c == 0) //not prime\n         break;\n}\n if (c == num) {...}\n</code></pre>\n\n<p>i have now :</p>\n\n<pre><code>   int prime = 1;\n\n   ...\n   for (c = 2; c &lt;= num - 1 &amp;&amp; prime; c++)\n    { \n        if (num % c == 0) prime = 0; // not prime\n    }\n     if (prime) {...} // if prime\n</code></pre>\n\n<p>Now i can generate 10k in 0.707s. I was just wondering why such speed up with a this simple modification, is break that bad?</p>\n"},{"tags":["iphone","ios","performance","phonegap","titanium"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":558,"score":0,"question_id":9980079,"title":"Why does PhoneGap seem faster than Titanium?","body":"<p>I'm trying to measure the execution perfomance of a few cross-platform solutions, among which are: Titanium and PhoneGap.</p>\n\n<p>So here's an example of the Titanium version of my performance tester, it's very simple, but I'm just trying to get a feeling of how fast my code gets executed:</p>\n\n<pre><code>var looplength;\nvar start1;\nvar start2;\nvar end1;\nvar end2;\nvar duration1;\nvar duration2;\nvar diff;\nvar diffpiter;\nvar power;\nvar info;\n\nfor (power = 0; power &lt; 24; power++) {\n  looplength = Math.pow(2, power);\n\n  start1 = new Date().getTime();\n  for (iterator = 0; iterator &lt; looplength; iterator++) {a=iterator;b=iterator;}\n  end1 = new Date().getTime();\n\n  start2 = new Date().getTime();\n  for (iterator = 0; iterator &lt; looplength; iterator++) {a=iterator;}\n  end2 = new Date().getTime();\n\n  duration1 = end1 - start1;\n  duration2 = end2 - start2;\n  diff      = duration1 - duration2;\n  diffpiter = diff / looplength;\n\n  info={title:'2^' + power + ' ' + diffpiter};\n  tableView.appendRow(Ti.UI.createTableViewRow(info),{animated:true});\n}\n</code></pre>\n\n<p>The PhoneGap version is the same except for the last two lines which get replaced </p>\n\n<pre><code>document.write('2^' + power + ' ' + diffpiter + '&lt;br /&gt;');\n</code></pre>\n\n<p>Both are executed on an iPhone 4S. I've run the test numerous times, to eliminate errors.</p>\n\n<p>How in the name of all that is holy can the Titanium version measure <code>~0.0009</code> milliseconds per iteration while the PhoneGap version measures <code>~0.0002</code> milliseconds per iteration?</p>\n\n<p>Titanium is supposed to compile my javascript code so I expect it to be faster. In this case however it's at least <strong>4 times</strong> slower! I'm not an expert on performance testing, but the test I designed should be at least remotely accurate...</p>\n\n<p>Thank you for any tips you can give me.</p>\n"},{"tags":["asp.net","performance","architecture","thread-safety","application-pool"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":185,"score":-1,"question_id":9558333,"title":"Confusion about Worker Process threading","body":"<p>Try open the following page in <strong>two different tabs</strong> in your browser.  hit the refresh button to avoid getting the page from browser cache :</p>\n\n<pre><code>    protected void Page_Load(object sender, EventArgs e)\n    {\n        Thread.Sleep(10000);\n        Response.Write(DateTime.Now.ToString());\n    }\n</code></pre>\n\n<p>As you can see, it seems that there is a thread created for every request, and they did not wait for each other there is no lock acquired.  </p>\n\n<p>Now Create the following pages, and GlobalCustomClass</p>\n\n<p><strong>GlobalCustomClass</strong></p>\n\n<pre><code>public class GlobalCustomClass\n    {\n        public static string GlobalVariable\n        {\n            get;\n            set;\n        }\n    }\n</code></pre>\n\n<p><strong>Default.aspx</strong></p>\n\n<pre><code>protected void Page_Load(object sender, EventArgs e)\n        {\n            GlobalCustomClass.GlobalVariable = \"Default page\";\n            Thread.Sleep(10000);\n            Response.Write(DateTime.Now.ToString());\n            Response.Write(\"&lt;br /&gt;\");\n            Response.Write(GlobalCustomClass.GlobalVariable);\n        }\n</code></pre>\n\n<p><strong>Page2.aspx</strong></p>\n\n<pre><code> protected void Page_Load(object sender, EventArgs e)\n        {\n            Response.Write(DateTime.Now.ToString());\n            Response.Write(\"&lt;br /&gt;\");\n            GlobalCustomClass.GlobalVariable = \"Page2\";\n            Response.Write(GlobalCustomClass.GlobalVariable);\n\n        }\n</code></pre>\n\n<p>Refresh Default page, and before 10 secs elapses, refresh Page2....Default.aspx is rendering \"Page2\".  Yes it is not thread safe.  </p>\n\n<p>Now try this, open the following page in two browser tabs: </p>\n\n<pre><code>    protected void Page_Load(object sender, EventArgs e)\n    {\n         Session[\"x\"] = \"ABC\";\n         Thread.Sleep(10000);\n         Response.Write(DateTime.Now.ToString());\n    }\n</code></pre>\n\n<p>Now it seems that first thread lock on the session object, and the other have to wait for the whole 10 secs!!</p>\n\n<p>Now try First case but put Global.ascx in the project...and say what, first thread locks on something!!!!!</p>\n\n<p>In real application, it is common to access global state variables that need a thread safety like Sessions.  </p>\n\n<p>If you have a Backend application that contains a report that need 30 secs to render.  and if 60 users opens that report, then user number 61 will wait for half hour till page load into his browser!!  That is a Typical thread starvation!!  and I will be probably fired :(</p>\n\n<p><strong>1)</strong> Every user is waiting till another finish his request. that should make any heavy page a problem.  because it can make the whole web application in-responsive.  <strong>Agree?</strong></p>\n\n<p><strong>2) How to fix that situation?</strong> </p>\n\n<p><strong>3) Under which topic I should research about that? Do you know a good book?</strong></p>\n\n<p>Thanks</p>\n"},{"tags":["performance","ipad","ios6","mkmapview"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":94,"score":1,"question_id":13040044,"title":"MKMapView iOS6 - buggy and slow on scroll and zoom","body":"<p>with the iOS5 MKMapView (google maps) i could zoom and scroll fine on any device.\nNow with iOS6 MKMapView (apple maps) when I start scrolling around the app ends up hanging pretty much every time. \nThis worked fine under iOS5</p>\n\n<p>issues\n- new memory footprint of apple maps is 15x compared to google maps\n- MKMapView becomes unresponsive on my ipad2 in full screen if you scroll around enough ( 20 seconds)\n- annotations start jumping around as well as it begins to become unresponsive</p>\n\n<p>Has anyone got a nice working example on ipad2 with a full screen MKMapView that lets you scroll and zoom without crashing?</p>\n\n<p>Thanks</p>\n"},{"tags":["asp.net","profiling","performance","query-optimization","production-environment"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":675,"score":0,"question_id":3572318,"title":"possible ways of profiling an asp.net website in a production server?","body":"<p>I have an asp.net website up and running in my production server. I want to get the possible ways of profiling an asp.net website in a production server because my application is really slow? As i say slow i don't mean the delivery of static content but the database operations and my c# code? So any suggestion?</p>\n"},{"tags":["c#","performance","performance-testing","evaluation","evaluator"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":7,"view_count":53,"score":-6,"question_id":13237232,"title":"Performance Evaluator in C#","body":"<p>I need to develop an evaluation tool using C# that will run on the system for hours and then will show the overal performance of the system. </p>\n\n<p>The system is supposed to run a service and we want to evaluate how this service is affecting the performance of the system. Will be great if I could use the performance counters that are available in \"Windows Performance Monitor\"... I'm not sure if there is any API available for developers to use them.</p>\n\n<p>I was just looking for suggestions...</p>\n\n<p>Thanks</p>\n"},{"tags":["javascript","html","css","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":33,"score":1,"question_id":13236427,"title":"Auditing front end performance on web application","body":"<p>I am currently trying to performance tune the UI of a company web application. The application is only ever going to be accessed by staff, so the speed of the connection between the server and client will always be considerably more than if it was on the internet. </p>\n\n<p>I have been using performance auditing tools such as Y Slow! and Google Chrome's profiling tool to try and highlight areas that are worth targeting for investigation. However, these tools are written with the internet in mind. For example, the current suggestions from a Google Chrome audit of the application suggests is as follows:</p>\n\n<p><strong>Network Utilization</strong></p>\n\n<ul>\n<li>Combine external CSS (Red warning)</li>\n<li>Combine external JavaScript (Red warning)</li>\n<li>Enable gzip compression (Red warning)</li>\n<li>Leverage browser caching (Red warning)</li>\n<li>Leverage proxy caching (Amber warning)</li>\n<li>Minimise cookie size (Amber warning)</li>\n<li>Parallelize downloads across hostnames (Amber warning)</li>\n<li>Serve static content from a cookieless domain (Amber warning)</li>\n</ul>\n\n<p><strong>Web Page Performance</strong></p>\n\n<ul>\n<li>Remove unused CSS rules (Amber warning)</li>\n<li>Use normal CSS property names instead of vendor-prefixed ones (Amber warning)</li>\n</ul>\n\n<p>Are any of these bits of advice totally redundant given the connection speed and usage pattern? The users will be using the application frequently throughout the day, so it doesn't matter if the initial hit is large (when they first visit the page and build their cache) so long as a minimal amount of work is done on future page views.</p>\n\n<p>For example, is it worth the effort of combining all of our CSS and JavaScript files? It may speed up the initial page view, but how much of a difference will it really make on subsequent page views throughout the working day? </p>\n\n<p>I've tried searching for this but all I keep coming up with is the standard internet facing performance advice. Any advice on what to focus my performance tweaking efforts on in this scenario, or other auditing tool recommendations, would be much appreciated.</p>\n"},{"tags":["android","performance","webview"],"answer_count":3,"favorite_count":9,"up_vote_count":5,"down_vote_count":0,"view_count":4782,"score":5,"question_id":7422427,"title":"Android webview slow","body":"<p>My android webviews are slow. This is on everything from phones to 3.0+ tablets with more than adequate specs</p>\n\n<p>I know that webviews are supposed to be \"limited\" but I see web apps done with phone gap that must be using all sorts of CSS3 and JQuery sorcery, they run just fine and speedy</p>\n\n<p>so I'm missing something, is there some kind of <code>myWebview.SPEEDHACK(1)</code> that I can use to speed things up? Thanks</p>\n\n<p>also, sometimes the contents of my webview just simply don't load, instead of slowly loading, it just wont load. The asset I am testing with is stored locally, no errors.</p>\n"},{"tags":["wcf","performance","windows-8","winrt"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":58,"score":0,"question_id":13231754,"title":"Why is my ARM chip/Surface processing WCF calls a lot slower than my i7 laptop, and is there anything I can do to speed it up?","body":"<p>I am diagnostic testing my winRT store application, and am noticing considerable performance differences between my Surface RT device and my i7 laptop.</p>\n\n<p>Now - i know there is a big difference in expected performance between an ARM CPU and an i7 - but when my average WCF web call on my i7 takes ~0.2s, and my surface device takes ~1.2s I am forced to start looking at optimization and improvements. If the performance difference between the two was only a few hundred milliseconds then I wouldn't mind so much, but the surface device does feel a little bit clunky - and the only bottleneck seems to be the services!</p>\n\n<p>Does anyone have an explanation, or even some performance improvements tips? I should mention that I am running the services across basicBinding with binary serialization. </p>\n"},{"tags":["performance","algorithm","rating"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":69,"score":-1,"question_id":13227946,"title":"Rating Algorithm","body":"<p>I'm trying to develop a rating system for an application I'm working on. Basically app allows you to rate an <b>object</b> from 1 to 5(represented by stars). But I of course know that keeping a rating count and adding the rating the number itself is <b>not</b> feasible.</p>\n\n<p>So the first thing that came up in my mind was <b>dividing the received rating by the total ratings given.</b> Like if the object has received the rating 2 from a user and if the number of times that object has been rated is 100 maybe adding the 2/100. However I believe this method is not good enough since 1)A naive approach 2) In order for me to get the number of times that object has been rated I have to do a look up on db which might end up having time complexity O(n)</p>\n\n<p>So I was wondering what alternative and possibly <b>better</b> ways to approach this problem?</p>\n"},{"tags":["mysql","performance","view","subquery","table-relationships"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":49,"score":0,"question_id":13113411,"title":"Dissolve Nested Subqueries in View","body":"<p>First, I = MySQL.stupid, so please bear with me.</p>\n\n<p>I don't even come close to having the vocabulary to search this out, so apologies if this has already been covered.</p>\n\n<p>I've seen on many pages (older) that nested subqueries can degrade performance.  I'd like to avoid that.  Can anyone show me how to \"dissolve\" them properly into my view if possible?</p>\n\n<p>Please help.</p>\n\n<p>Many thanks in advance!</p>\n\n<pre><code>    CREATE VIEW database1.table1_output \nAS \n  SELECT table1.id                                  AS id, \n     table1.table1column1                       AS table1column1, \n     (SELECT COALESCE(Sum(table2column1), 0) \n      FROM   table2 \n      WHERE  table2column2 = 1 \n             AND table1_id = table1.id) - COALESCE(Sum( \n                                          Abs(table3.table3column1)), 0) \n                                                AS sum1, \n     (SELECT COALESCE(Count(table3column2), 0) \n      FROM   table3 \n      WHERE  ( table3column2 = table4.table4column1 \n                OR table4.table4column1 IS NULL ) \n             AND table1_id = table1.id) - (SELECT \n     COALESCE(Count(table3column2 \n              ), 0) \n                                           FROM   table3 \n                                           WHERE  table3column2 = \n                                                  -1 * table4.table4column1 \n                                                  AND table1_id = table1.id) \n     - COALESCE(Count(table3.table3column3), 0) AS sum2 \n  FROM   table4, \n     table3, \n     table1 \n  WHERE  table3.table1_id = table1.id \n     AND table4.id = table3.article_id \n  GROUP  BY table1.id \n</code></pre>\n\n<p><strong>EDIT</strong></p>\n\n<p>I got rid of the low-hanging fruit, and now my problem is isolated to those three selects.  I'll probably change the title to reflect that; however, I have no idea what the problem is, so I can't even get started on terminology.</p>\n"},{"tags":["sql","performance","stored-procedures"],"answer_count":2,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":38,"score":3,"question_id":13202227,"title":"SQL stored proc runs extremely slow when filtering by high row numbers","body":"<p>This query is generated from a very long dynamic sequel stored procedure -- the procedure returns the requested number of records starting at a given index to be displayed in a Telerik Radgrid, effectively handling paging.  A simplified version of the stored proc's output:</p>\n\n<pre><code>SELECT r.* FROM (\n       SELECT ROW_NUMBER() OVER(ORDER BY InventoryId DESC) as row,\n       v.* FROM vInventorySearch v\n       ) as R WHERE [ROW] BETWEEN 1 AND 10\n</code></pre>\n\n<p>When the \"BETWEEN\" clause is between 1 and 10, it runs in a fraction of a second, but if it's between something like 10000 and 1010 it takes almost a full minute to execute.</p>\n\n<p>I feel like I may be missing something fundamental here, but it seems to me that it shouldn't matter which 10 records I'm retrieving, it should take the same amount of time.</p>\n\n<p>Thanks for any input, I'm looking forward to being embarrassed!</p>\n\n<hr>\n\n<p>Solution, courtesy Martin Smith (below) :</p>\n\n<pre><code>SELECT r.*, inv.* FROM \n(\n    SELECT ROW_NUMBER() OVER(ORDER BY InventoryId DESC) as row, v.InventoryID\n    FROM vInventorySearch v\n    WHERE 1=1 \n) as R \ninner join vInventory inv on r.InventoryID = inv.InventoryID\nWHERE [ROW] BETWEEN 10001 AND 10010\n</code></pre>\n\n<p>Thanks for your help!</p>\n"},{"tags":["java","eclipse","performance","windows-7","scala-ide"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":34,"score":1,"question_id":13050300,"title":"When Package Explorer is pinned open Eclipse takes a long time to redraw itself when restored","body":"<p>I'm out of ideas so I'm posing the question to StackOverflow.  I recently started a scala project.  I setup Eclipse 3.7.2 on Windows 7 and downloaded the latest stable release of the <a href=\"http://scala-ide.org/\" rel=\"nofollow\">Scala IDE</a> 2.0.2.  The problem I'm experiencing is that when the package explorer pane is pinned open in my workspace and I switch context away from Eclipse and then back to Eclipse, it takes a very long time for Eclipse to redraw itself.  It can take anywhere from 2 to 10 seconds to fully redraw.  I know it's related to the package explorer because I don't experience the issue at all when it's not open.  I'm not sure it's specifically related to Scala IDE, but all I've been using Eclipse for recently is with Scala projects and so far I've experienced this problem with all of them.</p>\n\n<p>So far I've tried optimizing my JVM settings as described in the <a href=\"http://stackoverflow.com/questions/142357/what-are-the-best-jvm-settings-for-eclipse\">following post</a>.  I've also tried reverting to a Java 6 JVM.</p>\n\n<p>I'm open for new suggestions or advice on how to troubleshoot this problem further.</p>\n\n<p>EDIT:</p>\n\n<p>I've since discovered there are many views similar to package explorer in the way that they let you navigate a project via it's directory or package structure.  There's the package explorer, project explorer, and navigation view.  Now that I'm using the navigation view I no longer experience any performance problems mentioned in my original post.  However, the question still stands: why does using package explorer have such a detrimental effect on Eclipse performance.</p>\n\n<p>EDIT 2: Scratch that, even while using other \"project navigation\" type views I still have the problem.</p>\n"},{"tags":["mysql","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":13228301,"title":"MySQL query performance slow down suddenly","body":"<p>I have a MySQL (InnoDB) database that contains tables with rows count between 1 000 000 and 50 000 000. \nAt night there is aggregating job which counts some information and writes them to reporting tables. </p>\n\n<p>Fist job execution is very fast. Every query executes between 100ms and 1s. \nAfter that almost every single query is very slow. </p>\n\n<p>The example query is: </p>\n\n<pre><code>SELECT count(*) FROM tableA \n  JOIN tableB ON tableA.id = tableB.tableA_id\n</code></pre>\n\n<p>execution plan for that query shows that for both tables indexes will be used. </p>\n\n<p>Important thing is that CPU, I/O, memory usage is very low. \nMySQL server version: 5.5.28 with default setup (just installed on windows 7 developer computer). </p>\n"},{"tags":[".net","wpf","performance",".net-3.5","measureoverride"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":684,"score":0,"question_id":6959012,"title":"WPF MeasureOverride called multiple times after multiple PropertyChanged notifications","body":"<p>In my application, I have a graph (Custom Wpf Panel) with Wpf controls as elements. The elements are custom controls derived from Wpf Control. DataTemplates associate the controls to their view models.\nThe view models collection, \"GraphElements\" is binded to the itemsControl as shown below.</p>\n\n<pre><code>&lt;ItemsControl x:Name=\"PART_GraphItemsControl\" Grid.Column=\"1\"\n              VerticalAlignment=\"Stretch\" \n              HorizontalAlignment=\"Left\"\n              ItemsSource=\"{Binding Path=GraphElements}\"\n              VirtualizingStackPanel.IsVirtualizing=\"True\"\n              VirtualizingStackPanel.VirtualizationMode=\"Recycling\"&gt;\n    &lt;ItemsControl.ItemsPanel&gt;\n        &lt;ItemsPanelTemplate&gt;\n            &lt;local:GraphDesignerPanel HorizontalAlignment=\"Stretch\"\n                                      VerticalAlignment=\"Top\" /&gt;\n        &lt;/ItemsPanelTemplate&gt;\n    &lt;/ItemsControl.ItemsPanel&gt;\n&lt;/ItemsControl&gt;\n</code></pre>\n\n<p>The elements in the graph can vary from 2 to 500. Under certain mode of the application, the elements display their value. To display the value, the ViewModel for the element fires the <code>INotifyPropertyChanged.PropertyChanged(\"VariableValue\")</code></p>\n\n<p><strong>Issue:</strong>\nWhen i have 100+ elements in the graph, each elements view model fires INotifyPropertyChanged.PropertyChanged to have the elements value displayed. This calls MeasureOverride 100+ times leading to memory and peformance issues.</p>\n\n<p>How can I reduce the number of MeasureOverride calls?</p>\n\n<h2>XAML for Value display for the graph element:</h2>\n\n<pre><code> &lt;TextBlock  \n    Text=\"{Binding Path=VariableValue, StringFormat={}({0})}\" Width=\"60\"\n    FontSize=\"11\" Name=\"txtBlk\"&gt;\n&lt;/TextBlock&gt;\n</code></pre>\n\n<p>The TextBlock above is collapsed if VariableValue is null</p>\n\n<pre><code>&lt;DataTrigger Binding=\"{Binding Path=VariableValue}\" Value=\"{x:Null}\"&gt;\n    &lt;Setter TargetName=\"txtBlk\"  Property=\"Visibility\" Value=\"Collapsed\"/&gt;\n&lt;/DataTrigger&gt;\n</code></pre>\n\n<p><strong>UPDATE:</strong> The issue is reproducible in the sample at the link given below. Download, build, debug. Once App is opened, set a breakpoint in Window.xaml.cs MeasureOverride. Come back to the App and hit the \"Click Me\" button. The breakpoint is hit 11 times !.</p>\n\n<p><a href=\"http://sivainfotech.co.uk/measureoverrideissue.zip\" rel=\"nofollow\">http://sivainfotech.co.uk/measureoverrideissue.zip</a></p>\n\n<p>Any ideas greatly appreciated.</p>\n"},{"tags":["performance","caching","joomla","session","joomla1.5"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":3161,"score":1,"question_id":2897765,"title":"Disabling user sessions for guest","body":"<p>Is it possible to disable session handling in Joomla 1.5 for guests?</p>\n\n<p>I do not use user system in the front end, i assumed that it's not needed to run queries like below:</p>\n\n<p>Site will use APC or Memcache as caching system under heavy load, so it's important for me.</p>\n\n<pre><code>DELETE FROM jos_session WHERE ( time &lt; '1274709357' )\n\nSELECT *  FROM jos_session WHERE session_id = '70c247cde8dcc4dad1ce111991772475'\n\nUPDATE `jos_session` SET `time`='1274710257',`userid`='0',`usertype`='',`username`='',`gid`='0',`guest`='1',`client_id`='0',`data`='__default|a:8:{s:15:\\\"session.counter\\\";i:5;s:19:\\\"session.timer.start\\\";i:1274709740;s:18:\\\"session.timer.last\\\";i:1274709749;s:17:\\\"session.timer.now\\\";i:1274709754;s:22:\\\"session.client.browser\\\";s:88:\\\"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.2.3) Gecko/20100401 Firefox/3.6.3\\\";s:8:\\\"registry\\\";O:9:\\\"JRegistry\\\":3:{s:17:\\\"_defaultNameSpace\\\";s:7:\\\"session\\\";s:9:\\\"_registry\\\";a:1:{s:7:\\\"session\\\";a:1:{s:4:\\\"data\\\";O:8:\\\"stdClass\\\":0:{}}}s:7:\\\"_errors\\\";a:0:{}}s:4:\\\"user\\\";O:5:\\\"JUser\\\":19:{s:2:\\\"id\\\";i:0;s:4:\\\"name\\\";N;s:8:\\\"username\\\";N;s:5:\\\"email\\\";N;s:8:\\\"password\\\";N;s:14:\\\"password_clear\\\";s:0:\\\"\\\";s:8:\\\"usertype\\\";N;s:5:\\\"block\\\";N;s:9:\\\"sendEmail\\\";i:0;s:3:\\\"gid\\\";i:0;s:12:\\\"registerDate\\\";N;s:13:\\\"lastvisitDate\\\";N;s:10:\\\"activation\\\";N;s:6:\\\"params\\\";N;s:3:\\\"aid\\\";i:0;s:5:\\\"guest\\\";i:1;s:7:\\\"_params\\\";O:10:\\\"JParameter\\\":7:{s:4:\\\"_raw\\\";s:0:\\\"\\\";s:4:\\\"_xml\\\";N;s:9:\\\"_elements\\\";a:0:{}s:12:\\\"_elementPath\\\";a:1:{i:0;s:74:\\\"C:\\\\xampp\\\\htdocs\\\\sites\\\\iv.mynet.com\\\\libraries\\\\joomla\\\\html\\\\parameter\\\\element\\\";}s:17:\\\"_defaultNameSpace\\\";s:8:\\\"_default\\\";s:9:\\\"_registry\\\";a:1:{s:8:\\\"_default\\\";a:1:{s:4:\\\"data\\\";O:8:\\\"stdClass\\\":0:{}}}s:7:\\\"_errors\\\";a:0:{}}s:9:\\\"_errorMsg\\\";N;s:7:\\\"_errors\\\";a:0:{}}s:13:\\\"session.token\\\";s:32:\\\"a2b19c7baf223ad5fd2d5503e18ed323\\\";}'\n\n  WHERE session_id='70c247cde8dcc4dad1ce111991772475'\n</code></pre>\n"},{"tags":["performance","delphi","report","fastreport"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":53,"score":0,"question_id":13231372,"title":"Can I use two types of barcode on Fast Report?","body":"<p>I'm want develop a system to print barcode using fastreport, but, I want print Code128B for numbers &lt; 13 and EAN13 for numbers = 13.</p>\n\n<p>How I can use two (2) types of barcode in my report? I using a databand.</p>\n\n<p>Thanks!</p>\n\n<ul>\n<li>I using Delphi and Fast Report 4.9</li>\n</ul>\n"},{"tags":["javascript","html","css","performance","improvements"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":43,"score":1,"question_id":13230332,"title":"Good Tools to Improve JavaScript, CSS and HTML performance","body":"<p>I have an app I've developed and put through PhoneGap Build, and am currently trying to find good tools to improve the performance of the app. It uses HTML, JavaScript &amp; CSS. I'm having trouble finding tools to compress the files and was hoping for suggestions to websites that have them. </p>\n\n<p>Thanks a lot in advance xxx</p>\n"},{"tags":["java","performance","jvm","daemon","startup"],"answer_count":5,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":1029,"score":3,"question_id":4056280,"title":"Anyway to Boost java JVM Startup Speed?","body":"<p>Well ,  it is said that Java is 10x faster than python in terms of performance, thats what i see from benchmarks too. But what really brings down java is its startup time of JVM which is quite stupid.</p>\n\n<p>This is a test i made:</p>\n\n<pre><code>$time xlsx2csv.py Types\\ of\\ ESI\\ v2.doc-emb-Package-9\n...\n&lt;output skipped&gt;\nreal    0m0.085s\nuser    0m0.072s\nsys     0m0.013s\n\n\n$time java  -jar -client /usr/local/bin/tika-app-0.7.jar -m Types\\ of\\ ESI\\ v2.doc-emb-Package-9\n\nreal    0m2.055s\nuser    0m2.433s\nsys     0m0.078s\n</code></pre>\n\n<p>Same file , a 12 KB ms XLSX embedded file inside Docx and Python is 25x faster !! WTH!!</p>\n\n<p>its takes 2.055 sec for Java , which is a joke,.</p>\n\n<p>I know it is all due to startup time, but what i need is i need to call it via a script to parse some documents which i do not want to re-invent the wheel in python.</p>\n\n<p>But as to parse 10k+ files , it is just not practical..</p>\n\n<p>Anyway to speed it up (I already tried -client option and it only speed up by so little(20%) ).</p>\n\n<p>My another idea? Run it as a long-running daemon , communicate using UDP or Linux-ICP sockets locally?</p>\n"},{"tags":["c#","performance","email","imap","imaplib"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":26,"score":1,"question_id":13229465,"title":"High performance IMAP library for .NET","body":"<p>I'm looking for a commercial library to use in my C# app for retrieving messages for IMAP servers. The library would be deployed on a server that handles multiple retrievals from different inboxes. As such, I'm looking for an async library that downloads and parses messages quickly and manages and disposed of its memory efficiently to handle inboxes of potentially hundreds of MB.\nI've tried a few commercial libraries (and ones suggested in other posts), but none of them provide all the required functionality </p>\n\n<p>Would like to know if anybody has successfully deployed a library in such an environment.</p>\n"},{"tags":["javascript","jquery","css","performance","frontend"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":51,"score":0,"question_id":13219367,"title":"The right way to remove a class with jQuery","body":"<p>I have a simple question that interests me a lot:\nIf I want to remove a CSS class with jQuery, what's the right way?\n1. removing after checking for the existence of the class?</p>\n\n<pre><code>if($(div).hasClass('css-class')) {\n  $(div).removeClass('css-class');\n}\n</code></pre>\n\n<p>2. just removing it?</p>\n\n<pre><code>$(div).removeClass('css-class');\n</code></pre>\n\n<p>3.any other suggestions?</p>\n"},{"tags":["wpf","performance","bing"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":175,"score":1,"question_id":8457613,"title":"WPF Application with Bing Maps Control painfully slow on startup","body":"<p>I currently work on a WPF application that has mapping support. I use the Bing Maps WPF Control (from here: <a href=\"http://www.microsoft.com/download/en/details.aspx?id=27165\" rel=\"nofollow\">http://www.microsoft.com/download/en/details.aspx?id=27165</a>) to help with the mapping but now there is a pretty big problem:</p>\n\n<p>The application takes quite some time to start up because the Bing Maps Control retrieves all the initial data to display the map. </p>\n\n<p>The mapping part of my application is only rarely needed and so it would be pretty bad to have slow start up for a feature that is not even used every time so I initially set the control's visibility to \"Collapsed\" in the hope that then no requests would be made, but that doesn't help.</p>\n\n<p>Is there a way to explicitly initialize the Bing Maps control when I want to use it and not at application start up?</p>\n"},{"tags":["java","performance","messaging"],"answer_count":7,"favorite_count":5,"up_vote_count":7,"down_vote_count":0,"view_count":3326,"score":7,"question_id":1481853,"title":"Technique or utility to minimize Java \"warm-up\" time?","body":"<p>I am supporting a Java messaging application that requires low latency (&lt; 300 microseconds processing each message). However, our profiling shows that the Sun Java Virtual Machine runs slowly at first, and speeds up after the first 5,000 messages or so. The first 5,000 messages have latency of 1-4 milliseconds. After about the first 5,000, subsequent messages have ~250 microseconds latency, with occasional outliers.</p>\n\n<p>It's generally understood that this is typical behavior for a Java application. However, from a business standpoint it's not acceptable to tell the customer that they have to wait for the JVM to \"warm-up\" before they see the performance they demand.  The application needs to be \"warmed-up\" before the first customer message is processed</p>\n\n<p>The JVM is Sun 1.6.0 update 4.</p>\n\n<p>Ideas for overcoming this issue:</p>\n\n<ol>\n<li>JVM settings, such as -XX:CompileThreshold= </li>\n<li>Add a component to \"warm-up\" the application on startup, for example by sending \"fake messages\" through the application. </li>\n<li>Statically load application and JDK classes upon application startup, so that classes aren't loaded from JARs while processing customer messages. </li>\n<li>Some utility or Java agent that accomplishes either or both of the above two ideas, so that I don't have to re-invent the wheel.</li>\n</ol>\n\n<p>NOTE: Obviously for this solution I'm looking at all factors, including chip arch, disk type and configuration and OS settings. However, for this question I want to focus on what can be done to optimize the Java application and minimize \"warm up\" time.</p>\n"},{"tags":["matlab","performance","anonymous-function"],"answer_count":3,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":1711,"score":3,"question_id":1673193,"title":"Slow performance using anonymous functions in MATLAB... have others noticed this?","body":"<p>In order to refactor my MATLAB code, I thought I'd pass around functions as arguments (what MATLAB calls anonymous functions), inspired by functional programming.</p>\n\n<p>However, it seems performance is hit quite severely. In the examples below, I compare different approaches. (The code snippet is wrapped in a function in order to be able to use subfunctions)</p>\n\n<p>The result I get is 0 seconds for direct, almost 0 seconds using a subfunction, and 5 seconds using anonymous functions. I'm running MATLAB 7.7 (R2007b) on OS X 10.6, on a C2D 1.8 GHz. </p>\n\n<p>Can anyone run the code and see what they get? I'm especially interested in performance on Windows.</p>\n\n<pre><code>function [] = speedtest()\n\n\nclear all; close all;\n\nfunction y = foo(x)\n    y = zeros(1,length(x));\n    for j=1:N\n        y(j) = x(j)^2;\n    end\nend\n\nx = linspace(-100,100,100000);\nN = length(x);\n\n\n%% direct\nt = cputime;\n\ny = zeros(1,N);\nfor i=1:N\n    y(i) = x(i)^2;\nend\n\nr1 = cputime - t;\n\n%% using subfunction\nt = cputime;\ny = foo(x);\nr2 = cputime - t;\n\n%% using anon function\nfn = @(x) x^2;\n\nt = cputime;\n\ny = zeros(1,N);\nfor i=1:N\n    y(i) = fn(x(i));\nend\n\nr3 = cputime-t;\n\n[r1 r2 r3]\n\nend\n</code></pre>\n"},{"tags":["sql","performance","postgresql"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":112,"score":3,"question_id":12848711,"title":"CTE scan is far far slower than it should be","body":"<p>The following query is being run on about 4 million rows. The first two CTE statements execute in about an hour. The final one however is on track to last more than 15 years.</p>\n\n<pre><code>WITH parsed AS (\n   SELECT name, array(...) description FROM import\n), counts AS (\n   SELECT unnest(description) token, count(*) FROM parsed GROUP BY 1\n) \nINSERT INTO table (name, description) \nSELECT name, ARRAY(\n    SELECT ROW(token, count)::a \n    FROM (\n        SELECT token, (\n            SELECT count \n            FROM counts \n            WHERE a.token=counts.token\n            ) \n        FROM UNNEST(description) a(token)\n        ) _\n    )::a[] description \nFROM parsed;\n\n                                                                  QUERY PLAN                                                                   \n-----------------------------------------------------------------------------------------------------------------------------------------------\n Insert on table  (cost=55100824.40..162597717038.41 rows=3611956 width=96)\n   CTE parsed\n     -&gt;  Seq Scan on import  (cost=0.00..51425557.67 rows=3611956 width=787)\n           Filter: ((name IS NOT NULL) AND (description IS NOT NULL))\n           SubPlan 1\n             -&gt;  HashAggregate  (cost=11.59..12.60 rows=101 width=55)\n                   -&gt;  Append  (cost=0.00..11.34 rows=101 width=55)\n                         -&gt;  Result  (cost=0.00..0.01 rows=1 width=0)\n                         -&gt;  Index Scan using import_aliases_mid_idx on import_aliases  (cost=0.00..10.32 rows=100 width=56)\n                               Index Cond: (mid = \"substring\"(import.mid, 5))\n           SubPlan 2\n             -&gt;  HashAggregate  (cost=0.78..1.30 rows=100 width=0)\n                   -&gt;  Result  (cost=0.00..0.53 rows=100 width=0)\n   CTE counts\n     -&gt;  HashAggregate  (cost=3675165.23..3675266.73 rows=20000 width=32)\n           -&gt;  CTE Scan on parsed  (cost=0.00..1869187.23 rows=361195600 width=32)\n   -&gt;  CTE Scan on parsed  (cost=0.00..162542616214.01 rows=3611956 width=96)\n         SubPlan 6\n           -&gt;  Function Scan on unnest a  (cost=0.00..45001.25 rows=100 width=32)\n                 SubPlan 5\n                   -&gt;  CTE Scan on counts  (cost=0.00..450.00 rows=100 width=8)\n                         Filter: (a.token = token)\n</code></pre>\n\n<p>There are about 4 million rows in both <code>parsed</code> and <code>counts</code>. The query's currently running, and the final statement's inserting a row roughly every 2 minutes. It's barely touching disk, but eating CPU like crazy, and I'm confused.</p>\n\n<p>What's wrong with the query?</p>\n\n<p>The final statement's supposed to look up each element of <code>description</code> in <code>counts</code>, transforming something like this <code>[a,b,c]</code> to something like this <code>[(a,9),(b,4),(c,0)]</code> and inserting it.</p>\n\n<p><br/>\n<strong>Edit</strong></p>\n\n<p>With parsed and counts as tables, and <code>token</code> in counts indexed, this is the plan:</p>\n\n<pre><code>explain INSERT INTO table (name, mid, description) SELECT name, mid, ARRAY(SELECT ROW(token, count)::a FROM (SELECT token, (SELECT count FROM counts WHERE a.token=counts.token) FROM UNNEST(description) a(token)) _)::a[] description FROM parsed;\n                                              QUERY PLAN                                              \n------------------------------------------------------------------------------------------------------\n Insert on table  (cost=0.00..5761751808.75 rows=4002061 width=721)\n   -&gt;  Seq Scan on parsed  (cost=0.00..5761751808.75 rows=4002061 width=721)\n         SubPlan 2\n           -&gt;  Function Scan on unnest a  (cost=0.00..1439.59 rows=100 width=32)\n                 SubPlan 1\n                   -&gt;  Index Scan using counts_token_idx on counts  (cost=0.00..14.39 rows=1 width=4)\n                         Index Cond: (a.token = token)\n</code></pre>\n\n<p>Which is much more reasonable. The arrays have an average of 57 elements, so I guess it was just the sheer number of lookups against the presumable fairly inefficient CTE table that was killing performance. It's now going at 300 rows per second, which I'm happy with.</p>\n"},{"tags":["css","performance","stylesheet","sprite","css-sprites"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":16,"score":0,"question_id":13228039,"title":"Sprite only partially shown","body":"<p>I attempted to make a sprite image for the bottom of my asp.net home page, to reduce the number of http requests.  I looked at all the coding, and can't find any errors.  Here is the image:</p>\n\n<p><img src=\"http://i.stack.imgur.com/T0oEN.jpg\" alt=\"5 social media buttons\"></p>\n\n<p>But for some reason, my google+ image doesn't show up; it only shows 4 of the 5 social media buttons.  Here is the jsfiddle, which doesn't show the google+ button also:</p>\n\n<p><a href=\"http://jsfiddle.net/jasonpaulweber/R8Eu8/\" rel=\"nofollow\">JSFiddle - social media buttons</a></p>\n\n<p>Thank you for anybody who can offer any guidance in this regard!</p>\n\n<p>SE wants me to include code in here, although it's on the Fiddle.  But anyway, here is what I have in my stylesheet:</p>\n\n<pre><code>.homepage{\nwidth:23px;\nheight:23px;\nbackground-image: url(http://www.ussvision.com/images/sprite-homepage.jpg); \nbackground-repeat:no-repeat;\n}\n.fbook{\nbackground-position: 0px 0px;\n}\n.twit{\nbackground-position: -25px 0px;\n}\n.yt{\nbackground-position: -50px 0px;\n}\n.goog{\nbackground-position: -103px 0px;\n}\n.linked{\nbackground-position: -150px 0px;\n}\n</code></pre>\n"},{"tags":["arrays","performance","scala"],"answer_count":1,"favorite_count":3,"up_vote_count":4,"down_vote_count":0,"view_count":87,"score":4,"question_id":13226192,"title":"Fast packed arrays of structs in Scala","body":"<p>I'm investigating what it would take to turn an existing mixed Python/C++ numerical codebase into mixed Scala/C++ (ideally mostly Scala in the long run).  I expect the biggest issue to be packed arrays of structs.  For example, in C++ we have types like</p>\n\n<pre><code>Array&lt;Vector&lt;double,3&gt;&gt; # analogous to double [][3]\nArray&lt;Frame&lt;Vector&lt;double,3&gt;&gt;&gt; # a bunch of translation,quaternion pairs\n</code></pre>\n\n<p>These can be converted back and forth between Python and C++ without copying thanks to Numpy.</p>\n\n<p>On the JVM, since unboxed arrays can only have a handful of types, the only way I can imagine proceeding is to create (1) a boxed Scala type for each struct, such as <code>Vector&lt;double,3&gt;</code> and (2) a typed thin wrapper around <code>Array[Double]</code> that knows what struct it's supposed to be and creates/consumes boxed singletons as necessary.</p>\n\n<p>Are there any existing libraries that do such a thing, or that implement any alternatives for packed arrays of structs?  Does anyone have experience regarding what the performance characters would be likely be, and whether existing compilers and JVM's would be able to optimize the boxes away in at least the nonpolymorphic, sealed case?</p>\n\n<p>Note that packing and nice typing are not optional: Without packing I'll blow memory very quickly, and if all I have is Array[Double] C++'s type system (unfortunately) wins.</p>\n"},{"tags":["c#","performance","optimization","copying"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":56,"score":0,"question_id":13227856,"title":"Will copying occur if the methods of an array element, which is a value type object, are called?","body":"<p>To save the memory, I defined some record-like structs and stored their instances in arrays, as the following code shows:</p>\n\n<pre><code>struct Foo\n{\n    readonly char _bar;\n    readonly int _baz;\n    // other probable fields ...\n    public char Bar{get{return _bar;}}\n    public int Baz{get{return _baz;}}\n\n    public Foo(char bar, int baz){_bar = bar; _baz = baz;}\n}\n\nstatic void Main ()\n{\n    Foo[] arr = new Foo[1000000];\n    int baz;\n    for(var i = 1000000; i-- &gt; 0;){ arr[i] = new Foo('x',42); }\n    for(var i = 1000000; i-- &gt; 0;)\n    { \n        baz = arr[i].Baz; //Will the Foo obj resides at arr[i] be copied?\n    }\n}\n</code></pre>\n\n<p>I know the copying won't happen if the stuff above was implemented in C/C++, yet I'm not sure about C#, what if I replace the <code>Foo[]</code> with an <code>List&lt;Foo&gt;</code>? C# doesn't have a mechanism to return a reference, so theoretically the indexer would return either a pointer(4 bytes) for reference type or the whole copy for value type, if the return value optimization was not involved. So, could anyone tell me, whether this sort of ridiculous copying is guaranteed to be avoided by .NET/MONO jit ?</p>\n"},{"tags":["performance","pex"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":35,"score":0,"question_id":12884982,"title":"Theory : starting Pex Exploration and number of PUT generated","body":"<p>I read many papers about pex , I'm focus on the explorations strategies . I understand that the Fitnex strategy is default used but I don't understand how Pex create the first parametrized unit test .</p>\n\n<p>Where I can find this information ?</p>\n\n<p>By sperimental tests I think :\n- If parameter is \"int\" start with 0\n- If parameter is \"bool\" start with false\n- If parameter is an object start with Null</p>\n\n<p>Is it correct ?</p>\n\n<p>Now about Fitnex strategy and solver constraints Z3 I think that it is able to solve only one constrain by iteration , sorry for my little English now I do an example :</p>\n\n<p>Suppose to have the following method :</p>\n\n<pre><code>public void branchOverTests(bool a, bool b)\n{\n1   if (a)\n    {\n2       Console.WriteLine(\"B1\");\n    }\n    else\n    {\n3       Console.WriteLine(\"B2\");\n    }\n4   if (b)\n    {\n5       Console.WriteLine(\"B3\");\n    }\n    else\n    {\n6       Console.WriteLine(\"B4\");\n    }\n}\n</code></pre>\n\n<p>Numbers are lines identify, Pex generate 3 tests :</p>\n\n<pre><code>--- Test 1\nbranchOverTests(a=false,b=false)\nPath: 1F 3T 4F 6T\nreturn target != (ClassMethod)null;\nreturn target != (ClassMethod)null &amp;&amp; a == false;\nreturn target != (ClassMethod)null &amp;&amp; a == false;\nreturn target != (ClassMethod)null &amp;&amp; a == false &amp;&amp; b == false;\n\n--- Test 2\nbranchOverTests(a=false,b=true)\nPath: 1F 3T 4T 5T\nreturn target != (ClassMethod)null;\nreturn target != (ClassMethod)null &amp;&amp; a == false;\nreturn target != (ClassMethod)null &amp;&amp; a == false;\nreturn target != (ClassMethod)null &amp;&amp; b != false &amp;&amp; a == false;\n\nNote: From Test 1 Flipped last branch: \nreturn target != (ClassMethod)null &amp;&amp; a == false &amp;&amp; b == false; \n-&gt; return target != (ClassMethod)null &amp;&amp; b != false &amp;&amp; a == false; \n=&gt; b = true\n\n--- Test 3\nbranchOverTests(a=true,b=false)\nPath: 1T 2T 4F 6T\nreturn target != (ClassMethod)null;\nreturn target != (ClassMethod)null &amp;&amp; a != false;\nreturn target != (ClassMethod)null &amp;&amp; a != false;\nreturn target != (ClassMethod)null &amp;&amp; a != false &amp;&amp; b == false;\n\nNote: From Test 2 Resolve second condition of last branch:\nreturn return target != (ClassMethod)null &amp;&amp; b != false &amp;&amp; a == false; \n-&gt; return target != (ClassMethod)null &amp;&amp; a != false &amp;&amp; b == false; \n=&gt; a = true\n=&gt; return target != (ClassMethod)null &amp;&amp; a != false;\n=&gt; return target != (ClassMethod)null &amp;&amp; a != false;\n</code></pre>\n\n<p>But the most efficient set of parametrized tests is :</p>\n\n<pre><code>branchOverTests(a=false,b=false)\nbranchOverTests(a=true,b=true)\n</code></pre>\n\n<p>After Test 1 :</p>\n\n<pre><code>return target != (ClassMethod)null &amp;&amp; a == false;\n-&gt; return target != (ClassMethod)null &amp;&amp; a != false;\n=&gt; a = true\n=&gt; return target != (ClassMethod)null &amp;&amp; a != false &amp;&amp; b != false;\n=&gt; b = true\n</code></pre>\n\n<p>Ideal Test 2 :</p>\n\n<pre><code>branchOverTests(a=true,b=true)\nPath: 1T 2T 4T 5T\nreturn target != (ClassMethod)null;\nreturn target != (ClassMethod)null &amp;&amp; a != false;\nreturn target != (ClassMethod)null &amp;&amp; a != false;\nreturn target != (ClassMethod)null &amp;&amp; a != false &amp;&amp; b != false;\n</code></pre>\n\n<p>Is That I think correct ?</p>\n\n<p>Thanks best regards.</p>\n"},{"tags":["performance","assembly","bytecode","opcode","multiplatform"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":181,"score":1,"question_id":5685601,"title":"Unified assembly language","body":"<p>I wonder if there exists some kind of universal and easy-to-code opcode (or assembly) language which provides basic set of instructions available in most of today's CPUs (not some fancy CISC, register-only computer, just common one). With possibility to \"compile\", micro-optimize and \"interpret\" on any mentioned CPUs? </p>\n\n<p>I'm thinking about something like MARS MIPS simulator (rather simple and easy to read code), with possibility to make real programs. No libraries necessary (but nice thing if that possible), just to make things (libraries or UNIX-like tools) faster in uniform way.</p>\n\n<p>Sorry if that's silly question, I'm new to assembler. I just don't find NASM or UNIX assembly language neither extremely cross-platform nor easy to read and code.</p>\n"},{"tags":["performance","internet-explorer-8","knockout.js","leak"],"answer_count":0,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":56,"score":5,"question_id":13227501,"title":"KnockoutJS IE8 performance issues and memory leaks","body":"<p>We build complex enterprise application using KnockoutJS and faced poor performance especially in IE8. Application might take more than 1GB of RAM and never frees memory. During investigation we discovered that KnockoutJS keeps references to the DOM nodes and never deletes it. This can be reproduced using IESieve against any public KnockoutJS example, just watch DOM utilization while playing with example which adds and deletes DOM nodes.</p>\n\n<p>Have anyone faced this problem and has any ideas of how to work this out?</p>\n"},{"tags":["c++","performance"],"answer_count":1,"favorite_count":2,"up_vote_count":2,"down_vote_count":5,"view_count":76,"score":-3,"question_id":13226761,"title":"Fast i/o function implementation","body":"<p>I was searching net about fast i/o in c++ for various contest and i found one piece of fast input function . But i am just a beginner in c++ and couldn't implement it to a simple programme to input using that function . So if someone can give an example code like to input a variable using that function , it would really be a help to me . Here's is the function i found :-</p>\n\n<pre><code>inline void fastRead(int *a)\n{\n register char c=0;\n while (c&lt;33) c=getchar_unlocked();\n *a=0;\n while (c&gt;33)\n {\n     *a=*a*10+c-'0';\n     c=getchar_unlocked();\n }\n}\n</code></pre>\n"},{"tags":["iphone","objective-c","ios","performance","cocoa"],"answer_count":1,"favorite_count":2,"up_vote_count":5,"down_vote_count":0,"view_count":109,"score":5,"question_id":12998733,"title":"Benchmarking performance using Instruments","body":"<p>I am looking for some advice on how to use Instruments' Time Profiler to enhance a specific operation. I have a paging scroll view that loads its content on demand. As a new page is scrolled to, another is loaded two pages to the right. This happens when the current page is scrolled 50% of screen, and on slower devices the loading is enough of a bottle neck that it interrupts the smoothness of the scroll. The scroll feels like it pauses very briefly at the 50% pint and then jumps back into action.</p>\n\n<p>I should note that there is no network component to my application, so the bottle neck is not while data is fetched - it is all in the loading of the new view.</p>\n\n<p>As I work on improving this, I need to benchmark the transition so I can evaluate the effect of my enhancements. Having watched the WWDC sessions, I understand the basics of the time profiler but I'm not confident I am looking at the right thing.</p>\n\n<p>I am running the instrument and then executing the scroll. I see the expected spike in CPU activity. I am then selecting the spike and looking at the symbol names. As you can see below, when I hide system libraries and show Obj-C I am dealing a brief 90% spike with 81.0 ms of running time.</p>\n\n<p><img src=\"http://i.stack.imgur.com/MSdQT.png\" alt=\"enter image description here\"></p>\n\n<p>My confusion comes when I drill down to my code. I find that one of the largest contributors (23 ms) is a small routine that just uses <code>sortedArrayUsingDescriptors</code>. Not much I can do about that one. Other times the root cause will me using CGRectInset - a few more milliseconds. I guess I expected to see more related to image use or something like that.</p>\n\n<p>I guess I don't know if I'm benchmarking the right things. Am I really meant to try and reduce my 6ms operations down to 5ms and expect to see a difference? Am I looking at the right tools to diagnose my problem?</p>\n\n<p>Any tips or instructions on what to measure and where to focus my efforts would be really valued.</p>\n"},{"tags":["sql-server","performance","stored-procedures","database-connection",".net-4.5"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":13224186,"title":"1 stored procedure returning 2 recordsets or 2 database calls","body":"<p>I am writing an ASP.NET 4.5 application that communicates with a SQL Server database. My repository layer communicates with the SQL Server via stored procedures.</p>\n\n<p>I want to query 2 related tables. The first query returns 1 row. The second query returns multiple rows related to the first query. Its not possible to return both in the one query. </p>\n\n<p>At the moment both queries are in the one stored procedure. I user multiple ado.net readers to parse the record set in to an object. </p>\n\n<p>Is the following more performant? </p>\n\n<p>Have two separate stored procedures. Each returning the above record sets in 2 separate calls. I then use the await keyword to indicate that the second call can asynchronously wait on the first i.e. using the new features in .net 4.5. This will keep the database connection open for a shorter period of time but means 2 database connections as oppose to 1.</p>\n"},{"tags":["css","performance","scroll","responsive-design"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":13,"score":0,"question_id":13216926,"title":"Portfolio page on skeleton theme scrolling slow and or jerking in Chrome","body":"<p>If someone can, please view this page:</p>\n\n<p><a href=\"http://thebc.co/our-work\" rel=\"nofollow\">http://thebc.co/our-work</a></p>\n\n<p>Everything is loading fast but when you scroll in Chrome, it isn't smooth at all.</p>\n\n<p>This only happens on the page above. I do not think the images are a problem.  They are super optimized and again, all of the pages load very fast.</p>\n\n<p>The website is built on the skeleton responsive framework.</p>\n\n<p>Thanks for your time.</p>\n"},{"tags":["c","performance","low-level","assembly"],"answer_count":28,"favorite_count":18,"up_vote_count":41,"down_vote_count":0,"view_count":6216,"score":41,"question_id":791533,"title":"Why do you program in assembly?","body":"<p>I have a question for all the hardcore low level hackers out there.  I ran across this sentence in a blog.  I don't really think the source matters (it's Haack if you really care) because it seems to be a common statement.</p>\n\n<blockquote>\n  <p>For example, many modern 3-D Games have their high performance core engine written in C++ and Assembly.</p>\n</blockquote>\n\n<p>As far as the assembly goes - is the code written in assembly because you don't want a compiler emitting extra instructions or using excessive bytes, or are you using better algorithms that you can't express in C (or can't express without the compiler mussing them up)?</p>\n\n<p>I completely get that it's important to understand the low-level stuff.  I just want to understand the <em>why</em> program in assembly after you do understand it.</p>\n\n<p><strong>Edit</strong>:  Wow!  Thank you all for the fantastic responses!  This was way better than I had hoped for!</p>\n"},{"tags":["c#","visual-studio-2010","performance","video","xna"],"answer_count":0,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":30,"score":3,"question_id":13224448,"title":"How to control HD video play position and speed in XNA?","body":"<p>I am developing a game for Windows in <strong>C# using Visual Studio 2010</strong> and <strong>XNA 4.0</strong>. I would like to be able to set and change the play position of an HD video and also play the video in reverse, depending on user input. </p>\n\n<p>I am having trouble finding where to start. XNA's videoPlayer class does not provide these type of functions. I've read that XNA DirectShow is now out of date  and slow when using HD video. </p>\n\n<p>I don't quite understand how I would be able to use or implement tools such as ffmpeg with my project. It seems some people have had similar questions and posted solutions but without much detail. These are below.</p>\n\n<ul>\n<li><p>interop out to talk to the core DX functionality.</p></li>\n<li><p>write a managed c++ wrapper to interop ffmpeg.</p></li>\n<li><p>write an mpeg decoder.</p></li>\n</ul>\n\n<p>I am not sure what would be best and where to begin.\nThanks!</p>\n"},{"tags":["mysql","windows","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":526,"score":2,"question_id":7735546,"title":"Any MySQLTuner equivalent/alternative for Windows?","body":"<p>I know auto tuning has its limits, but it would be great to quickly determine reasonable starting points for my Windows MySQL InnoDB configuration (<code>innodb_additional_mem_pool_size, innodb_buffer_pool_size, innodb_log_file_size, innodb_log_buffer_size</code>), given the RAM that I want to allocate to MySQL (512 Mo from a total 4 GB installed).\nI found <a href=\"http://stackoverflow.com/questions/4667185/mysqltuner-pl-on-a-windows-2008-server-r2\">this previous similar question</a> but it had no answer.</p>\n\n<p>Your help is much appreciated!</p>\n"},{"tags":["performance","algorithm","language-agnostic","data-structures","fibonacci-heap"],"answer_count":2,"favorite_count":21,"up_vote_count":64,"down_vote_count":0,"view_count":12040,"score":64,"question_id":504823,"title":"Has anyone actually implemented a Fibonacci-Heap efficiently?","body":"<p>Has anyone of you ever implemented a <a href=\"http://en.wikipedia.org/wiki/Fibonacci_heap\">Fibonacci-Heap</a>? I did so a few years back, but it was several orders of magnitude slower than using array-based BinHeaps.</p>\n\n<p>Back then, I thought of it as a valuable lesson in how research is not always as good as it claims to be. However, a lot of research papers claim the running times of their algorithms based on using a Fibonacci-Heap. </p>\n\n<p>Did you ever manage to produce an efficient implementation? Or did you work with data-sets so large that the Fibonacci-Heap was more efficient? If so, some details would be appreciated.</p>\n"},{"tags":["performance","opengl"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":47,"score":0,"question_id":13223416,"title":"Is drawing outside the viewport in OpenGL expensive?","body":"<p>I have several thousand quads to draw, some of which might fall entirely outside the viewport. I could write code which will detect which quads fall wholly outside viewport and ask OpenGL to draw only those which will be at least partially visible. Alternatively, I could simply have OpenGL draw all of the quads, regardless of whether they intersect with the viewport.</p>\n\n<p>I don't have enough experience with OpenGL to know if one of these is obviously better (or if OpenGL offers some quick viewport intersection test I can use). Are draws outside the viewport close to being no-ops, or are they expensive enough that I should I try to avoid them?</p>\n"},{"tags":["performance","optimization","set","computer-science","combinations"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":28,"score":0,"question_id":13203216,"title":"Optimizing Sets and Combinations","body":"<p>I have a bunch of sets, A1, A2, A3, ... AN. Each set contains N elements (lets say 1000 max) of values from 0 to 2000.\nEx.</p>\n\n<pre><code>A1{1,2,3,4}\nA2{2,4,3,5}\nA3(1,2,5,6)\n</code></pre>\n\n<p>Now for a size k, e.g. 3, the combinations of A1 would be C1(1,2,3), C2(1,2,4), C3(2,3,4) For A1 to AN I need to figure out if all combinations in A1 also exists somewhere in the combinations of A2 to AN.</p>\n\n<p>i.e. Combination C3 of A1 would match a combination in A2, and maybe AN. Now I need the results of C1 and C2 of A1 aswell against A2 to AN.\nThe simple and inefficient method would be to generate all combinations of k size for a all sets A1 to AN. Then for C1 of A where/if it exists in A2 to AN. After that C2, then C3. Then move on to the next set and repeat.</p>\n\n<p>How could I improve this method as it requires frequent and expensive computation once a new set is added?</p>\n\n<p>One other solution I've found would run in O(N^2 + N) without optimizing which basically involves taking intersections of A1 and A2...AN, computing the combinations of those intersections, then seeing how many times each of those combinations occur in the generated result.</p>\n"},{"tags":["performance","oracle","testing","awr"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":20,"score":-1,"question_id":13221144,"title":"Oracle10g AWR How to measure improvement after splitting tablespaces into multiple disk volumes?","body":"<p>I have a recently released 10 000 user analytical application that is suffering from performance issues because of excessive amount of transactions. Apart from re-writing many of the SQLs in the application for better performance we are also on a top down approach where we are adding more volumes and tweeks to memory etc. An Oracle specialist redesigned our server from single tablespace with 2 volumes to 2 tablespaces with 4 volumes where the indexes will have their own volume and tablespace.   Each volume is a separate disk array so they do not compete for IO. </p>\n\n<p>We have executed this change in development environment and we are about to move into acceptance but before paying for the change the business would like to see measurable gains. </p>\n\n<p><strong>Where in the AWR report can I make comparisons to measure such data on the before and after the change is executed?</strong> </p>\n"},{"tags":[".net","wcf","performance","serialization",".net-4.5"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":24,"score":1,"question_id":13223062,"title":"direct project reference or communication via wcf","body":"<p>I am currently architecting my .net 4.5 applications. At present I have web layer and a wcf services layer. They communicate to each other over soap.</p>\n\n<p>Both layers are co located on the one server.</p>\n\n<p>I am considering moving from communicating over soap to direct project references. So that it moves from one application from two. Main motivation behind this is to improve performance of not having to undergo serializing and deserializing of communication between web and services.</p>\n\n<p>What I lose from this change, is scalability of moving my services layer to a separate tier in the future, ie a different server.</p>\n\n<p>I am looking for feedback on this, is there a major performance hit from the serialization. Thoughts on the above? Anything that I should consider that I have omitted.</p>\n"},{"tags":["mysql","primary-key","sql-order-by","performance","limit"],"answer_count":3,"favorite_count":11,"up_vote_count":16,"down_vote_count":0,"view_count":3097,"score":16,"question_id":4481388,"title":"Why does MYSQL higher LIMIT offset slow the query down?","body":"<p>Scenario in short: A table with more than 16 million records [2GB in size]. The higher LIMIT offset with SELECT, the slower the query becomes, when using     ORDER BY *primary_key*</p>\n\n<p>So  </p>\n\n<pre><code>SELECT * FROM large ORDER BY `id`  LIMIT 0, 30 \n</code></pre>\n\n<p>takes far less than  </p>\n\n<pre><code>SELECT * FROM large ORDER BY `id` LIMIT 10000, 30 \n</code></pre>\n\n<p>That only orders 30 records and same eitherway. So it's not the overhead from ORDER BY.<br>\nNow when fetching the latest 30 rows it takes around 180 seconds. How can I optimize that simple query?</p>\n"},{"tags":["java","performance","biginteger"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":115,"score":2,"question_id":10933470,"title":"Improving performance of addition with very big numbers","body":"<p>I wrote this program to calculate very big numbers without using any BigInteger method. I finished it and it's working properly. I used StringBuilder and lots of parseInt call to get it done. Is there a more efficient way to do this?</p>\n\n<p><em>By the way, this is just worksheet, ignore bad programming style, after finishing my job, I will reorganize that.</em></p>\n\n<pre><code>private String add (String x, String y)\n{\n    String g = \"\";\n    StringBuilder str = new StringBuilder();\n    int sum;\n    double atHand = 0;\n    int dif = (int)(Math.abs(x.length()-y.length())); \n\n    if(x.length() &gt;= y.length())   //adding zero for equalise number of digits.\n    {\n        for(int i = 0; i&lt;dif; i++)\n            g += \"0\";\n        y = g+y;    \n    }\n    else \n    {\n        for(int i = 0; i&lt;dif; i++)\n            g += \"0\";\n        x = g + x;\n    }\n\n    for (int i = y.length()-1; i &gt;=0 ; i--)\n    {\n        sum = Integer.parseInt(x.substring(i, i+1)) +Integer.parseInt(y.substring(i,i+1)) + (int)atHand; \n\n        if(sum&lt;10) \n        {\n            str.insert(0, Integer.toString(sum)); \n            atHand = 0;  \n        }else\n        {\n            if(i==0)\n                str.insert(0, Integer.toString(sum)); \n            else\n            {\n                atHand = sum *0.1;\n                sum = sum %10;  \n                str.insert(0, Integer.toString(sum));\n            }\n        }\n    }\n    return str.toString();\n}\n</code></pre>\n"},{"tags":["c#","asp.net","html","performance"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":966,"score":2,"question_id":7080773,"title":"HtmlGenericControl(\"a\") vs. HtmlAnchor","body":"<p>I was looking into why one of my applications was running quite slowly.  The application generates and displays what is kind of like a Gantt Chart; days along the top, people down the side, and  tasks stretching horizontally across the table to show how long they were worked on for.  Inside each task (table cell) there is an <code>&lt;a href=\"...</code> which brings up some more info about the task.</p>\n\n<p>As this is all built up dynamically from the codebehind, I've used <code>HTMLTableRows/Cells</code> to create the rows and cells, then used the <code>Controls</code> properties to add <code>HTMLAnchors</code>.  Whenever I'm setting attributes I've used <code>HTMLAnchor.HRef</code>, <code>HTMLTableCell.ColSpan</code>, etc.</p>\n\n<p>I noticed that if I use <code>HTMLGenericControl</code> and then just use its <code>Attributes</code> collection, e.g. </p>\n\n<pre><code>HTMLGenericControl a = new HTMLGenericControl(\"a\");\na.Attributes[\"href\"] = task.getLink();\n</code></pre>\n\n<p>it runs significantly quicker than what I would have thought is the preferred way of doing the same thing: </p>\n\n<pre><code>HtmlAnchor a = new HTMLAnchor;\na.HRef = task.getLink();\n</code></pre>\n\n<p>Does anyone have any explanation for where this apparent extra 'overhead' comes from?</p>\n\n<p><strong>EDIT</strong> </p>\n\n<p>In case anyone is confused by my explanation, I posted another question for the same project, which has a <a href=\"http://stackoverflow.com/questions/6278562\">screenshot</a>.</p>\n"},{"tags":["performance","gcc","compilation","ccache"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":193,"score":0,"question_id":8553928,"title":"disadvantages of ccache","body":"<p>I am using ccache for experiments, but I am not quite sure that I should use this. Can anyone explain the situation when ccache can result in wrong behavior. Or should we always use ccache ? Anyone who got that ccache is producing wrong object files or changes in header files are not being considered ?<br></p>\n"},{"tags":["iphone","performance","cocoa","uiview","uiviewcontroller"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":13221005,"title":"Forcing a controller's view into memory","body":"<p>To address some performance issues, I started recycling some view controllers. However, the performance benefit of re-using a recycled controller's view is only present if that view has been drawn. If, for, example, I want to pre-populate the recycle queue with a controller but never place its view on screen, I get no such benefit.</p>\n\n<p>How can I force the controller's view to be 'pre-rendered' and added to my queue such that when it is recycled I receive the performance benefit I am seeing from my other recycled controllers? I know that a controllers's view is created when first needed, but even adding the view and immediately removing it (before the parent view is displayed) doesn't seem to do it.</p>\n"},{"tags":["java","multithreading","performance","concurrency","threadpool"],"answer_count":1,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":129,"score":3,"question_id":12649024,"title":"ExecutorService suitable for a huge amount of short-lived tasks","body":"<p>Is there an ExecutorService that is suitable for a huge amount of very short-lived tasks? I envision something that internally tries busy waiting before switching over to synchronized waiting. Keeping the order of the tasks is not important, but it should be possible to enforce memory consistency (all tasks <em>happen-before</em> the main thread regains control).</p>\n\n<p>The test posted below consists of 100'000 tasks that each generate 100 <code>double</code>s in a row. It accepts the size of the thread pool as command-line parameter and always tests the serial version vs. the parallel one. (If no command-line arg is given, only the serial version is tested.) The parallel version uses a thread pool of fixed size, allocation of the tasks is not even part of the time measurement. Still, the parallel version is <em>never</em> faster than the serial version, I've tried up to 80 threads (on a machine with 40 hyperthreaded cores). Why?</p>\n\n<pre><code>import java.util.ArrayList;\nimport java.util.Random;\nimport java.util.concurrent.Callable;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\n\npublic class ExecutorPerfTest {\n    public static final int TASKS = 100000;\n    public static final int SUBTASKS = 100;\n\n    static final ThreadLocal&lt;Random&gt; R = new ThreadLocal&lt;Random&gt;() {\n        @Override\n        protected synchronized Random initialValue() {\n            return new Random();\n        }\n    };\n\n    public class SeqTest implements Runnable {\n        @Override\n        public void run() {\n            Random r = R.get();\n            for (int i = 0; i &lt; TASKS; i++)\n                for (int j = 0; j &lt; SUBTASKS; j++)\n                    r.nextDouble();\n        }\n    }\n\n    public class ExecutorTest implements Runnable {\n        private final class RandomGenerating implements Callable&lt;Double&gt; {\n            @Override\n            public Double call() {\n                double d = 0;\n                Random r = R.get();\n                for (int j = 0; j &lt; SUBTASKS; j++)\n                    d = r.nextDouble();\n                return d;\n            }\n        }\n\n        private final ExecutorService threadPool;\n        private ArrayList&lt;Callable&lt;Double&gt;&gt; tasks = new ArrayList&lt;Callable&lt;Double&gt;&gt;(TASKS);\n\n        public ExecutorTest(int nThreads) {\n            threadPool = Executors.newFixedThreadPool(nThreads);\n            for (int i = 0; i &lt; TASKS; i++)\n                tasks.add(new RandomGenerating());\n        }\n\n        public void run() {\n            try {\n                threadPool.invokeAll(tasks);\n            } catch (InterruptedException e) {\n                e.printStackTrace();\n            } finally {\n                threadPool.shutdown();\n            }\n        }\n    }\n\n    public static void main(String[] args) {\n        ExecutorPerfTest executorPerfTest = new ExecutorPerfTest();\n        if (args.length &gt; 0)\n            executorPerfTest.start(new String[]{});\n        executorPerfTest.start(args);\n    }\n\n    private void start(String[] args) {\n        final Runnable r;\n        if (args.length == 0) {\n            r = new SeqTest();\n        }\n        else {\n            final int nThreads = Integer.parseInt(args[0]);\n            r = new ExecutorTest(nThreads);\n        }\n        System.out.printf(\"Starting\\n\");\n        long t = System.nanoTime();\n        r.run();\n        long dt = System.nanoTime() - t;\n        System.out.printf(\"Time: %.6fms\\n\", 1e-6 * dt);\n    }\n}\n</code></pre>\n"},{"tags":["c++","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":110,"score":-1,"question_id":13220205,"title":"What is faster? two ints or __int64?","body":"<p>What is faster: (Performance)</p>\n\n<pre><code>__int64 x,y;\nx=y;\n</code></pre>\n\n<p>or</p>\n\n<pre><code>int x,y,a,b;\nx=a;\ny=b;\n</code></pre>\n\n<p>?</p>\n\n<p>Or they are equal?</p>\n"},{"tags":["performance","matlab","matrix-multiplication"],"answer_count":10,"favorite_count":9,"up_vote_count":41,"down_vote_count":0,"view_count":6019,"score":41,"question_id":6058139,"title":"Why is MATLAB so fast in matrix multiplication?","body":"<p>I am making some benchmarks with CUDA, C++, C#, and Java, and using MATLAB for verification and matrix generation.  But when I multiply with MATLAB, 2048x2048 and even bigger matrices are almost instantly multiplied.</p>\n\n\n\n<pre class=\"lang-none prettyprint-override\"><code>             1024x1024   2048x2048   4096x4096\n             ---------   ---------   ---------\nCUDA C (ms)      43.11      391.05     3407.99\nC++ (ms)       6137.10    64369.29   551390.93\nC# (ms)       10509.00   300684.00  2527250.00\nJava (ms)      9149.90    92562.28   838357.94\nMATLAB (ms)      75.01      423.10     3133.90\n</code></pre>\n\n<p>Only CUDA is competitive, but I thought that at least C++ will be somewhat close and not 60x slower.</p>\n\n<p>So my question is - How is MATLAB doing it that fast? </p>\n\n<p>C++ Code:</p>\n\n<pre><code>float temp = 0;\ntimer.start();\nfor(int j = 0; j &lt; rozmer; j++)\n{\n    for (int k = 0; k &lt; rozmer; k++)\n    {\n        temp = 0;\n        for (int m = 0; m &lt; rozmer; m++)\n        {\n            temp = temp + matice1[j][m] * matice2[m][k];\n        }\n        matice3[j][k] = temp;\n    }\n}\ntimer.stop();\n</code></pre>\n\n<p>Edit:\nI also dont know what to think about the C# results. The algorithm is just the same as C++ and Java, but there's a giant jump 2048 from 1024?</p>\n\n<p>Edit2:\nUpdated MATLAB and 4096x4096 results</p>\n"},{"tags":["performance","sybase","ssas"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":132,"score":1,"question_id":12643452,"title":"SSAS 2008 R2 with Sybase OLEDB (ASEOLEDB.1)","body":"<p>I am designing a high volume data warehouse using Adaptive Server Enterprise 15.7. The business is already using SSAS 2008 R2 for much of their data analysis and wishes to continue using it on top of the aforementioed data warehouse.</p>\n\n<p>I was wondering if anyone out there in the community had done anything like this before and could share some advice. A few estimations about this data warehouse follow:</p>\n\n<ul>\n<li>Dimensions are slowly changing (every 3 days or so)</li>\n<li>Facts will get ~15 million new records daily (12 measures)</li>\n</ul>\n\n<p>I'm most concerned with the performance of the processing rather than the querying.</p>\n\n<p>Cheers,</p>\n\n<p>RA</p>\n"},{"tags":["java","performance","sorting"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":38,"score":0,"question_id":13219491,"title":"Sorting different type of objects (performance)","body":"<p>Let's say I have a class called Unit (with a position variable x) and extend the class to UnitA, UnitB, UnitC, etc..</p>\n\n<p>this is something I came up with:</p>\n\n<pre><code>Unit[] ordered = new Unit[a_num+b_num+c_num];\nordered = Arrays.copyOf(a_units, a_num); //and add b_units, c_units, etc\nArrays.sort(ordered); //sort using compareTo method\n</code></pre>\n\n<ul>\n<li>For best performance and neat programming, what is the best way to sort these values from left to right (the x variable)?</li>\n<li>When the array is sorted, in order to access unit specific variables, how do I find out what type of object each entry is?</li>\n</ul>\n"},{"tags":["performance","assembly","intel","rdtsc"],"answer_count":3,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":588,"score":4,"question_id":6432669,"title":"Variance in RDTSC overhead","body":"<p>I'm constructing a micro-benchmark to measure performance changes as I experiment with the use of SIMD instruction intrinsics in some primitive image processing operations. However, writing useful micro-benchmarks is difficult, so I'd like to first understand (and if possible eliminate) as many sources of variation and error as possible.</p>\n\n<p>One factor that I have to account for is the overhead of the measurement code itself. I'm measuring with RDTSC, and I'm using the following code to find the measurement overhead:</p>\n\n<pre><code>extern inline unsigned long long __attribute__((always_inline)) rdtsc64() {\n    unsigned int hi, lo;\n        __asm__ __volatile__(\n            \"xorl %%eax, %%eax\\n\\t\"\n            \"cpuid\\n\\t\"\n            \"rdtsc\"\n        : \"=a\"(lo), \"=d\"(hi)\n        : /* no inputs */\n        : \"rbx\", \"rcx\");\n    return ((unsigned long long)hi &lt;&lt; 32ull) | (unsigned long long)lo;\n}\n\nunsigned int find_rdtsc_overhead() {\n    const int trials = 1000000;\n\n    std::vector&lt;unsigned long long&gt; times;\n    times.resize(trials, 0.0);\n\n    for (int i = 0; i &lt; trials; ++i) {\n        unsigned long long t_begin = rdtsc64();\n        unsigned long long t_end = rdtsc64();\n        times[i] = (t_end - t_begin);\n    }\n\n    // print frequencies of cycle counts\n}\n</code></pre>\n\n<p>When running this code, I get output like this:</p>\n\n<pre><code>Frequency of occurrence (for 1000000 trials):\n234 cycles (counted 28 times)\n243 cycles (counted 875703 times)\n252 cycles (counted 124194 times)\n261 cycles (counted 37 times)\n270 cycles (counted 2 times)\n693 cycles (counted 1 times)\n1611 cycles (counted 1 times)\n1665 cycles (counted 1 times)\n... (a bunch of larger times each only seen once)\n</code></pre>\n\n<p>My questions are these:</p>\n\n<ol>\n<li><strong>What are the possible causes of the bi-modal distribution of cycle counts generated by the code above?</strong></li>\n<li><strong>Why does the fastest time (234 cycles) only occur a handful of times&mdash;what highly unusual circumstance could <em>reduce</em> the count?</strong></li>\n</ol>\n\n<hr>\n\n<p><strong>Further Information</strong></p>\n\n<p>Platform:</p>\n\n<ul>\n<li>Linux 2.6.32 (Ubuntu 10.04)</li>\n<li>g++ 4.4.3</li>\n<li>Core 2 Duo (E6600); this has constant rate TSC.</li>\n</ul>\n\n<p>SpeedStep has been turned off (processor is set to performance mode and is running at 2.4GHz); if running in 'ondemand' mode, I get two peaks at 243 and 252 cycles, and two (presumably corresponding) peaks at 360 and 369 cycles.</p>\n\n<p>I'm using <code>sched_setaffinity</code> to lock the process to one core. If I run the test on each core in turn (i.e., lock to core 0 and run, then lock to core 1 and run), I get similar results for the two cores, except that the fastest time of 234 cycles tends to occur slightly fewer times on core 1 than on core 0.</p>\n\n<p>Compile command is:</p>\n\n<pre><code>g++ -Wall -mssse3 -mtune=core2 -O3 -o test.bin test.cpp\n</code></pre>\n\n<p>The code that GCC generates for the core loop is:</p>\n\n<pre><code>.L105:\n#APP\n# 27 \"test.cpp\" 1\n    xorl %eax, %eax\n    cpuid\n    rdtsc\n# 0 \"\" 2\n#NO_APP\n    movl    %edx, %ebp\n    movl    %eax, %edi\n#APP\n# 27 \"test.cpp\" 1\n    xorl %eax, %eax\n    cpuid\n    rdtsc\n# 0 \"\" 2\n#NO_APP\n    salq    $32, %rdx\n    salq    $32, %rbp\n    mov %eax, %eax\n    mov %edi, %edi\n    orq %rax, %rdx\n    orq %rdi, %rbp\n    subq    %rbp, %rdx\n    movq    %rdx, (%r8,%rsi)\n    addq    $8, %rsi\n    cmpq    $8000000, %rsi\n    jne .L105\n</code></pre>\n"},{"tags":["visual-studio-2010","performance","workflow"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":16,"score":0,"question_id":13217313,"title":"Visual C++ Workflow: Edit > (Slow) Compile > Run?","body":"<p>Whats your workflow when developing VC++ applications in Visual Studio? Coming from a mostly dynamic interpreted language (and mostly Java in school), I am increasingly frustrated by speed of VC++ development workflow ... I am doing <code>edit &gt; save/compile &gt; run/test</code>, a typical workflow? But the compile step is long ... considering I'm new to C++ and thus try alot of things (many times) then compile (~10-20s). The compile times is significant, how can I speed things up? </p>\n\n<p>I am on </p>\n\n<ul>\n<li>i3 2100</li>\n<li>have an SSD but I'm not sure its a good idea to have VS on it, will it write alot when compiling thus wearing down the SSD? Its a Crucial M4. </li>\n<li>8GB DDR3 </li>\n</ul>\n\n<p>I think specs are reasonable? I heard a better processor helps more than an SSD? </p>\n"},{"tags":["php","performance","concurrency"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":43,"score":2,"question_id":13215757,"title":"A Busy Php script making other scripts slower- how?","body":"<p>On my localserver, XAMPP environment, I'm running a test PHP script that takes 20 seconds to run, but uses only 2MB of memory and 10% CPU. </p>\n\n<p>When I open a new window and run the same script at the same time as the first, it takes over 30 seconds for both scripts to finish. </p>\n\n<p>--The script is a simple for loop that writes to mysql DB, InnoDb, 200 times.</p>\n\n<p>Shouldn't the script take the same amount of time, but use more system resources?</p>\n\n<p>As in, scale linearly.</p>\n\n<p>Why is this?</p>\n\n<pre><code>  //the code in all its glory-- Post extends a CRUD class\n  // These are the values to be saved:\n   $values = array(\n  'id' =&gt; '',\n  'content' =&gt; 'This is the VALUE'\n                 );\n  //And the action. I know-Saving Mysql in a loop is a no-no-- \n    //for demonstration only  \n   for($i=0; $i&lt;250; $i++){\n\n   $object = new Post($values); //instantiate the Post Class with values\n   $object-&gt;create($values);   //save the values to the Db. The end\n                           }\n</code></pre>\n\n<p>20 seconds.</p>\n"},{"tags":["c#","performance"],"answer_count":3,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":70,"score":3,"question_id":13215065,"title":"Find equal or nearest smaller value from an array","body":"<p>Let's suppose I have this array <em>(it is actually 255 long, values up to int.MaxValue)</em>:</p>\n\n<pre><code>int[] lows = {0,9,0,0,5,0,0,8,4,1,3,0,0,0,0};\n</code></pre>\n\n<p>From this array I would like to get index of a value <strong>equal</strong> or <strong>smaller</strong> to my number.</p>\n\n<pre><code>number = 7 -&gt; index = 4\nnumber = 2 -&gt; index = 9\nnumber = 8 -&gt; index = 7\nnumber = 9 -&gt; index = 1\n</code></pre>\n\n<p>What would be the fastest way of finding it? </p>\n\n<p>So far I've used linear search, but that turned out to be too inefficient for my need, because even though this array is only 255 long, values will be searched for a few million times. </p>\n\n<p>I would need something equal to <em>TreeSet.floor(E)</em> used in java. I wanted to use <em>Dictionary</em>, but i don't know if it can find first smaller or equal value like I need.</p>\n"},{"tags":["java","performance","random"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":35,"score":1,"question_id":13213395,"title":"Adjusting XORShift generator to return a number within a maximum","body":"<p>I need to generate random integers within a maximum. Since <strong>performance is critical</strong>, I decided to use a XORShift generator instead of Java's Random class.</p>\n\n<pre><code>long seed = System.nanoTime();\nseed ^= (seed &lt;&lt; 21);\nseed ^= (seed &gt;&gt;&gt; 35);\nseed ^= (seed &lt;&lt; 4);\n</code></pre>\n\n<p>This implementation <sub><a href=\"http://www.javamex.com/tutorials/random_numbers/xorshift.shtml\" rel=\"nofollow\">(source)</a></sub> gives me a long integer, but what I really want is an integer between 0 and a maximum.  </p>\n\n<pre><code>public int random(int max){ /*...*/}\n</code></pre>\n\n<p>What it is the most efficient way to implement this method?</p>\n"},{"tags":["performance","parallel-processing","processing-efficiency"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":60,"score":1,"question_id":13207743,"title":"Efficiency & speedup of parallel vs. serial","body":"<p>Currently, I am reading over a study a guide that my professor handed out in class. The study guide is not an assignment, just something to know what to expect on an exam. I've completed all but 1 problem and was hoping someone could help me out.</p>\n\n<p>Here is the question:\nSuppose Tserial = n and Tparallel = n/p + log2(p), where times are in miliseconds and p is the \nnumber of processes. If we increase p by a factor of k, find a formula for how much we’ll need to increase n in order to maintain constant efficiency. How much should we increase n by if we double the number of processes from 8 to 16? Is the parallel program scalable?</p>\n\n<p>Any help in understanding this would be greatly appreciated.</p>\n"},{"tags":["php","javascript","performance","html5"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":59,"score":0,"question_id":13205729,"title":"Optimum way to insert external html content HTML5 compliant? php vs HTML5 vs javascript methods","body":"<p>I know how simple this probably seems to you gurus, but I have been searching for over an hour to no avail...</p>\n\n<p>Goal:  Use a single footer file and menu file for all my webpages.  Taking into account blocking, speed, etc.  The content of my menu is pure html/css and the content of my footer is pure html/css.  Would the optimal solution change based on the content being injected?  e.g.  If videos, jscript, etc. were involved.</p>\n\n<p>Two part question:\n1)  Which method is optimal?  Some kind of php include, using the  tag, using jscript, etc.\n2)  How precisely is this achieved keeping HTML 5 standards?  i.e.  For the php method to work, does my calling webpage need to be .php and then does that make the HTML5 standard a moot point?  e.g.  If I want to inject footer.php into index.html, does my index file also have to be .php?  Similarly for the  tag, can the external file be an .html file(I don't like the idea of reloading all the header information with .css calls) or should it be .php?  </p>\n\n<p>Within the index.html file I have tried the following:</p>\n\n<pre><code>&lt;object id=\"footerArea\" width=\"100%\" height=\"20%\" \n  type=\"text/html\" data=\"footer.html\"&gt;\n&lt;/object&gt;\n</code></pre>\n\n<p>and</p>\n\n<pre><code>&lt;?php include 'footer.php' ?&gt;\n</code></pre>\n\n<p>Neither of these seem to work for me.</p>\n\n<p>In case you are wondering...  Here is the code for my footer I am trying to inject with sample data to make it shorter and easier to read:</p>\n\n<pre><code>&lt;div class=\"footer box\"&gt;\n&lt;p class=\"f-right t-right\"&gt;\n &lt;a href=\"#\"&gt;www.mysite.com&lt;/a&gt;&lt;br /&gt;\n  Address: Medford, OR&lt;br /&gt;\n  Phone: (541) 555-5555\n&lt;/p&gt;\n\n&lt;p class=\"f-left\"&gt;\n Copyright &amp;copy;&amp;nbsp;2011 &lt;a href=\"#\"&gt;My Name&lt;/a&gt;&lt;br /&gt;\n&lt;/p&gt;\n\n&lt;p class=\"f-left\" style=\"margin-left:20px;\"&gt;\n &lt;a href=\"http://sampleurl.com\" target=\"_blank\"&gt;\n  &lt;img style=\"border:0;width:88px;height:31px\"\n   src=\"http://sampleurl.com\"\n   alt=\"Valid CSS3!\" /&gt;\n &lt;/a&gt;\n&lt;/p&gt;\n\n&lt;p class=\"f-left\" style=\"margin-left:20px;\"&gt;\n &lt;a href=\"http://sampleurl\" target=\"_blank\"&gt;\n &lt;img src=\"http://sample.png\" width=\"228\" height=\"50\" alt=\"sample alt\" title=\"sample title\"&gt;\n &lt;/a&gt;\n&lt;/p&gt;\n&lt;/div&gt;\n</code></pre>\n\n<p>Please excuse my formatting.  I am still new to posting code in forums.  I tried my best :)</p>\n\n<p>Because this is really beginning to make me angry, I am including a link to my site: <a href=\"http://tiny.cc/zql7mw\" rel=\"nofollow\">http://tiny.cc/zql7mw</a> If you can help me fix this issue, I WILL PAY YOU.</p>\n"},{"tags":["performance","convolution"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":31,"score":2,"question_id":13208451,"title":"What is the best 2D convolution freely-available implementation?","body":"<h1>What is the best 2D convolution freely-available implementation? </h1>\n\n<p>Some context :</p>\n\n<ul>\n<li> <b>language agnostic</b>: whatever language you like C/C++/Python/Matlab ... you name it, I need the fastest 2D convolution implementation;</li>\n<li> <b>theoretically principled</b>: For example, <b><i>FFT</i> is NOT always the fastest</b> way because it highly depends on the sizes of the 2 inputs, thus I need an implementation that is smart enough to switch between different modes accordingly;</li>\n<li> <b>format agnostic between single and double input entries</b> BUT for 64bits computers;</li>\n<li> [optional] rather <b>UNIX-based OS</b> solution <i>e.g.</i> Linux and MacOSX;</li>\n<li> [optional] rather <b>NOT GPU-based</b> solution unfortunately because it is difficult to install it/deploy it/assume that anybody has it already ready to go both for compilation and execution especially if you are not <i>root</i>/<i>admin</i>;</li>\n<li> [optional] rather <b>multi-threaded</b> solution but with an easy way for the user <b>to set the number of allowed threads/cores</b> let's say at execution time for <i>e.g.</i> large scale cluster intensive computations.</li>\n</ul>\n\n<p>Although I spent a lot of time crawling around the www, I only found:</p>\n\n<p><i>git clone <a href=\"https://github.com/rbgirshick/voc-dpm.git\" rel=\"nofollow\">https://github.com/rbgirshick/voc-dpm.git</a></i></p>\n\n<p>in the gdetect folder there is <b>fconvsse.cc file that is much faster that anything I found anywhere else</b>. </p>\n\n<p>Is it the best possible?</p>\n\n<p>I think it could be great for the Research Community to get a nice code for this old Computer Science problem with even a popularized www link that I do not know in spite of my efforts.</p>\n\n<p>In other words:</p>\n\n<h1>Do you know a fast convolution implementation that you could give me the link?</h1>\n\n<p>All I am asking is one or a few links.</p>\n\n<p>Best regards,</p>\n\n<p><i>A new grateful Stack<b>Overflow</b> user</i></p>\n"},{"tags":["c++","performance","profiling","code-snippets"],"answer_count":6,"favorite_count":12,"up_vote_count":8,"down_vote_count":0,"view_count":2274,"score":8,"question_id":61278,"title":"Quick and dirty way to profile your code","body":"<p>What method do you use when you want to get performance data about specific code paths?</p>\n"},{"tags":["sql","sql-server","performance","sql-server-2008","query"],"answer_count":4,"favorite_count":1,"up_vote_count":7,"down_vote_count":0,"view_count":35148,"score":7,"question_id":4193705,"title":"SQL Server SELECT LAST N Rows","body":"<p>This is a known question but the best solution I've found is something like:</p>\n\n<pre><code>SELECT TOP N *\nFROM MyTable\nORDER BY Id DESC\n</code></pre>\n\n<p>I've a table with lots of rows. It is not a posibility to use that query because it takes lot of time. So how can I do to select last N rows without using ORDER BY?</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>Sorry duplicated question of <a href=\"http://stackoverflow.com/questions/311054/how-do-i-select-last-5-rows-in-a-table-without-sorting\">this one</a></p>\n"},{"tags":["java","performance","instantiation","bigdecimal"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":70,"score":2,"question_id":13209833,"title":"Fastest way to instantiate BigDecimal from string","body":"<p>Consider an application that</p>\n\n<ol>\n<li>Reads several thousands of <code>String</code> values from a text file.</li>\n<li>Selects (via a regular expression match) those values that represent a number (from simple integers to very large values written in scientific notation with mantissa).</li>\n<li>For each <code>String</code> value representing a number, instantiates a <code>BigDecimal</code> object (at a total rate of thousands of Bigdecimal objects per second).</li>\n<li>Uses each instantiated <code>BigDecimal</code> object for further processing.</li>\n</ol>\n\n<p>Given the above scenario, obviously the instantiation of each <code>BigDecimal</code> object has an impact on performance.</p>\n\n<p>One way to instantiate those <code>BigDecimal</code> objects from a non-null String <code>str</code>, is:</p>\n\n<pre><code>BigDecimal number = new BigDecimal(str.toCharArray(), 0, str.length()));\n</code></pre>\n\n<p>which is exactly what the <a href=\"http://docs.oracle.com/javase/6/docs/api/java/math/BigDecimal.html#BigDecimal%28java.lang.String%29\" rel=\"nofollow\">String constructor of BigDecimal</a> expands to in the <code>Oracle</code> implementation of the <code>JDK</code>.</p>\n\n<p>Is there a faster way of instantiating <code>BigDecimal</code> objects from such strings, or via an alternative approach?</p>\n"},{"tags":["java","performance","getter-setter"],"answer_count":5,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":80,"score":1,"question_id":13209311,"title":"Should we use collection, get and sets?","body":"<p>I was reading a java book, where it said that when accessing/modifying  to variables in different class it should be use the get/set methods to manipulate them.</p>\n\n<p>My question is, overtime and in big projects using gets/sets will not jeopardize the application performance?</p>\n\n<p>Similar question, typically should we preferable use arrays in detriment of more abstract data type (such likened list of instance), since array are normally more cache friendly.</p>\n"},{"tags":["c#",".net","performance","sockets"],"answer_count":11,"favorite_count":20,"up_vote_count":17,"down_vote_count":0,"view_count":15590,"score":17,"question_id":319732,"title":"Tips / techniques for high-performance C# server sockets","body":"<p>I have a .NET 2.0 server that seems to be running into scaling problems, probably due to poor design of the socket-handling code, and I am looking for guidance on how I might redesign it to improve performance.</p>\n\n<p><strong>Usage scenario:</strong> 50 - 150 clients, high rate (up to 100s / second) of small messages (10s of bytes each) to / from each client. Client connections are long-lived - typically hours. (The server is part of a trading system. The client messages are aggregated into groups to send to an exchange over a smaller number of 'outbound' socket connections, and acknowledgment messages are sent back to the clients as each group is processed by the exchange.) OS is Windows Server 2003, hardware is 2 x 4-core X5355. </p>\n\n<p><strong>Current client socket design:</strong> A <code>TcpListener</code> spawns a thread to read each client socket as clients connect. The threads block on <code>Socket.Receive</code>, parsing incoming messages and inserting them into a set of queues for processing by the core server logic. Acknowledgment messages are sent back out over the client sockets using async <code>Socket.BeginSend</code> calls from the threads that talk to the exchange side.</p>\n\n<p><strong>Observed problems:</strong> As the client count has grown (now 60-70), we have started to see intermittent delays of up to 100s of milliseconds while sending and receiving data to/from the clients. (We log timestamps for each acknowledgment message, and we can see occasional long gaps in the timestamp sequence for bunches of acks from the same group that normally go out in a few ms total.) </p>\n\n<p>Overall system CPU usage is low (&lt; 10%), there is plenty of free RAM, and the core logic and the outbound (exchange-facing) side are performing fine, so the problem seems to be isolated to the client-facing socket code. There is ample network bandwidth between the server and clients (gigabit LAN), and we have ruled out network or hardware-layer problems.</p>\n\n<p>Any suggestions or pointers to useful resources would be greatly appreciated. If anyone has any diagnostic or debugging tips for figuring out exactly what is going wrong, those would be great as well.</p>\n\n<p><em>Note: I have the MSDN Magazine article <a href=\"http://msdn2.microsoft.com/en-us/magazine/cc300760.aspx\" rel=\"nofollow\">Winsock: Get Closer to the Wire with High-Performance Sockets in .NET</a>, and I have glanced at the Kodart \"XF.Server\" component - it looks sketchy at best.</em></p>\n"},{"tags":["c#","performance","events","xna","keyboard"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":30,"score":0,"question_id":13208320,"title":"Keyboard button \"event\" efficiency in XNA","body":"<p>I have been trying to figure it out for myself, but all tutorials online and everything I could find, does not really explain my question, so I hope someone here can help me.</p>\n\n<p>I so far have only worked with C# mainly using WPF and if I want to raise an event whey a key is pressed on the keyboard, I simply use the <code>KeyDown</code> event. There I can easily identify the pressed key by <code>e.Key</code>.</p>\n\n<p>Now in XNA everything I have seen is using <code>KeyboardState state = Keyboard.GetState();</code> to get the state of the keyboard and constantly check in the <code>Update()</code>-method if e.g. <code>state.IsKeyDown(Keys.Left);</code> returns true of false.</p>\n\n<p>And my question is: Is that not really inefficient? If for example my game uses 15 keys for input, I would get the keyboard state and check every single of those 15 keys and that 30 times a second. Is there a reason, why it seems to be so common to use this approach in XNA?\nThe only explanation I could think of, is to make sure everything remains in the <code>Update</code>-method so it will definietly be executed, such that no delayed events cause problems in the game.</p>\n"},{"tags":["css","performance","selectors"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":48,"score":0,"question_id":13207683,"title":"Do browsers read css selectors from right to left?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/5797014/why-do-browsers-match-css-selectors-from-right-to-left\">Why do browsers match CSS selectors from right to left?</a>  </p>\n</blockquote>\n\n\n\n<p>I've heard browsers read css selectors from right to left. So if we write like this;</p>\n\n<pre><code>#block1 a{text-decoration:none;} \n</code></pre>\n\n<p>browsers find first, all a tag then find block1 and performance drops a bit. Is it true?</p>\n"},{"tags":["java","performance","activemq"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":21,"score":1,"question_id":13207660,"title":"Activemq messaging rate","body":"<p>I will be using activemq for a project and wanted to be able to stress test the application. In which way can I send x number of messages per second so that I can account for a normal period of messaging and see how it handles an abnormal rate of messaging also?</p>\n\n<p>Thanks</p>\n"},{"tags":["mysql","performance","wordpress"],"answer_count":5,"favorite_count":6,"up_vote_count":7,"down_vote_count":0,"view_count":5523,"score":7,"question_id":3753504,"title":"Mysqltuner suggestions and changes to my.cnf","body":"<p>Had this question on Serverfault for a few days with no luck.</p>\n\n<p>I've run mysqltuner.pl on a VPS and have a bunch of questions as to the suggestions on variables to change. I'm sure these are general questions with complex answers.</p>\n\n<p>I'm not knowledgable enough to write queries and test them against the server, but am just trying to get a bit more performance out of the server that runs five WordPress sites with >200,000 page views/month.</p>\n\n<p>I've optimized the database via phpmyadmin (and do that regularly), but the tuner still says there are fragmented tables. And because this is WordPress, I can't change queries in core code.</p>\n\n<p>But how much should I increase the variables like query_cache_size and  innodb_buffer_pool_size? What about the other innodb variables?</p>\n\n<p>Some of the variables suggested don't exist in my.cnf, like table_cache, and are flagged in tuner report, etc. Can I add them to my.cnf?</p>\n\n<blockquote>\n  <p><em>(And why is this block duplicated in my.cnf? Can I delete the duplicate?)</em></p>\n\n<pre><code>set-variable = innodb_buffer_pool_size=2M\nset-variable = innodb_additional_mem_pool_size=500K\nset-variable = innodb_log_buffer_size=500K\nset-variable = innodb_thread_concurrency=2\n</code></pre>\n</blockquote>\n\n<p>Below is the my.cnf and the output of mysqltuner:</p>\n\n<p><strong>Contents of my.cnf:</strong></p>\n\n<pre><code>query-cache-type = 1\nquery-cache-size = 8M\n\nset-variable=local-infile=0\ndatadir=/var/lib/mysql\nsocket=/var/lib/mysql/mysql.sock\nuser=mysql\n\nold_passwords=1\n\nskip-bdb\n\nset-variable = innodb_buffer_pool_size=2M\nset-variable = innodb_additional_mem_pool_size=500K\nset-variable = innodb_log_buffer_size=500K\nset-variable = innodb_thread_concurrency=2\n\n[mysqld_safe]\nlog-error=/var/log/mysqld.log\npid-file=/var/run/mysqld/mysqld.pid\nskip-bdb\n\nset-variable = innodb_buffer_pool_size=2M\nset-variable = innodb_additional_mem_pool_size=500K\nset-variable = innodb_log_buffer_size=500K\nset-variable = innodb_thread_concurrency=2\n</code></pre>\n\n<p><strong>Output of mysqltuner:</strong></p>\n\n<pre><code>------- General Statistics --------------------------------------------------\n[--] Skipped version check for MySQLTuner script\n[OK] Currently running supported MySQL version 5.0.45\n[!!] Switch to 64-bit OS - MySQL cannot currently use all of your RAM\n\n-------- Storage Engine Statistics -------------------------------------------\n[--] Status: -Archive -BDB -Federated +InnoDB -ISAM -NDBCluster \n[--] Data in MyISAM tables: 133M (Tables: 637)\n[--] Data in InnoDB tables: 10M (Tables: 344)\n[--] Data in MEMORY tables: 126K (Tables: 2)\n[!!] Total fragmented tables: 69\n\n-------- Security Recommendations  -------------------------------------------\n[OK] All database users have passwords assigned\n\n-------- Performance Metrics -------------------------------------------------\n[--] Up for: 1d 6h 24m 13s (2M q [22.135 qps], 116K conn, TX: 4B, RX: 530M)\n[--] Reads / Writes: 97% / 3%\n[--] Total buffers: 35.0M global + 2.7M per thread (100 max threads)\n[OK] Maximum possible memory usage: 303.7M (8% of installed RAM)\n[OK] Slow queries: 0% (4/2M)\n[OK] Highest usage of available connections: 53% (53/100)\n[OK] Key buffer size / total MyISAM indexes: 8.0M/46.1M\n[OK] Key buffer hit rate: 99.6% (749M cached / 2M reads)\n[OK] Query cache efficiency: 32.2% (685K cached / 2M selects)\n[!!] Query cache prunes per day: 948863\n[OK] Sorts requiring temporary tables: 0% (0 temp sorts / 660K sorts)\n[!!] Temporary tables created on disk: 46% (400K on disk / 869K total)\n[!!] Thread cache is disabled\n[!!] Table cache hit rate: 0% (64 open / 24K opened)\n[OK] Open file limit used: 10% (109/1K)\n[OK] Table locks acquired immediately: 99% (2M immediate / 2M locks)\n[!!] InnoDB data size / buffer pool: 10.6M/2.0M\n\n-------- Recommendations -----------------------------------------------------\nGeneral recommendations:\n    Run OPTIMIZE TABLE to defragment tables for better performance\n    Enable the slow query log to troubleshoot bad queries\n    When making adjustments, make tmp_table_size/max_heap_table_size equal\n    Reduce your SELECT DISTINCT queries without LIMIT clauses\n    Set thread_cache_size to 4 as a starting value\n    Increase table_cache gradually to avoid file descriptor limits\nVariables to adjust:\n    query_cache_size (&gt; 8M)\n    tmp_table_size (&gt; 32M)\n    max_heap_table_size (&gt; 16M)\n    thread_cache_size (start at 4)\n    table_cache (&gt; 64)\n    innodb_buffer_pool_size (&gt;= 10M)\n</code></pre>\n"},{"tags":["performance","oracle","business-intelligence","partitioning","tuning"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":859,"score":0,"question_id":3193715,"title":"Optimize SELECT from Partitioned Fact Table in Oracle 10","body":"<p>I have a fact table containing 8 Million rows with 1 Million rows increase per month. The table already contains indexes on it. The table is used by IBM Cognos environment to generate reports. Currently I am looking for way to optimize the table SELECT statements. </p>\n\n<p>As first try, I partitioned the table (each partition has equal distribution of rows) and the query is suitable for the partitions, but for some reason, I am getting equal or even worse performance, which is weird. Only one partition is affected per query. Can someone explain how to optimize this ?</p>\n\n<p>Second idea I came to is to implement the fact table as Index organized table, but it will have to have all the columns as primary key. Is this alright and will there be performance gain ?</p>\n\n<p>Third idea is to implement the fact table in a way that will contain all the columns that are joined from the star schema. Will there be performance gain ?</p>\n\n<p>EDIT: Here is the execution plan:\n<a href=\"http://i50.tinypic.com/11qtzr6.jpg\" rel=\"nofollow\">http://i50.tinypic.com/11qtzr6.jpg</a></p>\n\n<p>I have managed to reduce the access time to fact table FT_COSTS by 3 times (cost was 42000, now is 14900) AFTER I created indexes containing the partitioning criteria, but before that I was getting worse results than in unpartitioned table. I used this link to solve my partitioning problem <a href=\"http://stackoverflow.com/questions/2535908/range-partition-skip-check\">http://stackoverflow.com/questions/2535908/range-partition-skip-check</a></p>\n\n<p>From what I see now, the main bottleneck is the GROUP BY which raises the cost from 34000 to 85 000 , which is more than doubling . Does anyone have idea about a workaround on this ?</p>\n"},{"tags":["performance","python-3.x","double-underscore"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":36,"score":1,"question_id":13195510,"title":"the cost of rewrite __setattr__ () was too high","body":"<p>I want to save the time and mark the object as modified, so I wrote a class and override its <code>__setattr__</code> function. </p>\n\n<pre><code>import time\n\nclass CacheObject(object):\n    __slots__ = ('modified', 'lastAccess')\n    def __init__(self):\n        object.__setattr__(self,'modified',False)\n        object.__setattr__(self,'lastAccess',time.time())\n\n    def setModified(self):\n        object.__setattr__(self,'modified',True)\n        object.__setattr__(self,'lastAccess',time.time())\n\n    def resetTime(self):\n        object.__setattr__(self,'lastAccess',time.time())\n\n    def __setattr__(self,name,value):\n        if (not hasattr(self,name)) or object.__getattribute__(self,name)!=value: \n            object.__setattr__(self,name,value)\n            self.setModified()\n\nclass example(CacheObject):\n    __slots__ = ('abc',)\n    def __init__(self,i):\n        self.abc = i\n        super(example,self).__init__()\n\nt = time.time()\nf = example(0)\nfor i in range(100000):\n    f.abc = i\n\nprint(time.time()-t)\n</code></pre>\n\n<p>I measured the process time, and it took 2 seconds. When I commented out overridden function, the process time was 0.1 second, I know the overridden function would be slower but almost 20 times the gap is too much. I think I must get something wrong.</p>\n\n<p>take the suggestion from cfi</p>\n\n<p>1.elimate the if condition</p>\n\n<pre><code>    def __setattr__(self,name,value):\n#        if (not hasattr(self,name)) or object.__getattribute__(self,name)!=value: \n            object.__setattr__(self,name,value)\n            self.setModified()\n</code></pre>\n\n<p>the running time down to 1.9, a little improve but mark the object modified if it's not changed would cost more in other process, so not an option.</p>\n\n<p>2.change self.func to classname.func(self)</p>\n\n<pre><code>def __setattr__(self,name,value):\n    if (not hasattr(self,name)) or object.__getattribute__(self,name)!=value: \n        object.__setattr__(self,name,value)\n        CacheObject.setModified(self)\n</code></pre>\n\n<p>running time is 2.0 .so nothing really changed</p>\n\n<p>3)extract setmodified function </p>\n\n<pre><code>def __setattr__(self,name,value):\n    if (not hasattr(self,name)) or object.__getattribute__(self,name)!=value: \n        object.__setattr__(self,name,value)\n        object.__setattr__(self,'modified',True)\n        object.__setattr__(self,'lastAccess',time.time())\n</code></pre>\n\n<p>running time down to 1.2!!That's great ,it do save almost 50% time,though the cost is  still  high.</p>\n"},{"tags":["wpf","performance","datagrid"],"answer_count":5,"favorite_count":15,"up_vote_count":21,"down_vote_count":0,"view_count":14893,"score":21,"question_id":697701,"title":"WPF Datagrid Performance","body":"<p>I am working with the WPF Toolkit data grid and it is scrolling extremely slow at the moment.  The grid has 84 columns and 805 rows. (Including 3 fixed columns and the header is fixed.)  Scrolling both horizontally and vertically is extremely slow.  Virtualization is turned on and I have enabled column virtualization and row virtualization explicitly in the xaml.  Is there anything to watch out for that can really effect performance, such as binding methods, or what xaml is in each celltemplate?</p>\n\n<p>One thing to note is I am dynamically adding the columns on creation of the datagrid.  Could that be effecting anything? (I also dynamically create the celltemplate at the same time so that my bindings are set right.)</p>\n\n<p>Below is the code from the template for most of the cells that get generated.  Basically for the columns I need to dynamically add (which is most of them), I loop through my list and add the columns using the AddColumn method, plus I dynamically build the template so that the binding statements properly index the right item in the collection for that column.  The template isn't too complex, just two TextBlocks, but I do bind four different properties on each.  It seems like I was able to squeeze out a little bit more performance by changes the bindings to OneWay:</p>\n\n<pre><code> private void AddColumn(string s, int index)\n    {\n        DataGridTemplateColumn column = new DataGridTemplateColumn();\n        column.Header = s;\n        //Set template for inner cell's two rectangles\n        column.CellTemplate = CreateFactViewModelTemplate(index);\n        //Set Style for header, ie rotate 90 degrees\n        column.HeaderStyle = (Style)dgMatrix.Resources[\"HeaderRotateStyle\"];\n        column.Width = DataGridLength.Auto;\n        dgMatrix.Columns.Add(column);\n    }\n\n\n    //this method builds the template for each column in order to properly bind the rectangles to their color\n    private static DataTemplate CreateFactViewModelTemplate(int index)\n    {\n        string xamlTemplateFormat =\n            @\"&lt;DataTemplate xmlns=\"\"http://schemas.microsoft.com/winfx/2006/xaml/presentation\"\"\n            xmlns:x=\"\"http://schemas.microsoft.com/winfx/2006/xaml\"\"&gt;\n            &lt;Grid&gt;\n            &lt;Grid.ColumnDefinitions&gt;\n                &lt;ColumnDefinition /&gt;\n                &lt;ColumnDefinition /&gt;\n            &lt;/Grid.ColumnDefinitions&gt;\n            &lt;TextBlock Grid.Column=\"\"0\"\" MinHeight=\"\"10\"\" MinWidth=\"\"10\"\" HorizontalAlignment=\"\"Stretch\"\" Padding=\"\"3 1 3 1\"\" TextAlignment=\"\"Center\"\" Foreground=\"\"{Binding Path=FactViewModels[~Index~].LeftForeColor,Mode=OneWay}\"\" Background=\"\"{Binding Path=FactViewModels[~Index~].LeftColor,Mode=OneWay}\"\" Text=\"\"{Binding Path=FactViewModels[~Index~].LeftScore,Mode=OneWay}\"\" /&gt;\n            &lt;TextBlock Grid.Column=\"\"1\"\" MinHeight=\"\"10\"\" MinWidth=\"\"10\"\" HorizontalAlignment=\"\"Stretch\"\" Padding=\"\"3 1 3 1\"\" TextAlignment=\"\"Center\"\" Foreground=\"\"{Binding Path=FactViewModels[~Index~].RightForeColor,Mode=OneWay}\"\" Background=\"\"{Binding Path=FactViewModels[~Index~].RightColor,Mode=OneWay}\"\" Text=\"\"{Binding Path=FactViewModels[~Index~].RightScore,Mode=OneWay}\"\" /&gt;\n            &lt;/Grid&gt;\n            &lt;/DataTemplate&gt;\";\n\n\n\n\n        string xamlTemplate = xamlTemplateFormat.Replace(\"~Index~\", index.ToString());\n\n        return (DataTemplate)XamlReader.Parse(xamlTemplate);\n    }\n</code></pre>\n"},{"tags":["c#",".net","wpf","performance"],"answer_count":3,"favorite_count":5,"up_vote_count":10,"down_vote_count":0,"view_count":3683,"score":10,"question_id":3022921,"title":"WPF DataGrid performance concerns","body":"<p>I am testing WPF DataGrid in hopes of replacing some winforms controls, and so far have been very pleased with the development process. Performance seems to be my biggest concern right now. My development workstation has just about the best cpu on the market running windows 7, with 6 gigs of DDR3 memory. The windows control i am replacing is considerably more responsive which is worrisome.</p>\n\n<p>My test is a basic implementation of DataGrid bound to ObservableCollection which gets updated once per second. It also includes Details area which is expandable to reveal more info about each row. Details area is just a stackpanel with a ItemsControl wrapping TextBlock (which repeats 6 times)</p>\n\n<p>My complaint is that if i try to scroll this collection, it is often jerky with lag, and if i try to expand each row as they come in, about 15% of clicks do not trigger buttons click event (DataGridTmplateColumn > CellTemplate > DataTemplate > Button)\nAlso scrolling is more jittery if some rows detail is expanded (with scroll bar that resizes it self as it goes up/down)</p>\n\n<p>what are some things to look for / optimize / avoid ? </p>\n\n<p><strong>update</strong></p>\n\n<p>here are some points i found helpful so far: </p>\n\n<ul>\n<li><p>rely as little as possible on dynamic layout. as each component contains many subcomponents and in dynamic layout world, all of them have to call Measure and Layout methods which can be cpu intensive. so instead of column width Auto (or no width specified), use fixed widths</p></li>\n<li><p>install <a href=\"http://msdn.microsoft.com/en-us/library/aa969767.aspx#installing_the_wpf_performance_suite\" rel=\"nofollow\">WPF Performance Suite</a> and get in touch with how your app is rendered. trully awesome app</p></li>\n<li><p>as Andrew pointed out ListView is a great alternative, for when you don't need advanced DataGrid features, such as updating the data back, or possibly Details View (which i am still hoping to reproduce)</p></li>\n<li><p>also <a href=\"http://stackoverflow.com/questions/2460557/itemscontrol-itemssource-mvvm-performance\">SuspendableObservableCollection</a> is ideal for when you're adding multiple items in very short period of time (i.e. 100 items in 0.01 second etc) </p></li>\n<li><p>after lots of testing, i found that BindingList is much faster than ObservableCollection . I posted performance profiler snapshots <a href=\"http://stackoverflow.com/questions/3305383/wpf-whats-the-most-efficient-fast-way-of-adding-items-to-a-listview\">here</a> of the same load handled by BindingList vs Observable collection, and the former takes less than half cpu time. (keep in mind that this is not just collection performance, but when paired with a ListView)</p></li>\n</ul>\n\n<p>my search still continues as something appears to be leaking memory in my app and slows it down to a halt after couple hours.</p>\n"},{"tags":["javascript","asp.net","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":43,"score":0,"question_id":13205965,"title":"Is putting the javascript before the closing body tag okay on an asp.net website?","body":"<p>I pretty much stated what I have to ask.  But is taking all of your external .js files and putting them <strong>before</strong> the closing body tag on your master pages okay on an asp.net website?</p>\n\n<p>I'm just going off of what yslow and google speed have been showing.  I can't combine these javascripts, so I'm trying to load them \"<strong>after page load</strong>\", but doing so makes them useless; <strong>some of my jquery things don't work</strong>.</p>\n\n<p>I moved my .js files above the opening body tag, and they work.  What am I doing wrong?  And what could I do to load my .js files <strong>after page load</strong>?  Thanks for any advice anybody can offer!</p>\n"},{"tags":["c++","performance","visual-c++","c++11"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":125,"score":0,"question_id":13205885,"title":"Performance wise, is it faster to use 'nullptr' or just '0'?","body":"<p>For example:</p>\n\n<pre><code>object* pObject = nullptr;\n</code></pre>\n\n<p>OR:</p>\n\n<pre><code>object* pObject = 0;\n</code></pre>\n\n<p>Again, which one is better performance wise?</p>\n"},{"tags":["asp.net-mvc","performance","nhibernate","guid"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":26,"score":1,"question_id":13202558,"title":"nhibernate guid performance","body":"<p>What would be the overhead of using GUID's instead of an integer identity in nHibernate for table primary keys? </p>\n\n<p>The main reason for use would be the obfuscation of table ids on user facing data.</p>\n"},{"tags":["android","performance","listview","image-loading"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":235,"score":0,"question_id":12099383,"title":"ListView VERY slow when images are loaded (using Universal Image Loader)","body":"<p>My listView runs very smooth with just text - but as soon as I try to load in thumbnails (even from cache), it runs SOO choppy.</p>\n\n<p>I'm using the <a href=\"https://github.com/nostra13/Android-Universal-Image-Loader\" rel=\"nofollow\">Universal Image Loader script</a></p>\n\n<p>The code in my ArticleEntryAdapter within <code>public View getView(...)</code> method:</p>\n\n<pre><code>/**\n     * PHOTOS\n     */\n    ImageLoaderConfiguration config = new ImageLoaderConfiguration.Builder(this.mContext)\n        .enableLogging()\n        .memoryCacheSize(41943040)\n        .discCacheSize(104857600)\n        .threadPoolSize(10)\n        .build();\n\n    DisplayImageOptions imgDisplayOptions = new DisplayImageOptions.Builder()\n        //.showStubImage(R.drawable.stub_image)\n        .cacheInMemory() \n        .cacheOnDisc() \n        //.imageScaleType(ImageScaleType.EXACT) \n        .build();\n\n    ImageLoader imageLoader = ImageLoader.getInstance();\n    imageLoader.init(config);\n\n    //loads image (or hides image area)\n    imageLoader.displayImage(\"\", viewHolder.thumbView); //clears previous one\n    if(article.photopath != null &amp;&amp; article.photopath.length() != 0)\n    {\n        imageLoader.displayImage(\n            \"http://img.mysite.com/processes/resize_android.php?image=\" + article.photopath + \"&amp;size=150&amp;quality=80\",\n            viewHolder.thumbView,\n            imgDisplayOptions\n            );\n        viewHolder.thumbView.setVisibility(View.VISIBLE);\n    }\n    else\n    {\n        viewHolder.thumbView.setVisibility(View.GONE); //hide image\n        viewHolder.thumbView.invalidate(); //should call after changing to GONE\n    }\n</code></pre>\n\n<p>Logcat shows that it's loading the images from cache (I think):</p>\n\n<pre><code>ImageLoader    Load image from memory cache [http://img.mysite.com/processes/...\n</code></pre>\n\n<p>I'm testing it on my Samsung Galaxy Nexus and running Android 4.0.4 (though my minSdkVersion=\"8\")</p>\n"},{"tags":["performance","query","google-app-engine","memcached","gae-datastore"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":21,"score":-1,"question_id":13205472,"title":"Most Efficient Way for Users to Search for all of a Database Model","body":"<p>I have two database models in my webapp and one has around 10,000 entries and the other has close to double that. I have it set up right now so that all of these entries are kept in memcache.  I also have it set up so that the user can search by certain constraints and I just filter the cache by those constraints with some if statements.  Part of this is so that I don't have to pay for the search quota with the Google Search API.</p>\n\n<p>I wanted to see if using memcache here is the best option or if I should simply do a DB query which will quickly run up my read quota or if there is any other good option.  One of my problems here is that as my database gets larger the searches are getting slower.</p>\n\n<p>Here is some example code where I am searching by location and then filtering by a sport category:</p>\n\n<pre><code>        #this is the cache I am using\n        groups = group_cache()\n        search_location = self.request.get(\"search_location\")\n        search_postsport = self.request.get(\"search_postsport\")\n        GPSlocation = None\n        error=None\n        groups_to_render = None\n\n        if search_location:\n            g = geocoders.Google()\n            searched_location = None\n            #to catch an error where there is no corresponding location\n            try:\n                #to catch an error where there are multiple returned locations\n                try:\n                    place, (lat, lng) = g.geocode(search_location)\n                    place, searched_location = g.geocode(search_location)\n                except ValueError:\n                    geocodespot = g.geocode(search_location, exactly_one=False)\n                    place, (lat, lng) = geocodespot[0]\n                    place, searched_location = geocodespot[0]\n                GPSlocation = \"(\"+str(lat)+\", \"+str(lng)+\")\"\n\n                groups = group_cache()\n                return_groups = []                   \n                for group in groups:\n                    distance.distance = distance.GreatCircleDistance\n                    location_distance = distance.distance(searched_location, point.Point(group.lat, group.lng)).miles\n\n                    if location_distance &lt; 300:\n                        return_groups.append(group)\n                groups = return_groups\n                search=True\n                groups_to_render = sorted(groups, key=attrgetter('recordCount'), reverse=True)\n                groups_to_render = groups_to_render[:10]\n</code></pre>\n"},{"tags":["python","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":3,"down_vote_count":1,"view_count":31,"score":2,"question_id":13205026,"title":"Determining Pythagorean Triples using c value","body":"<p>Hello stackoverflow community, I have a quick question to ask of you. Below is my written code to determine if based on the hypotenuse, if a pythagorean triple is possible, and what are the lengths. I was wondering if there a more efficient way of doing so. Thanks! </p>\n\n<pre><code>#Victor C\n#Determines if a right triangle with user inputted hypotenuse is capable of being a Pythagorean Triple\n\nimport math\ntriplecheck = True\nwhile True:\n    hypotenuse = int(input(\"Please enter the hypotentuse:\")) #Smallest hypotenuse which results in a pythagorean triple is 5.\n    if hypotenuse &gt; 4:\n        break\nc = csqr = hypotenuse**2 #sets two variables to be equivalent to c^^2. c is to be modified to shorten factoring, csqr as a reference to set a value\nif c % 2 != 0:   #even, odd check of value c, to create an integer when dividing by half.\n    c = (c+1)//2\nelse:\n    c = c//2\nfor b in range(1,c+1):  #let b^^2 = b || b is equal to each iteration of factors of c^^2, a is set to be remainder of c^^2 minus length b.\n    a = csqr-b\n    if (math.sqrt(a))%1 == 0 and (math.sqrt(b))%1 == 0: #if squareroots of a and b are both equal to 0, they are integers, therefore fulfilling conditions\n       tripleprompt = \"You have a Pythagorean Triple, with the lengths of \"+str(int(math.sqrt(b)))+\", \"+str(int(math.sqrt(a)))+\" and \"+str(hypotenuse)\n       print(tripleprompt)\n       triplecheck = False\nif triplecheck == True:\n    print(\"Sorry, your hypotenuse does not make a Pythagorean Triple.\")\n</code></pre>\n"},{"tags":["php","json","performance","api","slow-load"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":340,"score":0,"question_id":12130091,"title":"I am using json api for twitch and own3d tv and my page is veeery slow","body":"<p>As i said in the title i am using through php the json api for twitch tv and own3d tv to get the information of the stream i want.</p>\n\n<p>Ths problem is that the page is not loading fast, as a matter of fact some times the php server stops because of 30 or more secs of function.\nThe error : Fatal error: Maximum execution time of 30 seconds exceeded in</p>\n\n<p>I am using an online indicator : </p>\n\n<pre><code> function status($stream_id, $type){\nif($type == 't'){\n\n    $chan = \"http://api.justin.tv/api/stream/list.json?channel=\" . $stream_id;\n    $json = file_get_contents($chan);\n    $exist = strpos($json, $stream_id);\n    if($exist) {\n        return true;\n    }else{\n        return false;   \n    }\n\n\n}else if($type == 'o'){\n\n    $url = 'http://api.own3d.tv/liveCheck.php?live_id=' . $stream_id;\n    $xml = simplexml_load_file($url);\n\n    $isLive=$xml-&gt;liveEvent-&gt;isLive;\n\n    if ($isLive == \"true\") {\n        return true;\n    }else{\n        return false;   \n    }\n\n}\n}\n</code></pre>\n\n<p>and i am using a function that get some info from the stream :</p>\n\n<pre><code>function api_stream_data($stream_id, $type){\n$stream_id = sanitize($stream_id);\n$type = sanitize($type);\n\nif($type == 't'){\n\n    $streamData = json_decode(file_get_contents(\"http://api.justin.tv/api/stream/list.json?channel=$stream_id\"),true);\n\n    $data = array(\n        'image'=&gt;$streamData[0]['channel']['image_url_medium'],\n        'title'=&gt;$streamData[0]['title'],\n        'limage'=&gt;$streamData[0]['channel']['screen_cap_url_huge'],\n        'game'=&gt;$streamData[0]['meta_game']\n    );\n}else if($type == 'o'){\n    $streamData = json_decode(file_get_contents(\"http://api.own3d.tv/rest/live/list.json?liveid=$stream_id\"),true);\n    $data = array(\n        'image'=&gt;$streamData[0]['thumbnail_small'],\n        'title'=&gt;$streamData[0]['live_name'],\n        'limage'=&gt;$streamData[0]['thumbnail_large'],\n        'game'=&gt;$streamData[0]['game_name']\n    );\n}\n\nreturn $data;\n\n}\n</code></pre>\n\n<p>All functions works perfectly but the problem is the time they get to excecute....</p>\n\n<p>Is there any possible way to do that faster??\nI have seen some other site examples that are loading very fast like <a href=\"http://www.solomid.net\" rel=\"nofollow\">www.solomid.net</a> and <a href=\"http://www.clgaming.net\" rel=\"nofollow\">www.clgaming.net</a> .</p>\n\n<p>Thanks in advance for any help!</p>\n"},{"tags":["performance","ios5","ios6","abaddressbook","slowness"],"answer_count":0,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":110,"score":0,"question_id":12754683,"title":"ABAddressBook poor performance on since iOS6","body":"<p>i have a App that imports round about 12000 numbers in 3 contacts and 1 group. Since iOS6 this process needs 5 times longer than before with iOS 5.1. In the simulator it isn't that slow than on a device.</p>\n\n<p>I tried some profiling and i found out that the main time is used by ABAddressBookSave with 28% of execusion time. I also got the feeling that everything with Addressbook is slower than befor. On iOS 5 its about 3% of the total execusion time. Memory and CPU are ok.</p>\n\n<p>Has anyone similar problems, did anyone found out why this happens or a solution to fix it?</p>\n\n<p><strong>Here is a stackTrace why it is so slow in iOS6</strong> </p>\n\n<p>Running Time    Self        Symbol Name\n3212.0ms   41.6%    0,0       ABAddressBookSave</p>\n\n<p>3212.0ms   41.6%    0,0        ABCSave</p>\n\n<p>3212.0ms   41.6%    0,0         ABCAddressBookSaveWithConflictPolicy</p>\n\n<p>3198.0ms   41.4%    0,0          CPRecordStoreSaveWithPreAndPostCallbacksAndTransactionType</p>\n\n<p>3134.0ms   40.6%    0,0           CFDictionaryApplyFunction</p>\n\n<p>3134.0ms   40.6%    0,0            CFBasicHashApply</p>\n\n<p>3134.0ms   40.6%    0,0             __CFDictionaryApplyFunction_block_invoke_0</p>\n\n<p>3134.0ms   40.6%    0,0              CPRecordStoreUpdateRecord</p>\n\n<p>2971.0ms   38.5%    0,0               _didSave</p>\n\n<p>2971.0ms   38.5%    0,0                ABCDContextUpdateSearchIndexForPersonAndProperties</p>\n\n<p>2773.0ms   35.9%    0,0                 CPSqliteStatementPerform</p>\n\n<p>2773.0ms   35.9%    0,0                  sqlite3_step</p>\n\n<p>2773.0ms   35.9%    0,0                   sqlite3VdbeExec</p>\n\n<p>2772.0ms   35.9%    0,0                    fts3UpdateMethod</p>\n\n<p>2765.0ms   35.8%    0,0                     fts3PendingTermsAdd</p>\n\n<p>2734.0ms   35.4%    0,0                      ABCFFTSTokenizerOpen</p>\n\n<p>2734.0ms   35.4%    0,0                       ABTokenListPopulateFromString</p>\n\n<p>2631.0ms   34.1%    1,0                        CFStringGetBytes</p>\n\n<p>2630.0ms   34.1%    2624,0                          __CFStringEncodeByteStream</p>\n\n<p>6.0ms    0.0%   0,0                          CFStringEncodingIsValidEncoding</p>\n\n<p><strong>And here the same method in iOS 5</strong></p>\n\n<p>Running Time    Self        Symbol Name</p>\n\n<p>245.0ms   12.9% 0,0     ABAddressBookSave</p>\n\n<p>245.0ms   12.9% 0,0      ABCSave</p>\n\n<p>245.0ms   12.9% 0,0       ABCAddressBookSaveWithConflictPolicy</p>\n\n<p>234.0ms   12.3% 0,0        CPRecordStoreSaveWithPreAndPostCallbacksAndTransactionType</p>\n\n<p>167.0ms    8.8% 0,0         CFDictionaryApplyFunction</p>\n\n<p>167.0ms    8.8% 0,0          CFBasicHashApply</p>\n\n<p>167.0ms    8.8% 0,0           __CFDictionaryApplyFunction_block_invoke_0</p>\n\n<p>167.0ms    8.8% 0,0            CPRecordStoreUpdateRecord</p>\n\n<p>162.0ms    8.5% 0,0             CFDictionaryApplyFunction</p>\n\n<p>162.0ms    8.5% 0,0              CFBasicHashApply</p>\n\n<p>162.0ms    8.5% 0,0               __CFDictionaryApplyFunction_block_invoke_0</p>\n\n<p>162.0ms    8.5% 0,0                CPRecordStoreSaveProperty</p>\n\n<p>158.0ms    8.3% 0,0                 ABCMultiValueSave</p>\n\n<p>158.0ms    8.3% 1,0                  ABCDBContextSaveMultiValue</p>\n\n<p>143.0ms    7.5% 0,0                   CPSqliteConnectionAddRecord</p>\n\n<p>143.0ms    7.5% 1,0                    CPSqliteConnectionAddRecordWithRowid</p>\n\n<p>85.0ms    4.5%  0,0                     CPSqliteStatementPerform</p>\n\n<p>16.0ms    0.8%  2,0                     CFRelease</p>\n"},{"tags":["performance","caching","computer-science"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":19,"score":0,"question_id":13204285,"title":"How to determine cache misses in this example of code","body":"<p>I wanted help on determining misses in the cache, and how/what is the process of determining it? <br>Thank you so much for your time and help!</p>\n\n<p><img src=\"http://i.stack.imgur.com/tXDN1.png\" alt=\"enter image description here\"></p>\n\n<p>DM: Direct Mapped<br>\nC:Cache Size<br>\nB:Block Size (Bytes)<br>\nS:Number of Sets<br>\nE:Number of Lines per Set</p>\n"},{"tags":["performance","oracle","exadata"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":22,"score":0,"question_id":13202412,"title":"Can I run exadata with OPTIMIZER_FEATURES_ENABLE set to 9 or 10?","body":"<p>Does Exadata support OPTIMIZER_FEATURES_ENABLE with pre-11 values?</p>\n\n<p>Or does the unique nature of Exadata prevent that from making sense?</p>\n"},{"tags":["mysql","performance","joomla","webserver"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":44,"score":1,"question_id":13137792,"title":"Joomla 300 concurrent users","body":"<p>I've an issue on a website made in Joomla 2.5 with Ja teline IV template\nthat has 300 concurrent user, \nit is a soccer magazine, so the article are updated often, \nalso minute by minute during the match. </p>\n\n<p>I've a server of 16gb ram and quad core processor, but the website freeze when 300 users are accessing to the website.</p>\n\n<p>I've done all the frontend optimization, but, I the last optimization could be enable caching. \nMy issues are: \n - caching enabled also for logged in users \n - caching timing, if I have that type of article, I can enable cache\n   expiring to 1 minute? It is also a good option? Could optimize the performance.</p>\n\n<p>Can you suggest me what to do? Other possible optimization?</p>\n"},{"tags":["python","performance","profiling","profile","cprofile"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":26,"score":0,"question_id":13199657,"title":"Python Profiling - Rollup function calls that are outside my code","body":"<p>I am trying to profile our django unittests (if the tests are faster, we'll run 'em more often). I've ran it through python's built in cProfile profiler, producing a pstats file.</p>\n\n<p>However the signal to noise ration is bad. There are too many functions listed. Lots and lots of django internal functions are called when I make one database query. This makes it hard to see what's going on.</p>\n\n<p>Is there anyway I can \"roll up\" all function calls that are outside a certain directory?</p>\n\n<p>e.g. if I call a python function outside my directory, and it then calls 5 other functions (all outside my directory), then it should roll all those up, so it looks like there was only one function call, and it should show the cumulative time for the whole thing.</p>\n\n<p>This, obviously, is bad if you want to profile (say) Django, but I don't want to do that.</p>\n\n<p>I looked at the pstats.Stats object, but can't see an obvious way to modify this data.</p>\n"},{"tags":["c++","performance","gcc"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":212,"score":4,"question_id":13200858,"title":"C++: Why does this speed my code up?","body":"<p>I have the following function </p>\n\n<pre><code>double single_channel_add(int patch_top_left_row, int patch_top_left_col, \n        int image_hash_key, \n        Mat* preloaded_images,\n        int* random_values){\n\n    int first_pixel_row = patch_top_left_row + random_values[0];\n    int first_pixel_col = patch_top_left_col + random_values[1];\n    int second_pixel_row = patch_top_left_row + random_values[2];\n    int second_pixel_col = patch_top_left_col + random_values[3];\n\n    int channel = random_values[4];\n\n    Vec3b* first_pixel_bgr = preloaded_images[image_hash_key].ptr&lt;Vec3b&gt;(first_pixel_row, first_pixel_col);\n    Vec3b* second_pixel_bgr = preloaded_images[image_hash_key].ptr&lt;Vec3b&gt;(second_pixel_row, second_pixel_col);\n\n    return (*first_pixel_bgr)[channel] + (*second_pixel_bgr)[channel];\n}\n</code></pre>\n\n<p>Which is called about one and a half million times with different values for <code>patch_top_left_row</code> and <code>patch_top_left_col</code>. This takes about 2 seconds to run, now when I change the calculation of first_pixel_row etc to not use the arguments but hard coded numbers instead (shown below), the thing runs sub second and I don't know why. Is the compiler doing something smart here ( I am using gcc cross compiler)? </p>\n\n<pre><code>double single_channel_add(int patch_top_left_row, int patch_top_left_col, \n        int image_hash_key, \n        Mat* preloaded_images,\n        int* random_values){\n\n        int first_pixel_row = 5 + random_values[0];\n        int first_pixel_col = 6 + random_values[1];\n        int second_pixel_row = 8 + random_values[2];\n        int second_pixel_col = 10 + random_values[3];\n            int channel = random_values[4];\n\n    Vec3b* first_pixel_bgr = preloaded_images[image_hash_key].ptr&lt;Vec3b&gt;(first_pixel_row, first_pixel_col);\n    Vec3b* second_pixel_bgr = preloaded_images[image_hash_key].ptr&lt;Vec3b&gt;(second_pixel_row, second_pixel_col);\n\n    return (*first_pixel_bgr)[channel] + (*second_pixel_bgr)[channel];\n}\n</code></pre>\n\n<p>EDIT:</p>\n\n<p>I have pasted the assembly from the two versions of the function\nusing arguments: <a href=\"http://pastebin.com/tpCi8c0F\" rel=\"nofollow\">http://pastebin.com/tpCi8c0F</a>\nusing constants: <a href=\"http://pastebin.com/bV0d7QH7\" rel=\"nofollow\">http://pastebin.com/bV0d7QH7</a></p>\n\n<p>EDIT:</p>\n\n<p>After compiling with -O3 I get the following clock ticks and speeds:</p>\n\n<p>using arguments: 1990000 ticks and 1.99seconds \nusing constants: 330000 ticks and 0.33seconds </p>\n\n<p>EDIT: \nusing argumenst with -03 compilation: <a href=\"http://pastebin.com/fW2HCnHc\" rel=\"nofollow\">http://pastebin.com/fW2HCnHc</a>\nusing constant with -03 compilation: <a href=\"http://pastebin.com/FHs68Agi\" rel=\"nofollow\">http://pastebin.com/FHs68Agi</a></p>\n"},{"tags":["performance","task-parallel-library","sql-azure","async-await","federation"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":41,"score":1,"question_id":13163850,"title":"query many sql azure atomic units as fast as possible","body":"<p>I have a SQL Federation with 97 members, i.e. physical shards. Each member has 1-16 virtual shards, i.e. atomic units. This data tier powers a search lookup web service (on an Azure web role web server), which requires all atomic units to respond before it knows enough to answer the user response. </p>\n\n<p>Given the search parameters, the web server is able to determine the atomic unit IDs that it needs to query, but not their associated federation members (I am using USE Federation for this translation). The goal is to have the web server query all atomic units (wherever they are) as quickly as possible. </p>\n\n<p>Currently, the best solution I have for this works as follows:</p>\n\n<ol>\n<li>Generate list of needed atomic units.</li>\n<li>Generate USE Federation and SQL statement for each atomic unit. Currently, the best performance I have found specifies FILTERING = OFF in the USE Federation statement, while manually specifying the predicate for the atomic unit in the SQL statement (rather than relying on FILTERING = ON to add these predicates for me).</li>\n<li>For each atomic unit, open a SqlConnection to the Federation Root, execute the USE Federation statement, and then the SQL query, both asynchronously. I use the TPL Dataflow library and async/await to wait for all of the atomic unit queries to conclude, after which I apply (optional, depending on what web request it was) business logic to the results and send back the response.</li>\n</ol>\n\n<p>Each atomic unit in these queries will return between 100-600 records, never more than 2000. The design objective is to query at most 200 atomic units at once => so 400,000 records is the maximum amount that the web server will ever need to apply business logic to, although that logic is never more than \"get one object of each distinct numerical ID in the result,\" for which I have implemented IEqualityComparer.</p>\n\n<p>This approach does not seem to scale <em>that</em> well. Even when querying 30-40 atomic units, there is a marked increase in response time, even though I can test and see that individual atomic unit responses each take less than 1 second.</p>\n\n<p>I think likely places for my issues are:</p>\n\n<ol>\n<li>Using a separate SqlConnection for each atomic unit query => am I leveraging connection pooling effectively?</li>\n<li>Business logic and object serialization => should I be approaching the distinct operation across atomic units differently? What if I want to do sum operations on other fields as well (not the default use-case, but a common one).</li>\n</ol>\n\n<p>If anyone has a favorite way of handling this, I would love to hear about it. Thanks.</p>\n\n<p>Current solution:</p>\n\n<p><code></p>\n\n<pre><code>        #region Identify atomic units of search query from search parameters (static methods, no i/o, very fast)\n        List&lt;string&gt; AtomicUnits = AtomicUnitsOfSearch(data);\n        #endregion                       \n\n        #region Atomic unit-targeted subqueries setup\n        var atomicUnitQueries = AtomicUnits.Select(au =&gt; \n            {                    \n                return GetItemsFromAtomicUnit(\n                    CreateConstraintSQLQuery(data), \n                    au,\n                    verbosity);\n            });            \n        #endregion\n\n        #region Execute async query across relevant shards; Distinct result item summary query business logic\n        if (Verbosity.basic.Equals(verbosity)) // return one item per conceptid\n        {\n            return (await TaskEx.WhenAll(atomicUnitQueries)).SelectMany(a =&gt; a)                    \n                .Distinct(new SimpleItemComparer()).ToList(); // Distinct() the fan-out results based on ID\n        }\n        else // assume _count verbosity =&gt; pick one item per Id, sum Categories and Item counts across atomic units (do not double-count for synonyms)\n        { \n            // create the ending schema before executing the results lookup\n            List&lt;Item&gt; atomicResults = (await TaskEx.WhenAll(atomicUnitQueries)).SelectMany(a =&gt; a).ToList();\n            List&lt;Item&gt; distinctResults = new List&lt;Item&gt;();\n            foreach (IGrouping&lt;int, Item&gt; g in atomicResults.GroupBy(a =&gt; a.Id)) // gets the lists of equivalent items across all atomic units queried\n            {\n                Item f = g.First();\n                distinctResults.Add(new Item(f.Id, f.Name, g.Sum(a =&gt; a.CategoryCount), g.Sum(a =&gt; a.ItemCount)));\n            }\n            return distinctResults;                \n        }                        \n        #endregion\n</code></pre>\n\n<p></code></p>\n"},{"tags":["php","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":56,"score":1,"question_id":13195426,"title":"who's online with timestamp with high performance","body":"<p>i just created a user login system with php session and now users can register and login to site and do another things...</p>\n\n<p>now i want to create online.php which will fetch all online users.i almost search everything in google and stackoverflow for this with no success.</p>\n\n<p>ok now i want to describe the system which i want to create with high performance..\nwhen a user logged in we just update table <code>user</code>.<code>lastlogin</code> which is a timestamp and then in online.php we SELECT * every user where time interval is &lt; 5 minutes.\nfor this purpose i can update this timestamp <code>lastlogin</code> field in database when user load each page,and this cost many mysql query to do the job... then in each page load i have to update </p>\n\n<blockquote>\n  <p>UPDATE <code>user</code> set <code>last</code>=now()</p>\n</blockquote>\n\n<p>that will cost me many mysql query.now i am looking for some another way like using sessions or something that i found in <a href=\"http://stackoverflow.com/questions/6385880/how-to-handle-user-online-status-when-he-she-close-the-browser?answertab=votes#tab-top\">this link</a></p>\n\n<blockquote>\n  <p>\"The normal solution is to store a timestamp in the table which you update every time the user does something. Users with a recent timestamp (say, five minutes) are shown as logged in, everybody else are logged out.</p>\n  \n  <p>It doesn't even have to be updated on every page load. Keep a timestamp in the session with the last update time as well, and only update the table when the database flag are about to expire.\"</p>\n</blockquote>\n\n<p>but unfortunately the answer wasnt quite helpful and i need an example or more describe on this.</p>\n"},{"tags":["database","performance","full-text-search","sql-server-2012","sql-server-performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":30,"score":0,"question_id":13200498,"title":"What is the fastest approach to populate a SQL Server database with large amount of data","body":"<p><strong>Dilemma:</strong></p>\n\n<p>I am about to perform population of data on SQL Server 2012 Dev Edition. Data is based on production data. Amount is around 4TB. </p>\n\n<p><strong>Purpose:</strong></p>\n\n<p>To test performance on full text search and on regular index as well. Target number should be around 300 million items around 500K each. </p>\n\n<p><strong>Question:</strong> </p>\n\n<p>What should I do before to speed up the process or consequences that I should worry about?</p>\n\n<p>Ex. </p>\n\n<ol>\n<li>Switching off statistics? </li>\n<li>Should I do a bulk insert of 1k items per transaction instead of single transaction? </li>\n<li>Simple recovery model? </li>\n<li>Log truncation? </li>\n</ol>\n\n<p><strong>Important:</strong></p>\n\n<p>I will use sample of 2k of production items to create every random item that will be inserted into database. I will use near unique samples generated in c#. It will be one table:</p>\n\n<pre><code>table \n(\n    long[id], \n    nvarchar(50)[index], \n    nvarchar(50)[index], \n    int[index], \n    float, \n    nvarchar(50)[index], \n    text[full text search index]\n)\n</code></pre>\n"},{"tags":["xcode","performance","search","ide","apple"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":11,"score":0,"question_id":13200602,"title":"Sluggish XCode searches in project navigator and through code","body":"<p>This is not a coding question but I'm not sure which stack exchange this really belongs in.</p>\n\n<p>I'm experiencing very slow search speeds on the project navigator and code Search fields. Whenever I type into either fields, my key presses show about 1.5 characters per second or slower. I can tell that my input is being buffered and slowly being fed into the field. </p>\n\n<p>From what I can see, it's not even doing instant searching while I'm typing - at least not while I'm typing into the project navigator search bar. I know this because it won't return results until I hit enter. When I search through my code, it's similar but at least there I know there might be some slow downs due to instant search. However, this still seems extremely sluggish compared to other IDEs that I've used on Ubuntu and Windows. </p>\n\n<p>I'm spec'd with a Macbook from 2010 (2.4 GHz Intel Core 2 Duo w/ 4 GB 1067 MHz DDR3 RAM). The main drive is a 120 GB SSD with 22 GB free on it (I know I shouldn't go this full on an SSD, but I doubt that's why the search fields in xcode are super sluggish.</p>\n\n<p>Thanks all!</p>\n"},{"tags":["performance","c#-4.0"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":17,"score":0,"question_id":13200572,"title":"Restrict Specific Control - Not to Post to Server","body":"<p>I am working in C#.Net. I am having a Treeview in my page. In the page load, i am generating a XML Format and binding the XML to the treeview component. In the samepage, i am having a button, \"SHOW TREEVIEW\". Whenever i click the button i should show the tree view in a Panel using model popup extender. Everything is working fine..But the performance of the page goes down. Whenever any post back occurs, the whole form is posted to the  server, at that time all the XML Data (which i generated for treeview) are going to the server. Because of this the sending time, waiting time goes high.</p>\n\n<p>Whether we can do anything, so that particular treeview control can be stopped of post to server.?</p>\n\n<p>Code to bind XML Data to Treeview...</p>\n\n<pre><code>XmlDocument document = new XmlDocument();\n        document.LoadXml(sb.ToString());\n        ASTreeViewXMLDescriptor descripter1 = new ASTreeViewXMLDescriptor();\n        this.astvMyTreeProd.DataSourceDescriptor = descripter1;\n        this.astvMyTreeProd.DataSource = document;\n        this.astvMyTreeProd.DataBind();\n</code></pre>\n\n<p>Model Popup Extender Code part..</p>\n\n<pre><code>&lt;asp:UpdatePanel ID=\"UpdatePanel4\" runat=\"server\"&gt;\n                    &lt;ContentTemplate&gt;\n                        &lt;ajaxToolKit:ModalPopupExtender ID=\"mp1\" runat=\"server\" PopupControlID=\"Panel1\" TargetControlID=\"btnLoc\"\n                            CancelControlID=\"imgClose\"&gt;\n                        &lt;/ajaxToolKit:ModalPopupExtender&gt;\n\n                        &lt;asp:Button ID=\"btnLoc\" ToolTip=\"Location\" runat=\"server\" Text=\"Location\" CausesValidation=\"false\"\n                            BackColor=\"LightGray\" ForeColor=\"Black\" OnClientClick=\"return FindSelectedItems();\"\n                            Enabled=\"false\" /&gt;\n                        &lt;asp:Panel ID=\"Panel1\" runat=\"server\" Style=\"display: none; width: 600px; border: 2px solid;\"\n                            BackColor=\"#f0f0f0\"&gt;\n                            &lt;div style=\"position: fixed; width: 44.4%; border: 1px solid; text-align: center;\"&gt;\n                                &lt;br /&gt;\n                                &lt;asp:Label ID=\"Label4\" runat=\"server\" Text=\"LOCATION HIERARCHY\" Font-Bold=\"true\"\n                                    Font-Size=\"Small\" Style=\"text-align: center; vertical-align: middle\"&gt;&lt;/asp:Label&gt;\n                                &lt;asp:ImageButton ID=\"imgClose\" runat=\"server\" ImageUrl=\"~/img/close_icon.png\" align=\"right\"&gt;\n                                &lt;/asp:ImageButton&gt;\n                                &lt;br /&gt;\n                            &lt;/div&gt;\n                            &lt;div style=\"margin-top: 33px; height: 450px; overflow: auto;\"&gt;\n                                &lt;table&gt;\n                                    &lt;tr valign=\"top\"&gt;\n                                        &lt;td width=\"650\"&gt;\n                                            &lt;ct:ASTreeView ID=\"astvMyTree\"/&gt;\n                                        &lt;/td&gt;\n                                                                              &lt;/tr&gt;\n                                &lt;/table&gt;\n                            &lt;/div&gt;\n                        &lt;/asp:Panel&gt;\n                    &lt;/ContentTemplate&gt;\n                &lt;/asp:UpdatePanel&gt;\n</code></pre>\n"},{"tags":["performance","scala","collections","autoboxing","specialized-annotation"],"answer_count":2,"favorite_count":2,"up_vote_count":15,"down_vote_count":0,"view_count":1268,"score":15,"question_id":5477675,"title":"Why are so few things @specialized in Scala's standard library?","body":"<p>I've searched for the use of <code>@specialized</code> in the source code of the standard library of Scala 2.8.1. It looks like only a handful of traits and classes use this annotation: <code>Function0</code>, <code>Function1</code>, <code>Function2</code>, <code>Tuple1</code>, <code>Tuple2</code>, <code>Product1</code>, <code>Product2</code>, <code>AbstractFunction0</code>, <code>AbstractFunction1</code>, <code>AbstractFunction2</code>.</p>\n\n<p>None of the collection classes are <code>@specialized</code>. Why not? Would this generate too many classes?</p>\n\n<p>This means that using collection classes with primitive types is very inefficient, because there will be a lot of unnecessary boxing and unboxing going on.</p>\n\n<p>What's the most efficient way to have an immutable list or sequence (with <code>IndexedSeq</code> characteristics) of <code>Int</code>s, avoiding boxing and unboxing?</p>\n"},{"tags":["c","performance","bash","shell"],"answer_count":3,"favorite_count":1,"up_vote_count":7,"down_vote_count":1,"view_count":164,"score":6,"question_id":13088807,"title":"Shell script vs C performance","body":"<p>I was wondering how bad would be the impact in the performance of a program migrated to shell script from C. </p>\n\n<p>I have intensive I/O operations.</p>\n\n<p>For example, in C, I have a loop reading from a filesystem file and writing into another one. I'm taking parts of each line without any consistent relation. I'm doing this using pointers. A really simple program.</p>\n\n<p>In the Shell script, to move through a line, I'm using <code>${var:(char):(num_bytes)}</code>. After I finish processing each line I just concatenate it to another file.</p>\n\n<pre><code>\"$out\" &gt;&gt; \"$filename\"\n</code></pre>\n\n<p>The program does something like:</p>\n\n<pre><code>while read line; do\n    out=\"$out${line:10:16}.${line:45:2}\"\n    out=\"$out${line:106:61}\"\n    out=\"$out${line:189:3}\"\n    out=\"$out${line:215:15}\"\n    ...\n    echo \"$out\" &gt;&gt; \"outFileName\"\n\ndone &lt; \"$fileName\"\n</code></pre>\n\n<p>The problem is, C takes like half a minute to process a 400MB file and the shell script takes 15 minutes.</p>\n\n<p>I don't know if I'm doing something wrong or not using the right operator in the shell script.</p>\n\n<p>Edit: I cannot  use awk since there is not a pattern to process the line</p>\n\n<p>I tried commenting the \"echo $out\" >> \"$outFileName\" but it doesn't gets much better. I think the problem is the ${line:106:61} operation. Any suggestions? </p>\n\n<p>Thanks for your help.</p>\n"},{"tags":["linux","performance","popup","javafx","intel-atom"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":18,"score":0,"question_id":13193035,"title":"JavaFX Popup performance on Linux – Intel Atom","body":"<p><strong>Background/Context:</strong></p>\n\n<p>I am developing touch screen based kiosk application.\nHW: Intel Atom E640 CPU @1.0GHz, SDRAM size 1GB\nOS: Linux Mint 10 (Julia)</p>\n\n<p>I am using virtual/on-screen keyboard which is based on Popup class (the keyboard is added to Popup content) similar to this:</p>\n\n<p><a href=\"http://code.google.com/p/fx-onscreen-keyboard/\" rel=\"nofollow\">on-screen-keyboard</a></p>\n\n<p>however, once the keyboard is shown the app gets very slow, each click on a button or even move over button takes very long.</p>\n\n<p>Once the popup is made visible I got this in terminal:</p>\n\n<p><em>Can't create transparent stage, because your screen doesn't support alpha channel. You need to enable XComposite extension.</em></p>\n\n<p>However, in my xorg.conf I have:</p>\n\n<pre><code>Section \"Extensions\"    \n    Option  \"Composite\" \"enable\"    \nEndSection\n</code></pre>\n\n<p>It is not only my app with this problem I got exactly same message when running Ensamble.jar. The extra-info popup in the search box is problematic. </p>\n\n<p><strong>My questions:</strong></p>\n\n<p>How do I enable XComposite extension or manage to get popup perform relatively smoothly?</p>\n\n<p>In case I need to find another solution for virtual keyb. instead of popup, what that should be? I need possibility to click on keyboard´s buttons and still preserve focus on TextField or HTML input element in WebView.  If you were to tackle this problem, what would be your approach to that?</p>\n"},{"tags":["sql","mysql","performance","exists"],"answer_count":9,"favorite_count":12,"up_vote_count":36,"down_vote_count":0,"view_count":22749,"score":36,"question_id":1676551,"title":"Best way to test if a row exists in a MySQL table","body":"<p>I'm trying to find out if a row exists in a table. Using MySQL, is it better to do a query like this:</p>\n\n<pre><code>SELECT COUNT(*) AS total FROM table1 WHERE ...\n</code></pre>\n\n<p>and check to see if the total is non-zero or is it better to do a query like this:</p>\n\n<pre><code>SELECT * FROM table1 WHERE ... LIMIT 1\n</code></pre>\n\n<p>and check to see if any rows were returned?</p>\n\n<p>In both queries, the WHERE clause uses an index.</p>\n"},{"tags":["c++","performance","optimization","language-agnostic","branch-prediction"],"answer_count":8,"favorite_count":1596,"up_vote_count":3041,"down_vote_count":7,"view_count":155973,"score":3034,"question_id":11227809,"title":"Why is processing a sorted array faster than an unsorted array?","body":"<p>Here is a piece of C++ code that shows some very peculiar performance. For some strange reason, sorting the data miraculously speeds up the code by almost 6x:</p>\n\n<pre class=\"lang-cpp prettyprint-override\"><code>#include &lt;algorithm&gt;\n#include &lt;ctime&gt;\n#include &lt;iostream&gt;\n\nint main()\n{\n    // Generate data\n    const unsigned arraySize = 32768;\n    int data[arraySize];\n\n    for (unsigned c = 0; c &lt; arraySize; ++c)\n        data[c] = std::rand() % 256;\n\n    // !!! With this, the next loop runs faster\n    std::sort(data, data + arraySize);\n\n    // Test\n    clock_t start = clock();\n    long long sum = 0;\n\n    for (unsigned i = 0; i &lt; 100000; ++i)\n    {\n        // Primary loop\n        for (unsigned c = 0; c &lt; arraySize; ++c)\n        {\n            if (data[c] &gt;= 128)\n                sum += data[c];\n        }\n    }\n\n    double elapsedTime = static_cast&lt;double&gt;(clock() - start) / CLOCKS_PER_SEC;\n\n    std::cout &lt;&lt; elapsedTime &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"sum = \" &lt;&lt; sum &lt;&lt; std::endl;\n}\n</code></pre>\n\n<ul>\n<li>Without <code>std::sort(data, data + arraySize);</code>, the code runs in <strong>11.54</strong> seconds.</li>\n<li>With the sorted data, the code runs in <strong>1.93</strong> seconds.</li>\n</ul>\n\n<hr>\n\n<p>Initially I thought this might be just a language or compiler anomaly. So I tried it in Java:</p>\n\n<pre class=\"lang-java prettyprint-override\"><code>import java.util.Arrays;\nimport java.util.Random;\n\npublic class Main\n{\n    public static void main(String[] args)\n    {\n        // Generate data\n        int arraySize = 32768;\n        int data[] = new int[arraySize];\n\n        Random rnd = new Random(0);\n        for (int c = 0; c &lt; arraySize; ++c)\n            data[c] = rnd.nextInt() % 256;\n\n        // !!! With this, the next loop runs faster\n        Arrays.sort(data);\n\n        // Test\n        long start = System.nanoTime();\n        long sum = 0;\n\n        for (int i = 0; i &lt; 100000; ++i)\n        {\n            // Primary loop\n            for (int c = 0; c &lt; arraySize; ++c)\n            {\n                if (data[c] &gt;= 128)\n                    sum += data[c];\n            }\n        }\n\n        System.out.println((System.nanoTime() - start) / 1000000000.0);\n        System.out.println(\"sum = \" + sum);\n    }\n}\n</code></pre>\n\n<p>with a similar but less extreme result.</p>\n\n<hr>\n\n<p>My first thought was that sorting brings the data into cache, but my next thought was how silly that is because the array was just generated.</p>\n\n<p>What is going on? Why is a sorted array faster than an unsorted array? The code is summing up some independent terms, the order should not matter.</p>\n"},{"tags":["mysql","performance","innodb"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":13196407,"title":"Mysql innoDB write operations are extremely slow","body":"<p>I'm having serious performance problems with MySQL and the InnoDB engine. Even the simplest table makes writing operations (creating the table, inserting, updating and deleting) horribly slow, as you can see in the following snippet.</p>\n\n<pre><code>mysql&gt; CREATE TABLE `test` (`id` int(11) not null auto_increment,\n   -&gt; PRIMARY KEY(`id`)) ENGINE=InnoDB;\nQuery OK, 0 rows affected (4.61 sec)\n\nmysql&gt; insert into test values ();\nQuery OK, 1 row affected (1.92 sec)\n\nmysql&gt; insert into test values ();\nQuery OK, 1 row affected (0.88 sec)\n\nmysql&gt; insert into test values ();\nQuery OK, 1 row affected (1.10 sec)\n\nmysql&gt; insert into test values ();\nQuery OK, 1 row affected (6.27 sec)\n\nmysql&gt; select * from test;\n+----+\n| id |\n+----+\n|  1 |\n|  2 |\n|  3 |\n|  4 |\n+----+\n4 rows in set (0.00 sec)\n\nmysql&gt; delete from test where id = 2;\nQuery OK, 1 row affected (0.28 sec)\n\nmysql&gt; delete from test where id = 3;\nQuery OK, 1 row affected (6.37 sec)\n</code></pre>\n\n<p>I have been looking at htop and the long waiting times are not because of abnormal CPU load. It's almost zero, and memory usage is also normal. If I create the same table using the MyISAM engine, then it works normally. My my.cnf file contains this (if I remember right I haven't changed anything from the default Debian configuration):</p>\n\n<pre><code>[client]\nport        = 3306\nsocket      = /var/run/mysqld/mysqld.sock\n[mysqld_safe]\nsocket      = /var/run/mysqld/mysqld.sock\nnice        = 0\n\n[mysqld]\nuser        = mysql\npid-file    = /var/run/mysqld/mysqld.pid\nsocket      = /var/run/mysqld/mysqld.sock\nport        = 3306\nbasedir     = /usr\ndatadir     = /var/lib/mysql\ntmpdir      = /tmp\nlanguage    = /usr/share/mysql/english\nskip-external-locking\nbind-address        = 127.0.0.1\nkey_buffer      = 40M\nmax_allowed_packet  = 16M\nthread_stack        = 128K\nthread_cache_size       = 8\nmyisam-recover         = BACKUP\nmax_connections        = 100\ntable_cache            = 64\nthread_concurrency     = 10\nquery_cache_limit   = 1M\nquery_cache_size        = 40M\nlog_slow_queries    = /var/log/mysql/mysql-slow.log\nlong_query_time = 2\nlog-queries-not-using-indexes\nexpire_logs_days    = 10\nmax_binlog_size         = 100M\n\n[mysqldump]\nquick\nquote-names\nmax_allowed_packet  = 16M\n\n[isamchk]\nkey_buffer      = 16M\n!includedir /etc/mysql/conf.d/\n</code></pre>\n\n<p>I have also tried to restart the server, but it doesn't solve anything.</p>\n\n<p>The slow queries log doesn't give any extra information.</p>\n"},{"tags":["java","performance","simulation","statemachine"],"answer_count":3,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":94,"score":1,"question_id":13177347,"title":"Efficient state machine pattern in java","body":"<p>I am writing a java simulation application which has a lot of entities to simulate. Each of these entities has a certain state at any time in the system. A possible and natural approach to model such an entity would be using the <a href=\"http://en.wikipedia.org/wiki/State_pattern\" rel=\"nofollow\">state (or state machine)</a> pattern. The problem is that it creates a lot of objects during the runtime if there are a lot of state switches, what might cause bad system performance. What design alternatives do I have? I want performance to be the main criteria after maintainability.</p>\n\n<p>Thanks</p>\n"},{"tags":["asp.net","css","performance","sprite"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":59,"score":2,"question_id":13186033,"title":"Should I turn these images into a sprite, and if so, how would I do so? (picture included)","body":"<p>I have an asp.net (vb) website with a clients page. On that page, there are hyperlinked logos to some of our larger clients.  Each logo is identical in size (w 207 h 119).  There are 3 columns and 5 rows of images, so 15 logos in total.</p>\n\n<p>Currently, the images are coded like this -- but I'm thinking I need to remove the asp images and just use regular images:</p>\n\n<pre><code>&lt;td&gt;\n&lt;asp:HyperLink ID=\"HyperLink15\" runat=\"server\" ImageUrl=\"~/images/clgm.jpg\" \nNavigateUrl=\"http://www.gm.com\" Target=\"_blank\"    \nitemprop=\"url\"&gt;HyperLink&lt;/asp:HyperLink&gt;\n&lt;/td&gt;\n&lt;td&gt;\n&lt;asp:HyperLink ID=\"HyperLink16\" runat=\"server\" ImageUrl=\"~/images/clford.jpg\" \nNavigateUrl=\"http://www.ford.com\" Target=\"_blank\" \nitemprop=\"url\"&gt;HyperLink&lt;/asp:HyperLink&gt;\n&lt;/td&gt; \n</code></pre>\n\n<p>And so on, for all 15 clients.  Would it be better for speed and performance (by reducing http requests if I sprited these images into one image?  Assuming that's the case, should I <strong>change these asp:hyperlinks and asp:images to just regular html links and images</strong>?  And then how would I sprite them?  I'm not too good with css, so I'd truly appreciate any help anybody can offer in this regard.</p>\n\n<p>If needed, below is what they look like -- again, 3 columns and 5 rows (<strong>that's where I get even more confused</strong>):</p>\n\n<p><img src=\"http://i.stack.imgur.com/m93cd.jpg\" alt=\"enter image description here\"></p>\n"},{"tags":["hibernate","orm","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":535,"score":2,"question_id":4468150,"title":"Hibernate/EclipseLink? Which is best with Weblogic,Oracle DB?","body":"<p>In my company, we are using Oracle DB and Weblogic application server. So in the process to upgrade or switch to new ORM, we shortlisted two options - Hibernate and EclipseLink.</p>\n\n<p>I gathered following summary regarding both ORMs -</p>\n\n<p>Hibernate:</p>\n\n<ol>\n<li>When you need to train people, like we are going to do next week – most of the companies have Hibernate experts.</li>\n<li>When you hire new developers, most of them come with specific Hibernate experience.</li>\n<li>When you need to consult with experts, both in the internet or consultants, you have LOTS of options. Endless forums and communities all regarding Hibernate.</li>\n<li>Hibernate is an open source which has a huge community. This means that it will be improved all the time and will push the ORM market forward.</li>\n<li>Hibernate is an open source which means you have the code to handle, and in case needed, fit it to your needs.</li>\n<li>There are lots of plugins to Hibernate, such as validations tool, audit tools, etc. These becomes standard as well and dismiss you from impl. things yourself.</li>\n<li>One most important thing with ORM tool, is to configure it according to your application’s needs. Usually the default setting doesn’t fit to your needs.\nFor that sake, when the market has a huge experience with the tool’s configuration, and lots of experts (see point 1 and 3) – most of chances you will find similar cases and\nlots of knowledge about how to configure the tool and thus – your application.</li>\n</ol>\n\n<p>EclipseLink:</p>\n\n<ol>\n<li>Fully supported by Oracle. Hibernate no. In case of pb, it could be cumbersome to prove that it is a pure Weblogic one. Concretely, we will have to prove it (waste of time and complexity).</li>\n<li>Eclipse link is developed by Oracle and the preferred ORM in the Weblogic /Oracle DB world.</li>\n<li>Even if at a certain time EclipseLink was a bit late compared to Hibernate (feature), EclipseLink evolved very fast and we can consider now that they close the gap.</li>\n<li>No additional fee as soon as you have Weblogic license. You will need to pays additional fee if you want some professional support on Hibernate.</li>\n<li>We are currently relying on Hibernate for our legacy offer and are facing pb in second level cache (JGroups). Today, we are riding off this part!. Consequences are limitation in clustering approach (perf)</li>\n<li>On EclipseLink side we do succeed to manage first and second level cache in a clustering approach.</li>\n<li>Indeed Hibernate is open source, so you can imagine handling it. In reality, the code is so complex that it is nearly impossible to modify it. Moreover as it is LGPL, you need to feedback all the modified sources to the community systematically.</li>\n<li>All tests performed by Oracle concerning Weblogic are using EclipseLink. Moreover, Oracle says that some specific optimizations are done to manage Oracle DB.</li>\n<li>Hibernate comes from JBoss community.</li>\n</ol>\n\n<p>Right now we are preferring Hibernate but there are concerns/reasons like EclipseLink developed by Oracle and preferred ORM in Webogic/ Oracle DB world (compatibility of ORM with DB and App. server), support comparison with both ORM, which are preventing to finalize the decision.</p>\n\n<p>Please help me with you views and opinions and share you experience with us as which one is better and why so that we can make a perfect decision.</p>\n\n<p>If you want you can also reply to me @ yogesh.golande@gmail.com.</p>\n\n<p>Thanks.</p>\n"},{"tags":["performance","query","postgresql","index"],"answer_count":4,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":55,"score":3,"question_id":13183735,"title":"Query performance in PostgreSQL using 'similar to'","body":"<p>I need to retrieve certain rows from a table depending on certain values in a specific column, named <strong>columnX</strong> in the example:</p>\n\n<pre><code>select *\nfrom tableName \nwhere columnX similar to ('%A%|%B%|%C%|%1%|%2%|%3%')\n</code></pre>\n\n<p>So if <strong>columnX</strong> contains at least one of the values specified (A, B, C, 1, 2, 3), I will keep the row.  </p>\n\n<p>I can't find a better approach than using <strong>similar to</strong>.  The problem is that the query takes too long for a table with more than a million rows.</p>\n\n<p>I've tried indexing it:</p>\n\n<pre><code>create index tableName_columnX_idx on tableName (columnX) \nwhere columnX similar to ('%A%|%B%|%C%|%1%|%2%|%3%')\n</code></pre>\n\n<p>However, if the condition is variable (the values could be other than A, B, C, 1, 2, 3), I would need a different index for each condition.</p>\n\n<p>Is there any better solution for this problem?</p>\n\n<p>EDIT: Thanks everybody for the feedback.  Looks like I've achieved to this point maybe because of a design mistake (topic I've posted in a <a href=\"http://stackoverflow.com/questions/13194906/data-structure-design-for-database-replication-support\">separated question</a>).</p>\n"},{"tags":["java","python","performance","slowness"],"answer_count":1,"favorite_count":2,"up_vote_count":2,"down_vote_count":2,"view_count":62,"score":0,"question_id":13193871,"title":"using Python for a solution cannot be handled in JAVA because of slow execution","body":"<p>I have 3 millions line of data each has 30 features - it is hard to include all in memory for my computer and slow to process it with learning algorithm - . I want to write a little code that makes random sampling but in JAVA and with my PC configurations it does not work or takes so much times to execute. I know that writing in C or C++ gives better solution but I am also curious about the availability of python for such case. Is it reasonable to use Python in such a case that Java is not working efficiently because of slowness and memory restriction - please do not say to increase heap size or such-?</p>\n"},{"tags":["c#","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":9,"view_count":79,"score":-8,"question_id":13193024,"title":"using @ to locate file via string","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/5179389/at-sign-in-file-path-string\">@(at) sign in file path/string</a>  </p>\n</blockquote>\n\n\n\n<p>in C# for example this function you can use <code>@\"stringpath\"</code> instead of <code>\"stringpath\"</code></p>\n\n<p>Why should I add an @ there in front? I get the same results without using @??</p>\n\n<p>example:</p>\n\n<pre><code>UploadFileMethod(@\"C:\\test.txt\", @\"http://site.com/bla/file.txt\");\n\npublic static bool UploadFileToDocumentLibrary(string sourceFilePath, string targetDocumentLibraryPath)\n\n{\n//stuff here\n}\n</code></pre>\n"},{"tags":["javascript","performance","html5","google-chrome","web-worker"],"answer_count":0,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":31,"score":0,"question_id":13170828,"title":"Unresponsive Google Chrome while running HTML5 Web workers","body":"<p>I am using the following code to run webworker to fune prime numbers in a web page in the latest Google Chrome</p>\n\n<p><a href=\"https://dl.dropbox.com/u/655237/project/prime.html\" rel=\"nofollow\">https://dl.dropbox.com/u/655237/project/prime.html</a></p>\n\n<p>However after clicking the start Worker button,  the stop button becomes unresponsive for some time. After 30 second or so it becomes active when it is mouse hovered. This does not happen in Firefox.</p>\n\n<p>Is there any flaw in the code ?</p>\n\n<p><strong>prime.html</strong> </p>\n\n<pre><code>        &lt;section&gt;\n            Last Prime Found:-  &lt;p id=\"number\"&gt;NA&lt;/p&gt;\n            &lt;button id=\"prime\"&gt;Find &amp;nbsp; Prime&lt;/button&gt;\n            &lt;button id=\"primew\"&gt;Start &amp;nbsp;Worker&lt;/button&gt;\n            &lt;button id=\"primes\"&gt;Stop &amp;nbsp;Worker&lt;/button&gt;\n        &lt;/section&gt;\n        &lt;script&gt;\n            var worker;\n            document.querySelector('#prime').addEventListener('click', function () {\n                findPrime();\n            }, false);\n            document.querySelector('#primew').addEventListener('click', function () {\n                findPrimeW();\n            }, false);\n            document.querySelector('#primes').addEventListener('click', function () {\n                stopWorker();\n            }, false);\n\n            function findPrime(){\n                var n = 1;\n                search: while (true) {\n                    n += 1;\n                    for (var i = 2; i &lt;= Math.sqrt(n); i += 1)\n                        if (n % i == 0)\n                            continue search;\n                    // found a prime!\n                    document.querySelector(\"#number\").textContent=n;\n                }\n            }\n            function findPrimeW(){\n                worker = new Worker('js/worker1.js');\n                worker.onmessage = function (event) {\n                    document.querySelector(\"#number\").textContent = event.data;\n                };\n            }\n\n            function stopWorker()\n            { \n                worker.terminate();\n            }\n        &lt;/script&gt;\n</code></pre>\n\n<p><strong>worker1.js</strong></p>\n\n<pre><code>    var n = 1;\nsearch: while (true) {\n  n += 1;\n  for (var i = 2; i &lt;= Math.sqrt(n); i += 1)\n    if (n % i == 0)\n     continue search;\n  // found a prime!\n  postMessage(n);\n}\n</code></pre>\n\n<p>Another example is this\n<a href=\"https://dl.dropbox.com/u/655237/events/gdg-html5/index.html#/webworkers\" rel=\"nofollow\">https://dl.dropbox.com/u/655237/events/gdg-html5/index.html#/webworkers</a></p>\n\n<p>You can not change the slide for a some time after clicking \"Start Worker\" button. Ideally it should not happen as the heavy weight computation is delegated to a separate web worker.</p>\n"},{"tags":["c#",".net","performance","dynamic","compiler"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":99,"score":3,"question_id":13193799,"title":"Performance cost of using dynamic typing in .NET","body":"<p>What is the performance cost of using <code>dynamic</code> vs <code>object</code> in .NET? Say for example I have a method which accepts a parameter of any type. E.G.</p>\n\n<pre><code>public void Foo(object obj)\n{\n}\n</code></pre>\n\n<p>or</p>\n\n<pre><code>public void Foo(dynamic obj)\n{\n}\n</code></pre>\n\n<p>ILSpy tells me that when using dynamic code, the compiler must insert a code block to handle dynamism. Therefore I want to know if using dynamic in place of object is advised and to what level this usage comes at the cost of performance?</p>\n"},{"tags":["python","search","list","performance"],"answer_count":4,"favorite_count":2,"up_vote_count":8,"down_vote_count":0,"view_count":2175,"score":8,"question_id":2701173,"title":"Most efficient way for a lookup/search in a huge list (python)","body":"<p>-- I just parsed a big file and I created a list containing 42.000 strings/words. I want to query [against this list] to check if a given word/string belongs to it. So my question is:</p>\n\n<p>What is the most efficient way for such a  lookup?</p>\n\n<p>A first approach is to sort the list (<code>list.sort()</code>) and then just use </p>\n\n<pre><code>&gt;&gt; if word in list: print 'word'\n</code></pre>\n\n<p>which is really trivial and I am sure there is a better way to do it. My goal is to apply a fast lookup that finds whether a given string is in this list or not.  If you have any ideas of another data structure, they are welcome. Yet, I want to avoid for now more sophisticated data-structures like Tries etc. I am interested in hearing ideas (or tricks) about fast lookups or any other python library methods that might do the search faster than the simple <code>in</code>. </p>\n\n<p>And also i want to know the index of the search item</p>\n"},{"tags":["c++","visual-studio-2010","performance","opengl","windows-8"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":139,"score":3,"question_id":13182563,"title":"OpenGL game runs fine in Win7, drops to 5fps in Windows 8?","body":"<p>I have just recently installed Windows 8, and I tried to compile and build a simple c++ game project in VS 2010, but when I did, it was running at 5 fps. On windows 7, it runs at a solid 60 fps. Nothing has been changed in the code, but there is just horrible slow down.</p>\n\n<p>I have updated my video drivers, but there is still horrible lag. I thought the problem was to do with compatibility issues with windows 8 and OpenGL, but I can't find anything to confirm this. I was wondering if anyone else has had this problem, and if you have solved it.</p>\n"},{"tags":["c++","c","performance","algorithm","io"],"answer_count":0,"favorite_count":0,"up_vote_count":3,"down_vote_count":9,"view_count":175,"score":-6,"question_id":13192537,"title":"fastest method to print output in c/c++","body":"<p>My program generates a lot of integers(about 300000) to be output on the screen. Each integer needs to be printed on a separate line. Traditional output methods using printf for each line or sprintf to a buffer and then printf the buffer take huge time. I want to print my result within 3 seconds.</p>\n\n<p>Please suggest some faster ways</p>\n"},{"tags":["c#",".net","performance","linq"],"answer_count":6,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":185,"score":2,"question_id":13166443,"title":"How to delete every 2nd character in a string?","body":"<p>How to delete every 2nd character in a string?</p>\n\n<p>For example:</p>\n\n<pre><code>3030313535333635  -&gt; 00155365\n3030303336313435  -&gt; 00036145\n3032323437353530  -&gt; 02247550\n</code></pre>\n\n<p>The strings are always 16-characters long and the result is always 8 characters long - and the character that is being removed is always a '3' - Don't ask why however - I did not dream up this crazy source data.</p>\n"},{"tags":["javascript","jquery","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":76,"score":1,"question_id":13192045,"title":"How to optimize javascript/jquery code to speed up it's performance?","body":"<p>On one of my web projects, I use a lot of javascript/jqeury code, which is pretty slow on browsers (Windows 7 x64), especially on IE. \nI use 3 Ajax requests at the same time only on Home page. \nOn Search page, I also use ajax requests, which are fired on scroll event, on any 'search tag' (simple anchor tag) click event and etc. which in general is making data loading very slow. </p>\n\n<p>I use jquery plugins such as, Anythingslider, jquery coockies plugin, Raty( rating plugin ), Tipsuy, jQuery coreUISelect, jScrollPane, mouse wheel and etc. All those 3rd party plugins i have minified and combinied in jquery.plugins.js file, which is almosw 80KB. </p>\n\n<p>I select a lot of DOM elements with jQuery. For example I use the folowing code:</p>\n\n<pre><code>$(\"#element\")\n</code></pre>\n\n<p>Instead of:</p>\n\n<pre><code>document.getElementById('element');\n</code></pre>\n\n<p>I also have one big CSS file, which is more than 5 000 lines, because I have combined all 3rd party jquery plugins's css files into one file, for caching and less HTTP requests.</p>\n\n<p>1) Well, I wonder, what can I do to optimize my code for better performance and speeding up web page load? </p>\n\n<p>2) What Kind of tools can I use to debug and my JS code? I forgot to mention that, when I refresh page in Google Chrome or Firefox with firebug or Chrome native developer tools opened, the page in that case loads also very slow. Sometimes the Firefox is even crushed. </p>\n\n<p>3) Will selecting of DOM elements with raw js give me a better and faster way to parse the document? Or should I leave, the jQuery selecting? Talk about is about 50 elements. </p>\n\n<p>4) Should I seperate and after that minify external plugins, such as Anythingslider? Or is it better when I have 'all in one' js file?</p>\n\n<p>5) Is it better to aslo seperate jquery plugins's css code from main style.css? Because even hovering on element and affecting the :hover state from css file, is pretty slow. </p>\n\n<p>Well guys, I'm really counting on you.\nI've been googling all night to find answers on my questions and really hope to find it here. \nThanks. </p>\n"},{"tags":["ruby-on-rails","performance","data-structures","volatile"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":45,"score":0,"question_id":12403706,"title":"Rails: A Volatile Data Structure To Store Temporary Data","body":"<p>I'm programming the backend of a mobile application and I've come across this problem, wondering whether I can use a rails tool or should I implement a new technology to my current system.</p>\n\n<p>We have our user that is able to make a request, demanding to chat anyone who is around. However our system (the backend) has to collect this data and choose one of users who agree to chat randomly. But for that I want to keep all the ones that agree to chat in a list and pick one element randomly. But I would like to implement this in a volatile way so that when someone random selected all the other candidates will be gone.</p>\n\n<p>Of course, those candidates could be easily stored in a table and later on could be deleted but I believe that there is a structure that I can use on demand and dump whenever I want. So what kind of data structure I should use to provide this efficiency?</p>\n"},{"tags":["c#","performance","design-patterns","design"],"answer_count":3,"favorite_count":2,"up_vote_count":8,"down_vote_count":0,"view_count":249,"score":8,"question_id":12000635,"title":"High Performance Development","body":"<p><strong>Background</strong></p>\n\n<p>We have been working very hard to try come up with solutions for a \"High Performance\" application. The application is basically a high throughput in-memory manager, with a sync back to disk. The \"reads\" and \"writes\" are tremendously high, around 3000 transactions a second. We try and do as much as possible in memory, but eventually the data gets stale and needs to be flushed to disk, and this is where a huge \"bottleneck\" ensues. The app is multi-threaded, with about 50 threads. There is no IPC (inter-process comms)</p>\n\n<p><strong>Attempts</strong></p>\n\n<p>We initially wrote this in Java, and it worked quite well, up until a certain load, the bottleneck was hit and it just couldn't keep up.\nThen we tried it in C#, and the same bottle-neck was reached.\nWe tried this with unmanaged code (C#), and though on initial tests was blindingly fast using MMF (Memory-map files), in production, reading was slow (are using Views).\nWe did try CouchBase, but we stumbled into problems surround high network utilization. This might be poor configuration on our part!</p>\n\n<p><strong>Extra Info:</strong> In our Java attempt (non-MMF), our thread with the Queue of information that needs to get flushed to disk builds to the extent of being unable to keep up \"writing\" to disk.\nIn our C# Memory-Map File Approach, the problems is that READS are very slow, and the WRITES working perfectly. For some reason, the Views are slow!</p>\n\n<p><strong>Question</strong></p>\n\n<p>So the question is, situations where you intend of transferring massive amounts of data; can someone please assist with a possible approach or architectural design that might be able to assist? I know this seems a bit broad, but I think the specific nature of high performance, high throughput should narrow down the answers.</p>\n\n<p>Can anyone vouch for using Couchbase, MongoDB or Cassandra at such a level? Other ideas or\nsolutions would be appreciated.</p>\n"},{"tags":["performance","postgresql","unique-constraint"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":32,"score":2,"question_id":13190682,"title":"How does unique constraint affect write performance in Postgres DB","body":"<p>Does the <code>UNIQUE</code> constraint specified on a column or group of columns affect the write performance of Postgres DB in any way? How does it internally function? </p>\n\n<p>I mean, does it perform unique checking at the time of insertion of a new record? If yes, how does it do that, does it do a linear search for a duplicate value already existing in the DB? In that case, it is deemed to affect the performance i.e. more the number of unique constraints worse would be the write/insert performance? Is it true? </p>\n"},{"tags":["java","performance","data-structures","queue"],"answer_count":4,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":2620,"score":2,"question_id":827831,"title":"What are the advantages of Blocking Queue in Java?","body":"<p>I am working on a project that uses a queue that keeps information about the messages that to be sent to remote hosts. In that case one thread is responsible to put the information into the queue and another thread is responsible for get the pop the information from the queue and send. In that case the 2nd thread needs to check the queue for the information periodically. </p>\n\n<p>But later I found that it is a \"Reinvention of Wheel\" :) I could use a blocking queue for this purpose.</p>\n\n<p>Now my question is What are the other advantages of using a blocking queue for above application? (Ex : Performance, Modifiable of the code, Any special tricks etc )</p>\n"},{"tags":["c#","multithreading","performance","parallel-processing","threadpool"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":937,"score":1,"question_id":349513,"title":"Using Parallel.For to test SQL queries and comparison with the ThreadPool","body":"<p>I'm looking for a way to easily load test and benchmark some of our SQL (using ADO.NET, nothing fancy using LINQ or PLINQ) that has to be performant when running under high parallel load.</p>\n\n<p>I've thought of using the new parallel extensions CTP and specifically <code>Parallel.For</code> / <code>Parallel.ForEach</code> to simply run the SQL over 10k iterations or so - but I've not been able to find any data on what these have been optimized for.</p>\n\n<p>Essentially I'm worried that because database access is inherently I/O bound, it won't create sufficient load. Does anyone know if Parallel. For is  intelligent enough to use > x threads (where x = # of CPUs) if the tasks it is executing are not totally CPU bound? I.e. does it behave in a similar manner to the managed thread pool?</p>\n\n<p>Would be rather cool if it was so!</p>\n\n<p><b>EDIT: As @CVertex has kindly referred to below, you can set the number of threads independently. Does anyone know if the parallel libraries by default are intelligent enough to keep adding threads if a job is I/O bound?</b></p>\n"},{"tags":["performance","hibernate","join","associations","one-to-many"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":27,"score":0,"question_id":13190307,"title":"Hibernate: LazyLoading - Search by Object in One-To-Many Collection","body":"<p>I got an Entity 1 -> SubEntity n One-To-Many-relation.</p>\n\n<p>I just would like to check, if this is the <em>most efficient</em> way <strong>to search for an Entity, including a specified SubEntity in its \"One-To-Many-Collection\"</strong>. It works, but do I have to join fetch the SubEntities or is there a <strong>more lightweight</strong> solution if I don't need all SubEntities to be loaded? (FetchMode = Lazy)</p>\n\n<pre><code>public Entity getEntityBySubEntity(SubEntity subEntity) { \n     List&lt;Entity&gt; result = (List&lt;Entity&gt;)getHibernateTemplate.findByNamedParam(\n         \"From Entity as e left join fetch e.subEntities as sub where sub.id = :id\",\"id\",subEntity.getId()); \n     if (!result.isEmpty()) { \n          return result.get(0);\n     } else {\n          throw new NoResultException();\n     }\n }\n</code></pre>\n\n<p>(by the way, there should always be just one result...)     </p>\n\n<p>thx in adv,</p>\n\n<p>cav              </p>\n"},{"tags":["sql","sql-server","performance","sql-server-2008"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":94,"score":0,"question_id":11724333,"title":"SQL Server - joining 4 fast queries gives me one slow query","body":"<p>I have 4 views in my MS Sql Server Database which are all quite fast (less than 2 seconds) and return all less than 50 rows.</p>\n\n<p>BUT when I create a query where I join those 4 views (left outer joins) I get a query which takes almost one minute to finish.</p>\n\n<p>I think the query optimizer is doing a bad job here, is there any way to speed this up. I am tempted to copy each of the 4 views into a table and join them together but this seems like too much of a workaround to me.</p>\n\n<p>(Sidenote: I can't set any indexes on any tables because the views come from a different database and I am not allowed to change anything there, so this is not an option)</p>\n\n<p><strong>EDIT:</strong> I am sorry, but I don't think posting the sql queries will help. They are quite complex and use around 50 different tables. I cannot post an execution plan either because I don't have enought access rights to generate an execution plan on some of the databases.</p>\n\n<p>I guess my best solution right now is to generate temporary tables to store the results of each query.</p>\n"},{"tags":["javascript","asp.net","performance","onload"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":23,"score":0,"question_id":13190552,"title":"How do I combine javascripts and load them after page load on an asp.net website?","body":"<p>I just read this <a href=\"http://stackoverflow.com/questions/3291681/combining-javascript-files-as-recommended-by-yslow-optimal-size\">SE Post</a> where it explains the question asked is similar to what I'm asking.  Yslow recommends combining all of your external javascripts, if possible. I've actually taken internal javascripts and made them into an external .js file and put it in my /scripts folder, per some recommendation I found on some website.  But this leads me to my 2 (similar) questions; I didn't think separate posts were needed.</p>\n\n<p><strong>(1)</strong> - How would I combine javascript codes?  Just leave a blank line and then copy and paste the other javascript after the other, and so on, until it's all in one file?</p>\n\n<p><strong>(2)</strong> - I have an asp.net website with 5 master pages.  4 of the 5 have 6 javascript files (all minified, some on my own using some compression tool that worked).  But I read on a SE post and online that you should load the script(s) <strong>after page load</strong>.  How do you do this on an asp.net website?  All my .js files are in the \n\n<p>@bluesmoon gave a nice, thorough answer, but I wasn't able to comprehend it:</p>\n\n<pre><code>To load a script after page load, do something like this:\n\n// This function should be attached to your onload handler\n// it assumes a variable named script_url exists.  You could easily\n// extend it to use an array of scripts or figure it out some other\n// way (see note late)\nfunction lazy_load() {\n    setTimeout(function() {\n            var s = document.createElement(\"script\");\n            s.src=script_url;\n            document.body.appendChild(s);\n        }, 50);\n}\n\nThis is called from onload, and sets a timeout for 50ms later at which point it will \nadd a new script node to the document's body. The script will start downloading after \nthat. Now since javascript is single threaded, the timeout will only fire after onload \nhas completed even if onload takes more than 50ms to complete.\n</code></pre>\n\n<p>I don't really understand what he's saying or how to go about implementing it.  So that's all ... I just need help in <strong>combining javascripts</strong> and loading them (or it) <strong>after page load</strong>; I don't see anything saying \"onload\" in my master pages or .aspx pages.  Thank you for any help or guidance anybody can offer me!</p>\n"},{"tags":["c","performance","sse","vectorization","avx"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":75,"score":2,"question_id":13183258,"title":"Union with __m256 and array of two __m128","body":"<p>Can I have a union like this</p>\n\n<pre><code>  union eight_floats_t\n  {\n    __m256 a;\n    __m128 b[2];\n  };\n  eight_floats_t eight_floats;\n</code></pre>\n\n<p>to have an instant access to the two 128 bit parts of a 256 bit register?</p>\n\n<p>Edit: I was asking to understand the performance impact of this approach.</p>\n"},{"tags":["mysql","performance","flask","load-testing"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":51,"score":1,"question_id":13181114,"title":"Web request performance really bad under stress","body":"<p>I wrote a web application using python and Flask framework, and set it up on Apache with mod_wsgi. \nToday I use JMeter to perform some load testing on this application. \nFor one web URL:</p>\n\n<ul>\n<li><p>when I set only 1 thread to send request, the response time is 200ms</p></li>\n<li><p>when I set 20 concurrent threads to send requests, the response time increases to more than 4000ms(4s). THIS IS UNACCEPTABLE!</p></li>\n</ul>\n\n<p>I am trying to find the problem, so I recorded the time in before_request and teardown_request methods of flask. And it turns out the time taken to process the request is just over 10ms.</p>\n\n<p>In this URL handler, the app just performs some SQL queries (about 10) in Mysql database, nothing special.</p>\n\n<p>To test if the problem is with web server or framework configuration, I wrote another method Hello in the same flask application, which just returns a string. It performs perfectly under load, the response time is 13ms with 20-thread concurrency.</p>\n\n<p>And when doing the load test, I execute 'top' on my server, there are about 10 apache threads, but the CPU is mostly idle.</p>\n\n<p>I am at my wit's end now. Even if the request are performed serially, the performance should not drop so drastically... My guess is that there is some queuing somewhere that I am unaware of, and there must be overhead besides handling the request.</p>\n\n<p>If you have experience in tuning performance of web applications, please help!</p>\n\n<p><em>EDIT</em></p>\n\n<p>About apache configuration, I used MPM worker mode, the configuration:</p>\n\n<pre><code>&lt;IfModule mpm_worker_module&gt;\n    StartServers          4\n    MinSpareThreads      25\n    MaxSpareThreads      75\n    ThreadLimit          64\n    ThreadsPerChild      50\n    MaxClients          200\n    MaxRequestsPerChild   0\n&lt;/IfModule&gt;\n</code></pre>\n\n<p>As for mod_wsgi, I tried turning WSGIDaemonProcess on and off (by commenting the following line out), the performance looks the same.</p>\n\n<pre><code># WSGIDaemonProcess tqt processes=3 threads=15 display-name=TQTSERVER\n</code></pre>\n"},{"tags":["performance","r","data.frame"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":90,"score":0,"question_id":13122882,"title":"a faster implementation of merge.data.frame() in R","body":"<p>Let's say <code>a</code> and <code>b</code> are two data frames. The goal is to write a function\n<code>f(a,b)</code> that produces a merged data frame, in the same way as merge\n<code>merge(a,b,all=TRUE)</code> would do, that is filling missing variables in <code>a</code> or <code>b</code> with NAs. (The problem is <code>merge()</code> appears to be very slow.)</p>\n\n<p>This can be done as follows (pseudo-code):</p>\n\n<pre><code>for each variable `var` found in either `a` or `b`, do:\n    unlist(list(a.srcvar, b.srcvar), recursive=FALSE, use.names=FALSE)\n\nwhere:\nx.srcvar is x$var if x$var exists, or else\n            rep(NA, nrow(x)) if y$var !is.factor, or else\n            as.factor(rep(NA, nrow(x)))\n</code></pre>\n\n<p>and then wrap everything in a data frame.</p>\n\n<p>Here's a \"naive\" implementation:</p>\n\n<pre><code>merge.datasets1 &lt;- function(a, b) {\n  a.fill &lt;- rep(NA, nrow(a))\n  b.fill &lt;- rep(NA, nrow(b))\n  a.fill.factor &lt;- as.factor(a.fill)\n  b.fill.factor &lt;- as.factor(b.fill)\n  out &lt;- list()\n  for (v in union(names(a), names(b))) {\n    if (!v %in% names(a)) {\n      b.srcvar &lt;- b[[v]]\n      if (is.factor(b.srcvar))\n        a.srcvar &lt;- a.fill.factor\n      else\n        a.srcvar &lt;- a.fill\n    } else {\n      a.srcvar &lt;- a[[v]]\n      if (v %in% names(b))\n        b.srcvar &lt;- b[[v]]\n      else if (is.factor(a.srcvar))\n        b.srcvar &lt;- b.fill.factor\n      else\n        b.srcvar &lt;- b.fill\n    }\n    out[[v]] &lt;- unlist(list(a.srcvar, b.srcvar),\n                       recursive=FALSE, use.names=FALSE)\n  }\n  data.frame(out)\n}\n</code></pre>\n\n<p>Here's a different implementation that uses \"vectorized\" functions:</p>\n\n<pre><code>merge.datasets2 &lt;- function(a, b) {\n  srcvar &lt;- within(list(var=union(names(a), names(b))), {\n    a.exists &lt;- var %in% names(a)\n    b.exists &lt;- var %in% names(b)\n    a.isfactor &lt;- unlist(lapply(var, function(v) is.factor(a[[v]])))\n    b.isfactor &lt;- unlist(lapply(var, function(v) is.factor(b[[v]])))\n    a &lt;- ifelse(a.exists, var, ifelse(b.isfactor, 'fill.factor', 'fill'))\n    b &lt;- ifelse(b.exists, var, ifelse(a.isfactor, 'fill.factor', 'fill'))\n  })\n  a &lt;- within(a, {\n    fill &lt;- NA\n    fill.factor &lt;- factor(fill)\n  })\n  b &lt;- within(b, {\n    fill &lt;- NA\n    fill.factor &lt;- factor(fill)\n  })\n  out &lt;- mapply(function(x,y) unlist(list(a[[x]], b[[y]]),\n                                     recursive=FALSE, use.names=FALSE),\n                srcvar$a, srcvar$b, SIMPLIFY=FALSE, USE.NAMES=FALSE)\n  out &lt;- data.frame(out)\n  names(out) &lt;- srcvar$var\n  out\n}\n</code></pre>\n\n<p>Now we can test:</p>\n\n<pre><code>sample.datasets &lt;- lapply(1:50, function(i) iris[,sample(names(iris), 4)])\n\nsystem.time(invisible(Reduce(merge.datasets1, sample.datasets)))\n&gt;&gt;   user  system elapsed \n&gt;&gt;  0.192   0.000   0.190 \nsystem.time(invisible(Reduce(merge.datasets2, sample.datasets)))\n&gt;&gt;   user  system elapsed \n&gt;&gt;  2.292   0.000   2.293 \n</code></pre>\n\n<p>So, the naive version is orders of magnitude faster than the other. How can\nthis be? I always thought that <code>for</code> loops are slow, and that one should\nrather use <code>lapply</code> and friends and steer clear of loops in R. I would welcome any idea on how to improve my function in terms of speed.</p>\n"},{"tags":["performance","solr","lucene"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":1,"view_count":269,"score":1,"question_id":9162290,"title":"Lucene performance impact of returning large result sets","body":"<p>Does anyone know the performance impact of letting Lucene (or Solr) return <em>very</em> long result sets instead of just the usual \"top 10\". \nWe would like to return all results (which can be around 100.000 documents) from a user search  and then post-process the returned document ids before returning the actual result.</p>\n\n<p>Our current index contains about 10-20 million documents.</p>\n"},{"tags":["sql","performance","indexing","db2"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":53,"score":0,"question_id":13148822,"title":"How should I improve my query performance?","body":"<p>I have created a table with a couple of columns.</p>\n\n<p>MyTable </p>\n\n<ul>\n<li>Column1 -> RecordId (int)</li>\n<li>Column2 -> Title (varchar)</li>\n<li>Column3 -> Shortname (varchar)</li>\n<li>Column4 ->  (varchar) </li>\n<li><p>Column5 -> varchar</p>\n\n<p>...</p>\n\n<p>...</p></li>\n<li>Column10 -> varchar</li>\n</ul>\n\n<p>I am not joining this table with any other tables. I have 4 rows of data in the table.</p>\n\n<p>When I am querying for title, shortname for a particular RecordId, the query seems to be too slow. It nearly takes 7 seconds to load the page. </p>\n\n<p>I haven't used any indexing on my tables. Can you please suggest how do I improve my table/query performance? </p>\n\n<p>I am making db connection from my jsp in order to display the query results in the listbox. </p>\n\n<p>My code looks something like this- \nconn= DataObjectConnectionPool.getInstance().getConnection();\nprepStmt = conn.prepStmt(\"select title,shortname from MyTable where RecordId=1\");</p>\n"},{"tags":["php","database","performance","messaging","dbms"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":38,"score":1,"question_id":13187476,"title":"PHP-MYSQL messaging system performance problems","body":"<p>I want to create a messaging module for the social network web-site project (PHP &amp; MySQL) I've been working on. What substructures do well-known sites use for modules like this?</p>\n\n<p>I think, generating a new record on MySQL for each of new messages will slow down the system and cause problems after a while. What would you suggest me for saving the datas? SQLLite, file system or a high-performance dbms ? (exp: postgresql)</p>\n\n<p>Thanks</p>\n"},{"tags":["performance","memcached","redis"],"answer_count":6,"favorite_count":62,"up_vote_count":84,"down_vote_count":0,"view_count":30157,"score":84,"question_id":2873249,"title":"Is memcached a dinosaur in comparison to Redis?","body":"<p>I have worked quite a bit with memcached the last weeks and just found out about Redis. When I read this part of their readme, I suddenly got a warm, cozy feeling in my stomach:</p>\n\n<blockquote>\n  <p>Redis can be used as a memcached on steroids because is as fast as\n  memcached but with a number of\n  features more.\n  Like memcached, Redis also supports setting timeouts to keys so\n  that this key will be automatically\n  removed when a given amount of time\n  passes.</p>\n</blockquote>\n\n<p>This sounds amazing. I'd also found this page with benchmarks: <a href=\"http://www.ruturaj.net/redis-memcached-tokyo-tyrant-mysql-comparison\" rel=\"nofollow\">http://www.ruturaj.net/redis-memcached-tokyo-tyrant-mysql-comparison</a></p>\n\n<p>So, honestly - Is memcache really that old dinousaur that is a bad choice from a performance perspective when compared to this newcomer called Redis?</p>\n\n<p>I haven't heard lot about Redis previously, thereby the approach for my question!</p>\n"},{"tags":["mysql","performance","activerecord","ruby-on-rails-3.2","mysql2"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":61,"score":2,"question_id":12349318,"title":"rails activerecord, mysql and mysql2 performance degradation?","body":"<p>I have only recently upgraded from Rails 2.3.5 to Rails 3.2.7 and noticed some performance degradation in some of my queries. I am aware of Rails 3 ActiveRecord being slower in some cases than Rails 2.3.5, but the benchmarks I have surprised me and I just want to make sure I am not missing anything.</p>\n\n<p>I ran the following query, which is very popular in my application, as a benchmark</p>\n\n<pre><code>SELECT SQL_NO_CACHE table_name.* FROM table_name WHERE ((string_col = 'value') AND (int_col1 BETWEEN 5 AND 30)) ORDER BY int_col2 DESC LIMIT 1000\n</code></pre>\n\n<p>I checked:</p>\n\n<ul>\n<li>rails 3.2.7 vs. rails 2.3.5</li>\n<li>rails 3.2.7 with mysql adapter vs mysql2</li>\n<li>ActiveRecord.find_by_sql vs ActiveRecord.connection.select_all</li>\n</ul>\n\n<h1>Results</h1>\n\n<h3>Rails 3.2.7</h3>\n\n<p>rails 3.2.7, mysql adapter, \"select_all\": avg. 0.0148 seconds<br/>\nrails 3.2.7, mysql adapter, \"find_by_sql\" avg. 0.0555 seconds</p>\n\n<p>rails 3.2.7, mysql2 adapter, \"select_all\": avg. 0.045 seconds<br/>\nrails 3.2.7, mysql2 adapter, \"find_by_sql\" avg. 0.088 seconds</p>\n\n<h3>Rails 2.3.5</h3>\n\n<p>rails 2.3.5, mysql adapter \"select_all\": avg. 0.013 seconds<br/>\nrails 2.3.5, mysql adapter \"find_by_sql\": avg. 0.0177 seconds</p>\n\n<p>Although my original code is using ActiveRecord query api, I used hardcoded sql for the benchmark and also verified that calling mysql directly from the bash command line is stable and the above numbers result from rails/mysql adapter and not the db.</p>\n\n<h1>Question</h1>\n\n<p>Are these differences reasonable?<br/>\nThe diff between \"find_by_sql\" and \"select_all\" is much bigger in Rails 3.2.7 than in Rails 2.3.5.<br/>\nAnd why is mysql2 slower than mysql?<br/></p>\n"},{"tags":["php","performance"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":32,"score":2,"question_id":13187273,"title":"Minimal memory usage per call?","body":"<p>I just noticed - by calling memory_get_peak_usage() on an 'empty' php file, using php-fastcgi and NGINX, the result is ~120KB of memory</p>\n\n<pre><code>&lt;?php\n\n    print_r(memory_get_peak_usage());\n\n?&gt;\n</code></pre>\n\n<p>Does PHP really need that 'much' memory for every call, or does that only happen for the first call (initializing something I guess) and then every consecutive call needs less Memory?</p>\n\n<p>I'm asking, because I'm kind of surprised that an empty file already uses up 140KB - guessing that a couple of classes, functions and arrays will push that number up quite fast.</p>\n\n<p>And yeah, I know that this probably counts as premature optimization, but I'm really curious about knowing where those 120KB are coming from, and if there's a way to minify that cost per call.</p>\n"},{"tags":["mysql","performance","mongodb"],"answer_count":3,"favorite_count":3,"up_vote_count":5,"down_vote_count":0,"view_count":3681,"score":5,"question_id":9702643,"title":"Mysql vs Mongodb 1000 reads","body":"<p>I have been very excited about MongoDb and have been testing it lately. I had a table called posts in MySql with about 20million records indexed only on a field called 'id'.</p>\n\n<p>I wanted to compare speed with MongoDb and I ran a test which would get and print 15 records randomly from our huge databases. I ran the query about 1,000 times each for mysql and mongodb and I am surpised that I do not notice a lot of difference in speed. Maybe Monogodb is 1.1 times faster. Thats very disappointing. Is there something I am doing wrong? I know that my tests are not perfect but is MySQL on par with MongoDb when it comes to read intensive chores. </p>\n\n<p>Note:</p>\n\n<ul>\n<li>I have dual core  + ( 2 threads ) i7 cpu and 4GB ram</li>\n<li>I have 20  partitions on mysql each of 1 million records</li>\n</ul>\n\n<p><strong>Sample Code Used For Testing MongoDB</strong></p>\n\n<pre><code>&lt;?php\nfunction microtime_float()\n{\n    list($usec, $sec) = explode(\" \", microtime());\n    return ((float)$usec + (float)$sec);\n}\n$time_taken = 0;\n$tries = 100;\n// connect\n$time_start = microtime_float();\n\nfor($i=1;$i&lt;=$tries;$i++)\n{\n    $m = new Mongo();\n    $db = $m-&gt;swalif;\n    $cursor = $db-&gt;posts-&gt;find(array('id' =&gt; array('$in' =&gt; get_15_random_numbers())));\n    foreach ($cursor as $obj)\n    {\n        //echo $obj[\"thread_title\"] . \"&lt;br&gt;&lt;Br&gt;\";\n    }\n}\n\n$time_end = microtime_float();\n$time_taken = $time_taken + ($time_end - $time_start);\necho $time_taken;\n\nfunction get_15_random_numbers()\n{\n    $numbers = array();\n    for($i=1;$i&lt;=15;$i++)\n    {\n        $numbers[] = mt_rand(1, 20000000) ;\n\n    }\n    return $numbers;\n}\n\n?&gt;\n</code></pre>\n\n<p><strong>Sample Code For Testing MYSQL</strong></p>\n\n<pre><code>&lt;?php\nfunction microtime_float()\n{\n    list($usec, $sec) = explode(\" \", microtime());\n    return ((float)$usec + (float)$sec);\n}\n$BASE_PATH = \"../src/\";\ninclude_once($BASE_PATH  . \"classes/forumdb.php\");\n\n$time_taken = 0;\n$tries = 100;\n$time_start = microtime_float();\nfor($i=1;$i&lt;=$tries;$i++)\n{\n    $db = new AQLDatabase();\n    $sql = \"select * from posts_really_big where id in (\".implode(',',get_15_random_numbers()).\")\";\n    $result = $db-&gt;executeSQL($sql);\n    while ($row = mysql_fetch_array($result) )\n    {\n        //echo $row[\"thread_title\"] . \"&lt;br&gt;&lt;Br&gt;\";\n    }\n}\n$time_end = microtime_float();\n$time_taken = $time_taken + ($time_end - $time_start);\necho $time_taken;\n\nfunction get_15_random_numbers()\n{\n    $numbers = array();\n    for($i=1;$i&lt;=15;$i++)\n    {\n        $numbers[] = mt_rand(1, 20000000);\n\n    }\n    return $numbers;\n}\n?&gt;\n</code></pre>\n\n<p>Thanking you\nImran</p>\n"},{"tags":["performance","iis","powershell","monitoring"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":19,"score":0,"question_id":13184697,"title":"IIS 7 request duration monitoring","body":"<p>i am curious if there is a way of monitoring the request duration time on an iis server. Personally I have came up with a solution but it's really resource intensive and that is why i'm asking the question, just to gather more opinions. </p>\n\n<p>My plan is to extract the duration time of each request and send it to graphite so as to have a real time overview of the performance of the webserver. The idea i've came up with is to use poweshell with its webadministration module. And if you run  <code>get-item IIS:\\AppPools\\DefaultAppPool | Get-WebRequest</code> for example you get all the requests on that app pool with a lot of info including the time info.</p>\n\n<p>The thing is that i should have a script which runs every 100 ms to get all requests and that is kinda wasteful. Is there a way to tell iis to put the request duration time(in miliseconds) in the logs? Because then it would be much easier to get the information I need.</p>\n"},{"tags":["php","arrays","performance","warnings"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":41,"score":0,"question_id":13184704,"title":"Calling isset() vs. returning undefined index","body":"<p>I have code that is used very extensively which fetches an array from another method, and sometimes returns the first element of that array. Given that <code>null</code> is an acceptable return value for the function, is it worth the performance overhead of calling <code>isset()</code> on the array index (or checking the array length, etc), or is it better to just return the non-existant index (warnings aside). What are the advantages of calling <code>isset()</code> aside from preventing the warning.</p>\n\n<p>The example below is simplified, the real function doesn't <em>just</em> get the first element of the array.</p>\n\n<p>Return index which may not exist:</p>\n\n<pre><code>function get_array_element(){\n    $array = get_array();       // function that returns array\n    return $array[0];           // return index 0 which may not exist\n}\n</code></pre>\n\n<p>Versus checking if index is set:</p>\n\n<pre><code>function get_array_element(){\n    $array = get_array();       // function that returns array\n    return (isset($array[0]))?  // check if index 0 isset() else return null\n        $array[0] : \n        null; \n}\n</code></pre>\n"},{"tags":["java","android","performance","hibernate"],"answer_count":3,"favorite_count":11,"up_vote_count":19,"down_vote_count":0,"view_count":8839,"score":19,"question_id":4257374,"title":"Is Hibernate an overkill for an Android application?","body":"<p>I'm looking for a good ORM for my android application and at first glance it seems like for a mobile device I would prefer to use something simpler maybe. The thing is I'm just assuming here with no real evidence, so I thought I would ask the community's opinion (maybe there's is someone that has been through the experience). It is a fairly large(for mobile) application and will be run on a dedicated tablet.</p>\n\n<p>What does everyone else think ? Is Hibernate too much for an android application ? Will there be performance problems ?</p>\n\n<p>What would you use instead if you think it is too much ?</p>\n\n<p>I am aware there are other questions asking for alternatives, but I decided to ask since most of those questions simply assumed it was an overkill and asked for other options and I started wondering \"Is it really and overkill ? Why ?\" Due to my lack of experience I simply think it it, but can't really provide an answer if I'm asked to explain why. Is it performance ? Is it too much configuration (Which I don't mind) ? </p>\n\n<p>Thanks!</p>\n"},{"tags":["php","performance","benchmarking","zend-framework2"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":2,"view_count":51,"score":-2,"question_id":13182088,"title":"possible ways to increase performance in ZF2 applications?","body":"<p>i read a lot benchmark weblog posts about ZF2 that says zf2 has a low performance for response time because of huge functions load and class loads ...</p>\n\n<p>so i need a list of possible ways to increase zf2 performance</p>\n\n<p>thanks</p>\n"},{"tags":["java","multithreading","performance"],"answer_count":6,"favorite_count":0,"up_vote_count":3,"down_vote_count":1,"view_count":1377,"score":2,"question_id":5538847,"title":"Multi threaded game - updating, rendering, and how to split them","body":"<p>So, I'm working on a game engine, and I've made pretty good progress. However, my engine is single-threaded, and the advantages of splitting updating and rendering into separate threads sounds like a very good idea.</p>\n\n<p>How should I do this? Single threaded game engines are (conceptually) very easy to make, you have a loop where you update -> render -> sleep -> repeat. However, I can't think of a good way to break updating and rendering apart, especially if I change their update rates (say I go through the update loop 25x a second, and have 60fps for rendering) - what if I begin updating halfway through a render loop, or vice versa?</p>\n"},{"tags":["c#","multithreading","performance","c#-4.0","parallel-processing"],"answer_count":2,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":82,"score":4,"question_id":13176529,"title":"Parallel GZip Decompression of Log Files - Tweaking MaxDegreeOfParallelism for the Highest Throughput","body":"<p>We have up to 30 GB of GZipped log files per day. Each file holds 100.000 lines and is between 6 and 8 MB when compressed. The simplified code in which the parsing logic has been stripped out, utilises the Parallel.ForEach loop. </p>\n\n<p>The maximum number of lines processed peaks at MaxDegreeOfParallelism of 8 on the two-NUMA node, 32 logical CPU box (Intel Xeon E7-2820 @ 2 GHz):</p>\n\n<pre class=\"lang-cs prettyprint-override\"><code>using System;\n\nusing System.Collections.Concurrent;\n\nusing System.Linq;\nusing System.IO;\nusing System.IO.Compression;\n\nusing System.Threading.Tasks;\n\nnamespace ParallelLineCount\n{\n    public class ScriptMain\n    {\n        static void Main(String[] args)\n        {\n            int    maxMaxDOP      = (args.Length &gt; 0) ? Convert.ToInt16(args[0]) : 2;\n            string fileLocation   = (args.Length &gt; 1) ? args[1] : \"C:\\\\Temp\\\\SomeFiles\" ;\n            string filePattern    = (args.Length &gt; 1) ? args[2] : \"*2012-10-30.*.gz\";\n            string fileNamePrefix = (args.Length &gt; 1) ? args[3] : \"LineCounts\";\n\n            Console.WriteLine(\"Start:                 {0}\", DateTime.UtcNow.ToString(\"yyyy-MM-ddTHH:mm:ss.fffffffZ\"));\n            Console.WriteLine(\"Processing file(s):    {0}\", filePattern);\n            Console.WriteLine(\"Max MaxDOP to be used: {0}\", maxMaxDOP.ToString());\n            Console.WriteLine(\"\");\n\n            Console.WriteLine(\"MaxDOP,FilesProcessed,ProcessingTime[ms],BytesProcessed,LinesRead,SomeBookLines,LinesPer[ms],BytesPer[ms]\");\n\n            for (int maxDOP = 1; maxDOP &lt;= maxMaxDOP; maxDOP++)\n            {\n\n                // Construct ConcurrentStacks for resulting strings and counters\n                ConcurrentStack&lt;Int64&gt; TotalLines = new ConcurrentStack&lt;Int64&gt;();\n                ConcurrentStack&lt;Int64&gt; TotalSomeBookLines = new ConcurrentStack&lt;Int64&gt;();\n                ConcurrentStack&lt;Int64&gt; TotalLength = new ConcurrentStack&lt;Int64&gt;();\n                ConcurrentStack&lt;int&gt;   TotalFiles = new ConcurrentStack&lt;int&gt;();\n\n                DateTime FullStartTime = DateTime.Now;\n\n                string[] files = System.IO.Directory.GetFiles(fileLocation, filePattern);\n\n                var options = new ParallelOptions() { MaxDegreeOfParallelism = maxDOP };\n\n                //  Method signature: Parallel.ForEach(IEnumerable&lt;TSource&gt; source, Action&lt;TSource&gt; body)\n                Parallel.ForEach(files, options, currentFile =&gt;\n                    {\n                        string filename = System.IO.Path.GetFileName(currentFile);\n                        DateTime fileStartTime = DateTime.Now;\n\n                        using (FileStream inFile = File.Open(fileLocation + \"\\\\\" + filename, FileMode.Open))\n                        {\n                            Int64 lines = 0, someBookLines = 0, length = 0;\n                            String line = \"\";\n\n                            using (var reader = new StreamReader(new GZipStream(inFile, CompressionMode.Decompress)))\n                            {\n                                while (!reader.EndOfStream)\n                                {\n                                    line = reader.ReadLine();\n                                    lines++; // total lines\n                                    length += line.Length;  // total line length\n\n                                    if (line.Contains(\"book\")) someBookLines++; // some special lines that need to be parsed later\n                                }\n\n                                TotalLines.Push(lines); TotalSomeBookLines.Push(someBookLines); TotalLength.Push(length);\n                                TotalFiles.Push(1); // silly way to count processed files :)\n                            }\n                        }\n                    }\n                );\n\n                TimeSpan runningTime = DateTime.Now - FullStartTime;\n\n                // Console.WriteLine(\"MaxDOP,FilesProcessed,ProcessingTime[ms],BytesProcessed,LinesRead,SomeBookLines,LinesPer[ms],BytesPer[ms]\");\n                Console.WriteLine(\"{0},{1},{2},{3},{4},{5},{6},{7}\",\n                    maxDOP.ToString(),\n                    TotalFiles.Sum().ToString(),\n                    Convert.ToInt32(runningTime.TotalMilliseconds).ToString(),\n                    TotalLength.Sum().ToString(),\n                    TotalLines.Sum(),\n                    TotalSomeBookLines.Sum().ToString(),\n                    Convert.ToInt64(TotalLines.Sum() / runningTime.TotalMilliseconds).ToString(),\n                    Convert.ToInt64(TotalLength.Sum() / runningTime.TotalMilliseconds).ToString());\n\n            }\n            Console.WriteLine();\n            Console.WriteLine(\"Finish:                \" + DateTime.UtcNow.ToString(\"yyyy-MM-ddTHH:mm:ss.fffffffZ\"));\n        }\n    }\n}\n</code></pre>\n\n<p>Here's a summary of the results, with a clear peak at MaxDegreeOfParallelism = 8:</p>\n\n<p><img src=\"http://i.stack.imgur.com/VvFz1.png\" alt=\"enter image description here\"></p>\n\n<p>The CPU load (shown aggregated here, most of the load was on a single NUMA node, even when DOP was in 20 to 30 range):</p>\n\n<p><img src=\"http://i.stack.imgur.com/hZoUW.png\" alt=\"enter image description here\"></p>\n\n<p>The only way I've found to make CPU load cross 95% mark was to split the files across 4 different folders and execute the same command 4 times, each one targeting a subset of all files.</p>\n\n<p>Can someone find a bottleneck?</p>\n"},{"tags":["windows","performance","winapi","delay","clock"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":27,"score":1,"question_id":13183350,"title":"So is Sleep() and this the same?","body":"<p>So, are these two the same?\nUsing delay CPU usage is crazy in task manager.. is this the same case as the System Idle Process?</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;time.h&gt;\n//#include &lt;windows.h&gt;\n\nint delay(long int time)\n{\n    clock_t beginning = clock();\n    while(clock() - beginning &lt; time) {}\n    return 0;\n}\n\nint main()\n{\n    clock_t beginning = clock();\n\n    begin:\n\n    std::cout &lt;&lt; \"delay this by 1000ms\\n\";\n\n    //Sleep(1000);\n    delay(1000);\n\n    goto begin; //i know, i know\n\n    return 0;\n}\n</code></pre>\n"},{"tags":["php","performance","numbers","generate"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":30,"score":1,"question_id":13183025,"title":"Fastest way to generate and check numbers in PHP?","body":"<p>I need some help with generating numbers in PHP then checking for example if that number is below a hundred give 0.1 points. This is for a coins system that will be called when certain actions in a flash game are used.</p>\n\n<p>I want to do this in the fastest time possible and here is the code i'm using to test it before i update it to the main server for the game.</p>\n\n<pre><code>&lt;?php\n$coins = 0;\n$rand = rand(0, 1000);\n\nif($rand &lt; 50){\n$coins = $coins + 0.01;\n}\n\nif($rand &lt; 100 &amp;&amp; $rand &gt; 50){\n$coins = $coins + 0.2;\n}\n\nif($rand &gt; 100 &amp;&amp; $rand &lt; 200){\n$coins = $coins + 0.2;\n}\n\nif($rand &gt; 500){\n$coins = $coins + 0.5;\n}\n\necho \"Generated $coins coins \\n\";\n?&gt;\n</code></pre>\n\n<p>Sorry if anyone does not understand my quest in a few words i'm asking how could i make this faster and does anyone have any tips for it?</p>\n"},{"tags":["sql","sql-server","performance","application","architecture"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":54,"score":0,"question_id":13173984,"title":"Is having a copy of SQL data in an application a good idea to save SQL SELECTS?","body":"<p>I am working on a multithreading .NET 4 application which acquires data continuously and writes them into a SQL database (MySQL or SQL Server - not yet sure).</p>\n\n<p>Everytime when a <code>INSERT</code> is executed, at leat one prior <code>SELECT</code> is necessary in order so synchronize with the databaes. This means the applications gets a block which contains new and old data and then has to check which data sets are new and which are already in the database.</p>\n\n<p>This means a lot of <code>SELECTS</code> which result everytime in more or less the same data.</p>\n\n<p>Would it be a good idea to have a copy of the last x entries per table within the application?\nThis way the synchronization could be done on the copy instead of the database.</p>\n\n<p>Pro: </p>\n\n<ul>\n<li>Faster</li>\n</ul>\n\n<p>Contra:</p>\n\n<ul>\n<li>Uses a lot of memory</li>\n<li>Risk of becomming unsynchronized with the database</li>\n</ul>\n\n<p>What do you think? What is the best practice for such a use case?\nAny other pros and cons?</p>\n"},{"tags":["sql","mysql","performance","types"],"answer_count":8,"favorite_count":2,"up_vote_count":19,"down_vote_count":0,"view_count":5462,"score":19,"question_id":1962310,"title":"Importance of varchar length in MySQL table","body":"<p>I have a MySQL table where rows are inserted dynamically.  Because I can not be certain of the length of strings and do not want them cut off, I make them varchar(200) which is generally much bigger than I need.  Is there a big performance hit in giving a varchar field much more length than necessary?</p>\n"},{"tags":["mysql","performance","caching"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":33,"score":1,"question_id":13181954,"title":"query cache mysql","body":"<p>I would like to use query cache on MySQL, but my doubt is if I update or insert data on already cached query, will the cache reflect it?</p>\n\n<p>Will it return only the cached query or the updated query?</p>\n"},{"tags":["c++","performance","visual-c++","opengl","drawing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":3,"view_count":49,"score":-3,"question_id":13176514,"title":"slow redrawing how to draw with better performance","body":"<p>I am trying to draw a recursive tree with OpenGL but when I change the position of the camera, redrawing is very slow. I try to use <code>glGenLists</code> but the drawing is still very slow.</p>\n\n<pre><code>void glTree(void)\n{\n    tree = glGenLists (1);\n    if (tree != 0) \n    {   \n        glNewList(tree, GL_COMPILE);\n        drawTree(5);\n        glEndList();\n    }\n}\n\n\nvoid Dibuja (void)\n{\n    glClear( GL_COLOR_BUFFER_BIT );\n    glMatrixMode(GL_MODELVIEW);\n    glLoadIdentity();\n    glRotated (beta, 1.0,0.0,0.0);\n    glRotated (-alfa, 0.0,1.0,0.0);\n    glCallList(treeLeaf);\n    glFlush();\n}\n\n\n\nvoid drawTree( int level){\n\n   GLUquadric* quadricobj=gluNewQuadric();\n\n\n   //dibuja tronco\n\n    glPushMatrix();\n    if(level&gt;1){\n        glColor3b(92,64,23);\n\n    }\n    else{\n        glColor3f(0,1,0);\n    }\n    glRotated(-90,1,0,0);\n    gluCylinder(quadricobj, 0.4, 0.2, 5.0, 8, 4);\n    glPopMatrix();\n\n//dibuja ramas\nif (level&lt;=0) return;\n\n\n\n    glPushMatrix();\n    glTranslated(0,1,0);\n    glRotated(40,0,0,1);\n   glScaled(0.5,0.5,0.5);\n    drawTree(level - 1);\n    glPopMatrix();\n\n    glPushMatrix();\n    glTranslated(0,2,0);\n    glRotated(30,1,0,0);\n    glScaled(0.5,0.5,0.5);\n    drawTree(level  - 1);\n    glPopMatrix();\n\n    glPushMatrix();\n    glTranslated(0,3,0);\n    glRotated(-25,0,0,1);\n    glScaled(0.5,0.5,0.5);\n    drawTree( level - 1);\n    glPopMatrix();\n\n    glPushMatrix();\n    glTranslated(0,4,0);\n    glRotated(-20,1,0,0);\n    glScaled(0.5,0.5,0.5);\n    drawTree( level - 1);\n    glPopMatrix();\n\n    glPushMatrix();\n    glTranslated(0,5,0);\n    glScaled(0.5,0.5,0.5);\n    drawTree(level - 1);\n    glPopMatrix();\n\n\n\n }\n\nvoid openwindow(int numeroArgumentos, char ** listaArgumentos)\n\n    {\n        glutInit(&amp;numeroArgumentos, listaArgumentos);\n        glutInitDisplayMode (GLUT_SINGLE | GLUT_RGB);\n        glutInitWindowSize (VentanaAncho, VentanaAlto);\n        glutInitWindowPosition (VentanaX, VentanaY);\n        glutCreateWindow (listaArgumentos[0]);\n        glutDisplayFunc (Dibuja);\n        glutReshapeFunc (TamanyoVentana);\n        glClearColor (0.0f, 0.0f, 0.0f, 0.0f); \n        glClear (GL_COLOR_BUFFER_BIT); \n        glColor3f (1.0f, 1.0f, 1.0f); \n    }\nint main(int numArgumentos, char ** listaArgumentos)\n{   \n\n    openwindow(numArgumentos, listaArgumentos);\n        startFuntionCallback ();\n    startDisplayLists ();\n    glutMainLoop();\n\n    return (0);\n}\n</code></pre>\n"},{"tags":["php","mysql","search","performance"],"answer_count":3,"favorite_count":6,"up_vote_count":3,"down_vote_count":0,"view_count":1111,"score":3,"question_id":2954022,"title":"MySQL/PHP Search Efficiency","body":"<p>I'm trying to create a small search for my site. I've tried using full-text index search, but I could never get it to work. Here is what I've come up with:</p>\n\n<pre><code>if(isset($_GET['search'])) {\n\n$search = str_replace('-', ' ', $_GET['search']);\n$result = array();\n\n$titles = mysql_query(\"SELECT title FROM Entries WHERE title LIKE '%$search%'\");\nwhile($row = mysql_fetch_assoc($titles)) {\n    $result[] = $row['title'];\n}\n\n$tags = mysql_query(\"SELECT title FROM Entries WHERE tags LIKE '%$search%'\");\nwhile($row = mysql_fetch_assoc($tags)) {\n    $result[] = $row['title'];\n}\n\n$text = mysql_query(\"SELECT title FROM Entries WHERE entry LIKE '%$search%'\");\nwhile($row = mysql_fetch_assoc($text)) {\n    $result[] = $row['title'];\n}\n\n$result = array_unique($result);\n}\n</code></pre>\n\n<p>So basically, it searches through all the titles, body-text, and tags of all the entries in the DB. This works decently well, but I'm just wondering how efficient would it be? This would only be for a small blog, too. Either way I'm just wondering if this could be made any more efficient.</p>\n"},{"tags":["c++","performance","qt","foreach","for-loop"],"answer_count":10,"favorite_count":1,"up_vote_count":8,"down_vote_count":0,"view_count":15199,"score":8,"question_id":771008,"title":"'for' loop vs 'foreach' in C++","body":"<p>Which is better (or faster), a C++ <code>for</code> loop or the <code>foreach</code> operator provided by Qt? For example, the following condition</p>\n\n<pre><code>QList&lt;QString&gt; listofstrings;\n</code></pre>\n\n<p>Which is better?</p>\n\n<pre><code>foreach(QString str, listofstrings)\n{\n    //code\n}\n</code></pre>\n\n<p>or</p>\n\n<pre><code>int count = listofstrings.count();\nQString str = QString();\nfor(int i=0;i&lt;count;i++)\n{\n    str = listofstrings.at(i);\n    //Code\n}\n</code></pre>\n"},{"tags":["php","performance","architecture"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":3,"view_count":34,"score":-1,"question_id":13180000,"title":"whole process at a single php file","body":"<p>I'm doing the whole process at a single php file. For example; crud operation, writing forms,post/get data. Is that wrong? Or that causing performance loss?</p>\n"},{"tags":["arrays","programming-languages","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":835,"score":1,"question_id":5029575,"title":"Best programming language for very large arrays and very large numbers?","body":"<p>What would be the best programming language for very large arrays and very large numbers?</p>\n\n<ul>\n<li>With arrays over 30,000 indexes</li>\n<li>And numbers over 100 digits</li>\n</ul>\n\n<p>Also it needs to be efficient, or easy to make efficient. </p>\n\n<p>Thanks. </p>\n"},{"tags":["c#","string","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":66,"score":0,"question_id":13179554,"title":"Is it faster to access char in string via [] operator or faster to access char in char[] via [] operator?","body":"<p>I have a simple question. I'm working with a gigantic <code>string</code> in C# and I'm repeatedly going through it and accessing individual characters via the <code>[]</code> operator. Is it faster to turn the <code>string</code> into a <code>char[]</code> and use the <code>[]</code> operator on the <code>char</code> array or is my approach faster? Or are they both the same? I want my program to be bleeding edge fast.</p>\n"},{"tags":["c++","performance","time","timestamp","epoch"],"answer_count":4,"favorite_count":3,"up_vote_count":2,"down_vote_count":0,"view_count":89,"score":2,"question_id":13175573,"title":"why is microsecond timestamp is repetetive using (a private) gettimeoftheday() i.e. epoch","body":"<p>I am printing microseconds continuously using gettimeofday(). As given in program output you can see that the time is not updated microsecond interval rather its repetitive for certain samples then increments not in microseconds but in milliseconds. </p>\n\n<pre><code>while(1)\n{\n  gettimeofday(&amp;capture_time, NULL);\n  printf(\".%ld\\n\", capture_time.tv_usec);\n}\n</code></pre>\n\n<p>Program output:</p>\n\n<pre><code>.414719\n.414719\n.414719\n.414719\n.430344\n.430344\n.430344\n.430344\n\n e.t.c\n</code></pre>\n\n<p>I want the output to increment sequentially like,</p>\n\n<pre><code>.414719\n.414720\n.414721\n.414722\n.414723\n</code></pre>\n\n<p>or </p>\n\n<pre><code>.414723, .414723+x, .414723+2x, .414723 +3x + ...+ .414723+nx\n</code></pre>\n\n<p>It seems that microseconds are not refreshed when I acquire it from capture_time.tv_usec.</p>\n\n<p>=================================\n//Full Program</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;windows.h&gt;\n#include &lt;conio.h&gt;\n#include &lt;time.h&gt;\n#include &lt;stdio.h&gt;\n\n#if defined(_MSC_VER) || defined(_MSC_EXTENSIONS)\n  #define DELTA_EPOCH_IN_MICROSECS  11644473600000000Ui64\n#else\n  #define DELTA_EPOCH_IN_MICROSECS  11644473600000000ULL\n#endif\n\nstruct timezone \n{\n  int  tz_minuteswest; /* minutes W of Greenwich */\n  int  tz_dsttime;     /* type of dst correction */\n};\n\ntimeval capture_time;  // structure\n\nint gettimeofday(struct timeval *tv, struct timezone *tz)\n{\n  FILETIME ft;\n  unsigned __int64 tmpres = 0;\n  static int tzflag;\n\n  if (NULL != tv)\n  {\n    GetSystemTimeAsFileTime(&amp;ft);\n\n    tmpres |= ft.dwHighDateTime;\n    tmpres &lt;&lt;= 32;\n    tmpres |= ft.dwLowDateTime;\n\n    /*converting file time to unix epoch*/\n    tmpres -= DELTA_EPOCH_IN_MICROSECS; \n    tmpres /= 10;  /*convert into microseconds*/\n    tv-&gt;tv_sec = (long)(tmpres / 1000000UL);\n    tv-&gt;tv_usec = (long)(tmpres % 1000000UL);\n  }\n\n  if (NULL != tz)\n  {\n    if (!tzflag)\n    {\n      _tzset();\n      tzflag++;\n    }\n\n    tz-&gt;tz_minuteswest = _timezone / 60;\n    tz-&gt;tz_dsttime = _daylight;\n  }\n\n  return 0;\n}\n\nint main()\n{\n   while(1)\n  {     \n    gettimeofday(&amp;capture_time, NULL);     \n    printf(\".%ld\\n\", capture_time.tv_usec);// JUST PRINTING MICROSECONDS    \n   }    \n}\n</code></pre>\n"},{"tags":["performance","physics","gameloop"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":15,"score":0,"question_id":13178649,"title":"Efficient Game Loop - Singly-Threaded / Fixed Time Step With Remainder","body":"<p>(For this question, assume I already have functionality that lets me yield to the OS in such a way that I'm negligibly-likely to encounter hitching.)</p>\n\n<p>My criteria for a simple game loop are this:</p>\n\n<ul>\n<li>Singly-threaded (hopefully multiply-threaded in the future, but let's set that aside)</li>\n<li>For physics calculations, time steps do not need to be fixed but they need to be within a range</li>\n<li>CPU power should be preserved by taking maximal time steps when possible</li>\n<li>The renderer using interpolation between physics states is undesirable for my situation</li>\n</ul>\n\n<p>This is the loop I've come up with, in pseudocode, and I'd like to see if anyone can see any glaring flaws in my plan.</p>\n\n<pre><code>max_time_step = (largest time step that lets physics run comfortably)\nmin_time_step = (smallest time step that lets physics run comfortably)\nfixed_time_step = max_time_step - min_time_step\n\ncontinual_loop\n{\n    process_input\n\n    frame_time_remaining = get_time_elapsed_since_last_render_frame\n\n    if frame_time_remaining &lt; min_time_step\n        yield\n        restart_loop\n\n    while frame_time_remaining &gt; max_time_step\n        step ( fixed_time_step )\n        frame_time_remaining -= fixed_time_step\n\n    step ( frame_time_remaining )\n}\n</code></pre>\n\n<p>What I hope to accomplish with this are:</p>\n\n<ul>\n<li>Processing input before checking minimum frame time hopefully will allow us to skip the yield step any time there is input (since checking input takes time)</li>\n<li>The minimum number of physics updates are performed per render frame while preserving the physics calculations' integrity</li>\n<li>Rendering always uses exact determined positions and never interpolates, while still maintaining maximum framerate</li>\n<li>If the game is blocked for multiple seconds, minimum slowdown results from large amounts of physics calculation to catch up</li>\n<li>If the game is running too fast (i.e., without vsync/fps_limit on), will not often stutter and hitch while waiting for sufficient time to pass</li>\n<li>Is maximally-synchronized with user input, instead of trying to use downtime to predict next physics step without input</li>\n</ul>\n"},{"tags":["java","performance","optimization","virtual-machine","final"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":84,"score":4,"question_id":13173046,"title":"static final field, static field and performances","body":"<p>Even thought it's not its main purpose, I've always thought that the <code>final</code> keyword (in some situations and VM implementations) could help the JIT. <br/>\nIt might be an urban legend but I've never imagined that setting a field <code>final</code> could negatively affect the performances. \n<br/><br/>Until I ran into some code like that:</p>\n\n<pre class=\"lang-java prettyprint-override\"><code>   private static final int THRESHOLD = 10_000_000;\n   private static int [] myArray = new int [THRESHOLD];\n\n   public static void main(String... args) {\n      final long begin = System.currentTimeMillis();\n\n      //Playing with myArray\n      int index1,index2;\n      for(index1 = THRESHOLD - 1; index1 &gt; 1; index1--)\n          myArray[index1] = 42;             //Array initial data\n      for(index1 = THRESHOLD - 1; index1 &gt; 1; index1--) {\n                                            //Filling the array\n          for(index2 = index1 &lt;&lt; 1; index2 &lt; THRESHOLD; index2 += index1)\n              myArray[index2] += 32;\n      }\n\n      long result = 0;\n      for(index1 = THRESHOLD - 1; index1 &gt; 1; index1-=100)\n          result += myArray[index1];\n\n      //Stop playing, let's see how long it took\n      System.out.println(result);\n      System.out.println((System.currentTimeMillis())-begin+\"ms\");\n   }\n</code></pre>\n\n<p><br/></p>\n\n<p>Let's have a look at:\n<code>private static int [] myArray = new int [THRESHOLD];</code><br/>\nUnder W7 64-bit and on a basis of 10 successive runs, I get the following results:<br/></p>\n\n<ol>\n<li><p><code>THRESHOLD = 10^7</code>, 1.7.0u09 client VM (Oracle):<br/></p>\n\n<ul>\n<li>runs in ~2133ms when <code>myArray</code> is not final.</li>\n<li>runs in ~2287ms when <code>myArray</code> is final.</li>\n<li>The -server VM produces similar figures i.e 2131ms and 2284ms.\n<br/><br/></li>\n</ul></li>\n<li><p><code>THRESHOLD = 3x10^7</code>, 1.7.0u09 client VM (Oracle):<br/></p>\n\n<ul>\n<li>runs in ~7647ms when <code>myArray</code> is not final.</li>\n<li>runs in ~8190ms when <code>myArray</code> is final.</li>\n<li>The -server VM produces ~7653ms and ~8150ms.\n<br/><br/></li>\n</ul></li>\n<li><p><code>THRESHOLD = 3x10^7</code>, 1.7.0u01 client VM (Oracle):<br/></p>\n\n<ul>\n<li>runs in ~8166ms  when <code>myArray</code> is not final.</li>\n<li>runs in ~9694ms when <code>myArray</code> is final. That's more than 15% difference !</li>\n<li>The -server VM produces a neglectable difference in favour of the non-final version, about 1%.\n<br/><br/></li>\n</ul></li>\n</ol>\n\n<p><em>Remark: I used the bytecode produced by JDK 1.7.0u09's javac for all my tests. The bytecode produced is exactly the same for both versions except for <code>myArray</code> declaration, that was expected.</em><br/><br/>\n<b>So why is the version with a <code>static final myArray</code> slower than the one with <code>static myArray</code> ?</b><br/></p>\n\n<hr>\n\n<p><strong>EDIT (using Aubin's version of my snippet):</strong> </p>\n\n<p>It appears that the differences between the version with <code>final</code> keyword and the one without only lies in the first iteration. Somehow, the version with <code>final</code> is always slower than its counterpart without on the first iteration, then next iterations have similar timings.</p>\n\n<p>For example, with <code>THRESHOLD = 10^8</code> and running with 1.7.0u09 client the first computation takes approx 35s while the second 'only' takes 30s.</p>\n\n<p>Obviously the VM performed an optimization, was that the JIT in action and why didn't it kick earlier (for example by compiling the second level of the nested loop, this part was the hotspot) ?<br></p>\n\n<p><strong>Note that my remarks are still valid with 1.7.0u01 client VM. With that very version</strong> (and maybe earlier releases), <strong>the code with <code>final myArray</code> runs slower than the one without this keyword: 2671ms vs 2331ms on a basis of 200 iterations.</strong></p>\n"},{"tags":["mysql","performance","sphinx"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":36,"score":0,"question_id":13170509,"title":"Sphinx get millions of results","body":"<p>I use <strong>MySQL</strong> + <strong>Sphinx</strong> to store many millions rows of data. We have website to view all information from our database. </p>\n\n<p>For example, movie titles (<strong>100,000,000</strong> rows). I need to view ALL of them on our website, 100 titles per page. Also, i need view them sorted by actors popularity. </p>\n\n<p>For the first 10 pages all works great. But after that i reached <a href=\"http://sphinxsearch.com/docs/1.10/conf-max-matches.html\" rel=\"nofollow\">max_matches</a> limit. Increasing this limit will force sphinx to use more CPU/RAM.</p>\n\n<p>Also, i can't even set max_matches to <strong>20,000,000</strong>.</p>\n\n<blockquote>\n  <p>WARNING: max_matches=20000000 out of bounds; using default 1000</p>\n</blockquote>\n\n<p>I can use MySQL to perform queries like this:</p>\n\n<pre><code>SELECT * FROM titles WHERE tid &gt;= $start AND tid &lt;= $end\n</code></pre>\n\n<p>to use <strong>tid</strong> index. But i can't sort it by <strong>tid</strong>. I need to sort my titles by info from other tables.</p>\n\n<p>What is the best way to get access to many millions of rows, sorted and do it quick. Please help.</p>\n\n<p><strong>UPDATE</strong>: from sphinx source: <strong>/src/searchd.cpp</strong></p>\n\n<pre><code>if ( iMax&lt;0 || iMax&gt;10000000 )\n{\n    sphWarning ( \"max_matches=%d out of bounds; using default 1000\", iMax );\n} else\n{\n    g_iMaxMatches = iMax;\n}\n</code></pre>\n\n<p>Is <strong>10000000</strong> is a limit ? How can i get offset more than that ?</p>\n"},{"tags":["c#","winforms","performance","image"],"answer_count":4,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":89,"score":4,"question_id":13176192,"title":"What image format can c# load fastest?","body":"<p>this may seem like an odd question, and I don't know the formats of images so I'll just go ahead and ask...</p>\n\n<p>I'm making a minesweeper game (relevant to different things too) which utilizes a grid of buttons, then I'm adding a sprite to the buttons using <code>backgroundImage</code>. If the grid is 9x9 it's fine. 15x15 it slows down and 30x30 you can visibly see each button being loaded. </p>\n\n<p>That raises me to my question: Which image format would load fastest? Obviously, file size takes a part in the loading speed, however, I am enquiring as to if, say, a jpeg - with the same filesize as a gif - will load faster. or a bmp, png, etc.</p>\n\n<p>I'm asking this as to see if I can get my grid to load faster using a different format.</p>\n\n<p>Thanks!</p>\n"},{"tags":["php","mysql","performance","benchmarking"],"answer_count":5,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":78,"score":4,"question_id":13176469,"title":"PHP vs MySQL Performance ( if , functions ) in query","body":"<p>I just see this <a href=\"http://www.onextrapixel.com/2010/06/23/mysql-has-functions-part-5-php-vs-mysql-performance/\" rel=\"nofollow\">artice</a></p>\n\n<p>i need to know what's is best berformance in this cases</p>\n\n<p>if statment in query</p>\n\n<p><code>SELECT *,if( status = 1 , \"active\" ,\"unactive\") as status_val FROM comments</code></p>\n\n<p><strong>VS</strong></p>\n\n<pre><code>&lt;?php\n    $x = mysql_query(\"SELECT * FROM comments\");\n\n     while( $res = mysql_fetch_assoc( $x ) ){\n       if( $x['status'] == 1 ){\n             $status_val = 'active';\n        }else{\n             $status_val = 'unactive';\n        }\n     }\n  ?&gt;\n</code></pre>\n\n<p>Cut 10 from string</p>\n\n<p><code>SELECT * , SUBSTR(comment, 0, 10) as min_comment FROM comments</code></p>\n\n<p><strong>VS</strong></p>\n\n<pre><code>&lt;?php\n    $x = mysql_query(\"SELECT * FROM comments\");\n\n     while( $res = mysql_fetch_assoc( $x ) ){\n       $min_comment = substr( $x['comment'],0,10 ) ;\n     }\n  ?&gt;\n</code></pre>\n\n<p>etc ?????   <strong>and</strong> When i use MYSQL functions or PHP functions ?</p>\n"},{"tags":["ruby-on-rails-3","performance","redis","ohm"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":12730716,"title":"redis ohm slow performance","body":"<p>We're scratching our heads the last couple of days trying to understand why some things seem a bit too slow with redis/ohm on our rails project.</p>\n\n<p>The gist of it is that some requests to ohm/redis take around 100-200 ms, which seem rather high considering our high-expectations of blazing-fast performance.</p>\n\n<p>The command we're running looks like:</p>\n\n<p><code>Stats::TermStats.find(term_slug: 'term_slug', user_id: 123).to_a</code></p>\n\n<p>It seems to us that we're not doing anything extremely complex or crazy. Our profiling (using <a href=\"https://github.com/SamSaffron/MiniProfiler/tree/master/Ruby\" rel=\"nofollow\">miniprofiler</a>) of this single command so far revealed the following:</p>\n\n<ul>\n<li>Some of those calls complete within about 2-4 ms (is this ok/good/bad?)</li>\n<li>Some however take 100-200 ms (this definitely feels bad)</li>\n<li>Using <code>slowlog get</code> on the redis-client does not show anything particularly slow on redis. Most of the redis command complete within less than 20 <strong>microseconds</strong> (0.02 ms)</li>\n<li>Using the rails console and running a quick benchmark using the exact same slug/id in a loop, the same behaviour is observed, i.e. a few of those (same) requests seem to take considerably longer than most others</li>\n</ul>\n\n<p>Our redis config is pretty much out-of-the-box with basically no tweaking. During testing the server is not doing much else.</p>\n\n<p>Any suggestions how to improve performance / test what slows down things so much between redis and ohm/rails ??</p>\n"},{"tags":["flash","actionscript-3","performance","math","negative-number"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":177,"score":0,"question_id":6301965,"title":"Fastest way to make Negative numbers","body":"<p>Is there any Fastest way for this line?</p>\n\n<pre><code>ballAngelRadianVector = -ballAngelRadianVector;\n</code></pre>\n\n<p>and also this:</p>\n\n<pre><code>ballDegree = fee - ballDegree ;\n</code></pre>\n"},{"tags":["sql","sql-server","recursion","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":1467,"score":1,"question_id":1224623,"title":"WITH Common Table Expression vs. CREATE VIEW performance","body":"<p>I have several queries that use a WITH clause, or Common Table Expression, with a UNION ALL statement to recur through a table with a tree like structure in SQL server as described <a href=\"http://www.webinade.com/web-development/creating-recursive-sql-calls-for-tables-with-parent-child-relationships\" rel=\"nofollow\">here</a>.  Would I see a difference in performance if I were to CREATE that same VIEW instead of including it with the WITH clause and having it generated every time I run the query?  Would it generally be considered good practice to actually CREATE the view since it is used in several queries?</p>\n"},{"tags":["performance","sharepoint","search","claims-based-identity"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":13,"score":0,"question_id":13175402,"title":"Custom Claims Provider - The search request was unable to connect to the Search Service","body":"<p>I have a custom claims provider in a farm SharePoint 2010 and FAST search service.\nCustom claims provider is deployed as farm feature.</p>\n\n<p>If the farm feature claims provider is not activate the search works fine.\nIf I active the feature then I receive a random error in search results page:\n\"The search request was unable to connect to the Search Service\".\nWhat I have to check?</p>\n"},{"tags":["asp.net",".net","wcf","performance","appharbor"],"answer_count":5,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":394,"score":3,"question_id":9348598,"title":"How to improve performance on a .net wcf service","body":"<p>I'm porting a cpu-heavy .net 4.0 windows application to a .net 4.0 wcf service. Basically I just imported the .net classes to the wcf service.</p>\n\n<p>All is working well except for performance at the wcf service - a task that takes 6267947 ticks (2539ms) uses 815349861 ticks (13045ms) on the aspx.net wcf service running locally on the same develop machine.</p>\n\n<p>I allready have uploaded the service + a test client to appharbor where the performance is as bad as on my local machine - the link to my test app is: <a href=\"http://www.wsolver.com\" rel=\"nofollow\">http://www.wsolver.com/</a>. Any ideas on how I can improve performance?</p>\n"},{"tags":["performance","monitoring","esb","talend"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":9,"score":0,"question_id":13175163,"title":"Talend ESB Performance Monitoring","body":"<p>I've created a service job in Talend Open Studio for ESB 5.1.2 and I want to see the performance of specific sections of my job to discern which part is taking the longest to process.<br/><br/>\nI have used tChronometerStart and tChronometerStop to log timings for specific sections but I'd be interested to know if there are any overall monitoring features of Talend ESB that I'm not yet making use of. I am ultimately aiming to tune all of the steps but can imagine that the use of timers on each step will eventually get a bit Heisenberg.</p>\n\n<p>Thanks,</p>\n\n<p>mids</p>\n"},{"tags":["wcf","performance"],"answer_count":3,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":3092,"score":0,"question_id":264366,"title":"Performance Tuning WCF Service","body":"<p>What is the single most important performance tuning area for a WCF Web service?  </p>\n\n<ul>\n<li>ASP.net Thread settings?</li>\n<li>WCF throttling?</li>\n</ul>\n"},{"tags":["java","arrays","performance","recursion","splitting"],"answer_count":2,"favorite_count":1,"up_vote_count":8,"down_vote_count":0,"view_count":228,"score":8,"question_id":13156894,"title":"Performance of coin splitting algorithm","body":"<p>My question is about a CodeFu practice problem (2012 round 2 problem 3). It basically comes down to splitting an array of integers in two (almost) equal halves and returning the smallest possible difference between the two. I have included the problem description below. As noted in the comments this can be described as a <a href=\"http://stackoverflow.com/questions/12781159/balanced-partition\">balanced partition problem</a>, which is a problem in the realm of <a href=\"http://en.wikipedia.org/wiki/Dynamic_programming\" rel=\"nofollow\">dynamic programming</a>.</p>\n\n<p>Now similar problems have been discussed a lot, but I was unable find an efficient solution for this particular one. The problem is of course that the number of possible combinations to traverse soon grows too large for a brute force search (at least when using recursion). I have a recursive solution that works fine for all but the largest problem sets. I tried to add some optimizations that stop the recursion early, but the performance is still too slow to solve some arrays of the maximum length (30) within the 5 second maximum allowed by CodeFu. Any suggestions for how to improve or rewrite the code would be very welcome. I would also love to know if it might help to make the iterative version.</p>\n\n<p><strong>Update:</strong> on <a href=\"http://people.csail.mit.edu/bdean/6.046/dp/\" rel=\"nofollow\">this fine site</a> there is a theoretical discussion of the balanced partition problem, which gives a good idea of how to go about and solve this in a dynamic way. That is really what I am after, but I do not know how to put the theory into practice exactly. The movie mentions that the elements in the two subcollections can be found \"using the old trick of back pointers\", but I don't see how.</p>\n\n<h2>Problem</h2>\n\n<blockquote>\n  <p>You and your friend have a number of coins with various amounts. You\n  need to split the coins in two groups so that the difference between\n  those groups in minimal.</p>\n  \n  <p>E.g.   Coins of sizes 1,1,1,3,5,10,18 can be split as:  1,1,1,3,5 and\n  10,18  1,1,1,3,5,10 and 18  or  1,1,3,5,10 and 1,18 The third\n  combination is favorable as in that case the difference between the\n  groups is only 1.    Constraints:   coins will have between 2 and 30\n  elements inclusive   each element of coins will be between 1 and\n  100000 inclusive</p>\n  \n  <p>Return value:   Minimal difference possible when coins are split into\n  two groups</p>\n</blockquote>\n\n<p>NOTE: the CodeFu rules state that the execution time on CodeFu's server may be no more than 5 seconds.</p>\n\n<h2>Main Code</h2>\n\n<pre><code>Arrays.sort(coins);\n\nlower = Arrays.copyOfRange(coins, 0,coins.length-1);\n//(after sorting) put the largest element in upper\nupper = Arrays.copyOfRange(coins, coins.length-1,coins.length);            \n\nsmallestDifference = Math.abs(arraySum(upper) - arraySum(lower));\nreturn findSmallestDifference(lower, upper, arraySum(lower), arraySum(upper), smallestDifference);\n</code></pre>\n\n<h2>Recursion Code</h2>\n\n<pre><code>private int findSmallestDifference (int[] lower, int[] upper, int lowerSum, int upperSum, int smallestDifference) {\n    int[] newUpper = null, newLower = null;\n    int currentDifference = Math.abs(upperSum-lowerSum);\n    if (currentDifference &lt; smallestDifference) {\n        smallestDifference = currentDifference;\n    } \n    if (lowerSum &lt; upperSum || lower.length &lt; upper.length || lower[0] &gt; currentDifference \n            || lower[lower.length-1] &gt; currentDifference \n            || lower[lower.length-1] &lt; upper[0]/lower.length) {\n        return smallestDifference;\n    }\n    for (int i = lower.length-1; i &gt;= 0 &amp;&amp; smallestDifference &gt; 0; i--) {           \n       newUpper = addElement(upper, lower[i]);\n       newLower = removeElementAt(lower, i);\n       smallestDifference = findSmallestDifference(newLower, newUpper, \n               lowerSum - lower[i], upperSum + lower [i], smallestDifference);\n    }\n    return smallestDifference;\n}\n</code></pre>\n\n<h2>Data Set</h2>\n\n<p>Here is an example of a set that takes too long to solve.</p>\n\n<blockquote>\n  <p>{100000,60000,60000,60000,60000,60000,60000,60000,60000,\n              60000,60000,60000,60000,60000,60000,60000,60000,60000,\n              60000,60000,60000,60000,60000,60000,60000,60000,60000,\n              60000,60000,60000}</p>\n</blockquote>\n\n<p>If you would like the entire source code, I have put it on <a href=\"http://ideone.com/weHDoP\" rel=\"nofollow\">Ideone</a>.</p>\n"},{"tags":["wpf","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":98,"score":2,"question_id":11575142,"title":"WPF Application takes too much CPU for some users","body":"<p>I have a WPF application written in VS2010 Express for .Net 4. Some testers report that the application takes alot of CPU (~80%-100%) whenever the application window is visible and 0% when not visible. This is not the case on my computer, nor on the other computers i have tested. What I see is that it takes a bunch of CPU for the first few seconds after opening the window, but then goes down to a few percent.</p>\n\n<p>My suspicion is that this is an issue with the rendering of the WPF window. But i can not understand why it only occurs for some users. </p>\n\n<p>I understand that this is not much to go on, but if anyone have an idea where to start looking for the cause of this performance issue, it would be much appreciated.  </p>\n\n<p>Thank you!</p>\n"},{"tags":["c#","performance","bitmap","contrast"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":2020,"score":2,"question_id":8990926,"title":"Faster contrast algorithm for a bitmap","body":"<p>I have a tool with trackbar slider controls used to adjust an image's brightness, contrast, gamma, etc.</p>\n\n<p>I am trying to get real-time updates to my image while the user drags the slider.  The brightness and gamma algorithms are an acceptable speed (around 170ms).  But the contrast algorithm is about 380ms.</p>\n\n<p>Basically my form is a tool window with sliders.  Each time the image is updated, it sends an event to the parent which redraws the new image.  The tool window keeps the original unmodified image locked in memory so I always have access to the bytes of it.  So basically I do this each time the ValueChanged event for a slider (such as the Contrast slider) is changed.</p>\n\n<ul>\n<li>LockBits of the working (destination) bitmap as Format24bppRgb (original bitmap is in Format32bppPArgb)</li>\n<li>Marshal.Copy the bits to a byte[] array</li>\n<li>Check which operation I'm doing (which slider was chosen)</li>\n<li>Use the following code for Contrast:</li>\n</ul>\n\n<p>Code:</p>\n\n<pre><code>double newValue = 0;\ndouble c = (100.0 + contrast) / 100.0;\n\nc *= c;\n\nfor (int i = 0; i &lt; sourcePixels.Length; i++)\n{\n    newValue = sourcePixels[i];\n\n    newValue /= 255.0;\n    newValue -= 0.5;\n    newValue *= c;\n    newValue += 0.5;\n    newValue *= 255;\n\n    if (newValue &lt; 0)\n        newValue = 0;\n    if (newValue &gt; 255)\n        newValue = 255;\n\n    destPixels[i] = (byte)newValue;\n}\n</code></pre>\n\n<p>I read once about using integer instead of floating point values to increase the speed of contrast, but I couldn't find that article again.</p>\n\n<p>I tried using unsafe code (pointers) but actually noticed a speed decrease.  I assume it was because the code was using nested for loops to iterate x and y instead of a single loop.</p>\n"},{"tags":["asp.net","performance","hang","debugdiag"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":29,"score":0,"question_id":13173253,"title":"slow/hanging Asp.net web application - full dump shows several threads waiting for lock","body":"<p>I have a web application and sometimes it is hanging / performing very slow.  I have taken a full dump using DebugDiag, and tried to analyse it using the Crash/Hang analysis. </p>\n\n<p>The summary gave me that 7.86% of my threads (10) are blocked and waiting for <code>Monitor.Wait</code>.</p>\n\n<p>However, when I check the Call Stack / Stack Trace with the thread, the below is outputted:</p>\n\n<pre><code>.NET Call Stack\n\n\n\nFunction \nSystem.Threading.Monitor.ObjWait(Boolean, Int32, System.Object) \nSystem.Threading.Monitor.Wait(System.Object, Int32, Boolean) \nQuartz.Simpl.SimpleThreadPool+WorkerThread.Run() \nSystem.Threading.ThreadHelper.ThreadStart_Context(System.Object) \nSystem.Threading.ExecutionContext.runTryCode(System.Object) \nSystem.Runtime.CompilerServices.RuntimeHelpers.ExecuteCodeWithGuaranteedCleanup(TryCode, CleanupCode, System.Object) \nSystem.Threading.ExecutionContext.RunInternal(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object) \nSystem.Threading.ExecutionContext.Run(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object, Boolean) \nSystem.Threading.ExecutionContext.Run(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object) \nSystem.Threading.ThreadHelper.ThreadStart() \n\n\nFull Call Stack\n\n\n\nFunction   Source \nntdll!NtWaitForMultipleObjects+15    \nKERNELBASE!WaitForMultipleObjectsEx+100    \nkernel32!WaitForMultipleObjectsExImplementation+e0    \nclr!WaitForMultipleObjectsEx_SO_TOLERANT+56    \nclr!Thread::DoAppropriateAptStateWait+4d    \nclr!Thread::DoAppropriateWaitWorker+17d    \nclr!Thread::DoAppropriateWait+60    \nclr!CLREvent::WaitEx+106    \nclr!CLREvent::Wait+19    \nclr!Thread::Wait+1d    \nclr!Thread::Block+1a    \nclr!SyncBlock::Wait+169    \nclr!ObjHeader::Wait+2c    \nclr!ObjectNative::WaitTimeout+147    \nSystem.Threading.Monitor.Wait(System.Object, Int32, Boolean)    \nSystem.Threading.ThreadHelper.ThreadStart_Context(System.Object)    \nSystem.Threading.ExecutionContext.runTryCode(System.Object)    \nclr!CallDescrWorker+33    \nclr!CallDescrWorkerWithHandler+8e    \nclr!MethodDesc::CallDescr+194    \nclr!MethodDesc::CallTargetWorker+21    \nclr!MethodDescCallSite::Call+1c    \nclr!ExecuteCodeWithGuaranteedCleanupHelper+bb    \nclr!ReflectionInvocation::ExecuteCodeWithGuaranteedCleanup+138    \nSystem.Threading.ExecutionContext.RunInternal(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object)    \nSystem.Threading.ExecutionContext.Run(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object)    \nSystem.Threading.ThreadHelper.ThreadStart()    \nclr!CallDescrWorker+33    \nclr!CallDescrWorkerWithHandler+8e    \nclr!MethodDesc::CallDescr+194    \nclr!MethodDesc::CallTargetWorker+21    \nclr!ThreadNative::KickOffThread_Worker+1e1    \nclr!Thread::DoExtraWorkForFinalizer+114    \nclr!Thread::ShouldChangeAbortToUnload+101    \nclr!Thread::ShouldChangeAbortToUnload+399    \nclr!Thread::RaiseCrossContextException+3f8    \nclr!Thread::DoADCallBack+358    \nclr!Thread::DoExtraWorkForFinalizer+fa    \nclr!Thread::ShouldChangeAbortToUnload+101    \nclr!Thread::ShouldChangeAbortToUnload+399    \nclr!Thread::ShouldChangeAbortToUnload+43a    \nclr!ManagedThreadBase::KickOff+15    \nclr!ThreadNative::KickOffThread+23e    \nclr!Thread::intermediateThreadProc+4b    \nkernel32!BaseThreadInitThunk+e    \nntdll!__RtlUserThreadStart+70    \nntdll!_RtlUserThreadStart+1b \n</code></pre>\n\n<p>It doesn't actually show me which lock they are waiting to obtain - Any idea on how to get this information?</p>\n"},{"tags":["javascript","performance","algorithm","optimization"],"answer_count":23,"favorite_count":43,"up_vote_count":214,"down_vote_count":16,"view_count":9474,"score":198,"question_id":13136724,"title":"Why is i-- faster than i++ in loops?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/1340589/javascript-are-loops-really-faster-in-reverse\">JavaScript - Are loops really faster in reverse…?</a>  </p>\n</blockquote>\n\n\n\n<p>I don't know if this question is valid in other languages or not, but I'm asking this specifically for <code>JavaScript</code>.</p>\n\n<p>I see in some articles and questions that the fastest loop in JavaScript is something like:</p>\n\n<pre><code>for(var i = array.length; i--; )\n</code></pre>\n\n<p>Also in Sublime Text 2, when you try to write a loop, it suggests:</p>\n\n<pre><code>for (var i = Things.length - 1; i &gt;= 0; i--) {\n    Things[i]\n};\n</code></pre>\n\n<p>I want to know, why is <code>i--</code> faster than <code>i++</code> in loops?</p>\n"},{"tags":["javascript","performance","variables","ecmascript-harmony","ecmascript-4"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":107,"score":1,"question_id":12379191,"title":"Javascript optional type hinting","body":"<p>When a programming language is statically typed, the compiler can be more precise about memory allocation and thus be generally more performant (with all other things equal).</p>\n\n<p>I believe ES4 introduced optional type hinting (from what I understand, Adobe had a huge part in contributing to its spec due to actionscript).  Does javascript officially support type hinting as a result?  Will ES6 support optional type hinting for native variables?</p>\n\n<p>If Javascript does support type hinting, are there any benchmarks that show how it pays off in terms of performance?  I have not seen an open source project use this yet.</p>\n"},{"tags":["css","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":104,"score":2,"question_id":9962081,"title":"Which should I use? (performance)","body":"<p>I want to know a simple thing:</p>\n\n<p>when setting up a style that is inherited by all its children, is it recommended to be most specific?</p>\n\n<blockquote>\n  <p><strong>Structure:</strong> html > body > parent_content > wrapper > p</p>\n</blockquote>\n\n<p>I want to apply a style to <code>p</code> but respecting these:</p>\n\n<ul>\n<li>I don't care having <code>parent_content</code> or <code>wrapper</code> having the style</li>\n<li>I do care changing the <code>html</code> or <code>body</code> style (or all <code>p</code>)</li>\n</ul>\n\n<p>So what should I use?</p>\n\n<pre><code>#parent_content{\n    color:#555;\n}\n\n#parent_content p{\n    color:#555;\n}\n\n#wrapper{\n    color:#555;\n}\n\n#wrapper p{\n    color:#555;\n}\n\n/*...etc...*/\n</code></pre>\n\n<p>Also, some links to tutorials about this would be great</p>\n"},{"tags":["asp.net","performance","cookies","yslow"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":24,"score":2,"question_id":13169140,"title":"How do I make my asp.net website \"more cookie free\"?","body":"<p>I ran my website through Yahoo's <a href=\"http://developer.yahoo.com/yslow/\" rel=\"nofollow\">YSlow</a> on my asp.net (vb) website that has 47 pages. There were a few problems, but one of them said I get a \"<strong>Grade F on Use cookie-free domains</strong>\".</p>\n\n<p>Specifically, it says:</p>\n\n<p><em><strong>When the browser requests a static image and sends cookies with the request, the server ignores the cookies. These cookies are unnecessary network traffic. To workaround this problem, make sure that static components are requested with cookie-free requests by creating a subdomain and hosting them there.</em></strong></p>\n\n<p>I really don't know what they're trying to tell me.  They say 43 components on my homepage aren't cookie-free, including:  site.css, print.css, homeslider.js, and then 38 or 39 .jpg or .png images aren't cookie-free.</p>\n\n<p>Does anybody know how I can improve this and improve my site's performance?  Thank you for any suggestions you can offer!</p>\n"},{"tags":["perl","shell","performance","benchmarking"],"answer_count":6,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":1422,"score":5,"question_id":2703938,"title":"Is it possible to write a shell script which is faster than the equivalent script in Perl?","body":"<p>I wrote multiple scripts in Perl and shell and I have compared the real execution time. In all the cases, the Perl script was more than 10 times faster than the shell script.</p>\n\n<p>So I wondered if it possible to write a shell script which is faster than the same script in Perl? And why is Perl faster than shell although I use the <a href=\"http://perldoc.perl.org/functions/system.html\" rel=\"nofollow\">system</a> function in Perl script?</p>\n"},{"tags":["sql","performance","sql-server-2005","sql-server-2012"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":4,"view_count":820,"score":-4,"question_id":10926841,"title":"Sql Server 2012 slower than 2005?","body":"<p>We have deployed SQL Server 2012 Enterprise and we have performance issues:</p>\n\n<ul>\n<li>same data (backuped from our SQL Server 2005 Enterprise and restored on 2012)</li>\n<li>test script of 3200 sql SELECT statements</li>\n</ul>\n\n<p>We do tests using Management Studio:</p>\n\n<ol>\n<li>results as plain text</li>\n<li>results in a file</li>\n</ol>\n\n<p>On same computer:</p>\n\n<ol>\n<li>2005 : 15 sec, 2012 : 2 min</li>\n<li>2005 : 14 sec, 2012 : 30 sec</li>\n</ol>\n\n<p>Even with a more powerful computer, 2012 is still slower than 2005.</p>\n\n<p>What can be wrong? The way we installed SQL Server 2012 and default parameters? The way we restored the backup? What can we do?</p>\n"},{"tags":["database","performance","updates"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":12850180,"title":"MySQL: Direct Update or Select before a conditional Update?","body":"<p>Just curious to know- if we need to do a conditional Update in a <strong>large</strong> table, then which is the best approach-</p>\n\n<p>Directly doing an Update or check for existing entry before Update.</p>\n\n<pre><code>function doDirectUpdate()\n{\n   // UPDATE table WHERE condn\n}\n</code></pre>\n\n<p>OR</p>\n\n<pre><code>function doCheckAndUpdate()\n{\n   // SELECT COUNT(id) AS exist FROM table WHERE condn\n   if(id exists)\n   {\n      // UPDATE table WHERE condn\n   }\n   else\n   {\n      echo 'No matching entry';\n   }\n}\n</code></pre>\n"},{"tags":["c++","performance","for-loop","c++11","move-semantics"],"answer_count":3,"favorite_count":0,"up_vote_count":10,"down_vote_count":1,"view_count":196,"score":9,"question_id":13130708,"title":"What is the advantage of using universal references in range-based for loops?","body":"<p><code>const auto&amp;</code> would suffice if I want to perform read-only operations. However, I have bumped into</p>\n\n<pre><code>for (auto&amp;&amp; e : v)  // v is non-const\n</code></pre>\n\n<p>a couple of times recently. This makes me wonder:</p>\n\n<p>Is it possible that in some obscure corner cases there is some performance benefit in using universal references, compared to <code>auto&amp;</code> or <code>const auto&amp;</code>?</p>\n\n<p>(<code>shared_ptr</code> is a suspect for obscure corner cases)</p>\n\n<hr>\n\n<p><strong>Update</strong>\nTwo examples that I found in my favorites:</p>\n\n<p><a href=\"http://stackoverflow.com/a/13057846/341970\">Any disadvantage of using const reference when iterating over basic types?</a><br/>\n<a href=\"http://stackoverflow.com/a/13087074/341970\">Can I easily iterate over the values of a map using a range-based for loop?</a></p>\n\n<p>Please concentrate on the question: <strong>why would I want to use auto&amp;&amp; in range-based for loops?</strong></p>\n"},{"tags":["performance","azure","monitoring","windows-azure-storage","performance-counters"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":27,"score":0,"question_id":13154908,"title":"How do I monitor performance of Azure Storage operations in my code?","body":"<p>I have a web role that uses Azure Storage while serving requests. Now I see that requests are served damn slow - like dozens times slower that usual. I've looked over the performance counters saved by the role - they look usual to me. I've checked connectivity from my network to the role - it looks okay.</p>\n\n<p>I suspect the problem might be with Azure Storage operations invoked from inside my service being rather slow for some periods of time and that would slow serving requests down.</p>\n\n<p>How do I continuously gather data that would help me estimate performance of these requests? Is there anything like a performance counter that I could enable or anything similar?</p>\n"},{"tags":["performance","localhost","ruby-1.9.3","slow-load"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":13014345,"title":"rails app extremely slow","body":"<p>I know there are lots of similar questions but I wasn't able to find a solution from any of those questions. So here we go..\nI am using Ubuntu 12.04 on a sony vaio. I am running Rails 3.2.8 with Ruby1.9.3-p194. I am using thin server instead of webbrick. I have mongoid, elasticseaerch and redis running as well.</p>\n\n<p>the page load is taking extremely long (over 10+ mins) on localhost. I know the app is not slow because it works on my colleagues' macbooks just fine. I am not sure what is causing it to have a very high loading time. </p>\n\n<p>any tips is appreciated. i am really sad/unhappy as it is slowing me down considerably at work. i asked my colleagues but none of them seem to know the answer.</p>\n\n<p>the app runs fine on company's production machine. it's only on my laptop that seems to be the problem. keep in mind that I have a very fast laptop (i5, 6gb RAM)</p>\n"},{"tags":["c","performance","optimization","time-complexity","codechef"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":2,"view_count":82,"score":-2,"question_id":13028801,"title":"Optimizing C code -> Codechef solution time exceeded","body":"<p>What are tips and tricks so that my code can be executed in given time limit. May be some logical changes or anything.</p>\n\n<p>When I submit my solution in codechef it shows time limit exceeded but I think it is fast enough simple code. I am amateur in programming any help and suggestions would be helpful. Guide me so that it could be executed within the time limit i.e. 3 seconds. Thank you !!</p>\n"},{"tags":["performance","optimization","opencv"],"answer_count":2,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":46,"score":4,"question_id":13128696,"title":"Which is the most efficient way to do alpha mask in opencv?","body":"<p>I know OpenCV only supports binary masks.<br>\nBut I need to do an overlay where I have a grayscale mask that specifies transparency of the overlay. </p>\n\n<p>Eg. if a pixel in the mask is 50% white it should mean a <code>cv::addWeighted</code> operation for that pixel with alpha=beta=0.5, gamma = 0.0.</p>\n\n<p>Now, if there is no opencv library function, what algorithm would you suggest as the most efficient?</p>\n"},{"tags":["objective-c","performance","cocoa","exception","error-handling"],"answer_count":2,"favorite_count":0,"up_vote_count":8,"down_vote_count":0,"view_count":162,"score":8,"question_id":3683439,"title":"What is the cost of using exceptions in Objective-C?","body":"<p>I mean in the current implementation of clang or the gcc version.</p>\n\n<p>C++ and Java guys always tell me that exceptions do not cost any performance unless they are thrown. Is the same true for Objective-C? </p>\n"},{"tags":["eclipse","performance","eclipse-wtp"],"answer_count":6,"favorite_count":3,"up_vote_count":13,"down_vote_count":0,"view_count":2370,"score":13,"question_id":11639108,"title":"Eclipse Juno + WTP +EGit dead slow","body":"<p>I'm trying to use Eclipse Juno (Version: 4.2.0 Build id: I20120608-1400) with WTP for JavaScript/Node.js development on MacOSX Lion, on my 4GB RAM MacBook Pro. Sometimes it gets dead slow, and unusable. I've tried tweaking both through preferences (disabled all validators) and initialization variables, and also upgraded to 1.7 VM as recommended. Here's the contents of my eclipse.ini file:</p>\n\n<pre><code>-startup\n../../../plugins/org.eclipse.equinox.launcher_1.3.0.v20120522-1813.jar\n--launcher.library\n../../../plugins/org.eclipse.equinox.launcher.cocoa.macosx.x86_64_1.1.200.v20120522-1813\n-showsplash\norg.eclipse.platform\n--launcher.XXMaxPermSize\n256m\n--launcher.defaultAction\nopenFile\n-vm\n/Library/Java/JavaVirtualMachines/1.7.0.jdk/Contents/Home/bin/java\n-vmargs\n-Xms64m\n-Xmx256m\n-Xdock:icon=../Resources/Eclipse.icns\n-XstartOnFirstThread\n-Dorg.eclipse.swt.internal.carbon.smallFonts\n-XX:+UseParallelGC\n-XX:+UseCompressedOops\n-XX:MaxPermSize=256m\n-server\n</code></pre>\n\n<p>Can someone advise what I could do to improve WTP performance?</p>\n"},{"tags":["c#",".net","performance","task-parallel-library"],"answer_count":5,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":84,"score":2,"question_id":13142099,"title":"Making the most of the .NET Task Parallel Library","body":"<p><strong>Question 1.</strong></p>\n\n<p><em>Is using Parallel.For and Parallel.ForEach better suited to working with tasks that are ordered or unordered?</em></p>\n\n<p>My reason for asking is that I recently updated a serial loop where a StringBuilder was being used to generate a SQL statement based on various parameters. The result was that the SQL was a bit jumbled up (to the point it contained syntax errors) in comparison to when using a standard foreach loop, therefore my gut feeling is that TPL is not suited to performing tasks where the data must appear in a particular order.</p>\n\n<p><strong>Question 2.</strong></p>\n\n<p><em>Does the TPL automatically make use of multicore architectures of must I provision anything prior to execution?</em></p>\n\n<p>My reason for asking this relates back to an eariler question I asked relating to performance profiling of TPL operations. An answer to the question enlightened me to the fact that TPL is not always more efficient than a standard serial loop as the application may not have access to multiple cores, and therefore the overhead of creating additional threads and loops would create a performance decrease in comparison to a standard serial loop.</p>\n"},{"tags":["performance","graphics","time-complexity","raytracing"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":40,"score":2,"question_id":13138841,"title":"Performance of Ray Tracing","body":"<p>I wonder whether we can formulate the performance of ray-tracing. In my primitive ray-tracer, performance depends on this formula mostly: width x height x number of sampler x (number of objects + number of lights)</p>\n\n<p>So for example, in Pixar or any other big companies, do they follow such formula for performance evaluation. Isn't performance depends on triangle count of the objects? For example if I want to calculate roughly the maximum render time of the frame of 1000x1000 with average 500 objects that consists 5.000.000 triangles, would it be possible?</p>\n"},{"tags":["performance","postgresql"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":106,"score":3,"question_id":13014277,"title":"Why is this count query so slow?","body":"<p>Hi I'm hosted on Heroku running postgresql 9.1.6 on a their Ika plan (7,5gb ram). I have a table called cars. I need to do the following:</p>\n\n<pre><code>SELECT COUNT(*) FROM \"cars\" WHERE \"cars\".\"reference_id\" = 'toyota_hilux'\n</code></pre>\n\n<p>Now this takes an awful lot of time (64 sec!!!)</p>\n\n<pre><code>Aggregate  (cost=2849.52..2849.52 rows=1 width=0) (actual time=63388.390..63388.391 rows=1 loops=1)\n  -&gt;  Bitmap Heap Scan on cars  (cost=24.76..2848.78 rows=1464 width=0) (actual time=1169.581..63387.361 rows=739 loops=1)\n        Recheck Cond: ((reference_id)::text = 'toyota_hilux'::text)\n        -&gt;  Bitmap Index Scan on index_cars_on_reference_id  (cost=0.00..24.69 rows=1464 width=0) (actual time=547.530..547.530 rows=832 loops=1)\n              Index Cond: ((reference_id)::text = 'toyota_hilux'::text)\nTotal runtime: 64112.412 ms\n</code></pre>\n\n<p>A little background:</p>\n\n<p>The table holds around 3.2m rows, and the column that I'm trying to count on, has the following setup:</p>\n\n<pre><code>reference_id character varying(50);\n</code></pre>\n\n<p>and index:</p>\n\n<pre><code>CREATE INDEX index_cars_on_reference_id\n  ON cars\n  USING btree\n  (reference_id COLLATE pg_catalog.\"default\" );\n</code></pre>\n\n<p>What am I doing wrong? I expect that this performance is not what I should expect - or should I?</p>\n"},{"tags":["c","programming-languages","loops","performance"],"answer_count":15,"favorite_count":14,"up_vote_count":63,"down_vote_count":1,"view_count":4280,"score":62,"question_id":2823043,"title":"Is it faster to count down than it is to count up?","body":"<p>Our computer science teacher once said that for some reason it is more efficient to count down than to count up.\nFor example if you need to use a FOR loop and the loop index is not used somewhere (like printing a line of N * to the screen)\nI mean that code like this:</p>\n\n<pre><code>for (i = N; i &gt;= 0; i--)  \n  putchar('*');  \n</code></pre>\n\n<p>is better than:</p>\n\n<pre><code>for (i = 0; i &lt; N; i++)  \n  putchar('*');  \n</code></pre>\n\n<p>Is it really true? And if so, does anyone know why?</p>\n"},{"tags":["asp.net","vb.net","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":45,"score":1,"question_id":13162475,"title":"Why am I unable to specify height and width for images on my asp.net website? (html output included)","body":"<p>Whenever I run a speed test on Google or <a href=\"http://gtmetrix.com\" rel=\"nofollow\">GMetrix</a>, they always give me an F because they say my image sizes aren't specified, so more http requests are required.  But on my asp.net (vb) website, the links they specify are linkbuttons and look like this <strong>using CssClass</strong>:</p>\n\n<pre><code>&lt;asp:Image ID=\"Image9\" runat=\"server\" ImageUrl=\"~/images/flagpt.png\" \nCssClass=\"flagbutton\" tooltip=\"Veja este site em portuguÃªs\" \ntitle=\"Veja este site em portuguÃªs\"/&gt;\n</code></pre>\n\n<p>This results in this <strong>html output</strong> (using CssClass) but still doesn't have img dimensions:</p>\n\n<pre><code>&lt;img id=\"Image9\" title=\"Veja este site em portuguÃªs\" \n class=\"flagbutton\" title=\"Veja este site em portuguÃªs\" \n src=\"images/flagpt.png\" /&gt;\n</code></pre>\n\n<p>But if I just put <strong>height=\"15\" width=\"26\"</strong> like this, it still <strong>doesn't</strong> work:</p>\n\n<pre><code>  &lt;asp:Image ID=\"Image7\" runat=\"server\" ImageUrl=\"~/images/flagde.png\" \n  height=\"15\" width=\"26\" tooltip=\"View this website in Deutsch\" \n  title=\"View this website in Deutsch\"/&gt;\n</code></pre>\n\n<p>This results in this <strong>html output</strong>, but speed tests don't detect img dimensions:</p>\n\n<pre><code>&lt;img id=\"Image7\" title=\"View this website in Deutsch\" \n   title=\"View this website in Deutsch\" \n   src=\"images/flagde.png\" style=\"height:15px;width:26px;\" /&gt;\n</code></pre>\n\n<p>The speed tests still suggest I'm not specifying my dimensions.  Any suggestions to help me with this would be greatly appreciated!</p>\n"},{"tags":["eclipse","performance","optimization","eclipse-juno"],"answer_count":5,"favorite_count":4,"up_vote_count":15,"down_vote_count":0,"view_count":7792,"score":15,"question_id":11446825,"title":"Very slow Eclipse 4.2, how to make it more responsive?","body":"<p>I'm using Eclipse PDT on a rather large PHP project and the IDE is almost unusable. It takes nearly 30 seconds to open a file, and other actions, like selecting a folder in the file explorer, editing some text, etc. are equally slow.</p>\n\n<p>I followed various instructions to speed it up but nothing seems to work. This is my current <code>eclipse.ini</code> file. Any idea how I can improve it?</p>\n\n<pre><code>-startup\nplugins/org.eclipse.equinox.launcher_1.3.0.v20120522-1813.jar\n--launcher.library\nplugins/org.eclipse.equinox.launcher.win32.win32.x86_1.1.200.v20120522-1813\n-showsplash\norg.eclipse.platform\n--launcher.XXMaxPermSize\n256m\n--launcher.defaultAction\nopenFile\n-vmargs\n-server\n-Dosgi.requiredJavaVersion=1.7\n-Xmn128m\n-Xms1024m\n-Xmx1024m\n-Xss2m\n-XX:PermSize=128m\n-XX:MaxPermSize=128m\n-XX:+UseParallelGC\n</code></pre>\n\n<p>System: Eclipse 4.2.0, Windows 7, 4 GB RAM</p>\n"},{"tags":["javascript","performance","design","extjs"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":67,"score":2,"question_id":13162570,"title":"ExtJS 4.1 - Store.add() (followed by sync) vs Model.save()","body":"<p><strong>FIRST:</strong> I realize this question has been asked here: <a href=\"http://stackoverflow.com/questions/10115001/in-extjs-is-it-better-to-call-model-save-or-store-sync\">in ExtJS, is it better to call Model.save() or Store.Sync()?</a> - however I wish to examine this further, specifically regarding minimizing XHR's and unnecessary overhead on both the client and server. I do not feel either of these points were addressed in the linked question.</p>\n\n<p>I have a somewhat large application designed for enterprise resource management, consisting of many models, views and controllers. I handle all responses from my server by establishing a listener to Ext.Ajax <code>requestComplete</code> and <code>requestException</code> events. I took this approach rather than writing duplicate event handlers on every model's proxy <code>afterRequest</code> event. This enables me to have all of my back-end (using the Zend Framework) controllers responding with three parameters: <code>success</code>, <code>message</code> and <code>data</code>.</p>\n\n<p>After a successful request (i.e., HTTP 200), the method run for <code>requestComplete</code> will inspect the JSON response for the aforementioned parameters. If <code>success</code> is <code>false</code>, it is expected that there will be an error message contained in <code>message</code>, which is then displayed to the user (e.g. 'There was a problem saving that product. Invalid product name'). If success is true, action is taken depending on the type of request, i.e., Create, Read, Update or Destroy. After a successful <code>create</code>, the new record is added to the appropriate data store, after delete the record is destroyed, and so forth.</p>\n\n<p>I chose to take this approach rather than adding records to a store and calling the store's <code>sync</code> method in order to minimize XHR's and otherwise round trips. My current means of saving/updating data is to send the request to the backend and react to the result on the Ext front end. I do this by populating a model with data and calling model.save() for create/update requests, or model.destroy() to remove the data.</p>\n\n<p>I found that when adding/updating/removing records from a store, then calling store.sync(), I would have to react to server's response in a way that felt awkward. Take for example, deleting a record:</p>\n\n<ol>\n<li>First, remove the record from the store via <code>store.remove()</code></li>\n<li>Invoke <code>store.sync()</code> as I have store's <code>autoSync</code> set to <code>false</code>.</li>\n<li>This fires the AJAX destroy request from the store's model proxy.</li>\n<li>Here's where it gets weird.... if there is an error on the server while dropping the row from the database, the response will return <code>success: false</code>, however the record will have already been removed from the ExtJS Data Store.</li>\n<li>At this point, I can either call <code>store.sync()</code>, <code>store.load()</code> (both requiring a round trip) or get the record from the request and add it back to the store followed by a <code>commitChanges()</code> to avoid calling an additional sync/load and thus avoiding an unnecessary round trip.</li>\n</ol>\n\n<p>The same goes for adding records, if the server fails somewhere while adding data to the database, the record is still in the ExtJS store and must be removed manually to avoid a round trip with <code>store.sync()</code> or <code>store.load()</code>.</p>\n\n<p>In order to avoid this whole issue, as I previously explained, I instantiate one of my model objects (e.g. a Product model), populate it with data, and call <code>myModel.save()</code>. This, in turn, invokes the proxy's <code>create</code> or <code>update</code> depending on the ID of the model, and fires the appropriate AJAX request. In the event that the back-end fails, the front-end store is still unchanged. On successful requests (read: <code>success: true</code>, not HTTP 200), I manually add the record to the store and invoke <code>store.commitChanges(true)</code>, effectively syncing the store with the database without an additional round trip and avoiding unnecessary overhead. For all requests, the server will respond with the new/modified data as well as a success parameter, and conditionally a message to display on the client.</p>\n\n<p>Am I missing something here, or is this approach a good way to minimize XHR's and server/client overhead? I am happy to provide example code should that be requested, however I feel that this is a rather general concept with fundamental code. </p>\n"},{"tags":["c#","arrays","performance","x86","64bit"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":4,"view_count":49,"score":-4,"question_id":13162987,"title":"Most efficient array size in bytes in C# for class references for x86 and x64","body":"<p>I guess what I'm asking is, at what size in bytes does the first non-linear slowdown in accessing such an array come? Or, what's a good size for a small, very fast array?</p>\n\n<p>Note: Single-dimension array.</p>\n\n<p>Thanks.</p>\n\n<p>The object pools I've tested, themselves. Note that they're in order of speed:</p>\n\n<pre><code>using System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\n\nnamespace ObjectPool\n{\n    public sealed class FastPool&lt;T&gt;\n    {\n        private List&lt;Ref&gt; available;\n        private List&lt;Ref&gt; unavailable;\n        private List&lt;int&gt; availableUnavailables;\n        private Func&lt;T&gt; createFunc;\n        private int counter = 0;\n\n\n        public FastPool( int capacity, Func&lt;T&gt; createFunc )\n        {\n            available = new List&lt;Ref&gt;( 0 );\n            unavailable = new List&lt;Ref&gt;( 0 );\n            availableUnavailables = new List&lt;int&gt;( 0 );\n            this.createFunc = createFunc;\n            Increase( capacity );\n        }\n\n\n        public Ref Get( )\n        {\n            if ( counter == -1 )\n                Increase( available.Capacity );\n            if ( counter &lt; 0 )\n                throw new IndexOutOfRangeException( );\n            if ( counter &gt;= available.Count )\n                throw new NotImplementedException( );\n\n            var r = available[ counter ];\n            int i = availableUnavailables[ counter ];\n            --counter;\n            unavailable[ i ] = r;\n            r.index = i;\n            return r;\n        }\n\n\n        private void Increase( int addInstances )\n        {\n            int prevCapacity = available.Capacity;\n            available.Capacity += addInstances;\n            unavailable.Capacity += addInstances;\n            availableUnavailables.Capacity += addInstances;\n            for ( int t = 0 ; t &lt; addInstances ; ++t )\n            {\n                available.Add( new Ref( this, createFunc( ), -1 ) );\n                unavailable.Add( null );\n                availableUnavailables.Add( t );\n            }\n            counter = available.Count - 1;\n        }\n\n\n        public class Ref\n        {\n            public T Value\n            {\n                get\n                {\n                    if ( index == -1 )\n                        throw new NullReferenceException( );\n                    return value;\n                }\n            }\n\n\n            private FastPool&lt;T&gt; pool;\n            private T value;\n            internal int index;\n\n\n            internal Ref( FastPool&lt;T&gt; pool, T value, int index )\n            {\n                this.pool = pool;\n                this.value = value;\n                this.index = index;\n            }\n\n\n            public void Release( )\n            {\n                if ( index == -1 )\n                    return;\n                ++pool.counter;\n                pool.available[ pool.counter ] = this;\n                pool.unavailable[ index ] = null;\n                pool.availableUnavailables[ pool.counter ] = this.index;\n                index = -1;\n            }\n        }\n    }\n\n\n    public sealed class FastFixedPool&lt;T&gt;\n    {\n        private Ref[] available;\n        private Ref[] unavailable;\n        private int[] availableUnavailables;\n        private Func&lt;T&gt; createFunc;\n        private int counter;\n\n\n        public FastFixedPool( int capacity, Func&lt;T&gt; createFunc )\n        {\n            available = new Ref[ capacity ];\n            unavailable = new Ref[ capacity ];\n            availableUnavailables = new int[ capacity ];\n            this.createFunc = createFunc;\n\n            this.counter = available.Length - 1;\n            for ( int t = 0 ; t &lt; capacity ; ++t )\n            {\n                available[ t ] = new Ref( this, createFunc( ), -1 );\n                unavailable[ t ] = null;\n                availableUnavailables[ t ] =  t;\n            }\n        }\n\n\n        public Ref Get( )\n        {\n            if ( counter &lt; 0 )\n                throw new IndexOutOfRangeException( );\n            if ( counter &gt;= available.Length )\n                throw new NotImplementedException( );\n\n            var r = available[ counter ];\n            int i = availableUnavailables[ counter ];\n            --counter;\n            unavailable[ i ] = r;\n            r.index = i;\n            return r;\n        }\n\n\n        public class Ref\n        {\n            public T Value\n            {\n                get\n                {\n                    if ( index == -1 )\n                        throw new NullReferenceException( );\n                    return value;\n                }\n            }\n\n\n            private FastFixedPool&lt;T&gt; pool;\n            private T value;\n            internal int index;\n\n\n            internal Ref( FastFixedPool&lt;T&gt; pool, T value, int index )\n            {\n                this.pool = pool;\n                this.value = value;\n                this.index = index;\n            }\n\n\n            public void Release( )\n            {\n                if ( index == -1 )\n                    return;\n                ++pool.counter;\n                pool.available[ pool.counter ] = this;\n                pool.unavailable[ index ] = null;\n                pool.availableUnavailables[ pool.counter ] = this.index;\n                index = -1;\n            }\n        }\n    }\n\n\n    public sealed class FasterFixedPool&lt;T&gt;\n    {\n        private T[] values;\n        private Ref[] available;\n        private Func&lt;T&gt; createFunc;\n        private int counter;\n\n\n        public FasterFixedPool( int capacity, Func&lt;T&gt; createFunc )\n        {\n            values = new T[ capacity ];\n            available = new Ref[ capacity ];\n            this.createFunc = createFunc;\n\n            this.counter = available.Length - 1;\n            for ( int t = 0 ; t &lt; capacity ; ++t )\n            {\n                values[ t ] = createFunc( );\n                available[ t ] = new Ref( this, -1 );\n            }\n        }\n\n\n        public Ref Get( )\n        {\n            var r = available[ counter ];\n            r.live = true;\n            --counter;\n            return r;\n        }\n\n\n        public class Ref\n        {\n            public T Value\n            {\n                get\n                {\n                    if ( !live )\n                        throw new NullReferenceException( );\n                    return pool.values[ index ];\n                }\n            }\n\n\n            private FasterFixedPool&lt;T&gt; pool;\n            private int index;\n            internal bool live;\n\n\n            internal Ref( FasterFixedPool&lt;T&gt; pool, int index )\n            {\n                this.pool = pool;\n                this.index = index;\n                this.live = false;\n            }\n\n\n            public void Release( )\n            {\n                if ( !live )\n                    return;\n                ++pool.counter;\n                pool.available[ pool.counter ] = this;\n                live = false;\n            }\n        }\n    }\n\n\n    public sealed class EvenFasterFixedPool&lt;T&gt;\n    {\n        private Ref[] available;\n        private Func&lt;T&gt; createFunc;\n        private int counter;\n\n\n        public EvenFasterFixedPool( int capacity, Func&lt;T&gt; createFunc )\n        {\n            available = new Ref[ capacity ];\n            this.createFunc = createFunc;\n\n            this.counter = available.Length - 1;\n            for ( int t = 0 ; t &lt; capacity ; ++t )\n            {\n                available[ t ] = new Ref( this, createFunc( ), -1 );\n            }\n        }\n\n\n        public Ref Get( )\n        {\n            var r = available[ counter ];\n            r.live = true;\n            --counter;\n            return r;\n        }\n\n\n        public class Ref\n        {\n            public T Value\n            {\n                get\n                {\n                    if ( !live )\n                        throw new NullReferenceException( );\n                    return value;\n                }\n            }\n\n\n            private EvenFasterFixedPool&lt;T&gt; pool;\n            private T value;\n            internal bool live;\n\n\n            internal Ref( EvenFasterFixedPool&lt;T&gt; pool, T value, int index )\n            {\n                this.pool = pool;\n                this.value = value;\n                this.live = false;\n            }\n\n\n            public void Release( )\n            {\n                if ( !live )\n                    return;\n                ++pool.counter;\n                pool.available[ pool.counter ] = this;\n                live = false;\n            }\n        }\n    }\n\n\n    public interface ICreate&lt;T&gt;\n    {\n        T Create( );\n    }\n}\n</code></pre>\n\n<p>The test code itself:</p>\n\n<pre><code>using System;\nusing System.Collections.Generic;\nusing System.Diagnostics;\nusing System.Linq;\nusing System.Runtime.InteropServices;\nusing System.Text;\nusing ObjectPool;\n\n\nnamespace ObjectPool_Testing\n{\n    class Program\n    {\n        static void Main( string[ ] args )\n        {\n            int count = 4096;\n\n\n            // DefaultNewTesting( count );\n            // FastPoolTesting( count );\n            // FastFixedPoolTesting( count );\n            // FasterFixedPoolTesting( count );\n            EvenFasterFixedPoolTesting( count );\n\n\n            Console.ReadKey( true );\n        }\n\n\n        static void DefaultNewTesting( int count )\n        {\n            Console.WriteLine( \"Default new testing\" );\n\n            int t = 0;\n            var timer = new Stopwatch( );\n            timer.Restart( );\n            List&lt;Class16&gt; list = new List&lt;Class16&gt;( );\n            for ( ; t &lt; count ; ++t )\n            {\n                list.Add( new Class16( ) );\n            }\n            list.Clear( );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            timer.Restart( );\n            for ( int t2 = 0 ; t2 &lt; 10 ; ++t2 )\n            {\n                for ( t = 0 ; t &lt; count ; ++t )\n                {\n                    list.Add( new Class16() );\n                }\n                list.Clear( );\n            }\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            Console.WriteLine( );\n        }\n\n\n        static void FastPoolTesting( int count )\n        {\n            Console.WriteLine( \"FastPool&lt;T&gt; testing\" );\n\n            int t = 0;\n            var timer = new Stopwatch( );\n            timer.Restart( );\n            var pool =\n                new FastPool&lt;Class16&gt;(\n                    count,\n                    delegate()\n                    {\n                        return new Class16();\n                    } );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            t = 0;\n            timer.Restart( );\n            var list = new List&lt;FastPool&lt;Class16&gt;.Ref&gt;( );\n            for ( ; t &lt; count ; ++t )\n            {\n                list.Add( pool.Get( ) );\n            }\n            for ( t = 0 ; t &lt; list.Count ; ++t )\n                list[ t ].Release( );\n            list.Clear( );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            timer.Restart( );\n            for ( int t2 = 0 ; t2 &lt; 10 ; ++t2 )\n            {\n                for ( t = 0 ; t &lt; count ; ++t )\n                {\n                    list.Add( pool.Get( ) );\n                }\n                for ( t = 0 ; t &lt; list.Count ; ++t )\n                    list[ t ].Release( );\n                list.Clear( );\n            }\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            Console.WriteLine( );\n        }\n\n\n        static void FastFixedPoolTesting( int count )\n        {\n            Console.WriteLine( \"FastFixedPool&lt;T&gt; testing\" );\n\n            int t = 0;\n            var timer = new Stopwatch( );\n            timer.Restart( );\n            var pool =\n                new FastFixedPool&lt;Class16&gt;(\n                    count,\n                    delegate( )\n                    {\n                        return new Class16( );\n                    } );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            t = 0;\n            timer.Restart( );\n            var list = new List&lt;FastFixedPool&lt;Class16&gt;.Ref&gt;( );\n            for ( ; t &lt; count ; ++t )\n            {\n                list.Add( pool.Get( ) );\n            }\n            for ( t = 0 ; t &lt; list.Count ; ++t )\n                list[ t ].Release( );\n            list.Clear( );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            timer.Restart( );\n            for ( int t2 = 0 ; t2 &lt; 10 ; ++t2 )\n            {\n                for ( t = 0 ; t &lt; count ; ++t )\n                {\n                    list.Add( pool.Get( ) );\n                }\n                for ( t = 0 ; t &lt; list.Count ; ++t )\n                    list[ t ].Release( );\n                list.Clear( );\n            }\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            Console.WriteLine( );\n        }\n\n\n        static void FasterFixedPoolTesting( int count )\n        {\n            Console.WriteLine( \"FasterFixedPool&lt;T&gt; testing\" );\n\n            int t = 0;\n            var timer = new Stopwatch( );\n            timer.Restart( );\n            var pool =\n                new FasterFixedPool&lt;Class16&gt;(\n                    count,\n                    delegate( )\n                    {\n                        return new Class16( );\n                    } );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            t = 0;\n            timer.Restart( );\n            var list =\n                new List&lt;FasterFixedPool&lt;Class16&gt;.Ref&gt;(\n                    count );\n            for ( ; t &lt; count ; ++t )\n            {\n                list.Add( pool.Get( ) );\n            }\n            for ( t = 0 ; t &lt; list.Count ; ++t )\n                list[ t ].Release( );\n            list.Clear( );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            timer.Restart( );\n            for ( int t2 = 0 ; t2 &lt; 10 ; ++t2 )\n            {\n                for ( t = 0 ; t &lt; count ; ++t )\n                {\n                    list.Add( pool.Get( ) );\n                }\n                for ( t = 0 ; t &lt; list.Count ; ++t )\n                    list[ t ].Release( );\n                list.Clear( );\n            }\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            Console.WriteLine( );\n        }\n\n\n        static void EvenFasterFixedPoolTesting( int count )\n        {\n            Console.WriteLine( \"FasterFixedPool&lt;T&gt; testing\" );\n\n            int t = 0;\n            var timer = new Stopwatch( );\n            timer.Restart( );\n            var pool =\n                new EvenFasterFixedPool&lt;Class16&gt;(\n                    count,\n                    delegate( )\n                    {\n                        return new Class16( );\n                    } );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            t = 0;\n            timer.Restart( );\n            var list =\n                new List&lt;EvenFasterFixedPool&lt;Class16&gt;.Ref&gt;(\n                    count );\n            for ( ; t &lt; count ; ++t )\n            {\n                list.Add( pool.Get( ) );\n            }\n            for ( t = 0 ; t &lt; list.Count ; ++t )\n                list[ t ].Release( );\n            list.Clear( );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            timer.Restart( );\n            for ( int t2 = 0 ; t2 &lt; 100000; ++t2 )\n            {\n                for ( t = 0 ; t &lt; count ; ++t )\n                {\n                    list.Add( pool.Get( ) );\n                }\n                for ( t = 0 ; t &lt; list.Count ; ++t )\n                    list[ t ].Release( );\n                list.Clear( );\n            }\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count * 100000,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            Console.WriteLine( );\n        }\n    }\n\n\n    static class Handy\n    {\n        public static string Iterations(\n            int iterations,\n            double time,\n            string timeDesignation )\n        {\n            return\n                iterations\n                + \" iterations in \"\n                + time\n                + ' '\n                + timeDesignation\n                + \"s at \"\n                + ( iterations / time )\n                + \" per \"\n                + timeDesignation;\n        }\n    }\n\n\n    [StructLayout( LayoutKind.Auto, Size = 16 )]\n    class Class16\n    {\n    }\n\n\n    sealed class Class16Factory : ICreate&lt;Class16&gt;\n    {\n        public Class16 Create( )\n        {\n            return new Class16( );\n        }\n    }\n}\n</code></pre>\n\n<p>There. It's even formatted. You have nothing to complain about.</p>\n"},{"tags":["windows","performance","iis","queue","processor"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":23,"score":0,"question_id":13162631,"title":"Processor Queue Length high with IIS Server","body":"<p>I have a 2 core server (2 x 2.66 GHZ INTEL Xeon) w/ 8 GM Ram and 50 GB 15,000 RPM HDD space.\nThis is a VM running w/ VM ware and is running Microsoft Server 2008R2. We have had some performance issues recently. We have up to 350 users w/ multiple devices/sessions per user and sometimes 800 sessions at once. We are pushing out data (about 100KB per file) on an average of every 3 minutes. It is a reporting application that updates the data for everyone so they all have the data in real-ish time.</p>\n\n<p>We are hosting a .net application written in C# though IIS server 7.5. We are running 2 worker processes. We do no have memory utilization above 5 GB (out of 8) and disk I/O seems fine.</p>\n\n<p>The processor queue length has been generally above 50 and has averaged as high as 120. Microsoft says it should be between 2 and 6.</p>\n\n<p>technet.microsoft.com/en-us/library/bb742410.aspx</p>\n\n<p>I am happy to answer any questions you may have, and this is my first time on here so be nice if I didn't explain something correctly!</p>\n\n<p>I am guessing that I need more CPU cores, please let me know your thoughts.</p>\n\n<p>THANKS GUYS!</p>\n"},{"tags":["xcode","json","performance","webserver","html-parsing"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":8,"score":0,"question_id":13161805,"title":"XCode: JSON or parse HTML for performance if reading folders from web server","body":"<p>I am trying to access some folders in a web server and have become hopelessly confused so apologies if this seems vague.</p>\n\n<p>I'm testing on a local host (MAMP), with the folders located at:</p>\n\n<pre><code>http://localhost:8888/Files/\n</code></pre>\n\n<p>I have an obj-C routine which returns HTML similar to the following:</p>\n\n<pre><code>&lt;html&gt;\n &lt;head&gt;\n  &lt;title&gt;Index of /Files&lt;/title&gt;\n &lt;/head&gt;\n &lt;body&gt;\n&lt;h1&gt;Index of /Files&lt;/h1&gt;\n&lt;ul&gt;&lt;li&gt;&lt;a href=\"/\"&gt; Parent Directory&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"Folder 1/\"&gt; Folder 1/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"Folder 2/\"&gt; Folder 2/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"Folder 3/\"&gt; Folder 3/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"Folder 4/\"&gt; Folder 4/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"Folder 5/\"&gt; Folder 5/&lt;/a&gt;&lt;/li&gt;&lt;/li&gt;\n&lt;/ul&gt;\n</code></pre>\n\n<p></p>\n\n<p>Each folder contains either sub-folders, files (.pdf, .doc, etc) or a combination of both.</p>\n\n<p>What I'm trying to do is have the information returned so that I can process the files for display in an iPad app.</p>\n\n<p>Reading similar questions on here, I understand this can be done using an HTML parser but that this can increase bandwidth demands and there seems to be a majority opinion that JSON should be used instead. (I realise I may be misinterpreting that.)</p>\n\n<p>In this question:\n<a href=\"http://stackoverflow.com/questions/6396901/parse-html-iphone\">Parse HTML iPhone</a>\none answer suggests leaving a static JSON file on the web server.</p>\n\n<p>My question is, would I be better trying convert to the file structure to JSON (I've no experience of JSON at all) or carry on down the HTML-parsing route?\nAny pointers appreciated.</p>\n"},{"tags":["html","performance","cookies","yslow"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":27,"score":1,"question_id":13154782,"title":"Why does yslow identifiy my images, scripts and css as cookies?","body":"<p><img src=\"http://i.stack.imgur.com/yg5kx.png\" alt=\"Yslow\"></p>\n\n<p>I am using yslow version 3.1.4</p>\n"},{"tags":["c#","performance","sql-server-2008-r2","daab","data-access-app-block"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":24,"score":2,"question_id":13161009,"title":"Why is a call to DatabaseInstance.ExecuteSprocAccessor<T>(...) taking so long for such a simple query?","body":"<p>Very odd slowdown when calling the Data Access Application block.</p>\n\n<p>The SP (\"QuestionsToBeAnswered\") it's calling returns 58 rows with three columns (two GUIDs and an integer: 21AF77DA-2E76-47DB-AB54-0E5C85CD9AD8, 21AF77DA-2E76-47DB-AB54-0E5C85CD9AF0, 2) in less than 1 second when executed directly on the server. My SQL experience is pretty good, and I'm convinced the problem doesn't exist on the SQL server.</p>\n\n<p>However, when it's called through DAAB, it's taking a very long time to return the collection of objects. ExecuteSprocAccessor(...) normally returns an IEnumerable, and the SP isn't executed until the collection is enumerated or otherwise consumed, so this problem doesn't show up until consumption occurs.</p>\n\n<pre><code>DatabaseInstance.ExecuteSprocAccessor&lt;T&gt;(storedProcedure, rowMapper, args);\n</code></pre>\n\n<p>Given that the same code has no problem returning >200 rows of considerably more complex information, I am baffled as to why this code is taking so long (55 seconds!) to execute.</p>\n\n<p>Any ideas would be welcomed...</p>\n"},{"tags":["android","performance","jelly-bean"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":18,"score":2,"question_id":13160352,"title":"requestFocus for TextView on Jelly Bean slow","body":"<p>I am developing an application that has 4 text fields for entering data into and I have come across a performance issue when moving focus from one to the other. When a field has a character entered into it I use the addTextChangedListener to monitor the text and move the focus to the next text field. This was working fine on versions of android before 4.1.1 but since testing on 4.1.1 there is a noticeable lag when you press a key before the focus appears in the next field. This means if the user types rapidly, key presses can be lost.</p>\n\n<p>I have a simple app using the following code</p>\n\n<pre><code>public void onCreate(Bundle savedInstanceState) {\n    super.onCreate(savedInstanceState);\n    setContentView(R.layout.activity_main);\n\n\n    one = (EditText)findViewById(R.id.editText1);\n    two = (EditText)findViewById(R.id.editText2);\n\n    one.addTextChangedListener(new TextWatcher() {\n\n\n        @Override\n        public void afterTextChanged(Editable s) {\n            two.requestFocus();\n\n        }\n\n        @Override\n        public void beforeTextChanged(CharSequence s, int start, int count,\n                int after) {\n            // TODO Auto-generated method stub\n\n        }\n\n        @Override\n        public void onTextChanged(CharSequence s, int start, int before,\n                int count) {\n            // TODO Auto-generated method stub\n\n        }\n    });\n}\n</code></pre>\n\n<p>that highlights the issue. When run on a 4.0.4 based device everything is fine, but on 4.1.1 it takes a while to move the focus.</p>\n\n<p>I have tested this on 2 different Samsung Galaxy s3's one with 4.0.4 and one with 4.1.1.</p>\n\n<p>Has anyone else seen this?</p>\n\n<p>Many thanks</p>\n\n<p>Paul</p>\n"},{"tags":["performance","active-directory","windows-authentication"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":19,"score":0,"question_id":13160019,"title":"Active Directory - application performance c#","body":"<p>I have application with windows authentication mode.</p>\n\n<p>On deploy apps i am synchronizing all users from active directory (security group) to my database.</p>\n\n<p>Well across my apps i need current logged user guid. Example : </p>\n\n<pre><code>var pc = new PrincipalContext(ContextType.Domain, AppSettings.DomainName);\n// hitting to ad\nvar user = UserPrincipal.FindByIdentity(pc, HttpContext.Current.User.Identity.Name);\nif (user != null &amp;&amp; user.Guid.HasValue)\n    return user.Guid.Value;\n</code></pre>\n\n<p>Well my question is about ad performance. Is really bad, when i need current logged user Guid call this query to ad?</p>\n\n<p>Thanks in advance</p>\n"},{"tags":["performance","entity-framework","entity-framework-4","linq-to-entities"],"answer_count":2,"favorite_count":3,"up_vote_count":6,"down_vote_count":0,"view_count":1136,"score":6,"question_id":5458762,"title":"Entity Framework queries speed","body":"<p>Recently I started to learning Entity Framework.</p>\n\n<p>First question made in my mind is: </p>\n\n<p>When we want to use LINQ to fetching data in EF, every query like this:</p>\n\n<pre><code>var a = from p in contacts select p.name ;\n</code></pre>\n\n<p>will be converts to SQL commands like this :</p>\n\n<pre><code>select name from contacts\n</code></pre>\n\n<ol>\n<li>does this converting repeat every time that we are querying?</li>\n<li>I heard that stored procedures are cached in database, does this event happens in LINQ queries in Entity Framework ?</li>\n</ol>\n\n<p>And at last is my question clear?</p>\n"},{"tags":["java","performance","optimization"],"answer_count":5,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":870,"score":1,"question_id":9745389,"title":"Is the ternary operator faster than an \"if\" condition","body":"<p>I am prone to \"<em>if-conditional syndrome</em>\" which means I tend to use if conditions all the time. I rarely ever use the ternary operator. For instance:\n</p>\n\n<pre class=\"lang-java prettyprint-override\"><code>//I like to do this:\nint a;\nif (i == 0)\n{\n    a = 10;\n}\nelse\n{\n    a = 5;\n}\n\n//When I could do this:\nint a = (i == 0) ? 10:5;\n</code></pre>\n\n<p>Does it matter which I use? Which is faster? Are there any notable performance differences? Is it a better practice to use the shortest code whenever possible?</p>\n\n<p>I use Java programming language.</p>\n"},{"tags":["java","performance","bufferedreader"],"answer_count":3,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":116,"score":3,"question_id":13155700,"title":"Fastest Way To Read and Write Large Files Line By Line in Java","body":"<p>I have been searching a lot for the fastest way to read and write again a large files (0.5 - 1 GB) in java with limited memory (about 64MB). Each line in the file represents a record, so I need to get them line by line. The file is a normal text file.</p>\n\n<p>I tried BufferedReader and BufferedWriter but it doesn't seem to be the best option. It takes about 35 seconds to read and write a file of size 0.5 GB, only read write with no processing. I think the bottleneck here is writing as reading alone takes about 10 seconds.</p>\n\n<p>I tried to read array of bytes, but then searching for lines in each array that was read takes more time.</p>\n\n<p>Any suggestions please?\nThanks</p>\n"},{"tags":["sql","performance","select","ssis"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":57,"score":0,"question_id":13153932,"title":"Does it matter where I SELECT fewer columns in SSIS in order to increase performance?","body":"<p>Say in a data flow task, I have an OLE DB source. I would like to increase the performance of the SSIS. Does it matter where I SELECT less columns?</p>\n\n<ol>\n<li><p>Create a view in database that SELECT less columns, use that as the source.</p></li>\n<li><p>Type SQL SELECT inside the source to select less columns.</p></li>\n<li><p>Choose the table then untick the columns inside the source.</p></li>\n</ol>\n\n<p>Thank you</p>\n"},{"tags":["performance","oracle","query-optimization"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":57,"score":0,"question_id":13153395,"title":"Same query, slow on Oracle 9, fast on Oracle 10","body":"<p>We have a table called <code>t_reading</code>, with the following schema:</p>\n\n<pre><code>MEAS_ASS_ID     NUMBER(12,0)\nREAD_DATE       DATE\nREAD_TIME       VARCHAR2(5 BYTE)\nNUMERIC_VAL     NUMBER\nCHANGE_REASON   VARCHAR2(240 BYTE)\nOLD_IND         NUMBER(1,0)\n</code></pre>\n\n<p>This table is indexed as follows:</p>\n\n<pre><code>CREATE INDEX RED_X4 ON T_READING\n(\n  \"OLD_IND\",\n  \"READ_DATE\" DESC,\n  \"MEAS_ASS_ID\",\n  \"READ_TIME\"\n)\n</code></pre>\n\n<p>This exact table (with the same data) exists on two servers, the only difference is the Oracle version installed on each one.</p>\n\n<p>The query in question is:</p>\n\n<pre><code>SELECT * FROM t_reading WHERE OLD_IND = 0 AND MEAS_ASS_ID IN (5022, 5003) AND read_date BETWEEN to_date('30/10/2012', 'dd/mm/yyyy') AND to_date('31/10/2012', 'dd/mm/yyyy');\n</code></pre>\n\n<p>This query executes in less than a second on Oracle 10, and around a minute in Oracle 9.</p>\n\n<p>Are we missing something?</p>\n\n<p>EDIT:</p>\n\n<p>Execution plan for Oracle 9:\n<img src=\"http://i.stack.imgur.com/u359A.jpg\" alt=\"enter image description here\"></p>\n\n<p>Execution plan for Oracle 10:\n<img src=\"http://i.stack.imgur.com/uryvp.jpg\" alt=\"enter image description here\"></p>\n"},{"tags":["javascript","jquery","ajax","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":63,"score":4,"question_id":13144144,"title":"jQuery request a list of url while limit the max number of concurrent request","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/4797566/queue-ajax-calls\">Queue AJAX calls</a>  </p>\n</blockquote>\n\n\n\n<p>I have a list of id:</p>\n\n<p><code>var ids = [3738, 75995, 927, ... ]; // length is about 2000</code></p>\n\n<p>I'd like to request the url <code>http://xx/ + id</code> with <code>$.getJSON</code>, like:</p>\n\n<pre><code>ids.forEach(function(id, index){\n    $.getJSON('http://xx/' + id, function(data){\n        // store the data in another array.\n    });\n});\n</code></pre>\n\n<p>However, this will make too much requests in one time, making the browser blocking for a while, so my question is, how could I limit the number of concurrent ajax request in jQuery? for example, I send 10 request and when each of them got the response I send another request.</p>\n"},{"tags":["c","performance","sse"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":47,"score":1,"question_id":13157773,"title":"SSE unsigned/signed subtraction of 16 bit register","body":"<p>I have a __m128i register (Vector A) with 16 bit values with the content:</p>\n\n<pre><code>{100,26,26,26,26,26,26,100} // A Vector\n</code></pre>\n\n<p>Now I subtract the vector </p>\n\n<pre><code>{82,82,82,82,82,82,82,82}\n</code></pre>\n\n<p>With the instruction </p>\n\n<pre><code>_mm_sub_epi16(a_vec,_mm_set1_epi16(82)) \n</code></pre>\n\n<p>The expected result should be the following vector </p>\n\n<pre><code>{18,-56,-56,-56,-56,-56,-56,18}\n</code></pre>\n\n<p>But I get </p>\n\n<pre><code>{18,65480,65480,65480,65480,65480,65480,18}\n</code></pre>\n\n<p>How can I solve that the vector is treated as signed? </p>\n\n<p>The A Vector was created by this instruction: </p>\n\n<pre><code>__m128i a_vec = _mm_srli_epi16(_mm_unpacklo_epi8(score_vec_8bit, score_vec_8bit), 8)\n</code></pre>\n"},{"tags":["performance","dynamic-languages"],"answer_count":11,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":1418,"score":4,"question_id":2198684,"title":"Are dynamic languages slower than static languages?","body":"<p>Are dynamic languages slower than static languages because, for example, the run-time has to check the type consistently?</p>\n"},{"tags":["performance","nsarray","nsdictionary","nspredicate"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":539,"score":3,"question_id":7408047,"title":"NSDictionary VS NSArray+NSPredicate: which is faster/recommented","body":"<p>What is faster getting to an object from a collection?</p>\n\n<p>a. Searching in an NSDictionary with <code>[dictionary objectForKey:key];</code>\nor</p>\n\n<p>b. Searching in an NSArray with <code>[NSPredicate predicateWithFormat:@\"someKey like %@\",someKeyValue];</code></p>\n\n<p>In both cases  I create the collections.</p>\n\n<p>Regards!</p>\n"},{"tags":["c++","performance","list","size"],"answer_count":2,"favorite_count":1,"up_vote_count":12,"down_vote_count":0,"view_count":275,"score":12,"question_id":13157164,"title":"Why isn't std::list.size() constant-time?","body":"<p>This code ran for 0.012 seconds:</p>\n\n<pre><code> std::list&lt;int&gt; list;\n list.resize(100);\n int size;\n for(int i = 0 ; i &lt; 10000; i++)\n     size = list.size();\n</code></pre>\n\n<p>This one for 9.378 seconds:</p>\n\n<pre><code> std::list&lt;int&gt; list;\n list.resize(100000);\n int size;\n for(int i = 0 ; i &lt; 10000; i++)\n     size = list.size();\n</code></pre>\n\n<p>In my opinion it would be possible to implement std::list in such way, that size would be stored in a private variable but according to this it is computed again each time I call size. Can anyone explain why?</p>\n"},{"tags":["sql","performance","linq"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":73,"score":2,"question_id":13155738,"title":"SQL vs LINQ performance","body":"<p>We currently have a self-made entity framework that relies on a DB-indipendent ORM.</p>\n\n<p>I have to build a software that batch-loads metadata in the DB for about 150 excel template (with info on cell position, cell type, formatting and more).</p>\n\n<p>I can operate</p>\n\n<ul>\n<li><p>via SQL batch (faster but less interactive)</p></li>\n<li><p>via building objects in memory, processing them with LINQ queries for various integrity checks, and then committing modifications to the DB</p></li>\n</ul>\n\n<p>I know that SQL is absolutely faster, but I would know... <strong>how much is it faster?</strong></p>\n\n<p>In detail, how much is a SQL query faster then a LINQ query <em>(assuming that all needed data has been already loaded in memory by ORM)</em> ?</p>\n"},{"tags":["java","performance","swing","garbage-collection","awt"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":28,"score":0,"question_id":13154879,"title":"While importing a full package how to prevent the unwanted classes loaded? What are the important places GC has to be performed when using awt swing?","body":"<p>I'm doing programs via notepad (windows 7)importing the whole package makes the code simple, but the program looks quite weighing when i used verbose command. While compiling and running my java class, a lot of unwanted class files to the code has been loaded, is there any functionality in java so that it would prevent the unwanted loading of class files?</p>\n\n<p>I knew there are IDE's but right now I'm feeling comfortable with notepad. So someone suggest if there is any functionality that avoids unwanted class loading to the program that I compile when a package is completely imported?</p>\n\n<p>and When should be GC performed(specific areas whether when adding components/setting action events)?</p>\n"},{"tags":["ruby-on-rails","performance","heroku","newrelic-rpm"],"answer_count":2,"favorite_count":2,"up_vote_count":5,"down_vote_count":0,"view_count":124,"score":5,"question_id":12181133,"title":"Strange TTFB (time to first byte) issue","body":"<p>We're in the process of improving performance of the our rails app hosted at Heroku (rails 3.2.8 and ruby 1.9.3). During this we've come across one alarming problem for which the source seems to be extremely difficult to track. Let me quickly explain how we experience the problem and how we've tried to isolate it.</p>\n\n<p>--</p>\n\n<p>Since around June we've experienced weird lag behavior in Time to First Byte all over the site. The problems is obvious from using the site (sometimes the application doesn't respond for 10-20 seconds), and it's also present in waterfall analysis via webpagetest.org.\nWe're based in Denmark but get this result from any host.</p>\n\n<p>To confirm the problem we've performed a benchmark test where we send 300 identical requests to a simple page and measured the response time.\nIf we send 300 requests to the front page the median response time is below 1 second, which is fairly good. What scares us is that 60 requests takes more that double that time and 40 of those takes more than 4 seconds. Some requests take as much as 16 seconds.</p>\n\n<p>None of these slow requests show up in New Relic, which we use for performance monitoring. No request queuing shows up and the results are the same no matter how high we scale our web processes.\nStill, we couldn't reject that the problem was caused by application code, so we tried another experiment where we responded to the request via rack middleware.</p>\n\n<p>By placing this middleware (TestMiddleware) at the beginning of the rack stack, we returned a request before it even hit the application, ensuring that none of the following middleware or the rails app could cause the delay.</p>\n\n<pre><code>Middleware setup:\n$ heroku run rake middleware\nuse Rack::Cache\nuse ActionDispatch::Static\nuse TestMiddleware\nuse Rack::Rewrite\nuse Rack::Lock\nuse Rack::Runtime\nuse Rack::MethodOverride\nuse ActionDispatch::RequestId\nuse Rails::Rack::Logger\nuse ActionDispatch::ShowExceptions\nuse ActionDispatch::DebugExceptions\nuse ActionDispatch::RemoteIp\nuse Rack::Sendfile\nuse ActionDispatch::Callbacks\nuse ActiveRecord::ConnectionAdapters::ConnectionManagement\nuse ActiveRecord::QueryCache\nuse ActionDispatch::Cookies\nuse ActionDispatch::Session::DalliStore\nuse ActionDispatch::Flash\nuse ActionDispatch::ParamsParser\nuse ActionDispatch::Head\nuse Rack::ConditionalGet\nuse Rack::ETag\nuse ActionDispatch::BestStandardsSupport\nuse NewRelic::Rack::BrowserMonitoring\nuse Rack::RailsExceptional\nuse OmniAuth::Builder\nrun AU::Application.routes\n</code></pre>\n\n<p>We then ran the same script to document response time and got pretty much the same result. The median response time was around 130ms (obviously faster because it doesn't hit the app. But still 60 requests took more than 400ms and 25 requests took more than 1 second. Again, with some requests as slow as 16 seconds.</p>\n\n<p>One explanation could be related to slow hops on the network or DNS setup, but the results of traceroute looks perfectly OK.</p>\n\n<p>This result was confirmed from running the response script on another rails 3.2 and ruby 1.9.3 application hosted on Heroku - no weird behavior at all.</p>\n\n<p>The DNS setup follows Heroku's recommendations.</p>\n\n<p>--</p>\n\n<p>We're confused to say the least. Could there be something fishy with Heroku's routing network?\nWhy the heck are we seeing this weird behavior? How do we get rid of it? And why can't we see it in New Relic?</p>\n"},{"tags":["sql","database","performance","sqlite","sqlite3"],"answer_count":6,"favorite_count":2,"up_vote_count":8,"down_vote_count":0,"view_count":1218,"score":8,"question_id":8988915,"title":"SQLite: COUNT slow on big tables","body":"<p>I'm having a performance problem in SQLite with a SELECT COUNT(*) on a large tables.</p>\n\n<p>As I didn't yet receive a usable answer and I did some further testing, I edited my question to incorporate my new findings.</p>\n\n<p>I have 2 tables:</p>\n\n<pre><code>CREATE TABLE Table1 (\nKey INTEGER NOT NULL,\n... several other fields ...,\nStatus CHAR(1) NOT NULL,\nSelection VARCHAR NULL,\nCONSTRAINT PK_Table1 PRIMARY KEY (Key ASC))\n\nCREATE Table2 (\nKey INTEGER NOT NULL,\nKey2 INTEGER NOT NULL,\n... a few other fields ...,\nCONSTRAINT PK_Table2 PRIMARY KEY (Key ASC, Key2 ASC))\n</code></pre>\n\n<p>Table1 has around 8 million records and Table2 has around 51 million records, and the databasefile is over 5GB.</p>\n\n<p>Table1 has 2 more indexes:</p>\n\n<pre><code>CREATE INDEX IDX_Table1_Status ON Table1 (Status ASC, Key ASC)\nCREATE INDEX IDX_Table1_Selection ON Table1 (Selection ASC, Key ASC)\n</code></pre>\n\n<p>\"Status\" is required field, but has only 6 distinct values, \"Selection\" is not required and has only around 1.5 million values different from null and only around 600k distinct values.</p>\n\n<p>I did some tests on both tables, you can see the timings below, and I added the \"explain query plan\" for each request (QP). I placed the database file on an USB-memorystick so i could remove it after each test and get reliable results without interference of the disk cache. Some requests are faster on USB (I suppose due to lack of seektime), but some are slower (table scans).</p>\n\n<pre><code>SELECT COUNT(*) FROM Table1\n    Time: 105 sec\n    QP: SCAN TABLE Table1 USING COVERING INDEX IDX_Table1_Selection(~1000000 rows)\nSELECT COUNT(Key) FROM Table1\n    Time: 153 sec\n    QP: SCAN TABLE Table1 (~1000000 rows)\nSELECT * FROM Table1 WHERE Key = 5123456\n    Time: 5 ms\n    QP: SEARCH TABLE Table1 USING INTEGER PRIMARY KEY (rowid=?) (~1 rows)\nSELECT * FROM Table1 WHERE Status = 73 AND Key &gt; 5123456 LIMIT 1\n    Time: 16 sec\n    QP: SEARCH TABLE Table1 USING INDEX IDX_Table1_Status (Status=?) (~3 rows)\nSELECT * FROM Table1 WHERE Selection = 'SomeValue' AND Key &gt; 5123456 LIMIT 1\n    Time: 9 ms\n    QP: SEARCH TABLE Table1 USING INDEX IDX_Table1_Selection (Selection=?) (~3 rows)\n</code></pre>\n\n<p>As you can see the counts are very slow, but normal selects are fast (except for the 2nd one, which took 16 seconds).</p>\n\n<p>The same goes for Table2:</p>\n\n<pre><code>SELECT COUNT(*) FROM Table2\n    Time: 528 sec\n    QP: SCAN TABLE Table2 USING COVERING INDEX sqlite_autoindex_Table2_1(~1000000 rows)\nSELECT COUNT(Key) FROM Table2\n    Time: 249 sec\n    QP: SCAN TABLE Table2 (~1000000 rows)\nSELECT * FROM Table2 WHERE Key = 5123456 AND Key2 = 0\n    Time: 7 ms\n    QP: SEARCH TABLE Table2 USING INDEX sqlite_autoindex_Table2_1 (Key=? AND Key2=?) (~1 rows)\n</code></pre>\n\n<p>Why is SQLite not using the automatically created index on the primary key on Table1 ?\nAnd why, when he uses the auto-index on Table2, it still takes a lot of time ?</p>\n\n<p>I created the same tables with the same content and indexes on SQL Server 2008 R2 and there the counts are nearly instantaneous.</p>\n\n<p>One of the comments below suggested executing ANALYZE on the database. I did and it took 11 minutes to complete.\nAfter that, I ran some of the tests again:</p>\n\n<pre><code>SELECT COUNT(*) FROM Table1\n    Time: 104 sec\n    QP: SCAN TABLE Table1 USING COVERING INDEX IDX_Table1_Selection(~7848023 rows)\nSELECT COUNT(Key) FROM Table1\n    Time: 151 sec\n    QP: SCAN TABLE Table1 (~7848023 rows)\nSELECT * FROM Table1 WHERE Status = 73 AND Key &gt; 5123456 LIMIT 1\n    Time: 5 ms\n    QP: SEARCH TABLE Table1 USING INTEGER PRIMARY KEY (rowid&gt;?) (~196200 rows)\nSELECT COUNT(*) FROM Table2\n    Time: 529 sec\n    QP: SCAN TABLE Table2 USING COVERING INDEX sqlite_autoindex_Table2_1(~51152542 rows)\nSELECT COUNT(Key) FROM Table2\n    Time: 249 sec\n    QP: SCAN TABLE Table2 (~51152542 rows)\n</code></pre>\n\n<p>As you can see, the queries took the same time (except the query plan is now showing the real number of rows), only the slower select is now also fast.</p>\n\n<p>Next, I create dan extra index on the Key field of Table1, which should correspond to the auto-index. I did this on the original database, without the ANALYZE data. It took over 23 minutes to create this index (remember, this is on an USB-stick).</p>\n\n<pre><code>CREATE INDEX IDX_Table1_Key ON Table1 (Key ASC)\n</code></pre>\n\n<p>Then I ran the tests again:</p>\n\n<pre><code>SELECT COUNT(*) FROM Table1\n    Time: 4 sec\n    QP: SCAN TABLE Table1 USING COVERING INDEX IDX_Table1_Key(~1000000 rows)\nSELECT COUNT(Key) FROM Table1\n    Time: 167 sec\n    QP: SCAN TABLE Table2 (~1000000 rows)\nSELECT * FROM Table1 WHERE Status = 73 AND Key &gt; 5123456 LIMIT 1\n    Time: 17 sec\n    QP: SEARCH TABLE Table1 USING INDEX IDX_Table1_Status (Status=?) (~3 rows)\n</code></pre>\n\n<p>As you can see, the index helped with the count(*), but not with the count(Key).</p>\n\n<p>Finaly, I created the table using a column constraint instead of a table constraint:</p>\n\n<pre><code>CREATE TABLE Table1 (\nKey INTEGER PRIMARY KEY ASC NOT NULL,\n... several other fields ...,\nStatus CHAR(1) NOT NULL,\nSelection VARCHAR NULL)\n</code></pre>\n\n<p>Then I ran the tests again:</p>\n\n<pre><code>SELECT COUNT(*) FROM Table1\n    Time: 6 sec\n    QP: SCAN TABLE Table1 USING COVERING INDEX IDX_Table1_Selection(~1000000 rows)\nSELECT COUNT(Key) FROM Table1\n    Time: 28 sec\n    QP: SCAN TABLE Table1 (~1000000 rows)\nSELECT * FROM Table1 WHERE Status = 73 AND Key &gt; 5123456 LIMIT 1\n    Time: 10 sec\n    QP: SEARCH TABLE Table1 USING INDEX IDX_Table1_Status (Status=?) (~3 rows)\n</code></pre>\n\n<p>Although the query plans are the same, the times are a lot better. Why is this ?</p>\n\n<p>The problem is that ALTER TABLE does not permit to convert an existing table and I have a lot of existing databases which i can not convert to this form. Besides, using a column contraint instead of table constraint won't work for Table2.</p>\n\n<p>Has anyone any idea what I am doing wrong and how to solve this problem ?</p>\n\n<p>I used System.Data.SQLite version 1.0.74.0 to create the tables and to run the tests I used SQLiteSpy 1.9.1.</p>\n\n<p>Thanks,</p>\n\n<p>Marc</p>\n"},{"tags":["c#","performance","debugging","release"],"answer_count":10,"favorite_count":18,"up_vote_count":58,"down_vote_count":0,"view_count":12754,"score":58,"question_id":2446027,"title":"C# debug vs release performance","body":"<p>I've encountered the following paragraph:</p>\n\n<p>“Debug vs Release setting in the IDE when you compile your code in Visual Studio makes almost no difference to performance… the generated code is almost the same. The C# compiler doesn’t really do any optimisation. The C# compiler just spits out IL… and at the runtime it’s the JITer that does all the optimisation. The JITer does have a Debug/Release mode and that makes a huge difference to performance. But that doesn’t key off whether you run the Debug or Release configuration of your project, that keys off whether a debugger is attached.”</p>\n\n<p>The source is <a href=\"http://andrewmyhre.wordpress.com/2008/05/13/c-debugrelease-and-performance/\">here</a> and the podcast is <a href=\"http://go2.wordpress.com/?id=725X1342&amp;site=andrewmyhre.wordpress.com&amp;url=http%3A%2F%2Fwww.microsoft.com%2Fdownloads%2Fdetails.aspx%3FFamilyId%3DB11AD912-4158-44CC-A771-A5E044F7E3BB%26displaylang%3Den\">here</a>.</p>\n\n<p>Can someone direct me to a Microsoft article that can actually prove this?</p>\n\n<p>Googling \"C# debug vs release performance\" mostly returns results saying \"Debug has a lot of performance hit\", \"release is optimized\", and \"don't deploy debug to production.\"</p>\n"},{"tags":["performance","hibernate","jpa","openjpa","timesten"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":25,"score":0,"question_id":13152822,"title":"performance issue in find() method after migration to Hibernate 4.0 from OpenJPA 1.2","body":"<p>I migrate from OpenJPA 1.2 to Hiberante 4.0<br>\nI'm using TimesTen DB  </p>\n\n<p>I'm doing a native query to get id's of object's that I need , and then perform find on each on of them.\nIn <code>OpenJPA</code> instead of find I used <code>findCache()</code> method and if it return null I use the <code>find()</code> method , In hibernate I used only the <code>find()</code> method.</p>\n\n<p>I performed this operation on the same DB.</p>\n\n<p>after running couple of test I saw that the performance of OpenJPA is far better.</p>\n\n<p>I printed the statistics of hibernate session ( after querying and finding the same object's) and saw that the <code>hit\\miss</code> count to the first level cache is always 0.\nwhile the OpenJPA is clearly reaching it's cache by fetching object's with the <code>findCache</code> method.</p>\n\n<p>How can I improve the performance of find in Hibernate ?\nI suspect it referred to the difference in the first level cache implementation of this tools.</p>\n\n<p>another fact: I use the same EntityManager for the application run time ( I need to minimize the cost of creating of an EntityManager - my app is soft real time )</p>\n\n<p>thanks.</p>\n"},{"tags":["sql","sql-server","performance","tsql"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":76,"score":0,"question_id":13140520,"title":"Simple SQL Server query across databases runs extremely slow R2","body":"<p>I have two databases on a local machine, connected to <code>localhost</code>.  They both have roughly two million rows a piece.  I was doing the following very simple join and it took over a minute to complete.  </p>\n\n<pre><code>select distinct x.patid\n    from [i 3 sci study].dbo.clm_extract as x\n    left join [i 3 study].dbo.claims as y on y.patid=x.patid\n    where y.patid is null\n</code></pre>\n\n<p>When I looked at the execution plan I saw that the join showplan operator had this to say\n<img src=\"http://i.stack.imgur.com/eQAu6.jpg\" alt=\"enter image description here\"></p>\n\n<p>Why is the actual number of rows so exorbitantly high compared to the actual number of rows in both tables?</p>\n"},{"tags":["asp.net","performance","google","pagespeed"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":18,"score":0,"question_id":13152672,"title":"How do I improve my asp.net site when the Google speed test says \"expiration not specified\"?","body":"<p>I have taken different speed tests, including using <a href=\"https://developers.google.com/speed/pagespeed/insights\" rel=\"nofollow\">Google's PageSpeed Insights</a>.  For every image on every page, it warned me that I should specify heights and widths on every image, but more importantly, it said \"<strong>expiration not specified</strong>\" after every image.  It provided a link to suggestions on how to fix this, like use <strong>HTTP cache headers</strong>, but I didn't understand this at all.  I have 47 pages, many of them have 10+ pictures on a page.  </p>\n\n<p>All the images are optimized, but since I'm on an asp.net website with 5 master pages, what can I do?  I've seen solutions for php pages.  I don't even know what \"expiration not specified\" means.  Is there something I should just put in the head of my master pages, or do I need to do some type of coding on every image in my website?  Any guidance in this regard would be greatly appreciated!</p>\n"},{"tags":["sql-server","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":420,"score":0,"question_id":2042484,"title":"Ways to improve sql server query optimizer results","body":"<p>The question is quite simple: <b>What can I do to make sure that the SQL Server query optimizer has all the information it needs to choose the \"best\" query plan?</b></p>\n\n<p>The background of this question is that recently we've run into more and more cases where SQL Server chooses a bad query plan, i.e., cases, where adding query hints, join hints or explicitly using temporary tables instead of \"one big SQL\" drastically improved performance. I'm actually quite surprised that the query optimizer delivers such a lot of bad results, so I'm wondering whether we did something wrong. No indexes are missing (according to query analyzer and common sense), and statistics are updated frequently by a maintainance task.</p>\n\n<p>Let me emphasize that I am not talking about missing indexes here! I'm talking about the situation where there is a \"good\" and a \"bad\" query plan (given the current state of the DB), and SQL Server chooses a \"bad\" plan although the indexes present would allow it to use a \"good\" plan. I'm wondering whether there is some possibility to <em>improve</em> the results of the query optimizer without having to optimize all queries manually (with query hints or USE PLAN).</p>\n"},{"tags":["java","performance","compiler","gwt"],"answer_count":11,"favorite_count":45,"up_vote_count":90,"down_vote_count":0,"view_count":38234,"score":90,"question_id":1011863,"title":"How do I speed up the gwt compiler?","body":"<p>We're starting to make heavier use of GWT in our projects, and the performance of the GWT compiler is becoming increasingly annoying. </p>\n\n<p>We're going to start altering our working practices to mitigate the problem, including a greater emphasis on the hosted-mode browser, which defers the need to run the GWT compiler until a later time, but that brings its own risks, particularly that of not catching issues with real browsers until much later than we'd like.</p>\n\n<p>Ideally, we'd like to make the GWT compiler itself quicker - a minute to compile a fairly small application is taking the piss. However, we are using the compile if a fairly naive fashion, so I'm hoping we can make some quick and easy gains.</p>\n\n<p>We're currently invoking com.google.gwt.dev.Compiler as a java application from ant Ant target, with 256m max heap and lots of stack space. The compiler is launched by Ant using fork=true and the latest Java 6 JRE, to try and take advantage of Java6's improved performance. We pass our main controller class to the compiler along with the application classpath, and off it goes.</p>\n\n<p>What else can we do to get some extra speed? Can we give it more information so it spends less time doing discovery of what to do?</p>\n\n<p>I know we can tell it to only compile for one browser, but we need to do multi-browser testing, so that's not really practical.</p>\n\n<p>All suggestions welcome at this point.</p>\n"},{"tags":["performance","zoom","sample","graphicsmagick"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":6,"score":0,"question_id":13150758,"title":"GraphicsMagick zoom and sample function performance difference","body":"<p>We use GM to do image compressing, and observe a significant performance difference between zoom and sample functions, which we suppose to do the same thing.</p>\n\n<p>When using zoom function for a 68k image, a process could consume all the cpu time for all cores(12 core, 2.4G Hz machine), the throughput is 65/seconds, response time is 469 ms on average, load by top command is around 11, cpu usage is close to 100% </p>\n\n<p>When using sample function in the same environment, 24 process work together providing throughput close to 1000/seconds, response time is 37 ms on average, load by top command is around 3, cpu usage fluctuates between 50% and 80%</p>\n\n<p>the official document for these two functions is very simple as below:</p>\n\n<p>sample\nResize image by using pixel sampling algorithm:</p>\n\n<p>void            sample ( const Geometry &amp;geometry_ )</p>\n\n<p>zoom\nZoom (resize) image to specified size:</p>\n\n<p>void            zoom ( const Geometry &amp;geometry_ )</p>\n\n<p>the effects after the image processing are similar, but the difference is huge.</p>\n\n<ol>\n<li><p>Could anyone explain the different circumstances of using these two functions, since we might choose sample over zoom because of the performance issue</p></li>\n<li><p>Further, could anyone tell me why zoom is so cpu-time-consuming.</p></li>\n</ol>\n"},{"tags":["c#","c++","performance","for-loop"],"answer_count":5,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":88,"score":1,"question_id":13150001,"title":"What's the benefit of declaring for the loop index variable outside the loop?","body":"<p>I see this in a lot of game engine code. Is this supposed to be faster than if you were to declare it in the for loop body? Also this is followed by many other for loops, each of them using the same variable.</p>\n\n<pre><code>int i;\nfor(i=0; i&lt;count; ++i)\n{\n}\n</code></pre>\n\n<p>vs</p>\n\n<pre><code>for(int i=0; i&lt;count; ++i)\n{\n}\n</code></pre>\n\n<p>Btw I never do this myself, just curious about the idea behind it, since apart from performance I don't know why anyone would do this.</p>\n"},{"tags":["performance","r","matlab"],"answer_count":1,"favorite_count":0,"up_vote_count":4,"down_vote_count":2,"view_count":118,"score":2,"question_id":13142273,"title":"Discrepancy between R and Matlab speed","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/7142767/why-are-loops-slow-in-r\">Why are loops slow in R?</a>  </p>\n</blockquote>\n\n\n\n<p>Consider the following task. A dataset has 40 variables for 20,000 \"users\". Each user has between 1 and 150 observations. All users are stacked in a matrix called data. The first column is the id of the user and identifies the user. All id are stored in a 20,000 X 1 matrix called userid.</p>\n\n<p>Consider the following R code</p>\n\n<pre><code>useridl = length(userid)\nitime=proc.time()[3]    \nfor (i in 1:useridl) {\ntemp =data[data[,1]==userid[i],]\n   }\n etime=proc.time()[3]\n etime-itime\n</code></pre>\n\n<p>This code just goes through the 20,000 users, creating the temp matrix every time. With the subset of observations belonging to userid[i]. It takes about 6 minutes in a MacPro.</p>\n\n<p>In MatLab, the same task </p>\n\n<pre><code>tic\nfor i=1:useridl\ntemp=data(data(:,1)==userid(i),:);\nend\ntoc\n</code></pre>\n\n<p>takes 1 minute. </p>\n\n<p>Why is R so much slower? This is standard task, I am using matrices in both cases. Any ideas?</p>\n"},{"tags":["performance","algorithm","math","prime-factoring"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":90,"score":2,"question_id":13145636,"title":"number of divisors of a number which are not smaller than another number","body":"<p>Is there any efficient way to find the number of divisors of a number (say n) which are not smaller than another number (say m).\nn can be up to 10^12.\ni thought about sieve algorithm &amp; then find the number of divisors.\nmy method check all the numbers from m to square root of n.\nBut i think there is another way(efficient) to do that .</p>\n"},{"tags":["java","performance","optimization"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":28,"score":1,"question_id":13148544,"title":"Do Java JVMs perform any specialization or partial evaluation type optimizations?","body":"<p>Do any of the Java JVMs implement specialization or partial evaluation type optimizations? </p>\n\n<p>Does the final keyword get used for these optimizations? </p>\n"},{"tags":["django","performance","internet-explorer","loading"],"answer_count":4,"favorite_count":0,"up_vote_count":8,"down_vote_count":0,"view_count":554,"score":8,"question_id":6046625,"title":"Django: IE doesn't load locahost or loads very SLOWLY","body":"<p>I'm just starting to learn Django, building a project on my computer, running Windows 7 64-bit, Python 2.7, Django 1.3.</p>\n\n<p>Basically whatever I write, it loads in Chrome and Firefox instantly. But for IE (version 9), it just stalls there, and does nothing. I can load up \"http://127.0.0.1:8000\" on IE and leave the computer on for hours and it doesn't load. Sometimes, when I refresh a couple of times or restart IE it'll work. If I change something in the code, again, Chrome and Firefox reflects changes instantly, whereas IE doesn't - if it loads the page at all. </p>\n\n<p>What is going on? I'm losing my mind here.... </p>\n"},{"tags":["sql","performance","azure","sql-azure"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":67,"score":0,"question_id":13148473,"title":"SQL Azure - very slow compared to localhost database","body":"<p>I decided I wanted to try out Microsoft SQL Azure, as many people have talked very highly about it. It should be fast, flexible, cheap and many other things.</p>\n\n<p>I got it up and running, migrated my data to Azure and hooked up the connectionstring. I tried to run some queries on the database, and was shocked about how slow even simple queries were. A \"SELECT *\" from a table with 700 rows took 7 seconds. My page also seems extremely slow, compared to when I used a localhost managent studio or a database on a shared hosting. </p>\n\n<p>Now, when I setup my server, I couldn't pick a physical position. However, I live in Denmark, and I can see the server is the \"South Central US\". This might be the issue.</p>\n\n<p>I don't use any stored procedures (so I guess no parameter sniffing).. I can also see my indexes is transfered succesfully.</p>\n\n<p>Any ideas on what to do? Any performance things I am missing? </p>\n"},{"tags":["c","performance","optimization","x86","avx"],"answer_count":0,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":40,"score":2,"question_id":13148420,"title":"When should prefetch be used on modern machines?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/7327994/prefetching-examples\">Prefetching Examples?</a>  </p>\n</blockquote>\n\n\n\n<p>In many cases prefetch instructions seem to slow performance on modern machines, because there are typically a few different hardware prefetch units. </p>\n\n<p>Are there any particular cases where it always helps to use prefetch instructions? </p>\n"},{"tags":["performance","postgresql","index"],"answer_count":0,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":72,"score":4,"question_id":13130535,"title":"PostgreSQL Index optimization with dates","body":"<p>I have a large table of objects (15M rows +), for which I want to query for outdated field. PostgreSQL (9.0.8)</p>\n\n<p>I want to divide the query by millions, for scalability &amp; concurrency purposes, and I want to fetch all data with the updated_at field with a date of a few days ago.</p>\n\n<p>I have tried many indexes, and queries, on a million ids, and I can't seem to get performance under 100seconds. (with heroku's Ronin hardware).</p>\n\n<p>I am looking for suggestions I haven't tried to make this as efficient as possible.</p>\n\n<p>TRY #1</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE (date(updated_at)) &lt; (date(now())-7) AND id &gt;= 5000001 AND id &lt; 6000001;\n INDEX USED : (date(updated_at),id)\n 268578.934 ms\n</code></pre>\n\n<p>TRY #2</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE ((date(now()) - (date(updated_at)) &gt; 7)) AND id &gt;= 5000001 AND id &lt; 6000001;\n INDEX USED : primary key\n 335555.144 ms\n</code></pre>\n\n<p>TRY #3</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE (date(updated_at)) &lt; (date(now())-7) AND id/1000000 = 5;\n INDEX USED : (date(updated_at),(id/1000000))\n 243427.042 ms\n</code></pre>\n\n<p>TRY #4</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE (date(updated_at)) &lt; (date(now())-7) AND id/1000000 = 5 AND updated_at IS NOT NULL;\n INDEX USED : (date(updated_at),(id/1000000)) WHERE updated_at IS NOT NULL \n 706714.812 ms\n</code></pre>\n\n<p>TRY #5 (for a single month of outdated data)</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE (EXTRACT(MONTH from date(updated_at)) = 8) AND id/1000000 = 5 ;\n INDEX USED : (EXTRACT(MONTH from date(updated_at)),(id/1000000))\n 107241.472 ms\n</code></pre>\n\n<p>TRY #6</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE (date(updated_at)) &lt; (date(now())-7) AND id/1000000 = 5 ;\n INDEX USED : ( (id/1000000 ) ASC ,updated_at DESC NULLS LAST)\n 106842.395 ms\n</code></pre>\n\n<p>TRY #7</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE id/1000000 = 5 and (date(updated_at)) &lt; (date(now())-7)  ;\n INDEX USED : ( (id/1000000 ) ASC ,date(updated_at) DESC NULLS LAST);\n 100732.049 ms\n Second try: 87280.728 ms \n     http://explain.depesz.com/s/DQP\n</code></pre>\n\n<p>TRY #8</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE (date(updated_at)) &lt; (date(now())-7) AND id/1000000 = 5 AND updated_at IS NOT NULL;\n INDEX USED :  ( (id/1000000 ) ASC ,date(updated_at) ASC NULLS LAST);\n 129133.022 ms\n</code></pre>\n\n<p>TRY #9 (with partial index, as per Erwin's suggestion)</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE id BETWEEN 5000000 AND 5999999 AND (date(updated_at)) &lt; '2012-10-23'::date;\n INDEX USED : (date(updated_at) DESC NULLS LAST) WHERE id BETWEEN 5000000 AND 6000000 AND date(updated_at) &lt; '2012-10-23'::date ;\n 73861.047 ms\n http://explain.depesz.com/s/p9A\n</code></pre>\n\n<p>============================\nUPDATE (db settings)</p>\n\n<pre><code> select name,min_val, max_val, boot_val from pg_settings;\n\n              name               |  min_val  |   max_val    |     boot_val      \n---------------------------------+-----------+--------------+-------------------\n allow_system_table_mods         |           |              | off\n application_name                |           |              | \n archive_command                 |           |              | \n archive_mode                    |           |              | off\n  archive_timeout                 | 0         | 2147483647   | 0\n array_nulls                     |           |              | on\n authentication_timeout          | 1         | 600          | 60\n autovacuum                      |           |              | on\n autovacuum_analyze_scale_factor | 0         | 100          | 0.1\n autovacuum_analyze_threshold    | 0         | 2147483647   | 50\n autovacuum_freeze_max_age       | 100000000 | 2000000000   | 200000000\n autovacuum_max_workers          | 1         | 536870911    | 3\n autovacuum_naptime              | 1         | 2147483      | 60\n autovacuum_vacuum_cost_delay    | -1        | 100          | 20\n autovacuum_vacuum_cost_limit    | -1        | 10000        | -1\n autovacuum_vacuum_scale_factor  | 0         | 100          | 0.2\n autovacuum_vacuum_threshold     | 0         | 2147483647   | 50\n backslash_quote                 |           |              | safe_encoding\n bgwriter_delay                  | 10        | 10000        | 200\n bgwriter_lru_maxpages           | 0         | 1000         | 100\n bgwriter_lru_multiplier         | 0         | 10           | 2\n block_size                      | 8192      | 8192         | 8192\n bonjour                         |           |              | off\n bonjour_name                    |           |              | \n bytea_output                    |           |              | hex\n check_function_bodies           |           |              | on\n checkpoint_completion_target    | 0         | 1            | 0.5\n checkpoint_segments             | 1         | 2147483647   | 3\n checkpoint_timeout              | 30        | 3600         | 300\n checkpoint_warning              | 0         | 2147483647   | 30\n client_encoding                 |           |              | SQL_ASCII\n client_min_messages             |           |              | notice\n commit_delay                    | 0         | 100000       | 0\n commit_siblings                 | 1         | 1000         | 5\n constraint_exclusion            |           |              | partition\n cpu_index_tuple_cost            | 0         | 1.79769e+308 | 0.005\n cpu_operator_cost               | 0         | 1.79769e+308 | 0.0025\n cpu_tuple_cost                  | 0         | 1.79769e+308 | 0.01\n cursor_tuple_fraction           | 0         | 1            | 0.1\n custom_variable_classes         |           |              | \n DateStyle                       |           |              | ISO, MDY\n db_user_namespace               |           |              | off\n deadlock_timeout                | 1         | 2147483      | 1000\n debug_assertions                |           |              | off\n debug_pretty_print              |           |              | on\n debug_print_parse               |           |              | off\n debug_print_plan                |           |              | off\n debug_print_rewritten           |           |              | off\n default_statistics_target       | 1         | 10000        | 100\n default_tablespace              |           |              | \n default_text_search_config      |           |              | pg_catalog.simple\n default_transaction_isolation   |           |              | read committed\n default_transaction_read_only   |           |              | off\n default_with_oids               |           |              | off\n effective_cache_size            | 1         | 2147483647   | 16384\n effective_io_concurrency        | 0         | 1000         | 1\n enable_bitmapscan               |           |              | on\n enable_hashagg                  |           |              | on\n enable_hashjoin                 |           |              | on\n enable_indexscan                |           |              | on\n enable_material                 |           |              | on\n enable_mergejoin                |           |              | on\n enable_nestloop                 |           |              | on\n enable_seqscan                  |           |              | on\n enable_sort                     |           |              | on\n enable_tidscan                  |           |              | on\n escape_string_warning           |           |              | on\n extra_float_digits              | -15       | 3            | 0\n from_collapse_limit             | 1         | 2147483647   | 8\n fsync                           |           |              | on\n full_page_writes                |           |              | on\n geqo                            |           |              | on\n geqo_effort                     | 1         | 10           | 5\n  geqo_generations                | 0         | 2147483647   | 0\n  geqo_pool_size                  | 0         | 2147483647   | 0\n  geqo_seed                       | 0         | 1            | 0\n  geqo_selection_bias             | 1.5       | 2            | 2\n  geqo_threshold                  | 2         | 2147483647   | 12\n  gin_fuzzy_search_limit          | 0         | 2147483647   | 0\n  hot_standby                     |           |              | off\n  ignore_system_indexes           |           |              | off\n  integer_datetimes               |           |              | on\n  IntervalStyle                   |           |              | postgres\n  join_collapse_limit             | 1         | 2147483647   | 8\n  krb_caseins_users               |           |              | off\n  krb_srvname                     |           |              | postgres\n  lc_collate                      |           |              | C\n  lc_ctype                        |           |              | C\n  lc_messages                     |           |              |\n  lc_monetary                     |           |              | C\n  lc_numeric                      |           |              | C\n  lc_time                         |           |              | C\n  listen_addresses                |           |              | localhost\n  lo_compat_privileges            |           |              | off\n  local_preload_libraries         |           |              |\n  log_autovacuum_min_duration     | -1        | 2147483      | -1\n  log_checkpoints                 |           |              | off\n  log_connections                 |           |              | off\n  log_destination                 |           |              | stderr\n  log_disconnections              |           |              | off\n  log_duration                    |           |              | off\n  log_error_verbosity             |           |              | default\n  log_executor_stats              |           |              | off\n  log_hostname                    |           |              | off\n  log_line_prefix                 |           |              |\n  log_lock_waits                  |           |              | off\n  log_min_duration_statement      | -1        | 2147483      | -1\n  log_min_error_statement         |           |              | error\n  log_min_messages                |           |              | warning\n  log_parser_stats                |           |              | off\n  log_planner_stats               |           |              | off\n  log_rotation_age                | 0         | 35791394     | 1440\n  log_rotation_size               | 0         | 2097151      | 10240\n  log_statement                   |           |              | none\n  log_statement_stats             |           |              | off\n  log_temp_files                  | -1        | 2147483647   | -1\n  log_timezone                    |           |              | UNKNOWN\n  log_truncate_on_rotation        |           |              | off\n  logging_collector               |           |              | off\n  maintenance_work_mem            | 1024      | 2097151      | 16384\n  max_connections                 | 1         | 536870911    | 100\n  max_files_per_process           | 25        | 2147483647   | 1000\n  max_function_args               | 100       | 100          | 100\n  max_identifier_length           | 63        | 63           | 63\n  max_index_keys                  | 32        | 32           | 32\n  max_locks_per_transaction       | 10        | 2147483647   | 64\n  max_prepared_transactions       | 0         | 536870911    | 0\n  max_stack_depth                 | 100       | 2097151      | 100\n  max_standby_archive_delay       | -1        | 2147483      | 30000\n  max_standby_streaming_delay     | -1        | 2147483      | 30000\n  max_wal_senders                 | 0         | 536870911    | 0\n  password_encryption             |           |              | on\n  port                            | 1         | 65535        | 5432\n  post_auth_delay                 | 0         | 2147483647   | 0\n  pre_auth_delay                  | 0         | 60           | 0\n  random_page_cost                | 0         | 1.79769e+308 | 4\n  search_path                     |           |              | \"$user\",public\n  segment_size                    | 131072    | 131072       | 131072\n  seq_page_cost                   | 0         | 1.79769e+308 | 1\n  server_encoding                 |           |              | SQL_ASCII\n  server_version                  |           |              | 9.0.8\n  server_version_num              | 90008     | 90008        | 90008\n  session_replication_role        |           |              | origin\n  shared_buffers                  | 16        | 1073741823   | 1024\n  silent_mode                     |           |              | off\n  sql_inheritance                 |           |              | on\n  ssl                             |           |              | off\n  ssl_renegotiation_limit         | 0         | 2097151      | 524288\n  standard_conforming_strings     |           |              | off\n  statement_timeout               | 0         | 2147483647   | 0\n  superuser_reserved_connections  | 0         | 536870911    | 3\n  synchronize_seqscans            |           |              | on\n  synchronous_commit              |           |              | on\n  syslog_facility                 |           |              | local0\n  syslog_ident                    |           |              | postgres\n  tcp_keepalives_count            | 0         | 2147483647   | 0\n  tcp_keepalives_idle             | 0         | 2147483647   | 0\n  tcp_keepalives_interval         | 0         | 2147483647   | 0\n  temp_buffers                    | 100       | 1073741823   | 1024\n  temp_tablespaces                |           |              |\n  TimeZone                        |           |              | UNKNOWN\n  timezone_abbreviations          |           |              | UNKNOWN\n  trace_notify                    |           |              | off\n  trace_recovery_messages         |           |              | log\n  trace_sort                      |           |              | off\n  track_activities                |           |              | on\n  track_activity_query_size       | 100       | 102400       | 1024\n  track_counts                    |           |              | on\n  track_functions                 |           |              | none\n  transaction_isolation           |           |              |\n  transaction_read_only           |           |              | off\n  transform_null_equals           |           |              | off\n  unix_socket_group               |           |              |\n  unix_socket_permissions         | 0         | 511          | 511\n  update_process_title            |           |              | on\n  vacuum_cost_delay               | 0         | 100          | 0\n  vacuum_cost_limit               | 1         | 10000        | 200\n  vacuum_cost_page_dirty          | 0         | 10000        | 20\n  vacuum_cost_page_hit            | 0         | 10000        | 1\n  vacuum_cost_page_miss           | 0         | 10000        | 10\n  vacuum_defer_cleanup_age        | 0         | 1000000      | 0\n  vacuum_freeze_min_age           | 0         | 1000000000   | 50000000\n  vacuum_freeze_table_age         | 0         | 2000000000   | 150000000\n  wal_block_size                  | 8192      | 8192         | 8192\n  wal_buffers                     | 4         | 2147483647   | 8\n  wal_keep_segments               | 0         | 2147483647   | 0\n  wal_level                       |           |              | minimal\n  wal_segment_size                | 2048      | 2048         | 2048\n  wal_sender_delay                | 1         | 10000        | 200\n  wal_sync_method                 |           |              | fdatasync\n  wal_writer_delay                | 1         | 10000        | 200\n  work_mem                        | 64        | 2097151      | 1024\n  xmlbinary                       |           |              | base64\n  xmloption                       |           |              | content\n  zero_damaged_pages              |           |              | off\n  (195 rows) \n</code></pre>\n"},{"tags":["performance","azure"],"answer_count":11,"favorite_count":16,"up_vote_count":21,"down_vote_count":1,"view_count":7949,"score":20,"question_id":2711868,"title":"azure performance","body":"<p>I've moved my app from a dedicated server to azure (and sql azure), and have noticed substantial performance degradation.  </p>\n\n<p>obviously not having the database and web server on the same piece of hardware is much of it, but I'm curious what other people have found in migrating to azure, and if there is anything any of you would suggest I do to improve it.  Right now I'm considering moving back to my dedicated server...</p>\n\n<p>So in summary, are there any rules of thumb for this, existing research (wasn't able to find much) or other pieces of advice on improving the performance of the app?  has anyone else found the same to be true, and improved their site's performance in some way?  it's built in C# on asp.net mvc 2.</p>\n\n<p>Thanks!</p>\n"},{"tags":["javascript","performance","caching"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":13146657,"title":"Does the result of a function run several times is cached?","body":"<p>In JavaScript, I've an object with an array, and a method wich gets a slice of that array and a concatenation with another array.</p>\n\n<p>If that method is run several times in the same function to return always the same value, does the performance will be faster after of the first run (due to the result will be cached in CPU cache)?</p>\n"},{"tags":["performance","cassandra","scaling"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":13142819,"title":"Cassandra scaling cheat-sheet","body":"<p>Of course you can only know the performance of your system with your load with your use-cases by ... actually implementing it!  That aside, before embarking on a prototype, I'm searching for some very rough estimates of how Cassandra performs.</p>\n\n<p>For various configurations of nodes and data-centres, and for various read and write consistency levels, what the chances of reading a stale value?  What kind of key reads and writes per second would you expect to sustain, and what kind of latency would each read and write have?</p>\n"},{"tags":["android","performance","cpu","benchmarking","cpu-speed"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":24,"score":1,"question_id":13147168,"title":"Whetstone CPU performance algorithm","body":"<p>I'm going to develop an application for android that uses the whetstone algorithm to measure CPU performance. I've chosen the Whetstone algorithm because my research tells me it's an appropriate way of measure performance of less powerful cpu's.</p>\n\n<p>I've got the source code and pseudo code(from the sweet 60's) for the whetstone algorithm, and so far so good.\nBut the whole whetstone algorithm seem's a bit secret, and it's hard to find useful information about it. So my questions are:</p>\n\n<ul>\n<li>Why are the Whetstone algorithm suitable to measure performance of less powerful cpu's?</li>\n<li>In brief, can anyone tell me what exactly makes an cpu performance algorithm being a whetstone algorithm?</li>\n<li>Can anyone explain in short, the pseudo code of the Whetstone algorithm?</li>\n</ul>\n\n<p>Answer on any of this questions is really appreciated?</p>\n"},{"tags":["android","performance","testing","android-emulator","emulator"],"answer_count":11,"favorite_count":19,"up_vote_count":58,"down_vote_count":0,"view_count":29411,"score":58,"question_id":2662650,"title":"Making the Android emulator run faster","body":"<p>The Android emulator is a bit sluggish. For some devices, like the Motorola Droid and the Nexus One, the app runs faster in the actual device than the emulator. This is a problem when testing games and visual effects.</p>\n\n<p>How do you make the emulator run as fast as possible? I've been toying with its parameters but haven't found a configuration that shows a noticeable improvement yet.</p>\n"},{"tags":["javascript","performance","dom","browser"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":20,"score":1,"question_id":13146746,"title":"What's the fastest way to set DOMElement bounds?","body":"<p>Currently, to place an absolute element in DOM, I use :</p>\n\n<pre><code>this.myObject.style.left = aValue1 + 'px' ;\nthis.myObject.style.top = aValue2 + 'px' ;\nthis.myObject.style.width = aValue3 + 'px' ;\nthis.myObject.style.height = aValue4 + 'px' ;\n</code></pre>\n\n<p>Is there a better (quick for browsers) way to do this ?</p>\n\n<p>Also, maybe do I remove the element from DOM and re-append after ?</p>\n\n<p>Thanks for your anwsers.</p>\n"},{"tags":[".net","performance","collections","list","hash"],"answer_count":11,"favorite_count":7,"up_vote_count":36,"down_vote_count":0,"view_count":20363,"score":36,"question_id":150750,"title":"HashSet vs. List performance","body":"<p>It's clear that a search performance of the generic <code>HashSet&lt;T&gt;</code> class is higher than of the generic <code>List&lt;T&gt;</code> class. Just compare the hash-based key with the linear approach in the <code>List&lt;T&gt;</code> class.</p>\n\n<p>However calculating a hash key may itself take some CPU cycles, so for a small amount of items the linear search can be a real alternative to the <code>HashSet&lt;T&gt;</code>.</p>\n\n<p>My question: where is the break-even?</p>\n\n<p>To simplify the scenario (and to be fair) let's assume that the <code>List&lt;T&gt;</code> class uses the element's <code>Equals()</code> method to identify an item.</p>\n"},{"tags":["performance","unix","networking","connect","telnet"],"answer_count":1,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":48,"score":3,"question_id":13126235,"title":"force connect to work","body":"<p>I run the following line in bash, on a VM running red hat 5:</p>\n\n<pre><code>    for i in {1..100000};\n        do telnet 10.10.10.105 41941;\n    done\n</code></pre>\n\n<p>At some point, telnet connects to the port even though there is no one listening on it. It seems o be connecting to its self?\nThe same issue appears when i start the client side of an application, <strong>without</strong> starting the server - the client successfully connects to the ip:port. The client looks something like this: </p>\n\n<pre><code>    addr.sin_family = AF_INET;\n    addr.sin_port = htons(atoi(port));\n    addr.sin_addr.s_addr = inet_addr(hostname);\n\n    some_while_loop\n    {\n        status = ::connect(sock, (sockaddr *)&amp;addr, sizeof(addr));\n        if (status == -1)\n        {\n            shutdown(sock, 2);\n            close(sock);\n            return false;\n        }\n   }\n</code></pre>\n\n<p>I found this article: <a href=\"http://web.deu.edu.tr/doc/soket/\" rel=\"nofollow\">http://web.deu.edu.tr/doc/soket/</a> which states in 6.2 that the connection will succeed if you to the same machine you're running on. My question is, why is this happening? Is it a hardware issue or is it a fail-safe red-hat kernel is using, or maby it's because of the port i'm using (for 1025 for example, i don't have this issue)... ?</p>\n"},{"tags":["php","performance","image-processing","imagemagick"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":26,"score":0,"question_id":13143968,"title":"Imagick pixel iterator - slow as fck","body":"<p>I'm trying to implement <a href=\"http://www.php.net/manual/en/function.imagefilter.php#109809\" rel=\"nofollow\">this code snippet</a> (a \"vignette\" effect) with Imagick, but the processing is incredibly slow:</p>\n\n<pre><code>set_time_limit(90);\n$iterator = $imagick-&gt;getPixelIterator();\n$width = $imagick-&gt;getImageWidth();\n$height = $imagick-&gt;getImageHeight();\n\nforeach($iterator as $y =&gt; $pixels){\n  foreach($pixels as $x =&gt; $pixel){\n\n    $l = 1 - 0.7 * (1 - pow((sin(M_PI / $width * $x) * sin(M_PI / $height * $y)), 0.4));       \n\n    extract($pixel-&gt;getColor());   \n\n    $pixel-&gt;setColor(sprintf('rgb(%d,%d,%d)', $r * $l, $g * $l, $b * $l));\n  }\n\n  $iterator-&gt;syncIterator();\n}\n</code></pre>\n\n<p>Original:</p>\n\n<p><img src=\"http://i.stack.imgur.com/TSDEE.jpg\" alt=\"enter image description here\"></p>\n\n<p>Result:</p>\n\n<p><img src=\"http://i.stack.imgur.com/63E6e.jpg\" alt=\"enter image description here\"></p>\n\n<p>For a 1600x1200 image, it takes like 35 seconds for image to be processed. Is there a better way to do this?</p>\n"},{"tags":["performance","opengl","window","resolution","viewport"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":36,"score":0,"question_id":13139080,"title":"OpenGL Viewport Resolution and Performance","body":"<p>I want to allow the user to tweak windowed-mode performance by lowering the resolution without altering the size of the actual window which must remain fixed. If I alter it with glViewport, will it actually process fewer fragments, or is that purely a visual transformation? Assume I have early depth-testing on in my shader, if that matters at all.</p>\n"},{"tags":["sql","performance","sqlite","optimization"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":51,"score":3,"question_id":13143725,"title":"SQLite: speedup SQL statement with COUNT and GROUP BY","body":"<p>I am working with a table with a \"state\" column, which typically holds only 2 or 3 different values. Sometimes, when this table holds several million rows, following SQL statement becomes slow (I assume a full table scan is done):</p>\n\n<pre><code>SELECT state, count(*) FROM mytable GROUP BY state\n</code></pre>\n\n<p>I expect to get something like this:</p>\n\n<pre><code>disabled |  500000\nenabled  | 2000000\n</code></pre>\n\n<p>(basically I want to know how many items are \"enabled\" and how many items are \"disabled\" - actually that's a number instead of a text in my real application)</p>\n\n<p>I guess adding an index for my state column is pretty useless, since only very few different values can be found there. What other options do I have?</p>\n\n<p>There is also a \"timestamp\" column (with an index). Ideally the solution should also work well if I add:</p>\n\n<pre><code>WHERE timestamp BETWEEN x AND y\n</code></pre>\n\n<p>Right now I'm using an SQLite3 database, but it looks like other database engines are not too different, so solutions for other DB engines might be interesting as well.</p>\n\n<p>Thank you!</p>\n"},{"tags":["sql","performance","oracle","query-optimization"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":44,"score":0,"question_id":13141970,"title":"Performance of query with OR condition between join tables","body":"<p>There are two tables, tparent and tchild, which are created using script #1.\nIt's need to query all the information from the two tables where uname is equal to 'sky'.\nThe following query statement #2 is fast when there are only a few of records.\nBut it will become very slowly when inserting a huge number of records using the following script #3.\nI think it's caused by the OR condition between tables and the index will be unusefull for querying. \nSo I got a fast solution that changing the statement to three sub ones and union result, like #4.\nI want to konw is there a better solution? what's it? </p>\n\n<p>Thanks!</p>\n\n<p><strong># 1</strong></p>\n\n<pre><code>drop table tparent;\ndrop table tchild;\ncreate table tparent(typeid int,sno number,uname varchar2(50) );\ncreate table tchild(typeid int,sno number,seqno int,uname varchar2(50));\ncreate unique index uidx_tparent_typeidsno on tparent(typeid,sno);\ncreate unique index uidx_tchild_typeidsnoseqno on tchild(typeid,sno,seqno);\ncreate index idx_tparent_name on tparent(uname);\ncreate index idx_tchild_name on tchild(uname);\n\ninsert into tparent values (1,10,'lily');\ninsert into tparent values (1,11,'eric');\ninsert into tparent values (2,10,'tom');\ninsert into tparent values (2,11,'eric');\ninsert into tparent values (3,10,'sky');\n\ninsert into tchild values (1,10,1,'sam');\ninsert into tchild values (1,10,2,'sky');\ninsert into tchild values (1,11,1,'eric');\ninsert into tchild values (1,11,2,'john');\ninsert into tchild values (2,10,1,'sky');\ninsert into tchild values (2,11,1,'eric');\ninsert into tchild values (3,10,1,'tony');\n</code></pre>\n\n<p><strong># 2</strong></p>\n\n<pre><code>select p.typeid,p.sno,p.uname,c1.uname as uname1,c2.uname as uname2 from tparent p\n  left join tchild c1 on c1.typeid=p.typeid and c1.sno = p.sno and c1.seqno=1\n  left join tchild c2 on c2.typeid=p.typeid and c2.sno = p.sno and c2.seqno=2\nwhere (p.uname='sky' or c1.uname='sky' or c2.uname='sky');\n</code></pre>\n\n<p><strong># 3</strong></p>\n\n<pre><code>BEGIN\n    FOR x IN 1..10\n    LOOP\n        BEGIN\n            FOR y IN 10000..100000\n            LOOP\n                BEGIN\n                    insert into tparent values (x,y,'name'|| y);\n                    insert into tchild values (x,y,1,'name'|| y);\n                    insert into tchild values (x,y,2,'name'|| y);\n                END;\n            END LOOP;\n            COMMIT;\n        END;\n    END LOOP;\nEND;\n</code></pre>\n\n<p><strong>#4</strong></p>\n\n<pre><code>select typeid,sno,max(uname),max(uname1),max(uname2) from (\n\nselect p.typeid,p.sno,p.uname,c1.uname as uname1,c2.uname as uname2 from tparent p\n  left join tchild c1 on c1.typeid=p.typeid and c1.sno = p.sno and c1.seqno=1\n  left join tchild c2 on c2.typeid=p.typeid and c2.sno = p.sno and c2.seqno=2\nwhere (p.uname='sky' )\nunion \nselect p.typeid,p.sno,p.uname,c1.uname as uname1,c2.uname as uname2 from tparent p\n  left join tchild c1 on c1.typeid=p.typeid and c1.sno = p.sno and c1.seqno=1\n  left join tchild c2 on c2.typeid=p.typeid and c2.sno = p.sno and c2.seqno=2\nwhere ( c1.uname='sky' )\nunion\n\nselect p.typeid,p.sno,p.uname,c1.uname as uname1,c2.uname as uname2 from tparent p\n  left join tchild c1 on c1.typeid=p.typeid and c1.sno = p.sno and c1.seqno=1\n  left join tchild c2 on c2.typeid=p.typeid and c2.sno = p.sno and c2.seqno=2\nwhere ( c2.uname='sky')\n) tb group by typeid,sno\norder by typeid,sno\n;\n</code></pre>\n"},{"tags":["c","performance","pthreads","coroutine"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":40,"score":1,"question_id":13144044,"title":"Performance characteristics of pthreads vs ucontext","body":"<p>I'm trying to port a library that uses ucontext over to a platform which supports pthreads but not ucontext.  The code is pretty well written so it should be relatively easy to replace all the calls to the ucontext API with a call to pthread routines.  However, does this introduce a significant amount of additional overhead?  Or is this a satisfactory replacement.  I'm not sure how ucontext maps to operating system threads, and the purpose of this facility is to make coroutine spawning fairly cheap and easy.</p>\n\n<p>So, question is:  Does replacing ucontext calls with pthread calls significantly change the performance characteristics of a library?</p>\n"},{"tags":["c#",".net","winforms","performance","splash-screen"],"answer_count":4,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":1249,"score":4,"question_id":5743458,"title":"Show a splash screen at once","body":"<p>We are dealing with slow start for WinForm applications (it is a large application and has many control assemblies). Control assemblies are DevComponents. Ngen was applied to prevent jit compilation, but the loading time just decreased a little. </p>\n\n<p>The app has a splash screen, but it appears only in 12 seconds after the app has started. Is there any approach to show the splash screen at once?</p>\n\n<p>Our current suggestion is to create  a lightweight app with the splash screen, run the main app in a separate process, and close the lightweight app when initialization of the main app is done.</p>\n"},{"tags":["java","c","performance","caching","data-structures"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":86,"score":0,"question_id":13142896,"title":"Array of Structs are always faster than Structs of arrays?","body":"<p>I was wondering if the data layout Structs of Arrays (SoA) is always faster than Array of Structs (AoS) or Array of Pointers (AoP) for problems with an input that only fits in RAM in C/JAVA.</p>\n\n<p>Some days ago i was improving the performance of an Molecular dynamic algorithm (in C), summarizing in this algorithm it is calculated the force interaction between particles based on their force and position.</p>\n\n<p>Original the particles were represented by a struct containing 9 different doubles, 3 for particles forces (Fx,Fy,Fz) , 3 for positions and 3 for velocity, and the algorithm had a array containing pointers to all particles (AoP). I decided to change the layout from AoP to SoA to improve the cache use.</p>\n\n<p>So now i have a Struct with 9 array where each array stores Forces, velocity and positions (x,y,z) of each particle, each particle is accessed by it own array index.</p>\n\n<p>I had a gain in performance (for an input that only fits in RAM) of about 1.9x, so i was wondering if typically changing from AoP or AoS to SoA it will always performance better, and if not in each type algorithms where this did not occurs  </p>\n"},{"tags":["mysql","performance","configuration","batch-insert"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":23,"score":0,"question_id":13139224,"title":"Tracing slow batch insert queries in mysql","body":"<p>I have a program that at some point performs thousands of inserts into a MySQL table. Those inserts are surrounded by a transaction. When I run the code on a dev machine using production data (copied prod. db), it takes a couple of minutes to complete. When I run it in production, it runs for over 30 minutes. </p>\n\n<p>Both dev and prod servers run MySQL 5.1 (with a minor version diff, 5.1.64 vs 5.1.41 in prod). The server is a powerful machine with 12 cores, 16GB RAM, fast disks, etc (compared to my puny dev computer). The only difference is that the production machine is also a MySQL replication master. However, the specific schema that I write to isn't replicated.</p>\n\n<p>I'm leaning towards placing the problem on <code>my.cnf</code> configuration values, but any other ideas will help. I've also noticed that although the specific schema isn't replicated, it also isn't ignored in the binlog (<code>binlog-ignore-db = &lt;db-name&gt;</code> in the <code>[mysqld]</code> section), so this is also something I'd like to look into. </p>\n\n<p>What are other red flags I should pay attention to in configuration values to improve the speed of thousands-of-inserts scale transactions? Where else should I be looking to improve batch insert performance? thanks.</p>\n\n<h3>EDIT - the code that does this (very simplified)</h3>\n\n<p>in ruby, using <code>mysql2</code> adapter:</p>\n\n<pre><code>inserts = []\n\n# a loop that generates INSERT statements\n\n...\ninserts &lt;&lt; insert_stmt\n...\n\n# end loop\n\nbegin\n  connection.query(\"BEGIN;\")\n  inserts.each { |q| connection.query(q) }\n  connection.query(\"COMMIT;\")\nrescue\n  connection.query(\"ROLLBACK;\")\nend\n</code></pre>\n"},{"tags":["wpf","performance","drawingbrush"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":916,"score":1,"question_id":1337846,"title":"DrawingBrush Performance","body":"<p>Are there any differences when it comes to performance between the following three border objects?</p>\n\n<pre><code>&lt;Border Grid.Column=\"0\" Grid.ColumnSpan=\"2\" Opacity=\"1\"&gt;\n  &lt;Border.Background&gt;\n    &lt;DrawingBrush&gt;\n      &lt;DrawingBrush.Drawing&gt;\n        &lt;DrawingGroup&gt;\n          &lt;GeometryDrawing Brush=\"Red\"&gt;\n            &lt;GeometryDrawing.Geometry&gt;\n              &lt;GeometryGroup&gt;\n                &lt;RectangleGeometry Rect=\"0,0 100,1000\" /&gt;\n                &lt;LineGeometry StartPoint=\"0,0\" EndPoint=\"100,1000\"/&gt;\n                &lt;LineGeometry StartPoint=\"100,0\" EndPoint=\"0,1000\"/&gt;\n              &lt;/GeometryGroup&gt;\n            &lt;/GeometryDrawing.Geometry&gt;\n            &lt;GeometryDrawing.Pen&gt;\n              &lt;Pen Thickness=\"20\" Brush=\"Black\"/&gt;\n            &lt;/GeometryDrawing.Pen&gt;\n          &lt;/GeometryDrawing&gt;\n        &lt;/DrawingGroup&gt;\n      &lt;/DrawingBrush.Drawing&gt;\n    &lt;/DrawingBrush&gt;\n  &lt;/Border.Background&gt;\n&lt;/Border&gt;\n\n&lt;Border Grid.Column=\"0\" Grid.ColumnSpan=\"2\" Opacity=\"1\"&gt;\n  &lt;Border.Background&gt;\n    &lt;DrawingBrush&gt;\n      &lt;DrawingBrush.Drawing&gt;\n        &lt;DrawingGroup&gt;\n          &lt;GeometryDrawing Brush=\"Red\"&gt;\n            &lt;GeometryDrawing.Geometry&gt;\n              &lt;RectangleGeometry Rect=\"0,0 100,1000\" /&gt;\n            &lt;/GeometryDrawing.Geometry&gt;\n            &lt;GeometryDrawing.Pen&gt;\n              &lt;Pen Thickness=\"20\" Brush=\"Black\"/&gt;\n            &lt;/GeometryDrawing.Pen&gt;\n          &lt;/GeometryDrawing&gt;\n          &lt;GeometryDrawing&gt;\n            &lt;GeometryDrawing.Geometry&gt;\n              &lt;LineGeometry StartPoint=\"0,0\" EndPoint=\"100,1000\"/&gt;\n            &lt;/GeometryDrawing.Geometry&gt;\n            &lt;GeometryDrawing.Pen&gt;\n              &lt;Pen Thickness=\"20\" Brush=\"Black\"/&gt;\n            &lt;/GeometryDrawing.Pen&gt;\n          &lt;/GeometryDrawing&gt;\n          &lt;GeometryDrawing&gt;\n            &lt;GeometryDrawing.Geometry&gt;\n              &lt;LineGeometry StartPoint=\"100,0\" EndPoint=\"0,1000\"/&gt;\n            &lt;/GeometryDrawing.Geometry&gt;\n            &lt;GeometryDrawing.Pen&gt;\n              &lt;Pen Thickness=\"20\" Brush=\"Black\"/&gt;\n            &lt;/GeometryDrawing.Pen&gt;\n          &lt;/GeometryDrawing&gt;\n        &lt;/DrawingGroup&gt;\n      &lt;/DrawingBrush.Drawing&gt;\n    &lt;/DrawingBrush&gt;\n  &lt;/Border.Background&gt;\n&lt;/Border&gt;\n\n&lt;Border Grid.Column=\"3\" Grid.ColumnSpan=\"2\" Opacity=\"1\"&gt;\n  &lt;Image Stretch=\"Uniform\"&gt;\n    &lt;Image.Source&gt;\n      &lt;DrawingImage&gt;\n        &lt;DrawingImage.Drawing&gt;\n          &lt;DrawingGroup&gt;\n            &lt;GeometryDrawing Brush=\"Red\"&gt;\n              &lt;GeometryDrawing.Geometry&gt;\n                &lt;GeometryGroup&gt;\n                  &lt;RectangleGeometry Rect=\"0,0 100,1000\" /&gt;\n                  &lt;LineGeometry StartPoint=\"0,0\" EndPoint=\"100,1000\"/&gt;\n                  &lt;LineGeometry StartPoint=\"100,0\" EndPoint=\"0,1000\"/&gt;\n                &lt;/GeometryGroup&gt;\n              &lt;/GeometryDrawing.Geometry&gt;\n              &lt;GeometryDrawing.Pen&gt;\n                &lt;Pen Thickness=\"20\" Brush=\"Black\"/&gt;\n              &lt;/GeometryDrawing.Pen&gt;\n            &lt;/GeometryDrawing&gt;\n          &lt;/DrawingGroup&gt;\n        &lt;/DrawingImage.Drawing&gt;\n      &lt;/DrawingImage&gt;\n    &lt;/Image.Source&gt;\n  &lt;/Image&gt;\n&lt;/Border&gt;\n</code></pre>\n"},{"tags":["php","performance","image","analytics"],"answer_count":3,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":508,"score":3,"question_id":4665960,"title":"Most efficient way to display a 1x1 GIF (tracking pixel, web beacon)","body":"<p>I'm building a basic analytics service, based in theory off of how Google Analytics works, but instead of requesting an actual image, I'm routing the image request to a script that accepts the data and then outputs an image. Since browsers will be requesting this image on every load, every millisecond counts.</p>\n\n<p>I'm looking for the most efficient way for a file to output a gif file from a PHP script. So far, I've established 3 main methods. </p>\n\n<p><strong>Is there a more efficient way for me output a 1x1 GIF file from within a PHP script? If not, which of these is the most efficient and scalable?</strong> </p>\n\n<h2>Three Identified Methods</h2>\n\n<p>PHP image building libraries</p>\n\n<pre><code>$im = imagecreatetruecolor(1, 1);\nimagefilledrectangle($im, 0, 0, 0, 0, 0xFb6b6F);\nheader('Content-Type: image/gif');\nimagegif($im);\nimagedestroy($im);\n</code></pre>\n\n<p><code>file_get_contents</code> the image off of the server and output it</p>\n\n<pre><code>$im = file_get_contents('raw.gif'); \nheader('Content-Type: image/gif'); \necho $im; \n</code></pre>\n\n<p><code>base64_decode</code> the image</p>\n\n<pre><code>header('Content-Type: image/gif');\necho base64_decode(\"R0lGODdhAQABAIAAAPxqbAAAACwAAAAAAQABAAACAkQBADs=\");\n</code></pre>\n\n<p>(My gut was that base64 would be fastest, but I have no idea how resource intensive that function is; and that file_get_contents would likely scale less well, since it adds another file-system action.)</p>\n\n<p>For reference, the GIF I'm using is here: <a href=\"http://i.stack.imgur.com/LQ1CR.gif\" rel=\"nofollow\">http://i.stack.imgur.com/LQ1CR.gif</a></p>\n\n<h2>EDIT</h2>\n\n<p>So, the reason I'm serving this image is that my analytics library builds a query string and attaches it to this image request. Rather than parse logs, I'm routing the request to a PHP script which processes the data and responds with an image,so that the end user's browser doesn't hang or throw an error. My question is, how do I best serve that image within the confines of a script?</p>\n"},{"tags":["php","arrays","performance","maintainability"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":48,"score":1,"question_id":12929837,"title":"What is more performant, multiple Arrays or one Array containing multiple arrays?","body":"<p>What will be more performant and ressource friendlier?</p>\n\n<p>To use</p>\n\n<pre><code>$array1=Array();\n$array2=Array();\n$array3=Array();\n</code></pre>\n\n<p>or:</p>\n\n<pre><code>$arr=Array();\n$arr[] = Array();\n$arr[] = Array();\n$arr[] = Array();\n</code></pre>\n\n<p>And what is better to handle in the code when maintenance is required?</p>\n\n<p>I have to handle about 2800 different arrays so the performance is very important.</p>\n"},{"tags":["python","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":4,"down_vote_count":1,"view_count":72,"score":3,"question_id":13140619,"title":"Python performance of conditional evaluation","body":"<p>I was trying to find out if there's any penalty for negating a boolean when evaluation a conditional statement (python 2.6.6). I first tried this simple test (no <code>else</code> branch) </p>\n\n<pre><code>&gt;&gt;&gt; import timeit\n&gt;&gt;&gt; timeit.timeit(\"if not True: pass\", number=100000)\n0.011913061141967773\n&gt;&gt;&gt; timeit.timeit(\"if True: pass\", number=100000)\n0.018882036209106445\n</code></pre>\n\n<p>So I though the results where skewed because the pass statement might be translated to a noop which is at least <em>some</em> operation.</p>\n\n<p>I did a second try and got these results:</p>\n\n<pre><code>&gt;&gt;&gt; timeit.timeit(\"a=False\\nif not a: pass\\nelse: pass\", number=100000)\n0.02387714385986328\n&gt;&gt;&gt; timeit.timeit(\"a=False\\nif a: pass\\nelse: pass\", number=100000)\n0.015386819839477539\n&gt;&gt;&gt; timeit.timeit(\"a=True\\nif a: pass\\nelse: pass\", number=100000)\n0.02389812469482422\n&gt;&gt;&gt; timeit.timeit(\"a=True\\nif not a: pass\\nelse: pass\", number=100000)\n0.015424966812133789\n</code></pre>\n\n<p>I didn't expect to see any large penalty but from this results it looks like evaluating the <code>else</code> branch is cheaper than the implicit <code>then</code> branch. And the difference is huge!</p>\n\n<p>So The third attempt return these results:</p>\n\n<pre><code>&gt;&gt;&gt; timeit.timeit(\"if True: a=1\\nelse: a=1\", number=100000)\n0.022008895874023438\n&gt;&gt;&gt; timeit.timeit(\"if not True: a=1\\nelse: a=1\", number=100000)\n0.022121906280517578\n</code></pre>\n\n<p>And finally I got the expected results. Though out of curiosity I tried a last time:</p>\n\n<pre><code>&gt;&gt;&gt; timeit.timeit(\"if False: a=1\\nelse: a=1\", number=100000)\n0.02385997772216797\n&gt;&gt;&gt; timeit.timeit(\"if not False: a=1\\nelse: a=1\", number=100000)\n0.02244400978088379\n</code></pre>\n\n<p>And that's it...\nI have no idea why the negated condition leading to the <code>then</code> branch is faster.</p>\n\n<p>What might be happening?</p>\n\n<p>All those results are reproducible on my computer, it doesn't matter how many times I run them I get pretty much the same results.</p>\n\n<p>I think the first tests where skewed because the compiler might have removed the <code>else: pass</code> part altogether. Is that possible?</p>\n\n<p>Might all this results be related to the branch predictor in the CPU?</p>\n\n<p>Any other possible culprits?</p>\n"},{"tags":["javascript","asp.net","performance","checksum"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":26,"score":1,"question_id":13140298,"title":"Performance Javascript MD5 Checksum generation","body":"<p>I am trying to create a .net web site that allows users to upload files along with some form data.  To verify file integrity of file uploaded, a client side javascript checksum would be passed up to compare against a server generated checksum. My question is the javascript checksum generation performance if say 1-5 of the files uploading are 1 gb. The uploader is based on this project, <a href=\"http://www.codeproject.com/Articles/460142/ASP-NET-Multiple-File-Upload-With-Drag-Drop-and-Pr\" rel=\"nofollow\">http://www.codeproject.com/Articles/460142/ASP-NET-Multiple-File-Upload-With-Drag-Drop-and-Pr</a> </p>\n"},{"tags":["javascript","jquery","html","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":59,"score":0,"question_id":13139003,"title":"What would be a more programmatically effective way of doing this","body":"<p>I have built a simple accordian type side menu and looking at it, it's pretty heavy for what it does. What methods can I learn to reduce the amount of code and time to execute if any?</p>\n\n<p>I am mainly asking this as a learning point.</p>\n\n<pre><code>$('#one').css(\"height\", \"22\");\n$('#dtwo').css(\"height\", \"22\"); \n$('#three').css(\"height\", \"22\");   \n    $('#t1').click(function() {\n      if ($('#one').hasClass(\"extended\")) {\n        $('#one').stop(true, true).animate({height: '22px'},500);\n        $('#one').removeClass(\"extended\");\n        $('#a1').stop(true, true).animate({opacity: '1'},500);\n      } else {\n        $('#one').animate({height: '120' + 'px'},500);\n        $('#one').addClass(\"extended\");\n        $('#a1').animate({opacity: '0'},300);\n      }\n});\n\n$('#t2').click(function() {\n      if ($('#dtwo').hasClass(\"extended\")) {\n        $('#dtwo').stop(true, true).animate({height: '22px'},500);\n        $('#dtwo').removeClass(\"extended\");\n        $('#a2').stop(true, true).animate({opacity: '1'},500);\n      } else {\n        var height = 0;\n        $(this).closest(\"div\").children().each(function(){\n           height += $(this).outerHeight(true);\n        });\n        $('#dtwo').animate({height: height + 5 + 'px'},500);\n        $('#dtwo').addClass(\"extended\");\n        $('#a2').animate({opacity: '0'},300);\n      }\n});\n\n $('#t3').click(function() {\n      if ($('#three').hasClass(\"extended\")) {\n        $('#three').stop(true, true).animate({height: '22px'},500);\n        $('#three').removeClass(\"extended\");\n        $('#a3').stop(true, true).animate({opacity: '1'},500);\n      } else {\n        $('#three').animate({height: '270px'},500);\n        $('#three').addClass(\"extended\");\n        $('#a3').animate({opacity: '0'},300);\n      }\n});\n\n $('#a1').click(function() {\n      if ($('#one').hasClass(\"extended\")) {\n        $('#one').stop(true, true).animate({height: '22px'},500);\n        $('#one').removeClass(\"extended\");\n        $('#a1').stop(true, true).animate({opacity: '1'},500);\n      } else {\n        $('#one').animate({height: '120px'},500);\n        $('#one').addClass(\"extended\");\n        $('#a1').animate({opacity: '0'},300);\n      }\n});\n\n$('#a2').click(function() {\n      if ($('#dtwo').hasClass(\"extended\")) {\n        $('#dtwo').stop(true, true).animate({height: '22px'},500);\n        $('#dtwo').removeClass(\"extended\");\n        $('#a2').stop(true, true).animate({opacity: '1'},500);\n      } else {\n        $('#dtwo').animate({height: '120px'},500);\n        $('#dtwo').addClass(\"extended\");\n        $('#a2').animate({opacity: '0'},300);\n      }\n});\n\n $('#a3').click(function() {\n      if ($('#three').hasClass(\"extended\")) {\n        $('#three').stop(true, true).animate({height: '22px'},500);\n        $('#three').removeClass(\"extended\");\n        $('#a3').stop(true, true).animate({opacity: '1'},500);\n      } else {\n        $('#three').animate({height: '270px'},500);\n        $('#three').addClass(\"extended\");\n        $('#a3').animate({opacity: '0'},300);\n      }\n});\n</code></pre>\n"},{"tags":["python","performance","indexing","deduplication"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":227,"score":1,"question_id":9495623,"title":"Best way or algorithm to near duplicate check against huge list of files?","body":"<p>I am using python to neardupe huge list of file (over 20000 ) files. Totaling about 300 MB</p>\n\n<p>Current way is to do near-dupe checking using difflib's SequenceMatcher and getting result using QuickRatio .</p>\n\n<p>With 4 worker process it takes 25 hours to get the job done , which is quite slow.</p>\n\n<p>I also tried Livenstheine which gives C base near-dupe checking but its even slower and less accurate than difflib.</p>\n\n<p>The checking need to be done in this manner : \nThere are 20000 files in a folder. each file need to compare against 20000 files in the folder on every iterations. so there will be 20000 * 20000 iterations.</p>\n\n<p>What I think of is to index all the files and comparing indexes but i am new to indexing and i am not sure it would work. If that the way what is the best indexing options?</p>\n\n<p>Thanks.</p>\n\n<p>Below is the code :</p>\n\n<pre><code>import os,sys,chardet, csv,operator,time,subprocess\nfrom difflib import SequenceMatcher\nimport threading\n#from threading import Timer\nimport multiprocessing\nfrom multiprocessing import Pool\n\nOrgFile = \"\"\nmark = int(sys.argv[2])\n\ndef init_logger():\n    print \"Starting %s\" % multiprocessing.current_process().name\n\n#----Get_Near_DupeStatus--------#\ndef Get_Near_DupeStatus(score):\n    if score &gt; 30 and score &lt;= 50:\n        return \"Low Inclusive\"\n    elif score &gt; 50 and score &lt;= 75:\n        return \"Inclusive\"\n    elif score &gt; 75 and score &lt;= 85:\n        return \"Most Inclusive\"\n    elif score &gt; 85 and score &lt;= 99:\n        return \"Near-Dupe\"\n    elif score == 100:\n        return \"Unique\"\n    else: return \"No Inclusive\"\n\n#----Write_To_CSV --- ALL-------#\ndef Write_To_CSV_All(List):\n    writer = csv.writer(open('./TableList.csv','wb'),delimiter=';', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n    writer.writerow(['Path/FileName(Source);'+'ID;'+'NearDupeID;'+'Similarity Score;'+'Near_DupeStatus;'+'NearDupeProcess(Y/N);'+'Encoding'])\n    for i,li in enumerate(sorted(List, key=operator.itemgetter(\"NearDupeID\"))):\n        writer.writerow([li['Path/FileName(Source)']+\";\"+'ID00'+str(i+1)+\";\"+str(li['NearDupeID'])+\";\"+str(li['Similarity Score'])+\";\"+li['Near_DupeStatus']+\";\"+li['NearDupeProcess(Y/N)']+\";\"+li['Encoding']])\n\n#Get Finish File List\ndef Finish_Files(List,count,id):\n    finish_files = []\n    for i,li in enumerate(sorted(List, key=operator.itemgetter(\"Similarity Score\"), reverse=True)):\n        if i &lt; count:\n            li['NearDupeID'] = id\n            finish_files.append(li)\n        if count == 0:\n            li['NearDupeID'] = id\n#            if li['Similarity Score'] &gt; 50:\n            finish_files.append(li)\n    return finish_files\n\n#----Search Files in Dir--------#\ndef GetFileListFrom_Dir(dir):\n    FileList = []\n    for root,dirs,filenames in os.walk(dir):\n        for filename in filenames:\n            realpath = os.path.join(root, filename)\n            FileList.append(realpath)\n    return FileList\n\n#----Matcher--------#\ndef Matcher(realpath):\n    junk = [\"\\t\",\"\\n\",\"\\r\"]\n    score = 0\n    dict = {}\n    MatchFile = \"\"\n    dupe_Process = 'N'\n    if os.path.isfile(realpath):\n        MatchFile =  open(realpath).read()\n        matcher = SequenceMatcher(lambda x: x in junk,OrgFile, MatchFile)\n        score = int(matcher.ratio()*100)\n        if score &gt;= mark:\n            encoding = chardet.detect(MatchFile)['encoding']\n            if encoding == None: encoding = 'None'\n            if score &gt; 85: dupe_Process = 'Y'\n            dict = {'Path/FileName(Source)':realpath,'Similarity Score':score,'Near_DupeStatus':Get_Near_DupeStatus(score),'NearDupeProcess(Y/N)':dupe_Process,'Encoding':encoding}\n            return dict\n\n#-------------Pooling--------------------#\ndef MatcherPooling(FileList,orgFile,process):\n    global OrgFile\n    OrgFile = open(orgFile).read()\n    pool_obj = Pool(processes=process)\n    #pool_obj = Pool(processes=process,initializer=init_logger)\n    dict = {}\n    DictList = []\n    dict = pool_obj.map(Matcher,FileList)\n    DictList.append(dict)\n    pool_obj.close()\n    pool_obj.join()\n    return DictList\n\ndef Progress():\n    p = \"/-\\\\|\"\n#    global t\n    for s in p:\n        time.sleep(0.1)\n        sys.stdout.write(\"%c\" % s)\n        sys.stdout.flush()\n        sys.stdout.write('\\b')\n    t2 = threading.Timer(0.1,Progress).start()\n#    t.start()\n\n\n#----Main--------#\ndef Main():\n    Mainls = []\n    dictList = []\n    finish_List = []\n    BLINK = '\\033[05m'\n    NOBLINK = '\\033[25m'\n    dir = sys.argv[1]\n    process = int(sys.argv[3])\n    Top_rec = int(sys.argv[4])\n    Mainls = GetFileListFrom_Dir(dir)\n    bar = \"*\"\n    # setup toolbar\n    sys.stdout.write(\"%s\" % BLINK+\"Processing....\"+ NOBLINK + \"With \"+ str(process) + \" Multi Process...\")#+\" \\n\")\n    if Top_rec != 0:\n        charwidth = len(Mainls)/Top_rec\n    elif Top_rec == 0: charwidth = len(Mainls)\n    t = threading.Timer(0.1,Progress)\n    t.start()\n#    sys.stdout.write(\"[%s]\" % (\"-\" * charwidth))\n#    sys.stdout.flush()\n#    sys.stdout.write(\"\\b\" * (charwidth+1)) # return to start of line, after '['\n\n    #----------------------------------------------------------#\n    for id,orgFile in enumerate(sorted(Mainls)):\n        for dl in MatcherPooling(sorted(Mainls),orgFile,process):\n            for dict in dl:\n                if dict != None:\n                    dictList.append(dict)\n\n            #Append Finish Files List For CSV ALL(Write Once)\n            fl = Finish_Files(dictList,Top_rec,id+1)\n            if Top_rec != 0:\n                for del_List in fl:\n                    Mainls.remove(del_List['Path/FileName(Source)'])\n                    Mainls.sort()\n\n            finish_List.extend(fl)\n            dictList = []\n\n        sys.stdout.write(\"%s\" % bar)\n        sys.stdout.flush()\n\n        #Exit Loop\n        if len(Mainls) == 0:\n            break\n    #----------------------------------------------------------#\n    Write_To_CSV_All(finish_List)\n    #print os.system('clear')\n    sys.stdout.write(\"%s\" % \" \")\n    print \"Finished!\"\n    t.cancel()\n    print os._exit(99)\n\nif __name__ == '__main__':\n    Main()\n</code></pre>\n"},{"tags":["performance","matlab","vectorization"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":51,"score":1,"question_id":13136929,"title":"Can vectorization be done? will it do any good?","body":"<p>This function here is eating a lot of time in my run. But what is see is the most of the time goes in the inbuilt function <code>polyarea</code>. Can this code be vectorized for performance boost? </p>\n\n<p>Profiler Report - </p>\n\n<pre><code>  time   calls\n                  1 function [S S_area] = Polygons_intersection_Compute_area(S)\n                  2 % Guillaume JACQUENOT\n                  3 % guillaume at jacquenot at gmail dot com\n                  4 % 2007_10_08\n                  5 % 2009_06_16\n                  6 % Compute area of each polygon of in S.\n                  7 % Results are stored as a field in S\n                  8 \n  0.50   51945    9 S_area = struct('A', {}); \n  0.20   51945   10 for i=1:numel(S) \n  0.28  103890   11     S(i).area = 0; \n  1.34  103890   12     S_area(i).A = zeros(1,numel(S(i).P)); \n  0.69  103890   13     for j=1:numel(S(i).P) \n  9.24  103890   14         S_area(i).A(j) = polyarea(S(i).P(j).x,S(i).P(j).y); \n  0.28  103890   15         S(i).area      = S(i).area + (1-2*S(i).P(j).hole) * S_area(i).A(j);         \n  0.01  103890   16     end \n  0.08  103890   17 end \n</code></pre>\n"},{"tags":["c","linux","performance","data-structures","g-wan"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":210,"score":4,"question_id":12451398,"title":"Techniques to improve transaction rate","body":"<p>Lighttpd, nginx and others use a range of techniques to provide maximum application performance such as AIO, sendfile, MMIO, caching and epoll and lock free data structures.  </p>\n\n<p>My collegue and I have written a little application server which uses many of these techniques and can also server static files.  So we tested it with apache bench and compared ours with lighttpd and nginx and have at least matched the performance for static content for files from 100 bytes to 1K.</p>\n\n<p>However, when we compare the transaction rate over the same static files to that of G-WAN, G-WAN is miles ahead.</p>\n\n<p>I know this question may be a little subjective but what techniques apart from the obvious ones I've mentioned might Pierre Gauthier be using in GWAN that would enable him to achieve such astounding performance?</p>\n"},{"tags":["performance","cuda","real-time"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":92,"score":1,"question_id":13131183,"title":"Is CUDA suitable for real-time applications?","body":"<p>In continuation of my <a href=\"http://stackoverflow.com/questions/13130967/why-cuda-memory-copy-speed-behaves-like-this-some-constant-driver-overhead\">previous question</a>. Is CUDA suitable for real-time quick applications?\nThe task is: I need my application to make a lot of calculations in 0.1-0.3 ms. CUDA kernels cope with these calculations in a very good time suitable for my project, but with all the overheads I get (memory copy) the time is not acceptable.</p>\n\n<p>Is CUDA just not usable for this kind of applications or there are some hacks to avoid sutuations described in my previous question?</p>\n\n<p><a href=\"http://real-time.ccur.com/Real-Time_CUDA.aspx\" rel=\"nofollow\">These guys</a> provide so called \"GPU Workbench\" with the modified gpu driver built on their own linux verson. They say that their system performs much faster then typical GPU configuraions. Anyone knows about them?</p>\n"},{"tags":["android","performance","optimization","internationalization"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":43,"score":0,"question_id":13135249,"title":"Best practice for mapping variables with translations","body":"<p>I have a Table with lots of rows which should be translated. Most of the values are integers so the values don't need to be translated.</p>\n\n<p>However my data comes from a JSON structure where the data are stored normal in key value pairs. E.g.:</p>\n\n<pre><code>{\n  \"age\":24,\n  \"hair-color\":\"black\",\n  \"weight\":42,\n  \"height\":123,\n  // ...\n}\n</code></pre>\n\n<p>So far I have a strings.xml which looks like this:</p>\n\n<pre><code>&lt;resources&gt;\n  &lt;string name=\"meta_age\"&gt;Alter&lt;/string&gt;\n  &lt;string name=\"meta_hair_color\"&gt;Haarfarbe&lt;/string&gt;\n  &lt;string name=\"meta_weight\"&gt;Gewicht&lt;/string&gt;\n  &lt;string name=\"meta_height\"&gt;Körpergröße&lt;/string&gt;\n  &lt;!-- ... --&gt;\n&lt;/resources&gt;\n</code></pre>\n\n<p>This data are visualisated in a list view with a custom ArrayAdapter. This works fine however I'm not sure what is the best way for mapping the key value pairs with its translations.</p>\n\n<p>I have now this code here:</p>\n\n<pre><code>public static final int[] fields = new int[] {R.string.meta_age, R.string.meta_hair_color, R.string.meta_weight, R.string.meta_height, /* ... */;\n</code></pre>\n\n<p>For the building the UI the strings need to be mapped to the <em>keys</em> so far this is done with this messy code:</p>\n\n<pre><code>List&lt;Integer&gt; fieldIndex = new ArrayList&lt;Integer&gt;(fields.length);\nfor(int i : fields) {\n    fieldIndex.add(i);\n}\ntranslation = new HashMap&lt;String, Integer&gt;();\ntranslation.put(\"age\", fieldIndex.indexOf(R.string.meta_age));\ntranslation.put(\"hair-color\", fieldIndex.indexOf(R.string.meta_hair_color));\ntranslation.put(\"weight\", fieldIndex.indexOf(R.string.meta_weight));\ntranslation.put(\"height\", fieldIndex.indexOf(R.string.meta_height));\n</code></pre>\n\n<p>The next step is that I now know the target index for the json field and put them together with the fields list of string ids. But I think that this code is not very performant. How can I write that better?</p>\n"},{"tags":["css","performance","css3","css-transitions","transition"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":447,"score":0,"question_id":9516047,"title":"Does the CSS3 transition slow down a website?","body":"<p>I have currently added the <strong>CSS3 transition</strong> to my website.\nI'm not shure if it is possible, that it slows down my website, but everything seems flickering and there's this \"jerky behaviour\" on the transitions and flash videos.</p>\n\n<p>I'm using Mozilla Firefox 10.0.02.</p>\n\n<p>I added following to my CSS stylesheet:  </p>\n\n<pre><code>*:link, *:visited, *:hover, *:active, *:focus {\n    -webkit-transition: color .25s linear, background-color .25s linear, border-color .25s linear;\n    -o-transition: color .25s linear, background-color .25s linear, border-color .25s linear;\n    -moz-transition: color .25s linear, background-color .25s linear, border-color .25s linear;\n    transition: color .25s linear, background-color .25s linear, border-color .25s linear;\n}\n</code></pre>\n\n<p>Can you tell me if it's rather my browser being slow or if it is the CSS I added and if, then what's the evidence?</p>\n\n<p>Thank you!</p>\n"},{"tags":["c++","multithreading","performance","c++11","atomic"],"answer_count":3,"favorite_count":3,"up_vote_count":7,"down_vote_count":1,"view_count":304,"score":6,"question_id":13135834,"title":"std::atomic<bool> is VERY slow","body":"<p>I've been using volatile bool for years for thread execution control and it worked fine</p>\n\n<pre><code>// in my class declaration\nvolatile bool stop_;\n\n-----------------\n\n// In the thread function\nwhile (!stop_)\n{\n     do_things();\n}\n</code></pre>\n\n<p>Now, since c++11 added support for atimic operations, I decied to try that instead</p>\n\n<pre><code>// in my class declaration\nstd::atomic&lt;bool&gt; stop_;\n\n-----------------\n\n// In the thread function\nwhile (!stop_)\n{\n     do_things();\n}\n</code></pre>\n\n<p>But it's several orders of magnitude slower than the <code>volatile bool</code>!</p>\n\n<p>Simple test case I've written takes about 1 second to complete with <code>volatile bool</code> approach. With <code>std::atomic&lt;bool&gt;</code> however I've been waiting for about 10 minutes and gave up!</p>\n\n<p>I tried to use <code>memory_order_relaxed</code> flag with <code>load</code> and <code>store</code> to the same effect. </p>\n\n<p>My platform:\nWindows 7 64 bit\nMinGW gcc 4.6.x</p>\n\n<p>What I'm doing wrong?</p>\n\n<p><strong>UPD</strong></p>\n\n<p>Yes, I know that volatile does not make a variable thread safe. My question is not about volatile, it's about why atomic is redicolously slow.</p>\n\n<p><strong>UPD2</strong>\n@all, thank you for your comments - I will try all the suggested when I get to my machine tonight.</p>\n"},{"tags":["c++","performance","optimization","x64","double-precision"],"answer_count":3,"favorite_count":0,"up_vote_count":8,"down_vote_count":0,"view_count":572,"score":8,"question_id":9283717,"title":"Why c++ program compiled for x64 platform is slower than compiled for x86?","body":"<p>I've wrote program, and compiled it for x64 and x86 platform in Visual Studio 2010 on Intel Core i5-2500. x64 version take about 19 seconds for execution and x86 take about 17 seconds. What can be the reason of such behavior?</p>\n\n<pre><code>#include \"timer.h\"\n\n#include &lt;vector&gt;\n#include &lt;iostream&gt;\n#include &lt;algorithm&gt;\n#include &lt;string&gt;\n#include &lt;sstream&gt;\n\n/********************DECLARATIONS************************************************/\nclass Vector\n{\npublic:\n    Vector():x(0),y(0),z(0){}\n\n    Vector(double x, double y, double z)\n        : x(x)\n        , y(y)\n        , z(z)\n    {\n    }\n\n    double x;\n    double y;\n    double z;\n};\n\n\ndouble Dot(const Vector&amp; a, const Vector&amp; b)\n{\n    return a.x * b.x + a.y * b.y + a.z * b.z;\n}\n\n\nclass Vector2\n{\npublic:\n    typedef double value_type;\n\n    Vector2():x(0),y(0){}\n\n    Vector2(double x, double y)\n        : x(x)\n        , y(y)\n    {\n    }\n\n    double x;\n    double y;\n};\n\n/******************************TESTS***************************************************/\n\nvoid Test(const std::vector&lt;Vector&gt;&amp; m, std::vector&lt;Vector2&gt;&amp; m2)\n{\n    Vector axisX(0.3f, 0.001f, 0.25f);\n    Vector axisY(0.043f, 0.021f, 0.45f);\n\n    std::vector&lt;Vector2&gt;::iterator i2 = m2.begin();\n\n    std::for_each(m.begin(), m.end(),\n        [&amp;](const Vector&amp; v)\n    {\n        Vector2 r(0,0);\n        r.x = Dot(axisX, v);\n        r.y = Dot(axisY, v);\n\n        (*i2) = r;\n        ++i2;\n    });\n}\n\n\nint main()\n{\n    cpptask::Timer timer;\n\n    int len2 = 300;\n    size_t len = 5000000;\n    std::vector&lt;Vector&gt; m;\n    m.reserve(len);\n    for (size_t i = 0; i &lt; len; ++i)\n    {\n        m.push_back(Vector(i * 0.2345, i * 2.67, i * 0.98));\n    }\n\n    /***********************************************************************************/\n    {\n        std::vector&lt;Vector2&gt; m2(m.size());\n        double time = 0;\n        for (int i = 0; i &lt; len2; ++i)\n        {\n            timer.Start();\n            Test(m, m2);\n            time += timer.End();\n        }\n        std::cout &lt;&lt; \"Dot product double - \" &lt;&lt; time / len2 &lt;&lt; std::endl;\n    }\n    /***********************************************************************************/\n\n\n    return 0;\n}\n</code></pre>\n"},{"tags":["php","performance","apache","centos","lamp"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":35,"score":0,"question_id":13137702,"title":"how to find out what php scripts are slow with apache?","body":"<p>what are my methods of finding out what scripts on my server (LAMP (centos))?</p>\n\n<p>I've found mod_log_slow but it was last updated 2009. Is it worth trying?</p>\n\n<p>thanks</p>\n"},{"tags":["mysql","sql","performance","database-performance"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":27,"score":0,"question_id":13136278,"title":"Optimize sql request for statistics","body":"<p>I would like to perform some request in mysql that i know will be really slow:</p>\n\n<p>I have 3 tables:</p>\n\n<p>Users:</p>\n\n<pre><code>id, username, email\n</code></pre>\n\n<p>Question:</p>\n\n<pre><code>id, date, question\n</code></pre>\n\n<p>Answer</p>\n\n<pre><code>id_question, id_user, response, score\n</code></pre>\n\n<p>And i would like to do some statistics like the top X users with the best score (sum of all the scores) for all time or for a given amount of time (last month for example). Or it could be users between the 100th and the 110th range</p>\n\n<p>I will have thousands of users and hundred of questions so the requests could be very long since I'll need to order by sum of scores, limit to a given range and sometimes only select some questions depending on the date, ...</p>\n\n<p>I would like to know if there are some methods to optimize the requests!</p>\n"},{"tags":["performance","jqgrid","freeze","distortion","rownum"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":55,"score":1,"question_id":12927114,"title":"jqGrid freeze rowheader (rownum)","body":"<p>I am trying to freeze rownum column of grid. Because I have replaced text of rownum with meaningful row description.<br>\nI took help from jqgrid freezing columns demo. But it is not working out for rownum column.\nMy grid loads local array data and using version jQgrid 4.3.2\nAftert his I tried another approach to freeze first column with property </p>\n\n<pre><code>'frozen: true' and then I call function  $(grid_selector).jqGrid('setFrozenColumns');\n</code></pre>\n\n<p>and I get below issues:</p>\n\n<ul>\n<li>The grid loads becomes exceptionally slow (tested in IE, Firefox)</li>\n<li>After much wait when page loads, the row header alignment is distorted. All rowheaders get shifted one step up and overlaps Caption div.</li>\n</ul>\n\n<p>Row1Header goes into Caption place - Row2Header goes into Row1 and so on..</p>\n\n<p>My ultimate requirement is to freeze rowheaders. Any help is much appreciated.  </p>\n\n<p>Thanks</p>\n"},{"tags":["c#","performance","compiler"],"answer_count":4,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":74,"score":1,"question_id":13135759,"title":"What is the overhead for a method call in a loop?","body":"<p>I’ve been working on a C# maze generator for a while which can generate mazes of like 128000x128000 pixels. All memory usage is optimized already so I’m currently looking at speeding the generation up.</p>\n\n<p>A problem (well more off an interest point) I found was the following (just some example code to illustrate the problem):</p>\n\n<p>This code runs in about 1.4 seconds on my machine when pixelChanged is null:</p>\n\n<pre><code>public void Go()\n{\n    for (int i = 0; i &lt; bitjes.Length; i++)\n    {\n        BitArray curArray = bitjes[i];\n        for (int y = 0; y &lt; curArray.Length; y++)\n        {\n            curArray[y] = !curArray[y];\n            GoDrawPixel(i, y, false);\n        }\n    }\n}\n\npublic void GoDrawPixel(int i, int y, Boolean enabled)\n{\n    if (pixelChanged != null)\n    {\n        pixelChanged.Invoke(new PixelChangedEventArgs(i, y, enabled));\n    }\n}\n</code></pre>\n\n<p>Where the following code runs actually 0.4 seconds faster</p>\n\n<pre><code>public void Go()\n{\n    for (int i = 0; i &lt; bitjes.Length; i++)\n    {\n        BitArray curArray = bitjes[i];\n        for (int y = 0; y &lt; curArray.Length; y++)\n        {\n            curArray[y] = !curArray[y];\n            if (pixelChanged != null)\n            {\n                pixelChanged.Invoke(new PixelChangedEventArgs(i, y, false));\n            }\n        }\n    }\n}\n</code></pre>\n\n<p>It seems that when just calling an “empty” method takes about 20% of the cpu this algorithm uses. Isn’t this strange? I’ve tried to compile the solution in debug and release mode but haven’t found any noticeable differences.</p>\n\n<p>What this means is that every method call I have in this loop will slow my code down by about 0.4 seconds. Since the maze generator code currently consist of a lot of seperate method calls that excecute different actions this starts to get a substantial ammount.</p>\n\n<p>I've also checked google and other posts on Stack Overflow but haven't really found a solution yet.</p>\n\n<p>Is it possible to automatically optimize code like this? (Maybe with things like project Roslyn???) or should I place everything together in one big method?</p>\n\n<p>Edit:\nI'm also interested in maybe some analysis on the JIT/CLR code differences in these 2 cases. (So where this problem actually comes from)</p>\n\n<p>Edit2:\n<strong>All code was compiled in release mode</strong></p>\n"},{"tags":["performance","ember.js"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":69,"score":4,"question_id":13125146,"title":"Ember.JS: Observing @each, but just iterating over new/changed items","body":"<p>I'm currently observing some Ember arrays like so:</p>\n\n<pre><code> observe_array: function() {\n     this.get(\"my_array\").forEach(function(e,i) {\n         // do something\n     });\n }.observes(\"my_array.@each\");\n</code></pre>\n\n<p>Most times, if my_array is updated, multiple elements are added at once.\nHowever, the observer fires one-by-one as each element is added, which becomes extremely inefficient. Is there anyway to do this more efficiently? Essentially, I need to be able to have a mutated array based on \"my_array\"</p>\n\n<p>For reference, realistic sizes of my_array will be between 600-1200 elements. The \"do something\" block involves some operations that take a little more time - creating Date objects from strings and converting each element to json representation.</p>\n\n<p>Instead of doing an observer I also tried a property with the cacheable() method/flag, but that didn't seam to speed things up very much....</p>\n"},{"tags":["c#","winforms","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":51,"score":1,"question_id":13136157,"title":"Application performance when using user controls C#","body":"<p>I'm working on somewhat large application that is divided into groups by functionality. Since every functionality is mostly independent (they all use the same database, but there is no direct interaction between different functionalities), I'm using the user defined controls and treating them as individual \"applications\". The way the application works is this:</p>\n\n<ul>\n<li>The \"root\" application only contains the main menu and 2 panels. The main menu is used to select the group of functionalities.</li>\n<li>After the group is selected (on application startup the first group is automatically selected), the functionalities (represented by buttons) from that group are displayed in the first panel.</li>\n<li><p>The user selects the functionality (by clicking on the appropriate button) he/she wants and the user control that contains the \"form\" is displayed in the second panel.\nThe code that displays all of the user controls looks like this:</p>\n\n<pre><code>    panel2.Controls.Clear();\n    UserControl1 uc1 = new UserControl1();\n    uc1.Location = new Point(0, 0);\n    panel2.Controls.Add(uc1);\n    label6.Text = \"User control 1\";\n</code></pre></li>\n</ul>\n\n<p>So, when the user selects one of the functionalities, the application clears existing controls, and displays the selected one.\nThe application works fine (the part I implemented so far), so this is my question - how does this approach manage computer resources, mainly the memory. Specifically, if the user uses one functionality, and then switches to another one, will the .NET's services release the memory used by the previous functionality (I think garbage collector is in charge of that) and will the SQL connections, that I use to communicate with the database, be closed?\nAlso, are there some other issues that I should be aware of?\nAs I said, the functionalities work properly, but I'm still very far from full testing of the application as a whole (I only test every functionality individually when I create it, and only on the computer I create it on, so I can't consider it as a proper testing). Because of that, I am worried that the application's performances might deteriorate if the application is constantly used over a longer period of time.\nI'm using VS 2010 (C#) and SQL Server 2005 to create this application.\nIf you have any suggestions, please write them. With this question, I'm trying to prevent major reconstructions of the application once it comes to the phase of testing and implementing due to bad resource management.\nThanks.</p>\n"},{"tags":["php","sql-server","performance","apache","debugging"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":29,"score":2,"question_id":13135087,"title":"Apache Server and SQL Server 2008 Express Performance. Isolate, method or suggestions","body":"<p>I was wondering if some tips or guidance can be provided for the following issue. </p>\n\n<p>Environment type: Small office, approx 20 users.\nNetwork: All server, workstation belong to a single subnet\nInfrastructure: Virtualised webserver and virtualised database server.</p>\n\n<p>Currently I have a web server (Win7SP1 x64) running Apache 2.2.22 (Win32) and PHP 5.3.10. I have built some pages that connects to our database (SQL2k8 Express) and calls procedures, the procedures are basically just basic select statements with joins and conditions. Once the result set are returned, PHP loop through the result set, row by row and display the rows as it flow through. </p>\n\n<p>What happens is that from time to time, the result page can take a extremely long to load,where no results set are displayed but the web browser loading bar/icon flashes. Some times it will just give a blank screen at the end of the load, which typically means the query connection has timed-out or some times it will return the result set. For example, under normal behaviour, a page will take approx 1-3s to load, while at odd times, it may take up to 20-30s.</p>\n\n<p>I have noticed that if I run queries in SQLSVR Mgmt Studio, queries that may take ~5s to return a result set may take approx ~15s to load on my web server. </p>\n\n<p>I know poor PHP coding can cause slow downs but the odd performance behaviour has me bit baffled, as Im sure my coding isnt that bad.</p>\n\n<p>So if people could offer some tips in how I can go about diagnosting or isolating the problem, or advise areas that I could look at, it will be great. I know there are Apache logs, SQL SVR logs and performance indicators, but Im still fairly new to this area and am really lost in the approach /methodology I should take. </p>\n\n<p>Example, any specific mods I should enable in Apache etc. or the type of Apache server I should use, connection sockets in web server etc?</p>\n\n<p>Example of my PHP query/procedure calls</p>\n\n<pre><code>$queryStatement = \"exec myProc @para1='asd'\";\n$prepare = $dbConnect-&gt;prepare($queryStatement);\n$result = $prepare-&gt;execute();\nWHILE($result = $prepare-&gt;fetch(PDO::FETCH_ASSOC)){\nset_time_limit(180);\necho $result\n}\n</code></pre>\n\n<p>Cheers everyone :)</p>\n"},{"tags":[".net","performance","matlab"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":161,"score":4,"question_id":9912184,"title":".NET performance from Matlab","body":"<p>I have a large simulation suite written in Matlab, but due to concerns about better interfacing with other internal projects (as well execution speed) I'm thinking about moving some functionality to .NET and calling such objects from within Matlab. What is the overhead associated with calling .NET objects from <em>within</em> Matlab?</p>\n\n<p>Here's a good <a href=\"http://stackoverflow.com/questions/1693429/is-matlab-oop-slow-or-am-i-doing-something-wrong/1745686#1745686\">discussion on Matlab OO</a> that doesn't talk about .NET</p>\n\n<p><strong>Edit</strong>: Brief study</p>\n\n<p>I ran a quick test on my own from within Matlab of simple access and assignment operations within different objects including formal Matlab objects (R2011b), Java and .NET calling each 1,000,000 times. The method calls refer to internal looping, the property/field calls refer to accessing the public field from Matlab and looping in Matlab. The last results puzzle me as the overhead for .NET is much higher than Java but the actual run-time is about half. What is going on?</p>\n\n<pre>\n    Access(s)  Assign(s)  Type of object/call\n    --- MATLAB ---\n    0.003361   0.004268   'myObj.field'\n    0.003403   0.004263   'myStruct.field'\n    0.003376   0.003392   'myVar'   \n    0.152629   0.303579   'myHandleObj.field'\n    25.79159   -          'TestConstant.const'\n    0.003384   -          'myTestConstant.const' (instance)\n    0.006794   0.008689   'TestObj.methods'\n    0.157509   0.303357   'TestHandleObj.methods'\n\n    --- NON-MATLAB ---\n    10.70006   16.42527   'JavaObj fields'\n    0.005063   0.005441   'JavaObj methods'\n    43.49988   43.96159   'NetObj fields'\n    0.002194   0.002306   'NetObj methods'\n</pre>\n"},{"tags":["asp.net","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":19,"score":1,"question_id":13135452,"title":"Performance issues with TransferRequestHandler and BeginRequest","body":"<p>I have started to use New Relic to monitor the performance of <a href=\"http://alternativeto.net\" rel=\"nofollow\">http://alternativeto.net</a> that is a fairly large website.</p>\n\n<p>What I've noticed is that a significant time is spent in a method they report as \"TransferRequestHandler\" and when i dive into it i see that it's really the \"BeginRequest()\" method that is taking time.</p>\n\n<p>It looks like this in New Relic.</p>\n\n<p><img src=\"http://i.stack.imgur.com/RdY32.png\" alt=\"http://content.screencast.com/users/stuckish/folders/Jing/media/22c8137e-21b1-4b36-8185-15989e173f57/2012-10-30_0941.png\"></p>\n\n<p>The closest thing I've come to find anything that could be the problem is this thread here on Stack Overflow <a href=\"http://stackoverflow.com/questions/3629709/i-just-discovered-why-all-asp-net-websites-are-slow-and-i-am-trying-to-work-out\">I just discovered why all ASP.Net websites are slow, and I am trying to work out what to do about it</a> but i've actually tried to replace the Session Module but that didn't help.</p>\n\n<p>The site is a hybrid between ASP.NET MVC and Webforms.</p>\n\n<p>I've realized that this is a long shot and you don't have much to \"go on\" but if someone can put me in the right direction and most importantly be able to reproduce the behavior locally or something like that i would be extremely grateful :)</p>\n"},{"tags":["c++","performance","random"],"answer_count":4,"favorite_count":0,"up_vote_count":10,"down_vote_count":0,"view_count":179,"score":10,"question_id":13045214,"title":"Fast adding random variables in C++","body":"<p><strong>Short version</strong>: how to most efficiently represent and add two random variables given by lists of their realizations? </p>\n\n<p><strong>Mildly longer version:</strong>\nfor a workproject, I need to add several random variables each of which is given by a list of values. For example, the realizations of rand. var. A are {1,2,3} and the realizations of B are {5,6,7}. Hence, what I need is the distribution of A+B, i.e. {1+5,1+6,1+7,2+5,2+6,2+7,3+5,3+6,3+7}. And I need to do this kind of adding several times (let's denote this number of additions as COUNT, where COUNT might reach 720) for different random variables (C, D, ...). </p>\n\n<p><strong>The problem:</strong> if I use this stupid algorithm of summing each realization of A with each realization of B, the complexity is exponential in COUNT. Hence, for the case where each r.v. is given by three values, the amount of calculations for COUNT=720 is 3^720 ~ 3.36xe^343 which will last till the end of our days to calculate:) Not to mention that in real life, the lenght of each r.v. is gonna be 5000+.</p>\n\n<p><strong>Solutions:</strong>\n1/ The first solution is to use the fact that I am OK with rounding, i.e. having integer values of realizations. Like this, I can represent each r.v. as a vector and for at the index corresponding to a realization I have a value of 1 (when the r.v. has this realization once). So for a r.v. A and a vector of realizations indexed from 0 to 10, the vector representing A would be [0,1,1,1,0,0,0...] and the representation for B would be [0,0,0,0,0,1,1,1,0,0,10]. Now I create A+B by going through these vectors and do the same thing as above (sum each realization of A with each realization of B and codify it into the same vector structure, quadratic complexity in vector length). The upside of this approach is that the complexity is bound. The problem of this approach is that in real applications, the realizations of A will be in the interval [-50000,50000] with a granularity of 1. Hence, after adding two random variables, the span of A+B gets to -100K, 100K.. and after 720 additions, the span of SUM(A, B, ...) gets to [-36M, 36M] and even quadratic complexity (compared to exponential complexity) on arrays this large will take forever. </p>\n\n<p>2/ To have shorter arrays, one could possibly use a hashmap, which would most likely reduce the number of operations (array accesses) involved in A+B as the assumption is that some non-trivial portion of the theoreical span [-50K, 50K] will never be a realization. However, with continuing summing of more and more random variables, the number of realizations increases exponentially while the span increases only linearly, hence the density of numbers in the span increases over time. And this would kill the hashmap's benefits.</p>\n\n<p>So the question is: how can I do this problem efficiently? The solution is needed for calculating a VaR in electricity trading where all distributions are given empirically and are like no ordinary distributions, hence formulas are of no use, we can only simulate.</p>\n\n<hr>\n\n<p>Using math was considered as the first option as half of our dept. are mathematicians. However, the distributions that we're going to add are badly behaved and the COUNT=720 is an extreme. More likely, we are going to use COUNT=24 for a daily VaR. Taking into account the bad behaviour of distributions to add, for COUNT=24 the central limit theorem would not hold too closely (the distro of SUM(A1, A2, ..., A24) would not be close to normal). As we're calculating possible risks, we'd like to get a number as precise as possible.</p>\n\n<p>The intended use is this: you have hourly casflows from some operation. The distribution of cashflows for one hour is the r.v. A. For the next hour, it's r.v. B, etc. And your question is: what is the largest loss in 99 percent of cases? So you model the cashflows for each of those 24 hours and add these cashflows as random variables so as to get a distribution of the total casfhlow over the whole day. Then you take the 0.01 quantile.</p>\n"},{"tags":["performance","gwt","comparison","smartgwt"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":64,"score":0,"question_id":13133093,"title":"GWT vs SmartGWT","body":"<p>Can you please let me know which of the option( <code>GWT or SmartGWT</code>) is better, considering the below scenario.</p>\n\n<ol>\n<li>Will be used for designing screens and client-side validations.</li>\n<li>Performance ( page loading , grid loading) should be good.</li>\n<li>Need to communicate with server through soap (web services.)</li>\n</ol>\n"},{"tags":["php","mysql","performance","optimization"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":51,"score":1,"question_id":13134628,"title":"Best settings for mysql full text search","body":"<p>I'm making forum search engine that will crawl forums and help user to find related topics and answers.\nI will use php/mysql for front end.</p>\n\n<p>On each search query it will use mysql full text search across big amount of data (I don't have idea how many records will be there).</p>\n\n<p>Now I have concern about mysql optimization and settings and also about choosing right hardware.\nI don't know much about disk types, etc..</p>\n\n<p>I will start with hetzner server (http://www.hetzner.de/en/hosting/produkte_rootserver/ex5).</p>\n\n<p>Can anybody give any tips about this?</p>\n"},{"tags":["android","performance","google-maps","application","android-mapview"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":52,"score":0,"question_id":13133011,"title":"Enhance Performance of mapview with 200+ markers","body":"<p>I am having 3 types of overlays on the map. \n1) there is one kind of overlays which is having simple markers. And on click of overlay i show pop up.\n2) other kind of overlay contains canvas drawing. and onTouch event of these overlays we have to show third kind of overlay which contains bitmap drawing on the map. And click event of third kind of overlay is also there.</p>\n\n<p>so, due to heavy  calculations and lots of canvas drawing, performance of mapview is degraded.</p>\n\n<p>I have gone through Overlaymanager libreary from <a href=\"https://github.com/staroud-android/OverlayManager\" rel=\"nofollow\">https://github.com/staroud-android/OverlayManager</a>. but its not helping me.</p>\n\n<p>Is there any way which can fulfill my requirements with high performance? </p>\n\n<p>I also need suggestion that Polaris map library can help me for better performance? <a href=\"https://github.com/cyrilmottier/Polaris\" rel=\"nofollow\">https://github.com/cyrilmottier/Polaris</a></p>\n"},{"tags":["performance","mongodb","count"],"answer_count":3,"favorite_count":2,"up_vote_count":7,"down_vote_count":0,"view_count":2058,"score":7,"question_id":7658228,"title":"MongoDB 'count()' is very slow. How do we refine/work around with it?","body":"<p>I am currently using MongoDB with millions of data records. I discovered one thing that's pretty annoying.</p>\n\n<p>When I use 'count()' function with a small number of queried data collection, it's very fast. However, when the queried data collection contains thousand or even millions of data records, the entire system becomes very slow.</p>\n\n<p>I made sure that I have indexed the required fields.</p>\n\n<p>Has anybody encountered an identical thing? How do you do to improve that?</p>\n"},{"tags":["sql-server","performance","ado.net","sql-server-2012"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":48,"score":2,"question_id":13132903,"title":"SQL Server performance - schema vs multiple databases","body":"<p>This is purely from performance standpoint and only for SQL Server. I am using SQL Server 2012. I am migrating from a different database server (Ctree). The databases are from less than 100mbs to about 2-3GBs, five in total. There are a lot of tables - over 400 tables in all. </p>\n\n<p>Would it be better in terms of performance only to use a single database and multiple schema or multiple databases as is? The existing logic is that there are multiple databases and there are 7 different applications (C# - ADO.NET) that use these. </p>\n"},{"tags":["performance","html5","web-applications","jquery-mobile","inline"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":43,"score":1,"question_id":12341821,"title":"Considerations about optimizing singlepage mobile web apps by inlining all js and css","body":"<p>I am creating a mobile singlepage web app using jquery mobile. The webapp includes a number of javascript files and a number of css files. I have written a deploy script that concatenates and minifies js and css files, and now I am wondering whether I should inline the concatenated js and css directly in the HTML file - please note that I am talking about a singlepage app here (I know that this would be a bad idea in a traditional web 1.0 app with dynamically generated HTML). I am also using appcache/manifest file to cache the singlepage app so that subsequent access to the web app will be served from the cache, so it is the initial load time that is my primary concern. </p>\n\n<p>When I inline everything (jquery, jquery mobile etc.), my 7kb HTML file increases to 350kb (100kb zipped) but now everything can be loaded in a single request.  </p>\n\n<p>But am I missing some other benefits such as parallel downloading of js files - and would it therefore be better to not inline the css and js, but instead just concatenate all js and css to a single js file and a single css file and then fetch each of them in separate requests?</p>\n\n<p>Are there any limits regarding file size that I should be aware of? Maybe caching in network routers works better with smaller file sizes or whatever?    </p>\n\n<p>So my question boils down to whether it is a good idea to inline <em>everything</em> when making singlepage mobile web apps?</p>\n"},{"tags":["objective-c","performance","super"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":26,"score":1,"question_id":13132720,"title":"Is it inefficient to only override a method and only call super?","body":"<p>Do I take a performance hit by leaving code like the following in my app?</p>\n\n<pre><code>- (void)viewDidUnload \n{\n  [super viewDidUnload];\n  // Release any retained subviews of the main view.\n  // e.g. self.myOutlet = nil;\n}\n</code></pre>\n\n<p>I think the answer is yes, because it results in an unnecessary method call. But I wanted to make sure.</p>\n"},{"tags":["c++","performance","benchmarking"],"answer_count":4,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":247,"score":6,"question_id":13128430,"title":"Why is `std::copy` 5x (!) slower than `memcpy` in my test program?","body":"<p>This is a follow-up to <a href=\"http://stackoverflow.com/questions/13117211/why-is-memcpy-slower-than-a-reinterpret-cast-when-parsing-binary-data\">this question</a> where I posted this program:</p>\n\n<pre><code>#include &lt;algorithm&gt;\n#include &lt;cstdlib&gt;\n#include &lt;cstdio&gt;\n#include &lt;cstring&gt;\n#include &lt;ctime&gt;\n#include &lt;iomanip&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;chrono&gt;\n\nclass Stopwatch\n{\npublic:\n    typedef std::chrono::high_resolution_clock Clock;\n\n    //! Constructor starts the stopwatch\n    Stopwatch() : mStart(Clock::now())\n    {\n    }\n\n    //! Returns elapsed number of seconds in decimal form.\n    double elapsed()\n    {\n        return 1.0 * (Clock::now() - mStart).count() / Clock::period::den;\n    }\n\n    Clock::time_point mStart;\n};\n\nstruct test_cast\n{\n    int operator()(const char * data) const\n    {\n        return *((int*)data);\n    }\n};\n\nstruct test_memcpy\n{\n    int operator()(const char * data) const\n    {\n        int result;\n        memcpy(&amp;result, data, sizeof(result));\n        return result;\n    }\n};\n\nstruct test_memmove\n{\n    int operator()(const char * data) const\n    {\n        int result;\n        memmove(&amp;result, data, sizeof(result));\n        return result;\n    }\n};\n\nstruct test_std_copy\n{\n    int operator()(const char * data) const\n    {\n        int result;\n        std::copy(data, data + sizeof(int), reinterpret_cast&lt;char *&gt;(&amp;result));\n        return result;\n    }\n};\n\nenum\n{\n    iterations = 2000,\n    container_size = 2000\n};\n\n//! Returns a list of integers in binary form.\nstd::vector&lt;char&gt; get_binary_data()\n{\n    std::vector&lt;char&gt; bytes(sizeof(int) * container_size);\n    for (std::vector&lt;int&gt;::size_type i = 0; i != bytes.size(); i += sizeof(int))\n    {\n        memcpy(&amp;bytes[i], &amp;i, sizeof(i));\n    }\n    return bytes;\n}\n\ntemplate&lt;typename Function&gt;\nunsigned benchmark(const Function &amp; function, unsigned &amp; counter)\n{\n    std::vector&lt;char&gt; binary_data = get_binary_data();\n    Stopwatch sw;\n    for (unsigned iter = 0; iter != iterations; ++iter)\n    {\n        for (unsigned i = 0; i != binary_data.size(); i += 4)\n        {\n            const char * c = reinterpret_cast&lt;const char*&gt;(&amp;binary_data[i]);\n            counter += function(c);\n        }\n    }\n    return unsigned(0.5 + 1000.0 * sw.elapsed());\n}\n\nint main()\n{\n    srand(time(0));\n    unsigned counter = 0;\n\n    std::cout &lt;&lt; \"cast:      \" &lt;&lt; benchmark(test_cast(),     counter) &lt;&lt; \" ms\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"memcpy:    \" &lt;&lt; benchmark(test_memcpy(),   counter) &lt;&lt; \" ms\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"memmove:   \" &lt;&lt; benchmark(test_memmove(),  counter) &lt;&lt; \" ms\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"std::copy: \" &lt;&lt; benchmark(test_std_copy(), counter) &lt;&lt; \" ms\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"(counter:  \" &lt;&lt; counter &lt;&lt; \")\" &lt;&lt; std::endl &lt;&lt; std::endl;\n\n}\n</code></pre>\n\n<p>I noticed that for some reason <code>std::copy</code> performs much worse than memcpy. The output looks like this on my Mac using gcc 4.7.</p>\n\n<pre><code>g++ -o test -std=c++0x -O0 -Wall -Werror -Wextra -pedantic-errors main.cpp\ncast:      41 ms\nmemcpy:    46 ms\nmemmove:   53 ms\nstd::copy: 211 ms\n(counter:  3838457856)\n\ng++ -o test -std=c++0x -O1 -Wall -Werror -Wextra -pedantic-errors main.cpp\ncast:      8 ms\nmemcpy:    7 ms\nmemmove:   8 ms\nstd::copy: 19 ms\n(counter:  3838457856)\n\ng++ -o test -std=c++0x -O2 -Wall -Werror -Wextra -pedantic-errors main.cpp\ncast:      3 ms\nmemcpy:    2 ms\nmemmove:   3 ms\nstd::copy: 27 ms\n(counter:  3838457856)\n\ng++ -o test -std=c++0x -O3 -Wall -Werror -Wextra -pedantic-errors main.cpp\ncast:      2 ms\nmemcpy:    2 ms\nmemmove:   3 ms\nstd::copy: 16 ms\n(counter:  3838457856)\n</code></pre>\n\n<p>As you can see, even with <code>-O3</code>it is up to 5 times (!) slower than memcpy.</p>\n\n<p>The results are similar on Linux.</p>\n\n<p>Does anyone know why?</p>\n"},{"tags":["iphone","performance","pdf","catiledlayer"],"answer_count":4,"favorite_count":7,"up_vote_count":6,"down_vote_count":0,"view_count":5287,"score":6,"question_id":3442253,"title":"Why is this CATiledLayer/PDF code slow?","body":"<p>Here is the code:</p>\n\n<p><a href=\"https://dl.dropbox.com/u/147189/PDFScroller.zip\" rel=\"nofollow\">https://dl.dropbox.com/u/147189/PDFScroller.zip</a></p>\n\n<p>I took the WWDC 2010 PhotoScroller sample code that implements nested UIScrollViews for zooming, inside a UIScrollView for paging, and swapped out what I thought would be minimal amount of code required for displaying a multi-page PDF instead of images.</p>\n\n<p>It works. But it's slow on my iPhone4, about three seconds to paint the first page, and even slower on my iPod Touch. I can watch it painting the individual tiles. This same PDF already opens up more quickly, with no visible tile drawing, in an alternate CATiledLayer implementation I have which simply uses a single CATiledLayer/UIScrollView and touch events to change pages. I'd like to use this PhotoScroller technique, it's very nice.</p>\n\n<p>I watched it with CPU Sampler in Instruments, and it doesn't seem to be the PDF rendering code, it looks like the time is taken up in threading and messaging. I'd appreciate it if someone could help point out what this sample is doing to incur the overhead.</p>\n\n<p>Thanks,</p>\n\n<p>Jim</p>\n\n<hr>\n\n<p>Update 1: I had originally used the <code>TilingView</code> class technique from the sample code of defining</p>\n\n<pre><code>+ (Class) layerClass {\n  return [CATiledLayer class];\n}\n</code></pre>\n\n<p>And then drawing in <code>- (void)drawRect:(CGRect)rect</code> but switched to the explicit <code>CATiledLayer</code> subclass as a first attempt at seeing whether it would make a difference, but it did not, and so I left the code as-is for posting here.  There is also a missing <code>[tiledLayer release];</code> leak in TilingView.</p>\n"},{"tags":["iphone","ios","xcode","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":13130429,"title":"iOS app slow when segue between ViewControllers","body":"<p>i'm working on a iOS app and, when I try it on the device, it is too slow when changing the View. In the simulator it works perfectly.</p>\n\n<p>That's my viewDidLoad code. Y don't know if some function is too heavy to implement.</p>\n\n<pre><code>CGRect screenBounds = [[UIScreen mainScreen] bounds];\nif (screenBounds.size.height == 568) {\n    // code for 4-inch screen\n    self.backImage.image = [UIImage imageNamed:@\"settings-568h@2x.png\"];\n} else {\n    // code for 3.5-inch screen\n    self.backImage.image = [UIImage imageNamed:@\"settings@2x.png\"];\n}\n\narray = [[NSMutableArray alloc] initWithObjects:@\"Object1\",@\"Object2\",@\"Object3\", nil];\n\nNSString *loadedString = [array objectAtIndex:0]; //Defalut\nself.checkedIndexPath = [NSIndexPath indexPathForRow:0 inSection:0];\n\nNSString *savePath = [self pathOfSavingFile:@\"fileSetting\"];\nif([[NSFileManager defaultManager] fileExistsAtPath:savePath])\n{\n    NSString *tempSTR = [[NSString alloc] initWithContentsOfFile:savePath encoding:5 error:nil];\n    loadedString = tempSTR;\n}\n\nif([[array objectAtIndex:0] isEqualToString:loadedString]) {\n    self.checkedIndexPath = [NSIndexPath indexPathForRow:0 inSection:0];\n} else if ([[array objectAtIndex:1] isEqualToString:loadedString]) {\n    self.checkedIndexPath = [NSIndexPath indexPathForRow:1 inSection:0];\n} else if ([[array objectAtIndex:2] isEqualToString:loadedString]) {\n    self.checkedIndexPath = [NSIndexPath indexPathForRow:2 inSection:0];\n}\n\nUIApplication *pNote = [UIApplication sharedApplication];\n[[NSNotificationCenter defaultCenter] addObserver:self selector:@selector(applicationDidEnterBackground:) name:UIApplicationDidEnterBackgroundNotification object:myApp];\n\nself.titleLabel.text = NSLocalizedString(@\"settingsLoc\", nil);\n[self.backButton     setTitle:NSLocalizedString(@\"backLoc\", nil)     forState:UIControlStateNormal];\n\n[super viewDidLoad];\n</code></pre>\n\n<p>Thank you vey much.</p>\n"},{"tags":["performance","depth-first-search"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":12,"score":1,"question_id":13130378,"title":"Could this dfs implementation be made more faster","body":"<p>This is my first implementation on Graph Theory , and i think my implemented dfs code is very slow . How could i make it more fast ? </p>\n\n<p>Here's the code (it is actually solution to this problem <a href=\"http://www.iarcs.org.in/inoi/contests/dec2005/Advanced-2.php\" rel=\"nofollow\">http://www.iarcs.org.in/inoi/contests/dec2005/Advanced-2.php</a> ):- <a href=\"http://pastebin.com/FSizr10z\" rel=\"nofollow\">http://pastebin.com/FSizr10z</a></p>\n\n<p><strong>Can anybody please point out edit required in my dfs to make it fast , the dfs i use takes O(n^2) which is obviously too slow .</strong></p>\n"},{"tags":["performance","nhibernate"],"answer_count":4,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":1194,"score":4,"question_id":2151198,"title":"NHibernate select grows exponentially slow","body":"<p>My problem is that NHibernate gets exponentially slow when fetching records from the database. I had a request to basically pull all the data from a very large database to be used in a report. </p>\n\n<p>I figured, well since I can't get all the records in one shot because the recordset is so large, i thought try breaking it up. Basically I'm iterating through ranges of an index, ie. records id x to y, then y+1 to z, and so forth.</p>\n\n<p>Each result set is about 10megs. The first 20 or so pulls takes less than a minute each, then on the next pull, it takes 10minutes, then 30minutes, and 1hr. I stopped the program there, didn't want to wait till the next pull will come. I ran the program again starting from the index where I left off, again, the first 20 or so pulls are really quick, then for some odd reason there is a major slowdown.</p>\n\n<p>Any help would be greatly appreciated.</p>\n"},{"tags":["performance","matlab","for-loop","parallel-processing"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":46,"score":1,"question_id":13103229,"title":"parallelize a large loop","body":"<p>I have a large loop where I am trying to calculate certain attributes for each pixel in a DEM (4800x6000). I am calling a function demPHV in which I've vectorized all calculations that outputs a structure with 26 fields .  I have 4 cores but also have access to a multi-core cluster. I would like to speed up the weeks it would take to run this.</p>\n\n<p>Z is the dem for this example. R is the spatialref object (a vector for example's sake). latlim and lonlim are vectors of lat and long of the western US coastline (made up pairs in the example).\nfor example:</p>\n\n<pre><code>Z=rand(48,60);\nR=makerefmat(120,40,.5,.5)\nlatlim=[40:60]';\nlonlim=[136:(143-136)/(length(latlim)-1):143]';\n</code></pre>\n\n<p>Then my original loop:</p>\n\n<pre><code>for col=11:size(Z,2)-11\n    for row=11:size(Z,1)-11\n        dpv=demPHV(Z,R,row,col,latlim,lonlim)\n\n    fn=fieldnames(dpv);\n    for k=1:length(fieldnames(dpv))\n        DEM_PHV.(fn{k}).{row,col}=dpv.(fn{k});\n    end\nend\n</code></pre>\n\n<p>Loops for parallelizing:</p>\n\n<p>option 1:</p>\n\n<pre><code>[rows, cols] = meshgrid(12:(size(Z,1)-12), 12:(size(Z,2)-12));\ninds = sub2ind(size(Z), rows, cols);\ninds = inds(:)';\nparfor i=inds(1):inds(end)\n       dpv=demPHV(Z,R,i,latlim,lonlim)\nend\n</code></pre>\n\n<p>This includes <code>[r,c]=ind2sub(size(Z),i)</code> in the function to use in the function demPHV.</p>\n\n<p>option 2:</p>\n\n<pre><code>parfor col=11:size(Z,2)-11\n    for row=11:size(Z,1)-11\n         dpv=demPHV(Z,R,row,col,latlim,lonlim)\n    end\nend\n</code></pre>\n\n<p>parfor requires consecutive integers hence some of these changes.  I have to exclude the bordering 11 rows and columns because my function uses surrounding pixels to calculate some of the attributes.</p>\n\n<p>So, my questions:</p>\n\n<ol>\n<li>Would you expect either of these two options to be faster than the other?</li>\n<li><p>parfor does not allow me to include the second part of my original loop:</p>\n\n<p><code>fn=fieldnames(dpv);</code></p>\n\n<p><code>for k=1:length(fieldnames(dpv))</code></p>\n\n<p><code>DEM_PHV.(fn{k}).{row,col}=dpv.(fn{k});</code></p>\n\n<p><code>end</code></p></li>\n</ol>\n\n<p>during which I assign the output structure to another variable.  The ultimate goal is to have the variable DEM_PHV have fields for every attribute I need, and every field to be a matrix size(Z) where every cell is the corresponding value for that attribute. I've tried to have my function output the values in the correct cell of the matrix, but then I get a matrix size(Z) with <code>[]</code> everywhere except for the value at location <code>row,col</code>.  This seems like a horribly inefficient use of memory...  any better suggestions? I hope I covered everything.\nThanks for looking!</p>\n"},{"tags":["javascript","jquery","json","performance","jquery-autocomplete"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":49,"score":0,"question_id":13128628,"title":"Performance issue with JSON and auto Complete","body":"<p>I am trying to write an app that will work online and offline using technologies such as application cache and local storage. I am using jQuery mobile and an jqm autocomplete solution which can be found here <a href=\"http://andymatthews.net/code/autocomplete/\" rel=\"nofollow\">http://andymatthews.net/code/autocomplete/</a></p>\n\n<p>The idea is that if the app is on-line then I will be calling a remotely accessible function via ajax which will set data. The ajax calls that are made bring back data for events, countries and hospitals. Once the data as been returned I am setting the data in localStorage.</p>\n\n<p>If the data already exists in localStorage then I can continue while being offline as I don't have to rely on the ajax calls.</p>\n\n<p>I am having performance issues with this code when running on iPad/mobile devices. Where the hospital data is concerned. There is a large amount of data being returned for hospitals, please see sample of JSON for hospital data below. It can be a little slow on desktop but not nearly as bad. </p>\n\n<p>Can anyone see any improvements to the code I have below to increase the performance.</p>\n\n<p><strong>JSON hospital data example</strong></p>\n\n<pre><code>{ \"hospitals\" : { \n  \"1\" : { \"country\" : \"AD\",\n      \"label\" : \"CAIXA ANDORRANA SEGURETAT SOCIAL\"\n    },\n  \"10\" : { \"country\" : \"AE\",\n      \"label\" : \"Advance Intl Pharmaceuticals\"\n    },\n  \"100\" : { \"country\" : \"AT\",\n      \"label\" : \"BIO-KLIMA\"\n    },\n  \"1000\" : { \"country\" : \"BE\",\n      \"label\" : \"Seulin Nicolas SPRL\"\n    },\n  \"10000\" : { \"country\" : \"ES\",\n      \"label\" : \"Dental 3\"\n    },\n  \"10001\" : { \"country\" : \"ES\",\n      \"label\" : \"PROTESIS DENTAL MACHUCA PULIDO\"\n    },\n  \"10002\" : { \"country\" : \"ES\",\n      \"label\" : \"JUST IMPLANTS, S.L.\"\n    },\n  \"10003\" : { \"country\" : \"ES\",\n      \"label\" : \"CTRO DENTAL AGRIC ONUBENSE DR.DAMIA\"\n    },\n  \"10004\" : { \"country\" : \"ES\",\n      \"label\" : \"HTAL. VIRGEN DE ALTAGRACIA\"\n    },\n  \"10005\" : { \"country\" : \"ES\",\n      \"label\" : \"HOSPITAL INFANTA CRISTINA\"\n    }....\n\n\n/*global document,localStorage,alert,navigator: false, console: false, $: false */\n\n$(document).ready(function () {\n//ECMAScript 5 - It catches some common coding bloopers, throwing exceptions. http://ejohn.org/blog/ecmascript-5-strict-mode-json-and-more/\n//It prevents, or throws errors, when relatively \"unsafe\" actions are taken (such as gaining access to the global object).\n//It prevents, or throws errors, when relatively \"unsafe\" actions are taken (such as gaining access to the global object).\n\"use strict\";\n\n//initialise online/offline workflow variables\nvar continueWorkingOnline, continueWorkingOffline, availableEvents, availableHospitals, availableCountries, availableCities;\ncontinueWorkingOnline = navigator.onLine;\n\nvar getLocalItems = function () {\n\n    var localItems = [];    \n    availableEvents = localStorage.getItem('eventData');\n    availableHospitals = localStorage.getItem('hospitalData');\n    availableCountries = localStorage.getItem('countryData');\n\n    if (availableEvents) {\n        //only create the array if availableEvents exists\n        localItems[0] = [];\n        localItems[0].push(availableEvents);\n    }\n    if (availableCountries) {\n        //only create the array if availableCountries exists\n        localItems[1] = [];\n        localItems[1].push(availableCountries);\n    }\n    if (availableHospitals) {\n        //only create the array if availableHospitals exists\n        localItems[2] = [];\n        localItems[2].push(availableHospitals);\n    }\n    if (availableCities) {\n        //only create the array if availableHospitals exists\n        localItems[3] = [];\n        localItems[3].push(availableCities);\n    }\n    return localItems;              \n};\n\n\n//Check to see if there are 3 local items. Events, Countries, Cities. If true we know we can still run page off line\ncontinueWorkingOffline = getLocalItems().length === 3  ? true: false; \n\n//Does what is says on the tin\nvar populateEventsDropDown = function (data) {      \n    var eventsDropDown = $('#eventSelection');\n\n    var item = data.events;\n    $.each(item, function (i) {\n        eventsDropDown.append($('&lt;option&gt;&lt;/option&gt;').val(item[i].id).html(item[i].name));\n    });\n};\n\n//Called once getData's success call back is fired\nvar setFormData = function setData(data, storageName) {\n    //localStorage.setItem(storageName, data);\n    localStorage.setItem(storageName, data);\n};\n\n//This function is only called if continueWorkingOnline === true\nvar getRemoteFormData = function getData(ajaxURL, storageName) {\n    $.ajax({\n        url: ajaxURL,\n        type: \"POST\",\n        data: '',\n        success: function (data) {\n            setFormData(data, storageName);             \n        }           \n    });\n};\n\n//Function for autoComplete on Hospital data\nvar autoCompleteDataHospitals = function (sourceData) {\n\n    var domID = '#hospitalSearchField';\n    var country = $('#hiddenCountryID').val();\n\n    var items = $.map(sourceData, function (obj) { \n        if (obj.country === country) {\n            return obj;\n        } \n    });     \n\n    $(domID).autocomplete({\n        target: $('#hospitalSuggestions'),\n        source: items,\n        callback: function (e) {\n            var $a = $(e.currentTarget);\n            $(domID).val($a.data('autocomplete').label);                                        \n            $(domID).autocomplete('clear');\n        }\n    });\n};  \n\n//Function for autoComplete on Country data\nvar autoCompleteDataCountries = function (sourceData) {\n\n    var domID = '#countrySearchField';\n    var domHiddenID = '#hiddenCountryID';\n\n    var items = $.map(sourceData, function (obj) { \n        return obj; \n    }); \n\n    $(domID).autocomplete({\n        target: $('#countrySuggestions'),\n        source: items,\n        callback: function (e) {\n\n            var $a = $(e.currentTarget);\n            $(domID).val($a.data('autocomplete').label);                                        \n            $(domID).autocomplete('clear');\n            $(domHiddenID).val($a.data('autocomplete').value);\n\n            //enable field to enter Hospital\n            $('#hospitalSearchField').textinput('enable');\n\n            //Call to autoComplete function for Hospitals\n            autoCompleteDataHospitals(availableHospitals.hospitals);\n        }\n    });     \n};\n\nif (continueWorkingOnline === false &amp;&amp; continueWorkingOffline === false) {\n    alert(\"For best results this form should be initiated online. You can still use this but auto complete features will be disabled\");\n}\n\nif (continueWorkingOnline === true &amp;&amp; continueWorkingOffline === false) {               \n    getRemoteFormData('templates/cfc/Events.cfc?method=getEventsArray', 'eventData');       \n    getRemoteFormData('templates/cfc/Countries.cfc?method=getCountriesArray', 'countryData');\n    getRemoteFormData('templates/cfc/Hospitals.cfc?method=getHospitalsArray', 'hospitalData');\n\n    $(document).ajaxStop(function () {\n        //set the variables once localStorage has been set\n\n        availableEvents = JSON.parse(localStorage.getItem(\"eventData\"));\n        availableHospitals = JSON.parse(localStorage.getItem('hospitalData'));\n        availableCountries = JSON.parse(localStorage.getItem('countryData'));\n\n        //Inserts data into the events drop down\n        populateEventsDropDown(availableEvents);\n\n        autoCompleteDataCountries(availableCountries.countries);            \n    });\n}\n\nif (continueWorkingOnline === true &amp;&amp; continueWorkingOffline === true) {                \n    //get the localStorage which we know exists because of continueWorkingOffline is true\n\n    availableEvents = JSON.parse(localStorage.getItem('eventData'));\n    availableHospitals = JSON.parse(localStorage.getItem('hospitalData'));\n    availableCountries = JSON.parse(localStorage.getItem('countryData'));\n\n    //Inserts data into the events drop down\n    populateEventsDropDown(availableEvents);\n\n    autoCompleteDataCountries(availableCountries.countries);        \n}       \n});\n</code></pre>\n"},{"tags":["mysql","performance","web","cpu"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":38,"score":-1,"question_id":13128407,"title":"Mysql CPU usage over 800%","body":"<p>I've been having a lot of trouble recently with my webserver. The loading speed of my webpage is extremely slow and when I check htop I notice the mysql process to use around 800-1000% of CPU. I have tried everything the mysqltuner suggests but I have yet to see an improvement.</p>\n\n<p>Here are the mysqltuner results:</p>\n\n<blockquote>\n  <p>-------- General Statistics --------------------------------------------------</p>\n  \n  <p>[--] Skipped version check for MySQLTuner script</p>\n  \n  <p>[OK] Currently running supported MySQL version 5.1.54-log</p>\n  \n  <p>[OK] Operating on 64-bit architecture</p>\n  \n  <p>-------- Storage Engine Statistics -------------------------------------------</p>\n  \n  <p>[--] Status: +Archive -BDB -Federated +InnoDB -ISAM -NDBCluster</p>\n  \n  <p>[--] Data in MyISAM tables: 58M (Tables: 243)</p>\n  \n  <p>[--] Data in InnoDB tables: 34M (Tables: 303)</p>\n  \n  <p>[!!] Total fragmented tables: 304</p>\n  \n  <p>-------- Performance Metrics -------------------------------------------------</p>\n  \n  <p>[--] Up for: 2d 0h 10m 24s (3M q [20.584 qps], 594K conn, TX: 2B, RX:\n  373M)</p>\n  \n  <p>[--] Reads / Writes: 78% / 22%</p>\n  \n  <p>[--] Total buffers: 652.0M global + 14.6M per thread (250 max threads)</p>\n  \n  <p>[OK] Maximum possible memory usage: 4.2G (82% of installed RAM)</p>\n  \n  <p>[OK] Slow queries: 1% (50K/3M)</p>\n  \n  <p>[OK] Highest usage of available connections: 46% (115/250)</p>\n  \n  <p>[OK] Key buffer size / total MyISAM indexes: 256.0M/18.7M</p>\n  \n  <p>[OK] Key buffer hit rate: 100.0% (3B cached / 46K reads)</p>\n  \n  <p>[OK] Query cache efficiency: 49.9% (785K cached / 1M selects)</p>\n  \n  <p>[!!] Query cache prunes per day: 24539</p>\n  \n  <p>[OK] Sorts requiring temporary tables: 0% (0 temp sorts / 91K sorts)</p>\n  \n  <p>[!!] Joins performed without indexes: 199247</p>\n  \n  <p>[!!] Temporary tables created on disk: 29% (159K on disk / 547K total)</p>\n  \n  <p>[OK] Thread cache hit rate: 99% (118 created / 594K connections)</p>\n  \n  <p>[OK] Table cache hit rate: 66% (1K open / 1K opened)</p>\n  \n  <p>[OK] Open file limit used: 23% (1K/4K)</p>\n  \n  <p>[OK] Table locks acquired immediately: 99% (1M immediate / 1M locks)</p>\n  \n  <p>[OK] InnoDB data size / buffer pool: 34.4M/128.0M</p>\n  \n  <p>-------- Recommendations -----------------------------------------------------</p>\n  \n  <p>General recommendations:</p>\n\n<pre><code>Run OPTIMIZE TABLE to defragment tables for better performance\n\nAdjust your join queries to always utilize indexes\n\nWhen making adjustments, make tmp_table_size/max_heap_table_size equal\n\nReduce your SELECT DISTINCT queries without LIMIT clauses\n</code></pre>\n  \n  <p>Variables to adjust:</p>\n\n<pre><code>query_cache_size (&gt; 80M)\n\njoin_buffer_size (&gt; 12.0M, or always use indexes with joins)\n\ntmp_table_size (&gt; 180M)\n\nmax_heap_table_size (&gt; 180M)\n</code></pre>\n</blockquote>\n\n<p>What could I do to improve the webpage loading speed and reduce the mysql stress on the CPU?</p>\n"},{"tags":["mysql","database","performance","query"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":24,"score":1,"question_id":13127703,"title":"need a sql query to find out the Database wait time ratio for MySQL","body":"<p>Oracle supports sql queries to <code>SYS.V_$SYSMETRIC</code> to find out Database Wait Time Ratio (% of wait or bottlenecks experienced in time spent on user-level calls)  <a href=\"http://www.oracle.com/technetwork/articles/schumacher-analysis-099313.html\" rel=\"nofollow\">http://www.oracle.com/technetwork/articles/schumacher-analysis-099313.html</a>. What is an equivalent query to find similar stats for MySQL database? </p>\n\n<p>I have looked at <code>information_schema.PROFILING</code> and <code>information_schema.PROCESSLIST</code> tables to calculate a similar measure but unable to comprehend what values to use / address.</p>\n\n<p>Any suggestions?</p>\n\n<p>I need it for both Windows and Linux Platforms with remote MySql server. I think a light-weight DB query would help as opposed to gathering CPU Utilization for the machine hosting MySQL server. </p>\n\n<p>Thanks.</p>\n"},{"tags":["project-management","process","performance","kanban"],"answer_count":5,"favorite_count":3,"up_vote_count":6,"down_vote_count":0,"view_count":1346,"score":6,"question_id":1917264,"title":"Kanban as a Software Development Process in Practice","body":"<p>Has anyone used the <a href=\"http://www.kanbandistilled.com/\" rel=\"nofollow\">kanban method</a> for software development management?</p>\n\n<p>I am evaluating kanban as a technique and would be curious to hear from anyone who has actually applied it in practice as to how effective it is. I've seen questions like: <a href=\"http://stackoverflow.com/questions/924121/is-anyone-using-kanban\">is-anyone-using-kanban</a>, <a href=\"http://stackoverflow.com/questions/1156667/kanban-vs-scrum\">kanban-vs-scrum</a>, and <a href=\"http://stackoverflow.com/questions/1367491/apply-kanban-in-an-agile-team\">apply-kanban-in-an-agile-team</a> but they don't address my concerns.</p>\n\n<p>What I'm interested in specifically is:</p>\n\n<ol>\n<li>Does it actually offer the advantages is claims in terms of dynamically identifying bottlenecks?</li>\n<li>Is it easy to execute in practice, or does it have logistical challenges that you need to manage?</li>\n<li>Does it scale well to project teams with many parallel streams of work and many developers?</li>\n<li>How does it compare to critical path analysis (as implemented in MS Project), how is it different?</li>\n<li>What other benefits can be gained from applying kanban?</li>\n</ol>\n\n<p>Thanks.</p>\n"},{"tags":["django","performance","http","caching","http-etag"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":104,"score":1,"question_id":2752046,"title":"Any reason not to use USE_ETAGS with CommonMiddleware in Django?","body":"<p>The only reason I can think of is that calculating <code>ETag</code>'s might be expensive. If pages change very quickly, the browser's cache is likely to be invalidated by the <code>ETag</code>. In that case, calculating the <code>ETag</code> would be a waste of time. On the other hand, a giving a <code>304</code> response when possible minimizes the amount of time spent in transmission. What are some good guidelines for when <code>ETag</code>'s are likely to be a net winner when implemented with Django's <code>CommonMiddleware</code>?</p>\n"},{"tags":["mysql","performance","mysqli"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":39,"score":1,"question_id":13126795,"title":"What is the fastest way to insert thousand records in mysql?","body":"<p>I want to insert many thousand records and I need this to be fast. I read a lot about this issue so I decided to drop my old approach (mysql_connect) and use prepare statements and mysqli. So In order to test the speed of this I write the following.</p>\n\n<pre><code>function load_data2db($sms_id){\n$mysqli = new mysqli('localhost', 'root', 'cr3at1ve', 'tmp-clicknsend');\n/* check connection */\nif (mysqli_connect_errno()) {\n    printf(\"Connect failed: %s\\n\", mysqli_connect_error());\n    exit();\n}\n\n\n// Create statement object\n$stmt = $mysqli-&gt;stmt_init();\nif ($stmt = $mysqli-&gt;prepare(\"INSERT INTO isec_test (sms_id, status,msgid,prmsgid,mob,sms_history_id) values (?, ?, ?, ?, ?, ?)\")) {\n\n\n        /* Bind our params */\n        $stmt-&gt;bind_param('ssssss',$sms_id,$status,$msgid,$prmsgid,$mob,$sms_history_id);\n\n    for($i=0;$i&lt;100000;$i++)\n    {\n        /* Set our params */\n        $sms_id = \"110\";\n        $status = \"OK\";\n        $msgid  = \"msgid\";\n        $prmsgid = \"100-0\";\n        $mob = \"306974386068\";\n        $sms_history_id = 102;\n\n        /* Execute the prepared Statement */\n        $stmt-&gt;execute();\n    }\n\n\n        /* Close the statement */\n        $stmt-&gt;close();\n}\nelse {\n        /* Error */\n        printf(\"Prepared Statement Error: %s\\n\", $mysqli-&gt;error);\n}\n}\n\nload_data2db(10);\n</code></pre>\n\n<p>Then I did the same with the old way ( looping and inserting one by one)</p>\n\n<pre><code>include(\"mysql_connect.php\");\n for($i=0;$i&lt;100000;$i++)\n    {\n    $query = \"INSERT INTO isec_test(sms_id,status,msgid) values ('1','OK','123-123')\";\n    $query = @mysql_query($query);\n    }\nmysql_close($dbc);\n</code></pre>\n\n<p>I made a lot of tests and always the simple mysql way was 1 sec faster. So I am puzzled. What can I do? Use LOAD DATA?</p>\n"},{"tags":["mysql","performance","node.js","database-connection"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":23,"score":0,"question_id":13126824,"title":"nodejs-mysql-native vs node-mysql - Special situation","body":"<p>I'm working on a NodeJS server that will interact a lot with a MySQL server. This will be an API server that handles authenticated responses from clients. The special situation here is that the back end will have multiple databases, which means that every client request would require a new database connection. Has someone work on an architecture similar? what module would you suggest?</p>\n\n<p>My concern is that the database selection for each query would slow down the application considerably.</p>\n"},{"tags":["performance","gwt","zk"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":73,"score":1,"question_id":13064470,"title":"zk vs gwt zk too many requests","body":"<p>Hi I am trying to compare the performance of zk and gwt.</p>\n\n<p>In my comparision I cant write any javascript by myself if the framework itself converts some code into js its ok(like gwt) but I cant write js by myself</p>\n\n<p>On writing code in the above way for almost anything done on browser a request is sent to the server in ZK.</p>\n\n<p>Hence eventually if you compare the no of request sent by zk to server is too high as compared to gwt.</p>\n\n<p>i would like to ask the following.</p>\n\n<ol>\n<li>whose performance is better zk or gwt while ignoring the above.</li>\n<li>if we dont ignore the above then is my conclusion that gwts performance is better than  zk right ?</li>\n</ol>\n\n<p>I know that there might be other parameters when comparing performance... but if the difference between requests are so high i cant really see zk beating gwt which some people have said on some forums</p>\n\n<p>pls help\nthanks</p>\n"},{"tags":["performance","compiler","haskell","ghc"],"answer_count":4,"favorite_count":53,"up_vote_count":64,"down_vote_count":0,"view_count":2969,"score":64,"question_id":6121146,"title":"Reading GHC Core","body":"<p>Core is GHC's intermediate language. Reading Core can help you better understand the performance of your program. Someone asked me for documentation or tutorials on reading Core, but I couldn't find much.</p>\n\n<p>What documentation is available for reading GHC Core?</p>\n\n<p>Here's what I've found so far:</p>\n\n<ul>\n<li><a href=\"http://donsbot.wordpress.com/2008/05/06/write-haskell-as-fast-as-c-exploiting-strictness-laziness-and-recursion/\" rel=\"nofollow\">Write Haskell as fast as C: exploiting strictness, laziness and recursion</a></li>\n<li><a href=\"http://donsbot.wordpress.com/2008/06/04/haskell-as-fast-as-c-working-at-a-high-altitude-for-low-level-performance/\" rel=\"nofollow\">Haskell as fast as C: working at a high altitude for low level performance</a></li>\n<li><a href=\"http://book.realworldhaskell.org/read/profiling-and-optimization.html\" rel=\"nofollow\">RWH: Chapter 25. Profiling and optimization</a></li>\n<li><a href=\"http://blog.johantibell.com/2010/09/slides-from-my-high-performance-haskell.html\" rel=\"nofollow\">High-Performance Haskell talk at CUFP</a> (slide 65-80)</li>\n</ul>\n"},{"tags":["javascript","performance","compiler","v8"],"answer_count":1,"favorite_count":5,"up_vote_count":15,"down_vote_count":0,"view_count":233,"score":15,"question_id":12497995,"title":"JavaScript object code caching: which of these assertions are wrong?","body":"<p>Because I have been around engineers for so many years, I know that if I don't provide context, I'm just going to get a hundred answers of the form \"What are you trying to accomplish?\" I am going to give the background which motivates my question. But <strong>don't confuse the background context for the question I am asking</strong>, which is specifically related to the JavaScript semantics that made object code uncacheable between padge requests. I am not going to give marks for advice on how to make my webapp faster. It's completely tangential to my question, which will probably only be answerable by someone who has worked on a JavaScript compiler or at least the compiler for a dynamic language.</p>\n\n<p>Background:</p>\n\n<p>I am trying to improve the performance of a web application. Among its many resources, it contains one enormous JavaScript file with 40k lines and 1.3million characters pre-minification. Post-minification it's still large, and it still adds about 100ms to the window.onload event when synchronously loaded, even when the source is cached client-side. (I have conclusively ruled out the possibility that the resource is not being cached by watching the request logs and observing that it is not being requested.)</p>\n\n<p>After confirming that it's still slow after being cached, I started doing some research on JavaScript caching in the major browsers, and have learned that none of them cache object code.</p>\n\n<hr>\n\n<p>My question is in the form of some hypothetical assertions based on this research. Please object to these assertions if they are wrong.</p>\n\n<ol>\n<li><p>JavaScript object code is not cached in any modern browser.</p>\n\n<p>\"Object code\" can mean anything from a byte code representing a simple linearized parse tree all the way to <a href=\"https://developers.google.com/v8/design#mach_code\">native machine code</a>.</p></li>\n<li><p>JavaScript object code in a web browser is difficult to cache.</p>\n\n<p>In other words, even if you're including a cached JS source file in an external  tag, there is a linear cost to the inclusion of that script on a page, even if the script contains only function definitions, because all of that source needs to be compiled into an object code.</p></li>\n<li><p>JavaScript object code is difficult to cache because JS source must evaluated in order to be compiled.</p>\n\n<p>Statements have the ability to affect the compilation of downstream statements in a dynamic way that is difficult to statically analyze.</p>\n\n<p>3a. (3) is true mostly because of eval().</p></li>\n<li><p>Evaluation can have side effects on the DOM.</p></li>\n<li><p><em>Therefore</em>, JavaScript source needs to be compiled on every page request.</p></li>\n</ol>\n\n<p>Bonus question: do any modern browsers cache a parse tree for cached JS source files? If not, why not?</p>\n\n<p>Edit: If all of these assertions are correct, then I will give the answer to anyone who can <strong>expound on why they are correct,</strong> for example, by providing a sample of JS code that couldn't be cached as object code and then explaining why not.</p>\n\n<p>I appreciate the suggestions on how to proceed from here to make my app faster, and I mostly agree with them. But the knowledge gap that I'm trying to fill is related to JS object code caching.</p>\n"},{"tags":["sql","sql-server","performance","sql-server-2008","ssis"],"answer_count":3,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":497,"score":4,"question_id":9290291,"title":"SQL Script in VM taking long time for execution","body":"<p>I have the Execute SQL Script package that contains the script to insert about 150K records.</p>\n\n<p>Problem in here is when I execute the package in the Virtual machine its taking 25 min's approx and the same package in physical machine its taking 2 min's</p>\n\n<p>Question 1? Why its taking that much time to load the same data in VM.\nQuestion 2? How to solve this performance issue.</p>\n\n<p>Physical machine configuration has 4GB Ram and 250GB HD + Windows server 2008 R2 + SQL server 2008 R2 Standard Edition.\nVirtual machine has the same Configuration</p>\n\n<p><strong>Update: The Problem is with the SQL Server in VM.</strong></p>\n\n<p><strong>Question 1? Why its taking that much time to Run the same script in VM.</strong></p>\n\n<p><strong>Question 2? How to solve this performance issue.</strong></p>\n\n<p>Both the batabases schema in Physical Machine and VM are identical. Other databases are also same. There was no indexing applied for that tables in both machines. Datatypes are same. harddisk as I said has the same configuration.</p>\n\n<p>No RAID is done on both the machines.</p>\n\n<blockquote>\n  <p>Physical machine has the 2.67GHz RAM Quad Core and in the virtual machine has the\n  2.00GHz RAM Quad Core</p>\n</blockquote>\n\n<p>Version of SQL PM:</p>\n\n<p>Microsoft SQL Server 2008 R2 (RTM) - 10.50.1600.1 (X64) Apr  2 2010 15:48:46    Copyright (c) Microsoft Corporation Standard Edition (64-bit) on Windows NT 6.1  (Build 7601: Service Pack 1)</p>\n\n<p>Version of SQL PM:</p>\n\n<p>Microsoft SQL Server 2008 R2 (RTM) - 10.50.1600.1 (X64) Apr  2 2010 15:48:46    Copyright (c) Microsoft Corporation Standard Edition (64-bit) on Windows NT 6.1  (Build 7601: Service Pack 1) (Hypervisor)</p>\n\n<p>I executed the script Execution plan for both are the same as there is no difference in plan.</p>\n\n<p>Vendor is HP ML350 Machine.</p>\n\n<p>There are almost 20 VM's on the same physical server out of which 7 servers are active.</p>\n"},{"tags":["c++","performance"],"answer_count":3,"favorite_count":1,"up_vote_count":3,"down_vote_count":1,"view_count":160,"score":2,"question_id":13117211,"title":"Why is memcpy slower than a reinterpret_cast when parsing binary data?","body":"<h3>TLDR: I forgot to enable compiler optimizations. With the optimizations enabled the performance is (nearly) identical.</h3>\n\n<p><br/></p>\n\n<h3>Original post</h3>\n\n<p>When reading integer from binary data I noticed that memcpy is slower than a casting solution.</p>\n\n<p>Version 1: reinterpret_cast, smelly due to potential alignment problems, but also faster (?)</p>\n\n<pre><code>int get_int_v1(const char * data) { return *reinterpret_cast&lt;const int*&gt;(data); }\n</code></pre>\n\n<p>Version 2: memcpy, correct and a little slower:</p>\n\n<pre><code>int get_int_v2(const char * data) { int result; memcpy(&amp;result, data, sizeof(result)); return result; }\n</code></pre>\n\n<p>I have <a href=\"http://ideone.com/6LEmA3\" rel=\"nofollow\">a benchmark on Ideone</a>.</p>\n\n<p>For future reference, the code is:</p>\n\n<pre><code>#include &lt;cstdlib&gt;\n#include &lt;cstdio&gt;\n#include &lt;cstring&gt;\n#include &lt;ctime&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;sys/time.h&gt;\n\ndouble get_current_time()\n{\n    timeval tv;\n    gettimeofday(&amp;tv, NULL);\n    return double (tv.tv_sec) + 0.000001 * tv.tv_usec;\n}\n\nint get_int_v1(const char * data) { return *reinterpret_cast&lt;const int*&gt;(data); }\nint get_int_v2(const char * data) { int result; memcpy(&amp;result, data, sizeof(result)); return result; }\n\nconst unsigned iterations = 200 * 1000 * 1000;\n\ndouble test_v1(const char * c, unsigned &amp; prevent_optimization)\n{\n    double start = get_current_time();\n    for (unsigned i = 0; i != iterations; ++i)\n    {\n        prevent_optimization += get_int_v1(c);\n    }\n    return get_current_time() - start;\n}\n\ndouble test_v2(const char * c, unsigned &amp; prevent_optimization)\n{\n    double start = get_current_time();\n    for (unsigned i = 0; i != iterations; ++i)\n    {\n        prevent_optimization += get_int_v2(c);\n    }\n    return get_current_time() - start;\n}\n\nint main()\n{\n    srand(time(0));\n\n    // Initialize data\n    std::vector&lt;int&gt; numbers(1000);\n    for (std::vector&lt;int&gt;::size_type i = 0; i != numbers.size(); ++i)\n    {\n        numbers[i] = i;\n    }\n\n    // Repeat benchmark 4 times.\n    for (unsigned i = 0; i != 4; ++i)\n    {\n        unsigned p = 0;\n        std::vector&lt;int&gt;::size_type index = rand() % numbers.size();\n        const char * c = reinterpret_cast&lt;const char *&gt;(&amp;numbers[index]);    \n        std::cout &lt;&lt; \"v1: \" &lt;&lt; test_v1(c, p) &lt;&lt; std::endl;\n        std::cout &lt;&lt; \"v2: \" &lt;&lt; test_v2(c, p) &lt;&lt; std::endl &lt;&lt; std::endl;\n    }\n}\n</code></pre>\n\n<p>And the results are:</p>\n\n<pre><code>v1: 0.176457\nv2: 0.557588\n\nv1: 0.17654\nv2: 0.220581\n\nv1: 0.176826\nv2: 0.22012\n\nv1: 0.176131\nv2: 0.220633\n</code></pre>\n\n<p>My questions are: </p>\n\n<ul>\n<li>Is my benchmark correct?</li>\n<li>If yes, then why is v2 (with memcpy) slower? Since both version return a copy of the data I think there should be no difference in performance.</li>\n<li>How can I implement a solution that is correct and fast?</li>\n</ul>\n\n<p><br/></p>\n\n<h3>Update</h3>\n\n<p>I was being silly and forgot to consider that Ideone doesn't perform compiler optimizations. I also tweaked the code a little and came up with the following:</p>\n\n<pre><code>#include &lt;algorithm&gt;\n#include &lt;cstdlib&gt;\n#include &lt;cstdio&gt;\n#include &lt;cstring&gt;\n#include &lt;ctime&gt;\n#include &lt;iomanip&gt; \n#include &lt;iostream&gt; \n#include &lt;vector&gt;\n#include &lt;sys/time.h&gt;\n\ndouble get_current_time()\n{\n    timeval tv;\n    gettimeofday(&amp;tv, NULL);\n    return double (tv.tv_sec) + 0.000001 * tv.tv_usec;\n}\n\nstruct test_cast\n{\n    int operator()(const char * data) const \n    {\n        return *((int*)data);\n    }\n};\n\nstruct test_memcpy\n{\n    int operator()(const char * data) const \n    {\n        int result;\n        memcpy(&amp;result, data, sizeof(result));\n        return result;\n    }\n};\n\nstruct test_std_copy\n{\n    int operator()(const char * data) const \n    {\n        int result;\n        std::copy(data, data + sizeof(int), reinterpret_cast&lt;char *&gt;(&amp;result));\n        return result;\n    }\n};\n\nenum\n{\n    iterations = 2000,\n    container_size = 2000\n};\n\nstd::vector&lt;int&gt; get_random_numbers()\n{\n    std::vector&lt;int&gt; numbers(container_size);\n    for (std::vector&lt;int&gt;::size_type i = 0; i != numbers.size(); ++i)\n    {\n        numbers[i] = rand();\n    }\n    return numbers;\n}\n\nstd::vector&lt;int&gt; get_random_indices()\n{\n    std::vector&lt;int&gt; numbers(container_size);\n    for (std::vector&lt;int&gt;::size_type i = 0; i != numbers.size(); ++i)\n    {\n        numbers[i] = i;\n    }\n    std::random_shuffle(numbers.begin(), numbers.end());\n    return numbers;\n}\n\ntemplate&lt;typename Function&gt;\nunsigned benchmark(const Function &amp; f, unsigned &amp; counter)\n{\n    std::vector&lt;int&gt; container = get_random_numbers();\n    std::vector&lt;int&gt; indices = get_random_indices();\n    double start = get_current_time();\n    for (unsigned iter = 0; iter != iterations; ++iter)\n    {\n        for (unsigned i = 0; i != container.size(); ++i)\n        {\n            counter += f(reinterpret_cast&lt;const char*&gt;(&amp;container[indices[i]]));\n        }\n    }\n    return unsigned(0.5 + 1000.0 * (get_current_time() - start));\n}\n\nint main()\n{\n    srand(time(0));\n    unsigned counter = 0;\n\n    std::cout &lt;&lt; \"cast:      \" &lt;&lt; benchmark(test_cast(),     counter) &lt;&lt; \" ms\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"memcpy:    \" &lt;&lt; benchmark(test_memcpy(),   counter) &lt;&lt; \" ms\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"std::copy: \" &lt;&lt; benchmark(test_std_copy(), counter) &lt;&lt; \" ms\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"(counter:  \" &lt;&lt; counter &lt;&lt; \")\" &lt;&lt; std::endl &lt;&lt; std::endl;\n\n}\n</code></pre>\n\n<p>The results are now nearly equal (except for <code>std::copy</code> which is slower for some reason):</p>\n\n<pre><code>g++ -o test -O0 -Wall -Werror -Wextra -pedantic-errors main.cpp\ncast:      56 ms\nmemcpy:    60 ms\nstd::copy: 290 ms\n(counter:  2854155632)\n\ng++ -o test -O1 -Wall -Werror -Wextra -pedantic-errors main.cpp\ncast:      9 ms\nmemcpy:    14 ms\nstd::copy: 20 ms\n(counter:  3524665968)\n\ng++ -o test -O2 -Wall -Werror -Wextra -pedantic-errors main.cpp\ncast:      4 ms\nmemcpy:    5 ms\nstd::copy: 20 ms\n(counter:  2590914608)\n\ng++ -o test -O3 -Wall -Werror -Wextra -pedantic-errors main.cpp\ncast:      4 ms\nmemcpy:    5 ms\nstd::copy: 18 ms\n(counter:  2590914608)\n</code></pre>\n"},{"tags":["iphone","performance","html5"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":30,"score":0,"question_id":13125013,"title":"Iphone4 \"add to home screen\" instant startup","body":"<p>How do you make a web app start up instantly - is it possible to make as fast as a native app that is already loaded?</p>\n\n<p>I have developed a HTML5 web app that runs nicely on Iphone 4 with a splashscreen. But now I'm looking into performance:</p>\n\n<p>I use a manifest file to ensure that all files are loaded from the local storage. I have checked both in chrome and mobile safari, that the files are stored correctly locally. Now performance it quite different depending on how I access my web app:</p>\n\n<p>~4s When I load the web app in browser (not from a home screen icon).\n~6s When I load from an \"add to homescreen\" icon</p>\n\n<p>When I load in chrome browser it takes 234ms to load and render the whole page. I seems like the lack of speed is due to the rendering being pretty slow.</p>\n\n<p>Any performance suggestions are very welcome. </p>\n"},{"tags":["java","performance","character-encoding"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":87,"score":3,"question_id":13120585,"title":"Decoding characters in Java: why is it faster with a reader than using buffers?","body":"<p>I am trying several ways to decode the bytes of a file into characters.</p>\n\n<p>Using <em>java.io.Reader</em> and <em>Channels.newReader(...)</em></p>\n\n<pre><code>public static void decodeWithReader() throws Exception {\n    FileInputStream fis = new FileInputStream(FILE);\n    FileChannel channel = fis.getChannel();\n    CharsetDecoder decoder = Charset.defaultCharset().newDecoder();\n    Reader reader = Channels.newReader(channel, decoder, -1);\n\n    final char[] buffer = new char[4096];\n    for(;;) {\n        if(-1 == reader.read(buffer)) {\n            break;\n        }\n    }\n\n    fis.close();\n}\n</code></pre>\n\n<p>Using buffers and a decoder <em>manually</em>:</p>\n\n<pre><code>public static void readWithBuffers() throws Exception {\n    FileInputStream fis = new FileInputStream(FILE);\n    FileChannel channel = fis.getChannel();\n    CharsetDecoder decoder = Charset.defaultCharset().newDecoder();\n\n    final long fileLength = channel.size();\n    long position = 0;\n    final int bufferSize = 1024 * 1024;   // 1MB\n\n    CharBuffer cbuf = CharBuffer.allocate(4096);\n\n    while(position &lt; fileLength) {\n        MappedByteBuffer bbuf = channel.map(MapMode.READ_ONLY, position, Math.min(bufferSize, fileLength - position));\n        for(;;) {\n            CoderResult res = decoder.decode(bbuf, cbuf, false);\n\n            if(CoderResult.OVERFLOW == res) {\n                cbuf.clear();\n            } else if (CoderResult.UNDERFLOW == res) {\n                break;\n            }\n        }\n        position += bbuf.position();\n    }\n\n    fis.close();\n}\n</code></pre>\n\n<p>For a 200MB text file, the first approach consistently takes 300ms to complete. The second approach consistently takes 700ms. Do you have any idea why the reader approach is so much faster?</p>\n\n<p>Can it run even faster with another implementation?</p>\n\n<p>The benchmark is performed on Windows 7, and JDK7_07.</p>\n"},{"tags":["performance","timer"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":14,"score":1,"question_id":13122308,"title":"System.Diagnostics.Stopwatch timer off by factor of 3","body":"<p>I'm trying to measure elapsed time using <code>Stopwatch</code> but consistently get times which are lower ~3.1 times than actual elapsed time. I can't put my finger on why this happens, code is dead simple. I've tried the code on 3 machines and getting same result (although with nearly same hardware and software).</p>\n\n<p>i5-2500k, i5-2500, Windows 7 64 bit, Windows 8 RTM 64 bit</p>\n\n<pre><code>Stopwatch sw = new Stopwatch();\nsw.Restart();\nDateTime start = DateTime.Now;\n\nfor (int i = 0; i &lt; 5; i++)\n{\n    Thread.Sleep(1000);\n}\n\nDateTime end = DateTime.Now;\nsw.Stop();\n\nConsole.WriteLine(\"Time passed: \" + StringFormatter.ToShortString(sw));\nConsole.WriteLine(\"Time passed: \" + StringFormatter.ToShortString(end - start));\n</code></pre>\n\n<p>result is consistently ~3.1 times off.</p>\n\n<pre><code>Time passed: 00:00:01.6347\nTime passed: 00:00:05.0600\n</code></pre>\n\n<p>for 10 sec delays:</p>\n\n<pre><code>Time passed: 00:00:03.2358\nTime passed: 00:00:10.0100\n</code></pre>\n"},{"tags":["c#","performance","linq-to-sql"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":43,"score":0,"question_id":13122152,"title":"Linq-toSql mass update speedup possibilities","body":"<p>I would be grateful if community could suggest some ways to speed-up query like</p>\n\n<pre><code>(from item in _dbContext.Items\n where item.SomeField &gt; someConstant\n &amp;&amp; item.ParentID==otherConstant\n select item).ToList().ForEach(item=&gt;item.FieldToChange = item.FieldToChange+3);\ncontext.SubmitChanges();\n</code></pre>\n\n<p>As I saw from profiler this equals to CountOfSelectedItems separate updates. With 200+ ms ping it makes me cry.</p>\n\n<p>Stored procedures and plain sql queries are restricted by our architects. Looking for transactions-like solution. </p>\n"},{"tags":["performance","http","iis-7.5","windows-server-2008"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":119,"score":0,"question_id":12980115,"title":"Please help resolve bottle neck in wait times for Http Responses?","body":"<p>As far as a performance issue, the server is performing fine.  With the exception of the http response wait times.  This will become more of an issue as we grow our line of online services.  All things being equal, I’m confused how this new server is it not loading pages as quickly as an older server running multiple websites, logging, etc… </p>\n\n<p>Here is a screen shot from <a href=\"http://www.gtmetrix.com\" rel=\"nofollow\">http://www.gtmetrix.com</a> the online testing tool I’ve been using.  These results are consistent regardless of time of day,  The numbers here don’t make sense.  The new site page is 75% smaller, yet its total time to live is only 26ms faster.  In the below image the left side is NEW SERVER, the right side is OLD SERVER\nThe left portion of the timeline is the Handshaking portion.  So, you can see, the new server, is about the same speed.  The purple middle section, that represents wait time.  It’s about 4 times the delay in milliseconds as OLD SERVER.  The Grayish section on the right represents the actual time to download the file.  You will also notice that the new server is significantly faster at downloading the response, this is most likely due to the 75% decrease in the response size.</p>\n\n<p><img src=\"http://i.stack.imgur.com/UKW8m.gif\" alt=\"GtMetrix Resuts\"> You can see the complete results for the new server here. <a href=\"http://gtmetrix.com/reports/204.193.113.47/Kl614UCf\" rel=\"nofollow\">http://gtmetrix.com/reports/204.193.113.47/Kl614UCf</a></p>\n\n<p>Here’s a table of the differences that I’m aware of, let me know if you see one that could be the culprit.  I forgot to add this to the table, but the old server, is in production, right now serving requests, when www.gtmetrix is hitting it.  In contrast, to my New server, which is just me connecting and generating requests.</p>\n\n<p><img src=\"http://i.stack.imgur.com/Kazw0.gif\" alt=\"Server Config Comparison\">      </p>\n\n<p>My current hypothesis, is that the slowness is caused some combination of the server being virtualized, incorrect IIS settings, or the difference between 32bit and 64bit OSes</p>\n"},{"tags":["performance","ios5","uiview","uiimageview"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":28,"score":0,"question_id":13085462,"title":"UIImageView delay when adding to another UIView","body":"<p>This is my first post here - but I've been a reader for a long time. Thank you so much for this site! :-)</p>\n\n<p>I am currently working on a port of my XNA-based 2D engine from WP7 to iOS (5). I would prefer not to use OpenGL directly, because I prefer to invest my time more in gameplay than in technique. So I would be very happy to get an answer not involving OpenGL (directly).</p>\n\n<p>The problem:\nWhen I add an UIImageView to another UIView, there is a short delay before the UIImageView gets drawn. I assume, this is due to the caching the UIView-class performs before converting everything internally to OpenGL then drawing.</p>\n\n<p>What I want:\nTell the UIView (superview) to perform all neccessary calculations for all subviews and <em>then</em> draw them <em>all</em> at once.\nCurrently the behaviour I observe ist: Calculate uiimageview_1, draw uiimageview_1, calculate uiimageview_n, draw uiimageview_n, ...</p>\n\n<p>Dummycode of what I want:</p>\n\n<pre><code>// put code here to tell superview to pause drawing\nfor (int i = 0; i &lt; 400; i ++)\n{\n    add UIImageView[i] to superview;\n}\n// put code here to tell superview to draw now\n</code></pre>\n\n<p>Possible workaround (but coming from C# &amp; Windows, I have no idea how to implement it efficiently in Objective-C on iOS) - I am afraid that this code is inefficient because large blocks of RAM had to be transferred (per frame!) on retina displays at native resolution:</p>\n\n<pre><code>for (int i = 0; i &lt; 400; i ++)\n{\n    add UIImageView[i] to superview;\n}\n// put code here to get a bitmap in ram from superview\n// return bitmap and draw it in a view for the scenery/canvas\n</code></pre>\n\n<p>Any help on how to approach this \"popping\"-problem would be highly appreciated.</p>\n\n<p>Thanks</p>\n"},{"tags":["performance","matlab","vectorization"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":223,"score":3,"question_id":9008259,"title":"Speeding up MATLAB code for FDR estimation","body":"<p>I have 2 input variables: </p>\n\n<ul>\n<li>a vector of p-values (<strong>p</strong>) with <em>N</em> elements (unsorted)</li>\n<li>and <em>N</em> x <em>M</em> matrix with p-values obtained by random permutations (<strong>pr</strong>) with <em>M</em> iterations. <em>N</em> is quite large, 10K to 100K or more. <em>M</em> let's say 100.</li>\n</ul>\n\n<p>I'm estimating the False Discovery Rate (FDR) for each element of <code>p</code> representing how many p-values from random permutations will pass if the current p-value (from <code>p</code>) will be the threshold.</p>\n\n<p>I wrote the function with ARRAYFUN, but it takes lot of time for large N (2 <em>min</em> for <em>N</em>=20K), comparable to for-loop.</p>\n\n<pre><code>function pfdr = fdr_from_random_permutations(p, pr)\n%# ... skipping arguments checks\npfdr = arrayfun( @(x) mean(sum(pr&lt;=x))./sum(p&lt;=x), p);\n</code></pre>\n\n<p>Any ideas how to make it faster?</p>\n\n<p>Comments about statistical issues here are also welcome.</p>\n\n<p>The test data can be generated as <code>p = rand(N,1); pr = rand(N,M);</code>.</p>\n"},{"tags":["performance","functional-programming","ocaml","fold"],"answer_count":3,"favorite_count":1,"up_vote_count":7,"down_vote_count":0,"view_count":228,"score":7,"question_id":4851542,"title":"Optimisations with folds","body":"<p>I am just curious if there are any (first order polymorphic only) optimisations with folds.</p>\n\n<p>For maps, there's deforestation: <code>map g (map f ls) =&gt; map (g . f) ls</code>, and <code>rev (map f ls) =&gt; rev_map f ls</code> (faster in Ocaml).</p>\n\n<p>But fold is so powerful, it seems to defy any optimisation.</p>\n"},{"tags":["eclipse","performance","workspace"],"answer_count":4,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":2512,"score":5,"question_id":4386119,"title":"Eclipse getting too slow - workspace recreation helped","body":"<p>My Eclipse was getting slower and slower over time. Tips I found on the Internet did not help.\nWhat I did is completely deleted my workspace, created new one and reimported all my projects into the new workspace and this really made the difference.</p>\n\n<p>So my question is whether it's possible to perform this workspace clean-up without deleting and recreating workspace...\nMaybe there is some cache in workspace which is getting big? Any ideas?</p>\n\n<p>Thank you!</p>\n"},{"tags":["performance","apache","http-status-code-404","apache-config"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":597,"score":0,"question_id":4411827,"title":"Will broken css/image link 404 errors affect server performance under load?","body":"<p>I have a few images and a css file on my site that don't exist on the server, so each time a visitor comes Apache throws 3 404 errors in it's log.  The items are hidden on the page, so it does not affect the display of the page.  Our site performs fine in testing and production environment. We recently recieved a 2 day traffic spike of 30,000-40,000 visitors per day and apache slowed to a halt, waiting 22-25 seconds before returning anything to the browser.  </p>\n\n<p>Would the 404 errors thrown on the page change the server performance?  </p>\n\n<p>Is a 404 error more resource-intensive then a normal request?</p>\n\n<p>Any info re: the way Apache handles 404 errors would be appreciated.</p>\n\n<p>Thanks,\nSj</p>\n"},{"tags":["sql-server-2005","performance","full-text"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":284,"score":0,"question_id":1875126,"title":"Any idea why contains(...) querys so slow in SQL Server 2005","body":"<p>I've got a simple select query which executes in under 1 second normally, but when I add in a contains(column, 'text') into the where clause, suddenly it's running for 20 seconds up to a minute. The table it's selecting from has around 208k rows.</p>\n\n<p>Any ideas what would cause this query to run so slow with just the addition of the contains clause?</p>\n"},{"tags":["javascript","performance","setinterval"],"answer_count":5,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":61,"score":0,"question_id":13095972,"title":"Is it possible to have \"setInterval\" set too fast?","body":"<p>I have a setinterval function in my javascript that I want to be as fast as possible, i.e. check the status of an event every 1ms. Is it possible that this would be asking too much from a user's browser? It seems to work just fine, but I am wondering if its a bad practice.</p>\n"},{"tags":["arrays","performance","datatable"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":15,"score":1,"question_id":13115924,"title":"Return type string array or datatable?","body":"<p>I have a utility method from which I need to return set of strings which could binded to a list control in the UI. I am in doubt whether to go with array of strings which I think is the lighter than a datatable Or add them into datatable so that I can directly bind the datatable to list control? Which one is bettter in performance?</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["performance","networking","vim"],"answer_count":6,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":2214,"score":5,"question_id":2103968,"title":"GVim runs very slowly when editing files on a windows share","body":"<p>On my computer at work, any time I open a file located on a network share, GVim becomes completely unusable. Scrolling through the document can take 15 seconds to go one \"page\". Even using the movement keys to go from one word to another can take 2 to 3 seconds. This seems to be a new behavior, but for the life of me I can't remember changing anything that would cause it. I'm under the impression that Vim doesn't actually access a file except on open and on save. Is that right?</p>\n"},{"tags":["performance","architecture","nginx","netty"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":13113587,"title":"Push (i.e. Netty) vs. Pull (i.e. Nginx) for a live \"ping\" server?","body":"<p>Imagine an application which receives something like news updates every minute.</p>\n\n<p>Would it be more efficient to build the server-side with something like Netty- which would maintain the connections and push the data once a minute, or something like nginx/php which would drop/open the connection each time a pull-request is made?</p>\n\n<p>Each request would require a database lookup custom tailored to that user (i.e. no caching) and some basic processing (i.e. encryption/decryption)</p>\n\n<p>?</p>\n"},{"tags":["performance","testing","loadrunner"],"answer_count":3,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":946,"score":2,"question_id":5567634,"title":"Load Runner and Browser Client Performance","body":"<p>I'd like to confirm a few things:</p>\n\n<p>1) Standard load runner scripts do NOT record the time the browser would spend rendering HTML and processing JS?</p>\n\n<p>2) A GUI VUser would be needed to accomplish #1.  Are there other ways?</p>\n\n<p>3) The scripting for GUI Vuser is different than that of a standard vuser?</p>\n\n<p>4) Is there any full proof way of determining if LoadRunner is capturing client time (as some analyzing the results not the executor/designer of the test)?</p>\n\n<p>Thanks.</p>\n"},{"tags":["c#","performance","mono","scalability","server-side"],"answer_count":6,"favorite_count":4,"up_vote_count":15,"down_vote_count":0,"view_count":349,"score":15,"question_id":10660990,"title":"C# server scalability issue on linux","body":"<p>I've a C# server developed on both Visual Studio 2010 and Mono Develop 2.8. NET Framework 4.0</p>\n\n<p>It looks like this server behaves much better (in terms of scalability) on Windows than on Linux.\nI tested the server scalability on native Windows(12 physical cores), and 8 and 12 cores Windows and Ubuntu Virtual Machines using Apache's ab tool.</p>\n\n<p>The windows response time is pretty much flat. It starts picking up when the concurrency level approaches/overcomes the number of cores.</p>\n\n<p>For some reason the linux response times are much worse. They grow pretty much linearly starting from level 5 of concurrency. Also 8 and 12 cores Linux VM behave similarly. </p>\n\n<p><strong>So my question is: why does it perform worse on linux? (and How can I fix that?).</strong></p>\n\n<p>Please take a look at the graph attached, it shows the averaged time to fulfill 75% of the requests as a function of the requests concurrency(the range bar are set at 50% and 100%).\n<img src=\"http://i.stack.imgur.com/mVPMh.png\" alt=\"time to fulfill 75% of the request as a function of the request concurrency\"></p>\n\n<p>I have a feeling that this might be due to mono's Garbage Collector. I tried playing around with the GC settings but I had no success. <strong>Any suggestion?</strong></p>\n\n<p>Some additional background information: the server is based on an HTTP listener that quickly parses the requests and queues them on a thread pool. The thread pool takes care of replying to those requests with some intensive math (computing an answer in ~10secs).</p>\n"},{"tags":["c#","performance","sockets","crash","wireshark"],"answer_count":6,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":4036,"score":2,"question_id":679643,"title":"TCP Socket Server Builds Up CLOSE_WAITs Occasionally Over Time Until Inoperable","body":"<p>Hopefully someone can help us as we're reaching as far as investigation can go!</p>\n\n<p>We've got a simple asynchronous socket server written in C# that accepts connections from an ASP.NET web application, is sent a message, performs some processing (usually against a DB but other systems too) and then sends a response back to the client. The client is in charge of closing the connection.</p>\n\n<p>We've been having issues where if the system is under heavy load over a long period of time (days usually), CLOSE_WAIT sockets build up on the server box (netstat -a) to an extent that the process will not accept any further connections. At that point we have to bounce the process and off it runs again.</p>\n\n<p>We've tried running some load tests of our ASP.NET application to attempt to replicate the problem (because inferring some issue from the code wasn't possible). We think we've managed this and ended up with a WireShark <a href=\"http://drop.io/close%5Fwait3793\" rel=\"nofollow\">packet trace</a> of the issue manifesting itself as a SocketException in the socket server's logs:</p>\n\n<blockquote>\n  <p>System.Net.Sockets.SocketException: An existing connection was forcibly closed by the remote host\n  at System.Net.Sockets.Socket.BeginSend(Byte[] buffer, Int32 offset, Int32 size, SocketFlags socketFlags, AsyncCallback callback, Object state)</p>\n</blockquote>\n\n<p>I've tried to reproduce the issue from the packet trace as a single threaded process directly talking to the socket server (using the same code the ASP.NET app does) and am unable. </p>\n\n<p>Has anybody got any suggestions of next things to try, check for or obvious things we may be doing wrong?</p>\n"},{"tags":["css","performance","html5"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":3,"view_count":74,"score":1,"question_id":13114260,"title":"More CSS, less HTML?","body":"<p>Which is better performance wise?</p>\n\n<pre><code>&lt;a class=\"btn loginbtn\" href=\"#\"&gt;Login&lt;/a&gt;​\n\n.btn {\n    background: #555 \n}\n.loginbtn {\n    padding: 10px\n}​\n</code></pre>\n\n<p><strong>or</strong></p>\n\n<pre><code>&lt;a class=\"loginbtn\" href=\"#\"&gt;Login&lt;/a&gt;​\n\n.btn,.loginbtn {\n    background: #555  \n}\n.loginbtn {\n    padding: 10px     \n}​\n</code></pre>\n\n<p>Since my CSS will be cached I was thinking the second one would be better.</p>\n\n<p>Help me out please.</p>\n"},{"tags":["asp.net","database","performance","blob","httphandler"],"answer_count":2,"favorite_count":3,"up_vote_count":5,"down_vote_count":0,"view_count":69,"score":5,"question_id":13041310,"title":"perfomance of .ashx handlers for retrieving a lot of binary images","body":"<p>I used .ashx handler for getting images from database.I want to retrieve a lot of images (>1000) in this way:</p>\n\n<pre><code>    &lt;img src='GetImage.ashx?id= &lt;%# Eval(\"id\") %&gt;'/&gt;\n</code></pre>\n\n<p>(why I do this you can understand if read my previous question: <a href=\"http://stackoverflow.com/questions/13002578/bind-database-image-to-itemtemplate-in-ascx/\">bind database image to ItemTemplate in .ascx</a> ).I am afraid that multipiles database querys (first query to get all id's,all others for getting image one by one) will take a lot of time,is it? What are possible solutions?</p>\n"},{"tags":["django","performance","apache","memory","nginx"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":63,"score":0,"question_id":13110028,"title":"Django Performance / Memory usage","body":"<p>I am running an alpha version of my app on a EC2 Small instance (1.7 GB RAM) with postgres and apache (wsgi-mod not as daemon but directly) on it.</p>\n\n<p>Performance is alright, but it could be better. I am also worried about memory usage if too many test users would join.</p>\n\n<p>Is it wise to switch from Apache to nginx server? Has any Django developer done that and is happier with the results? Any other tips on the way are also welcome.</p>\n\n<p>Thanks</p>\n"},{"tags":["java","performance","reflection","map","annotations"],"answer_count":2,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":347,"score":2,"question_id":8098648,"title":"Performance of calling Method/Field.getAnnotation(Class) several times vs. Pre-caching this data in a Map","body":"<p>I'd like to know if there are any comparison/studies about the performance of repeatidly calling (in Java) the <code>Method.getAnnotation(Class)</code> and <code>Field.getAnnotation(Class)</code> methods, versus storing (at program start up time) a precomputed Map with this metadata information of the classes and querying it repeatidly later. Which one would provide the best runtime performance?</p>\n\n<p>And this performance would be the same under Java 5, 6 and 7?</p>\n"},{"tags":["performance","vb6"],"answer_count":5,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":321,"score":7,"question_id":8175336,"title":"How to decrease VB6 project startup time / Pinpointing what's taking so long","body":"<p>There are two vb6 applications that I work with.  One of them starts up very quickly whereas the other one takes quite a long time.  I thought I would do a little analysis to find out why the one takes so long.</p>\n\n<p>So I hit F8 to start at the beginning and I realize that a significant portion of that startup time is actually between the time I hit F8 and the time it highlights the very first line of code.</p>\n\n<p>Which of the following is most likely causing this?</p>\n\n<ul>\n<li>Number of dependencies</li>\n<li>Having too many projects in the group project instead of referencing them as dlls</li>\n<li>Number of forms</li>\n<li>Number of objects in the startup form</li>\n<li>Number of objects on all forms</li>\n<li>What else?</li>\n</ul>\n\n<p>And as a bonus, I would love any ideas on how to more specifically pinpoint the problem if it could be in multiple areas.</p>\n\n<p>Thanks! </p>\n\n<p>Edit: It seems I may have not been clear enough on exactly 'where' the slowdown is occurring.  So to make it clear I created the following procedure:</p>\n\n<pre><code>Sub Main()\nEnd Sub\n</code></pre>\n\n<p>That's it, and it's in a module that contains absolutely nothing besides these two lines.  No forms are getting loaded, and while there are other modules with \"Dim o as New SomeObject\", I know those objects aren't getting instantiated because I know that visual basic doesn't create objects declared this way until you actually use them for the first time.</p>\n\n<p>I believe I have now optimized the startup code as much as is technically possible.  Yet it still takes the same amount of time to startup.</p>\n\n<p>Edit 2: I just realized that the compiled application actually starts up reasonably fast.  It's just starting it in the ide that takes so long.  However, I care a lot more about the speed for me than I do the customer cause they just start it once and leave it running all day whereas I start it a couple dozen times a day.</p>\n"},{"tags":["java","performance","scala","memory"],"answer_count":8,"favorite_count":13,"up_vote_count":42,"down_vote_count":0,"view_count":15761,"score":42,"question_id":5901452,"title":"scala vs java, performance and memory?","body":"<p>I am keen to look into Scala, and have one basic question I cant seem to find an answer to:\nin general, is there a difference in performance and usage of memory between Scala and Java?</p>\n"},{"tags":["entity-framework","import","loops","performance","savechanges"],"answer_count":3,"favorite_count":15,"up_vote_count":12,"down_vote_count":0,"view_count":5080,"score":12,"question_id":1930982,"title":"When should I call SaveChanges() when creating 1000's of Entity Framework objects? (like during an import)","body":"<p>I am running an import that will have 1000's of records on each run.  Just looking for some confirmation on my assumptions:</p>\n\n<p>Which of these makes the most sense:</p>\n\n<ol>\n<li>Run <code>SaveChanges()</code> every <code>AddToClassName()</code> call.</li>\n<li>Run <code>SaveChanges()</code> every <em>n</em> number of <code>AddToClassName()</code> calls.</li>\n<li>Run <code>SaveChanges()</code> after <em>all</em> of the <code>AddToClassName()</code> calls.</li>\n</ol>\n\n<p>The first option is probably slow right?  Since it will need to analyze the EF objects in memory, generate SQL, etc.</p>\n\n<p>I assume that the second option is the best of both worlds, since we can wrap a try catch around that <code>SaveChanges()</code> call, and only lose <em>n</em> number of records at a time, if one of them fails.  Maybe store each batch in an List&lt;>.  If the <code>SaveChanges()</code> call succeeds, get rid of the list.  If it fails, log the items.</p>\n\n<p>The last option would probably end up being very slow as well, since every single EF object would have to be in memory until <code>SaveChanges()</code> is called.  And if the save failed nothing would be committed, right?</p>\n"},{"tags":["php","mysql","string","performance","query"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":33,"score":0,"question_id":13106195,"title":"How can I find word matches from a string in a simple mysql/php words in string wordsolver app?","body":"<p>I have 27 tables in my database.  One word table (a scrabble word list), and 26 association tables.</p>\n\n<pre><code>Table  Fields\n================\nword   [id,word]\na      [word_id]\nb      [word_id]\n...\nz      [word_id]\n</code></pre>\n\n<p>I'm trying to figure out matching words given a string.</p>\n\n<p>For example, if the given string is <code>pant</code>, I want to know: <code>pant, apt, pat, tap, ant, tan, nap, pan, at, ta, pa, an, na</code>.</p>\n\n<p>My current strategy is to explode each letter in the string and find the associated words that match all the letters.</p>\n\n<p>For example:</p>\n\n<pre><code>SELECT word.word\nFROM word, p, a, n, t\nWHERE\n    word.id = p.word_id OR\n    word.id = a.word_id OR\n    word.id = n.word_id OR\n    word.id = t.word_id\n</code></pre>\n\n<p>But this ends up printing all words that have a p,a,n or t in them.</p>\n\n<p>And if I switch all the operators to <code>AND</code>, I'm stuck with only one match: <code>pant</code>.</p>\n\n<p>Can you help me solve this riddle?</p>\n\n<p>I'm also concerned with how to handle duplicate letters in the string.  For example, <code>PPANT</code> should find a match for <code>app</code>, when plain <code>PANT</code> should not.</p>\n\n<p>Am I on the right track with the association tables or is there a better way?</p>\n\n<p>I'm trying to handle this fairly efficiently in php/mysql.  I'm aware there are others who have solved this riddle before in C, perl, java and the like.</p>\n"},{"tags":["javascript","performance","garbage-collection"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":73,"score":1,"question_id":13110791,"title":"Which way is better when we're focus on garbage collecting?","body":"<p>Simple question, what is better for Garbage Collector:</p>\n\n<ul>\n<li>adding object to array and assigning null to array cell</li>\n</ul>\n\n<p><code>this.requests[i] = new NiceClass();</code></p>\n\n<p><code>this.requests[i] = null;</code></p>\n\n<ul>\n<li>creating simple variable and assigning null to this variable</li>\n</ul>\n\n<p><code>var niceClass = new NiceClass();</code></p>\n\n<p><code>niceClass = null;</code></p>\n\n<p>I'm asking because I want to avoid using array, because during creating a lot of object - despite breaking reference to object so garbage collector is able to grab unnecessary objects - array is growing, so I'm curious, is it necessary here or it will be even better to use just variables?</p>\n"},{"tags":["c#","asp.net",".net","performance","profiling"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":477,"score":4,"question_id":5667978,"title":"Code Profiling ASP.NET MVC2 applications","body":"<p>The thread here seems close:\n<a href=\"http://stackoverflow.com/questions/378617/profiling-asp-net-websites-with-eqatec-profiler\">Profiling ASP.NET websites with EQATEC Profiler</a></p>\n\n<p>However, in the free version of Equatec I downloaded today there is no checkboxes for ASP.NET, and ordinary web. I have pointed the App path to my bin directory in my project folder as well as started up the localhost hosting for my application via Visual Studio.</p>\n\n<p>I am open to other <strong>free</strong> tools as well. I am just looking for someway to profile the code as to optimize some reflection we are using.</p>\n\n<p>I am using the professional edition so unfortunantly do not have access to MS Code Profiling.</p>\n\n<p>I am looking to do performance profiling at this point.</p>\n\n<p>Is the free version of Equatec capable of doing ASP.NET applications?</p>\n\n<p>Is there a free profiler (I realize this has been asked before, and little seems to have surfaced but paid apps, but might as well ask)?</p>\n\n<p>Is MVC a special thing to look for in a profiler?</p>\n"},{"tags":["c#",".net","performance","download","webclient"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":1342,"score":2,"question_id":4932541,"title":"C# WebClient acting slow the first time","body":"<p>I am using a WebClient to download a string from a website (which just contains plain text, nothing else), so I use the DownloadString method:</p>\n\n<pre><code>WebClient wc = new WebClient();\nstring str = wc.DownloadString(\"http://blah\");\n</code></pre>\n\n<p>It works fine, but the problem is that the first time it downloads the string it takes a long time, like 5 seconds. After that it works fast. Why does this happen and how can overcome this problem?</p>\n"},{"tags":["performance","vim","highlighting"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":37,"score":1,"question_id":13111330,"title":"Performance on highlighting words/lines in Vim","body":"<p>I have a set of small functions in VimL highlight a line (or a word) depending on certain conditions.</p>\n\n<p>You should consider the workings of the functions to act similar as the spelling (<code>:set spell</code>), underlining when the conditions are met.</p>\n\n<p>I have found, however, that when the number of highlighted lines exceed about 75, there is a significant lag when moving. Either from side to side or up or down.</p>\n\n<p>I had some convenient <code>AutoCommands</code> that I was enabling by default, (for example, to echo why the line is highlighted) but even with all of them disabled, as soon as I call the function that highlights everything, I can tell there is a huge lag.</p>\n\n<p>This is what I am using to highlight a word:</p>\n\n<pre><code>call matchadd('MyCheck', '^\\%'. line . 'l\\_.\\{-}\\zs\\k\\+\\k\\@!\\%&gt;' . column . 'c')\n</code></pre>\n\n<p>And this is what I use to highlight the whole line</p>\n\n<pre><code>call matchadd('MyCheck', '\\%' . line . 'l\\n\\@!')\n</code></pre>\n\n<p>The 75 number I use as a reference for determining a lag is just a reference, it is a bit of a sweet spot for me, but just to demonstrate that most anything beyond gets increasingly worse.</p>\n\n<p>I also use the <code>SpellBad</code> highlighting for <code>MyCheck</code>, but seriously doubt that this causes any problems.</p>\n\n<p>Is there something I could do differently to avoid the lag? Is <code>matchadd</code> the best option?</p>\n\n<p>EDIT:\nJust to make sure it is not any of my functions or code doing something weird, I opened a 500 line file and did this:</p>\n\n<pre><code>highlight link MyCheck SpellBad                                                      \nfor line in range(line('$'))\n    call matchadd('MyCheck', '\\%' . line . 'l\\n\\@!')\nendfor\n</code></pre>\n\n<p>Which basically highlights every single line on the file. Everything clearly got impossibly slow.</p>\n\n<p>EDIT 1: \nIt seems that unsetting <code>cursorline</code> has a drastic (positive) effect in performance. I did <code>:set nocursorline</code> and now my movements (regardless of highlighting) are snappy as before</p>\n"},{"tags":["c#","algorithm","performance","cartesian-product"],"answer_count":7,"favorite_count":3,"up_vote_count":10,"down_vote_count":1,"view_count":3372,"score":9,"question_id":1741364,"title":"Efficient Cartesian Product algorithm","body":"<p>Can somebody please demonstrate for me a more efficient Cartesian product algorithm than the one I am using currently (assuming there is one).  I've looked around SO and googled a bit but can't see anything obvious so I could be missing something.</p>\n\n<pre><code>foreach (int i in is) {\n   foreach (int j in js) {\n      //Pair i and j\n   }\n}\n</code></pre>\n\n<p>This is a highly simplified version of what I do in my code.  The two integers are lookup keys which are used to retrieve one/more objects and all the objects from the two lookups are paired together into new objects.</p>\n\n<p>This small block of code in a much larger more complex system becomes a major performance bottleneck as the dataset it's operating over scales.  Some of this could likely be mitigated by improving the data structures used to store the objects and the lookups involved but the main issue I feel is still the computation of the Cartesian product itself.</p>\n\n<p><strong>Edit</strong> </p>\n\n<p>So some more background on my specific usage of the algorithm to see if there may be any tricks that I can use in response to Marc's comment.  The overall system is a SPARQL query engine which processes SPARQL queries over sets of Graph data, SPARQL is a pattern based language so each query consists of a series of patterns which are matched against the Graph(s).  In the case where two subsequent patterns have no common variables (they are disjoint) it is necessary to compute the Cartesian product of the solutions produced by the two patterns to get the set of possible solutions for the overall query.  There may be any number of patterns and I may have to compute Cartesian products multiple times which can lead to a fairly exponential expansion in possible solutions if the query is composed of a series of disjoint patterns.</p>\n\n<p>Somehow from the existing answers I doubt whether there any tricks that could apply</p>\n\n<p><strong>Update</strong></p>\n\n<p>So I thought I would post an update on what I implemented in order to minimise the need to do Cartesian products and thus optimise the query engine generally.  Note that it's not always possible to completely eliminate the need for products but it's nearly always possible to optimise so the size of the two sets being joined is much smaller.</p>\n\n<p>Since each BGP (Basic Graph Pattern) which is a set of Triple Patterns is executed as a block (in essence) the engine is free to reorder patterns within a BGP for optimal performance.  For example consider the following BGP:</p>\n\n<pre><code>?a :someProperty ?b .\n?c :anotherProperty ?d .\n?b a :Class .\n</code></pre>\n\n<p>Executed as is the query requires a cartesian product since the results of the first pattern are disjoint from the second pattern so the results of the first two patterns is a cartesian product of their individual results.  This result will contain far more results than we actually need since the third pattern restricts the possible results of the first pattern but we don't apply this restriction till afterwards.  But if we reorder like so:</p>\n\n<pre><code>?b a :Class .\n?a :someProperty ?b .\n?c :anotherProperty ?d .\n</code></pre>\n\n<p>We'll still need a cartesian product to get the final results since the 2nd and 3rd patterns are still disjoint but by reordering we restrict the size of the results of the 2nd pattern meaning the size of our cartesian product will be much smaller.</p>\n\n<p>There are some various other optimisations we make but I'm not going to post them here as it starts to get into fairly detailed discussion of SPARQL engine internals.  If anyone is interested in further details just leave a comment or send me a tweet @RobVesse</p>\n"},{"tags":["c#","performance","try-catch","direct3d","slimdx"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":22,"score":1,"question_id":13111037,"title":"SlimDX Lost Device: Try Catch performance?","body":"<p>I'm using SlimDX and Direct3D9 classes to create a C# game engine.</p>\n\n<p>I started with it one year ago and I remember that I successfully and safely managed to catch lost devices and reset them (on Windows 7). Now (on Windows 8) I started working on it again, but it looks like I'm not catching lost devices in any case anymore, especially in Ctrl+Alt+Del cases.</p>\n\n<p>This is the previously working code: \nThe \"IsDeviceLost\" method erroneously returns false at Ctrl+Alt+Del, so the device is about to present, but it crashes to a Direct3D9Exception: \"D3DERR_DEVICELOST\":</p>\n\n<pre><code>    public void Update(float time)\n    {\n        if (!IsDeviceLost(true))\n        {\n            _device.Present();\n        }\n    }\n\n    private bool IsDeviceLost(bool resetIfNeeded)\n    {\n        bool deviceLost = false;\n\n        // Check if DeviceLost is detected\n        Result result = _device.TestCooperativeLevel();\n        if (result == ResultCode.DeviceLost)\n        {\n            Log.Write(LogType.Warning, \"Direct3D9Manager: Device lost and cannot be reset yet.\");\n            // Device has been lost and cannot be reset yet\n            deviceLost = true;\n        }\n        else if (result == ResultCode.DeviceNotReset)\n        {\n            Log.Write(LogType.Information, \"Direct3D9Manager: ResultCode.DeviceNotReset\");\n            // Device is available again but has not yet been reset\n            if (resetIfNeeded)\n            {\n                // Reset device and check if it can work again\n                _device.Reset(_presentParams);\n                deviceLost = IsDeviceLost(false);\n                if (deviceLost)\n                {\n                    // Reset failed, device still lost\n                }\n                else\n                {\n                    // Reset successful, device restored\n                    // TODO: Reload textures and render states which are lost after a reset\n                }\n            }\n            else\n            {\n                deviceLost = true;\n            }\n        }\n\n        return deviceLost;\n    }\n</code></pre>\n\n<p>So I researched the web about this problem and found several code, which puts the Update code into the try part of a try-catch block, but I'm not sure if that is the right way to fix this.</p>\n\n<ul>\n<li>Isn't try-catch slow for an Update method in a game engine which gets called every frame?</li>\n<li>Aren't there better ways to catch a lost device, which are not using try-catch?</li>\n</ul>\n"},{"tags":["performance","delphi","serialization"],"answer_count":0,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":111,"score":1,"question_id":13110821,"title":"Is there a benchmark for Delphi serialization libraries?","body":"<p>For high performance Delphi / Free Pascal applications which need to communicate over IPC / the network I am interested in performance tests of serialization libraries for Delphi.</p>\n\n<p>As this is not for cross-language operation, binary serialization is an option too, it is not limited to JSON or XML. I am also not limited to serialization of TPersistent or TRemotable descendants, or usage of classic vs extended RTTI.</p>\n\n<p>I have not yet seen a benchmark which allows to run performance statistics for available libraries. Have you seen anything in this direction?</p>\n"},{"tags":["ruby-on-rails","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":20,"score":0,"question_id":13110448,"title":"Ignoring bots when tracking events in Rails app","body":"<p>I am tracking events through Mixpanel in my Rails app. For example, my controller looks something like</p>\n\n<pre><code>class HomeController &lt; ApplicationController\n  def index\n    track_event \"Visitor: View Landing Page\"\n  end\nend\n</code></pre>\n\n<p>The problem is that the app gets hit by a number of bots, most notably Pingdom (performance tracking service we use). Is there a clean way to ignore tracking when it is a bot that hits my app?</p>\n\n<p><em>Note: I am interested in tracking unique visitors, so I assign a cookie to each visitor with a unique id. Bots obviously don't store cookies.</em></p>\n"},{"tags":["performance","logging","openjpa","runtime.exec"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":12,"score":-1,"question_id":13110347,"title":"openjpa mappingtool log","body":"<p>I use openjpa 1.2.0. </p>\n\n<p>Now, there is a strange thing that happens :\nwhen I run the application the simple way ( from eclipse or double click on exe ), it starts fine (about 2 minutes), but when I run it from another program ( by using runtime.exec(...) ) it takes about 20 minutes to start.</p>\n\n<p>After some test and profiling I found that it spends most of the time in MappingTool.run() writing logs. I couldn't find the log file or the specific logs that are written there. Since it runs in a separate process, I can't debug it the regular way.</p>\n\n<p>Can somebody please tell me:\nWhere to find the log file? or\nHow to debug this application? or\nHow to reduce the start time?</p>\n\n<p>Thanks in advance</p>\n"},{"tags":["xcode","performance","ipad"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":20,"score":-1,"question_id":13110122,"title":"Xcode - Speeding up iPad App","body":"<p>i have a question about speeding my iPad app. The main part of my app is the home viewcontroller which loads two another viewcontrollers in it. One of them is a calendar which i made, and the other a note view. </p>\n\n<p>When i change the month of the calendar, the calendar viewcontroller as it should, ads buttons, depends of how many months did you go back or forward. I know that the problem is in that, i tried to reduce the buttons image size to 2kb, didn't work. Are there any other solutions ?  </p>\n\n<p>I saw the problem at the main screen, when i press some button to show a modal view controller. </p>\n\n<p>Any idea how can i speed up my app ? </p>\n"},{"tags":["jquery","performance","jquery-ui","drag-and-drop"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":2692,"score":4,"question_id":1133413,"title":"jQuery drag drop slower for more DIV items","body":"<p>I have got a hierarchichal  tags (with parent child relationship) in my page and it will account to 500 - 4500 (can even grow). When i bound the draggable and droppable for all i saw very bad performance in IE7 and IE6. The custom helper wont move smoothly and was very very slow. Based on some other post i have made the droppable been bound/unbound on mouseover and mouseout events (dynamically). Its better now. </p>\n\n<p>But still i dont see the custom helper move very smoothly there is a gap between the mouse cursor and the helper when they move and gets very bad when i access the site from remote.</p>\n\n<p>Please help me to address this performance issue. Am totally stuck here.. :(</p>\n"},{"tags":["c#","asp.net-mvc","performance","entity-framework"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":54,"score":1,"question_id":13103852,"title":"I want to improve performance of my Edmx by using AutoDetectChanges","body":"<p>I have a reasonably large edmx generated from a database and I have been working on performance recently to improve my application I have read a number of articles in a variety of places some here some not</p>\n\n<p>this one on disabling auto detect of changes <a href=\"http://msdn.microsoft.com/en-us/data/jj556205.aspx\" rel=\"nofollow\">http://msdn.microsoft.com/en-us/data/jj556205.aspx</a> </p>\n\n<p>this one on improving performance on delete <a href=\"http://stackoverflow.com/questions/10103310/dbcontext-is-very-slow-when-adding-and-deleting\">DbContext is very slow when adding and deleting</a></p>\n\n<p>this one (which I think is pretty good) <a href=\"http://www.codeproject.com/Articles/38922/Performance-and-the-Entity-Framework\" rel=\"nofollow\">http://www.codeproject.com/Articles/38922/Performance-and-the-Entity-Framework</a></p>\n\n<p>I am already using myentities.tablename.MergeOption = MergeOption.NoTracking, i am using compiledqueries, I am pregenerated my View using EdmGen, I have reduced the data I am fetching etc..  and, of course,  I have gained performance in leaps and bounds so that a page that was loading in 54 seconds is now taking 16.1 seconds - however I have to get it to 3 seconds So I am still looking for the next improvement </p>\n\n<p>so the research is all well and great and as a result  I have upgraded to the latest EntityFramework, I have regenerated my .edmx from db etc... and tried a variety of things but I simply cannot find a myEntities.Configuration.AutoDetectChangesEnabled in order to set it to false. Now I must be missing a simple easy trick - how do I get my edmx to have this option.</p>\n\n<p>I am in this environment.Net 4.0.3, visual studio 2010, latest version of EntityFramework, MVC 4.0... All I need is somebody to say \"aha\" you need to go and do this....</p>\n\n<p>Currently if I delete 1000 records from one of my larger tables (134million rows) it takes nearly 10 minutes to savechanges. So from what I have read AutoDetectChangesEnabled is what I need to alter but it doesnt exist in my classes? where is it what must I do to get it?</p>\n\n<p>Any help appreciated I am trying to solve this one quickly</p>\n\n<p>Regards Julian</p>\n"},{"tags":["performance","assembly","x86"],"answer_count":0,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":68,"score":3,"question_id":13103680,"title":"Can a shift using the CL register result in a partial register stall?","body":"<p>Can a variable shift generate a partial register stall (or register recombining µops) on <code>ecx</code>? If so, on which microarchitecture(s)?</p>\n\n<p>I have tested this on Core2 (65nm), which seems to read only <code>cl</code>.</p>\n\n<pre><code>_shiftbench:\n    push rbx\n    mov edx, -10000000\n    mov ecx, 5\n  _shiftloop:\n    mov bl, 5   ; replace by cl to see possible recombining\n    shl eax, cl\n    add edx, 1\n    jnz _shiftloop\n    pop rbx\n    ret\n</code></pre>\n\n<p>Replacing <code>mov bl, 5</code> by <code>mov cl, 5</code> made no difference, which it would have if there was register recombining going on, as can be demonstrated by replacing <code>shl eax, cl</code> by <code>add eax, ecx</code> (in my tests the version with <code>add</code> experienced a 2.8x slowdown when writing to <code>cl</code> instead of <code>bl</code>).</p>\n\n<hr>\n\n<p>Test results:</p>\n\n<ul>\n<li>Merom: no stall observed</li>\n<li>Penryn: no stall observed</li>\n<li>Nehalem: no stall observed</li>\n</ul>\n"},{"tags":["mysql","performance","amazon-ec2","amazon-web-services","amazon-ebs"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":2520,"score":1,"question_id":5122889,"title":"Mysql and High CPU IO Wait","body":"<p>I've the following problem. I'm running a MySQL server 5.1.37 on Ubuntu 9.10 x86 on Amazon. For data store I use EBS volume formatted for ext3.</p>\n\n<p>From time to time the following problem occurs. MySQL start processing about queries 10~20 queries and processing of these takes more than 300sec (These SQL are using filesort). During that time no other transaction could be executed.</p>\n\n<p>I've checked CPU Wait and here what is shows:</p>\n\n<pre>\n----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--\nusr sys idl wai hiq siq| read  writ| recv  send|  in   out | int   csw \n24   9  66   0   0   0|   0    76k| 258k  989k|   0     0 |5970  3014 \n 23   1  75   0   0   1|4096B   28k| 229k 1536k|   0     0 |3249  2308 \n 19   6  74   0   0   0|4096B  316k| 209k  609k|   0     0 |4943  2542 \n 19  17  62   0   0   2|4096B   36k| 230k  718k|   0     0 |5482  2520 \n 21  19  57   2   0   2|  16k  800k| 271k  860k|   0     0 |6549  2923 \n 23  27  44   5   0   1| 480k   40k| 288k  979k|   0     0 |4140  2682 \n 12   0  86   1   0   0| 256k   48k| 237k  771k|   0     0 |3404  2627 \n 22   1  75   0   0   1|8192B   60k| 285k  908k|   0     0 |4009  2786 \n 54  21  19   3   0   2|4096B 3384k| 287k 1556k|   0     0 |3962  2284 \n 49  24  24   1   0   2|4096B  928k| 285k 2795k|   0     0 |3257  2005 \n 61  19  17   2   0   2|8192B   36k| 215k  577k|   0     0 |3246  1922 \n 40  49   8   0   0   3|   0    40k| 312k  905k|   0     0 |3282  1732 \n 56  23  20   1   0   1|4096B  188k| 247k  897k|   0     0 |3102  2238 \n 39  19  27  16   0   0|4096B   77M| 265k  819k|   0     0 |5147  3075 \n 35  35  12  16   0   1|4096B   56M| 259k 1052k|   0     0 |4656  2739 \n 36  27   8  28   0   1|4096B   59M| 259k 1139k|   0     0 |5549  2821 \n 27  13  36  23   0   1|4096B   64M| 251k 1218k|   0     0 |4207  2540\nusr sys idl wai hiq siq| read  writ| recv  send|  in   out | int   csw \n 26   4  13  57   0   1|4096B   66M| 275k  681k|   0     0 |5205  3291 \n 22   6  27  43   0   1|4096B   52M| 237k  684k|   0     0 |4906  2602 \n 14   3  24  58   0   0|4096B   46M| 278k 1058k|   0     0 |6448  3687 \n 19   3  34  43   0   2|  32k   51M| 233k  685k|   0     0 |5006  2652 \n 27   3   9  61   0   1|4096B   51M| 294k  800k|   0     0 |4428  2384 \n 17   3  30  50   0   1|4096B   42M| 243k  699k|   0     0 |5334  2830 \n 40  18   0  42   0   0|   0    89M| 247k  840k|   0     0 |4698  2977 \n 31  18  11  39   0   2|4096B   42M| 238k 1269k|   0     0 |4270  2474 \n 17   3  13  66   0   0|4096B   49M| 260k  773k|   0     0 |5153  3100 \n 21   2  14  62   0   1|8192B   46M| 269k  948k|   0     0 |6762  3581 \n 24   2  35  39   0   0|4096B   39M| 256k  777k|   0     0 |5313  2761 \n 15   2  10  72   0   1|4096B   49M| 237k  797k|   0     0 |5312  3018 \n 19   4  22  55   0   0|8192B   47M| 307k 1034k|   0     0 |5508  3278 \n 41   3  15  40   0   1|8192B   47M| 293k  727k|   0     0 |5630  3303 \n 16   2  26  54   0   1|4096B   56M| 282k 1750k|   0     0 |5016  2781 \n 17   3  12  67   0   2|8192B   43M| 238k  824k|   0     0 |5751  3147 \n 14  11  50  24   0   1|4096B   39M| 247k 1105k|   0     0 |4454  2389 \n 41   3  20  35   0   1|   0    58M| 152k  481k|   0     0 |4009  2958 \n 52   2   4  41   0   1|4096B   59M| 211k  621k|   0     0 |5449  2846 \n 31   2   0  66   0   1|   0    52M| 255k 1476k|   0     0 |5167  2693 \n 36   2  24  36   0   2|  12k   49M| 311k  888k|   0     0 |4537  2563 \n 47   7   2  43   0   2|4096B   50M| 231k  750k|   0     0 |4083  2165 \n 40   4   6  50   0   0|4096B   86M| 211k  819k|   0     0 |4768  2875 \n 29   5   2  65   0   0|   0    79M| 180k  580k|   0     0 |4271  4461 \n 40   3   0  57   0   0|4096B   58M| 238k 1489k|   0     0 |4366  4480 \n 27   8  26  38   0   1|4096B   33M| 301k  984k|   0     0 |4439  2838 \n 11   2   9  78   0   1|4096B   24M| 230k  646k|   0     0 |4894  4504 \n 10   3  14  72   0   0|4096B   21M| 183k  549k|   0     0 |4066  3952 \n 14   3  27  57   0   0|   0    64M| 147k  339k|   0     0 |3479  2860 \n 10   2  19  69   0   0|4096B   51M| 112k  452k|   0     0 |2847  2300 \n  9   4  18  69   0   0|4096B   37M| 131k  443k|   0     0 |2923  2004 \n  4   2  49  45   0   0|4096B   31M|  97k  230k|   0     0 |2163  1545 \n  1   2  73  24   0   0|   0    33M|  49k  130k|   0     0 |1425   824 \n  1   0  71  28   0   0|   0    26M|  36k   86k|   0     0 |1426   910 \n  0   0  55  45   0   0|   0    32M|  32k  148k|   0     0 |1334   695 \n  4   0  64  32   0   0|   0    39M|  14k   39k|   0     0 |1262   406 \n  0   2  38  60   0   0|   0    44M|  13k   44k|   0     0 |1136   382 \n  1   1  82  16   0   0|   0    47M|  25k   70k|   0     0 |1228   584 \n  1   3  69  27   0   0|4096B   46M|  23k   60k|   0     0 |1576   599 \n----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--\nusr sys idl wai hiq siq| read  writ| recv  send|  in   out | int   csw \n  3   1  70  27   0   0|4096B   43M|  22k   54k|   0     0 |1065   574 \n  1   1  33  65   0   0|   0    46M|6124B   17k|   0     0 |1190   345 \n  1   1  49  50   0   0|   0    47M|  11k   22k|   0     0 |1258   444 \n  2  11  23  64   0   0|  56k   58M|9749B   47k|   0     0 |1143   379 \n  1   1  64  34   0   0|   0    51M| 198B 5914B|   0     0 |1048   234 \n  0   1  63  36   0   0|   0    58M| 662B 1278B|   0     0 | 976   454 \n  1   0  81  18   0   0|   0    50M| 426B 6022B|   0     0 |1304   600 \n  0   1  70  29   0   0|   0    43M| 132B 1868B|   0     0 |1150   210 \n  1   1  79  19   0   0|   0    51M| 198B 5914B|   0     0 | 986   246 \n  1   2  30  66   0   0|   0    54M| 246B  420B|   0     0 |1150   288 \n  1   0  49  50   0   0|   0    55M| 659B 6752B|   0     0 |1038   280 \n  1   2  37  60   0   0|   0    47M|  66B  354B|   0     0 |1191   227 \n  0   0  80  19   0   0|   0    43M| 561B 6044B|   0     0 |1129   256 \n  5  13  44  38   0   0|   0    49M|1558B   19k|   0     0 |1225   243 \n  3   6  48  42   0   0|   0    52M| 705B 6022B|   0     0 | 948   327  \n</pre>\n\n<p>What could cause such a behavior? Are there any techniques to avoid this?</p>\n"},{"tags":["performance","json","r"],"answer_count":3,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":484,"score":4,"question_id":5185990,"title":"Efficient alternative to merge() when building dataframe from json files with R?","body":"<p>I have written the following code which works, but is painfully slow once I start executing it over thousands of records:</p>\n\n<pre><code>require(\"RJSONIO\")\npeople_data &lt;- data.frame(person_id=numeric(0))\n\njson_data &lt;- fromJSON(json_file)\nn_people &lt;- length(json_data)\nfor(person in 1:n_people) {\n        person_dataframe &lt;- as.data.frame(t(unlist(json_data[[person]])))\n        people_data &lt;- merge(people_data, person_dataframe, all=TRUE)\n    }\n\noutput_file &lt;- paste(\"people_data\",\".csv\")\nwrite.csv(people_data, file=output_file)\n</code></pre>\n\n<p>I am attempting to build a unified data table from a series of json-formated files. The <code>fromJSON()</code> function reads in the data as lists of lists. Each element of the list is a person, which then contains a list of the attributes for that person.</p>\n\n<p>For example:</p>\n\n<pre><code>[[1]]\n    person_id\n    name\n    gender\n    hair_color\n[[2]]\n    person_id\n    name\n    location\n    gender\n    height\n\n[[...]]\n\nstructure(list(person_id = \"Amy123\", name = \"Amy\", gender = \"F\",\n               hair_color = \"brown\"), \n          .Names = c(\"person_id\", \"name\", \"gender\", \"hair_color\"))\n\nstructure(list(person_id = \"matt53\", name = \"Matt\", \n               location = structure(c(47231, \"IN\"), \n                                    .Names = c(\"zip_code\", \"state\")), \n               gender = \"M\", height = 172), \n          .Names = c(\"person_id\", \"name\", \"location\", \"gender\", \"height\"))\n</code></pre>\n\n<p>The end result of the code above is matrix where the columns are every person-attribute that appears in the structure above, and the rows are the relevant values for each person. As you can see though, some data is missing for some of the people, so I need to ensure those show up as <code>NA</code> and make sure things end up in the right columns. Further, <code>location</code> itself is a vector with two components: <code>state</code> and <code>zip_code</code>, meaning it needs to be flattened to <code>location.state</code> and <code>location.zip_code</code> before it can be merged with another person record; this is what I use <code>unlist()</code> for. I then keep the running master table in <code>people_data</code>.</p>\n\n<p>The above code works, but do you know of a more efficient way to accomplish what I'm trying to do? It appears the <code>merge()</code> is slowing this to a crawl... I have hundreds of files with hundreds of people in each file.</p>\n\n<p>Thanks!\nBryan</p>\n\n<p>UPDATE:\nBased on the feedback below, I tried to build a list of all the people, and then convert it all at one time into a dataframe. I let it run overnight and still didn't finish making the dataframe. There are around 1/2 million people in the list. That codes looks like this:</p>\n\n<pre><code>require(\"RJSONIO\")\nrequire(\"plyr\")\npeople_data &lt;- data.frame(person_id=numeric(0))\npeople_list &lt;- list()\n\njson_data &lt;- fromJSON(json_file)\nn_people &lt;- length(json_data)\nfor(person in 1:n_people) {\n        people_list[[person]] &lt;- t(unlist(json_data[[person]]))\n    }\n\n#PROBLEM CODE, SLOW, 1/2 million records in people_list\npeople_data &lt;- rbind.fill(lapply(people_list, as.data.frame))\n\noutput_file &lt;- paste(\"people_data\",\".csv\")\nwrite.csv(people_data, file=output_file)\n</code></pre>\n"},{"tags":["java","performance","opengl","theory","graphics2d"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":34,"score":1,"question_id":13092142,"title":"Adjusting repaints in JComponent to improve performance","body":"<p>I am trying to do some tests on 2D \"game\"-programming by trying out different concepts to approach both designing and visualizing the environment.</p>\n\n<p>In my baby-steps I went to go with Swing and using a JComponent's paintComponent() method to retrieve a Graphics2D object that I then use to visualize my game board.</p>\n\n<p>It works quite well but I got to the point where I need to repeatedly check my whole game-model and update the view, basically once every 1/10 second something can change.</p>\n\n<p>I paint the visuals by calling repaint() on my JComponent to cause a complete update of the view: I check every tile my game board has for information and paint that tile according to this data, for every tile on the board. But as I approach about 1000 - 4000 tiles that need to be painted, I come to the point where the painting of the whole view takes more than 100ms and thus a constant lag occurs when doing anything.</p>\n\n<p>Now for the question(s): I am looking for a way, or opinions, on how to improve the performance of this approach. As not every tile on the board changes every \"tick\" I do not need to \"repaint\" this tile. But on the contrary, moving the visual area (camera offset) changes the position of every tile on the screen, so it would need to be repainted at a different position. Also, the later implementation of \"would be\" animations would need a constant update of the visual area, regardless of \"happenings\" or not. When looking at 3D games with high quality graphics (or even simple ones like minecraft) running at > 30 FPS, I am wondering weather or not I should immediately switch to OpenGL before running into even more problems graphics wise, or are there ways to improve the performance with algorithms checking for the right kind of changes both in the view and the model?</p>\n"},{"tags":["c++","performance","c++11","const"],"answer_count":3,"favorite_count":1,"up_vote_count":13,"down_vote_count":0,"view_count":310,"score":13,"question_id":13099942,"title":"Should I still return const objects in C++11?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/12051012/should-i-return-const-objects\">Should I return const objects?</a><br>\n  (The original title of that question was: <strong>int foo() or const int foo()?</strong> explaining why I missed it.)</p>\n</blockquote>\n\n<hr>\n\n<p>Effective C++, Item 3: Use const whenever possible. In particular, returning const objects is promoted to avoid unintended assignment like <code>if (a*b = c) {</code>. I find it a little paranoid, nevertheless I have been following this advice.</p>\n\n<p>It seems to me that returning const objects can degrade performance in C++11.</p>\n\n<pre class=\"lang-cpp prettyprint-override\"><code>#include &lt;iostream&gt;\nusing namespace std;\n\nclass C {\npublic:\n    C() : v(nullptr) { }\n\n    C&amp; operator=(const C&amp; other) {\n        cout &lt;&lt; \"copy\" &lt;&lt; endl;\n        // copy contents of v[]\n        return *this;\n    }\n\n    C&amp; operator=(C&amp;&amp; other) {\n        cout &lt;&lt; \"move\" &lt;&lt; endl;\n        v = other.v, other.v = nullptr;\n        return *this;\n    }\n\nprivate:\n    int* v;\n};\n\nconst C const_is_returned() { return C(); }\n\nC nonconst_is_returned() { return C(); }\n\nint main(int argc, char* argv[]) {\n    C c;\n    c = const_is_returned();\n    c = nonconst_is_returned();\n    return 0;\n}\n</code></pre>\n\n<p>This prints:</p>\n\n<pre><code>copy\nmove\n</code></pre>\n\n<p>Do I implement the move assignment correctly? Or I simply shouldn't return const objects anymore in C++11?</p>\n"},{"tags":["javascript","jquery","performance"],"answer_count":5,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":325,"score":6,"question_id":12978069,"title":"Poor JS/jQuery performance, particularly in IE9 and Firefox","body":"<p>I'm having serious visual &amp; performance issues with the script below. The biggest problem is that the animation of the object is becoming really jerky, almost cripplingly so in IE9, but increasingly annoying in Firefox. </p>\n\n<p>It has been pretty fast until recently - but I'm concerned the complexity is slowing things down. Oddly the <a href=\"http://www.webkit.org/perf/sunspider/sunspider.html\">Sunspider benchmark</a> runs faster in my IE9 instance, than in Firefox.</p>\n\n<p>The script (which is a snippet of a larger collection <em>*</em>):</p>\n\n<ol>\n<li>Checks a HTML5 session storage log of the users progression through\nthe game. </li>\n<li>Depending on the stage, animates an object between two\npoints using <a href=\"https://github.com/MmmCurry/jquery.crSpline\">crSpline</a>. </li>\n<li>Ensures the browser window follows the object\nacross a large canvas, via scrollLeft etc. </li>\n<li>Finally, it loads a popup window via <a href=\"http://www.jacklmoore.com/colorbox\">colorbox</a>. </li>\n<li>When this box is closed, the user progression log is incremented accordingly and the object moves again.</li>\n</ol>\n\n<p>Are there aby obvious speed improvments I could make to my code? There's a fair bit of repition, how can I reduce that? Are there any infinite loops running that I'm missing? Is there software I can use to profile slow points of the JS?</p>\n\n<p><em>*</em> (I can't provide the other JS files or HTML, but I have identified this script as the problem)</p>\n\n<hr>\n\n<p><strong>UPDATE:</strong>\nAfter a fair bit more testing, it appears that the step animate function - which follows the object in the window via scrollLeft - is causing the jerky animation. Removing it improves things considerably.</p>\n\n<p>This isn't a viable long term solution however. A quick fix is to call the follow function on complete, but this is a much less smooth experience for the end user, especially when the object moves longer distances.</p>\n\n<p><strong>So, how would I modify the step function to run a lot 'slower'/more efficiently?</strong> I'm guessing the jerkiness is caused by it using all the available resources to follow the object every millisecond.</p>\n\n<pre><code>(function ($) {\n\n  sessionStorage.gameMainStage = 0 \n\n  moveShip =  function() {\n\n    switch (sessionStorage.gameMainStage)\n\n{\n  case '1':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[715, 425], [582, 524], [556, 646], [722, 688], [963, 629], [1143, 467]]) },{\n      duration: 10000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n          },\n          complete: function() {\n            $.colorbox({href:\"dialog-1.html\", width:\"737px\", height:\"474px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n      }\n    );\n    break;\n\n  case '2':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[1143, 467], [1343, 667], [1443, 367],  [1243, 167], [1499, 285]]) },\n        {\n          duration: 5000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n          },\n          complete: function() {\n            $.colorbox({href:\"dialog-2\", width:\"737px\", height:\"547px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n\n        }\n    );\n    break;\n\n  case '3':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[1499, 285], [1922, 423]]) },\n        {\n          duration: 5000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n          },\n          complete: function() {\n            $.colorbox({href:\"dialog-3.html\", width:\"737px\", height:\"547px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n\n        }\n    );\n    break;  \n\n  case '4':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[1922, 423], [2216, 578]]) },{\n        duration: 5000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n            }, \n\n          complete: function() {\n            $.colorbox({href:\"game-1.html\", width:\"737px\", height:\"547px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n        }\n    );\n    break;\n\n  case '5':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[2216, 578], [2769, 904]]) },{\n      duration: 5000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n          }, \n\n          complete: function() {\n            $.colorbox({href:\"dialog-4.html\", width:\"737px\", height:\"547px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n      }\n    );\n    break;\n\n  case '6':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[2769, 904], [3263, 903]]) },{\n      duration: 5000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n          }, \n\n          complete: function() {\n            $.colorbox({href:\"dialog-5.html\", width:\"737px\", height:\"547px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n      }\n    );\n    break;\n\n  case '7':\n    $.colorbox({href:\"game-2.html\", width:\"500px\", height:\"600px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n  break;\n\n  case '8':\n    $.colorbox({href:\"dialog-6.html\", width:\"737px\", height:\"567px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n  break;\n\n  case '9':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[3263, 903], [4141, 820]]) },{\n      duration: 5000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n          }, \n\n          complete: function() {\n            $.colorbox({href:\"dialog-7.html\", width:\"737px\", height:\"547px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n      }\n    );\n    break;\n\n  case '10':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[4141, 820], [4568, 949], [4447, 1175]]) },{\n      duration: 5000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n          }, \n\n          complete: function() {\n            $.colorbox({href:\"dialog-8.html\", width:\"737px\", height:\"434px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n      }\n    );\n    break;\n\n  case '11':\n    $.colorbox({href:\"dialog-9.html\", width:\"737px\", height:\"567px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n  break;\n\n  case '12':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[4447, 1175], [4701, 1124], [4816, 822]]) },{\n      duration: 5000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n          }, \n\n          complete: function() {\n            $.colorbox({href:\"dialog-10.html\", width:\"900px\", height:\"687px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n      }\n    );\n    break;\n}\n\n};\n\n})(jQuery);\n</code></pre>\n"},{"tags":["c","performance","gcc","g++"],"answer_count":2,"favorite_count":1,"up_vote_count":9,"down_vote_count":2,"view_count":92,"score":7,"question_id":13104178,"title":"Program compiled by gcc runs faster than compiled by g++","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/3302657/performance-difference-between-gcc-and-g-for-c-program\">Performance difference between gcc and g++ for C program</a>  </p>\n</blockquote>\n\n\n\n<p>I was checking the improvement of performance with using register storage specifier for the loop control variable when I accidentally noticed that program compiled with gcc runs faster than compiled with g++. Can someone explain it to me?</p>\n\n<p>Here is the code:</p>\n\n<pre><code>#include &lt;stdio.h&gt;\n\nconst unsigned long scope = 1000000000;\n\nint main()\n{\n    register unsigned long i;\n    for (i=0; i &lt; scope; i++);\n    return 0;\n}\n</code></pre>\n\n<p>;</p>\n\n<pre><code>gcc register.c\ntime ./a.out   \nreal    0m0.466s\nuser    0m0.468s\nsys     0m0.000s\n\ng++ register.c\ntime ./a.out \nreal    0m0.923s\nuser    0m0.920s\nsys     0m0.000s\n</code></pre>\n"},{"tags":["mysql","sql","performance","optimization","mariadb"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":78,"score":2,"question_id":13103850,"title":"Why is my MySQL group by so slow?","body":"<p>I am trying to query against a partitioned table (by month) approaching 20M rows.  I need to group by DATE(transaction_utc) as well as country_id.  The rows that get returned if i turn off the group by and aggregates is just over 40k, which isn't too many, however adding the group by makes the query substantially slower unless said GROUP BY is on the transaction_utc column, in which case it gets FAST. </p>\n\n<p>I've been trying to optimize this first query below by tweaking the query and/or the indexes, and got to the point below (about 2x as fast as initially) however still stuck with a 5s query for summarizing 45k rows, which seems way too much.</p>\n\n<p>For reference, this box is a brand new 24 logical core, 64GB RAM, Mariadb-5.5.x server with way more INNODB buffer pool available than index space on the server, so shouldn't be any RAM or CPU pressures.</p>\n\n<p>So, I'm looking for ideas on what is causing this slow down and suggestions on speeding it up.  Any feedback would be greatly appreciated!  :) </p>\n\n<p>Ok, onto the details...</p>\n\n<p>The following query (the one I actually need) takes approx 5 seconds (+/-), and returns less than 100 rows.   </p>\n\n<pre><code>SELECT lss.`country_id` AS CountryId\n, Date(lss.`transaction_utc`) AS TransactionDate\n, c.`name` AS CountryName,  lss.`country_id` AS CountryId\n, COALESCE(SUM(lss.`sale_usd`),0) AS SaleUSD\n, COALESCE(SUM(lss.`commission_usd`),0) AS CommissionUSD  \nFROM `sales` lss  \nJOIN `countries` c ON lss.`country_id` = c.`country_id`  \nWHERE ( lss.`transaction_utc` BETWEEN '2012-09-26' AND '2012-10-26' AND lss.`username` = 'someuser' )  GROUP BY lss.`country_id`, DATE(lss.`transaction_utc`)\n</code></pre>\n\n<p>EXPLAIN SELECT for the same query is as follows.  Notice that it's not using the transaction_utc key.  Shouldn't it be using my covering index instead?</p>\n\n<pre><code>id  select_type table   type    possible_keys   key key_len ref rows    Extra\n1   SIMPLE  lss ref idx_unique,transaction_utc,country_id   idx_unique  50  const   1208802 Using where; Using temporary; Using filesort\n1   SIMPLE  c   eq_ref  PRIMARY PRIMARY 4   georiot.lss.country_id  1   \n</code></pre>\n\n<p>Now onto a couple other options that I've tried to attempt to determine whats going on...</p>\n\n<p>The following query (changed group by) takes about 5 seconds (+/-), and returns only 3 rows:</p>\n\n<pre><code>SELECT lss.`country_id` AS CountryId\n, DATE(lss.`transaction_utc`) AS TransactionDate\n, c.`name` AS CountryName,  lss.`country_id` AS CountryId\n, COALESCE(SUM(lss.`sale_usd`),0) AS SaleUSD\n, COALESCE(SUM(lss.`commission_usd`),0) AS CommissionUSD  \nFROM `sales` lss  \nJOIN `countries` c ON lss.`country_id` = c.`country_id`  \nWHERE ( lss.`transaction_utc` BETWEEN '2012-09-26' AND '2012-10-26' AND lss.`username` = 'someuser' )  GROUP BY lss.`country_id`\n</code></pre>\n\n<p>The following query (removed group by) takes 4-5 seconds (+/-) and returns 1 row:</p>\n\n<pre><code>SELECT lss.`country_id` AS CountryId\n    , DATE(lss.`transaction_utc`) AS TransactionDate\n    , c.`name` AS CountryName,  lss.`country_id` AS CountryId\n    , COALESCE(SUM(lss.`sale_usd`),0) AS SaleUSD\n    , COALESCE(SUM(lss.`commission_usd`),0) AS CommissionUSD  \n    FROM `sales` lss  \n    JOIN `countries` c ON lss.`country_id` = c.`country_id`  \n    WHERE ( lss.`transaction_utc` BETWEEN '2012-09-26' AND '2012-10-26' AND lss.`username` = 'someuser' )\n</code></pre>\n\n<p>The following query takes .00X seconds (+/-) and returns ~45k rows.  This to me shows that at max we're only trying to group 45K rows into less than 100 groups (as in my initial query):</p>\n\n<pre><code>SELECT lss.`country_id` AS CountryId\n    , DATE(lss.`transaction_utc`) AS TransactionDate\n    , c.`name` AS CountryName,  lss.`country_id` AS CountryId\n    , COALESCE(SUM(lss.`sale_usd`),0) AS SaleUSD\n    , COALESCE(SUM(lss.`commission_usd`),0) AS CommissionUSD  \n    FROM `sales` lss  \n    JOIN `countries` c ON lss.`country_id` = c.`country_id`  \n    WHERE ( lss.`transaction_utc` BETWEEN '2012-09-26' AND '2012-10-26' AND lss.`username` = 'someuser' )\nGROUP BY lss.`transaction_utc`\n</code></pre>\n\n<p>TABLE SCHEMA:</p>\n\n<pre><code>CREATE TABLE IF NOT EXISTS `sales` (\n  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,\n  `user_linkshare_account_id` int(11) unsigned NOT NULL,\n  `username` varchar(16) NOT NULL,\n  `country_id` int(4) unsigned NOT NULL,\n  `order` varchar(16) NOT NULL,\n  `raw_tracking_code` varchar(255) DEFAULT NULL,\n  `transaction_utc` datetime NOT NULL,\n  `processed_utc` datetime NOT NULL ,\n  `sku` varchar(16) NOT NULL,\n  `sale_original` decimal(10,4) NOT NULL,\n  `sale_usd` decimal(10,4) NOT NULL,\n  `quantity` int(11) NOT NULL,\n  `commission_original` decimal(10,4) NOT NULL,\n  `commission_usd` decimal(10,4) NOT NULL,\n  `original_currency` char(3) NOT NULL,\n  PRIMARY KEY (`id`,`transaction_utc`),\n  UNIQUE KEY `idx_unique` (`username`,`order`,`processed_utc`,`sku`,`transaction_utc`),\n  KEY `raw_tracking_code` (`raw_tracking_code`),\n  KEY `idx_usd_amounts` (`sale_usd`,`commission_usd`),\n  KEY `idx_countries` (`country_id`),\n  KEY `transaction_utc` (`transaction_utc`,`username`,`country_id`,`sale_usd`,`commission_usd`)\n) ENGINE=InnoDB  DEFAULT CHARSET=utf8\n/*!50100 PARTITION BY RANGE ( TO_DAYS(`transaction_utc`))\n(PARTITION pOLD VALUES LESS THAN (735112) ENGINE = InnoDB,\n PARTITION p201209 VALUES LESS THAN (735142) ENGINE = InnoDB,\n PARTITION p201210 VALUES LESS THAN (735173) ENGINE = InnoDB,\n PARTITION p201211 VALUES LESS THAN (735203) ENGINE = InnoDB,\n PARTITION p201212 VALUES LESS THAN (735234) ENGINE = InnoDB,\n PARTITION pMAX VALUES LESS THAN MAXVALUE ENGINE = InnoDB) */ AUTO_INCREMENT=19696320 ;\n</code></pre>\n"},{"tags":["ruby-on-rails-3","performance"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":212,"score":2,"question_id":12892937,"title":"Improve Rails loading time","body":"<p>This is a bit of a follow-up from <a href=\"http://stackoverflow.com/q/4736546/305019\">a previous question on improving rails console loading time</a>.</p>\n\n<p>The first great suggestion was to figure out <a href=\"http://stackoverflow.com/a/5071198/305019\">which gems take too long</a>.</p>\n\n<p>Next answer, suggested using <code>:require =&gt; nil</code> and <a href=\"http://stackoverflow.com/a/7146810/305019\">loading those gems later</a>.</p>\n\n<p>With some gems however, it's not entirely clear how to accomplish this without breaking things. Here's a list of our 'biggest offenders', I wonder if someone can suggest the best approach to loading them only when necessary?</p>\n\n<pre><code>require gon: 2.730000 (2.870059)\nrequire omniauth-openid: 1.410000 (1.503858)\nrequire cancan: 2.640000 (2.707467)\nrequire fog: 2.730000 (2.846530)\nrequire activeadmin: 3.650000 (3.923877)\n</code></pre>\n\n<p>and of course there are many more that take around 1 second or less, which also adds up... but at least removing the big ones will already improve things.</p>\n\n<h3>how can I selectively load gems later to make rails load faster?</h3>\n"},{"tags":["hash","random","performance"],"answer_count":5,"favorite_count":1,"up_vote_count":9,"down_vote_count":1,"view_count":3474,"score":8,"question_id":2575787,"title":"Looking for a fast hash-function","body":"<p>I'm looking for a special hash-function. Let's say I have a large list of strings, if I order them by their hash-values they should be ordered quasi randomly. </p>\n\n<p>The most important point is: it must be super fast. I've tried md5 and sha1 and they're using to much cpu power.</p>\n\n<p>Clashes are not a problem.</p>\n\n<p>I'm using javascript, so it shouldn't be too complicated to implement.</p>\n"},{"tags":["performance","caching","memory-management","operating-system"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":13091453,"title":"System/OS Caching vs. Application Caching","body":"<p>When developing applications that work with compressed on-disk indexes or on-disk files where parts of the index or the file are accessed repetitively (for arguments sake, let's say with something akin to a Zipfian  distribution), I wonder when is it sufficient/better to rely on OS-level caching (e.g., memory mapping on say a Debian system), and when is it better to implement something on the application layer (e.g., something like <a href=\"http://docs.oracle.com/javase/1.5.0/docs/api/java/nio/channels/FileChannel.html\" rel=\"nofollow\">FileChannel</a> buffering or Memcached or a custom LRU-cache in Java code).</p>\n\n<p>For example, <a href=\"http://juanggrande.wordpress.com/2011/01/05/os-cache-do-matter/\" rel=\"nofollow\">one article</a> (in reference to Solr) argues for leaving memory free for OS-caching:</p>\n\n<blockquote>\n  <p>The OS’s cache is really useful, it decreases significantly the time required to answer a query (even after completely restarting the server!), so always remember to keep some memory free for the OS.</p>\n</blockquote>\n\n<p>This got me wondering whether or not my application-level cache that fills memory with weak maps to LRU Java objects is doing more harm than good, esp. since Java is so greedy in terms of memory overhead ... instead of using that memory to cache a few final result objects, would that space be better used by the OS to cache lots of raw compressed data? On the other hand, the application layer cache would be better for platform independence, allowing for caching no matter what OS the code was running on. </p>\n\n<p>And so I realised that I had no idea how to go about answering that question in a principled way, other than running a couple of specific benchmarks. Which leads me to ask ...</p>\n\n<p><strong>What general guidelines exist for whether to assign available memory for application-level caching, or to leave that memory available for OS-level caching?</strong></p>\n\n<p>In particular, I'd love to be able to better recognise when coding an application-level cache is a waste of time, or even harmful for performance.</p>\n"},{"tags":["php","string","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":3,"view_count":104,"score":0,"question_id":8773270,"title":"Performance overhead for PHP string concatenation?","body":"<p>I'm starting a new project shortly and going to use coding standards. I've always written SQL statements like this:</p>\n\n<pre><code>$sql = sprintf(\"INSERT INTO users (name) VALUES ('%s')\", $name);\n</code></pre>\n\n<p>I'm wondering if there is any performance gained by using one of these:</p>\n\n<pre><code>$sql = \"INSERT INTO users (name) VALUES ('\".$name.\"')\";\n$sql = \"INSERT INTO users (name) VALUES ('$name')\";\n</code></pre>\n\n<p><strong>Also</strong>: Does this performance difference fluctuate with the addition of more \"parameters\" (as in the case of the first line of code) ?</p>\n\n<p>Thanks.</p>\n"},{"tags":["c#",".net","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":9,"down_vote_count":0,"view_count":227,"score":9,"question_id":13054708,"title":"What is the best way to route paths in a large grid?","body":"<p>I'm working on an algorithm to find a set of non intersected paths in a grid for a \ngiven pairs of points..\nLike this for these pairs:\n(9,4) and (12,13)\n<img src=\"http://i.stack.imgur.com/3ZW1M.png\" alt=\"Sample Grid\"></p>\n\n<p>The output should be something like this:</p>\n\n<pre><code>    9,10,11,7,3,4\n\n    13,14,15,16,12\n</code></pre>\n\n<p>and print \"Blocked\" if it can't route all paths</p>\n\n<p>First I searched for an already made algorithm to find all simple paths between 2 \npoints in a graph or a grid. and I found this one by @Casey Watson and @svick <a href=\"http://stackoverflow.com/questions/58306/graph-algorithm-to-find-all-connections-between-two-arbitrary-vertices\">here</a>.. \nIt works really well but for small graphs only.</p>\n\n<p>I converted it to C#.NET and enhanced it a little bit to be able to find paths of \nmaximum length X. and build on it my total algorithm.</p>\n\n<p>The one I built works fine in small graphs..\nHere is routes 9 pairs in a 8x8 grid..\n<img src=\"http://i.stack.imgur.com/i3WJK.png\" alt=\"enter image description here\"></p>\n\n<p>but it takes a huge time in larger ones like the 16x16 or even the final one I intended to do which is a 3D model of 16x16x2\nLike this</p>\n\n<p><img src=\"http://i.stack.imgur.com/vR484.png\" alt=\"8x8x2 grid\"></p>\n\n<p>The algorithm was developed to be a depth first search <strong>RECURSIVE</strong> algorithm, but it \ntook a huge time to return value to the user. so I decided to convert it to loops instead of the recursive calls so that I can benefit from <strong>yield return</strong> feature in .NET\nbut still it didn't help any better.</p>\n\n<p>The loops version of the algorithm find a route for a pair of points in less than a second but the recursive one took more than 90 seconds.</p>\n\n<p><img src=\"http://i.stack.imgur.com/lQT7O.png\" alt=\"enter image description here\"></p>\n\n<p>when I tried with 2 pairs, the loops version took around 342 seconds but the recursive one took around 200..</p>\n\n<p><img src=\"http://i.stack.imgur.com/OBM5M.png\" alt=\"enter image description here\"></p>\n\n<p>So I can't know which is faster..!? the recursive or the loops one..</p>\n\n<p>I really want to know the best way to do this..</p>\n\n<p>Note : the first digit in the number of the node determine the layer (Starts at 1)..</p>\n\n<p>Here is the code</p>\n\n<pre><code>    using System;\n    using System.Collections;\n    using System.Collections.Generic;\n    using System.Diagnostics;\n    using System.IO;\n    using System.Linq;\n\n    namespace AlgorithmTest\n    {\n     struct Connection\n    {\n    public int FirstNode;\n    public int SecondNode;\n\n    public Connection(int N1,int N2)\n    {\n        FirstNode = N1;\n        SecondNode = N2;\n    }\n}\nenum Algorithm\n{ Recursion, Loops }\n\npublic class Search\n{\n\n    private const int MAX = 15;\n\n    private const int Width = 16;\n    private const int Length = 16;\n    private const int Height = 2;\n\n\n\n    private static void Main(string[] args)\n    {\n\n\n        var graph = new Graph();\n\n\n        var str = new int[Height,Length, Width];\n        var level = ((int)Math.Pow(10, (Length * Width).ToString().Length) &gt;= 100) ? (int)Math.Pow(10, (Length * Width).ToString().Length) : 100;              \n        for (var i = 0; i &lt; Height; i++)\n        {\n            int num = 0;\n            for (var j = 0; j &lt; Length; j++)\n                for (var k = 0; k &lt; Width; k++)\n            {\n                str[i, j, k] = ++num + level;\n\n            }\n            level += level;\n        }\n\n\n        for (var i = 0; i &lt; Height; i++)\n        {\n            for (var j = 0; j &lt; Length; j++)\n            {\n                for (var k = 0; k &lt; Width; k++)\n                {\n\n                    if (i &lt; Height - 1) graph.addEdge(str[i, j, k], str[i + 1, j, k]);\n                    if (i &gt; 0) graph.addEdge(str[i, j, k], str[i - 1, j, k]);\n\n                    if (k &lt; Width - 1) graph.addEdge(str[i, j, k], str[i, j, k + 1]);\n                    if (k &gt; 0) graph.addEdge(str[i, j, k], str[i, j, k - 1]);\n\n                    if (j &lt; Length - 1) graph.addEdge(str[i, j, k], str[i, j + 1, k]);\n                    if (j &gt; 0) graph.addEdge(str[i, j, k], str[i, j - 1, k]);\n\n\n                }\n            }\n        }\n\n\n\n        var wt = new Stopwatch();\n\n       wt.Start();\n        var connectedNodes = new List&lt;Connection&gt;()\n                                 {\n\n\n\n                                     new Connection(1030, 1005),\n       //                              new Connection(1002, 1044),\n    //                                         new Connection(1015, 1064),\n    //                                        new Connection(1041, 1038),\n    //                                         new Connection(1009, 1027),\n    //                                         new Connection(1025, 1018),\n    //                                         new Connection(1037, 1054),\n    //                                         new Connection(1049, 1060),\n    //                                         new Connection(1008, 1031),\n    //                                         new Connection(1001, 1035),\n\n                                 };\n        wt.Start();\n        Console.WriteLine(\"Using Loops:\");\n        Console.WriteLine();\n        var allPaths = new Search().FindAllPaths(connectedNodes, graph, MAX, Algorithm.Loops);\n        wt.Stop();\n        foreach (var path in allPaths)\n        {\n            PrintPath(path);\n        }\n        Console.WriteLine(\"Total Seconds: \" + wt.Elapsed.TotalSeconds + \", Number of paths: \" + allPaths.Count());\n        Console.WriteLine(\"***************************************************************************************************\");\n        Console.WriteLine(\"Using Recursion:\");\n        Console.WriteLine();\n        wt.Reset();\n        wt.Start();\n        allPaths = new Search().FindAllPaths(connectedNodes, graph, MAX, Algorithm.Recursion);\n        wt.Stop();\n        foreach (var path in allPaths)\n        {\n            PrintPath(path);\n        }\n        Console.WriteLine(\"Total Seconds: \" + wt.Elapsed.TotalSeconds + \", Number of paths: \" + allPaths.Count());\n        Console.WriteLine();\n\n    }\n\n    private IEnumerable&lt;List&lt;int&gt;&gt; FindAllPaths(List&lt;Connection&gt; connectedNodes, Graph graph, int max, Algorithm algorithm)\n    {\n        var paths=new Stack&lt;List&lt;int&gt;&gt;();\n        var blocked=new List&lt;int&gt;();\n\n        for (var i = 0; i &lt; connectedNodes.Count; i++)\n        {\n            if (!blocked.Contains(connectedNodes[i].FirstNode)) blocked.Add(connectedNodes[i].FirstNode);\n            if (!blocked.Contains(connectedNodes[i].SecondNode)) blocked.Add(connectedNodes[i].SecondNode);\n        }\n\n        if (algorithm == Algorithm.Recursion)\n        {\n            if (FindAllPaths(connectedNodes, 0, max, graph, paths, blocked))\n            {\n                Console.WriteLine(\"BLOCKED\");\n                return new List&lt;List&lt;int&gt;&gt;();\n            }\n        }\n        else if(algorithm==Algorithm.Loops)\n        {\n            if (!FindAllPaths2(connectedNodes, 0, max, graph, paths, blocked))\n            {\n                Console.WriteLine(\"BLOCKED\");\n                return new List&lt;List&lt;int&gt;&gt;();\n            }\n        }\n\n        return paths;\n\n    }\n    private static bool FindAllPaths(List&lt;Connection&gt; connectedNodes,int order,int max, Graph graph, Stack&lt;List&lt;int&gt;&gt; allPaths, List&lt;int&gt; blocked)\n    {\n\n        if (order &gt;= connectedNodes.Count) return false;\n\n\n        var paths = SearchForPaths(graph, connectedNodes[order].FirstNode, connectedNodes[order].SecondNode, max, blocked);\n        if (paths.Count == 0) return true;\n        int i;\n        for (i = 0; i &lt; paths.Count; i++)\n        {\n            var path = paths[i];\n            allPaths.Push(path);\n            blocked.AddRange(path);\n\n\n            if (!FindAllPaths(connectedNodes, order + 1,max, graph, allPaths, blocked)) break;\n\n            allPaths.Pop();\n            foreach (var j in path)\n            {\n                blocked.RemoveAll(num =&gt; num==j);\n            }\n\n            paths.RemoveAll(list =&gt; IsListsSimilar(list,path));\n\n            i--;\n\n        }\n        if (i == paths.Count) return true;\n\n\n        return false;\n\n    }\n\n    private static bool IsListsSimilar(List&lt;int&gt; L1,List&lt;int&gt; L2)\n    {\n        if (L2.Count &gt; L1.Count) return false;\n\n        for (int i = 0; i &lt; L2.Count - 1; i++)\n        {\n            if (L1[i] != L2[i]) return false;\n        }\n        return true;\n    }\n\n    private static List&lt;List&lt;int&gt;&gt; SearchForPaths(Graph graph, int start, int end, int max, List&lt;int&gt; blocked)\n    {\n        blocked.Remove(start);\n        blocked.Remove(end);\n\n\n\n\n        var nodePaths = new List&lt;List&lt;int&gt;&gt;();\n        var visited = new LinkedList&lt;int&gt;();\n        visited.AddLast(start);\n        DepthFirstSearch(graph, visited, end, max, blocked, nodePaths);\n\n\n\n        nodePaths = nodePaths.OrderBy(list =&gt; list.Count).ToList();\n\n        return nodePaths;\n\n    }\n    private static void DepthFirstSearch(Graph graph, LinkedList&lt;int&gt; visited, int end, int max, List&lt;int&gt; blocked, List&lt;List&lt;int&gt;&gt; paths)\n    {\n        var nodes = graph.adjacentNodes(visited.Last.Value);\n        // examine adjacent nodes\n        var nodeCount = blocked.Count;\n        for (int i = 0; i &lt; nodeCount; i++)\n        {\n            if (visited.Contains(blocked[i])) return;\n        }\n\n        if (visited.Count &gt; max) return;\n\n\n        nodeCount = nodes.Count;\n        for (var i = 0; i &lt; nodeCount; i++)\n        {\n            if (visited.Contains(nodes[i]) || nodes[i] != end) continue;\n\n            visited.AddLast(nodes[i]);\n\n            {\n                paths.Add(new List&lt;int&gt;(visited));\n\n            }\n            visited.RemoveLast();\n            break;\n        }\n\n\n\n        nodeCount = nodes.Count;\n        for (var i = 0; i &lt; nodeCount; i++)\n        {\n            if (visited.Contains(nodes[i]) || nodes[i] == end) continue;\n\n            visited.AddLast(nodes[i]);\n            DepthFirstSearch(graph, visited, end, max, blocked, paths);\n            visited.RemoveLast();\n        }\n\n    }\n\n    private static bool FindAllPaths2(List&lt;Connection&gt; connectedNodes, int order, int max, Graph graph, Stack&lt;List&lt;int&gt;&gt; allPaths, List&lt;int&gt; blocked)\n    {\n\n        if (order &gt;= connectedNodes.Count) return false;\n\n\n        foreach (var path in SearchForPaths2(graph, connectedNodes[order].FirstNode, connectedNodes[order].SecondNode, max, blocked))\n        {\n\n            allPaths.Push(path);\n            blocked.AddRange(path);\n\n\n            if (!FindAllPaths2(connectedNodes, order + 1, max, graph, allPaths, blocked)) break;\n\n            allPaths.Pop();\n            foreach (var j in path)\n            {\n                blocked.RemoveAll(num =&gt; num == j);\n            }\n\n\n        }\n\n\n\n\n        return true;\n\n    }\n    private static IEnumerable&lt;List&lt;int&gt;&gt; SearchForPaths2(Graph graph, int start, int end, int max, List&lt;int&gt; blocked)\n    {\n        blocked.Remove(start);\n        blocked.Remove(end);\n\n\n        var visited = new LinkedList&lt;int&gt;();\n        visited.AddLast(start);\n        foreach (var VARIABLE in DepthFirstSearch(graph, visited, end, max, blocked))\n        {\n            yield return VARIABLE;\n        }\n\n    }\n    private static IEnumerable&lt;List&lt;int&gt;&gt; DepthFirstSearch(Graph graph, LinkedList&lt;int&gt; visited, int end, int max, List&lt;int&gt; blocked)\n    {\n\n\n\n\n\n        var nodes = graph.adjacentNodes(visited.Last.Value);\n\n\n        var nodeCount = blocked.Count;\n        for (int i = 0; i &lt; nodeCount; i++)\n        {\n            if (visited.Contains(blocked[i])) yield break;\n        }\n\n\n        if (visited.Count &gt; max) yield break;\n\n        nodeCount = nodes.Count;\n        for (var i = 0; i &lt; nodeCount; i++)\n        {\n            if (visited.Contains(nodes[i]) || nodes[i] != end) continue;\n\n            visited.AddLast(nodes[i]);\n\n            yield return (new List&lt;int&gt;(visited));\n            visited.RemoveLast();\n            break;\n        }\n\n\n\n\n        nodeCount = nodes.Count;\n        for (var i = 0; i &lt; nodeCount; i++)\n        {\n            if (visited.Contains(nodes[i]) || nodes[i] == end) continue;\n\n            visited.AddLast(nodes[i]);\n            foreach (var P in DepthFirstSearch(graph, visited, end, max, blocked))\n            {\n\n                yield return P;\n\n            }\n\n            visited.RemoveLast();\n\n        }\n\n\n\n\n\n\n    }\n\n\n    private static void PrintPath(List&lt;int&gt; visited)\n    {\n\n        for (int i = 0; i &lt; visited.Count()-1; i++)\n        {\n            Console.Write(visited[i]);\n            Console.Write(\" --&gt; \");\n        }\n        Console.Write(visited[visited.Count() - 1]);\n\n        Console.WriteLine();\n        Console.WriteLine();\n\n    }\n\n\n}\npublic class Graph\n{\n    private readonly Dictionary&lt;int, HashSet&lt;int&gt;&gt; map = new Dictionary&lt;int, HashSet&lt;int&gt;&gt;();\n\n    public void addEdge(int node1, int node2)\n    {\n        HashSet&lt;int&gt; adjacent = null;\n\n        map.TryGetValue(node1, out adjacent);\n\n        if (adjacent == null)\n        {\n            adjacent = new HashSet&lt;int&gt;();\n            map.Add(node1, adjacent);\n        }\n        adjacent.Add(node2);\n    }\n\n    public List&lt;int&gt; adjacentNodes(int last)\n    {\n        HashSet&lt;int&gt; adjacent = null;\n\n        map.TryGetValue(last, out adjacent);\n\n        if (adjacent == null)\n        {\n            return new List&lt;int&gt;();\n        }\n        return new List&lt;int&gt;(adjacent);\n    }\n}\n    }\n</code></pre>\n"},{"tags":["javascript","jquery","performance","optimization","jquery-plugins"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":112,"score":0,"question_id":7973726,"title":"JQuery plugin to shard domain names for <img>'s","body":"<p>I was reading Stoyan Stefanov's <a href=\"http://www.phpied.com/simple-sharding-logic/\" rel=\"nofollow\">excellent article about JS domain sharding</a> for image files and wanted to improve his recipe.</p>\n\n<p>The below script will iterate through the images in a page, assign each to a bucket based on the length of the \"src\" value, and assign it to a bucket (logging this to the console).</p>\n\n<p>What I'm trying to do is create a JQuery plugin that can accept a collection like $('img') or $('link') and rewrite the \"src\" attribute with a sharded domain like \"http://images1.mydomain.com/path/to/image.png\".</p>\n\n<p>If my page has:</p>\n\n<pre><code>&lt;img src=\"/Images/file1.png\" /&gt;\n&lt;img src=\"/Images/filenumber2.png\" /&gt;\n&lt;img src=\"/Images/footer.png\" /&gt;\n</code></pre>\n\n<p>I want to do something like:</p>\n\n<pre><code>$(document).ready(function(){\n  $('img').domainShard(3, \"subdomain\", \"mydomain.com\")\n});\n</code></pre>\n\n<p>To roughly produce:</p>\n\n<pre><code>&lt;img src=\"http://subdomain1.mydomain.com/Images/file1.png\" /&gt;\n&lt;img src=\"http://subdomain2.mydomain.com/Images/filenumber2.png\" /&gt;\n&lt;img src=\"http://subdomain3.mydomain.com/Images/footer.png\" /&gt;\n</code></pre>\n\n<p>Using a JQuery plugin like so:</p>\n\n<pre><code>(function ($) {\n    $.fn.domainShard = function (buckets, subdomain, domain) {\n        var numBuckets =  buckets || 3;\n        var subdomain = subdomain || \"images\";\n        var domain = domain || \"mydomain.com\";\n        return this.each(function () {\n            // look at the src\n                // var src = $(this).attr('src');\n            // compute bucket assignment\n                // do stuff\n            // set the new path\n                // newSrc = $(this).attr('src', path);\n        });\n    };\n})(jQuery);\n</code></pre>\n\n<p>Here is Stoyan's Javascript:</p>\n\n<pre><code>function getBucket(url, numbuckets) {\n  var number = url.length,\n  group = number % numbuckets;\n  return group;\n}\n\nfunction toBuckets(stuff, numbuckets) {\n  var numbuckets = parseInt(numbuckets, 10),\n  url, group,\n  buckets = Array(numbuckets),\n  cache = {};\n    for (var i = 0, max = stuff.length; i &lt; max; i++) {\n        url = stuff[i].src;\n\n        if (typeof cache[url] === 'number') {\n            continue;\n        }\n        group = getBucket(url, numbuckets);\n        if (!buckets[group]) {\n            buckets[group] = [];\n        }\n        buckets[group].push(url);\n        cache[url] = group;\n    }\n    return buckets;\n}\n\nconsole.log(toBuckets(document.images, 3));\n</code></pre>\n\n<p>I'll keep hacking away and report back when I have something working -- any advice or assistance is greatly appreciated.</p>\n\n<p>Thanks!</p>\n"},{"tags":["c","performance","algorithm","mathematical-optimization"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":284,"score":2,"question_id":11809502,"title":"Which is better way to calculate nCr","body":"<p>Approach 1:<br>\nC(n,r) = n!/(n-r)!r!</p>\n\n<p>Approach 2:<br>\nIn the book <a href=\"http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CF0QFjAA&amp;url=http://www.math.upenn.edu/~wilf/website/CombinatorialAlgorithms.pdf&amp;ei=aTcdUJiDHojsrAfAn4C4Aw&amp;usg=AFQjCNGgowD1UYCUKKvzibbMHWzfnSGSBQ&amp;sig2=SA2ePQJ-b-0FRkz1RnlGVA\" rel=\"nofollow\">Combinatorial Algorithms by wilf</a>, i have found this:<br>\nC(n,r) can be written as <code>C(n-1,r) + C(n-1,r-1)</code>.</p>\n\n<p>e.g.     </p>\n\n<pre><code>C(7,4) = C(6,4) + C(6,3) \n       = C(5,4) + C(5,3) + C(5,3) + C(5,2)\n       .   .\n       .   .\n       .   .\n       .   .\n       After solving\n       = C(4,4) + C(4,1) + 3*C(3,3) + 3*C(3,1) + 6*C(2,1) + 6*C(2,2)\n</code></pre>\n\n<p>As you can see, the final solution doesn't need any multiplication. In every form C(n,r), either n==r or r==1.</p>\n\n<p>Here is the sample code i have implemented:</p>\n\n<pre><code>int foo(int n,int r)\n{\n     if(n==r) return 1;\n     if(r==1) return n;\n     return foo(n-1,r) + foo(n-1,r-1);\n}\n</code></pre>\n\n<p>See <a href=\"http://ideone.com/KIUbg\" rel=\"nofollow\">output</a> here.</p>\n\n<p>In the approach 2, there are overlapping sub-problems where we are calling recursion to solve the same sub-problems again. We can avoid it by using <a href=\"http://en.wikipedia.org/wiki/Dynamic_programming\" rel=\"nofollow\">Dynamic Programming</a>.</p>\n\n<p>I want to know which is the better way to calculate C(n,r)?. </p>\n"},{"tags":["c#","performance","events"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":29,"score":0,"question_id":13101794,"title":"Event add and remove implementation details","body":"<p>I'm developing on a application which has a lot of performance critial code. When looking at my code i noticed that i also have a lot of <code>+=</code> and <code>-=</code>on events with many invokations, so i ask myself ( and now you ) how <code>+=</code> and <code>-=</code>are implemented and how fast it is when have a lot of invokations.</p>\n"},{"tags":["java","performance","synchronized","simpledateformat"],"answer_count":3,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":90,"score":2,"question_id":12984345,"title":"Java 7 Calendar.getInstance, TimeZone.getTimeZone got synchronized and slow, any work arounds?","body":"<p>After upgrading my runtime to Java 7 I see incredible slowness...and my program is spending all of its time in the SimpleDateFormat constructor.  As described in a great post here: <a href=\"http://coffeedriven.org/?p=83\" rel=\"nofollow\">http://coffeedriven.org/?p=83</a> the TimeZone code is now checking for the presence of an application context in the static synchronized method getDefaultInAppContext.</p>\n\n<p>The problem for me is that its Spring Batch file reader code that is creating a new SimpleDateFormat object for each line that it reads!</p>\n\n<p>Anyone got a work around for this?</p>\n"},{"tags":["performance","autofac","castle-dynamicproxy","dynamic-proxy"],"answer_count":2,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":624,"score":2,"question_id":5256676,"title":"Autofac: Tips for increasing performance when using DynamicProxy?","body":"<p>I just start using DynamicProxy2 today. And found it caused significant performance drop.</p>\n\n<p>See the code below. Test1 is 10 times slower than Test2.</p>\n\n<p>Any tips for increasing performance when using DynamicProxy?</p>\n\n<pre><code>class Program\n{\n    public void Main()\n    {\n        for (int i = 0; i &lt; 3; i++)\n        {\n            var stopWatch = Stopwatch.StartNew();\n            int count = 1 * 1000 * 1000;\n\n            Test1(count);\n            //Test2(count);\n\n            long t = stopWatch.ElapsedMilliseconds;\n            Console.WriteLine(t.ToString() + \" milliseconds\");\n            Console.WriteLine(((double)count/(t/1000)).ToString() + \" records/1 seconds\");\n        }\n    }\n\n    void Test1(int count)\n    {\n        var builder = new ContainerBuilder();\n        builder.RegisterType&lt;TestViewModel&gt;()\n            .EnableClassInterceptors()\n            .InterceptedBy(typeof(NotifyPropertyChangedInterceptor));\n        builder.RegisterType&lt;NotifyPropertyChangedInterceptor&gt;();\n\n        var container = builder.Build();\n        for (int i = 0; i &lt; count; i++)\n        {\n            container.Resolve&lt;TestViewModel&gt;();\n        }\n    }\n\n    void Test2(int count)\n    {\n        var builder = new ContainerBuilder();\n        builder.RegisterType&lt;TestViewModel&gt;();\n\n        var container = builder.Build();\n        for (int i = 0; i &lt; count; i++)\n        {\n            container.Resolve&lt;TestViewModel&gt;();\n        }\n    }\n}\n\npublic class TestViewModel : INotifyPropertyChanged\n{\n    [Notify]\n    public virtual string Value { get; set; }\n    public event PropertyChangedEventHandler PropertyChanged;\n}\n\n/// &lt;summary&gt;\n/// Copied from: http://serialseb.blogspot.com/2008/05/implementing-inotifypropertychanged.html\n/// &lt;/summary&gt;\npublic class NotifyPropertyChangedInterceptor : IInterceptor\n{\n    public void Intercept(IInvocation invocation)\n    {\n        // let the original call go through first, so we can notify *after*\n        invocation.Proceed();\n        if (invocation.Method.Name.StartsWith(\"set_\"))\n        {\n            string propertyName = invocation.Method.Name.Substring(4);\n            var pi = invocation.TargetType.GetProperty(propertyName);\n\n            // check that we have the attribute defined\n            if (Attribute.GetCustomAttribute(pi, typeof(NotifyAttribute)) == null)\n                return;\n\n            // get the field storing the delegate list that are stored by the event.\n            FieldInfo info = invocation.TargetType.GetFields(BindingFlags.Instance | BindingFlags.NonPublic)\n                .Where(f =&gt; f.FieldType == typeof(PropertyChangedEventHandler))\n                .FirstOrDefault();\n\n            if (info != null)\n            {\n                // get the value of the field\n                PropertyChangedEventHandler evHandler = info.GetValue(invocation.InvocationTarget) as PropertyChangedEventHandler;\n                // invoke the delegate if it's not null (aka empty)\n                if (evHandler != null)\n                    evHandler.Invoke(invocation.TargetType, new PropertyChangedEventArgs(propertyName));\n            }\n        }\n    }\n}\n</code></pre>\n\n<p><strong>Update:</strong></p>\n\n<p>On my machine, Test1 takes about 45 seconds, Test2 takes about 4.5 seconds. After read <a href=\"http://stackoverflow.com/users/13163/krzysztof-kozmic\">Krzysztof Koźmic</a>'s answer, I tried to put <em>NotifyPropertyChangedInterceptor</em> into singleton scope:</p>\n\n<pre><code>builder.RegisterType&lt;NotifyPropertyChangedInterceptor&gt;().SingleInstance();\n</code></pre>\n\n<p>that saved me about 4 seconds. Now Test1 takes about 41 seconds.</p>\n\n<p><strong>Update 2:</strong></p>\n\n<p>Test3 takes about 8.3 seconds on my machine. So it seems using Autofac or DynamicProxy alone performance is not a very big problem (in my project), but combining them together would cause great performance drop.</p>\n\n<pre><code>    public void Test3(int count)\n    {\n        var generator = new Castle.DynamicProxy.ProxyGenerator();\n        for (int i = 0; i &lt; count; i++)\n        {\n            generator.CreateClassProxy(typeof(TestViewModel), \n                new NotifyPropertyChangedInterceptor());\n        }\n    }\n</code></pre>\n"},{"tags":["javascript","performance","node.js","object","garbage-collection"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":107,"score":1,"question_id":12539574,"title":"What's the best way (most efficient) to turn all the keys of an object to lower case?","body":"<p>I've come up with</p>\n\n<pre><code>function keysToLowerCase (obj) {\n  var keys = Object.keys(obj);\n  var n = keys.length;\n  while (n--) {\n    var key = keys[n]; // \"cache\" it, for less lookups to the array\n    if (key !== key.toLowerCase()) { // might already be in its lower case version\n        obj[key.toLowerCase()] = obj[key] // swap the value to a new lower case key\n        delete obj[key] // delete the old key\n    }\n  }\n  return (obj);\n}\n</code></pre>\n\n<p>But I'm not sure how will v8 behave with that, for instance, will it really delete the other keys or will it only delete references and the garbage collector will bite me later ?</p>\n\n<p>Also, I created <a href=\"http://jsperf.com/object-keys-to-lower-case\" rel=\"nofollow\">these tests</a>, I'm hoping you could add your answer there so we could see how they match up.</p>\n\n<p><strong>EDIT 1:</strong>\nApparently, according to the tests, it's faster if we don't check if the key is already in lower case, but being faster aside, will it create more clutter by ignoring this and just creating new lower case keys ? Will the garbage collector be happy with this ?</p>\n"},{"tags":["performance","azure","sql-azure","latency"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":309,"score":1,"question_id":11574516,"title":"Latency between Azure Web Role and SQL Azure and Application performance","body":"<p><strong>Azure Web Role</strong> and <strong>Sql Azure Latency</strong> </p>\n\n<p>Hi Just to know that there is a latency and timeout between the <strong>web worker role</strong> and <strong>SQL Azure</strong>  , Event a Timeout at some times (These a are not random having frequently ) 40% of 100 pings does not have 0ms timeout </p>\n\n<p>if Web worker role and SQL Azure in the same data center WHY there is a timeout as they are communicating using their internal network </p>\n\n<p>Pls Refer the attached screenshots :</p>\n\n<p><img src=\"http://i.stack.imgur.com/Ji8en.png\" alt=\"enter image description here\"></p>\n\n<p>Application which runs on this web worker role has a mysteries performance ups and downs .. if may be due to various reasons but what i need  to know is that does theses statistics on latency  and timeout affects web application performance ?.</p>\n\n<p>Thanks,</p>\n"},{"tags":["java","performance","google-app-engine","gwt"],"answer_count":4,"favorite_count":4,"up_vote_count":9,"down_vote_count":0,"view_count":502,"score":9,"question_id":10016909,"title":"Is google app engine 1.6.4 slower in local?","body":"<p><strong>Original issue</strong></p>\n\n<p>Since I changed the version from 1.6.3. to 1.6.4 I get serious performance problems working together with GWT in hosted mode.</p>\n\n<p><strong>Update 18/04/2012</strong></p>\n\n<p>The issue is reproductible also in 1.6.4.1 in dev environment.\nBy now the best is to downgrade to 1.6.3</p>\n\n<p><strong>Update 09/07/2012</strong></p>\n\n<p>According to Kris Giesing:\nThis is still not fixed in 1.7.0. A request that takes 330ms to process in 1.4.3, and 415ms to process in 1.6.3, takes 13740ms to process in 1.7.0. That's from timing the Java analysis code (no I/O) - almost a 40x slowdown.</p>\n\n<p><strong>Update 09/08/2012</strong></p>\n\n<p>Google acknowledged the problem in the <a href=\"http://code.google.com/p/googleappengine/issues/detail?id=7282\" rel=\"nofollow\">issue 7282</a> of google appengine's public issue list.</p>\n"},{"tags":["android","performance","sqlite","profiling"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":33,"score":2,"question_id":13094931,"title":"How can I find how much time is being spent in the SQLite engine on Android?","body":"<p>We are trying to track down a performance issue in our app when we do a batch of database updates.  Using the Android DDMS Profiler in Eclipse seems to only show the amount of CPU time spent in the java code, it appears to not include the time spent in the SQLite engine.\nThis makes it VERY hard to determine where all the wall-clock time is being spent.</p>\n\n<p>Is there any way to find out how much wall-time is being spent in the various SQLite engine calls?</p>\n"},{"tags":["xml","performance","query","qt","xquery"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":20,"score":0,"question_id":13098726,"title":"Qt: how to make generic (X)Query in Qt to different XML documents?","body":"<p>I'm new to QXmlQuery lib and I want to learn what is capable on with XML documents. I have maybe more than 1000 XML documents, some of them are the same (the xml scheme is the same), some of them are different.. But all of them have one common element called 'list' and this element have always children called 'item' (many items). I don't know the absolute path to the list element in each document, but I know that it is somewhere in. I want to make one query to all of this files and extract the 'items'.\nMay be this will illustrate my idea better:</p>\n\n<pre><code>QXmlQuery xquery;\nxquery.setQuery( \"doc('some_doc.xml')/list/item\");\nif (xquery.isValid())\n{\n    QXmlResultItems itemResult;\n    xquery.evaluateTo( &amp;itemResult );\n    //take an iterator or QStringList.. nevermind\n}\n</code></pre>\n\n<p>I have a code somewhere like this.</p>\n\n<p>There are two examples of these xml docs:</p>\n\n<pre><code>&lt;globalroot&gt;\n  &lt;list&gt;\n    &lt;item&gt;20.15.14447.2214&lt;/item&gt;\n    &lt;item&gt;21.15.14447.2214&lt;/item&gt;\n    &lt;item&gt;22.15.14447.2214&lt;/item&gt;\n  &lt;/list&gt;\n&lt;/globalroot&gt;\n</code></pre>\n\n<p>and the second example:</p>\n\n<pre><code>&lt;root&gt;\n &lt;sdmlt&gt;random text, not important&lt;/sdmlt&gt;\n &lt;localroot&gt;\n  &lt;list&gt;\n    &lt;item&gt;40.15.14447.2214&lt;/item&gt;\n    &lt;item&gt;41.15.14447.2214&lt;/item&gt;\n    &lt;item&gt;42.15.14447.2214&lt;/item&gt;\n  &lt;/list&gt;\n &lt;/localroot&gt;\n&lt;/root&gt;\n</code></pre>\n\n<p>The output must look like this:</p>\n\n<pre><code> //I need an iterator or QStringList that contains following down elements:\n //from the first doc:\n 20.15.14447.2214\n 21.15.14447.2214\n 22.15.14447.2214\n\n //from the second doc:\n 40.15.14447.2214\n 41.15.14447.2214\n 42.15.14447.2214\n</code></pre>\n\n<p>How can I make such a generic query? I know how to iterate over the tree and just run the xQuery over each node until (isValid() == true), but I need some mechanism in Qt that allows me to do that implicitly. Is it possible? If you know something about it you can give me a short example; suggestions, similar topics are welcomed too.</p>\n"},{"tags":["performance","oracle"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":39,"score":0,"question_id":13098707,"title":"DBMS_ALERT.WAITANY uses up to 100% cpu","body":"<p>We have an application that uses <code>DBMS_ALERT.WAITANY</code>. Because  the users complained that the application becomes so slow at certain intervals we looked at the enterprise manager of our Oracle 11 DB. We saw that in the same intervals <code>DBMS_ALERT.WAITANY</code> uses up to 100% of one cpu.</p>\n\n<p>Is there a way to know what  notification causes <code>DBMS_ALERT.WAITANY</code> to use so much resource ?</p>\n\n<p>Thanks\nAndreas</p>\n"},{"tags":["javascript","jquery","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":37,"score":1,"question_id":13098649,"title":"How to calculate javascript functionality working speed?","body":"<p>In my Application, many places i used javascript and jquery functions.\nSome times javascript progress making to slow to perform actions.\nI able to see the speed after progress is complete via firefox tool.</p>\n\n<p>Is there any way to calculate javascript working speed while script on progress.\nThanks..</p>\n"},{"tags":["mysql","database","performance","load","stress"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":3464,"score":2,"question_id":2128824,"title":"Check load on mysql database","body":"<p>What would be the best ways to monitor mysql performance and load, queries per second, total queries over a hour etc?</p>\n\n<p>Thanks!</p>\n"},{"tags":["java","performance","graphics","drawimage"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":44,"score":1,"question_id":13097866,"title":"Graphics.drawImage() is too slow","body":"<p><code>Graphics.drawImage()</code> is slowing down my program significantly.</p>\n\n<p>As the background of my game, I am constantly drawing the image <strong>(700x500)</strong> then drawing the objects on top of it.</p>\n\n<p>Without drawing the background, the program runs perfectly. With the <code>drawImage()</code> it runs at less than half of that speed.</p>\n\n<p>I have tried</p>\n\n<pre><code>GraphicsEnvironment env = GraphicsEnvironment.getLocalGraphicsEnvironment();\nGraphicsDevice device = env.getDefaultScreenDevice();\nGraphicsConfiguration config = device.getDefaultConfiguration();\nBufferedImage buffy = config.createCompatibleImage(width, height);\n</code></pre>\n\n<p>But it doesn't (seem to) make any difference.</p>\n"},{"tags":["performance","concurrency","webserver"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":1,"view_count":450,"score":0,"question_id":8176501,"title":"Web Server: Theoretical limit to concurrent connections?","body":"<p>Is there a theoretical limit to how many concurrent connections a single web server can handle?</p>\n\n<p>I've been reading a lot about the <a href=\"http://gwan.ch/\" rel=\"nofollow\">G-WAN</a> web server that claims to be the fastest in the world, and the <a href=\"http://www.kegel.com/c10k.html\" rel=\"nofollow\">C10k</a> problem.</p>\n\n<p><strong>UPDATE</strong>:</p>\n\n<p>Another way to state this question, what is the ranked order of most likely bottlenecks that will prevent additional concurrent connections?</p>\n"},{"tags":["php","performance","caching","apc","eaccelerator"],"answer_count":5,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":1473,"score":4,"question_id":4821978,"title":"Choosing a PHP caching technique: output caching into files vs. opcode caching","body":"<p>I've heard of two caching techniques for the PHP code:</p>\n\n<ol>\n<li><p>When a PHP script generates output it stores it into local files. When the script is called again it check whether the file with previous output exists and if true returns the content of this file. It's mostly done with playing around the \"output buffer\". Somthing like this is described in <a href=\"http://www.theukwebdesigncompany.com/articles/php-caching.php\" rel=\"nofollow\">this</a> article.</p></li>\n<li><p>Using a kind of opcode caching plugin, where the compiled PHP code is stored in memory. The most popular of this one is APC, also eAccelerator.</p></li>\n</ol>\n\n<p>Now the question is whether it make any sense to use both of the techniques or just use one of them. I think that the first method is a bit complicated and time consuming in the implementation, when the second one seem to be a simple one where you just need to install the module.</p>\n\n<p>I use PHP 5.3 (PHP-FPM) on Ubuntu/Debian.</p>\n\n<p>BTW, are there any other methods to cache PHP code or output, which I didn't mention here? Are they worth considering?</p>\n"},{"tags":["c#","performance","random","generator","rate"],"answer_count":4,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":60,"score":1,"question_id":13094875,"title":"Generate random text files at a specific speed in C#","body":"<p>I have created a C# console app which continuously creates text files with randomly created strings. Time (in minute) is given as an input to the app and it runs for that number of minutes to generate continuous text files. I understand that the amount of data generated will vary depending upon the processor speed and other configurations. My machine is able to generate a total of 25 mb of data in text format in 1 minute.</p>\n\n<p>Now, my question is, can I control the rate of data generated per minute if given as another input to the app through my code? </p>\n\n<p>Any help would be very much appreciated.</p>\n"},{"tags":["php","mysql","arrays","performance","sleep"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":13094991,"title":"How do I ensure that php script gives enough time for execution of MySQL query?","body":"<p>I want to give my php script time to check queries.</p>\n\n<p>My cycle:</p>\n\n<ol>\n<li>Choose value</li>\n<li>--- I want (maybe here?) some delay, sleep or something ---</li>\n<li>MySQL SELECT Query</li>\n<li>Compare values depending on SELECT results</li>\n</ol>\n\n<p>Points 3 and 4 can theoretically take about 2 seconds. </p>\n\n<p>Will PHP wait until the SELECT is complete?</p>\n\n<p>Or will php wait for filling big arrays? (multidimensional, about 250 * 5 * 2 values..)</p>\n"},{"tags":["c#",".net","performance","linq"],"answer_count":4,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":131,"score":0,"question_id":13087328,"title":"Are there some disadvantages in using a lot of LINQ to Objects statements?","body":"<p>I come from Java programming and switching to C# programming I discovered the extreme powerful of LINQ.</p>\n\n<p>In my recent implementation I noticed that I use it (expecially LINQ to Objects) very often in my code to avoid <code>foreach</code> loops, to search elements in lists and for similar tasks.</p>\n\n<p>Now I'm wondering if there is some performance disadvantage in massively use Linq to Objects...</p>\n"},{"tags":["performance","cuda","gpu","nvidia"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":132,"score":0,"question_id":13035505,"title":"Converting a C for-loop to a CUDA for-loop","body":"<p>I have this low level for loop I've written in C that a friend suggested I write in CUDA. I've set up my CUDA enviroment and have been looking at the docs, but i'm still struggling with the syntax for what's been well over 2 weeks now. Can anyone help me out? What would this look like in CUDA? </p>\n\n<pre><code>float* red = new float [N];\nfloat* green = new float [N];\nfloat* blue = new float [N];\n\nfor (int y = 0; y &lt; h; y++)\n{\n    // Get row ptr from the color image\n    const unsigned char* src = rowptr&lt;unsigned char&gt;(color, 0, y, w);\n\n    // Get row ptrs for the destination channel features\n    float* rptr = rowptr&lt;float&gt;(red, 0, y, w);\n    float* gptr = rowptr&lt;float&gt;(green, 0, y, w);\n    float* bptr = rowptr&lt;float&gt;(blue, 0, y, w);\n\n    for (int x = 0; x &lt; w; x++)\n    {\n        *rptr++ = (float)*src++;\n        *gptr++ = (float)*src++;\n        *bptr++ = (float)*src++;\n    }\n}\n</code></pre>\n"},{"tags":["python","performance","algorithm","optimization","cython"],"answer_count":13,"favorite_count":7,"up_vote_count":30,"down_vote_count":0,"view_count":698,"score":30,"question_id":12926575,"title":"speeding up paring of strings into objects in Python","body":"<p>I'm trying to find an efficient way to pair together rows of data containing integer points, and storing them as Python objects. The data is made up of <code>X</code> and <code>Y</code> coordinate points, represented as a comma separated strings. The points have to be paired, as in <code>(x_1, y_1), (x_2, y_2), ...</code> etc. and then stored as a list of objects, where each point is an object. The function below <code>get_data</code> generates this example data:</p>\n\n<pre><code>def get_data(N=100000, M=10):\n    import random\n    data = []\n    for n in range(N):\n        pair = [[str(random.randint(1, 10)) for x in range(M)],\n                [str(random.randint(1, 10)) for x in range(M)]]\n        row = [\",\".join(pair[0]),\n               \",\".join(pair[1])]\n        data.append(row)\n    return data\n</code></pre>\n\n<p>The parsing code I have now is:</p>\n\n<pre><code>class Point:\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n\ndef test():\n    import time\n    data = get_data()\n    all_point_sets = []\n    time_start = time.time()\n    for row in data:\n        point_set = []\n        first_points, second_points = row\n        # Convert points from strings to integers\n        first_points = map(int, first_points.split(\",\"))\n        second_points = map(int, second_points.split(\",\"))\n        paired_points = zip(first_points, second_points)\n        curr_points = [Point(p[0], p[1]) \\\n                       for p in paired_points]\n        all_point_sets.append(curr_points)\n    time_end = time.time()\n    print \"total time: \", (time_end - time_start)\n</code></pre>\n\n<p>Currently, this takes nearly 7 seconds for 100,000 points, which seems very inefficient. Part of the inefficiency seems to stem from the calculation of <code>first_points</code>, <code>second_points</code> and <code>paired_points</code> - and the conversion of these into objects. </p>\n\n<p>Another part of the inefficiency seems to be the building up of <code>all_point_sets</code>. Taking out the <code>all_point_sets.append(...)</code> line seems to make the code go from ~7 seconds to 2 seconds!</p>\n\n<p>How can this be sped up? thanks.</p>\n\n<p><strong>FOLLOWUP</strong> Thanks for everyone's great suggestions - they were all helpful. but even with all the improvements, it's still about 3 seconds to process 100,000 entries. I'm not sure why in this case it's not just instant, and whether there's an alternative representation that would make it instant.  Would coding this in Cython change things?  Could someone offer an example of that?  thanks again.</p>\n"},{"tags":["sql","xml","query","performance"],"answer_count":4,"favorite_count":3,"up_vote_count":1,"down_vote_count":0,"view_count":698,"score":1,"question_id":3701616,"title":"Speed Up XML Queries in SQL Server 2005","body":"<p>I store all my data in on XML column in SQL Server 2005.</p>\n\n<p>As more and more records are being inserted, I notice the queries are slowing down.  I've tried creaeting a Primary XML Index, as well as a Secondary VALUE index and this did not do anything to help the speed.</p>\n\n<p>Any tips,thoughts, or tricks that I'm missing?</p>\n\n<p>Sample View that I query:</p>\n\n<pre><code>SELECT Id\n, CaseNumber\n, XmlTest.value('(/CodeFiveReport/ReportEvent/StartDate)[1]', 'varchar(25)') + ' ' + XmlTest.value('(/CodeFiveReport/ReportEvent/StartTime)[1]', 'varchar(25)') as StartDate\n, XmlTest.value('(/CodeFiveReport/@Status)[1]', 'varchar(10)') as [Status]\n, XmlTest.value('(/CodeFiveReport/ReportEvent/Address/PatrolDistrict/@Name)[1]', 'varchar(100)') as PatrolDistrict\n, XmlTest.value('(/CodeFiveReport/PrimaryUnit/@Name)[1]', 'varchar(40)') as PrimaryUnit\n, XmlTest.value('(/CodeFiveReport/ReportEvent/Address/@StreetNumber)[1]', 'varchar(50)') + ' ' + XmlTest.value('(/CodeFiveReport/ReportEvent/Address/@StreetName)[1]', 'varchar(50)') + ' ' + XmlTest.value('(/CodeFiveReport/ReportEvent/Address/StreetSuffix/@Name)[1]', 'varchar(50)') + ' ' + XmlTest.value('(/CodeFiveReport/ReportEvent/Address/@City)[1]', 'varchar(50)') + ' ' + XmlTest.value('(/CodeFiveReport/ReportEvent/Address/State/@Abbreviation)[1]', 'varchar(50)') + ' '  + XmlTest.value('(/CodeFiveReport/ReportEvent/Address/@ZipCode)[1]', 'varchar(50)') as Location\n, XmlTest.value('(/CodeFiveReport/ReportEvent/ReportType/@Name)[1]', 'varchar(50)') as ReportType\n, XmlTest.value('(/CodeFiveReport/ReportEvent/Offenses/OffenseDescription/OffenseType/@CodeAndDescription)[1]', 'varchar(50)') as IncidentType\n, XmlTest as Report\n, CreatedBy as UserId\n, XmlTest.value('(/CodeFiveReport/PrimaryUnit/@ID)[1]', 'integer') as UnitId\n, XmlTest.value('(/CodeFiveReport/PrimaryUnit/@Code)[1]', 'varchar(6)') as UnitCode\n, XmlTest.value('(/CodeFiveReport/Owner/AgencyID)[1]', 'char(2)') as AgencyId   \n, IsLocked\n, LockedBy\n, XmlTest.value('(/CodeFiveReport/VersionUsed)[1]', 'varchar(20)') as VersionUsed\nFROM UploadReport\nWHERE XmlTest.value('(/CodeFiveReport/Owner/AgencyID)[1]', 'char(2)') = '06'\n</code></pre>\n"},{"tags":["performance","hibernate","jpa"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":1569,"score":3,"question_id":4196921,"title":"Dynamic-update with JPA","body":"<p>I was surprised to recently learn that the default hibernate behavior is to update all of the fields in an object if only a single change is made and merge is called.</p>\n\n<p>Dynamic-update is the field that allows you to configure an alternative approach of just updating the changed field...</p>\n\n<p><a href=\"http://www.mkyong.com/hibernate/hibernate-dynamic-update-attribute-example/\" rel=\"nofollow\">http://www.mkyong.com/hibernate/hibernate-dynamic-update-attribute-example/</a></p>\n\n<p>I am using JPA with hibernate and I tried to add the following </p>\n\n<pre><code>@javax.persistence.Entity\n@org.hibernate.annotations.Entity(dynamicInsert=true, dynamicUpdate=true) \n</code></pre>\n\n<p>to my class (previously it only had the JPA annotation)</p>\n\n<p>Anyway, i've been monitoring the sql and unfortunately it didn't change it and I'm still seeing every field updated.</p>\n\n<p>This is my java that updates the object...</p>\n\n<pre><code>    @Transactional(readOnly = false, propagation = Propagation.REQUIRES_NEW)    \npublic void setAccountStatusForUser(String username, AccountStatus act){\n    User u = this.getUser(username);\n    u.setAccountStatus(act);\n    this.update(u);\n}\n</code></pre>\n\n<p>and the update method does the following:</p>\n\n<pre><code>    @Transactional(readOnly = false, propagation = Propagation.REQUIRES_NEW)\n  public Object update(Object o) {\n      Object a = this.entityManager.merge(o);\n      this.entityManager.flush();\n      return a;\n}\n</code></pre>\n\n<p>Any help would greatly be appreciated.</p>\n"},{"tags":["objective-c","cocoa-touch","performance","instruments"],"answer_count":2,"favorite_count":7,"up_vote_count":10,"down_vote_count":0,"view_count":3766,"score":10,"question_id":1488601,"title":"How to find out what mach_msg_trap waits for?","body":"<p>I a profiling my iPhone application on target, and according to Instruments 65% of the time is spent in <code>mach_msg_trap</code>.</p>\n\n<p>I have a background thread that runs-forever and send results back to the main thread using <code>performSelectorOnMainThread:withObject:waitUntilDone:</code>, aproximately every 2 seconds. I am not waiting until done.</p>\n"},{"tags":["python","performance","list","optimization","code-efficiency"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":102,"score":3,"question_id":13084619,"title":"First items in inner list efficiently as possible","body":"<p>I have a coordinated storage list in python   <code>A[row,col,value]</code> for storing non-zeros values.</p>\n\n<p>How can I get the list of all the row indexes? I expected this <code>A[0:][0]</code> to work as <code>print A[0:]</code> prints the whole list but <code>print A[0:][0]</code> only prints <code>A[0]</code>.</p>\n\n<p>The reason I ask is for efficient calculation of the number of non-zero values in each row <em>i.e</em> iterating over <code>range(0,n)</code> where n is the total number of rows. This should be much <em>cheaper</em> than my current way of <code>for i in range(0,n): for j in A: ...</code>. </p>\n\n<p>Something like:</p>\n\n<pre><code>c = []\n# for the total number of rows\nfor i in range(0,n):\n     # get number of rows with only one entry in coordinate storage list\n     if A[0:][0].count(i) == 1: c.append(i)                \nreturn c\n</code></pre>\n\n<p>Over:</p>\n\n<pre><code>c = []\n# for the total number of rows \nfor i in range(0,n):\n    # get the index and initialize the count to 0 \n    c.append([i,0])\n    # for every entry in coordinate storage list \n    for j in A:\n        # if row index (A[:][0]) is equal to current row i, increment count  \n        if j[0] == i:\n           c[i][1]+=1\nreturn c\n</code></pre>\n\n<p><strong>EDIT:</strong> </p>\n\n<p>Using Junuxx's answer, <a href=\"http://stackoverflow.com/questions/2600191/how-to-calculate-the-occurrences-of-a-list-item-in-python\">this question</a> and <a href=\"http://www.daniweb.com/software-development/python/code/217019/search-a-python-dictionary-both-ways\" rel=\"nofollow\">this post</a> I came up with the following <em>(for returning the number of singleton rows)</em> which is much faster for my current problems size of <code>A</code> than my original attempt. However it still grows with the number of rows and columns. I wonder if it's possible to not have to iterate over <code>A</code> but just upto <code>n</code>? </p>\n\n<pre><code># get total list of row indexes from coordinate storage list\nrow_indexes = [i[0] for i in A]\n# create dictionary {index:count}\nc = Counter(row_indexes)    \n# return only value where count == 1 \nreturn [c[0] for c in c.items() if c[1] == 1]\n</code></pre>\n"},{"tags":["sql","performance","sqlite","select","insert"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":57,"score":0,"question_id":13090775,"title":"Optimize this insert SQL Query","body":"<p>I've been trying to find out why my SQLite database is performing relatively slowly (4 seconds to insert 1500 records) and I think I've narrowed it down to this query. Is there a way to optimise this? </p>\n\n<pre><code> \"INSERT OR REPLACE INTO MainFrame(WID,PName,PAlias,PModel,FriendID, UniverseID, GalaxyID) VALUES\n  ((SELECT WID FROM Worlds WHERE WName= ?),\n  @pname,\n  @palias,\n  @pmodel,\n  (SELECT FriendID FROM Friend WHERE FriendName = @eFriend),\n  (SELECT UniverseID FROM Universes WHERE UniverseName = @eUniverse),\n  (SELECT GalaxyID FROM Galaxies WHERE GalaxyName = @eGalaxy ))\";\n</code></pre>\n\n<p>As you can see, there are a few <code>Selects</code> being used in an insert query. The reason for this is because the loop inserts data into other tables (<code>WID</code>, <code>FriendID</code>, <code>UniverseID</code>, <code>GalaxyID</code>) so I don't have that data until it's been inserted. I need this data to insert into the <code>MainFrame</code> table but this feels like a brute force approach. Any advice?</p>\n"},{"tags":["performance","assembly","x86","intel"],"answer_count":0,"favorite_count":1,"up_vote_count":3,"down_vote_count":1,"view_count":64,"score":2,"question_id":13092829,"title":"Any way to move 2 bytes in 32-bit x86 using MOV without causing a mode switch or cpu stall?","body":"<p>If I want to move 2 unsigned bytes from memory into a 32-bit register, can I do that with a MOV instruction and no mode switch?</p>\n\n<p>I notice that you CAN do that with the MOVSE and MOVZE instructions. For example, with MOVSE the encoding 0F B7 moves 16 bits to a 32 bit register. It is a 3 cycle instruction, though.</p>\n\n<p>Alternatively I guess I could move 4 bytes into the register and then somehow CMP just two of them somehow. What is the fastest strategy for retrieving and comparing 16-bit data on 32-bit x86? Note that I am mostly doing 32-bit operations so I can't switch to 16-bit mode and stay there.</p>\n\n<p>----- FYI to the uninitiated: the issue here is that 32-bit Intel x86 processors can MOV 8-bit data and 16-bit OR 32-bit data depending on what mode they are in. This mode is called the \"D-bit\" setting. You can use special prefixes 0x66 and 0x67 to use a non-default mode. For example, if you are in 32-bit mode, and you prefix the instruction with 0x66 this will cause the operand to be treated as 16-bit. The only problem is that doing this causes a big performance hit.</p>\n"},{"tags":["java","performance","coding-style"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":34,"score":0,"question_id":13092524,"title":"Setting a value only on first access -- best practice, (micro)performance?","body":"<p>In the below code, assume that <code>getAndClear()</code> will get called billions of times, i.e. assume that performance matters. It will return an array only during its first call. It must return null in all further calls. (That is, my question is about micro-optimization in some sense, and I'm aware of the fact it's bad practice, but you can also consider it as a question of \"which code is nicer\" or \"more elegant\".)</p>\n\n<pre><code>public class Boo {\n   public static int[] anything = new int[] { 2,3,4 };\n   private static int[] something = new int[] { 5,6,7 }; // this may be much bigger as well\n\n   public static final int[] getAndClear() {\n      int[] st = something;\n      something = null;\n      // ... (do something else, useful)            \n\n      return st;\n   }\n}\n</code></pre>\n\n<p>Is the below code faster? Is it better practice?</p>\n\n<pre><code>public static int[] getAndClear() {\n   int[] array = sDynamicTextIdList;\n   if (array != null) {\n      sDynamicTextIdList = null;\n      // ... (do something else, useful)     \n      return array;\n   }\n   // ... (do something else, useful)     \n\n   return null;\n}\n</code></pre>\n\n<p>A further variant could be this:</p>\n\n<pre><code>public static int[] getAndClear() {\n   int[] array = sDynamicTextIdList;\n   if (array != null) {\n      sDynamicTextIdList = null;\n   }\n   // ... (do something else, useful)     \n   return array;\n}\n</code></pre>\n\n<p>I know it probably breaks down to hardware architecture level and CPU instructions (setting something to 0 vs. checking for 0), and performance-wise, it doesn't matter, but then I would like to know which is the \"good practive\" or more quality code. In this case, the question can be reduced to this:</p>\n\n<pre><code>private static boolean value = true;       \n\npublic static int[] getTrueOnlyOnFirstCall() {\n   boolean b = value;\n   value = false;\n   return b;\n}\n</code></pre>\n\n<p>If the method is called 100000 times, this means that <code>value</code> will be set to <code>false</code> 99999 times unnecessarily. The other variant (faster? nicer?) would look like this:</p>\n\n<pre><code>public static int[] getTrueOnlyOnFirstCall() {\n   boolean b = value;\n   if (b) { \n      value = false;\n      return true;\n   }\n   return false;\n}\n</code></pre>\n\n<p>Moreover, compile-time and JIT-time optimizations may also play a role here, so this question could be extended by \"and what about in C++\". (If my example is not applicable to C++ in this form, then feel free to subtitute the statics with member fields of a class.)</p>\n"},{"tags":["mysql","performance","indexing","innodb","clustered-index"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":43,"score":2,"question_id":13057095,"title":"Behavior of InnoDB clustered compound index","body":"<p>We are running MySQL/ISAM database with a following table:</p>\n\n<pre><code>create table measurements (\n  `tm_stamp` int(11) NOT NULL DEFAULT '0',\n  `fk_channel` int(11) NOT NULL DEFAULT '0',\n  `value` int(11) DEFAULT NULL,\n  PRIMARY KEY (`tm_stamp`,`fk_channel`)\n);\n</code></pre>\n\n<p>The <code>tm_stamp</code>-<code>fk_channel</code> combination is required unique, hence the compound primary key. Now, for certain irrelevant reason, the database will be migrated to InnoDB engine. Upon googling something about it, i found out that the key will dictate the physical ordering of the data on the disk. 90% of the queries currently go as follows:</p>\n\n<pre><code>SELECT value FROM measurements\nWHERE fk_channel=A AND tm_stamp&gt;=B and tm_stamp&lt;=C\nORDER BY tm_stamp ASC\n</code></pre>\n\n<p>Inserts are 99% in order of <code>tm_stamp</code>, it's a storage for dataloggers network. The table has low millions of rows but growing steadily. The questions are</p>\n\n<ol>\n<li>Should the sole change of storage engine result in any significant performance change, better or worse?</li>\n<li>Does the order of columns in the index matter with regards to the most popular SELECT? <a href=\"http://www.cumps.be/nl/blog/read/efficient-compound-index-usage\" rel=\"nofollow\">This blog</a> suggest something along that line.</li>\n<li>Thanks to the nature of clustered index, may we perhaps leave out the ORDER BY clause and gain some performance?</li>\n</ol>\n"},{"tags":["c#","visual-studio-2010","colors","richtextbox","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":415,"score":1,"question_id":3545935,"title":"Why does the Rich Text Box freeze when loading a large string?","body":"<p>I have a program where I basically need to load Rich Text from a StringBuilder.\nThe problem is, somethimes I get a string that is 100,000 lines long (and this is a possible situation for the program), including Rtf codes and colours. </p>\n\n<p>The problem isn't building the string, it's when I asign the Rtf property to the StringBuilder.ToString(), it takes a solid <strong>4 minutes</strong> to load.</p>\n\n<pre><code>TextBox.Rtf = Build.ToString();\n</code></pre>\n\n<p>If I copy this same string from the StringBuilder, and load it in WordPad, it takes about <strong>2 or 3 seconds</strong>. I am diabling the RTB's redrawing by using SendMessage() and WM_SETREDRAW, but that doesn't change anything.</p>\n\n<p>Any suggestions?</p>\n"},{"tags":["python","performance","queue","multiprocessing","pipe"],"answer_count":1,"favorite_count":6,"up_vote_count":19,"down_vote_count":0,"view_count":2128,"score":19,"question_id":8463008,"title":"Python multiprocessing - Pipe vs Queue","body":"<p>What are the fundamental differences between queues and pipes in <a href=\"http://docs.python.org/library/multiprocessing.html\">Python's multiprocessing package</a>?</p>\n\n<p>In what scenarios should one choose one over the other?  When is it advantageous to use <code>Pipe()</code>?  When is it advantageous to use <code>Queue()</code>?</p>\n"},{"tags":["c#","performance","richtextbox"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":4,"view_count":80,"score":-2,"question_id":13091739,"title":"Inserting text into RichTextBox causes program to freeze","body":"<p>I have a RichTextBox control on my Form and I am trying to load about 1,800,000 characters into it. When I do, my application freezes.</p>\n\n<p>I know that my usage of this control is an edge case scenario, but does anyone have any recommendations for how I can prevent the application from freezing?</p>\n"},{"tags":["c#","performance","memory","memory-leaks","profiling"],"answer_count":3,"favorite_count":0,"up_vote_count":6,"down_vote_count":0,"view_count":72,"score":6,"question_id":13084585,"title":"Process.GetProcessesByName(String, String) Memory Leak","body":"<p>I have a piece of code that gets a list of processes on a remote computer using the static method <a href=\"http://msdn.microsoft.com/en-us/library/725c3z81%28v=vs.100%29.aspx\" rel=\"nofollow\">Process.GetProcessesByName(String, String)</a>, this runs on a lot of computers (a few thousands) and I've noticed it's a cause of a major memory leak.</p>\n\n<p>I ran ANTS memory profiler which told me that most of my memory is taken by strings, strings containing strage values like \"% Idle Time\", \"Processor Information\", and \"Cache Faults/sec\". I've recognized those strings as probably being a part of Performance Counters in the program, the problem is I don't have any performance counters in the program.</p>\n\n<p>Digging deeper found out those strings are held in hashtables that are held by PerformanceCounterLib which are held by ANOTHER hashtable that is stored inside an internal static member of the PerformanceCounterLib class (which in itself is internal).</p>\n\n<p>Digging even deeper into the rabbit hole, I've found out that Process.GetProcesesByName uses PerformanceCounterLib to get the process list running on a distant computer and that for each remote computer another PerformanceCounterLib instance is created and referenced in the static internal variable of PerformanceCounterLib. Each of those instances hold that hashtable of strings that I found out is clogging my memory (each of them is between 300-700 kb, meaning it's clogging up my Large Object Heap).</p>\n\n<p>I did not find a way to delete those unused PerformanceCounterLib instances, they are all internal and the user has no access to them.</p>\n\n<p>How can I fix my memory problem? This is REALLY bad, my program hits 5GB (my server's limit) within 24 hours.</p>\n\n<p><em>EDIT</em>: added a piece of code (not tested) that should reproduce the problem. For clarification:</p>\n\n<pre><code>/// computerNames is a list of computers that you have access to\npublic List&lt;string&gt; GetProcessesOnAllComputers(List&lt;string&gt; computerNames)\n{\n    var result = new List&lt;string&gt;();\n    foreach(string compName in computernames)\n    {\n        Process[] processes = Process.GetProcesses(compName); // Happens with every     method that gets processes on a remote computer\n        string processString = processes.Aggregate(new StringBuilder(), (sb,s) =&gt; sb.Append(';').Append(s), sb =&gt; sb.ToString());\n        result.Add(processString);\n        foreach (var p in processes)\n        {\n            p.Close();\n            p.Dispose();\n        }\n        processes = null;\n    }\n}\n</code></pre>\n"},{"tags":["database","performance","postgresql","postgresql-performance"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":94,"score":2,"question_id":13088407,"title":"Postgresql: inner join takes 70 seconds","body":"<p>I have two tables - </p>\n\n<p>Table A : 1MM rows, \nAsOfDate, Id, BId (foreign key to table B)</p>\n\n<p>Table B : 50k rows,\nId, Flag, ValidFrom, ValidTo</p>\n\n<p>Table A contains multiple records per day between 2011/01/01 and 2011/12/31 across 100 BId's.\nTable B contains multiple non overlapping (between validfrom and validto) records for 100 Bids. </p>\n\n<p>The task of the join will be to return the flag that was active for the BId on the given AsOfDate.</p>\n\n<pre><code>select \n    a.AsOfDate, b.Flag \nfrom \n    A a inner Join B b on \n        a.BId = b.BId and b.ValidFrom &lt;= a.AsOfDate and b.ValidTo &gt;= a.AsOfDate\nwhere\n    a.AsOfDate &gt;= 20110101 and a.AsOfDate &lt;= 20111231\n</code></pre>\n\n<p>This query takes ~70 seconds on a very high end server (+3Ghz) with 64Gb of memory.</p>\n\n<p>I have indexes on every combination of field as I'm testing this - to no avail.</p>\n\n<p>Indexes : a.AsOfDate, a.AsOfDate+a.bId, a.bid\nIndexes : b.bid, b.bid+b.validfrom</p>\n\n<p>Also tried the range queries suggested below (62seconds)</p>\n\n<p>This same query on the free version of Sql Server running in a VM takes ~1 second to complete.</p>\n\n<p>any ideas?</p>\n\n<p>Postgres 9.2</p>\n\n<p>Query Plan</p>\n\n<pre><code>QUERY PLAN                                       \n---------------------------------------------------------------------------------------\nAggregate  (cost=8274298.83..8274298.84 rows=1 width=0)\n-&gt;  Hash Join  (cost=1692.25..8137039.36 rows=54903787 width=0)\n    Hash Cond: (a.bid = b.bid)\n     Join Filter: ((b.validfrom &lt;= a.asofdate) AND (b.validto &gt;= a.asofdate))\n     -&gt;  Seq Scan on \"A\" a  (cost=0.00..37727.00 rows=986467 width=12)\n           Filter: ((asofdate &gt; 20110101) AND (asofdate &lt; 20111231))\n     -&gt;  Hash  (cost=821.00..821.00 rows=50100 width=12)\n           -&gt;  Seq Scan on \"B\" b  (cost=0.00..821.00 rows=50100 width=12)\n</code></pre>\n\n<p>see <a href=\"http://explain.depesz.com/s/1c5\" rel=\"nofollow\">http://explain.depesz.com/s/1c5</a> for the analyze output </p>\n\n<p><img src=\"http://i.stack.imgur.com/2CWWU.png\" alt=\"here is the query plan from sqlserver for the same query\"></p>\n"},{"tags":["performance","apache","benchmarking"],"answer_count":2,"favorite_count":2,"up_vote_count":12,"down_vote_count":0,"view_count":1677,"score":12,"question_id":2820306,"title":"Definition of Connect, Processing, Waiting in apache bench","body":"<p>When I run apache bench I get results like:</p>\n\n<pre><code>Command: abs.exe -v 3 -n 10 -c 1 https://mysite\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:      203  213   8.1    219     219\nProcessing:    78  177  88.1    172     359\nWaiting:       78  169  84.6    156     344\nTotal:        281  389  86.7    391     564\n</code></pre>\n\n<p>I can't seem to find the definition of Connect, Processing and Waiting.  What do those numbers mean?</p>\n"},{"tags":["ruby","performance","syntax"],"answer_count":9,"favorite_count":15,"up_vote_count":40,"down_vote_count":0,"view_count":4517,"score":40,"question_id":1836467,"title":"Is there a performance gain in using single quotes vs double quotes in ruby?","body":"<p>Do you know if using double quotes instead of single quotes in ruby decreases performance in any meaningful way in ruby 1.8 and 1.9.</p>\n\n<p>so if I type </p>\n\n<pre><code>question = 'my question'\n</code></pre>\n\n<p>is it faster than </p>\n\n<pre><code>question = \"my question\"\n</code></pre>\n\n<p>I imagine that ruby tries to figure out if something needs to be evaluated when it encounters double quotes and probably spends some cycles doing just that.</p>\n"},{"tags":["performance","compiler","cuda"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":13091705,"title":"Differences between CUDA 4.2 and 4.0 front-ends","body":"<p>This is from an experiment I did a while back, sadly I do not have access to the code or the machine, so I won't be able to check your profiling suggestions anytime soon. I used a high level directive based approach to parallelize two applications, one memory bound and the other was compute bound (which performed much better on Tesla M2090). The compute bound kernel was heavy on arithmetic operations (primarily multiplications). \nThese are my results for some data sizes for the compute bound kernel. The compiler options\nfor all these versions are minimal and consistent.</p>\n\n<pre><code>Size    cu4.2-llvm  cu4.2-open64    cu4.0   cu4.0-fastmath  cu4.2-fastmath\n--------------------------------------------------------------------------\n150       0.26          0.18         0.18       0.17             0.19\n250       1.04          0.55         0.55       0.53             0.67\n350       2.88          1.35         1.33       1.29             1.78\n450       6.31          2.8          2.78       2.69             3.88\n</code></pre>\n\n<p>From the above data, I have some questions:</p>\n\n<ol>\n<li><p>Since the results of <code>cu4.2-open64</code> (CUDA 4.2 with -open64 flag to use the Open64 front end), <code>cu4.0</code> and <code>cu4.0-fastmath</code> are almost the same, does CUDA 4.0 implicitly turn on the fast math options or is this because of FMA or something else ?</p></li>\n<li><p>When I provide <code>fastmath</code> options to CUDA 4.2 (cu4.2-fastmath), my results are close to the CUDA 4.0 group, so this difference between 4.2 and 4.0 could be related to optimization of arithmetic operations by the two compilers? </p></li>\n<li><p>As I said, I am not using CUDA directly, rather a high level pragma based approach (HMPP, PGI, OpenACC) wherein I specify clauses to distribute loops to GPU grid blocks, and specify data scoping of variables. \nCould this performance difference between compilers be related to how the high level \napproach translates the user code?</p></li>\n</ol>\n\n<p>For the memory intensive code, I see very minimal difference between these versions, which is why I don't show their results. </p>\n"},{"tags":["sql","performance","oracle","plsql"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":35,"score":0,"question_id":13091634,"title":"Oracle TimeStamp with Timezone vs timestamp vs datetime vs offsets","body":"<p>So i am trying to improve the performance of my reporting queries where time is being used as a filter for these sql statements. The columns that the time we're querying on are currently stored as timestamps with timezone. I have found that indexing on these columns seems to auto generate a function based index where it performs a sys_extract_utc on the column. I have also found that filtering on these columns seems to slow down the performance of my queries greatly. Considering i need these columns with the timestamp with timezone to perform calculations on other things beside the reporting, i had planned to create a new column for each of the timestamp with timezone fields.</p>\n\n<p>What i want to know is what would be the best way to go/ what would be the pros/cons with the creation of these two new columns. As in what type of field would give me the best performance on filtering my queries. a timestamped column with no timezone, a datetime, or would using an offset give me the best results for my queries?</p>\n\n<p><strong>NOTE:</strong> by offset what i mean is i would give what we consider as the beginning of time in our database a numeric value and from then on the time values would be given a numeric comparison to filter against. </p>\n\n<p>and help or suggestions are greatly appreciated. Thank you.</p>\n"},{"tags":["c#",".net","performance","webrequest"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":63,"score":0,"question_id":13078241,"title":"Understanding WebRequests","body":"<p>In my code (C#) I'm using a WebRequest:</p>\n\n<pre><code>var request = WebRequest.Create(url);\n</code></pre>\n\n<p>For some reason this line takes about 2-3 seconds to run. I've looked for a solution, but couldn't find any for the creation of the request. I've tried different urls (http and https, google, etc.) but nothing seems to help. Did anyone else experience such a behavior? Can anyone explain what exactly is going on during the creation of the request? Any alternatives?</p>\n\n<p>BTW - I'm on a 64bit Win 7 (Bootcamp)</p>\n\n<p>Edit - measured 3.126 seconds with Stopwatch for the following code (wireless):</p>\n\n<pre><code>var request = WebRequest.Create(\"http://www.google.com\");\n</code></pre>\n\n<p>On the same network, only wired, it took 0.01 seconds.</p>\n\n<p>Thanks everyone!</p>\n"},{"tags":["android","performance","android-ui"],"answer_count":1,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":71,"score":4,"question_id":12812003,"title":"What could cause an Android app to run slow on an identical device to one which it runs fast on?","body":"<p>I, and a few other of my Android app users, run a Galaxy Nexus. Most of us find the app to be blazing fast, but a couple are reporting that it is unusably slow <strong><em>also on a Galaxy Nexus</em></strong>. I'm shocked to hear them tell me that the buttons, scrolling, etc. are all slow. The main view of the app is a <code>ListView</code> containing many images, textviews, etc. In fact, you can check out <a href=\"https://play.google.com/store/apps/details?id=com.streamified.streamified\" rel=\"nofollow\">the app for free on Google Play</a> if you feel like digging deeper. I'm trying to compile a checklist of what might cause this issue.</p>\n\n<p>Here's what I have so far:</p>\n\n<ul>\n<li>Low memory</li>\n<li>Low disk space</li>\n<li>Uncaught errors</li>\n<li>Rooted device (?)</li>\n</ul>\n\n<p>Any other ideas?</p>\n\n<p>More importantly, is there any way to detect (or even adjust for!) potential problems?</p>\n"},{"tags":["python","sql","performance","parsing","obiee"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":31,"score":0,"question_id":13072593,"title":"Is there a good log parser for OBIEE's nqquery.log files?","body":"<p>I would like to extract a list of all the Logical SQLs executed by OBIEE. This information is present in OBIEE's nqquery.log log files. I am looking for a script which can parse this log file and also provide the following information for each Logical SQL, in a CSV file</p>\n\n<ul>\n<li>Hash Id of the Logical SQL and the complete query</li>\n<li>Time Taken to execute the logical sql</li>\n<li>Ability to group related Logical SQLs by Subject Area</li>\n</ul>\n\n<p>It should be able to collect all the Physical SQLs for a given Logical SQL after I increase the log level and disable cache.</p>\n\n<p>Added bonus, provide an Explain Plan for the Physical SQLs if I provide the Database connection information.</p>\n\n<p>Does such a script exist or is it asking for too much?</p>\n"},{"tags":["mysql","performance","insert"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":27,"score":0,"question_id":13088441,"title":"Efficient multiple row inserts in mysql","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/1793169/which-is-faster-multiple-single-inserts-or-one-multiple-row-insert\">Which is faster: multiple single INSERTs or one multiple-row INSERT?</a>  </p>\n</blockquote>\n\n\n\n<p>While going through a book on mysql, I found out two ways to insert a row in a database.</p>\n\n<pre><code>Method 1 \n\nINSERT INTO tableName (col1, col2, col3) VALUES('a', 'b', 'c');\nINSERT INTO tableName (col1, col2, col3) VALUES('d', 'b', 'c');\nINSERT INTO tableName (col1, col2, col3) VALUES('e', 'b', 'c');\n\nMethod 2\n\nINSERT INTO tableName (col1, col2, col3) VALUES('a', 'b', 'c'), ('d', 'b', 'c'), ('e', 'b', 'c');\n</code></pre>\n\n<p>Is the second method more efficient than the first one ? Or does it simply calls the <code>Method 1</code> multiple times ?</p>\n"},{"tags":["multithreading","performance","sql-server-2008","parallel-processing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":289,"score":0,"question_id":9561903,"title":"The query processor could not start the necessary thread resources","body":"<p>I having this error on my mssql server especially when there are many records inserted at the same time:\n <em>\"100|The query processor could not start the necessary thread resources for parallel query execution.\"</em> </p>\n\n<p>i try google search the problem to upgrade my sql performance so that can avoid this error, and plenty of it suggests to set maxdrop on the query or the max degree of parallelism on the mssql. </p>\n\n<p>But how can i know how many the value of the maxdrop or max degree of parallelism i should set to? \nand how i check the maximum value of max degree of parallelism of my server can run?\nif i change the max degree of parallelism of the server, means all the store proc will run following that setting, so will it affect other performance? or i just need to set the maxdrop on certain store proc?</p>\n"},{"tags":["ios","performance","screenshot","blur","low-level"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":13088131,"title":"Fast screenshot ios","body":"<p>In my project I have to make a screenshot of the screen and apply blur to create the effect of frosted glass. Content can be moved under the glass and blured picture changed. </p>\n\n<p>I'v used Accelerate.framework to speedup blurring, also i,v used OpenGL to draw CIImage directly to GLView.</p>\n\n<p>Now I'm looking for a way to optimize getting screenshot of the screen.\nI use this method to get screenshot of some area at the bottom of the screen:</p>\n\n<pre><code> CGSize size = CGSizeMake(rect.size.width, rect.size.height);\n\n    // get screenshot of self.view\n    CGColorSpaceRef colorSpaceRef = CGColorSpaceCreateDeviceRGB();\n    CGContextRef ctx = CGBitmapContextCreate(nil, size.width, size.height, 8, 0, colorSpaceRef, kCGImageAlphaPremultipliedFirst);\n    CGContextClearRect(ctx, rect);\n    CGColorSpaceRelease(colorSpaceRef);\n    CGContextSetInterpolationQuality(ctx, kCGInterpolationNone);\n    CGContextSetShouldAntialias(ctx, NO);\n    CGContextSetAllowsAntialiasing(ctx, NO);\n    CGContextTranslateCTM(ctx, 0.0, someView.frame.size.height);\n    CGContextScaleCTM(ctx, 1, -1);\n\n    //add mask\n    CGImageRef maskImage = [UIImage imageNamed:@\"mask.png\"].CGImage;\n    CGContextClipToMask(ctx, rect, maskImage);\n\n    [someView.layer renderInContext:ctx];\n\n    //get screenshot image\n    CGImageRef imageRef = CGBitmapContextCreateImage(ctx);\n</code></pre>\n\n<p>It works fine and fast if self.view has 1-2 subviews, but if there are several subviews (or it is tableview), then everything starts to slow down.</p>\n\n<p>So i try to find a fast way to get pixels from some rect on screen. Maybe using a low-level API.</p>\n"},{"tags":["mysql","performance","join"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":30,"score":0,"question_id":13087608,"title":"How should I write this MySQL query containing multiple Left Joins","body":"<p>I have a query consisting of multiple joins and I am wondering whether it can be re-written to improve performance.</p>\n\n<p>I have 2 tables as follows (I have removed non-important columns for this example):</p>\n\n<p>slots</p>\n\n<pre><code>------------------------------------------\n| id   | name | slot_1 | slot_2 | slot_3 |\n------------------------------------------\n| 1    | Bob  | 1      | 2      | 3      |\n| 2    | Jim  | 4      | 3      | 3      |\n| 3    | Alf  | 1      | 2      | 5      |\n------------------------------------------\n</code></pre>\n\n<p>(There are 25 slots in total, each in it's own column)</p>\n\n<p>slot_details</p>\n\n<pre><code>-----------------------------------\n| id   | stat_1 | stat_2 | stat_3 |\n-----------------------------------\n| 1    | 1      | 5      | 6      |\n| 2    | 4      | 31     | 23     |\n| 3    | 6      | 5      | 7      |\n| 4    | 7      | 4      | 9      |\n| 5    | 2      | 3      | 5      |\n-----------------------------------\n</code></pre>\n\n<p>(There are 10 stats in total)</p>\n\n<p>The query is as follows:</p>\n\n<pre><code>SELECT\n    slots.name,\n    slot_1_details.stat_1 AS slot_1_stat_1,\n    slot_1_details.stat_2 AS slot_1_stat_2,\n    slot_1_details.stat_3 AS slot_1_stat_3,\n    slot_2_details.stat_1 AS slot_2_stat_1,\n    slot_2_details.stat_2 AS slot_2_stat_2,\n    slot_2_details.stat_3 AS slot_2_stat_3,\n    slot_3_details.stat_1 AS slot_3_stat_1,\n    slot_3_details.stat_2 AS slot_3_stat_2,\n    slot_3_details.stat_3 AS slot_3_stat_3\nFROM\n    slots\nLEFT JOIN\n    slot_details AS slot_1_details\nON (\n    slot_1_details.id = slots.slot_1\n)\nLEFT JOIN\n    slot_details AS slot_2_details\nON (\n    slot_2_details.id = slots.slot_2\n)\nLEFT JOIN\n    slot_details AS slot_3_details\nON (\n    slot_3_details.id = slots.slot_3\n)\nWHERE (\n    slots.id = 1\n)\n</code></pre>\n\n<p>The expected outcome of this query would be as follows:</p>\n\n<pre><code>| name | slot_1_stat_1 | slot_1_stat_2 | slot_1_stat_3 | slot_2_stat_1 | slot_2_stat_2 | slot_2_stat_3 | slot_3_stat_1 | slot_3_stat_2 | slot_3_stat_3 |\n|bob   | 1             | 5             | 6             | 4             | 31            | 23            | 6             | 5             | 7             |\n</code></pre>\n\n<p>Unfortunately I am not in a situation where I can change the tables.</p>\n\n<p>Thank you for the help!</p>\n"},{"tags":["c#","performance","timer"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":110,"score":0,"question_id":13056349,"title":"is timer efficient?","body":"<p>I have created an application that is used to read a mail box at certain intervals. If there is a new mail it downloads the attachment creates pdf files say 100 + combines it and mail it back to a particular list. Due to some server policies am in a position to convert it to a window service. I have used a timer my code given below</p>\n\n<pre><code>private System.Threading.Timer timer;\ntimer = new System.Threading.Timer(TimerTick, null, TimeSpan.Zero, TimeSpan.FromMinutes(1));\n\nvoid TimerTick(object state)\n{\n  var minute = DateTime.Now.Minute;\n  if (minute != lastMinute &amp;&amp; minute % 5 == 0)\n  {\n    //check mail here\n  }\n}\n</code></pre>\n\n<p>Is implementing a timer like this an efficient way of doing this? Is there any better way to handle this? I am worried about the performance because the applications need to run 24 x7 and hence can end up in utilizing more cpu memory if inefficient.</p>\n\n<p><em><strong>is timer the only best available option in this scenario ?</em></strong></p>\n"},{"tags":[".net","silverlight","performance","http","httpwebrequest"],"answer_count":3,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":569,"score":4,"question_id":2400949,"title":"What might cause the big overhead of making a HttpWebRequest call?","body":"<p>When I send/receive data using <b>HttpWebRequest</b> (on <b>Silverlight</b>) in small blocks, I measure the <em>very small throughput</em> of 500 bytes/s over a \"localhost\" connection. When sending the data in large blocks, I get 2 MB/s, which is some <b>5000 times faster</b>.  </p>\n\n<p>Does anyone know what could cause this incredibly big overhead?</p>\n\n<p><b>Additional info</b>: </p>\n\n<ul>\n<li>I'm using the HTTP POST method</li>\n<li>I did the performance measurement on both Firefox 3.6 and Internet Explorer 7. Both showed similar results.</li>\n<li>My CPU is loaded for only 10% (quad core, so 40% actually)</li>\n<li>WebClient showed similar results</li>\n<li><a href=\"http://stackoverflow.com/questions/2407418/what-can-be-done-to-speed-up-synchronous-wcf-calls\">WCF/SOAP showed similar results</a></li>\n</ul>\n\n<p><b>Update</b>: The Silverlight client-side code I use is essentially my own implementation of the WebClient class. The reason I wrote it is because I noticed the same performance problem with WebClient, and I thought that the HttpWebRequest would allow to tweak the performance issue. Regrettably, this did not work. The implementation is as follows:</p>\n\n<pre><code>public class HttpCommChannel\n{\n    public delegate void ResponseArrivedCallback(object requestContext, BinaryDataBuffer response);\n\n    public HttpCommChannel(ResponseArrivedCallback responseArrivedCallback)\n    {\n        this.responseArrivedCallback = responseArrivedCallback;\n        this.requestSentEvent = new ManualResetEvent(false);\n        this.responseArrivedEvent = new ManualResetEvent(true);\n    }\n\n    public void MakeRequest(object requestContext, string url, BinaryDataBuffer requestPacket)\n    {\n        responseArrivedEvent.WaitOne();\n        responseArrivedEvent.Reset();\n\n        this.requestMsg = requestPacket;\n        this.requestContext = requestContext;\n\n        this.webRequest = WebRequest.Create(url) as HttpWebRequest;\n        this.webRequest.AllowReadStreamBuffering = true;\n        this.webRequest.ContentType = \"text/plain\";\n        this.webRequest.Method = \"POST\";\n\n        this.webRequest.BeginGetRequestStream(new AsyncCallback(this.GetRequestStreamCallback), null);\n        this.requestSentEvent.WaitOne();\n    }\n\n    void GetRequestStreamCallback(IAsyncResult asynchronousResult)\n    {\n        System.IO.Stream postStream = webRequest.EndGetRequestStream(asynchronousResult);\n\n        postStream.Write(requestMsg.Data, 0, (int)requestMsg.Size);\n        postStream.Close();\n\n        requestSentEvent.Set();\n        webRequest.BeginGetResponse(new AsyncCallback(this.GetResponseCallback), null);\n    }\n\n    void GetResponseCallback(IAsyncResult asynchronousResult)\n    {\n        HttpWebResponse response = (HttpWebResponse)webRequest.EndGetResponse(asynchronousResult);\n        Stream streamResponse = response.GetResponseStream();\n        Dim.Ensure(streamResponse.CanRead);\n        byte[] readData = new byte[streamResponse.Length];\n        Dim.Ensure(streamResponse.Read(readData, 0, (int)streamResponse.Length) == streamResponse.Length);\n        streamResponse.Close();\n        response.Close();\n\n        webRequest = null;\n        responseArrivedEvent.Set();\n        responseArrivedCallback(requestContext, new BinaryDataBuffer(readData));\n    }\n\n    HttpWebRequest webRequest;\n    ManualResetEvent requestSentEvent;\n    BinaryDataBuffer requestMsg;\n    object requestContext;\n    ManualResetEvent responseArrivedEvent;\n    ResponseArrivedCallback responseArrivedCallback;\n}\n</code></pre>\n\n<p>I use this code to send data back and forth to an HTTP server.</p>\n\n<p><b>Update</b>: after extensive research, I conclude that <a href=\"http://stackoverflow.com/questions/2407418/what-can-be-done-to-speed-up-synchronous-wcf-calls\">the performance problem is inherent to Silverlight v3</a>.</p>\n"},{"tags":["performance","algorithm","primes"],"answer_count":19,"favorite_count":12,"up_vote_count":23,"down_vote_count":4,"view_count":13083,"score":19,"question_id":622,"title":"Most efficient code for the first 10000 prime numbers?","body":"<p>I want to print the first 10000 prime numbers.\r\nCan anyone give me the most efficient code for this?\r\nClarifications:</p>\r\n\r\n<ol>\r\n<li>It does not matter if your code is inefficient for n &gt;10000.</li>\r\n<li>The size of the code does not matter.</li>\r\n<li>You cannot just hard code the values in any manner.</li>\r\n</ol>"},{"tags":["performance","benchmarking","neo4j"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":1103,"score":0,"question_id":7866832,"title":"Neo4j Benchmark","body":"<p>Does anyone know a simple benchmark for neo4j?</p>\n\n<p>I tried to build it by myself but it spends 1s to create 1 node.. maybe it isn't the right way to build it!</p>\n"},{"tags":["windows","performance","winapi","file-io","io"],"answer_count":2,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":381,"score":2,"question_id":7430959,"title":"How to make CreateFile as fast as possible","body":"<p>I need to read the contents of several thousands of small files at startup. On linux, just using fopen and reading is very fast. On Windows, this happens very slowly.</p>\n\n<p>I have switched to using Overlapped I/O (Asynchronous I/O) using ReadFileEx, where Windows does a callback when data is ready to read.</p>\n\n<p>However, the actual thousands of calls to CreateFile itself are still a bottleneck. Note that I supply my own buffers, turn on the NO_BUFFERING flag, give the SERIAL hint, etc. However, the calls to CreateFile take several 10s of seconds, whereas on linux everything is done much faster.</p>\n\n<p>Is there anything that can be done to get these files ready for reading more quickly?</p>\n\n<p>The call to CreateFile is:</p>\n\n<pre><code>            hFile = CreateFile(szFullFileName,\n                GENERIC_READ,\n                FILE_SHARE_READ | FILE_SHARE_WRITE,\n                NULL,\n                OPEN_EXISTING,\n                FILE_ATTRIBUTE_NORMAL | FILE_FLAG_OVERLAPPED | FILE_FLAG_NO_BUFFERING | FILE_FLAG_SEQUENTIAL_SCAN,\n                NULL);\n</code></pre>\n"},{"tags":["sql","database","performance","postgresql"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":13085521,"title":"PostgreSQL code optimalization","body":"<p>I have this code:</p>\n\n<pre><code>SELECT \n    rv_storage.m_product_id AS n_product_id,\n    rv_storage.value,\n    rv_storage.name,\n    m_warehouse.name AS warehouse_name,\n    rv_storage.qtyonhand,\n    rv_transaction.m_transaction_id,\n\n    CASE WHEN rv_transaction.movementtype = 'V+' THEN movementdate\n    ELSE NULL END AS last_in,\n\n    CASE WHEN rv_transaction.movementtype = 'C-' THEN movementdate\n    ELSE NULL END AS last_out,\n\n    rv_transaction.movementagedays,\n\n    CASE WHEN (movementagedays &lt; -90) AND (movementagedays &gt;= -180)  THEN qtyonhand\n    ELSE NULL END AS more_than_90,\n\n    CASE WHEN movementagedays &lt; -180 THEN qtyonhand\n    ELSE NULL END AS more_than_180\nFROM\n    adempiere.rv_storage\n    INNER JOIN\n    adempiere.rv_transaction ON\n        rv_transaction.m_product_id = rv_storage.m_product_id \n        AND rv_transaction.movementagedays = (\n            SELECT MAX(movementagedays) \n            FROM adempiere.rv_transaction \n            WHERE\n                rv_transaction.m_product_id = rv_storage.m_product_id \n                AND rv_transaction.movementtype = 'C-'\n                OR rv_transaction.movementtype = 'V+'\n            )\n    INNER JOIN\n    adempiere.m_warehouse ON\n        m_warehouse.m_warehouse_id = rv_storage.m_warehouse_id\nWHERE rv_storage.m_product_id IN (\n    SELECT m_product_id\n    FROM adempiere.rv_transaction\n    WHERE movementagedays &lt; -90\n    )\nORDER BY n_product_id;\n</code></pre>\n\n<p>That results in following table:</p>\n\n<p><img src=\"http://img9.imageshack.us/img9/4506/table3n.png\" alt=\"table\"></p>\n\n<p>But on the server with live data (100k+ rows on each table ) it is too slow.\nCan somebody tell me how can the code be optimized?</p>\n\n<p>Thank You</p>\n"},{"tags":["java","performance","garbage-collection","jvm","tuning"],"answer_count":4,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":1445,"score":7,"question_id":7916723,"title":"Full GC becoming very frequent","body":"<p>I've got a Java webapp running on one tomcat instance. During peak times the webapp serves around 30 pages per second and normally around 15.</p>\n\n<p>My environment is:</p>\n\n<pre><code>O/S: SUSE Linux Enterprise Server 10 (x86_64)\nRAM: 16GB\n\nserver: Tomcat 6.0.20\nJVM: Java HotSpot(TM) 64-Bit Server VM 1.6.0_14\nJVM options:\nCATALINA_OPTS=\"-Xms512m -Xmx1024m -XX:PermSize=128m -XX:MaxPermSize=256m\n               -XX:+UseParallelGC\n               -Djava.awt.headless=true\n               -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps\"\nJAVA_OPTS=\"-server\"\n</code></pre>\n\n<p>After a couple of days of uptime the Full GC starts occurring more frequently and it becomes a serious problem to the application's availability. After a tomcat restart the problem goes away but, of course, returns after 5 to 10 or 30 days (not consistent).</p>\n\n<p>The Full GC log before and after a restart is at <a href=\"http://pastebin.com/raw.php?i=4NtkNXmi\" rel=\"nofollow\">http://pastebin.com/raw.php?i=4NtkNXmi</a></p>\n\n<p>It shows a log before the restart at 6.6 days uptime where the app was suffering because Full GC needed 2.5 seconds and was happening every ~6 secs.</p>\n\n<p>Then it shows a log just after the restart where Full GC only happened every 5-10 minutes.</p>\n\n<p>I've got two dumps using <code>jmap -dump:format=b,file=dump.hprof PID</code> when the Full GCs where occurring (I'm not sure whether I got them exactly right when a Full GC was occurring or between 2 Full GCs) and opened them in <a href=\"http://www.eclipse.org/mat/\" rel=\"nofollow\">http://www.eclipse.org/mat/</a> but didn't get anything useful in Leak Suspects:</p>\n\n<ul>\n<li>60MB: 1 instance of \"org.hibernate.impl.SessionFactoryImpl\" (I use hibernate with ehcache)</li>\n<li>80MB: 1,024 instances of \"org.apache.tomcat.util.threads.ThreadWithAttributes\" (these are probably the 1024 workers of tomcat)</li>\n<li>45MB: 37 instances of \"net.sf.ehcache.store.compound.impl.MemoryOnlyStore\" (these should be my ~37 cache regions in ehcache)</li>\n</ul>\n\n<p>Note that I never get an OutOfMemoryError.</p>\n\n<p>Any ideas on where should I look next?</p>\n\n<p>thanks</p>\n"},{"tags":["linux","performance","ubuntu","kernel"],"answer_count":4,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":740,"score":6,"question_id":5399030,"title":"clone()/fork()/process creation is slow on some machines","body":"<p>Creating new processes is very slow on some of my machines, and not others. </p>\n\n<p>The machines are all similar, and some of the slow machines are running the exact same workloads on the same hardware and kernel (2.6.32-26, Ubuntu 10.04) as some of the fast machines. Tasks that do not involve process creation are the same speeds on all machines.</p>\n\n<p>For example, this program executes ~50 times slower on the affected machines:</p>\n\n<pre><code>int main()\n{\n    int i;\n    for (i=0;i&lt;10000;i++)\n    {\n        int p = fork();\n        if (!p) exit(0);\n        waitpid(p);\n    }\n    return 0;\n}\n</code></pre>\n\n<p>What could be causing task creation to be much slower, and what other differences could I look for in the machines?</p>\n\n<p>Edit1: Running bash scripts (as they spawn a lot of subprocesses) is also very slow on these machines, and strace on the slow scripts shows the slowdown in the <code>clone()</code> kernel call.</p>\n\n<p>Edit2: <code>vmstat</code> doesn't show any significant differences on the fast vs slow machines. They all have more than enough RAM for their workloads and don't go to swap.</p>\n\n<p>Edit3: I don't see anything suspicious in <code>dmesg</code></p>\n\n<p>Edit4: I'm not sure why this is on stackoverflow now, I'm not asking about the example program above (just using it to demonstrate the problem), but linux administration/tuning, but if people think it belongs here, cool.</p>\n"},{"tags":["performance","script","multiple","requests","loadui"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":13,"score":1,"question_id":13084710,"title":"How to run multiple test cases at the same time in LoadUI?","body":"<p>I have created a testsuite in SoapUI and a testsuite contains two testcases. I need to execute a testsuite at the same time from loadUI.</p>\n\n<p>Either command line or from loadUI.</p>\n\n<p>Thanks</p>\n"},{"tags":["performance","matlab","optimization","vectorization","mex"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":65,"score":2,"question_id":13070637,"title":"Optimizing a Vectorized Matlab Function","body":"<p>When i run profiler it tell me that the most time consuming code is the function <code>vdist</code>. Its a program that measures distance between two points on earth considering earth as an ellipsoid. The code looks standard and i don't know where and how it can be improved upon. The initial comments say, it has already been vectorized. Is there a counterpart to it  in some other language which can be used as a MEX file. All i want is improvement in terms of time efficiency. Here is a link to the code from Matlab FEX. </p>\n\n<p><a href=\"http://www.mathworks.com/matlabcentral/fileexchange/8607-vectorized-geodetic-distance-and-azimuth-on-the-wgs84-earth-ellipsoid/content/vdist.m\" rel=\"nofollow\">http://www.mathworks.com/matlabcentral/fileexchange/8607-vectorized-geodetic-distance-and-azimuth-on-the-wgs84-earth-ellipsoid/content/vdist.m</a>   </p>\n\n<p>The function is called from within a loop as- (You can find the function as its the most time consuming line here)</p>\n\n<pre><code>              109 for i=1:polySize \n              110    % find the two vectors needed\n       11755  111    if i~=1 \n0.02   11503  112        if i&lt;polySize \n0.02   11251  113         p0=Polygon(i,:); p1=Polygon(i-1,:); p2=Polygon(i+1,:);    \n         252  114        else \n         252  115         p0=Polygon(i,:); p1=Polygon(i-1,:); p2=Polygon(1,:); %special case for i=polySize \n         252  116        end \n         252  117    else \n         252  118         p0=Polygon(i,:); p1=Polygon(polySize,:); p2=Polygon(i+1,:); %special case for i=1 \n         252  119    end \n 0.02  11755  120    Vector1=(p0-p1); Vector2=(p0-p2); \n 0.06  11755  121    if ~(isequal(Vector1,Vector2) || isequal(Vector1,ZeroVec) || isequal(Vector2,ZeroVec)); \n              122        %determine normals and normalise and\n 0.17  11755  123        NV1=rotateVector(Vector1, pi./2); NV2=rotateVector(Vector2, -pi./2); \n 0.21  11755  124        NormV1=normaliseVector(NV1); NormV2=normaliseVector(NV2); \n              125        %determine rotation by means of the atan2 (because sign matters!)\n       11755  126        totalRotation = vectorAngle(NormV2, NormV1); % Bestimme den Winkel totalRotation zwischen den normierten Vektoren \n       11755  127      if totalRotation&lt;10 \n       11755  128          totalRotation=totalRotation*50; \n       11755  129      end \n0.01   11755  130      for res=1:6 \n0.07   70530  131         U_neu=p0+NV1; \n17.01  70530  132         [pos,a12] = vdist(p0(:,2),p0(:,1),U_neu(:,2),U_neu(:,1)); \n0.02   70530  133         a12=a12+1/6.*res*totalRotation; \n       70530  134         ddist=1852*safety_distance; \n4.88   70530  135         [lat2,lon2] = vreckon(p0(:,2),p0(:,1),ddist, a12); \n0.15   70530  136         extendedPoly(f,:)=[lon2,lat2];f=f+1; \n&lt; 0.01 70530  137      end \n       11755  138    end \n       11755  139 end \n</code></pre>\n"},{"tags":["java",".net","string","performance"],"answer_count":5,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":178,"score":3,"question_id":12590097,"title":"Is checking whether string.length == 0 still faster than checking string == \"\"?","body":"<p>I read from a programming book about 7-8 years ago that checking <code>string.length == 0</code> is a faster way to check for empty strings. I'm wondering if that statement still holds true today (or if it has ever been true at all), because I personally think <code>string == \"\"</code> is more straightforward and more readable. I mostly deal with high-level languages such as .NET and java.</p>\n"},{"tags":["performance","hibernate","oracle11g"],"answer_count":0,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":34,"score":4,"question_id":13074500,"title":"Hibernate Query Running Slow","body":"<p>I am currently facing a problem with a slow running Hibernate query that is being kicked off from a Java program. It is making a call to Oracle 11g at the back end.</p>\n\n<p>This query is taking anything from 40-90 seconds the first time it is run. On subsequent executions however the query is returned in a fraction of the time (and I don't even see the database being hit, so assume that hibernate is caching it).</p>\n\n<p>If I copy and paste the query from Enterprise Manager into a SQL client and run the very same query directly (even changing a few parameters) the query returns in a fraction of a second.</p>\n\n<p>If I look at the performance tuning tab in EM I see that the time taken is primarily taken up with User I/O Waits (97.5%), and CPU (2.5%). Could this mean that the fetch size I am using in hibernate is configured at too small a value?</p>\n\n<p>If there is any other information you might need to help me get to the bottom of this issue then please let me know.</p>\n\n<p>=====</p>\n\n<h2>Additional information:</h2>\n\n<p>We do have an index on the table and I can see that it is being used as part of the execution of the query, unfortunately it isn't very readable but I am not sure how else to include it:</p>\n\n<p>Id  Operation   Name    Rows\n(Estim)     Cost    Time\nActive(s)   Start\nActive  Execs   Rows\n(Actual)    Read\nReqs    Read\nBytes   Mem\n(Max)   Activity\n(%)     Activity Detail\n(# samples)\n0   SELECT STATEMENT                        1<br>\n1   . FILTER                        1<br>\n2   .. HASH JOIN RIGHT OUTER        2674    7223    1   +4  1   0           1M<br>\n3   ... TABLE ACCESS FULL   TOTEM_EQ_EXPIRYCODES    475     4   1   +4  1   481<br>\n4   ... HASH JOIN RIGHT OUTER       2674    7219    1   +4  1   0           399K<br>\n5   .... TABLE ACCESS BY INDEX ROWID    TOTEM_EQ_UNDERLYINGS    1   2   1   +4  1   1<br>\n6   ..... INDEX UNIQUE SCAN     TOTEM_EQ_UND_PK     1   1   1   +4  1   1<br>\n7   .... NESTED LOOPS                       1<br>\n8   ..... NESTED LOOPS      2674    7216    42  +4  1   0<br>\n9   ...... TABLE ACCESS BY GLOBAL INDEX ROWID   EQUITIES_MONTHLY_INSTRUMENTS    2671    1871    45  +1  1   8438    3517    27MB        24.44   db file sequential read (11)\n10  ....... INDEX RANGE SCAN    EQ_MON_INS_UNDERLYING_INDX  2671    12  42  +4  1   8438    27  216KB<br>\n11  ...... PARTITION RANGE ITERATOR         1   2           8438<br>\n12  ....... INDEX RANGE SCAN    EQ_MON_RESULT_INSPT_UNQ     1   2   44  +2  8438    0   5403    42MB        75.56   Cpu (1)\ndb file sequential read (33)\n13  ..... TABLE ACCESS BY LOCAL INDEX ROWID     EQUITIES_MONTHLY_RESULTS    1   3                   </p>\n\n<p>Here are the Global stats:</p>\n\n<p>Elapsed\nTime(s)     Cpu\nTime(s)     IO\nWaits(s)    Fetch\nCalls   Buffer\nGets    Read\nReqs    Read\nBytes\n45  0.73    45  1   22980   9740    76MB            </p>\n"},{"tags":["c#",".net","performance","datetime","optimization"],"answer_count":3,"favorite_count":2,"up_vote_count":10,"down_vote_count":0,"view_count":752,"score":10,"question_id":1561791,"title":"Optimizing alternatives to DateTime.Now","body":"<p>A colleague and I are going back and forth on this issue and I'm hoping to get some outside opinions as to whether or not my proposed solution is a good idea.</p>\n\n<p>First, a disclaimer: I realize that the notion of \"optimizing <code>DateTime.Now</code>\" sounds crazy to some of you. I have a couple of pre-emptive defenses:</p>\n\n<ol>\n<li>I sometimes suspect that those people who always say, \"Computers are fast; readability <em>always</em> comes before optimization\" are often speaking from experience developing applications where performance, though it may be <em>important</em>, is not <strong>critical</strong>. I'm talking about needing things to happen as close to instantaneously as possible -- like, within nanoseconds (in certain industries, this <em>does</em> matter -- for instance, real-time high-frequency trading).</li>\n<li>Even with that in mind, the alternative approach I describe below is, in fact, quite readable. It is not a bizarre hack, just a simple method that works reliably and fast.</li>\n<li>We <strong>have</strong> runs tests. <code>DateTime.Now</code> <strong>is</strong> slow (relatively speaking). The method below <strong>is</strong> faster.</li>\n</ol>\n\n<p>Now, onto the question itself.</p>\n\n<p>Basically, from tests, we've found that <code>DateTime.Now</code> takes roughly 25 ticks (around 2.5 microseconds) to run. This is averaged out over thousands to millions of calls, of course. It appears that the first call actually takes a significant amount of time and subsequent calls are much faster. But still, 25 ticks is the average.</p>\n\n<p>However, my colleague and I noticed that <code>DateTime.UtcNow</code> takes substantially less time to run -- on average, a mere 0.03 microseconds.</p>\n\n<p><em>Given that our application will never be running while there is a change in Daylight Savings Time</em>, my suggestion was to create the following class:</p>\n\n<pre><code>public static class FastDateTime {\n    public static TimeSpan LocalUtcOffset { get; private set; }\n\n    public static DateTime Now {\n        get { return DateTime.UtcNow + LocalUtcOffset; }\n    }\n\n    static FastDateTime() {\n        LocalUtcOffset = TimeZone.CurrentTimeZone.GetUtcOffset(DateTime.Now);\n    }\n}\n</code></pre>\n\n<p>In other words, determine the UTC offset for the local timezone <strong>once</strong> -- at startup -- and from that point onward leverage the speed of <code>DateTime.UtcNow</code> to get the current time a lot faster via <code>FastDateTime.Now</code>.</p>\n\n<p>I could see this being a problem if the UTC offset changed during the time the application was running (if, for example, the application was running overnight); but as I stated already, in <em>our</em> case, that will not happen.</p>\n\n<p>My colleague has a different idea about how to do it, which is a bit too involved for me to explain here. Ultimately, <em>as far as I can tell</em>, both of our approaches return an accurate result, mine being slightly faster (~0.07 microseconds vs. ~0.21 microseconds).</p>\n\n<p>What I want to know is:</p>\n\n<ol>\n<li>Am I missing something here? Given the abovementioned fact that the application will only run within the time frame of a single date, is <code>FastDateTime.Now</code> safe?</li>\n<li>Can anyone else perhaps think of an even <em>faster</em> way of getting the current time?</li>\n</ol>\n"},{"tags":["performance","query","tsql","sql-server-2005"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":74,"score":3,"question_id":12854990,"title":"Why would ISNULL improve the performance of a query?","body":"<p>We're currently trying to speedup some queries and we've run into something which I (not a DBA just a .NET developer) cannot explain or comprehent. We're running this query on SQL Server 2005.</p>\n\n<p>We have the following query (made small and simple for the sake of argument);</p>\n\n<pre><code>SELECT    \n    *\nFROM    \n    RandomTable \nWHERE   \n    MoneyColumn &lt;&gt; 0\nGROUP BY\n    SomeColumn\n</code></pre>\n\n<p>This query run in around <strong>three</strong> seconds, then we randomly tried to following to speed it up (shot in the dark really)</p>\n\n<pre><code>SELECT    \n    *\nFROM    \n    RandomTable \nWHERE   \n    isnull(MoneyColumn,0) &lt;&gt; 0\nGROUP BY\n    SomeColumn\n</code></pre>\n\n<p>This reduces the query speed to around <strong>one</strong> second..</p>\n\n<p>This column has no NULL values (yet due to the database design beeing HORRIBLE) it is however NULLABLE...</p>\n\n<p>Is the fact that it's NULLABLE making SQL Server do something to account for this making it slow where the ISNULL isn't beeing mentioned? I simply have no idea why the ISNULL would make it perform faster (and by such a big margin). I'd think SQL would actually have more to do when there is a ISNULL statement in the query.</p>\n\n<p>Can anyone shed some light on this?</p>\n\n<p><strong>EDIT</strong> Execution plans added</p>\n\n<p><strong>With ISNULL</strong></p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-16\"?&gt;\n&lt;ShowPlanXML xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" Version=\"1.0\" Build=\"9.00.5000.00\" xmlns=\"http://schemas.microsoft.com/sqlserver/2004/07/showplan\"&gt;\n  &lt;BatchSequence&gt;\n    &lt;Batch&gt;\n      &lt;Statements&gt;\n        &lt;StmtSimple StatementCompId=\"1\" StatementEstRows=\"9019.76\" StatementId=\"1\" StatementOptmLevel=\"FULL\" StatementSubTreeCost=\"1.48105\" StatementText=\"SELECT    debiteur_id, MIN(Faktuurdatum) AS OldestOpenInvoiceDate, ISNULL(SUM(Totaal_Open),0) AS TotalOpenAmount&amp;#xD;&amp;#xA;FROM    dbo.tbl_Faktuur &amp;#xD;&amp;#xA;WHERE   (Afgehandeld_NeeJa = 0 OR Afgehandeld_NeeJa IS NULL)&amp;#xD;&amp;#xA;AND        (ISNULL(Totaal_Open,0) &amp;lt;&amp;gt; 0) &amp;#xD;&amp;#xA;--AND        (Totaal_Open &amp;lt;&amp;gt; 0) &amp;#xD;&amp;#xA;GROUP BY debiteur_id\" StatementType=\"SELECT\"&gt;\n          &lt;StatementSetOptions ANSI_NULLS=\"false\" ANSI_PADDING=\"false\" ANSI_WARNINGS=\"false\" ARITHABORT=\"true\" CONCAT_NULL_YIELDS_NULL=\"false\" NUMERIC_ROUNDABORT=\"false\" QUOTED_IDENTIFIER=\"false\" /&gt;\n          &lt;QueryPlan DegreeOfParallelism=\"1\" MemoryGrant=\"1520\" CachedPlanSize=\"54\" CompileTime=\"11\" CompileCPU=\"11\" CompileMemory=\"704\"&gt;\n            &lt;RelOp AvgRowSize=\"23\" EstimateCPU=\"0.000901976\" EstimateIO=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimateRows=\"9019.76\" LogicalOp=\"Compute Scalar\" NodeId=\"0\" Parallel=\"false\" PhysicalOp=\"Compute Scalar\" EstimatedTotalSubtreeCost=\"1.48105\"&gt;\n              &lt;OutputList&gt;\n                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                &lt;ColumnReference Column=\"Expr1003\" /&gt;\n                &lt;ColumnReference Column=\"Expr1005\" /&gt;\n              &lt;/OutputList&gt;\n              &lt;ComputeScalar&gt;\n                &lt;DefinedValues&gt;\n                  &lt;DefinedValue&gt;\n                    &lt;ColumnReference Column=\"Expr1005\" /&gt;\n                    &lt;ScalarOperator ScalarString=\"isnull([Expr1004],($0.0000))\"&gt;\n                      &lt;Intrinsic FunctionName=\"isnull\"&gt;\n                        &lt;ScalarOperator&gt;\n                          &lt;Identifier&gt;\n                            &lt;ColumnReference Column=\"Expr1004\" /&gt;\n                          &lt;/Identifier&gt;\n                        &lt;/ScalarOperator&gt;\n                        &lt;ScalarOperator&gt;\n                          &lt;Const ConstValue=\"($0.0000)\" /&gt;\n                        &lt;/ScalarOperator&gt;\n                      &lt;/Intrinsic&gt;\n                    &lt;/ScalarOperator&gt;\n                  &lt;/DefinedValue&gt;\n                &lt;/DefinedValues&gt;\n                &lt;RelOp AvgRowSize=\"23\" EstimateCPU=\"0.291662\" EstimateIO=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimateRows=\"9019.76\" LogicalOp=\"Compute Scalar\" NodeId=\"1\" Parallel=\"false\" PhysicalOp=\"Compute Scalar\" EstimatedTotalSubtreeCost=\"1.48014\"&gt;\n                  &lt;OutputList&gt;\n                    &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                    &lt;ColumnReference Column=\"Expr1003\" /&gt;\n                    &lt;ColumnReference Column=\"Expr1004\" /&gt;\n                  &lt;/OutputList&gt;\n                  &lt;ComputeScalar&gt;\n                    &lt;DefinedValues&gt;\n                      &lt;DefinedValue&gt;\n                        &lt;ColumnReference Column=\"Expr1004\" /&gt;\n                        &lt;ScalarOperator ScalarString=\"CASE WHEN [Expr1013]=(0) THEN NULL ELSE [Expr1014] END\"&gt;\n                          &lt;IF&gt;\n                            &lt;Condition&gt;\n                              &lt;ScalarOperator&gt;\n                                &lt;Compare CompareOp=\"EQ\"&gt;\n                                  &lt;ScalarOperator&gt;\n                                    &lt;Identifier&gt;\n                                      &lt;ColumnReference Column=\"Expr1013\" /&gt;\n                                    &lt;/Identifier&gt;\n                                  &lt;/ScalarOperator&gt;\n                                  &lt;ScalarOperator&gt;\n                                    &lt;Const ConstValue=\"(0)\" /&gt;\n                                  &lt;/ScalarOperator&gt;\n                                &lt;/Compare&gt;\n                              &lt;/ScalarOperator&gt;\n                            &lt;/Condition&gt;\n                            &lt;Then&gt;\n                              &lt;ScalarOperator&gt;\n                                &lt;Const ConstValue=\"NULL\" /&gt;\n                              &lt;/ScalarOperator&gt;\n                            &lt;/Then&gt;\n                            &lt;Else&gt;\n                              &lt;ScalarOperator&gt;\n                                &lt;Identifier&gt;\n                                  &lt;ColumnReference Column=\"Expr1014\" /&gt;\n                                &lt;/Identifier&gt;\n                              &lt;/ScalarOperator&gt;\n                            &lt;/Else&gt;\n                          &lt;/IF&gt;\n                        &lt;/ScalarOperator&gt;\n                      &lt;/DefinedValue&gt;\n                    &lt;/DefinedValues&gt;\n                    &lt;RelOp AvgRowSize=\"23\" EstimateCPU=\"0.291662\" EstimateIO=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimateRows=\"9019.76\" LogicalOp=\"Aggregate\" NodeId=\"2\" Parallel=\"false\" PhysicalOp=\"Hash Match\" EstimatedTotalSubtreeCost=\"1.48014\"&gt;\n                      &lt;OutputList&gt;\n                        &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                        &lt;ColumnReference Column=\"Expr1003\" /&gt;\n                        &lt;ColumnReference Column=\"Expr1013\" /&gt;\n                        &lt;ColumnReference Column=\"Expr1014\" /&gt;\n                      &lt;/OutputList&gt;\n                      &lt;MemoryFractions Input=\"1\" Output=\"1\" /&gt;\n                      &lt;RunTimeInformation&gt;\n                        &lt;RunTimeCountersPerThread Thread=\"0\" ActualRows=\"156794\" ActualEndOfScans=\"1\" ActualExecutions=\"1\" /&gt;\n                      &lt;/RunTimeInformation&gt;\n                      &lt;Hash&gt;\n                        &lt;DefinedValues&gt;\n                          &lt;DefinedValue&gt;\n                            &lt;ColumnReference Column=\"Expr1003\" /&gt;\n                            &lt;ScalarOperator ScalarString=\"MIN([directpay].[dbo].[tbl_Faktuur].[Faktuurdatum])\"&gt;\n                              &lt;Aggregate AggType=\"MIN\" Distinct=\"false\"&gt;\n                                &lt;ScalarOperator&gt;\n                                  &lt;Identifier&gt;\n                                    &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Faktuurdatum\" /&gt;\n                                  &lt;/Identifier&gt;\n                                &lt;/ScalarOperator&gt;\n                              &lt;/Aggregate&gt;\n                            &lt;/ScalarOperator&gt;\n                          &lt;/DefinedValue&gt;\n                          &lt;DefinedValue&gt;\n                            &lt;ColumnReference Column=\"Expr1013\" /&gt;\n                            &lt;ScalarOperator ScalarString=\"COUNT_BIG([directpay].[dbo].[tbl_Faktuur].[Totaal_Open])\"&gt;\n                              &lt;Aggregate AggType=\"COUNT_BIG\" Distinct=\"false\"&gt;\n                                &lt;ScalarOperator&gt;\n                                  &lt;Identifier&gt;\n                                    &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                                  &lt;/Identifier&gt;\n                                &lt;/ScalarOperator&gt;\n                              &lt;/Aggregate&gt;\n                            &lt;/ScalarOperator&gt;\n                          &lt;/DefinedValue&gt;\n                          &lt;DefinedValue&gt;\n                            &lt;ColumnReference Column=\"Expr1014\" /&gt;\n                            &lt;ScalarOperator ScalarString=\"SUM([directpay].[dbo].[tbl_Faktuur].[Totaal_Open])\"&gt;\n                              &lt;Aggregate AggType=\"SUM\" Distinct=\"false\"&gt;\n                                &lt;ScalarOperator&gt;\n                                  &lt;Identifier&gt;\n                                    &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                                  &lt;/Identifier&gt;\n                                &lt;/ScalarOperator&gt;\n                              &lt;/Aggregate&gt;\n                            &lt;/ScalarOperator&gt;\n                          &lt;/DefinedValue&gt;\n                        &lt;/DefinedValues&gt;\n                        &lt;HashKeysBuild&gt;\n                          &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                        &lt;/HashKeysBuild&gt;\n                        &lt;RelOp AvgRowSize=\"23\" EstimateCPU=\"0.255\" EstimateIO=\"0.634196\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimateRows=\"27420\" LogicalOp=\"Index Seek\" NodeId=\"4\" Parallel=\"false\" PhysicalOp=\"Index Seek\" EstimatedTotalSubtreeCost=\"0.889196\"&gt;\n                          &lt;OutputList&gt;\n                            &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                            &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Faktuurdatum\" /&gt;\n                            &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                          &lt;/OutputList&gt;\n                          &lt;RunTimeInformation&gt;\n                            &lt;RunTimeCountersPerThread Thread=\"0\" ActualRows=\"298726\" ActualEndOfScans=\"1\" ActualExecutions=\"1\" /&gt;\n                          &lt;/RunTimeInformation&gt;\n                          &lt;IndexScan Ordered=\"true\" ScanDirection=\"FORWARD\" ForcedIndex=\"false\" NoExpandHint=\"false\"&gt;\n                            &lt;DefinedValues&gt;\n                              &lt;DefinedValue&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                              &lt;/DefinedValue&gt;\n                              &lt;DefinedValue&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Faktuurdatum\" /&gt;\n                              &lt;/DefinedValue&gt;\n                              &lt;DefinedValue&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                              &lt;/DefinedValue&gt;\n                            &lt;/DefinedValues&gt;\n                            &lt;Object Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Index=\"[_dta_index_tbl_Faktuur_5_583009158__K13_K9_K19_K2_5]\" /&gt;\n                            &lt;SeekPredicates&gt;\n                              &lt;SeekPredicate&gt;\n                                &lt;Prefix ScanType=\"EQ\"&gt;\n                                  &lt;RangeColumns&gt;\n                                    &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Afgehandeld_NeeJa\" /&gt;\n                                  &lt;/RangeColumns&gt;\n                                  &lt;RangeExpressions&gt;\n                                    &lt;ScalarOperator ScalarString=\"(0)\"&gt;\n                                      &lt;Const ConstValue=\"(0)\" /&gt;\n                                    &lt;/ScalarOperator&gt;\n                                  &lt;/RangeExpressions&gt;\n                                &lt;/Prefix&gt;\n                              &lt;/SeekPredicate&gt;\n                            &lt;/SeekPredicates&gt;\n                            &lt;Predicate&gt;\n                              &lt;ScalarOperator ScalarString=\"isnull([directpay].[dbo].[tbl_Faktuur].[Totaal_Open],($0.0000))&amp;lt;($0.0000) OR isnull([directpay].[dbo].[tbl_Faktuur].[Totaal_Open],($0.0000))&amp;gt;($0.0000)\"&gt;\n                                &lt;Logical Operation=\"OR\"&gt;\n                                  &lt;ScalarOperator&gt;\n                                    &lt;Compare CompareOp=\"LT\"&gt;\n                                      &lt;ScalarOperator&gt;\n                                        &lt;Intrinsic FunctionName=\"isnull\"&gt;\n                                          &lt;ScalarOperator&gt;\n                                            &lt;Identifier&gt;\n                                              &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                                            &lt;/Identifier&gt;\n                                          &lt;/ScalarOperator&gt;\n                                          &lt;ScalarOperator&gt;\n                                            &lt;Const ConstValue=\"($0.0000)\" /&gt;\n                                          &lt;/ScalarOperator&gt;\n                                        &lt;/Intrinsic&gt;\n                                      &lt;/ScalarOperator&gt;\n                                      &lt;ScalarOperator&gt;\n                                        &lt;Const ConstValue=\"($0.0000)\" /&gt;\n                                      &lt;/ScalarOperator&gt;\n                                    &lt;/Compare&gt;\n                                  &lt;/ScalarOperator&gt;\n                                  &lt;ScalarOperator&gt;\n                                    &lt;Compare CompareOp=\"GT\"&gt;\n                                      &lt;ScalarOperator&gt;\n                                        &lt;Intrinsic FunctionName=\"isnull\"&gt;\n                                          &lt;ScalarOperator&gt;\n                                            &lt;Identifier&gt;\n                                              &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                                            &lt;/Identifier&gt;\n                                          &lt;/ScalarOperator&gt;\n                                          &lt;ScalarOperator&gt;\n                                            &lt;Const ConstValue=\"($0.0000)\" /&gt;\n                                          &lt;/ScalarOperator&gt;\n                                        &lt;/Intrinsic&gt;\n                                      &lt;/ScalarOperator&gt;\n                                      &lt;ScalarOperator&gt;\n                                        &lt;Const ConstValue=\"($0.0000)\" /&gt;\n                                      &lt;/ScalarOperator&gt;\n                                    &lt;/Compare&gt;\n                                  &lt;/ScalarOperator&gt;\n                                &lt;/Logical&gt;\n                              &lt;/ScalarOperator&gt;\n                            &lt;/Predicate&gt;\n                          &lt;/IndexScan&gt;\n                        &lt;/RelOp&gt;\n                      &lt;/Hash&gt;\n                    &lt;/RelOp&gt;\n                  &lt;/ComputeScalar&gt;\n                &lt;/RelOp&gt;\n              &lt;/ComputeScalar&gt;\n            &lt;/RelOp&gt;\n          &lt;/QueryPlan&gt;\n        &lt;/StmtSimple&gt;\n      &lt;/Statements&gt;\n    &lt;/Batch&gt;\n  &lt;/BatchSequence&gt;\n&lt;/ShowPlanXML&gt;\n</code></pre>\n\n<p><strong>Without ISNULL</strong></p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-16\"?&gt;\n&lt;ShowPlanXML xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" Version=\"1.0\" Build=\"9.00.5000.00\" xmlns=\"http://schemas.microsoft.com/sqlserver/2004/07/showplan\"&gt;\n  &lt;BatchSequence&gt;\n    &lt;Batch&gt;\n      &lt;Statements&gt;\n        &lt;StmtSimple StatementCompId=\"1\" StatementEstRows=\"1322.43\" StatementId=\"1\" StatementOptmLevel=\"FULL\" StatementOptmEarlyAbortReason=\"GoodEnoughPlanFound\" StatementSubTreeCost=\"0.274954\" StatementText=\"SELECT    debiteur_id, MIN(Faktuurdatum) AS OldestOpenInvoiceDate, ISNULL(SUM(Totaal_Open),0) AS TotalOpenAmount&amp;#xD;&amp;#xA;FROM    dbo.tbl_Faktuur &amp;#xD;&amp;#xA;WHERE   (Afgehandeld_NeeJa = 0 OR Afgehandeld_NeeJa IS NULL)&amp;#xD;&amp;#xA;--AND        (ISNULL(Totaal_Open,0) &amp;lt;&amp;gt; 0) &amp;#xD;&amp;#xA;AND        (Totaal_Open &amp;lt;&amp;gt; 0) &amp;#xD;&amp;#xA;GROUP BY debiteur_id\" StatementType=\"SELECT\"&gt;\n          &lt;StatementSetOptions ANSI_NULLS=\"false\" ANSI_PADDING=\"false\" ANSI_WARNINGS=\"false\" ARITHABORT=\"true\" CONCAT_NULL_YIELDS_NULL=\"false\" NUMERIC_ROUNDABORT=\"false\" QUOTED_IDENTIFIER=\"false\" /&gt;\n          &lt;QueryPlan CachedPlanSize=\"47\" CompileTime=\"9\" CompileCPU=\"9\" CompileMemory=\"528\"&gt;\n            &lt;RelOp AvgRowSize=\"23\" EstimateCPU=\"0.000132243\" EstimateIO=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimateRows=\"1322.43\" LogicalOp=\"Compute Scalar\" NodeId=\"0\" Parallel=\"false\" PhysicalOp=\"Compute Scalar\" EstimatedTotalSubtreeCost=\"0.274954\"&gt;\n              &lt;OutputList&gt;\n                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                &lt;ColumnReference Column=\"Expr1003\" /&gt;\n                &lt;ColumnReference Column=\"Expr1005\" /&gt;\n              &lt;/OutputList&gt;\n              &lt;ComputeScalar&gt;\n                &lt;DefinedValues&gt;\n                  &lt;DefinedValue&gt;\n                    &lt;ColumnReference Column=\"Expr1005\" /&gt;\n                    &lt;ScalarOperator ScalarString=\"isnull([Expr1004],($0.0000))\"&gt;\n                      &lt;Intrinsic FunctionName=\"isnull\"&gt;\n                        &lt;ScalarOperator&gt;\n                          &lt;Identifier&gt;\n                            &lt;ColumnReference Column=\"Expr1004\" /&gt;\n                          &lt;/Identifier&gt;\n                        &lt;/ScalarOperator&gt;\n                        &lt;ScalarOperator&gt;\n                          &lt;Const ConstValue=\"($0.0000)\" /&gt;\n                        &lt;/ScalarOperator&gt;\n                      &lt;/Intrinsic&gt;\n                    &lt;/ScalarOperator&gt;\n                  &lt;/DefinedValue&gt;\n                &lt;/DefinedValues&gt;\n                &lt;RelOp AvgRowSize=\"23\" EstimateCPU=\"0.167304\" EstimateIO=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimateRows=\"1322.43\" LogicalOp=\"Aggregate\" NodeId=\"1\" Parallel=\"false\" PhysicalOp=\"Hash Match\" EstimatedTotalSubtreeCost=\"0.274822\"&gt;\n                  &lt;OutputList&gt;\n                    &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                    &lt;ColumnReference Column=\"Expr1003\" /&gt;\n                    &lt;ColumnReference Column=\"Expr1004\" /&gt;\n                  &lt;/OutputList&gt;\n                  &lt;MemoryFractions Input=\"0\" Output=\"0\" /&gt;\n                  &lt;Hash&gt;\n                    &lt;DefinedValues&gt;\n                      &lt;DefinedValue&gt;\n                        &lt;ColumnReference Column=\"Expr1003\" /&gt;\n                        &lt;ScalarOperator ScalarString=\"MIN([directpay].[dbo].[tbl_Faktuur].[Faktuurdatum])\"&gt;\n                          &lt;Aggregate AggType=\"MIN\" Distinct=\"false\"&gt;\n                            &lt;ScalarOperator&gt;\n                              &lt;Identifier&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Faktuurdatum\" /&gt;\n                              &lt;/Identifier&gt;\n                            &lt;/ScalarOperator&gt;\n                          &lt;/Aggregate&gt;\n                        &lt;/ScalarOperator&gt;\n                      &lt;/DefinedValue&gt;\n                      &lt;DefinedValue&gt;\n                        &lt;ColumnReference Column=\"Expr1004\" /&gt;\n                        &lt;ScalarOperator ScalarString=\"SUM([directpay].[dbo].[tbl_Faktuur].[Totaal_Open])\"&gt;\n                          &lt;Aggregate AggType=\"SUM\" Distinct=\"false\"&gt;\n                            &lt;ScalarOperator&gt;\n                              &lt;Identifier&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                              &lt;/Identifier&gt;\n                            &lt;/ScalarOperator&gt;\n                          &lt;/Aggregate&gt;\n                        &lt;/ScalarOperator&gt;\n                      &lt;/DefinedValue&gt;\n                    &lt;/DefinedValues&gt;\n                    &lt;HashKeysBuild&gt;\n                      &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                    &lt;/HashKeysBuild&gt;\n                    &lt;RelOp AvgRowSize=\"23\" EstimateCPU=\"0.030319\" EstimateIO=\"0.0771991\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimateRows=\"27420\" LogicalOp=\"Index Seek\" NodeId=\"2\" Parallel=\"false\" PhysicalOp=\"Index Seek\" EstimatedTotalSubtreeCost=\"0.107518\"&gt;\n                      &lt;OutputList&gt;\n                        &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                        &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Faktuurdatum\" /&gt;\n                        &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                      &lt;/OutputList&gt;\n                      &lt;IndexScan Ordered=\"true\" ScanDirection=\"FORWARD\" ForcedIndex=\"false\" NoExpandHint=\"false\"&gt;\n                        &lt;DefinedValues&gt;\n                          &lt;DefinedValue&gt;\n                            &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                          &lt;/DefinedValue&gt;\n                          &lt;DefinedValue&gt;\n                            &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Faktuurdatum\" /&gt;\n                          &lt;/DefinedValue&gt;\n                          &lt;DefinedValue&gt;\n                            &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                          &lt;/DefinedValue&gt;\n                        &lt;/DefinedValues&gt;\n                        &lt;Object Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Index=\"[_dta_index_tbl_Faktuur_5_583009158__K13_K9_K19_K2_5]\" /&gt;\n                        &lt;SeekPredicates&gt;\n                          &lt;SeekPredicate&gt;\n                            &lt;Prefix ScanType=\"EQ\"&gt;\n                              &lt;RangeColumns&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Afgehandeld_NeeJa\" /&gt;\n                              &lt;/RangeColumns&gt;\n                              &lt;RangeExpressions&gt;\n                                &lt;ScalarOperator ScalarString=\"(0)\"&gt;\n                                  &lt;Const ConstValue=\"(0)\" /&gt;\n                                &lt;/ScalarOperator&gt;\n                              &lt;/RangeExpressions&gt;\n                            &lt;/Prefix&gt;\n                            &lt;EndRange ScanType=\"LT\"&gt;\n                              &lt;RangeColumns&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                              &lt;/RangeColumns&gt;\n                              &lt;RangeExpressions&gt;\n                                &lt;ScalarOperator ScalarString=\"($0.0000)\"&gt;\n                                  &lt;Const ConstValue=\"($0.0000)\" /&gt;\n                                &lt;/ScalarOperator&gt;\n                              &lt;/RangeExpressions&gt;\n                            &lt;/EndRange&gt;\n                          &lt;/SeekPredicate&gt;\n                          &lt;SeekPredicate&gt;\n                            &lt;Prefix ScanType=\"EQ\"&gt;\n                              &lt;RangeColumns&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Afgehandeld_NeeJa\" /&gt;\n                              &lt;/RangeColumns&gt;\n                              &lt;RangeExpressions&gt;\n                                &lt;ScalarOperator ScalarString=\"(0)\"&gt;\n                                  &lt;Const ConstValue=\"(0)\" /&gt;\n                                &lt;/ScalarOperator&gt;\n                              &lt;/RangeExpressions&gt;\n                            &lt;/Prefix&gt;\n                            &lt;StartRange ScanType=\"GT\"&gt;\n                              &lt;RangeColumns&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                              &lt;/RangeColumns&gt;\n                              &lt;RangeExpressions&gt;\n                                &lt;ScalarOperator ScalarString=\"($0.0000)\"&gt;\n                                  &lt;Const ConstValue=\"($0.0000)\" /&gt;\n                                &lt;/ScalarOperator&gt;\n                              &lt;/RangeExpressions&gt;\n                            &lt;/StartRange&gt;\n                          &lt;/SeekPredicate&gt;\n                        &lt;/SeekPredicates&gt;\n                      &lt;/IndexScan&gt;\n                    &lt;/RelOp&gt;\n                  &lt;/Hash&gt;\n                &lt;/RelOp&gt;\n              &lt;/ComputeScalar&gt;\n            &lt;/RelOp&gt;\n          &lt;/QueryPlan&gt;\n        &lt;/StmtSimple&gt;\n      &lt;/Statements&gt;\n    &lt;/Batch&gt;\n  &lt;/BatchSequence&gt;\n&lt;/ShowPlanXML&gt;\n</code></pre>\n"},{"tags":["performance","oracle","oracle11gr2","impdp"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":25,"score":0,"question_id":13078530,"title":"How can I identify and fix bottlenecks in an Oracle11gR2 import","body":"<p>I've recently been tasked with moving our Oracle 11g database from a linux CentOS desktop to a windows 2008 server. Given I've not done much behind the scenes stuff with oracle before I'm a bit out of my depth.</p>\n\n<p>I did a full expdp from the source server and imported it to the target server (i built tablespaces before the load but nothing else).</p>\n\n<p>Everything went well enough (a few errors that were related to having a different global_name which, from documentation, appeared ignorable.</p>\n\n<p>The problem I am having is that the new server appears to take longer to run queries compared to the old server. This is in spite of more cores and more RAM. I think the problem is due to I/O limitations as the old server ran on an SSD.</p>\n\n<p>I get the same 'explain plans' on queries yet getting through the blocks on v$sql_longops just seems to take longer... Can anyone provide details on what steps to take to check and compare output so I can try to ascertain the underlying issue?</p>\n"},{"tags":["performance","oracle","hibernate","index","order"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":450,"score":2,"question_id":8729760,"title":"Query from Hibernate App does not use DB indexes","body":"<p>I try to solve a performance issue of my application. The query hibernate generates, is of the form:</p>\n\n<pre><code>select * \nfrom ( \n    select this.a, this.b, this.state, this.id \n    from view_user this \n    where this.state=:1 order by this.a asc, this.b\n) \nwhere rownum &lt;= :2\n</code></pre>\n\n<p>where</p>\n\n<ul>\n<li>id is the primary key</li>\n<li>there is a combined, unique index on (a, b, id).</li>\n<li>view_user has ~ 2 million entries</li>\n<li>view_user performs some further joins to other tables</li>\n</ul>\n\n<p><strong>Issue</strong></p>\n\n<p>The above query performs\n - fast from SQLDeveloper\n - fast from a small Java app with hibernate\n - extremely slow (>100x slower) from the application with hibernate\n - values for the bind variables are 2 respectively 30 (rownum origins from paging)\n - the hibernate query is \"of the form\" above. There are actually about 20 columns in the view.</p>\n\n<p><strong>Current state of analysis</strong></p>\n\n<ul>\n<li>query plan shows that index is used when query comes from SQlDeveloper or \"small java app\".</li>\n<li>query plan shows that full table scans are performed if query comes from hibernate app</li>\n<li>DB tracing shows only two differences: NLS settings (from SQLDeveloper) and slightly different formatting (whitespaces). Everything else seems to be the same...</li>\n</ul>\n\n<p><strong>Versions</strong></p>\n\n<ul>\n<li>hibernate: 2.1.8</li>\n<li>jdbc driver: used ojdbc14, 5 and 6. Makes no difference</li>\n<li>Oracle: 10.2 and 11. Makes no difference</li>\n</ul>\n\n<p>=> I'm glad about every hint somebody might have concerning this issue. What troubles me is the fact that the DB tracing did not show any differences... Yes, it looks like it is something about hibernate. But what? How to detect?</p>\n\n<hr>\n\n<p>For the sake of completeness, here the hibernate query (from the log):</p>\n\n<pre><code>Select * from ( \n    select this.USER_ID as USER_ID0_, this.CLIENT_ID as CLIENT_ID0_, \n    this.USER_NAME as USER_NAME0_, this.USER_FIRST_NAME as USER_FIR5_0_, this.USER_REMARKS as \n    USER_REM6_0_, this.USER_LOGIN_ID as USER_LOG7_0_, this.USER_TITLE as USER_TITLE0_, \n    this.user_language_code as user_lan9_0_, this.USER_SEX as USER_SEX0_, \n    this.USER_BIRTH_DATE as USER_BI11_0_, this.USER_TELEPHONE as USER_TE12_0_, \n    this.USER_TELEFAX as USER_TE13_0_, this.USER_MOBILE as USER_MO14_0_, \n    this.USER_EMAIL as USER_EMAIL0_, this.USER_ADDRESSLINE1 as USER_AD16_0_, \n    this.USER_ADDRESSLINE2 as USER_AD17_0_, this.USER_POSTALCODE as USER_PO18_0_, \n    this.USER_CITY as USER_CITY0_, this.USER_COUNTRY_CD as USER_CO20_0_, \n    this.USER_COUNTRY_NAME as USER_CO21_0_, this.USER_STATE_ID as USER_ST24_0_, \n    this.USER_STATE as USER_STATE0_, this.USER_TEMP_COLL_ID as USER_TE26_0_, \n    this.USER_TEMP_COLL_NAME as USER_TE27_0_, this.UNIT_ID as UNIT_ID0_, \n    this.CLIENT_NAME as CLIENT_38_0_, this.PROFILE_EXTID as PROFILE39_0_\n    from VIEW_USER this\n    where this.USER_STATE_ID=:1 order by this.USER_NAME asc, this.USER_FIRST_NAME asc\n) \nwhere rownum &lt;= :2\n</code></pre>\n\n<p>Unique index is over user_name, user_first_name, user_id.</p>\n"},{"tags":["mysql","performance","query"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":43,"score":0,"question_id":13082066,"title":"to improve performance on query","body":"<p>I hava nest query:</p>\n\n<pre><code>SELECT PIXEL_X as 'X_Coord', PIXEL_Y as 'Y_Coord', \nCONVERTWATTS2DBM_udf(SUM(L2_VALUE)/SUM(L3_VALUE)) as 'Pixel_Value' \n   FROM table  \n   WHERE  \n      ('GSM 850/900' like CONCAT('%',FILTER2,'/%') OR \n       'GSM 850/900' like CONCAT('%/',FILTER2,'%') ) \nGROUP BY X_Coord, Y_Coord;\n</code></pre>\n\n<p>but takes a long time, could you help me to improve their performance?</p>\n\n<p>Thanks</p>\n"},{"tags":["arrays","performance","r"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":30,"score":0,"question_id":13081611,"title":"Most efficient way to fill out a matrix of elements given by function f(i,j) in R?","body":"<p>I have a function of two scalar values <code>x_i</code>, <code>y_j</code>.  I have a vector of n <code>x_i</code> values, X and n <code>y_j</code> values, e.g.</p>\n\n<pre><code>myfunction &lt;- function(x,y) min(x,y)\nX &lt;- 1:3\nY &lt;- 2:4\n</code></pre>\n\n<p>I want to fill out the $n$ by $n$ matrix whose elements <code>(i,j)</code> are given by <code>myfunction(x_i, y_j)</code>.  There's a lot of ways to do this in <code>R</code>, and I'm curious about their relative performance.</p>\n\n<p>For instance, this seems like a task for <code>outer</code>, but it seems to get confused whether it is passing a vector or scalar to <code>myfunction</code>.  First consider: </p>\n\n<p><code>r\n    outer(X, Y, paste)\n</code></p>\n\n<p>gives me each of the pairs</p>\n\n<pre><code>     [,1]  [,2]  [,3] \n[1,] \"1 2\" \"1 3\" \"1 4\"\n[2,] \"2 2\" \"2 3\" \"2 4\"\n[3,] \"3 2\" \"3 3\" \"3 4\"\n</code></pre>\n\n<p>Looks good.  But </p>\n\n<pre><code>outer(X, Y, myfunction)\n</code></pre>\n\n<p>throws the error:</p>\n\n<pre><code>Error: dims [product 9] do not match the length of object [1]\n</code></pre>\n\n<p>Meanwhile other possible functions seem to behave as I expected with scalars, such as:</p>\n\n<pre><code>myfunction &lt;- function(x,y) exp((x-y)^2)\n</code></pre>\n\n<p>which works fine</p>\n\n<pre><code>outer(X, Y, myfunction)\n\n\n         [,1]      [,2]        [,3]\n[1,] 2.718282 54.598150 8103.083928\n[2,] 1.000000  2.718282   54.598150\n[3,] 2.718282  1.000000    2.718282\n</code></pre>\n\n<p>In a few quick numerical experiments, it seems this is slightly faster than <code>expand.grid</code>, and the function call more compact, but I don't seem to understand why some functions appear to work as I anticipate and others do not.  </p>\n\n<p>The classic <code>expand.grid</code> solution also requires the function to work with vector arguments, which means a very different thing for my example with <code>min</code>; a different version of the same problem.  Is there a way to enforce the fact that the arguments to my function must be scalars rather than vectors?</p>\n"},{"tags":["mysql","performance","join"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":34,"score":0,"question_id":13081434,"title":"Multiple Joins really slow down MySQL query","body":"<p>This is my first time doing such a large query and so many joins. When I do this query it is super slow. I am not sure how to use Foreign Keys and if it would even help... but if anyone knows a way I could speed this up, it would be very helpful. Here is my query:     </p>\n\n<pre><code>SELECT FLOOR(AVG(ra.double)) AS price, hi.hname,  hi.hotel_id as hid, hi.hstars, hi.haddress, im.image_file, hd.short_desc, pr.promo_name, pr.discount, pr.discount_type, pr.book_start, pr.book_end \n    FROM hotel_info AS hi \n    JOIN images AS im ON im.foreign_id = hi.hotel_id \n    LEFT JOIN hotel_desc AS hd ON hd.hotel_id = hi.hotel_id \n    RIGHT JOIN rates AS ra ON ra.hotel_id = hi.hotel_id \n    RIGHT JOIN promotions AS pr ON pr.hotel_id = ra.hotel_id \n    WHERE ra.booking_date  BETWEEN '2012-01-01' AND '2012-01-06' \n    AND ra.double != '0.00' \n    AND hi.status = '1' \n    AND hi.destination_id = '$destination' \n    GROUP BY hi.hotel_id\n</code></pre>\n"},{"tags":["python","algorithm","performance","recommendation-engine"],"answer_count":3,"favorite_count":5,"up_vote_count":4,"down_vote_count":0,"view_count":830,"score":4,"question_id":2136941,"title":"efficient library to recommend product based on user history","body":"<p>I have a database of which products every user has viewed and I want to recommend a product based on what similar users have viewed. Is there a Python library that can achieve this? I don't need Netflix quality results, just products that are more likely than not of interest. Any ideas?</p>\n"},{"tags":["java","performance","opengl","png","jogl"],"answer_count":5,"favorite_count":5,"up_vote_count":5,"down_vote_count":0,"view_count":5093,"score":5,"question_id":1927419,"title":"Loading PNGs into OpenGL performance issues - Java & JOGL much slower than C# & Tao.OpenGL","body":"<p>I am noticing a large performance difference between Java &amp; JOGL and C# &amp; Tao.OpenGL when both loading PNGs from storage into memory, and when loading that BufferedImage (java) or Bitmap (C# - both are PNGs on hard drive) 'into' OpenGL.</p>\n\n<p>This difference is quite large, so I assumed I was doing something wrong, however after quite a lot of searching and trying different loading techniques I've been unable to reduce this difference.</p>\n\n<p>With Java I get an image loaded in 248ms and loaded into OpenGL in 728ms\nThe same on C# takes 54ms to load the image, and 34ms to load/create texture.</p>\n\n<p>The image in question above is a PNG containing transparency, sized 7200x255, used for a 2D animated sprite. I realise the size is really quite ridiculous and am considering cutting up the sprite, however the large difference is still there (and confusing).</p>\n\n<p>On the Java side the code looks like this:</p>\n\n<pre><code>BufferedImage image = ImageIO.read(new File(fileName));\ntexture = TextureIO.newTexture(image, false);\ntexture.setTexParameteri(GL.GL_TEXTURE_MIN_FILTER, GL.GL_LINEAR);\ntexture.setTexParameteri(GL.GL_TEXTURE_MAG_FILTER, GL.GL_LINEAR);\n</code></pre>\n\n<p>The C# code uses:</p>\n\n<pre><code>Bitmap t = new Bitmap(fileName);\n\nt.RotateFlip(RotateFlipType.RotateNoneFlipY);\nRectangle r = new Rectangle(0, 0, t.Width, t.Height);\n\nBitmapData bd = t.LockBits(r, ImageLockMode.ReadOnly, PixelFormat.Format32bppArgb);\n\nGl.glBindTexture(Gl.GL_TEXTURE_2D, tID);\nGl.glTexImage2D(Gl.GL_TEXTURE_2D, 0, Gl.GL_RGBA, t.Width, t.Height, 0, Gl.GL_BGRA, Gl.GL_UNSIGNED_BYTE, bd.Scan0);\nGl.glTexParameteri(Gl.GL_TEXTURE_2D, Gl.GL_TEXTURE_MIN_FILTER, Gl.GL_LINEAR);\nGl.glTexParameteri(Gl.GL_TEXTURE_2D, Gl.GL_TEXTURE_MAG_FILTER, Gl.GL_LINEAR);\n\nt.UnlockBits(bd);\nt.Dispose();\n</code></pre>\n\n<p>After quite a lot of testing I can only come to the conclusion that Java/JOGL is just slower here - PNG reading might not be as quick, or that I'm still doing something wrong.</p>\n\n<p>Thanks.</p>\n\n<p>Edit2:</p>\n\n<p>I have found that creating a new BufferedImage with format TYPE_INT_ARGB_PRE decreases OpenGL texture load time by almost half - this includes having to create the new BufferedImage, getting the Graphics2D from it and then rendering the previously loaded image to it.</p>\n\n<p>Edit3: Benchmark results for 5 variations.\nI wrote a small benchmarking tool, the following results come from loading a set of 33 pngs, most are very wide, 5 times.</p>\n\n<pre><code>testStart: ImageIO.read(file) -&gt; TextureIO.newTexture(image)  \nresult: avg = 10250ms, total = 51251  \ntestStart: ImageIO.read(bis) -&gt; TextureIO.newTexture(image)  \nresult: avg = 10029ms, total = 50147  \ntestStart: ImageIO.read(file) -&gt; TextureIO.newTexture(argbImage)  \nresult: avg = 5343ms, total = 26717  \ntestStart: ImageIO.read(bis) -&gt; TextureIO.newTexture(argbImage)  \nresult: avg = 5534ms, total = 27673  \ntestStart: TextureIO.newTexture(file)  \nresult: avg = 10395ms, total = 51979\n</code></pre>\n\n<p>ImageIO.read(bis) refers to the technique described in James Branigan's answer below.\nargbImage refers to the technique described in my previous edit:</p>\n\n<pre><code>img = ImageIO.read(file);\nargbImg = new BufferedImage(img.getWidth(), img.getHeight(), TYPE_INT_ARGB_PRE);\ng = argbImg.createGraphics();\ng.drawImage(img, 0, 0, null);\ntexture = TextureIO.newTexture(argbImg, false);\n</code></pre>\n\n<p>Any more methods of loading (either images from file, or images to OpenGL) would be appreciated, I will update these benchmarks.</p>\n"},{"tags":["c++","c","performance","memory-management","hardware"],"answer_count":2,"favorite_count":2,"up_vote_count":5,"down_vote_count":0,"view_count":308,"score":5,"question_id":11401717,"title":"Why do integers process faster than bytes on NDS?","body":"<p>i've noticed that my nds application works a little faster when I replace all the instances of bytes with integers. all the examples online put u8/u16 instances whenever possible. is there a specific reason as to why this is the case?</p>\n"},{"tags":["xcode","performance","osx","instruments","performance-counters"],"answer_count":1,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":58,"score":6,"question_id":13075113,"title":"Is there anyway to read performance counters on OS X Mountain Lion?","body":"<p>Shark, Apple's profiler which let you configure custom performance counters, is no longer supported in OSX Mountain Lion since it can't run a 32-bit kernel. Instruments.app, Apple's replacement for Shark, doesn't seem to support reading performance counters such as L1 cache hits/misses**. Is there anyway to actually setup and read performance counters on OS X? Even if there is no application, is there some user-land API to do this?</p>\n\n<p>**Instruments.app does seem to have an interface for performance counters, but on my Retina MacBook Pro, the PM Events window lists no events, and indicates \"Device: Unknown.\" Are there any other alternatives to Instruments?</p>\n"},{"tags":["android","json","performance","gridview","adapter"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":69,"score":0,"question_id":13080143,"title":"Make application load faster with ViewHolder class","body":"<p>I've made gridview application that get path of image from json. it work find but when it load it's so slow. and i try to find solution and they recommended me to use ViewHolder but I don't know how to use in my gridview application.</p>\n\n<p>How can I use ViewHolder in my application? how I speed up my application?</p>\n\n<p>This code work find but it load so slow.</p>\n\n<pre><code>public class MainActivity extends Activity {\n\nString strUrl = \"http://192.168.10.104/adchara1/\";\nGridView gridView;\n\n@Override\npublic void onCreate(Bundle savedInstanceState) {\n    super.onCreate(savedInstanceState);\n    setContentView(R.layout.activity_main);\n\n    // Creating a new non-ui thread task to download json data\n    DownloadTask downloadTask = new DownloadTask();\n\n    // Starting the download process\n    downloadTask.execute(strUrl);\n\n    gridView = (GridView) findViewById(R.id.lv_countries);        \n    gridView.setOnItemClickListener(new OnItemClickListener() {\n\n        public void onItemClick(AdapterView&lt;?&gt; parent, View v, int positon,\n                long id) {\n\n             HashMap&lt;String, Object&gt; hm = (HashMap&lt;String, Object&gt;) gridView.getAdapter().getItem(positon);\n             String imgPath = (String) hm.get(\"photo\"); //get downloaded image path\n             Intent i = new Intent(MainActivity.this, DisplayActivity.class); //start new Intent to another Activity.\n             i.putExtra(\"ClickedImagePath\", imgPath ); //put image link in intent.\n             startActivity(i);\n        }\n    });\n}\n\n/** A method to download json data from url */\nprivate String downloadUrl(String strUrl) throws IOException{\n  String data = \"\";\n  InputStream iStream = null;\n\n    HttpClient httpClient = new DefaultHttpClient();\n    HttpPost httpPost = new HttpPost(strUrl);\n    ArrayList&lt;NameValuePair&gt; param = new ArrayList&lt;NameValuePair&gt;();\n    try {\n        httpPost.setEntity(new UrlEncodedFormEntity(param));\n        HttpResponse httpResponse = httpClient.execute(httpPost);\n        HttpEntity httpEntity = httpResponse.getEntity();\n        iStream = httpEntity.getContent();\n    } catch (Exception e) {\n        Log.e(\"log_tag\", \"Error in http connection \" + e.toString());\n    }\n    try {\n        BufferedReader br = new BufferedReader(new InputStreamReader(iStream));\n        StringBuilder sb = new StringBuilder();\n        String line = \"\";\n        while((line = br.readLine()) != null){\n            sb.append(line + \"\\n\");\n        }\n        iStream.close();\n        data = sb.toString();\n    } catch (Exception e) {\n        Log.e(\"log_tag\", \"Error converting result \" + e.toString());\n    }\n    return data;\n}\n\n/** AsyncTask to download json data */\nprivate class DownloadTask extends AsyncTask&lt;String, Integer, String&gt;{\n    String data = null;\n    @Override\n    protected String doInBackground(String... url) {\n        try{\n            data = downloadUrl(url[0]);\n        }catch(Exception e){\n            Log.d(\"Background Task\",e.toString());\n        }\n        return data;\n    }\n\n    @Override\n    protected void onPostExecute(String result) {\n\n        // The parsing of the xml data is done in a non-ui thread\n        GridViewLoaderTask gridViewLoaderTask = new GridViewLoaderTask();\n\n        // Start parsing xml data\n        gridViewLoaderTask.execute(result);\n    }\n}\n\n/** AsyncTask to parse json data and load ListView */\nprivate class GridViewLoaderTask extends AsyncTask&lt;String, Void, SimpleAdapter&gt;{\n\n    JSONObject jObject;\n    // Doing the parsing of xml data in a non-ui thread\n    @Override\n    protected SimpleAdapter doInBackground(String... strJson) {\n        try{\n            jObject = new JSONObject(strJson[0]);\n            CountryJSONParser countryJsonParser = new CountryJSONParser();\n            countryJsonParser.parse(jObject);\n        }catch(Exception e){\n            Log.d(\"JSON Exception1\",e.toString());\n        }\n\n        // Instantiating json parser class\n        CountryJSONParser countryJsonParser = new CountryJSONParser();\n\n        // A list object to store the parsed countries list\n        List&lt;HashMap&lt;String, Object&gt;&gt; countries = null;\n\n        try{\n            // Getting the parsed data as a List construct\n            countries = countryJsonParser.parse(jObject);\n        }catch(Exception e){\n            Log.d(\"Exception\",e.toString());\n        }\n\n        // Keys used in Hashmap\n        String[] from = { \"frame\",\"photo\"};\n\n        // Ids of views in listview_layout\n        int[] to = { R.id.iv_frame,R.id.iv_photo};\n\n        // Instantiating an adapter to store each items\n        // R.layout.listview_layout defines the layout of each item\n        SimpleAdapter adapter = new SimpleAdapter(getBaseContext(), countries, R.layout.lv_layout, from, to);\n        return adapter;\n    }\n\n    /** Invoked by the Android on \"doInBackground\" is executed */\n    @Override\n    protected void onPostExecute(SimpleAdapter adapter) {\n\n        // Setting adapter for the listview\n        gridView.setAdapter(adapter);\n\n        for(int i=0;i&lt;adapter.getCount();i++){\n            HashMap&lt;String, Object&gt; hm = (HashMap&lt;String, Object&gt;) adapter.getItem(i);\n            String frameUrl = (String) hm.get(\"frame_path\");\n            String imgUrl = (String) hm.get(\"photo_path\");\n            ImageLoaderTask imageLoaderTask = new ImageLoaderTask();\n\n            HashMap&lt;String, Object&gt; hmDownload = new HashMap&lt;String, Object&gt;();\n            hm.put(\"frame_path\", frameUrl);\n            hm.put(\"photo_path\",imgUrl);\n            hm.put(\"position\", i);\n\n            // Starting ImageLoaderTask to download and populate image in the listview\n            imageLoaderTask.execute(hm);\n        }\n    }\n} \n\n    /** AsyncTask to download and load an image in ListView */\n    private class ImageLoaderTask extends AsyncTask&lt;HashMap&lt;String, Object&gt;, Void, HashMap&lt;String, Object&gt;&gt;{\n\n        @Override\n        protected HashMap&lt;String, Object&gt; doInBackground(HashMap&lt;String, Object&gt;... hm) {\n\n            InputStream iStream = null;\n            String imgUrl;\n            String frameUrl;\n            imgUrl = (String) hm[0].get(\"photo_path\");\n            frameUrl = (String) hm[0].get(\"frame_path\");\n            int position = (Integer) hm[0].get(\"position\");\n\n            URL url;\n            URL urlFrame;\n            try {\n                url = new URL(imgUrl);\n                urlFrame = new URL(frameUrl);\n                // Creating an http connection to communicate with url\n                HttpURLConnection urlConnection = (HttpURLConnection) url\n                        .openConnection();\n\n                // Connecting to url\n                urlConnection.connect();\n\n                // Reading data from url\n                iStream = urlConnection.getInputStream();\n\n                // Getting Caching directory\n                File cacheDirectory = getBaseContext().getCacheDir();\n\n                // Temporary file to store the downloaded image\n                File tmpFile = new File(cacheDirectory.getPath() + \"/wpta_\"+ position + \".png\");\n\n                // The FileOutputStream to the temporary file\n                FileOutputStream fOutStream = new FileOutputStream(tmpFile);\n\n                // Creating a bitmap from the downloaded inputstream\n                Bitmap b = BitmapFactory.decodeStream(iStream);\n\n                // Writing the bitmap to the temporary file as png file\n                b.compress(Bitmap.CompressFormat.PNG, 100, fOutStream);\n\n                // Flush the FileOutputStream\n                fOutStream.flush();\n\n                // Close the FileOutputStream\n                fOutStream.close();\n\n                // Create a hashmap object to store image path and its position in the listview\n                HashMap&lt;String, Object&gt; hmBitmap = new HashMap&lt;String, Object&gt;();\n\n                // Storing the path to the temporary image file\n                hmBitmap.put(\"photo\", tmpFile.getPath());\n                hmBitmap.put(\"frame\", tmpFile.getPath());\n\n                // Storing the position of the image in the listview\n                hmBitmap.put(\"position\", position);\n\n                // Returning the HashMap object containing the image path and position\n                return hmBitmap;\n            } catch (MalformedURLException e) {\n                return null;\n            } catch (FileNotFoundException e) {\n                return null;\n            } catch (IOException e) {\n                return null;\n            }\n        }\n\n    @Override\n    protected void onPostExecute(HashMap&lt;String, Object&gt; result) {\n        // Getting the path to the downloaded image\n        String path = (String) result.get(\"photo\");\n        String framePath = (String) result.get(\"frame\");\n        // Getting the position of the downloaded image\n        int position = (Integer) result.get(\"position\");\n\n        // Getting adapter of the listview\n        SimpleAdapter adapter = (SimpleAdapter ) gridView.getAdapter();\n\n        // Getting the hashmap object at the specified position of the listview\n        HashMap&lt;String, Object&gt; hm = (HashMap&lt;String, Object&gt;) adapter.getItem(position);\n\n        // Overwriting the existing path in the adapter\n        hm.put(\"photo\",path);\n        hm.put(\"frame\", framePath);\n        // Noticing listview about the dataset changes\n        adapter.notifyDataSetChanged();\n    }\n}\n\n\n\n@Override\npublic boolean onCreateOptionsMenu(Menu menu) {\n    getMenuInflater().inflate(R.menu.activity_main, menu);\n    return true;\n}\n</code></pre>\n\n<p>}</p>\n"},{"tags":["c","performance","algorithm","opencl","measurement"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":389,"score":2,"question_id":10155579,"title":"C vs OpenCL, how to compare results of time measurement?","body":"<p>So, in the other post I questioned about C time measurement. Now, I wanna know how to compare the result of the C \"function\" vs the OpenCL \"function\"</p>\n\n<p>This is the code of the host OpenCL and C</p>\n\n<pre><code>#define PROGRAM_FILE \"sum.cl\"\n#define KERNEL_FUNC \"float_sum\"\n#define ARRAY_SIZE 1000000\n\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\n#include &lt;CL/cl.h&gt;\n\nint main()\n{\n    /* OpenCL Data structures */\n\n    cl_platform_id platform;\n    cl_device_id device;\n    cl_context context;\n    cl_program program;\n    cl_kernel kernel;    \n    cl_command_queue queue;\n    cl_mem vec_buffer, result_buffer;\n\n    cl_event prof_event;;\n\n    /* ********************* */\n\n    /* C Data Structures / Data types */\n    FILE *program_handle; //Kernel file handle\n    char *program_buffer; //Kernel buffer\n\n    float *vec, *non_parallel;\n    float result[ARRAY_SIZE];\n\n    size_t program_size; //Kernel file size\n\n    cl_ulong time_start, time_end, total_time;\n\n    int i;\n    /* ****************************** */\n\n    /* Errors */\n    cl_int err;\n    /* ****** */\n\n    non_parallel = (float*)malloc(ARRAY_SIZE * sizeof(float));\n    vec          = (float*)malloc(ARRAY_SIZE * sizeof(float));\n\n    //Initialize the vector of floats\n    for(i = 0; i &lt; ARRAY_SIZE; i++)\n    vec[i] = i + 1;\n\n    /************************* C Function **************************************/\n    clock_t start, end;\n\n    start = clock();\n\n    for( i = 0; i &lt; ARRAY_SIZE; i++) \n    {\n    non_parallel[i] = vec[i] * vec[i];\n    }\n    end = clock();\n    printf( \"Number of seconds: %f\\n\", (clock()-start)/(double)CLOCKS_PER_SEC );\n\n    free(non_parallel);\n    /***************************************************************************/\n\n\n\n\n    clGetPlatformIDs(1, &amp;platform, NULL);//Just want NVIDIA platform\n    clGetDeviceIDs(platform, CL_DEVICE_TYPE_GPU, 1, &amp;device, NULL);\n    context = clCreateContext(NULL, 1, &amp;device, NULL, NULL, &amp;err);\n\n    // Context error?\n    if(err)\n    {\n    perror(\"Cannot create context\");\n    return 1;\n    }\n\n    //Read the kernel file\n    program_handle = fopen(PROGRAM_FILE,\"r\");\n    fseek(program_handle, 0, SEEK_END);\n    program_size = ftell(program_handle);\n    rewind(program_handle);\n\n    program_buffer = (char*)malloc(program_size + 1);\n    program_buffer[program_size] = '\\0';\n    fread(program_buffer, sizeof(char), program_size, program_handle);\n    fclose(program_handle);\n\n    //Create the program\n    program = clCreateProgramWithSource(context, 1, (const char**)&amp;program_buffer, \n                    &amp;program_size, &amp;err);\n\n    if(err)\n    {\n    perror(\"Cannot create program\");\n    return 1;\n    }\n\n    free(program_buffer);\n\n    clBuildProgram(program, 0, NULL, NULL, NULL, NULL);\n\n    kernel = clCreateKernel(program, KERNEL_FUNC, &amp;err);\n\n    if(err)\n    {\n    perror(\"Cannot create kernel\");\n    return 1;\n    }\n\n    queue = clCreateCommandQueue(context, device, CL_QUEU_PROFILING_ENABLE, &amp;err);\n\n    if(err)\n    {\n    perror(\"Cannot create command queue\");\n    return 1;\n    }\n\n    vec_buffer = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR,\n                sizeof(float) * ARRAY_SIZE, vec, &amp;err);\n    result_buffer = clCreateBuffer(context, CL_MEM_WRITE_ONLY, sizeof(float)*ARRAY_SIZE, NULL, &amp;err);\n\n    if(err)\n    {\n    perror(\"Cannot create the vector buffer\");\n    return 1;\n    }\n\n    clSetKernelArg(kernel, 0, sizeof(cl_mem), &amp;vec_buffer);\n    clSetKernelArg(kernel, 1, sizeof(cl_mem), &amp;result_buffer);\n\n    size_t global_size = ARRAY_SIZE;\n    size_t local_size = 0;\n\n    clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &amp;global_size, NULL, 0, NULL, &amp;prof_event);\n\n    clEnqueueReadBuffer(queue, result_buffer, CL_TRUE, 0, sizeof(float)*ARRAY_SIZE, &amp;result, 0, NULL, NULL);\n    clFinish(queue);\n\n\n\n     clGetEventProfilingInfo(prof_event, CL_PROFILING_COMMAND_START,\n           sizeof(time_start), &amp;time_start, NULL);\n     clGetEventProfilingInfo(prof_event, CL_PROFILING_COMMAND_END,\n           sizeof(time_end), &amp;time_end, NULL);\n     total_time += time_end - time_start;\n\n    printf(\"\\nAverage time in nanoseconds = %lu\\n\", total_time/ARRAY_SIZE);\n\n\n\n    clReleaseMemObject(vec_buffer);\n    clReleaseMemObject(result_buffer);\n    clReleaseKernel(kernel);\n    clReleaseCommandQueue(queue);\n    clReleaseProgram(program);\n    clReleaseContext(context);\n\n    free(vec);\n\n    return 0;\n}\n</code></pre>\n\n<p>And the kernel is:</p>\n\n<pre><code>__kernel void float_sum(__global float* vec,__global float* result){\n    int gid = get_global_id(0);\n    result[gid] = vec[gid] * vec[gid];\n}\n</code></pre>\n\n<p>Now, the results are:</p>\n\n<blockquote>\n  <p>Number of seconds: 0.010000 &lt;- This is the for the C code</p>\n  \n  <p>Average time in nanoseconds = 140737284 &lt;- OpenCL function </p>\n</blockquote>\n\n<p>0,1407 seconds is the time of the OpenCL time kernel execution, and it's more than the C function, is it correct? Beacause I think OpenCL should be fastest than C non parallel algorithm...</p>\n"},{"tags":["sql","performance","postgresql","plpgsql","window-functions"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":67,"score":1,"question_id":13078964,"title":"Ordered count of consecutive repeats / duplicates","body":"<p>I highly doubt I'm doing this in the most efficient manner, which is why I tagged <code>plpgsql</code> on here. I need to run this on <strong>2 billion rows</strong> for a <strong>thousand  measurement systems</strong>.</p>\n\n<p>You have measurement systems that often report the previous value when they lose connectivity, and they lose connectivity for spurts often but sometimes for a long time. You need to aggregate but when you do so, you need to look at how long it was repeating and make various filters based on that information. Say you are measuring mpg on a car but it's stuck at 20 mpg for an hour than moves around to 20.1 and so on. You'll want to evaluate the accuracy when it's stuck. You could also place some alternative rules that look for when the car is on the highway, and with window functions you can generate the 'state' of the car and have something to group on. Without further ado:</p>\n\n<pre><code>--here's my data, you have different systems, the time of measurement, and the actual measurement\n--as well, the raw data has whether or not it's a repeat (hense the included window function\nselect * into temporary table cumulative_repeat_calculator_data\nFROM\n    (\n    select \n    system_measured, time_of_measurement, measurement, \n    case when \n     measurement = lag(measurement,1) over (partition by system_measured order by time_of_measurement asc) \n     then 1 else 0 end as repeat\n    FROM\n    (\n    SELECT 5 as measurement, 1 as time_of_measurement, 1 as system_measured\n    UNION\n    SELECT 150 as measurement, 2 as time_of_measurement, 1 as system_measured\n    UNION\n    SELECT 5 as measurement, 3 as time_of_measurement, 1 as system_measured\n    UNION\n    SELECT 5 as measurement, 4 as time_of_measurement, 1 as system_measured\n    UNION\n    SELECT 5 as measurement, 1 as time_of_measurement, 2 as system_measured\n    UNION\n    SELECT 5 as measurement, 2 as time_of_measurement, 2 as system_measured\n    UNION\n    SELECT 5 as measurement, 3 as time_of_measurement, 2 as system_measured\n    UNION\n    SELECT 5 as measurement, 4 as time_of_measurement, 2 as system_measured\n    UNION\n    SELECT 150 as measurement, 5 as time_of_measurement, 2 as system_measured\n    UNION\n    SELECT 5 as measurement, 6 as time_of_measurement, 2 as system_measured\n    UNION\n    SELECT 5 as measurement, 7 as time_of_measurement, 2 as system_measured\n    UNION\n    SELECT 5 as measurement, 8 as time_of_measurement, 2 as system_measured\n    ) as data\n) as data;\n\n--unfortunately you can't have window functions within window functions, so I had to break it down into subquery\n--what we need is something to partion on, the 'state' of the system if you will, so I ran a running total of the nonrepeats\n--this creates a row that stays the same when your data is repeating - aka something you can partition/group on\nselect * into temporary table cumulative_repeat_calculator_step_1\nFROM\n    (\n    select \n    *,\n    sum(case when repeat = 0 then 1 else 0 end) over (partition by system_measured order by time_of_measurement asc) as cumlative_sum_of_nonrepeats_by_system\n    from cumulative_repeat_calculator_data\n    order by system_measured, time_of_measurement\n) as data;\n\n--finally, the query. I didn't bother showing my desired output, because this (finally) got it\n--I wanted a sequential count of repeats that restarts when it stops repeating, and starts with the first repeat\n--what you can do now is take the average measurement under some condition based on how long it was repeating, for example  \nselect *, \ncase when repeat = 0 then 0\nelse\nrow_number() over (partition by cumlative_sum_of_nonrepeats_by_system, system_measured order by time_of_measurement) - 1\nend as ordered_repeat\nfrom cumulative_repeat_calculator_step_1\norder by system_measured, time_of_measurement\n</code></pre>\n\n<p>So, what would you do differently in order to run this on a huge table, or what alternative tools would you use? I'm thinking plpgsql because I suspect this needs to done in-database, or during the data insertion process, although I generally work with the data after it's loaded. Is there any way to get this in one sweep without resorting to sub-queries? </p>\n\n<p>I have tested one <em>alternative method</em>, but it still relies on a sub-query and I think this is faster. For that method you create a \"starts and stops\" table with start_timestamp, end_timestamp, system. Then you join to the larger table and if the timestamp is between those, you classify it as being in that state, which is essentially an alternative to <code>cumlative_sum_of_nonrepeats_by_system</code>. But when you do this, you join on 1=1 for thousands of devices and thousands or millions of 'events'. Do you think that's a better way to go?</p>\n"},{"tags":["php","mysql","ajax","performance","query"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":29,"score":1,"question_id":13079933,"title":"Many datbase calls that load less data or fewer database calls that load more data?","body":"<p>I have a website I'm working on that loads 8 rows of data by making an ajax call to a php content loader that calls my database each time the user hits a next button to load the new content.</p>\n\n<p>This method works very fast for me right now as a single user of the site. </p>\n\n<p>My question is though, if I have many users on the site, would it be better to load the next 16,32, etc rows of data up front when they hit next or keep the way it is now by loading in 8 rows each time they hit next? Does it matter?</p>\n"},{"tags":["performance","mongodb","spring-data"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":31,"score":1,"question_id":13062897,"title":"MongoDB + SpringData Query very slow","body":"<p>I have this simple class: </p>\n\n<pre><code>@Document (collection = \"advertise\")\npublic class AdvertiseCache {\n    @Id\n    private int id;\n\n    private int brandId;\n    private String brandName;\n    private String modelName;\n\n    @Indexed\n    private int odometer;\n\n    @Indexed\n    private int price;\n    private boolean learner;\n    private int manufacturedYear;\n    private double engineSize;\n    private String transmissionTypeName;\n    private String stateName;\n    private String ownerTypeName; //private/dealer\n    private String conditionTypeName; //new/used\n}\n</code></pre>\n\n<p>I have another class with same attributes but annotated with @Entity.</p>\n\n<p>They are stored in MongoDB and respectively in PostgreSQL.</p>\n\n<p>I am using Spring Data JPA for PostgreSQL and Spring Data MongoDB ... for mongo.</p>\n\n<p>Both databases contain same data, 30 rows.</p>\n\n<ul>\n<li><p>10000 queries of type findAll will cost: Mongo ~8000-9000ms and PostgreSQL ~10000-11000ms</p></li>\n<li><p>10000 queries of type findAll where price >= 1 and price &lt;=9000 and odometer >=1 and odometer &lt;= 40000 will cost: Mongo: ~7000ms and PostgreSQL ~7200ms</p></li>\n</ul>\n\n<p><strong>WHY? Am I doing something wrong?</strong> I was expecting mongo much faster. (I my application rarely I am using just find all. Most of the times I use filters for sorting)</p>\n\n<p>Both servers are running in a FreeBSD 9 virtual machine. I tested this on another VM with CentOS 6.3. Similar results +-100ms.</p>\n\n<p>Tnx</p>\n\n<p>/// more code for explanations (my filter builder will contain only odometerMin, odometerMax for between criteria and priceMin and priceMax for between criteria:</p>\n\n<pre><code>public List&lt;AdvertiseCache&gt; findByFilter(FilterBuilder filter) {\n    List&lt;AdvertiseCache&gt; result = null;\n    Query query = new Query();\n    Criteria criteria = new Criteria();\n    criteria = criteria.and(\"price\").gte(filter.getPriceMin()).lte(filter.getPriceMax());\n    criteria = criteria.and(\"odometer\").gte(filter.getOdometerMin()).lte(filter.getOdometerMax()); \n    query.addCriteria(criteria);\n    query.limit(filter.getLimit());\n    query.skip(filter.getOffset());\n    result = mongoTemplate.find(query, AdvertiseCache.class, collectionName);\n    return result;\n}\n</code></pre>\n"},{"tags":["jquery","performance","google-chrome","dom"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":79,"score":4,"question_id":12797416,"title":"Chrome event triggering takes ages in large DOM","body":"<p>I have a relatively large DOM and have noticed an incredible performance degradation in chrome when triggering events on an element.  A single event e.g:</p>\n\n<pre><code>myElem.trigger('myevent.myscope',arguments);\n</code></pre>\n\n<p>takes 14ms!! (22.0.1229.92 m)</p>\n\n<p>the same event in firefox 15.0.1 and msie 9 take less than 1ms to trigger!</p>\n\n<p>The element is a jquery object and has been cached so there is no DOM lookup taking place prior to the trigger.  I am using console.time() </p>\n\n<pre><code>console.time('trigger');\nmyElem.trigger('myevent.myscope',arguments);\nconsole.timeEnd('trigger');\n</code></pre>\n\n<p>Can someone shed a bit of light on this situation</p>\n\n<p>Thanks</p>\n\n<p>Gary</p>\n"},{"tags":[".net","performance","windows-server-2008-r2","couchbase"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":85,"score":0,"question_id":12790110,"title":"Couchbase client is too slow on production server","body":"<p>We recently purchased a new server for our project. After that I've noticed a performance problem with Couchbase client. Then I wrote a simple load tool to compare performance on different machines:</p>\n\n<pre><code>internal class Program {\n    private static IMemcachedClient _client;\n    private static string _key = \"mykey\";\n    private static bool _value = false;\n    private static void Main() {\n        _client = new CouchbaseClient();\n\n        _client.Store(StoreMode.Set, _key, _value);\n        while (true) {\n            _client.Get(_key);\n        }\n    }\n}\n</code></pre>\n\n<p>On my development machine this tool makes 35k gets per sec to localhost memcached instance. \nBut on the server it's much slower - 4k gets per sec with same settings.</p>\n\n<p>It is very big difference and I don't understand the reason.</p>\n\n<p>Dev machine configuration:</p>\n\n<ul>\n<li>Windows 7 Professional x64 </li>\n<li>Core i7-2600 3.4GHz </li>\n<li>8Gb RAM</li>\n</ul>\n\n<p>Production server configuration:</p>\n\n<ul>\n<li>Windows 2008 R2 Enterprise x64</li>\n<li>2x Xeon E5645 2.4GHz</li>\n<li>48Gb RAM</li>\n</ul>\n\n<p>Can you help me to understand why server is so slow?</p>\n"},{"tags":["linux","performance","real-time","scheduling"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":105,"score":1,"question_id":11111852,"title":"how to shield a cpu from the linux scheduler (prevent it scheduling threads onto that cpu)?","body":"<p>It is possible to use <code>sched_setaffinity</code> to pin a thread to a cpu, increasing performance (in some situations)</p>\n\n<p>From the linux man page:</p>\n\n<blockquote>\n  <p>Restricting a process to run on a single CPU also avoids the\n  performance cost caused by the cache invalidation that occurs when a\n  process ceases to execute on one CPU and then recommences execution on\n  a different CPU</p>\n</blockquote>\n\n<p>Further, if I desire a more real-time response, I can change the scheduler policy for that thread to <code>SCHED_FIFO</code>, and up the priority to some high value (up to <code>sched_get_priority_max</code>), meaning the thread in question should always pre-empt any other thread running on its cpu when it becomes ready.</p>\n\n<p>However, at this point, the thread running on the cpu which the real-time thread just pre-empted will possibly have evicted much of the real-time thread's level-1 cache entries.</p>\n\n<p>My questions are as follows:</p>\n\n<ol>\n<li>Is it possible to prevent the scheduler from scheduling any threads onto a given cpu? (eg: either hide the cpu completely from the scheduler, or some other way)</li>\n<li>Are there some threads which absolutely have to be able to run on that cpu? (eg: kernel threads / interrupt threads)</li>\n<li>If I need to have kernel threads running on that cpu, what is a reasonable maximum priority value to use such that I don't starve out the kernel threads?</li>\n</ol>\n"},{"tags":["c#","performance","linq","list","iqueryable"],"answer_count":1,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":81,"score":5,"question_id":13078208,"title":"List queries 20 times faster than IQueryable?","body":"<p>Here is a test that i have setup this evening. It was made to prove something different, but the outcome was not quite as i expected. </p>\n\n<p>I'm running a test with 10000 random queries on an IQueryable and while testing i found out that if i do the same on a List, my test is 20 times faster.</p>\n\n<p>See below. My CarBrandManager.GetList originally returns an IQueryable, but now i first issue a ToList(), and then it's way faster. </p>\n\n<p>Can anyone tell me something about why i see this big difference? </p>\n\n<pre><code>var sw = new Stopwatch();\nsw.Start();\n\nint queries = 10000;\n\n//IQueryable&lt;Model.CarBrand&gt; carBrands = CarBrandManager.GetList(context);\nList&lt;Model.CarBrand&gt; carBrands = CarBrandManager.GetList(context).ToList();\n\nRandom random = new Random();\nint randomChar = 65;\n\nfor (int i = 0; i &lt; queries; i++)\n{\n    randomChar = random.Next(65, 90);\n    Model.CarBrand carBrand = carBrands.Where(x =&gt; x.Name.StartsWith(((char)randomChar).ToString())).FirstOrDefault();\n}\n\nsw.Stop();\nlblStopWatch.Text = String.Format(\"Queries: {0} Elapsed ticks: {1}\", queries, sw.ElapsedTicks);\n</code></pre>\n"},{"tags":["c#","c++",".net","performance","comparison"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":6,"view_count":229,"score":-2,"question_id":13063714,"title":"Why does game dev use C++ and why doesn't the rest of the industry?","body":"<p>Its a fact today that almost every AAA video game released for the PC, Xbox or PS3 is written entirely in C++. Some vendors say they can't move to C# because large codebases for the various engines they reuse internally are already built and tested in C++ (physics, rendering, etc). And performance-wise, its a generally accepted fact that <a href=\"http://www.codeproject.com/Articles/212856/Head-to-head-benchmark-Csharp-vs-NET\" rel=\"nofollow\">C++ code runs faster than C#</a>.</p>\n\n<p>To re-iterate : <strong>Why does the professional game-dev industry in general use C++, and why don't we?</strong> Is it because the general line-of-business application doesn't require the \"best performance you can get\" and trading reliability (memory leaks, OOBs, stack overflows) for performance \"isn't important\"? Or is it just because C++ is generally \"harder to maintain\" (templates, pointers, malloc, etc) than the same code written in C#?</p>\n\n<p>Or is it because development in C++ is \"more time consuming\" than developing the same featureset in C#, so you can't implement as many features as you'd like to? But even today, many high-end game-dev kits such as <a href=\"http://www.unrealengine.com/en/\" rel=\"nofollow\">UDK</a> and <a href=\"http://www.crytek.com/cryengine\" rel=\"nofollow\">CryENGINE</a> are written entirely in C++ and their <a href=\"http://www.unrealengine.com/features/\" rel=\"nofollow\">featureset</a> is among the best you can get on any platform. <strong>So if C++ isn't \"harder\" or \"limiting\" or \"buggier\" for such companies then why is it used so little outside the game-dev or scientific/HPC industries?</strong> </p>\n"},{"tags":["performance","node.js","openssl","spawn"],"answer_count":0,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":13,"score":0,"question_id":13077869,"title":"What are the drawbacks of using child_process.spawn vs. native bindings in NodeJS","body":"<p>I'm currently working on a NodeJS SCEP server. It heavily uses the <code>openssl</code> executable to work.</p>\n\n<p>Basically, I'm using <code>child_process.spawn</code> to wrap openssl functionality.</p>\n\n<p>I'm probably sure it would be better to use native bindings to the openssl lib, but since it is not available (<code>crypto</code> does not have all the smime.verify smime.decrypt req.verify stuff), I had to find an easy way to make it work, but I'm not sure it is a viable solution.</p>\n\n<p>To me the first drawback is the unstable API, I do rely on a cli output that can change between servers, etc. But what about performance?</p>\n"},{"tags":["sql","performance","ms-access"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":43,"score":0,"question_id":13075009,"title":"typeperf logging to SQL","body":"<p>I am trying to log typeperf to an ACCESS database and am not having any luck.  I created an access database, added the File DSN, and when I try to run the command:</p>\n\n<p>typeperf -si 15 -f SQL \"\\Process(*)\\ID Process\" -o SQL:accessDSN!log</p>\n\n<p>the output just says Unknown.  </p>\n"},{"tags":["c++","performance"],"answer_count":5,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":105,"score":0,"question_id":13073909,"title":"How to optimize input/output in C++","body":"<p>I'm solving a problem which requires very fast input/output. More precisely, the input data file will be up to 15MB. Is there a fast, way to read/print integer values.</p>\n\n<p><em>Note:</em> I don't know if it helps, but the input file has the following form: </p>\n\n<ul>\n<li>line 1: a number n</li>\n<li>line 2..n+1: three numbers a,b,c;</li>\n<li>line n+2: a number r</li>\n<li>line n+3..n+4+r: four numbers a,b,c,d</li>\n</ul>\n\n<p><em>Note 2:</em> The input file will be <code>stdin</code>.</p>\n\n<p><strong>Edit:</strong> Something like the following isn't fast enough:</p>\n\n<pre><code>void fast_scan(int &amp;n) {\n  char buffer[10];\n  gets(buffer);\n  n=atoi(buffer);\n}\n\nvoid fast_scan_three(int &amp;a,int &amp;b,int &amp;c) {\n  char buffval[3][20],buffer[60];\n  gets(buffer);\n  int n=strlen(buffer);\n  int buffindex=0, curindex=0;\n  for(int i=0; i&lt;n; ++i) {\n    if(!isdigit(buffer[i]) &amp;&amp; !isspace(buffer[i]))break;\n    if(isspace(buffer[i])) {\n      buffindex++;\n      curindex=0;\n    } else {\n      buffval[buffindex][curindex++]=buffer[i];\n    }\n  }\n  a=atoi(buffval[0]);\n  b=atoi(buffval[1]);\n  c=atoi(buffval[2]);\n}\n</code></pre>\n"},{"tags":["java","android","c","performance","android-ndk"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":108,"score":0,"question_id":12998245,"title":"JAVA Matrix-Vector-Multiplication 100 times slower than C-Version","body":"<p>im working on performance-differences between Android JAVA- and Android NDK-applications.\nI performed a Matrix4D-Vector4D Transformation on more than 90000 vertices as an example for 3D Graphics.</p>\n\n<p>It seemes, that the JAVA Version is nearly <strong>100 times slower</strong> than the C-Version. Did i something wrong? Does anyone have similar experiences?</p>\n\n<p>my Java-Code for transformation:</p>\n\n<pre><code>        long t1 = System.nanoTime();\n        for ( int i = 0; i &lt; vCount; i++)\n        {\n\n            Vector4 vOut = new Vector4();\n            Vector4 v = vertices[i];\n\n            vOut.v_[0] = v.v_[0] * matrix[0].v_[0];\n            vOut.v_[1] = v.v_[0] * matrix[0].v_[1];\n            vOut.v_[2] = v.v_[0] * matrix[0].v_[2];\n            vOut.v_[3] = v.v_[0] * matrix[0].v_[3];\n\n            vOut.v_[0] += v.v_[1] * matrix[1].v_[0];\n            vOut.v_[1] += v.v_[1] * matrix[1].v_[1];\n            vOut.v_[2] += v.v_[1] * matrix[1].v_[2];\n            vOut.v_[3] += v.v_[1] * matrix[1].v_[3];\n\n            vOut.v_[0] += v.v_[2] * matrix[2].v_[0];\n            vOut.v_[1] += v.v_[2] * matrix[2].v_[1];\n            vOut.v_[2] += v.v_[2] * matrix[2].v_[2];\n            vOut.v_[3] += v.v_[2] * matrix[2].v_[3];\n\n            vOut.v_[0] += v.v_[3] * matrix[3].v_[0];\n            vOut.v_[1] += v.v_[3] * matrix[3].v_[1];\n            vOut.v_[2] += v.v_[3] * matrix[3].v_[2];\n            vOut.v_[3] += v.v_[3] * matrix[3].v_[3]; \n\n            vertices[i] = vOut;\n\n        }\n        long t2 = System.nanoTime();        \n        long diff = t2 - t1;        \n        double ms = (double)(diff / 1000000.0f);\n        Log.w(\"GL2JNIView\", String.format(\"ms %.2f \", ms));\n</code></pre>\n\n<p>Performance (Transform > 90 000 Vertices | Android 4.0.4 SGS II):\n(Median-value of 200 runs)</p>\n\n<pre><code>JAVA-Version:   2 FPS\nC-Version:    190 FPS\n</code></pre>\n"},{"tags":["performance","mongodb","indexing","schema","geospatial"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":122,"score":1,"question_id":12908871,"title":"MongoDB geospatial query with sort - performance issues","body":"<p>I have query (which is very slow <strong>~2,5s</strong>):</p>\n\n<pre><code>db.markers.find({ latlng: { '$within': { '$box': [ [ -16, -140 ], [ 75, 140 ] ] } } }).sort({_id: -1}).limit(1000)\n</code></pre>\n\n<p>When I run explain for this query I get</p>\n\n<pre><code>{\n   \"cursor\" : \"GeoBrowse-box\",\n   \"isMultiKey\" : false,\n   \"n\" : 1000,\n   \"nscannedObjects\" : 242331,\n   \"nscanned\" : 242331,\n   \"nscannedObjectsAllPlans\" : 242331,\n   \"nscannedAllPlans\" : 242331,\n   \"scanAndOrder\" : true,\n   \"indexOnly\" : false,\n   \"nYields\" : 1383,\n    \"nChunkSkips\" : 0,\n    \"millis\" : 2351,\n    \"indexBounds\" : {\n        \"latlng\" : [ ]\n    },\n    \"lookedAt\" : NumberLong(262221),\n    \"matchesPerfd\" : NumberLong(242331),\n    \"objectsLoaded\" : NumberLong(242331),\n    \"pointsLoaded\" : NumberLong(0),\n    \"pointsSavedForYield\" : NumberLong(0),\n    \"pointsChangedOnYield\" : NumberLong(0),\n    \"pointsRemovedOnYield\" : NumberLong(0),\n    \"server\" : \"xx:27017\"\n}\n</code></pre>\n\n<p>When I remove <strong>sort({_id: -1})</strong> explain gives me (fast query <strong>5 milis</strong>):</p>\n\n<pre><code>{\n    \"cursor\" : \"GeoBrowse-box\",\n    \"isMultiKey\" : false,\n    \"n\" : 1000,\n    \"nscannedObjects\" : 1000,\n    \"nscanned\" : 1000,\n    \"nscannedObjectsAllPlans\" : 1000,\n    \"nscannedAllPlans\" : 1000,\n    \"scanAndOrder\" : false,\n    \"indexOnly\" : false,\n    \"nYields\" : 0,\n    \"nChunkSkips\" : 0,\n    \"millis\" : 5,\n    \"indexBounds\" : {\n        \"latlng\" : [ ]\n    },\n    \"lookedAt\" : NumberLong(1000),\n    \"matchesPerfd\" : NumberLong(1000),\n    \"objectsLoaded\" : NumberLong(1000),\n    \"pointsLoaded\" : NumberLong(0),\n    \"pointsSavedForYield\" : NumberLong(0),\n    \"pointsChangedOnYield\" : NumberLong(0),\n    \"pointsRemovedOnYield\" : NumberLong(0),\n        \"server\" : \"xx:27017\"\n}\n</code></pre>\n\n<p>I have  2d index on latlng, desc index on _id and <strong>compound</strong> indexes.</p>\n\n<pre><code>db.markers.ensureIndex({latlng: '2d', _id:-1})\ndb.markers.ensureIndex({ latlng: '2d' })\ndb.markers.ensureIndex({ _id: -1 })\n</code></pre>\n\n<p>What I want to achieve is to get markers from a particular area sorted from newest.</p>\n\n<p>Any ideas or suggestions how to do a lot less than <strong>2.5 seconds</strong>??</p>\n\n<p>If someone wants to do their own tests</p>\n\n<pre><code>var i = 0,\n  lat = 0,\n  lng = 0;\n\nfor (i; i &lt; 260000; i++) {\n  lat = parseFloat(Math.min(-90 + (Math.random() * 180), 90).toFixed(6));\n  lng = parseFloat(Math.min(-180 + (Math.random() * 360), 180).toFixed(6));\n  collection.insert({latlng: [lat, lng]}, function () {});\n}\n\ncollection.find({ latlng: { '$within': { '$box': [ [ -90, -180 ], [ 90, 180 ] ] } } }, {latlng: 1, _id: 1 }).sort({_id: -1}).limit(1000).explain()\n</code></pre>\n\n<p>On my local machine I receives (<strong>~ 2,6s</strong>):</p>\n\n<pre><code>{\n    \"cursor\" : \"GeoBrowse-box\",\n    \"isMultiKey\" : false,\n    \"n\" : 1000,\n    \"nscannedObjects\" : 260000,\n    \"nscanned\" : 260000,\n    \"nscannedObjectsAllPlans\" : 260000,\n    \"nscannedAllPlans\" : 260000,\n    \"scanAndOrder\" : true,\n    \"indexOnly\" : false,\n    \"nYields\" : 1612,\n    \"nChunkSkips\" : 0,\n    \"millis\" : 2613,\n    \"indexBounds\" : {\n            \"latlng\" : [ ]\n    },\n    \"lookedAt\" : NumberLong(260000),\n    \"matchesPerfd\" : NumberLong(260000),\n    \"objectsLoaded\" : NumberLong(260000),\n    \"pointsLoaded\" : NumberLong(0),\n    \"pointsSavedForYield\" : NumberLong(0),\n    \"pointsChangedOnYield\" : NumberLong(0),\n    \"pointsRemovedOnYield\" : NumberLong(0),\n    \"server\" : \"xx:27017\"\n}\n</code></pre>\n\n<p>Thx</p>\n"},{"tags":["python","performance","optimization","if-statement"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":64,"score":2,"question_id":13073777,"title":"Python: improve in elegant way (code saving) a function in order to avoid several Statements","body":"<p>I wrote this function to read <a href=\"http://www.liblas.org/tutorial/python.html\" rel=\"nofollow\">Las</a> file and save a shapefile. The function creates a shapefile with 8 fields. What i wish insert a <strong>parse</strong> element in the function in order to select the fields i wish to save LAS2SHP(inFile,outFile=None,parse=None). if None all fields are saved. if <strong>parse</strong> is \nparse=\"irn\" the fields intensity, return_number, and number_of_returns are saved. following the legend</p>\n\n<pre><code>\"i\": p.intensity,\n\"r\": p.return_number,\n\"n\": p.number_of_returns,\n\"s\": p.scan_direction,\n\"e\": p.flightline_edge,\n\"c\": p.classification,\n\"a\": p.scan_angle, \n</code></pre>\n\n<p>I wrote a solution if....ifelse....else really code consuming (and not elegant). Thanks for all helps and suggestions for saving code </p>\n\n<p>thanks in advance\nGianni</p>\n\n<p>here the original function in python</p>\n\n<pre><code>import shapefile\nfrom liblas import file as lasfile\n\ndef LAS2SHP(inFile,outFile=None):\n    w = shapefile.Writer(shapefile.POINT)\n    w.field('Z','C','10')\n    w.field('Intensity','C','10')\n    w.field('Return','C','10')\n    w.field('NumberRet','C','10')\n    w.field('ScanDir','C','10')\n    w.field('FlightEdge','C','10')\n    w.field('Class','C','10')\n    w.field('ScanAngle','C','10')\n    for p in lasfile.File(inFile,None,'r'):\n        w.point(p.x,p.y)\n        w.record(float(p.z),float(p.intensity),float(p.return_number),float(p.number_of_returns),float(p.scan_direction),float(p.flightline_edge),float(p.classification),float(p.scan_angle))\n    if outFile == None:\n        inFile_path, inFile_name_ext = os.path.split(os.path.abspath(inFile))\n        inFile_name = os.path.splitext(inFile_name_ext)[0]\n        w.save(\"{0}\\\\{1}.shp\".format(inFile_path,inFile_name))\n    else:\n        w.save(outFile)\n</code></pre>\n"},{"tags":["json","performance","servlets","jersey","response"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":56,"score":0,"question_id":13043590,"title":"Jersey response very slow","body":"<p>I am using jersey for REST service, and during measuring request time I encountered a delay in the response which seems to come from jersey itself.</p>\n\n<p>While the computation of the request is about 8 seconds (testsystem), the time until jersey delivers the request is additionally 12 seconds. The 8 seconds are measured from the first jersey \"Server in-bound request\" after the request ist build in our code:</p>\n\n<pre><code>Response.ok( entity ).build( );\n</code></pre>\n\n<p>Then, some magic in jersey happens and this magic seems to last 12 sec until the \" Server out-bound response\" log appears and the json stuff appears on the console. cURL says, that the request lasted 20secs. The resonse contains 32 more or less big json data strings.</p>\n\n<p>I have already turned off the link and logging filter in web.xml, no improvements.</p>\n\n<p>I have no idea where to start searching for. Thanks for your ideas!</p>\n\n<p>We</p>\n"},{"tags":["c++","c","performance","optimization"],"answer_count":3,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":92,"score":1,"question_id":13058315,"title":"How to optimize simple gaussian filter for performance?","body":"<p>I am trying to write an android app which needs to calculate gaussian and laplacian pyramids for multiple full resolution images, i wrote this it on C++ with NDK, the most critical part of the code is applying gaussian filter to images abd i am applying this filter with horizontally and vertically.   </p>\n\n<p>The filter is (0.0625, 0.25, 0.375, 0.25, 0.0625)\nSince i am working on integers i am calculating (1, 4, 6, 4, 1)/16</p>\n\n<pre><code>dst[index] = ( src[index-2] + src[index-1]*4 + src[index]*6+src[index+1]*4+src[index+2])/16;\n</code></pre>\n\n<p>I have made a few simple optimization however it still is working slow than expected and i was wondering if there are any other optimization options that i am missing. </p>\n\n<p>PS: I should mention that i have tried to write this filter part with inline arm assembly however it give 2x slower results.</p>\n\n<pre><code>//horizontal  filter\nfor(unsigned y = 0; y &lt; height;  y++) {\n    for(unsigned x = 2; x &lt; width-2;  x++) {\n        int index = y*width+x;\n            dst[index].r = (src[index-2].r+ src[index+2].r + (src[index-1].r + src[index+1].r)*4 + src[index].r*6)&gt;&gt;4;\n            dst[index].g = (src[index-2].g+ src[index+2].g + (src[index-1].g + src[index+1].g)*4 + src[index].g*6)&gt;&gt;4;\n            dst[index].b = (src[index-2].b+ src[index+2].b + (src[index-1].b + src[index+1].b)*4 + src[index].b*6)&gt;&gt;4;                \n     }\n}\n//vertical filter\nfor(unsigned y = 2;  y &lt; height-2;  y++) {\n    for(unsigned x = 0;  x &lt; width;  x++) {\n        int index = y*width+x;\n            dst[index].r = (src[index-2*width].r + src[index+2*width].r  + (src[index-width].r + src[index+width].r)*4 + src[index].r*6)&gt;&gt;4;\n            dst[index].g = (src[index-2*width].g + src[index+2*width].g  + (src[index-width].g + src[index+width].g)*4 + src[index].g*6)&gt;&gt;4;\n            dst[index].b = (src[index-2*width].b + src[index+2*width].b  + (src[index-width].b + src[index+width].b)*4 + src[index].b*6)&gt;&gt;4;\n     }\n}\n</code></pre>\n"},{"tags":["performance","cassandra"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":41,"score":1,"question_id":13020195,"title":"cassandra 1 big column vs multi small columns in read performance?","body":"<p>I'm getting around 1000 distinct events per second, (4 nodes cluster). After each event I will need to increase some counters. My question is, is it better to have a normal column family which has only one column and all the counters are treated like string with comma \",\" separated (example: \"1,3,5,6,0,2\") or it is better to create a Counter Column family with multiple columns? I read some document it says that counter column family can do read and write with consistency level 1 which is fast for reading. I don't really care much about write performance.</p>\n"},{"tags":["performance","multithreading","jvm","multicore"],"answer_count":4,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":4247,"score":2,"question_id":1649402,"title":"Would a multithreaded Java application exploit a multi-core machine very well?","body":"<p>If I write a multi-threaded java application, will the JVM take care of utilizing all available cores? Do I have to do some work?</p>\n"},{"tags":["performance","node.js","redis"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":791,"score":4,"question_id":5709866,"title":"Redis performance issues?","body":"<p>I was trying to put some heavy load on my Redis for testing purposes and find out any upper limits. First I loaded it with 50,000 and 100,000 keys of size 32characters with values around 32 characters. It took no more than 8-15 seconds in both key sizes. Now I try to put 4kb of data as value for each key. First 10000 keys take 800 milli seconds to set. But from that point it slows down gradually and to set whole 50,000 keys it takes aroudn 40 minutes. I am loading the database using NodeJs with <a href=\"http://github.com/mranney/node_redis\" rel=\"nofollow\">node_redis (Mranney)</a> . Is there any mistake I am doing or is Redis just that slow with big values of size 4 KB?</p>\n\n<p>One more thing I found now is when I run another client parallel to the current one and update keys this 2nd client finishes up loading the 50000 keys with 4kb values within 8 seconds while the first client still does its thing forever. Is it a bug in node or the redis library? This is alarming and not acceptable for production.</p>\n"},{"tags":["javascript","performance","optimization","coding-style"],"answer_count":7,"favorite_count":8,"up_vote_count":34,"down_vote_count":0,"view_count":8629,"score":34,"question_id":143486,"title":"Unobtrusive JavaScript: <script> at the top or the bottom of the HTML code?","body":"<p>I've recently read the Yahoo manifesto <a href=\"http://developer.yahoo.com/performance/rules.html#postload\" rel=\"nofollow\">Best Practices for Speeding Up Your Web Site</a>. They recommend to put the JavaScript inclusion at the bottom of the HTML code when we can.</p>\n\n<p>But where exactly and when?</p>\n\n<p>Should we put it before closing <code>&lt;/html&gt;</code> or after ? And above all, when should we still put it in the <code>&lt;head&gt;</code> section?</p>\n"},{"tags":["php","performance","caching","apc"],"answer_count":6,"favorite_count":29,"up_vote_count":55,"down_vote_count":0,"view_count":37721,"score":55,"question_id":911158,"title":"How to clear APC cache entries?","body":"<p>I need to clear all APC cache entries when I deploy a new version of the site.\nAPC.php has a button for clearing all opcode caches, but I don't see buttons for clearing all User Entries, or all System Entries, or all Per-Directory Entries.</p>\n\n<p>Is it possible to clear all cache entries via the command-line, or some other way?</p>\n"},{"tags":["ruby-on-rails","ruby-on-rails-3","performance","where"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":47,"score":2,"question_id":13054759,"title":"Rails efficiency of where clause","body":"<p>I'm worried about the efficiency of this line in the controller of my Rails project</p>\n\n<pre><code>posts_list = Post.where(:title =&gt; params[:title])\n</code></pre>\n\n<p>If the number of \"Posts\" in the database grows, will the line become slow to execute? Is there any possible optimization?</p>\n"},{"tags":["c#","multithreading","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":100,"score":4,"question_id":13070339,"title":"Parallel.For vs regular threads","body":"<p>I'm trying to understand why Parallel.For is able to outperform a number of threads in the following scenario: consider a batch of jobs that can be processed in parallel. While processing these jobs, new work may be added, which then needs to be processed as well. The <code>Parallel.For</code> solution would look as follows:</p>\n\n<pre><code>var jobs = new List&lt;Job&gt; { firstJob };\nint startIdx = 0, endIdx = jobs.Count;\nwhile (startIdx &lt; endIdx) {\n  Parallel.For(startIdx, endIdx, i =&gt; WorkJob(jobs[i]));\n  startIdx = endIdx; endIdx = jobs.Count;\n}\n</code></pre>\n\n<p>This means that there are multiple times where the Parallel.For needs to synchronize. Consider a bread-first graph algorithm algorithm; the number of synchronizations would be quite large. Waste of time, no?</p>\n\n<p>Trying the same in the old-fashioned threading approach:</p>\n\n<pre><code>var queue = new ConcurrentQueue&lt;Job&gt; { firstJob };\nvar threads = new List&lt;Thread&gt;();\nvar waitHandle = new AutoResetEvent(false);\nint numBusy = 0;\nfor (int i = 0; i &lt; maxThreads; i++) \n  threads.Add(new Thread(new ThreadStart(delegate {\n    while (!queue.IsEmpty || numBusy &gt; 0) {\n      if (queue.IsEmpty)\n        // numbusy &gt; 0 implies more data may arrive\n        waitHandle.WaitOne();\n\n      Job job;\n      if (queue.TryDequeue(out job)) {\n        Interlocked.Increment(ref numBusy);\n        WorkJob(job); // WorkJob does a waitHandle.Set() when more work was found\n        Interlocked.Decrement(ref numBusy);\n      }\n    }\n    // others are possibly waiting for us to enable more work which won't happen\n    waitHandle.Set(); \n})));\nthreads.ForEach(t =&gt; t.Start());\nthreads.ForEach(t =&gt; t.Join());\n</code></pre>\n\n<p>The <code>Parallel.For</code> code is of course much cleaner, but what I cannot comprehend, it's even faster as well! Is the task scheduler just that good? The synchronizations were eleminated, there's no busy waiting, yet the threaded approach is consistently slower (for me). What's going on? Can the threading approach be made faster?</p>\n\n<p>Edit: thanks for all the answers, I wish I could pick multiple ones. I chose to go with the one that also shows an actual possible improvement.</p>\n"},{"tags":["c","performance","load-testing","qa","loadrunner"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":43,"score":1,"question_id":13063840,"title":"web_reg_find vs strstr(). Performance","body":"<p>What do you think which operation is fastest and creating less loading, LR <code>web_reg_find()</code> or C <code>strstr()</code>? Which is more preferable for a very strong loading test?</p>\n\n<p>And if somebody knows how does <code>web_reg_find()</code> works, please tell me.</p>\n"},{"tags":["performance","java-ee","glassfish-3","cpu-usage"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":1320,"score":1,"question_id":9158842,"title":"Glassfish - high CPU usge","body":"<p>I'm using Glassfish 3.1.1 web profile in production envirnoment and it's eating too much CPU.\nHere is server settings: Windows Server 2008 R2 64bit, Intel Xeon 8core @ 3,2GHz, 8GB ram.\nI'm using JDK 1.7u2 64bit. Glassfish JVM settings:</p>\n\n<pre><code>&lt;jvm-options&gt;-XX:+UseCompressedOops&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-Xmn1g&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-Xss128k&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-XX:+UseParallelOldGC&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-XX:ParallelGCThreads=4&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-Xmx3g&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-XX:+DisableExplicitGC&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-d64&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-XX:PermSize=256m&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-Xms3g&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-XX:MaxPermSize=256m&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-XX:+AggressiveHeap&lt;/jvm-options&gt;\n</code></pre>\n\n<p>I've also tune some options according to <a href=\"http://jfarcand.wordpress.com/2009/11/27/putting-glassfish-v3-in-production-essential-surviving-guide/\" rel=\"nofollow\">http://jfarcand.wordpress.com/2009/11/27/putting-glassfish-v3-in-production-essential-surviving-guide/</a>\nRunning for few hours or 1 day Glassfish use 15% to 40% CPU, someties 90% and application dosen't respond.</p>\n\n<pre><code>[#|2012-02-03T10:30:46.837+0100|WARNING|glassfish3.1.1|com.sun.grizzly.config.GrizzlyServiceListener|_ThreadID=43;_ThreadName=Thread-2;|GRIZZLY0023: Interrupting idle Thread: http-thread-pool-80(32).|#]\n\n[#|2012-02-03T10:30:55.074+0100|WARNING|glassfish3.1.1|com.sun.grizzly.config.GrizzlyServiceListener|_ThreadID=41;_ThreadName=Thread-2;|GRIZZLY0023: Interrupting idle Thread: http-thread-pool-80(41).|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|java.nio.channels.ClosedChannelException\n    at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:249)\n    at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:440)\n    at com.sun.grizzly.util.OutputWriter.flushChannel(OutputWriter.java:108)\n    at com.sun.grizzly.util.OutputWriter.flushChannel(OutputWriter.java:76)\n    at com.sun.grizzly.http.SocketChannelOutputBuffer.flushChannel(SocketChannelOutputBuffer.java:326)\n    at com.sun.grizzly.http.SocketChannelOutputBuffer.flushBuffer(SocketChannelOutputBuffer.java:398)\n    at com.sun.grizzly.http.SocketChannelOutputBuffer.realWriteBytes(SocketChannelOutputBuffer.java:282)\n    at com.sun.grizzly.tcp.http11.InternalOutputBuffer$OutputStreamOutputBuffer.doWrite(InternalOutputBuffer.java:898)\n    at com.sun.grizzly.tcp.http11.filters.ChunkedOutputFilter.doWrite(ChunkedOutputFilter.java:167)\n    at com.sun.grizzly.tcp.http11.filters.GzipOutputFilter$FakeOutputStream.write(GzipOutputFilter.java:223)\n    at java.util.zip.GZIPOutputStream.finish(GZIPOutputStream.java:169)\n    at java.util.zip.DeflaterOutputStream.close(DeflaterOutputStream.java:238)\n    at com.sun.grizzly.tcp.http11.filters.GzipOutputFilter.recycle(GzipOutputFilter.java:186)\n    at com.sun.grizzly.http.SocketChannelOutputBuffer.recycle(SocketChannelOutputBuffer.java:417)\n    at com.sun.grizzly.http.ProcessorTask.finishResponse(ProcessorTask.java:817)\n    at com.sun.grizzly.http.ProcessorTask.postResponse(ProcessorTask.java:750)\n    at com.sun.grizzly.http.ProcessorTask.doProcess(ProcessorTask.java:726)\n    at com.sun.grizzly.http.ProcessorTask.process(ProcessorTask.java:1019)\n    at com.sun.grizzly.http.DefaultProtocolFilter.execute(DefaultProtocolFilter.java:225)\n    at com.sun.grizzly.DefaultProtocolChain.executeProtocolFilter(DefaultProtocolChain.java:137)\n    at com.sun.grizzly.DefaultProtocolChain.execute(DefaultProtocolChain.java:104)\n    at com.sun.grizzly.DefaultProtocolChain.execute(DefaultProtocolChain.java:90)\n    at com.sun.grizzly.http.HttpProtocolChain.execute(HttpProtocolChain.java:79)\n    at com.sun.grizzly.ProtocolChainContextTask.doCall(ProtocolChainContextTask.java:54)\n    at com.sun.grizzly.SelectionKeyContextTask.call(SelectionKeyContextTask.java:59)\n    at com.sun.grizzly.ContextTask.run(ContextTask.java:71)\n    at com.sun.grizzly.util.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:532)\n    at com.sun.grizzly.util.AbstractThreadPool$Worker.run(AbstractThreadPool.java:513)\n    at java.lang.Thread.run(Thread.java:722)\n|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:249)|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:440)|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.util.OutputWriter.flushChannel(OutputWriter.java:108)|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.util.OutputWriter.flushChannel(OutputWriter.java:76)|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.SocketChannelOutputBuffer.flushChannel(SocketChannelOutputBuffer.java:326)|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.SocketChannelOutputBuffer.flushBuffer(SocketChannelOutputBuffer.java:398)|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.SocketChannelOutputBuffer.realWriteBytes(SocketChannelOutputBuffer.java:282)|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.tcp.http11.InternalOutputBuffer$OutputStreamOutputBuffer.doWrite(InternalOutputBuffer.java:898)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.tcp.http11.filters.ChunkedOutputFilter.doWrite(ChunkedOutputFilter.java:167)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.tcp.http11.filters.GzipOutputFilter$FakeOutputStream.write(GzipOutputFilter.java:223)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at java.util.zip.GZIPOutputStream.finish(GZIPOutputStream.java:169)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at java.util.zip.DeflaterOutputStream.close(DeflaterOutputStream.java:238)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.tcp.http11.filters.GzipOutputFilter.recycle(GzipOutputFilter.java:186)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.SocketChannelOutputBuffer.recycle(SocketChannelOutputBuffer.java:417)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.ProcessorTask.finishResponse(ProcessorTask.java:817)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.ProcessorTask.postResponse(ProcessorTask.java:750)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.ProcessorTask.doProcess(ProcessorTask.java:726)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.ProcessorTask.process(ProcessorTask.java:1019)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.DefaultProtocolFilter.execute(DefaultProtocolFilter.java:225)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.DefaultProtocolChain.executeProtocolFilter(DefaultProtocolChain.java:137)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.DefaultProtocolChain.execute(DefaultProtocolChain.java:104)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.DefaultProtocolChain.execute(DefaultProtocolChain.java:90)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.HttpProtocolChain.execute(HttpProtocolChain.java:79)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.ProtocolChainContextTask.doCall(ProtocolChainContextTask.java:54)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.SelectionKeyContextTask.call(SelectionKeyContextTask.java:59)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.ContextTask.run(ContextTask.java:71)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.util.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:532)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.util.AbstractThreadPool$Worker.run(AbstractThreadPool.java:513)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at java.lang.Thread.run(Thread.java:722)|#]\n\n[#|2012-02-03T10:31:00.440+0100|WARNING|glassfish3.1.1|com.sun.grizzly.config.GrizzlyServiceListener|_ThreadID=40;_ThreadName=Thread-2;|GRIZZLY0023: Interrupting idle Thread: http-thread-pool-80(38).|#]\n\n[#|2012-02-03T10:33:49.170+0100|WARNING|glassfish3.1.1|com.sun.grizzly.config.GrizzlyServiceListener|_ThreadID=40;_ThreadName=Thread-2;|GRIZZLY0023: Interrupting idle Thread: http-thread-pool-80(11).|#]\n\n[#|2012-02-03T10:33:57.235+0100|WARNING|glassfish3.1.1|com.sun.grizzly.config.GrizzlyServiceListener|_ThreadID=43;_ThreadName=Thread-2;|GRIZZLY0023: Interrupting idle Thread: http-thread-pool-80(16).|#]\n</code></pre>\n\n<p>Edit: I solved the problem, there is bug in Glassfish gzip compression, so I turned it off.\n<a href=\"http://www.java.net/forum/topic/glassfish/glassfish/glassfish-301-gzip-problem-threads-apparently-spinning-100-cpu-use\" rel=\"nofollow\">http://www.java.net/forum/topic/glassfish/glassfish/glassfish-301-gzip-problem-threads-apparently-spinning-100-cpu-use</a></p>\n\n<p>Instead I would use this: <a href=\"http://www.servletsuite.com/servlets/gzipflt.htm\" rel=\"nofollow\">http://www.servletsuite.com/servlets/gzipflt.htm</a> to compress content.</p>\n"},{"tags":["ruby-on-rails","performance","views","newrelic-rpm"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":42,"score":1,"question_id":13047992,"title":"Slow rendering of Rails view according to newrelic statistics","body":"<p>I am seeing a performance problem with rails view rendering.\nAccording to Newrelic Stats:</p>\n\n<p><img src=\"http://s10.postimage.org/e155cw0iv/Screen_Shot_2012_10_24_at_1_51_05_PM.png\" alt=\"newrelic stats\"></p>\n\n<p>This slow rendering <em>randomly</em> appears during requests.</p>\n\n<ul>\n<li>Could this problem be connected to slow file reading from the file system?</li>\n<li>Is there any way to debug it in lower level?</li>\n</ul>\n"},{"tags":["c#","windows","performance","hardware"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":34,"score":1,"question_id":13068759,"title":"How do I best-determine system requirements for a new app?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/170939/how-to-specify-the-hardware-your-software-needs\">How to specify the hardware your software needs?</a>  </p>\n</blockquote>\n\n\n\n<p>How do you determine the system requirements of a user's PC in order for them to install and run your software?</p>\n\n<p>I am aware of the obvious, such as Windows, .NET Framework [version number]. But how do you come up with the correct RAM, Processor and all of that?</p>\n\n<p>Is this just something that you observe while you're debugging your app? Do you just check out the Resource Monitor and watch for how much Disk usage your app is using, or how much memory it is taking up?</p>\n\n<p>Are there any tools, or would you recommend I use tools to help determine system requirements for my applications?</p>\n\n<p>I've searched for this but I have not been able to find much information.</p>\n\n<p><em>More importantly, what about the Windows Experience Index? I've seen a few box apps in the shop say you need a Windows Exp. Index of N, but are there tools that determine what index is required for my app to run?</em></p>\n"},{"tags":["performance","class","struct",".net-4.0"],"answer_count":5,"favorite_count":5,"up_vote_count":19,"down_vote_count":0,"view_count":1484,"score":19,"question_id":2410710,"title":"Why is the new Tuple type in .Net 4.0 a reference type (class) and not a value type (struct)","body":"<p>Does anyone know the answer and/or have an oppinion about it?</p>\n\n<p>Since tuples would normally not be very large I would assume it would make more sense to use structs than classes for these. What say you?</p>\n"},{"tags":["c#","performance","clr","gettype"],"answer_count":5,"favorite_count":6,"up_vote_count":28,"down_vote_count":0,"view_count":11645,"score":28,"question_id":686412,"title":"C# 'is' operator performance","body":"<p>I have a program that requires fast performance.  Within one of its inner loops, I need to test the type of an object to see whether it inherits from a certain interface.</p>\n\n<p>One way to do this would be with the CLR's built-in type-checking functionality.  The most elegant method there probably being the 'is' keyword:</p>\n\n<pre><code>if (obj is ISpecialType)\n</code></pre>\n\n<p>Another approach would be to give the base class my own virtual GetType() function which returns a pre-defined enum value (in my case, actually, i only need a bool).  That method would be fast, but less elegant.</p>\n\n<p>I have heard that there is an IL instruction specifically for the 'is' keyword, but that doesn't mean it executes fast when translated into native assembly.  Can anyone share some insight into the performance of 'is' versus the other method?</p>\n\n<p><strong>UPDATE:</strong>  Thanks for all the informed answers!  It seem a couple helpful points are spread out among the answers:  Andrew's point about 'is' automatically performing a cast is essential, but the performance data gathered by Binary Worrier and Ian is also extremely useful.  It would be great if one of the answers were edited to include <em>all</em> of this information.</p>\n"},{"tags":["c++","performance","function","object","functional-programming"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":140,"score":2,"question_id":13067432,"title":"Fast function objects in C++?","body":"<p>I'm writing a program for scientific computing and my foremost interest (after correctness) is speed. Recently I have noticed that I need readable code too. :)</p>\n\n<p>Instead of writing</p>\n\n<pre><code>for (int k=0;k!=10;k+=1)\n   array[k] = fun(a, k);\n</code></pre>\n\n<p>I'm considering writing</p>\n\n<pre><code>class fun_t {\nprivate:\n   type a;\npublic:\n   fun_t(type in) : a(in) {};\n\n   type operator() (int k) {\n      ...computation...\n   }\n};\n...\nfun_t fun(a);\nfor (int k=0;k!=10;k+=1)\n   array[k] = fun(k);\n</code></pre>\n\n<p>Will the function object style be as fast as the first example? Can I expect the same inlinings in both? Is there a better a way? (Note that I'm only presenting the idea here, this is not my actual code.)</p>\n"},{"tags":["c++","performance","void-pointers"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":98,"score":1,"question_id":3224467,"title":"What's the best way to make a bitwise function work for any type of integer input c++?","body":"<p>So I need to read flags in bits and set flags in bits. These bits are in various sizes of integer: int16, int32, int64, etc.</p>\n\n<p>I would like to have a function that does something like this:</p>\n\n<pre><code>static integertype function(integertype data, char startbit, char endbit);\n</code></pre>\n\n<p>I don't want to code what will be the same code to isolate bits from for different sizes of integers in separate but identical functions (for the multitude of bit functions I want to write).</p>\n\n<p>I thought about using a void pointer for the data so everything could run through one function. Is this a bad design? What about as far as efficiency goes? I have no concept of bad/good design due to my inexperience.</p>\n\n<pre><code>static int function(void *data, char startbit, char endbit)\n</code></pre>\n\n<p>These flags have to be looked at very often as this is for a data acquisition system. Would a void pointer implementation be reasonably efficient?</p>\n\n<p>I know premature optimization is bad, but I would like to know what things are generally less or more efficient than others so I can make good decisions.</p>\n\n<p>Thanks in advance for taking me to school.</p>\n"},{"tags":["python","performance","swap"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":126,"score":2,"question_id":13040891,"title":"Python Swap two digits in a number?","body":"<p>What is the fastest way to swap two digits in a number in Python?  I am given the numbers as strings, so it'd be nice if I could have something as fast as</p>\n\n<pre><code>string[j] = string[j] ^ string[j+1] \nstring[j+1] = string[j] ^ string[j+1]\nstring[j] = string[j] ^ string[j+1]\n</code></pre>\n\n<p>Everything I've seen has been much more expensive than it would be in C, and involves making a list and then converting the list back or some variant thereof.</p>\n"},{"tags":["performance","query","optimization","expressionengine"],"answer_count":6,"favorite_count":1,"up_vote_count":9,"down_vote_count":0,"view_count":139,"score":9,"question_id":13041463,"title":"What's the optimal amount of queries an ExpressionEngine page should load?","body":"<p>I saw @parscale tweet: <a href=\"https://twitter.com/parscale/status/260808185424781312\">How many queries are you happy with for a home page? When do you say this is Optimized?</a></p>\n\n<p>I saw responses that &lt; 50 is good, 30 or less is best, and 100+ is danger zone. Is there really any proper number? And if say you do have > 50 queries running on your pages, what are some ways to bring it down?</p>\n\n<p>I generally have sites that run the gamut that are under 50 queries and some more, though the \"more\" don't seem to be too slow, I'm always interested in making it faster. How?</p>\n"},{"tags":["java","performance","jsf","profiling","profiler"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":2,"view_count":26,"score":-2,"question_id":13066057,"title":"creating own web profiler","body":"<p>i need to create my own profiler which could profile some thing like netbeans profiler, which shows execution time of each method and class. i found a link which gives basics of  profiling web application. </p>\n\n<p><a href=\"http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/servletFilters/servlet-filters.html\" rel=\"nofollow\">http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/servletFilters/servlet-filters.html</a></p>\n\n<p>is it possible to use the above method and create a tool which can profile any war files. </p>\n"},{"tags":["android","performance","hierarchy"],"answer_count":1,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":378,"score":2,"question_id":4437796,"title":"hierarchy viewer results interpretation","body":"<p>Some days ago I've installed the updates for ADT Plugin in Eclipse and I've just tried the new <strong>Hierarchy viewer</strong> tool. Beside it took me a while to find that the measurements are calculated only at the first (parent) view, I have observed that the values shown are not the same every time, even if there are no changes in my application.</p>\n\n<p>For example, I've launched in the emulator one of my applications, loaded it in the Hierarchy viewer and got the next results: (I've wanted to include the screen shots, but my reputation doesn't allow me to post pictures yet, so I'll just write the values)</p>\n\n<p><strong>Measure:</strong> 175.340 ms<br>\n<strong>Layout:</strong> 5.179 ms<br>\n<strong>Draw:</strong> 47.115 ms       </p>\n\n<p>Then, without any changes, I launched again the application, and got the following results:</p>\n\n<p><strong>Measure:</strong> 98.696 ms<br>\n<strong>Layout:</strong> 4.819 ms<br>\n<strong>Draw:</strong> 50.923 ms </p>\n\n<p>Could someone tell me why there is such a big difference between the values of Measure, for example?</p>\n\n<p>Also, did someone know the meaning / the difference between the 3 values provided: <strong>Measure</strong>, <strong>Layout</strong> and <strong>Draw</strong>? Are each related with some specific attributs of the views?  </p>\n\n<p>The Android developers page doesn't provide too many explanations for that, and all I know is those values must be as small as possible.</p>\n"},{"tags":["c++","performance","thrust","code-timing"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":66,"score":2,"question_id":12869725,"title":"performance tuning a thrust application","body":"<p>I am running a small C++/thrust program (below) on my macbook pro w/ 9600M GT gpu and am interested in understanding where the time is spent in the function h, because the goal is to run this code as quickly as possible for larger values of NEPS.</p>\n\n<p>For that purpose, I have littered the function with clock() calls.</p>\n\n<p>The times printed indicate that almost all of the time is spent in thrust::reduce.\nIndeed, the reported time for thrust::reduce is several hundred times greater than that for thrust::transform, which invokes three calls to cosine per element.  Why?</p>\n\n<p>Naturally, I'm suspicious of the measured times.\nI inserted a 2nd call to thrust::reduce just to see if the time reported would be similar:  it's not.  The time reported for the 2nd call has much higher variance and is smaller.\nMore confusion:  why?</p>\n\n<p>I had also tried using thrust::transform_reduce (commented out) in place of the two kernel calls expecting that to run faster -- instead, it was 4% slower.  Why?</p>\n\n<p>Suggestions appreciated!</p>\n\n<pre><code>#include &lt;thrust/host_vector.h&gt;\n#include &lt;thrust/device_vector.h&gt;\n#include &lt;thrust/sequence.h&gt;\n#include &lt;iostream&gt;\n\n#include &lt;stdio.h&gt;\n#include &lt;stdint.h&gt;\n\n\n float NEPS = 6.0;\n __device__ float EPS;\n __device__ float SQEPS;\n\n __device__ float CNV_win;\n __device__ float CNV_dt;\n int CNV_n;\n float EU_dt;\n\n__host__ __device__ float f(float x,float t){\n    return x*cos(t)+x*cos(t/SQEPS)+cos(t/EPS);\n}\n\nstruct h_functor\n{\n  const float x, t;\n  h_functor(float _x, float _t) : x(_x),t(_t) {}\n  __host__ __device__\n  float operator()(const float &amp; t_f) const {\n    return f(x,   t-CNV_win+CNV_dt*(t_f+1)   )*CNV_dt;\n  } \n};\n\n\nclock_t my_clock() __attribute__ ((noinline));\nclock_t my_clock() {\n  return clock();\n}\nfloat h(float x,float t){\n    float sum;\n\n    sum = CNV_dt*(f(x,t-CNV_win/2)+f(x,t+CNV_win/2))/2;\n    clock_t start = my_clock(), diff1, diff2, diff3, diff4, diff5;\n    thrust::device_vector&lt;float&gt; t_f(CNV_n-2);\n    diff1 = my_clock() - start;\n    /* initialize t_f to 0.. CNV_n-3 */\n    start = my_clock();\n    thrust::sequence(t_f.begin(), t_f.end());\n    diff2 = my_clock() - start;\n\n    start = my_clock();\n    thrust::transform(t_f.begin(), t_f.end(), t_f.begin(), h_functor(x,t));\n    diff3 = my_clock() - start;\n    start = my_clock();\n    sum += thrust::reduce(t_f.begin(), t_f.end());\n    diff4 = my_clock() - start;\n    start = my_clock();\n    sum += thrust::reduce(t_f.begin(), t_f.end());\n    diff5 = my_clock() - start;\n#define usec(d) (d)\n    fprintf(stderr, \"Time taken %ld %ld %ld %ld %ld usecs\\n\", usec(diff1), usec(diff2), usec(diff3), usec(diff4), usec(diff5));\n        /* a bit slower, surprisingly:\n       sum += thrust::transform_reduce(t_f.begin(), t_f.end(), h_functor(x,t), 0, thrust::plus&lt;float&gt;());\n       */\n\n    return sum;\n}\nmain(int argc, char ** argv) {\n  if (argc &gt;= 1) NEPS = strtod(argv[1], 0);\n  fprintf(stderr, \"NEPS = %g\\n\", NEPS);\n\n  EPS= powf(10.0,-NEPS);\n  SQEPS= powf(10.0,-NEPS/2.0);\n  CNV_win= powf(EPS,1.0/4.0);\n  CNV_dt = EPS;\n  CNV_n = powf(EPS,-3.0/4.0);\n  EU_dt = powf(EPS,3.0/4.0);\n\n  cudaMemcpyToSymbol(CNV_win, &amp;CNV_win, sizeof(float));\n  cudaMemcpyToSymbol(CNV_dt, &amp;CNV_dt, sizeof(float));\n  cudaMemcpyToSymbol(SQEPS, &amp;SQEPS, sizeof(float));\n  cudaMemcpyToSymbol(EPS, &amp;EPS, sizeof(float));\n\n  float x=1.0;\n  float t = 0.0;\n  int n = floor(1.0/EU_dt);\n  fprintf(stderr, \"CNV_n = %d\\n\", CNV_n);\n  while (n--) {\n    float sum = h(x,t);\n    x=x+EU_dt*sum;\n    t=t+EU_dt;\n  }\n  printf(\"%f\\n\",x);\n}\n</code></pre>\n"},{"tags":["iphone","performance","opengl-es","computer-vision"],"answer_count":1,"favorite_count":5,"up_vote_count":4,"down_vote_count":0,"view_count":2290,"score":4,"question_id":4270737,"title":"Convolving an image with OpenGL ES on iPhone: possible?","body":"<p>I've googled around a few times, but I have not gotten a straight answer.  I have a matrix that I would like to convolve with a discrete filter (e.g. the Sobel operator for edge detection).  Is it possible to do this in an accelerated way with OpenGL ES on the iPhone?</p>\n\n<p>If it is, how how?  If it is not, are there other high-performance tricks I can use to speed up the operation?  Wizardly ARM assembly operations that can do it fast?  Ultimately I want to perform as fast of a convolution as possible on an iPhone's ARM processor.</p>\n"},{"tags":["java","performance","ssd","ext4"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":71,"score":1,"question_id":13043849,"title":"Whats the fastest way to write a journal to an SSD in Java?","body":"<p>I have small transactions which I need to sync to the filesystem (ext4) onto a SSD within a java program.</p>\n\n<p>What is the fastest way to write these transactions sequencial into a file, if I need to sync with the filesystem after each transaction?\nThe data per transaction is realy small, but it would be ok to write more (to fill a page for example), if that increases transactions/s.</p>\n\n<p>After the data was written once, it is only read.</p>\n\n<p>Also, are there any tweaks to the filesystem that increase the performance?\nIs there a stable filesystem that is suited better for that task?</p>\n\n<p><strong>UPDATE:</strong> This seems to be a filesystem problem. Ext4 is much slower than Ext3, when using sync?\nSuggestion?\n<strong>UPDATE</strong> The solution to the problem is, preallocation the file (in java _file.setLength(size) ). This will cause on ext4 that the space is preallocated and all metadata is writen once. After this, writeing to the file will only edit the user data and metadata is keeped unchanged.\nThis causeed a speedup by factor 10 in my case.</p>\n"},{"tags":["mysql","performance","query","select-query"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":33,"score":1,"question_id":13048951,"title":"I need to speed up specific mysql query on large table","body":"<p>Hi I know there is a lot of topics dedicated to query optimizing strategies, but this one is so specific I couldnt find the answer anywhere on the interenet.</p>\n\n<p>I have large table of product in eshop (appx. 180k rows) and the table has 65 columns. Yeah yeah I know its quite a lot, but I store there information about books, dvds, bluerays and games.</p>\n\n<p>Still I am not considering a lot of cols into query, but the select is still quite tricky. There are many conditions that need to be considered and compared. Query below</p>\n\n<pre><code>SELECT *\nFROM products\nWHERE production = 1 \nAND publish_on &lt; '2012-10-23 11:10:06' \nAND publish_off &gt; '2012-10-23 11:10:06' \nAND price_vat &gt; '0.5' \nAND ean &lt;&gt; ''\nAND publisher LIKE '%Johnny Cash%'\nORDER BY bought DESC, datec DESC, quantity_storage1 DESC, quantity_storege2 DESC, quantity_storage3 DESC\nLIMIT 0, 20\n</code></pre>\n\n<p>I have already tried to put there indexes one by one on cols in where clause and even in order by clause, then I tried to create compound index on (production, publish_on, publish_off, price_vat, ean). </p>\n\n<p>Query is still slow (couple of seconds) and it need to be fast since its eshop solution and people are leaving as they are not getting their results fast. And I am still not counting the time I need to perform the search for all found rows so I can make paging.</p>\n\n<p>I mean, the best way to make it quick is to simplify the query, but all the conditions and sorting is a must in this case. </p>\n\n<p>Can anyone help with this kind of issue? Is it even possible to speed this kind of query up, or is there any other way how I can for example simplify the query and leave the rest on php engine to sort the results.. </p>\n\n<p>Oh, Iam really clueless in this.. Share your wisdom peple, please...</p>\n\n<p>Many thanks in advance</p>\n"},{"tags":["performance","excel","vba","optimization","excel-vba"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":7,"view_count":140,"score":-7,"question_id":13039069,"title":"Optimize VBA Code","body":"<p>Can somebody help me optimize this code.  It is a macro to input formulas into a was-is spreadsheet (deltas, rows subtotals, column subtotals).  It works well with small amounts of data, but with large datadumps it takes upward of 15 minutes and sometimes bombs out excel.  I wrote this code myself, so i am sure there are many ways to make it more efficient.  </p>\n\n<p>Here is the code (Sorry about format):</p>\n\n<pre><code>'Variables\nDim Rcnt As Long\nDim Ccnt As Long\nDim i As Long\nDim j As Long\nDim n As Long\nDim rValues As Long\nDim msgComplete As Long\n\n 'Delete extra rows\nActiveSheet.UsedRange\nRcnt = Cells.SpecialCells(xlLastCell).Row\nCcnt = Cells.SpecialCells(xlLastCell).Column\n\nFor n = 1 To Ccnt\n    If IsError(Application.VLookup(\"Values\", Columns(n), 1, False)) Then\n        'do nothing\n    Else\n        If WorksheetFunction.Match(\"Values\", Columns(n), 0) = 1 Then\n            MsgBox (\"Error.  Did not paste updated pivot table data.  Exiting Macro.\")\n            Exit Sub\n        Else\n            Rows(1 &amp; \":\" &amp; (WorksheetFunction.Match(\"Values\", Columns(n), 0)) - 1).Delete\n        End If\n    End If\nNext n\n\n'Copy down pivot data\nActiveSheet.UsedRange\nRcnt = Cells.SpecialCells(xlLastCell).Row\nCcnt = Cells.SpecialCells(xlLastCell).Column\n\nFor n = WorksheetFunction.Match(\"Values\", Rows(1), 0) - 1 To 1 Step -1\n    For i = 1 To Rcnt\n        If Cells(i, n).Value = \"\" Then\n            Cells(i, n) = Cells(i, n).Offset(-1, 0)\n            Cells(i, n).Font.Color = RGB(255, 255, 255)\n        Else\n            'do nothing\n        End If\n    Next i\nNext n\n\n'Input delta formulas\nFor n = WorksheetFunction.Match(\"Values\", Rows(1), 0) To 1 Step -1\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"BAC Delta\") _\n        Or InStr(1, Cells(i, n), \"EAC Delta\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=R[-1]C-R[-2]C\"\n            Next j\n        End If\n    Next i\nNext n\n\n'Input grand total sum formulas\nFor n = WorksheetFunction.Match(\"Values\", Rows(1), 0) To 1 Step -1\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Revised BAC\") Or InStr(1, Cells(i, n), \"BAC Delta\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Or InStr(1, Cells(i, n), \"EAC Delta\") Then\n            Cells(i, Ccnt) = \"=Sum(\" &amp; Cells(i, Ccnt).Offset(0, -1).Address(False, False) &amp; \":\" _\n            &amp; Cells(i, WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1).Address(False, False) &amp; \")\"\n        End If\n    Next i\nNext n\n\n'Input sumifs formulas\n n = 10\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(n).Offset(0, -2).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -2).Address &amp; \",\" &amp; Columns(n).Offset(0, -3).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -3).Address &amp; \",\" &amp; Columns(n).Offset(0, -4).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -4).Address &amp; \",\" &amp; Columns(n).Offset(0, -5).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -5).Address &amp; \",\" &amp; Columns(n).Offset(0, -6).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -6).Address &amp; \",\" &amp; Columns(n).Offset(0, -7).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -7).Address &amp; \",\" &amp; Columns(n).Offset(0, -8).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -8).Address &amp; \",\" &amp; Columns(n).Offset(0, -9).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -9).Address &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 9\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(n).Offset(0, -2).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -2).Address &amp; \",\" &amp; Columns(n).Offset(0, -3).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -3).Address &amp; \",\" &amp; Columns(n).Offset(0, -4).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -4).Address &amp; \",\" &amp; Columns(n).Offset(0, -5).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -5).Address &amp; \",\" &amp; Columns(n).Offset(0, -6).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -6).Address &amp; \",\" &amp; Columns(n).Offset(0, -7).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -7).Address &amp; \",\" &amp; Columns(n).Offset(0, -8).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -8).Address &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 8\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(n).Offset(0, -2).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -2).Address &amp; \",\" &amp; Columns(n).Offset(0, -3).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -3).Address &amp; \",\" &amp; Columns(n).Offset(0, -4).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -4).Address &amp; \",\" &amp; Columns(n).Offset(0, -5).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -5).Address &amp; \",\" &amp; Columns(n).Offset(0, -6).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -6).Address &amp; \",\" &amp; Columns(n).Offset(0, -7).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -7).Address &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 7\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(n).Offset(0, -2).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -2).Address &amp; \",\" &amp; Columns(n).Offset(0, -3).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -3).Address &amp; \",\" &amp; Columns(n).Offset(0, -4).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -4).Address &amp; \",\" &amp; Columns(n).Offset(0, -5).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -5).Address &amp; \",\" &amp; Columns(n).Offset(0, -6).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -6).Address &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 6\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(n).Offset(0, -2).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -2).Address &amp; \",\" &amp; Columns(n).Offset(0, -3).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -3).Address &amp; \",\" &amp; Columns(n).Offset(0, -4).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -4).Address &amp; \",\" &amp; Columns(n).Offset(0, -5).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -5).Address &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 5\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(n).Offset(0, -2).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -2).Address &amp; \",\" &amp; Columns(n).Offset(0, -3).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -3).Address &amp; \",\" &amp; Columns(n).Offset(0, -4).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -4).Address &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 4\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(n).Offset(0, -2).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -2).Address &amp; \",\" &amp; Columns(n).Offset(0, -3).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -3).Address &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 3\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(n).Offset(0, -2).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -2).Address &amp; \", \" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 2\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 1\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)),\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 1\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Total Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Total Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Total Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Total Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n</code></pre>\n"},{"tags":["iphone","connection","performance","ios-simulator"],"answer_count":4,"favorite_count":10,"up_vote_count":22,"down_vote_count":0,"view_count":4321,"score":22,"question_id":2593971,"title":"iPhone Simulator - SImulate a Slow Connection?","body":"<p>Is there a way to slow down the internet connection to the iPhone Simulator, so as to mimic how the App might react when you are in a slow spot on the cellular network?</p>\n"},{"tags":["performance","linq","select","for-loop"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":44,"score":0,"question_id":13064224,"title":"Performance for vs. select","body":"<p>I read in Bill Wagners Book \"Effective C#\" that one should favor more declarative <em>select</em> statements over traditional loops (<em>for</em>). </p>\n\n<p>For example:</p>\n\n<pre><code> int[] foo = new int[1000];\n for (int i = 0; i &lt; foo.Length; i++)\n     foo[i] = i * i;\n</code></pre>\n\n<p>is traditional imperative code, whereas this would be declarative Linq code:</p>\n\n<pre><code> int[] foo2 = (from i in Enumerable.Range(0, 1000)\n               select i * i).ToArray();\n</code></pre>\n\n<p>Being an old-fashioned programmer, I prefer the first version. </p>\n\n<p>The question is how about performance? I suppose the first version is also faster.</p>\n"},{"tags":["mysql","sql","performance","many-to-many"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":53,"score":2,"question_id":13063772,"title":"Is it better to use INNER JOIN or EXISTS to find belonging to several in m2m relation?","body":"<p>Given m2m relation: <strong>items-categories</strong> I have three tables: </p>\n\n<ul>\n<li><strong>items</strong>, </li>\n<li><strong>categories</strong> and </li>\n<li><strong>items_categories</strong> that hold references to both</li>\n</ul>\n\n<p>I want to find an item belonging to <strong>all given</strong> category sets:</p>\n\n<pre><code>Find Item \nbelonging to a category in [1,3,6] \nand belonging to a category in [7,8,4] \nand belonging to a category in [12,66,42]\nand ...\n</code></pre>\n\n<p>There are two ways I can think of to accomplish this in mySQL.</p>\n\n<p><strong>OPTION A: INNER JOIN:</strong></p>\n\n<pre><code>SELECT id from items \nINNER JOIN category c1 ON (item.id = c1.item_id)\nINNER JOIN category c2 ON (item.id = c2.item_id)\nINNER JOIN category c3 ON (item.id = c3.item_id)\n...\nWHERE\nc1.category_id IN [1,3,6] AND\nc2.category_id IN [7,8,4] AND\nc3.category_id IN [12,66,42] AND\n...;\n</code></pre>\n\n<p><strong>OPTION B: EXISTS:</strong></p>\n\n<pre><code>SELECT id from items\nWHERE\nEXISTS(SELECT category_id FROM category WHERE category.item_id = id AND category_id in [1,3,6] AND\nEXISTS(SELECT category_id FROM category WHERE category.item_id = id AND category_id in [7,8,4] AND\nEXISTS(SELECT category_id FROM category WHERE category.item_id = id AND category_id in [12,66,42] AND\n...;\n</code></pre>\n\n<p>Both options work. The question is: <strong>Which is the fastest / most optimal for large item table?</strong> Or is there an OPTION C I am missing?</p>\n"},{"tags":["javascript","jquery","performance","jquery-selectors","jquery-context"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":53,"score":2,"question_id":13057781,"title":"jQuery context slows down search","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/2421782/performance-of-jquery-selector-with-context\">Performance of jQuery selector with context</a>  </p>\n</blockquote>\n\n\n\n<p>In the <a href=\"http://api.jquery.com/jQuery/\" rel=\"nofollow\">jQuery DOCS</a> it says </p>\n\n<blockquote>\n  <p>By default, selectors perform their searches within the DOM starting\n  at the document root. However, an alternate context can be given for\n  the search by using the optional second parameter to the $() function.</p>\n</blockquote>\n\n<p>Based on that my understanding is that a selection using a <code>context</code> passed in as the second parameter should be faster then the same selection without the <code>context</code> passed in. However I ran some tests and it seems as if this isn't the case, or at least isn't always the case.</p>\n\n<p>To elaborate, I originally wanted to see if searching for multiple elements at once (<code>$(\"div1, #div2\")</code>) was faster then searching for the two separately (<code>$(\"#div1\") $(\"div2\")</code>). I then decided to test it with the <code>context</code> and without to see how much faster it was with the <code>context</code>, but was surprised when it turned out that the <code>context</code> seemed to be slowing it down.</p>\n\n<p>For example given the following basic HTML markup</p>\n\n<pre><code>&lt;div id=\"testCnt\"&gt;\n    &lt;div id=\"Div0\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div1\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div2\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div3\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div4\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div5\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div6\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div7\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div8\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div9\"&gt;&lt;/div&gt;\n&lt;/div&gt;\n</code></pre>\n\n<p>And the following JavaScript (jQuery 1.8.2, and tested using FireBug) </p>\n\n<pre><code>$(function () {    \n    var $dvCnt = $('#testCnt');\n    var dvCnt = $dvCnt[0];\n\n    console.time('Individual without cache');\n    for (var i = 0; i &lt; 10000; i++) {\n        $('#Div0').text('Test');\n        $('#Div1').text('Test');\n        $('#Div2').text('Test');\n        $('#Div3').text('Test');\n        $('#Div4').text('Test');\n        $('#Div5').text('Test');\n        $('#Div6').text('Test');\n        $('#Div7').text('Test');\n        $('#Div8').text('Test');\n        $('#Div9').text('Test');\n\n    }\n    console.timeEnd('Individual without cache');\n\n    console.time('Individual with $cache');\n    for (var i = 0; i &lt; 10000; i++) {\n        $('#Div0', $dvCnt).text('Test');\n        $('#Div1', $dvCnt).text('Test');\n        $('#Div2', $dvCnt).text('Test');\n        $('#Div3', $dvCnt).text('Test');\n        $('#Div4', $dvCnt).text('Test');\n        $('#Div5', $dvCnt).text('Test');\n        $('#Div6', $dvCnt).text('Test');\n        $('#Div7', $dvCnt).text('Test');\n        $('#Div8', $dvCnt).text('Test');\n        $('#Div9', $dvCnt).text('Test');\n\n    }\n    console.timeEnd('Individual with $cache');\n\n    console.time('Individual with DOM cache');\n    for (var i = 0; i &lt; 10000; i++) {\n        $('#Div0', dvCnt).text('Test');\n        $('#Div1', dvCnt).text('Test');\n        $('#Div2', dvCnt).text('Test');\n        $('#Div3', dvCnt).text('Test');\n        $('#Div4', dvCnt).text('Test');\n        $('#Div5', dvCnt).text('Test');\n        $('#Div6', dvCnt).text('Test');\n        $('#Div7', dvCnt).text('Test');\n        $('#Div8', dvCnt).text('Test');\n        $('#Div9', dvCnt).text('Test');\n\n    }\n    console.timeEnd('Individual with DOM cache');\n\n\n    console.time('Multiple without cache');\n    for (var i = 0; i &lt; 10000; i++) {\n        $('#Div0,#Div1 ,#Div2 ,#Div3 ,#Div4 ,#Div5 ,#Div6, #Div7, #Div8, #Div9').text('Test');\n    }\n    console.timeEnd('Multiple without cache');\n\n    console.time('Multiple with $cache');\n    for (var i = 0; i &lt; 10000; i++) {\n        $('#Div0,#Div1 ,#Div2 ,#Div3 ,#Div4 ,#Div5 ,#Div6, #Div7, #Div8, #Div9', $dvCnt).text('Test');\n    }\n    console.timeEnd('Multiple with $cache');\n\n    console.time('Multiple with DOM cache');\n    for (var i = 0; i &lt; 10000; i++) {\n        $('#Div0,#Div1 ,#Div2 ,#Div3 ,#Div4 ,#Div5 ,#Div6, #Div7, #Div8, #Div9', dvCnt).text('Test');\n    }\n    console.timeEnd('Multiple with DOM cache');\n});\n</code></pre>\n\n<p>Here's a <a href=\"http://jsbin.com/ehumic/3/edit\" rel=\"nofollow\">jsbin</a></p>\n\n<p>I'm getting something like the following results</p>\n\n<p>Individual without cache: 11490ms<br>\nIndividual with $cache: 13315ms<br>\nIndividual with DOM cache: 14487ms   </p>\n\n<p>Multiple without cache: 7557ms<br>\nMultiple with $cache: 7824ms<br>\nMultiple with DOM cache: 8589ms   </p>\n\n<p>Can someone shed some insight on whats going on? Specifically why the search is slowing down when the jQuery context is passed in? </p>\n\n<p><em>EDIT:</em></p>\n\n<p>Most of the anwsers here (as well as <a href=\"http://stackoverflow.com/q/2421782/384985\">Performance of jQuery selector with context</a>) basically say that that either the DOM in this example is too small to really gain much or that selecting by <code>ID</code> is going to be fast regardless. I understand both points, the main point of my question is why would the <code>context</code> <strong>slow</strong> down the search, the size of the <code>DOM</code> shouldn't make a difference for that, and neither should the fact that searching by ID is already very fast. </p>\n\n<p><a href=\"http://stackoverflow.com/users/1490904/pebbl\">@pebble</a> suggested that the reason that its slower is because jQuery can't use the native browser methods (<code>getElementByID</code>), this seems to make sense to me, but then why is it faster to search for multiple elements in one selection? </p>\n\n<p>Anyway I dumped the tests into a <a href=\"http://jsperf.com/jquery-selection-with-context\" rel=\"nofollow\">jsPerf</a> adding cases to search by class and was again surprised to see that the search for multiple classes with a cache this time was the fastest.</p>\n"},{"tags":["c++","performance","visual-c++","binary","byte"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":70,"score":1,"question_id":13063306,"title":"C++: Fastest way to merge and split a uint into 4 bytes","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/741212/fastest-method-to-split-a-32-bit-number-into-bytes-in-c\">Fastest Method to Split a 32 Bit number into Bytes in C++</a>  </p>\n</blockquote>\n\n\n\n<p>Is there a way to merge/split a uint into 4 bytes quicker than what I'm currently doing? Maybe some inline assembler that has a native opcode that can do it in a single instruction?</p>\n\n<pre><code>// merge into x0\nunsigned int x0 = (data[i] &lt;&lt; 24) | (data[i+1] &lt;&lt; 16) | (data[i+2] &lt;&lt; 8) | data[i+3]; \n\n\n// split x0\noutputBuffer[i] = (x0 &gt;&gt; 24);\noutputBuffer[i+1]  = (x0 &gt;&gt; 16) &amp; 0xFF;\noutputBuffer[i+2]  = (x0 &gt;&gt; 8) &amp; 0xFF;\noutputBuffer[i+3]  = (x0) &amp; 0xFF;\n</code></pre>\n"},{"tags":["c#",".net","performance","sqlite","indexing"],"answer_count":2,"favorite_count":0,"up_vote_count":6,"down_vote_count":0,"view_count":96,"score":6,"question_id":13056193,"title":"Escape wildcards (%, _) in SQLite LIKE without sacrificing index use?","body":"<p>I have a couple of issues with SQLite query. Actually I start thinking that SQLite is not designed for tables with more then 10 rows, really, SQLite is a nightmare.</p>\n\n<p>The following query</p>\n\n<pre><code>SELECT * FROM [Table] WHERE [Name] LIKE 'Text%'\n</code></pre>\n\n<p>It works fine. <code>EXPLAIN</code> shows that the index is used and result is returned after about <code>70ms</code>.</p>\n\n<p>Now I need to run this query from .NET SQLite driver, so I'm changing query</p>\n\n<pre><code>SELECT * FROM [Table] WHERE [Name] LIKE @Pattern || '%'\n</code></pre>\n\n<p>Index is not used. When I run the following query in any SQLite tool the index is not used as well</p>\n\n<pre><code>SELECT * FROM [Table] WHERE [Name] LIKE 'Text' || '%'\n</code></pre>\n\n<p>So I guess SQLite doesn't have any kind of preprocessing logic implemented.</p>\n\n<p>OK. Let's try to solve it, I'm still binding variables and doing the following</p>\n\n<pre><code>SELECT * FROM [Table] WHERE [Name] LIKE @Pattern\n</code></pre>\n\n<p>But now I append <code>%</code> wildcard symbol to the end of my pattern string, like this</p>\n\n<pre><code>command.Parameters.Add(new SQLiteParameter(\"@Pattern\", pattern + '%'));\n</code></pre>\n\n<p>It works very slow. I can't say why, because when I run this query from SQLite tool it works fine, however when I bind this variable from .NET code it works slow.</p>\n\n<p>OK. I'm still trying to solve this. I'm getting rid of the pattern parameter binding and building this condition dynamically.</p>\n\n<pre><code>pattern = pattern.Replace(\"'\", \"''\");\npattern = pattern.Replace(\"%\", \"\\\\%\");\nwhere = string.Format(\"LIKE '{0}%' ESCAPE '\\\\'\", pattern);\n</code></pre>\n\n<p>Index is not used again. It's not used because of <code>ESCAPE</code>. I see that when I run </p>\n\n<pre><code>EXPLAIN QUERY PLAN SELECT * FROM [Table] WHERE [Name] LIKE 'Text%' ESCAPE '\\'\n</code></pre>\n\n<p>As soon as I remove <code>ESCAPE</code> it starts using index again and the query finishes in 60-70ms.</p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>Here are the results.</p>\n\n<pre><code>EXPLAIN QUERY PLAN\nSELECT * FROM [RegistryValues]\nWHERE\n     [ValueName] LIKE 'windir%' ESCAPE '\\' \n</code></pre>\n\n<p><code>SCAN TABLE RegistryValues (~3441573 rows)</code></p>\n\n<p>and the one without <code>ESCAPE</code></p>\n\n<pre><code>EXPLAIN QUERY PLAN\nSELECT * FROM [RegistryValues]\nWHERE\n     [ValueName] LIKE 'windir%'\n</code></pre>\n\n<p><code>SEARCH TABLE RegistryValues USING INDEX IdxRegistryValuesValueNameKeyIdKeyHiveFileId (ValueName&gt;? AND ValueName&lt;?) (~31250 rows)</code></p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>Just found this</p>\n\n<p><a href=\"http://www.sqlite.org/optoverview.html\" rel=\"nofollow\">http://www.sqlite.org/optoverview.html</a></p>\n\n<p><strong>4.0 The LIKE optimization</strong></p>\n\n<p><code>The ESCAPE clause cannot appear on the LIKE operator</code></p>\n\n<p>So what should I do then?</p>\n\n<p>Do I understand it right? I can't search string containing wildcards using <code>LIKE</code> operator in <code>SQLite</code>. By saying wildcards I mean <code>_</code> <code>%</code> <code>^</code> <code>!</code></p>\n\n<p>It's impossible simply because I can't escape them.\nActually I can, but I can't use indexes in this case, so the query will not be efficient.</p>\n\n<p>Am I right?</p>\n"},{"tags":["php","performance","oracle"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":54,"score":1,"question_id":12967784,"title":"Define value edit by another php file","body":"<p>I have 2 questions</p>\n\n<p>1.) how to write <code>update_defile($array_value){...}</code> function?</p>\n\n<p>define_file.php</p>\n\n<pre><code>&lt;?php\n  define(\"FIST_NAME\", \"something1\");\n  define(\"LAST_NAME\", \"something2\");\n  define(\"ADDRESS\", \"something3\");\n?&gt;\n</code></pre>\n\n<p>\"something\" is not a constant value that can be change every method <code>Call(update_defile($array_value)</code></p>\n\n<p>value set</p>\n\n<pre><code>$array_value = (\"FIST_NAMe\" =&gt; \"duleep\", \"LAST_NAME\" =&gt; \"dissnayaka\", \"AGE\" =&gt; \"28\" );\n</code></pre>\n\n<p>after call method(update_defile($array_value){.....}) \"define_file.php\"\nfile want to be look like bellow</p>\n\n<pre><code>&lt;?php\n  define(\"FIST_NAME\", \"duleep\");\n  define(\"LAST_NAME\", \"dissnayaka\");\n  define(\"ADDRESS\", \"something3\");\n  define(\"AGE\", \"28\");\n?&gt;\n</code></pre>\n\n<p>2). </p>\n\n<p>My datbase is Oracle. I already saved configuration value in the data base but frequently use these configuration value for my application. So i get value form database and save in the define_file.php as increase performance(down rate database call) but I'm not sure i can increase performance keep configuration value in the PHP file please explain. what is the best way increase performance my application and other alternative solutions welcome.</p>\n"},{"tags":["html","css","performance","pagespeed"],"answer_count":3,"favorite_count":2,"up_vote_count":6,"down_vote_count":0,"view_count":3157,"score":6,"question_id":1800137,"title":"Google Page Speed - what do these messages mean?","body":"<p>I ran the Google Page Speed Firefox extension on a few pages, and under \"efficient CSS selectors\" it listed various things that are inefficient in my CSS.</p>\n\n<p>But some of the messages seem a bit cryptic - what do these (in bold) mean:</p>\n\n<blockquote>\n  <p>div#menu h3.soon small<br>\n  <strong>Tag key with 2 descendant selectors and ID overly qualified with tag and Class overly qualified with tag</strong></p>\n  \n  <p>table.data tr:nth-child(2n) td<br>\n  <strong>Tag key with 2 descendant selectors and Class overly qualified with tag</strong></p>\n  \n  <p>table.data tr.disabled td<br>\n  <strong>Tag key with 2 descendant selectors and Class overly qualified with tag and Class overly qualified with tag</strong></p>\n</blockquote>\n\n<p>I'm assuming they think descendant selectors are bad but there are lots of \"overly qualified\" as well. I probably won't go to too much effort fixing all these up (there are many) but it would be nice to know what Google actually means here!</p>\n"},{"tags":["performance","memory-management","language-agnostic","distributed-computing","bandwidth"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":45,"score":2,"question_id":13062114,"title":"What is bandwidth demand?","body":"<p>What does bandwidth demand mean?\nI've seen it being used in this paragraph: </p>\n\n<blockquote>\n  <p>\"Memory must be distributed among the processors rather than\n  centralized; otherwise the memory system would not be able to support\n  the <strong>bandwidth demands</strong> of a large number of processors without\n  incurring long access latency\"</p>\n</blockquote>\n"},{"tags":["performance","selenium","web","web-performance-test"],"answer_count":0,"favorite_count":2,"up_vote_count":0,"down_vote_count":0,"view_count":31,"score":0,"question_id":13058907,"title":"web performance testing that renders javascript","body":"<p>I would like to load test pages AND render javascript. It seems there are 3 categories of applications which might solve this problem, and all three miss the mark.</p>\n\n<p>1) Jmeter, apachebench, tsung, Grinder, Iago\nWhy it wont work: It doesn't render javascript. These are handy tools, but they won't work for this purpose.</p>\n\n<p>2) Watir, Selenium\nThese tools are excellent, they use real browsers and render javascript / ajax, but alas, they are designed mostly for functional testing, and not performance testing. You could create your own app to use these things for a load test, but it would be a massive pain in the ass to collect all of the performance metrics and aggregate them. </p>\n\n<p>If only there was a combination of the first two types of web performance tests, that would be great.</p>\n\n<p>The third option, solves this problem, but unfortunately you have to pay for it.\n3) Web load testing services\nServices like Keynote Load Pro, BrowserMob and others are great, and they solve the problem of using real browsers and rendering JavaScript. The only problem is, I don't want to pay $300 ever time I run a damn load test (this is an exaggeration, but not really, depending on how many virtual users you use).</p>\n\n<p>So that option won't work, unless I want to hemorrhage money.</p>\n\n<p>Isn't there a test harness out there that solves this problem? It seems like a big gaping hole where there is a need for an open source tool that no one has solved yet. The commercial companies dominate this space (and those who want to employ a full time developer to write a selenium performance test framework).</p>\n"},{"tags":["java","performance","memory-management"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":47,"score":2,"question_id":13061234,"title":"Storing Data in a Variable vs Inline Arithmetic","body":"<p>Is there any difference between these two code snippets in terms of memory usage and performance/overhead?</p>\n\n<pre><code>final float xPos = (CAMERA_WIDTH / 2) - (mSprite.getWidth() / 2);\nfinal float yPos = (CAMERA_HEIGHT / 2) - (mSprite.getHeight() / 2);\nmSprite.setPosition(xPos, yPos);\n</code></pre>\n\n<p>and the other case:</p>\n\n<pre><code>mSprite.setPosition(((CAMERA_WIDTH / 2) - (mSprite.getWidth() / 2)), ((CAMERA_HEIGHT / 2) - (mSprite.getHeight() / 2)));\n</code></pre>\n\n<p>The only difference I can see is that the first snippet is storing the variable in what I assume to be a different area of memory than the second snippet, but I'm not very familiar with Java memory allocation (I'm more of a C/C++ person). </p>\n\n<p>My question is: is there any benefit to one way or the other? Does using the <code>final</code> keyword in the first example affect it at all?</p>\n\n<p>Thank you!</p>\n"},{"tags":["performance","parallel-processing","cuda"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":66,"score":3,"question_id":13060620,"title":"A couple of CUDA-performance questions","body":"<p>This is the first time i ask question here so thanks very much in advance and please forgive my ignorance. And also I've just started to CUDA programming.</p>\n\n<p>Basically, i have a bunch of points, and i want to calculate all the pair-wise distances. Currently my kernel function just holds on one point, and iteratively read in all other points (from global memory), and conduct the calculation. Here's some of my confusions:</p>\n\n<ul>\n<li><p>I'm using a Tesla M2050 with 448 cores. But my current parallel version (kernel&lt;&lt;&lt;128,16,16>>>) achieves a much higher parallelism (about 600x faster than kernel&lt;&lt;&lt;1,1,1>>>). Is it possibly due to the multithreading thing or pipeline issue, or they actually indicate the same thing?</p></li>\n<li><p>I want to further improve the performance. So i figure to use shared memory to hold some input points for each multiprocessing block. But the new code is just as fast. What's the possible cause? Could it be related to the fact that i set too many threads?</p></li>\n<li><p>Or, is it because i have a if-statement in the code? The thing is, i only consider and count the short distances, so i have a statement like (if dist &lt; 200). How much should i worry about this one?</p></li>\n</ul>\n\n<p>A million thanks!\nBin</p>\n"},{"tags":["python","performance","matplotlib"],"answer_count":1,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":89,"score":3,"question_id":13046127,"title":"matplotlib very slow. Is it normal?","body":"<p>I am creating a couple of pdf plots with matplotlib which is composed of 400 subplots. Each one has only 5 data points. It takes 420 s on a good computer to save 5 pdf picture. Is there any way to optimize the code or it is just normal  for matplotlib?</p>\n\n<p>Portion of code for plotting:</p>\n\n<pre><code>plot_cnt = 1\nfor k in np.arange(K_min, K_max + 1):\n    for l in np.arange(L_min, L_max + 1):\n        ax = plt.subplot(grid[0], grid[1], plot_cnt)\n        plot_cnt += 1\n        plt.setp(ax, 'frame_on', False)\n        ax.set_ylim([-0.1, 1.1])\n        ax.set_xlabel('K={},L={}'.format(k, l), size=3)\n        ax.set_xlim([-0.1, 4.1])\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.grid('off')\n        ax.plot(np.arange(5), (data['S1']['Azimuth'][:, k - 1, l + offset_l] + \\\n                data['S1']['Delta Speed'][:, k - 1, l + offset_l] + \\\n                data['S1']['Speed'][:, k - 1, l + offset_l]) / 3,\n                'r-o', ms=1, mew=0, mfc='r')\n        ax.plot(np.arange(5), data['S2'][case][:, k - 1, l + offset_l],\n                'b-o', ms=1, mew=0, mfc='b')\nplt.savefig(os.path.join(os.getcwd(), 'plot-average.pdf'))\nplt.clf()\nprint 'Final plot created.'\n</code></pre>\n\n<p>Final Picture:\n<img src=\"http://i.stack.imgur.com/xgzdN.png\" alt=\"enter image description here\"></p>\n"},{"tags":[".net","performance","testing"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":21,"score":0,"question_id":13024739,"title":"Efficiently storing performance across multiple tests","body":"<p>I have many different games (test simulations) that will hopefully be run by several thousand computers monthly; they all will have varying hardware. I want to be able to store the data from all these computers for processing later such as figuring out what processors, GPUs and other factors affect performance the most. </p>\n\n<p>I've considered sending down the hardware everytime this test is run, but that seems wasteful</p>\n\n<p>The entire configuration is re-sent and re-added upon test run; everytime. And it still leaves me a lot on how to parse it all.</p>\n\n<p>However, all these varying pieces of information still don't get me where I want to be. I don't neccesarily need the data - but a way to calculate how performant a particular hardware configuration is. I need to be able to guess how well a given computer configuration coming in will do based on previous results - and do so in a way that won't harm bandwidth.</p>\n\n<p>Is there an algorithm I can use to track performance, specifically that will give accurate similar results?</p>\n"},{"tags":["performance","decompression","in-memory","hdd"],"answer_count":3,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":41,"score":0,"question_id":13052043,"title":"which is faster: in-memory decompression or accessing uncompressed data in HDD","body":"<p>I have a dataset larger than main memory. After compression, it fits into memory. However, in-memory decompression is kind of compute-intensive. \nCompared to accessing uncompressed data in hard drive, does in-memory decompression have any advantage in term of time-to-completion? assuming data from HDD will loaded into memory in its entirety (i.e. no random access to HDD during processing). Anyone has done any benchmark before. Thanks. </p>\n"},{"tags":["python","performance","range","max","min"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":63,"score":4,"question_id":13060458,"title":"Efficiently find the range of an array in python?","body":"<p>Is there an accepted efficient way to find the range (ie. max value - min value) of a list of numbers in python? I have tried using a loop and I know I can use the <code>min</code> and <code>max</code> functions with subtraction. I am just wondering if there is some kind of built-in that is faster.</p>\n"},{"tags":["javascript","performance","jsperf"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":45,"score":-1,"question_id":13059144,"title":"jsPerf accuracy","body":"<p>Having spent an entire evening being baffled by some reasonably straightforward benchmarking tests I've whittled things down to a curious test case - the same function twice. It seems there can be some pretty big performance differences (up to 44%), between THE SAME FUNCTION TWICE.</p>\n\n<p>TEST: <a href=\"http://jsperf.com/it-s-the-same\" rel=\"nofollow\">http://jsperf.com/it-s-the-same</a></p>\n\n<pre><code>var ns = {i:0};\n\n(function(){\n\n    function add(n1, n2){\n            ns.i = n1 + n2;\n            return ns.i;\n    };\n\n    ns.test1 = function(){\n            add(2, 12);\n    }\n\n    ns.test2 = function(){\n            add(2, 12);\n    }\n\n})();\n</code></pre>\n\n<p>Any arguments in the defence of jsPerf? Which I might add I have spent many hours glued to. Could 44% be caused by the ns lookup? Is there a better way to structure a test like this? Here I am trying to 'do something' with the output - as per this discussion <a href=\"http://stackoverflow.com/questions/12662497/performance-penalty-for-undefined-arguments\">Performance penalty for undefined arguments</a></p>\n"},{"tags":["database","performance","postgresql","monitoring"],"answer_count":4,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":214,"score":3,"question_id":1452807,"title":"What are the good resources for database monitoring?","body":"<p>I was wondering if anyone could point me in the direction of some good resources (web sites, technical articles/journals, books, etc.) related to database monitoring/performance?</p>\n\n<p>I am looking to write a paper that explores what kinds of statistics are useful in database monitoring, and to whom they are useful for. </p>\n\n<p>So, I figured I'd ask the experts ...</p>\n\n<p>(In my research, I will be focusing mostly on a PostgreSQL database, but I am looking to gather resources on many different databases).</p>\n\n<p>Any suggestions would be appreciated, thanks!</p>\n"},{"tags":["java","python","performance","project-euler"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":116,"score":2,"question_id":13054325,"title":"Why is there such a huge performance different between the same Python/Java code?","body":"<p>I'm currently trying to work through the ProjectEuler questions with Python (something I've only picked up today).  The question I'm working is question 5, where</p>\n\n<pre><code>2520 is the smallest number that can be divided by each of the numbers from 1 to 10 without any remainder.\n\nWhat is the smallest positive number that is evenly divisible by all of the numbers from 1 to 20?\n</code></pre>\n\n<p>I have worked through the problems using Java before, so using the same method as I did before, I just created a loop that iterated, however it seems that my code never ends.</p>\n\n<p>Python:</p>\n\n<pre><code>i = 1\nwhile 1:\n    if i%2 == 0 and i%3==0 and i%4==0 and i%5==0 and i%6==0 and i%7==0 and i%8==0 and i%9==0 and i%10==0 and i%11==0 and i%12==0 and i%13==0 and i%14==0 and i%15==0 and i%16==0 and i%17==0 and i%18==0 and i%19==0:\n        print i\n        break\n    i+=1\n</code></pre>\n\n<p>Java:</p>\n\n<pre><code>public class p5\n{\n    public static void main(String[] args)\n    {\n        for (int i=1;;i++)\n        {\n            if (i%1==0&amp;&amp;i%2==0&amp;&amp;i%3==0&amp;&amp;i%4==0&amp;&amp;i%5==0&amp;&amp;i%6==0&amp;&amp;i%7==0&amp;&amp;i%8==0&amp;&amp;i%9==0&amp;&amp;i%10==0&amp;&amp;i%11==0&amp;&amp;i%12==0&amp;&amp;i%13==0&amp;&amp;i%14==0&amp;&amp;i%15==0&amp;&amp;i%16==0&amp;&amp;i%17==0&amp;&amp;i%18==0&amp;&amp;i%19==0&amp;&amp;i%20==0)\n            {\n                System.out.println(i);\n                break;\n            }\n        }\n    }\n}\n</code></pre>\n\n<p>Java executed that in under 3 seconds on my computer, whereas the Python code never seemed to end.  Any tips?</p>\n\n<p><strong>Edit:</strong></p>\n\n<p>Apparently I typed something wrong, which caused it to never end.  However, even with the entire thing written correctly (with the same output as my Java), it still took <strong>1 minute and 20 seconds</strong>, whereas for Java it took around 1 - 2 seconds.  Am I doing anything wrong?  Or is Python's performance that bad (which shouldn't be afaik)</p>\n"},{"tags":["performance","excel-vba"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":39,"score":-1,"question_id":13057510,"title":"Performance of VBA-2003 vs. VBA-2007","body":"<p>I have upgraded Excel 2003 to Excel 2007 on my office desktop, but I still keeping Excel 2003 on my laptop. What I found out is that it takes significantly more time to run the same VBA application with the same input parameters on VBA-2007 than on VBA-2003.</p>\n\n<p>I have some simulation tool, that should (1) read input data, (2) run several thousand iterations in memory and then (3) printout the results. The second step is very CPU intensive, and takes the most of the time. I measured a processing time for this step, and it occurs that it takes ~3 times longer to run the same scenario on my desktop (with VBA-2007) than on my laptop (with VBA-2003), e.g. 125 sec. vs. 37 sec.</p>\n\n<p>Both computers use Windows XP Professional, Version 2002, Service Pack 3.</p>\n"},{"tags":["java","performance","arraylist","primes"],"answer_count":5,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":93,"score":2,"question_id":13056199,"title":"Most efficient way to print an ArrayList","body":"<p>I'm trying to print an ArrayList with upwards of a couple thousand entries (it has to find all the prime numbers between 1 and 1000000). At the end of the program, I call this method:</p>\n\n<pre><code>println(myArrayList);\n</code></pre>\n\n<p>While this works with only a couple hundred entries, it takes more time to print the array list than to find the primes once there gets to be more entries.</p>\n\n<p>Would it be more efficient to iterate over it? Or to use a different class?</p>\n"},{"tags":["performance","matlab","math","matrix","vectorization"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":3,"view_count":85,"score":-2,"question_id":12977219,"title":"Improve Performance of Matlab Function","body":"<p>I would like to improve one of my simple matlab functions.\nIs there any arithmetic way to implement this function? I think that would perform much better.</p>\n\n<pre><code>function img_output = cutchannels(img_input, min, max)\n[r c l] = size(img_input);\nimg_output = double(img_input);\n\nfor i = 1:r\n    for j = 1:c\n        for k = 1:l\n            if(img_output(i:j:k)&gt; max)\n                img_output(i:j:k) = max;\n            elseif(img_output(i:j:k) &lt; min)\n                img_output(i:j:k) = min;\n            end\n        end\n    end\nend\nend\n</code></pre>\n"},{"tags":["performance","matlab","interpolation","vectorization"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":46,"score":2,"question_id":13054340,"title":"Improving performance of interpolation (Barycentric formula)","body":"<p>I have been given an assignment in which I am supposed to write an algorithm which performs polynomial interpolation by the barycentric formula.  The formulas states that:</p>\n\n<p>p(x) = (SIGMA_(j=0 to n) w(j)*f(j)/(x - x(j)))/(SIGMA_(j=0 to n) w(j)/(x - x(j)))</p>\n\n<p>I have written an algorithm which works just fine, and I get the polynomial output I desire.  However, this requires the use of some quite long loops, and for a large grid number, lots of nastly loop operations will have to be done.  Thus, I would appreciate it greatly if anyone has any hints as to how I may improve this, so that I will avoid all these loops.</p>\n\n<p>In the algorithm, <code>x</code> and <code>f</code> stand for the given points we are supposed to interpolate.  <code>w</code> stands for the barycentric weights, which have been calculated before running the algorithm.  And <code>grid</code> is the linspace over which the interpolation should take place:</p>\n\n<pre><code>function p = barycentric_formula(x,f,w,grid)\n\n%Assert x-vectors and f-vectors have same length.\nif length(x) ~= length(f)\n    sprintf('Not equal amounts of x- and y-values. Function is terminated.')\n    return;\nend\n\nn = length(x);\nm = length(grid);\np = zeros(1,m);\n\n% Loops for finding polynomial values at grid points.  All values are\n% calculated by the barycentric formula.\nfor i = 1:m\n    var = 0;\n    sum1 = 0;\n    sum2 = 0;\n    for j = 1:n\n        if grid(i) == x(j)\n            p(i) = f(j);\n            var = 1;\n        else\n            sum1 = sum1 + (w(j)*f(j))/(grid(i) - x(j));\n            sum2 = sum2 + (w(j)/(grid(i) - x(j)));\n        end\n    end\n    if var == 0\n        p(i) = sum1/sum2;\n    end    \nend\n</code></pre>\n"},{"tags":["php","performance","apache","session"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":84,"score":2,"question_id":12417769,"title":"Simultaneous connections to single browser through PHP and Apache2","body":"<p>I have at the moment an AJAX request going to sendMail.php, it closes the connection immediately(using the header <em>Connection: Close</em>) and continues processing for approx 30 seconds. </p>\n\n<p>But the problem I'm experincing now is that when that same client trys to load any PHP page from that server, it has to wait until sendMail.php has finished processing. </p>\n\n<p><strong>Is there any way around this?</strong></p>\n\n<p>I was reading on some other SO questions that it may be session related, but I'm not using any sessions, I even tried calling <strong>session_write_close()</strong> at the start of the sendMail.php script.</p>\n\n<p>Example code (this is hacky and over done, but it works):</p>\n\n<pre><code>   //File: SendMail.php\n   //error_reporting(E_ALL);\n    error_reporting(0);\n    session_write_close();\n    set_time_limit(0); \n    ignore_user_abort(1);\n    ignore_user_abort(true);\n    ini_set('ignore_user_abort','1');\n    apache_setenv('no-gzip', 1);\n    apache_setenv('KeepAlive',0);\n    ini_set('zlib.output_compression', 0);\n    ini_set('output_buffering', 0);\n\n    $size = ob_get_length();\n\n    // send headers to tell the browser to close the connection\n    header(\"Content-Length: $size\");\n    header('Connection: Close');\n        // flush all output\n    ignore_user_abort(true);\n    ob_end_flush();\n    ob_flush();\n    flush();\n    sleep(30);//The real code has more stuff, but just for example lets just say it sleeps for 30 seconds\n</code></pre>\n\n<p>The rest of the referenced material is a normal navigation via GET. </p>\n"},{"tags":["jquery","performance","jquery-selectors"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":28,"score":0,"question_id":13055707,"title":"Is there any performance advantage to using the E#C CSS selector over #C?","body":"<p>I'm currently reading up on jQuery to fill some gaps in knowledge.</p>\n\n<p>Looking at the various CSS selectors, I see:-</p>\n\n<ul>\n<li><p><code>#C</code> (any element with an ID of C) </p>\n\n<pre><code>$('#profile')\n</code></pre></li>\n<li><p><code>E#C</code> (any element of type E with an ID of #C).</p>\n\n<pre><code>$('div#profile')\n</code></pre></li>\n</ul>\n\n<p>I know specificity is a big deal in the application of CSS rules, but given that it is bad practice to have duplicate IDs on a page, I'm wondering why the second form exists and how it is treated in jQuery.</p>\n\n<p>Does it confer a performance advantage when interrogating the DOM? ( i.e. immediately limiting the scope of the selection ).  This question applies mostly to jQuery, but I'd also be interested to know if it had any bearing on rendering engines, etc.</p>\n"},{"tags":["performance","prolog","dcg"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":74,"score":1,"question_id":10683843,"title":"Prolog - Do plain rules have better performance than lists?","body":"<p>I have a set of DCG rules (in this case german personal pronouns):</p>\n\n<pre><code>% personal pronoun (person, case, number, genus)\nppers(1,0,sg,_) --&gt; [ich].\nppers(1,1,sg,_) --&gt; [meiner].\nppers(1,2,sg,_) --&gt; [mir].\nppers(1,3,sg,_) --&gt; [mich].\nppers(2,0,sg,_) --&gt; [du].\nppers(2,1,sg,_) --&gt; [deiner].\nppers(2,2,sg,_) --&gt; [dir].\nppers(2,3,sg,_) --&gt; [dich].\n...\n</code></pre>\n\n<p>Because they are semantically connected, it would make sense to me to keep this information by moving them into a list (grouped by person for example) instead of unrelated rules. This also makes things a bit neater:</p>\n\n<pre><code>ppers(1,sg,_,[ich, meiner, mir, mich]).\nppers(2,sg,_,[du,deiner,dir,dich]).\n...\n</code></pre>\n\n<p>I would then select the item I want with <code>nth0()</code> where the case I need is the index within the list.</p>\n\n<p>However, I noticed when tracing through the program, that when checking a german sentence for correct grammar and trying to find if a part is a personal pronoun, Prolog will not step through every instanc when I use the upper version (plain rules), but will crawl through every list when I use the list version below.</p>\n\n<p>Does this mean that performance will be worse if I use lists and nth0 versus plain rules? Or does the Prolog tracer just not show the crawling for plain rules as it does for lists?</p>\n\n<p>(I hope I could make my question obvious enough, if not I will expand.)</p>\n"},{"tags":["android","performance","graphics","android-canvas","scenegraph"],"answer_count":2,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":98,"score":5,"question_id":13012514,"title":"Canvas manipulation vs element manipulation","body":"<p>I am developing a small library as a basis for some applications. As I am about to create a scenegraph (2D) I am wondering which of the following approaches looks more promising under the view of performance, maintainability, easy to use etc.</p>\n\n<ol>\n<li>I could give each drawable element a matrix where I perform translation, rotation and more.</li>\n<li>I could do everything on the canvas instead of the elements.</li>\n</ol>\n\n<p>The first solution has a disadvantages: For primitive elements like circles, where I can't pass a matrix in the draw call, I must access the translated values from the matrix like this:</p>\n\n<pre><code>private float get(int index) {\n    final float[] values = new float[9];\n    getValues(values);\n    return values[index];\n}\n\npublic float getX() {\n    return get(Matrix.MTRANS_X);\n}\n\npublic float getY() {\n    return get(Matrix.MTRANS_Y);\n}\n</code></pre>\n\n<p>So on every draw call I create a float array for each getter call (one for getX(), one for getY()). Assuming that I have plenty of elements on the screen, this could lead to a memory and performance impact.</p>\n\n<p>The second approach has the disadvantage of \"negative\" thinking. If I want an element be drawn at point 100/100 I must translate the canvas to -100/-100 as I would draw on 0/0. If I restore the canvas after that, the result would be the element be drawn on the wanted 100/100. I am not sure if this negative thinking would result in a heavy impact on code maintainability and decreased understanding (never even started to think about introducing bugs by simply forgetting to negate something...).</p>\n\n<p>Does someone have a tip which way should be preferred?</p>\n"},{"tags":["performance","search","full-text-search"],"answer_count":5,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":179,"score":5,"question_id":12882000,"title":"Hybrid of fulltext and property-based search engine","body":"<h3>Background:</h3>\n\n<p>SQL database representing different types of events (concerts, football matches, charity collections etc.), where each contain event-related data (concert - artist name, match - host/visitor team). All of these events inherit from one, general table <code>event</code>, which contains data related to all of them (name, description, location, start/end date).\nInheritance is implemented using table-per-subclass model known from <a href=\"http://docs.jboss.org/hibernate/orm/3.3/reference/en/html/inheritance.html#inheritance-tablepersubclass\" rel=\"nofollow\">Hibernate</a> or <a href=\"http://docs.doctrine-project.org/en/2.0.x/reference/inheritance-mapping.html#class-table-inheritance\" rel=\"nofollow\">Doctrine</a>. The database also stores tables <code>artists</code> (<code>id</code>, <code>name</code>, <code>birth_date</code>) and <code>football_teams</code> (<code>id</code>, <code>name</code>, <code>country</code>, <code>coach_name</code>) used in <code>event_concerts</code> and <code>event_football_matches</code> tables (through FKs).</p>\n\n<h3>Problem:</h3>\n\n<p>Create a search engine that given some criteria (<code>{name: \"manchester\", startDate: \"01.01.2012 - 01.02.2012\"}</code> or <code>{location: \"london\", description: \"artists +metallica -bieber\"}</code>) will return all events that meet the criteria, as well as results from <code>artists</code>/<code>football_teams</code> tables.</p>\n\n<p>Some properties of those events contain large pieces of text, that should be searched through in fulltext-search manner.</p>\n\n<h3>Example:</h3>\n\n<p>Given following search criteria:</p>\n\n<pre><code>{ location: \"london\", startDate: \"05.11.2012 - 07.11.2012\" }\n</code></pre>\n\n<p>Search engine should return:</p>\n\n<ol>\n<li>(football event) Arsenal vs Manchester United match, Emirates Stadium, London, 06.11.2012</li>\n<li>(concert event) Metallica concert, Some-Fancy-Location, 05.11.2012</li>\n<li>(football team/not an event) Arsenal, founded: 1886, league: Premier League</li>\n<li>(football team/not an event) Chelsea, founded: 1905, league: Premier League</li>\n<li>(festival event) Halloween in London, 07.11.2012</li>\n<li>(dance event) Sleeping Beauty at Sadler's Wells, £45, 07.11.2012</li>\n<li>(musician, not an event) Neil Christian, 1943 - 2012, Rock'n'Roll vocalist</li>\n</ol>\n\n<p>As you can see, <em>startDate</em> (event-related property) is considered only in case of events.</p>\n\n<hr>\n\n<p>Search engine has to scan lots of tables, that's why I believe I should use dedicated software (Sphinx, Lucene, ...?) and create separate index just for the searching.</p>\n\n<hr>\n\n<p>Could anyone suggest some solution for building such an index? What software could I use as a base for that search engine?</p>\n\n<hr>\n\n<h3>EDIT:</h3>\n\n<p>Just to clarify: none of the properties is required. Some of them contain dates which will be searched using exact-match, some of them contain short text (like a location) that also will be searched using exact-match. But some of them contain long pieces of text, and that needs to be searched in full-text manner.</p>\n"},{"tags":["php","ajax","performance","forms","table"],"answer_count":3,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":71,"score":2,"question_id":12974317,"title":"1500 row table with each row containing a form - Ajax or something else?","body":"<p>I have a very large HTML table containing 1500 rows (markup produced by PHP).  Each row has a 5-element checkbox and one  textarea.  </p>\n\n<p>Here's an example row:</p>\n\n<pre><code>&lt;tr id=\"abc123\"&gt;\n    &lt;td&gt;abc123&lt;/td&gt;\n    &lt;td valign=\"top\"&gt;&lt;input name=\"v1\" value=1 type=\"checkbox\"&gt;&lt;/td&gt;\n    &lt;td valign=\"top\"&gt;&lt;input name=\"v2\" value=2 type=\"checkbox\"&gt;&lt;/td&gt;\n    &lt;td valign=\"top\"&gt;&lt;input name=\"v3\" value=3 type=\"checkbox\"&gt;&lt;/td&gt;\n    &lt;td valign=\"top\"&gt;&lt;input name=\"v4\" value=4 type=\"checkbox\"&gt;&lt;/td&gt;\n    &lt;td valign=\"top\"&gt;&lt;input name=\"v5\" value=5 type=\"checkbox\"&gt;&lt;/td&gt;\n    &lt;td valign=\"top\"&gt;&lt;textarea name=\"notes\"&gt;&lt;/textarea&gt;&lt;/td&gt;\n    &lt;td valign=\"top\"&gt;&lt;input type=\"submit\" name=\"submit\"&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n</code></pre>\n\n<p>There are more columns but this is the important part. </p>\n\n<p>What would be the best way of setting this up so that the submit button is checked/redrawn every 60 seconds?   If the row/form has been submitted, the submit button would need to change to a link.   </p>\n\n<p>Would having that many writes occurring all at once lead to horrible performance?  Our server can handle it, but I'm more concerned about client capabilities (who are using mid-grade or worse desktop machines).</p>\n\n<p>Other than breaking up the records onto separate pages (client not interested in that), is there a better way of handling this many forms on a single page? </p>\n\n<p><strong>Update</strong>\nI'm thinking it might make more sense to let users click on the row they want to update, which would then convert only that row to a form.  If a row loses focus, I'll display a confim box so they don't lose or submit incomplete data.</p>\n"},{"tags":["performance","automapper","emitmapper"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":23,"score":0,"question_id":13053590,"title":"Emit mapper vs valueinjecter or automapper performance","body":"<p>I have spent some time comparing this three mappers and it is interesting why so big performance diffrenece between emitmapper and any of valueinjecter or automapper(last two comparable by performance). From benchmark test in emitmapper solution(1000000 iterations):</p>\n\n<pre><code>    Auto Mapper (simple): 38483 milliseconds\n    Emit Mapper (simple): 118 milliseconds\n    Handwritten Mapper (simple): 37 milliseconds\n    Auto Mapper (Nested): 53800 milliseconds\n    Emit Mapper (Nested): 130 milliseconds\n    Handwritten Mapper (Nested): 128 milliseconds\n    Auto Mapper (Custom): 49587 milliseconds\n    Emit Mapper (Custom): 231 milliseconds\n</code></pre>\n\n<p>Also some benchmarks from valueinjecter runned with added emitmapper(for 10000 iterations):</p>\n\n<pre><code>    Convention: 00:00:00.5016074\n    Automapper: 00:00:00.1992945 \n    Smart convention: 00:00:00.2132185\n    Emit mapper(each time new mapper): 00:00:00.1168676\n    Emit mapper(one mapper): 00:00:00.0012337\n</code></pre>\n\n<p>There in first emit mapper test - it was created each time, in second - one mapper for all conversions. </p>\n\n<p>Taking this into account, have result as valueinjecter(also as automapper) slower than in 100 times than emit mapper. What is a reason of so huge performance difference? As for me object to object mapper cannot took so much time comparing to handwritten mapper as it be a bottleneck of project(if we need to map collection of objects for example).</p>\n\n<p>At this moment I'm thinking about using emit mapper, but only one reason why I'm not ready to decide : emit mapper not supported at all by first developers, but I'm not sure that this is very important(very low possibility to requirement of some additional functionality).</p>\n"},{"tags":["mysql","performance"],"answer_count":6,"favorite_count":1,"up_vote_count":7,"down_vote_count":0,"view_count":422,"score":7,"question_id":3840906,"title":"Extra column ruins MySQL performance","body":"<p>I have a warehouse table that looks like this:</p>\n\n<pre><code>CREATE TABLE Warehouse (\n  id BIGINT(20) UNSIGNED NOT NULL AUTO_INCREMENT,\n  eventId BIGINT(20) UNSIGNED NOT NULL,\n  groupId BIGINT(20) NOT NULL,\n  activityId BIGINT(20) UNSIGNED NOT NULL,\n  ... many more ids,\n  \"txtProperty1\" VARCHAR(255),\n  \"txtProperty2\" VARCHAR(255),\n  \"txtProperty3\" VARCHAR(255),\n  \"txtProperty4\" VARCHAR(255),\n  \"txtProperty5\" VARCHAR(255),\n  ... many more of these\n  PRIMARY KEY (\"id\")\n  KEY \"WInvestmentDetail_idx01\" (\"groupId\"),\n  ... several more indices\n) ENGINE=INNODB;\n</code></pre>\n\n<p>Now, the following query spends about 0.8s in <em>query time</em> and 0.2s in <em>fetch time</em>, for a total of about one second.  The query returns ~67,000 rows.</p>\n\n<pre><code>SELECT eventId\nFROM Warehouse\nWHERE accountId IN (10, 8, 13, 9, 7, 6, 12, 11)\n  AND scenarioId IS NULL\n  AND insertDate BETWEEN DATE '2002-01-01' AND DATE '2011-12-31'\nORDER BY insertDate;\n</code></pre>\n\n<p>Adding more ids to the select clause doesn't really change the performance at all.</p>\n\n<pre><code>SELECT eventId, groupId, activityId, insertDate\nFROM Warehouse\nWHERE accountId IN (10, 8, 13, 9, 7, 6, 12, 11)\n  AND scenarioId IS NULL\n  AND insertDate BETWEEN DATE '2002-01-01' AND DATE '2011-12-31'\nORDER BY insertDate;\n</code></pre>\n\n<p>However, adding a \"property\" column does change it to 0.6s fetch time and 1.8s query time.</p>\n\n<pre><code>SELECT eventId, txtProperty1\nFROM Warehouse\nWHERE accountId IN (10, 8, 13, 9, 7, 6, 12, 11)\n  AND scenarioId IS NULL\n  AND insertDate BETWEEN DATE '2002-01-01' AND DATE '2011-12-31'\nORDER BY insertDate;\n</code></pre>\n\n<p>Now to really blow your socks off.  Instead of <em>txtProperty1</em>, using <em>txtProperty2</em> changes the times to 0.8s fetch, 24s query!</p>\n\n<pre><code>SELECT eventId, txtProperty2\nFROM Warehouse\nWHERE accountId IN (10, 8, 13, 9, 7, 6, 12, 11)\n  AND scenarioId IS NULL\n  AND insertDate BETWEEN DATE '2002-01-01' AND DATE '2011-12-31'\nORDER BY insertDate;\n</code></pre>\n\n<p>The two columns are pretty much identical in the type of data they hold: mostly non-null, and neither are indexed (not that that should make a difference anyways).  To be sure the table itself is healthy I ran analyze/optimize against it.</p>\n\n<p>This is really mystifying to me.  I can see why adding columns to the select clause only can slightly increase fetch time, but it should not change query time, especially not significantly.  I would appreciate any ideas as to what is causing this slowdown.</p>\n\n<p>EDIT - More data points</p>\n\n<p>SELECT * actually outperforms txtProperty2 - 0.8s query, 8.4s fetch.  Too bad I can't use it because the fetch time is (expectedly) too long.</p>\n"},{"tags":["performance","amazon-ec2","monitoring"],"answer_count":3,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":1024,"score":2,"question_id":5808797,"title":"Know of any tools to monitor the REAL performance of an amazon ec2 instance?","body":"<p>There are times (like this morning for 15 minutes) when the performance of my cc1.4xlarge is horrible and it is not anything running on my instance as evidenced by the aws console monitoring graphs and my own investigation.</p>\n\n<p>It seems like the physical box that it is on is having problems or maybe Xen isn't properly allocating resources.</p>\n\n<p>Are there any tools that will monitor not the \"reported\" stats on the CPU and disk, but do actual performance tests, e.g. divide some floats 1,000 times and make sure it completes within a typical time, write/read stuff to disk?</p>\n"},{"tags":["android","iphone","performance","opengl-es","glsl"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":553,"score":2,"question_id":10702065,"title":"OpenGL ES shader degrades too much performance","body":"<p>I'm optimizing a game that works for both, iPhone and Android. I'm using 4 shader to draw the scene and I noticed that if I change one of them to another the fps goes from 32 to 42, even though there's only 1 sprite being drawn with that shader, and the only difference in this 2 shaders is just a product in the fragmente shader.</p>\n\n<p>These are the shaders:  </p>\n\n<p>default-2d-tex.shader</p>\n\n<pre><code>#ifdef GL_ES\nprecision highp float;\nprecision lowp int;\n#endif\n\n#ifdef VERTEX\n\nuniform mat4        umvp;\n\nattribute vec4      avertex;\nattribute vec2      auv;\n\nvarying vec2        vuv;\n\nvoid main()\n{\n    // Pass the texture coordinate attribute to a varying.\n    vuv = auv;\n\n    // Here we set the final position to this vertex.\n    gl_Position = umvp * avertex;\n}\n\n#endif\n\n#ifdef FRAGMENT\n\nuniform sampler2D   map0;\nuniform vec4 ucolor;\n\nvarying vec2        vuv;\n\nvoid main()\n{\n    gl_FragColor =  texture2D(map0, vuv) * ucolor;\n}\n\n#endif\n</code></pre>\n\n<p>default-2d-tex-white.shader</p>\n\n<pre><code>#ifdef GL_ES\nprecision highp float;\nprecision lowp int;\n#endif\n\n#ifdef VERTEX\n\nuniform mat4        umvp;\n\nattribute vec4      avertex;\nattribute vec2      auv;\n\nvarying vec2        vuv;\n\nvoid main()\n{\n    // Pass the texture coordinate attribute to a varying.\n    vuv = auv;\n\n    // Here we set the final position to this vertex.\n    gl_Position = umvp * avertex;\n}\n\n#endif\n\n#ifdef FRAGMENT\n\nuniform sampler2D   map0;\n\nvarying vec2        vuv;\n\nvoid main()\n{\n    gl_FragColor =  texture2D(map0, vuv);\n}\n\n#endif\n</code></pre>\n\n<p>Again,<br>\nIf I modify default-2d-tex.shader and remove the product \"* ucolor\", the fps goes from 32 to 42, and I'm using it for just one sprite in the scene!</p>\n\n<p>Is this normal? Why is this shader being so slow and how can I improve it?</p>\n\n<p><strong>EDIT:</strong></p>\n\n<p>I see this performance slowdown on both iPod and Android in an equal ratio. Both are PowerVr SGX GPUs (iPod 3rd gen and Samsung Galaxy SL -PowerVR SGX 530-). iOS version is 4.1 and Android is 2.3.3</p>\n\n<p>The sprite I'm drawing is scaled to fill the screen (scaled to 4x) and I'm drawing it once per frame. It's taken from a texture map so the texture is actually larger (1024x1024) but the portion taken is 80x120. Alpha blending is enabled.</p>\n\n<p><strong>EDIT 2</strong></p>\n\n<p>I made a mistake. The sprite is scaled 11x: its 32x48.\nIf I don't draw that sprite at all, fps goes to 45. I'm drawing a lot of sprites in the scene, why is that one taking so much time? Could it be because it's scaled so much?</p>\n"},{"tags":["performance","html5","udp","websocket"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":86,"score":1,"question_id":13040752,"title":"WebSockets, UDP, and benchmarks","body":"<p>HTML5 websockets currently use a form of TCP communication. However, for real-time games, TCP just won't cut it (and is great reason to use some other platform, like native). As I probably need UDP to continue a project, I'd like to know if the specs for HTML6 or whatever will support UDP?</p>\n\n<p>Also, are there any reliable benchmarks for WebSockets that would compare the WS protocol to a  low-level, direct socket protocol?</p>\n"},{"tags":["performance","sql-server-2005","computed-columns"],"answer_count":0,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":41,"score":3,"question_id":13051507,"title":"Computed column index","body":"<p>I have a table <code>Com_Main</code> which contains column <code>CompanyName nvarchar(250)</code>. It has average length of 19, max length = 250.</p>\n\n<p>To improve performance I want to add a computed column <code>left20_CompanyName</code> which holds the first 20 characters of <code>CompanyName</code>:</p>\n\n<pre><code>alter table Com_main \nadd left20_CompanyName as LEFT(CompanyName, 20) PERSISTED\n</code></pre>\n\n<p>Then I create Index on this column:</p>\n\n<pre><code>create index ix_com_main_left20CompanyName \non Com_main (LEFT20_CompanyName)\n</code></pre>\n\n<p>So when I use</p>\n\n<pre><code>select CompanyName from Com_Main\nwhere LEFT20_CompanyName LIKE '122%'\n</code></pre>\n\n<p>it uses this nonclustered index, but when the query is like:</p>\n\n<pre><code>select CompanyName from Com_Main \nwhere CompanyName LIKE '122%'\n</code></pre>\n\n<p>It uses full table scan, and don't use this index. </p>\n\n<p>So the question:</p>\n\n<p>Is it possible to make SQL Server use this index on computable column in last query?</p>\n"},{"tags":["java","performance","memory","enums"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":60,"score":4,"question_id":13051223,"title":"ENUM storage (memory, etc)","body":"<p>I was just asking myself a little question, and I'm not sure if I can find the right answer to this:</p>\n\n<p>If I use an ENUM in Java, with an own constructor (and many, many, maaaaany parameters) - are those stored in Memory every time the program gets executed, or are they only 'loaded' in Memory, if they get used?</p>\n\n<p>What I mean is, if I have an ENUM with 400 entries, and only use one of the entries - are all others still present in Memory?</p>\n\n<p><strong>some pseudocode:</strong></p>\n\n<pre><code>public enum Type {\n    ENTRY_A(val1, val2, val3, val4, new Object(val5, val6, val7, ...)),\n    ENTRY_B(val1, val2, val3, val4, new Object(val5, val6, val7, ...)),\n    ENTRY_C(val1, val2, val3, val4, new Object(val5, val6, val7, ...)),\n    ...\n}\n</code></pre>\n\n<p>If I only use ENTRY_A, and dont touch ENTRY_B, ENTRY_C, etc - how will Java handle that exactly?</p>\n\n<p>Thanks for the Answer - and yes, this is mainly curiousity</p>\n"},{"tags":["performance","cuda","cublas"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":61,"score":0,"question_id":13051205,"title":"cuBLAS performance","body":"<p>I use CUDA 5.0, and I want to compare matrix multiplication in C and cuBLAS. I already wrote a program in which matrix multiplication in C and cuBLAS both gave correct answers. </p>\n\n<p>Now I want to compare their performance. For implementation in C, I used the <code>clock()</code>, but I found that cutil doesn't exist in CUDA 5.0, so I used <code>cudaEvent</code>. Both implementations use the same matrix, and in C, I just measured the time when C do the matrix multiplication, while in cuBLAS I began the measurement from <code>createhandle</code> till <code>destroyhandle</code>. </p>\n\n<p>I got this result:<br>\nWhen C spends just 0.08ms, cuBLAS spend 59ms, and then I used <code>clock()</code> to measure time for cuBLAS, cuBLAS became faster than C. I don't know whether the method I used to measure time is correct. Why do <code>cudaevent</code> and <code>clock()</code> give different answers? </p>\n\n<p>I use cuBLAS, cudaevent just following Nvidia's documentation. I'm really puzzled about how to measure time correctly.</p>\n"},{"tags":["performance","linux-kernel","i2c"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":26,"score":0,"question_id":13050877,"title":"I2C Data Rate Configuration 100/400KHz","body":"<p>I am currently using an I2C device within an android application and think it may be operating at 100KHz. I would like it to run at 400KHz. Assuming the I2C device accommodates this How would I go about doing this.</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["javascript","performance","asynchronous","nonblocking"],"answer_count":0,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":48,"score":0,"question_id":13050723,"title":"Way to detect if script was loaded asynch?","body":"<p>Is there a way to detect how a script was loaded without looking at the html source?</p>\n\n<p>Either via browser addon or with a little javascript or sth. like that without interfering with the page? Pagespeed for example detects if google's analytics snippet was loaded asynch, but I don't now how they do it.</p>\n\n<p>Any ideas? I'd like to do automated performance tests like this one.</p>\n"},{"tags":["c#","performance","performance-measurement"],"answer_count":9,"favorite_count":2,"up_vote_count":9,"down_vote_count":0,"view_count":2748,"score":9,"question_id":2072361,"title":"What is the best way to measure how long code takes to execute?","body":"<p>I'm trying to determine which approach to removing a string is the <strong>fastest</strong>.</p>\n\n<p>I simply get the <strong>start</strong> and <strong>end</strong> time and show the difference.</p>\n\n<p>But the results are so <strong>varied</strong>, e.g. as shown below the same method can take from 60 ms to 231 ms.</p>\n\n<p><strong>What is a better method to get more accurate results?</strong></p>\n\n<p><img src=\"http://www.deviantsart.com/upload/1q4t3rl.png\" alt=\"alt text\"></p>\n\n<pre><code>using System;\nusing System.Collections;\nusing System.Collections.Generic;\n\nnamespace TestRemoveFast\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            for (int j = 0; j &lt; 10; j++)\n            {\n                string newone = \"\";\n                List&lt;string&gt; tests = new List&lt;string&gt;();\n                for (int i = 0; i &lt; 100000; i++)\n                {\n                    tests.Add(\"{http://company.com/Services/Types}ModifiedAt\");\n                }\n\n                DateTime start = DateTime.Now;\n                foreach (var test in tests)\n                {\n                    //newone = ((System.Xml.Linq.XName)\"{http://company.com/Services/Types}ModifiedAt\").LocalName;\n                    newone = Clean(test);\n                }\n\n                Console.WriteLine(newone);\n                DateTime end = DateTime.Now;\n                TimeSpan duration = end - start;\n                Console.WriteLine(duration.ToString());\n            }\n\n            Console.ReadLine();\n        }\n\n        static string Clean(string line)\n        {\n            int pos = line.LastIndexOf('}');\n            if (pos &gt; 0)\n                return line.Substring(pos + 1, line.Length - pos - 1);\n                //return line.Substring(pos + 1);\n            else\n                return line;\n        }\n    }\n}\n</code></pre>\n"},{"tags":["c#","performance","datetime","timing","stopwatch"],"answer_count":3,"favorite_count":2,"up_vote_count":26,"down_vote_count":0,"view_count":4451,"score":26,"question_id":2923283,"title":"Stopwatch vs. using System.DateTime.Now for timing events","body":"<p>I wanted to track the performance of my code so I stored the start and end time using <code>System.DateTime.Now</code>. I took the difference between the two as the time my code to execute. </p>\n\n<p>I noticed though that the difference didn't appear to be accurate. So I tried using a <code>Stopwatch</code> object. This turned out to be much, much more accurate.</p>\n\n<p>Can anyone tell me why <code>Stopwatch</code> would be more accurate than calculating the difference between a start and end time using <code>System.DateTime.Now</code>?</p>\n\n<p>BTW, I'm not talking about a tenths of a percent. I get about a 15-20% difference.</p>\n"},{"tags":["performance","sql-server-2008","c#-4.0","table","entity-framework-4"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":13048907,"title":"Sql Server heavily queried Table - should I store secondary info (html text) in another table","body":"<p>The Overview:</p>\n\n<p>I have a table \"category\" that is for the most part used to categorise products and currently looks like this:</p>\n\n<pre><code>CREATE TABLE [dbo].[Category]\n( \nCategoryId int IDENTITY(1,1) NOT NULL, \nCategoryNode hierarchyid NOT NULL UNIQUE,\nCategoryString AS CategoryNode.ToString() PERSISTED,\nCategoryLevel AS CategoryNode.GetLevel() PERSISTED,\nCategoryTitle varchar(50) NOT NULL,\nIsActive bit NOT NULL DEFAULT 1\n)\n</code></pre>\n\n<p>This table is heavily queried to display the category hierarchy on a shopping website (typically every page view) and can have a substantial number of items.</p>\n\n<p>I'm using the Entity Framework in my data layer.</p>\n\n<p>The Question:</p>\n\n<p>I have a need to add what could potentially be a fairly large \"description\" which could come in the form of the entire contents of a web-page and I'm wondering whether I should store this in a related table rather than adding it to the existing category table given that the entity framework will drag the \"description\" column out of the database 100% of the time when 99.5% of the time I'll only want the CategoryTitle and CategoryId.</p>\n\n<p>Typically I wouldn't worry about the overhead of the Entity Framework, but in the case I think it might be important to take it into consideration. I could work around this with a view or a complex type from a stored proc, but this means a lot of refactoring that I'd prefer to avoid.</p>\n\n<p>I'm just interested to know if anyone has any thoughts, suggestions or a desire to slap my wrists in relation to this scenario...</p>\n\n<p>EDIT:</p>\n\n<p>I should add that the reason I'm hesitating to set up a secondary table is because I don't like the idea of adding an additional table that has a 1 to 1 relationship with the Category table - it seems somewhat pointless. But I'm also not a DBA so I'm not sure whether this is an acceptable practice or not.</p>\n"},{"tags":["java","database","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":56,"score":1,"question_id":13048762,"title":"Result Set to Multi Hash Map","body":"<p>I have a situation here. I have a huge database with >10 columns and millions of rows. I am using a matching algorithm which matches each input records with the values in database. </p>\n\n<p>The database operation is taking lot of time when there are millions of records to match. I am thinking of using a multi-hash map or any resultset alternative so that i can save the whole table in memory and prevent hitting database again....</p>\n\n<p>Can anybody tell me what should i do??</p>\n"},{"tags":["java","search","lucene","performance"],"answer_count":2,"favorite_count":1,"up_vote_count":7,"down_vote_count":0,"view_count":1248,"score":7,"question_id":4919001,"title":"Fastest way to count all results in Lucene (java)","body":"<p>What is the fastest way to count all results for a given Query in Lucene?</p>\n\n<ol>\n<li>TopDocs.totalHits </li>\n<li>implement and manage a Filter, using QueryFilter</li>\n<li>implement a custom 'counting' Collector. This simply increments a count in the collect(int doc) method and returns true for the acceptsDocOutOfOrder() method. All other methods are NOOPS.</li>\n</ol>\n\n<p>Since 1. will do scoring on all docs, and 2. could have an upfront hit due to loading of the FieldCache, I assume the answer is 3. It just seems odd that Lucene doesn't provide such a collector out of the box?</p>\n"},{"tags":["performance","apache","node.js","apache2"],"answer_count":6,"favorite_count":5,"up_vote_count":17,"down_vote_count":0,"view_count":2375,"score":17,"question_id":6634299,"title":"Node.js slower than Apache","body":"<p>I am comparing performance of Node.js (0.5.1-pre) vs Apache (2.2.17) for a very simple scenario - serving a text file.</p>\n\n<p>Here's the code I use for node server:</p>\n\n<pre><code>var http = require('http')\n  , fs = require('fs')\n\nfs.readFile('/var/www/README.txt',\n    function(err, data) {\n        http.createServer(function(req, res) {\n            res.writeHead(200, {'Content-Type': 'text/plain'})\n            res.end(data)\n        }).listen(8080, '127.0.0.1')\n    }\n)\n</code></pre>\n\n<p>For Apache I am just using whatever default configuration which goes with Ubuntu 11.04</p>\n\n<p>When running Apache Bench with the following parameters against <strong>Apache</strong></p>\n\n<pre><code>ab -n10000 -c100 http://127.0.0.1/README.txt\n</code></pre>\n\n<p>I get the following runtimes:</p>\n\n<pre><code>Time taken for tests:   1.083 seconds\nComplete requests:      10000\nFailed requests:        0\nWrite errors:           0\nTotal transferred:      27630000 bytes\nHTML transferred:       24830000 bytes\nRequests per second:    9229.38 [#/sec] (mean)\nTime per request:       10.835 [ms] (mean)\nTime per request:       0.108 [ms] (mean, across all concurrent requests)\nTransfer rate:          24903.11 [Kbytes/sec] received\n\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        0    0   0.8      0       9\nProcessing:     5   10   2.0     10      23\nWaiting:        4   10   1.9     10      21\nTotal:          6   11   2.1     10      23\n\nPercentage of the requests served within a certain time (ms)\n  50%     10\n  66%     11\n  75%     11\n  80%     11\n  90%     14\n  95%     15\n  98%     18\n  99%     19\n 100%     23 (longest request)\n</code></pre>\n\n<p>When running Apache bench against <strong>node</strong> instance, these are the runtimes:</p>\n\n<pre><code>Time taken for tests:   1.712 seconds\nComplete requests:      10000\nFailed requests:        0\nWrite errors:           0\nTotal transferred:      25470000 bytes\nHTML transferred:       24830000 bytes\nRequests per second:    5840.83 [#/sec] (mean)\nTime per request:       17.121 [ms] (mean)\nTime per request:       0.171 [ms] (mean, across all concurrent requests)\nTransfer rate:          14527.94 [Kbytes/sec] received\n\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        0    0   0.9      0       8\nProcessing:     0   17   8.8     16      53\nWaiting:        0   17   8.6     16      48\nTotal:          1   17   8.7     17      53\n\nPercentage of the requests served within a certain time (ms)\n  50%     17\n  66%     21\n  75%     23\n  80%     25\n  90%     28\n  95%     31\n  98%     35\n  99%     38\n 100%     53 (longest request)\n</code></pre>\n\n<p>Which is clearly slower than Apache. This is especially surprising if you consider the fact that Apache is doing a lot of other stuff, like logging etc.</p>\n\n<p>Am I doing it wrong? Or is Node.js really slower in this scenario?</p>\n\n<p><strong>Edit 1</strong>: I do notice that node's concurrency is better - when increasing a number of simultaneous request to 1000, Apache starts dropping few of them, while node works fine with no connections dropped.</p>\n"},{"tags":["performance","internet-explorer","gwt","grid","smartgwt"],"answer_count":3,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":5323,"score":2,"question_id":1070856,"title":"SmartGWT ListGrid is slow, but only in Internet Explorer","body":"<p>we have migrated from gwtext to smartgwt and overall the experience is ok. However, we have big problems with the ListGrid component of SmartGWT. </p>\n\n<p>It is very slow if both of the following conditions are met:</p>\n\n<ul>\n<li>Internet Explorer is used</li>\n<li>5 or more columns</li>\n</ul>\n\n<p>the speed will decrease if you add more columns up to the point where the whole thing is unusable and you have to kill the browser, e.g. through the windows task manager.</p>\n\n<p>Grids with 1 column are fine in internet explorer</p>\n\n<p>Grids with a large number of columns are fine in firefox, opera</p>\n\n<p>In the smartgwt forums I've found two entries that are about this problem, but none of the suggested workarounds has fixed the problem. </p>\n\n<p><a href=\"http://forums.smartclient.com/showthread.php?t=5896\" rel=\"nofollow\">http://forums.smartclient.com/showthread.php?t=5896</a></p>\n\n<p>Since I am only allowed to post one hyperlink, here's the number of the second thread:</p>\n\n<p>t=5193</p>\n\n<p>Any help is greatly appreciated</p>\n"},{"tags":["java","performance","java-ee","websphere"],"answer_count":8,"favorite_count":2,"up_vote_count":7,"down_vote_count":1,"view_count":4892,"score":6,"question_id":1178210,"title":"Websphere Application Server - What on earth will it take to start any fast?","body":"<p>I am using Rational Application Developer v7.0 that ships with an integrated test environment. When I get to debugging my webapp, the server startup time in debug mode is close to 5-6 minutes - enough time to take a coffe break! </p>\n\n<p>At times, it so pisses me off that I start cursing IBM for building an operating system! instead of an app server: Spawning 20+ processes and useless services with no documented configuration to tuning it, to starting any faster. </p>\n\n<p>I am sure there are many java developers out there, who would agree with me on this. Now, I tried to disable the default apps and a set of services via my admin console, however, that hasn't helped much.</p>\n\n<p>I have no webservices, no enterprise beans, no queues, just a simple web app which requires a connection pool. Have you done something in the past to make your integrated test environment, start fast in debug mode and there by consume less RAM? </p>\n\n<p>UPDATE: \nI tried disabling a few services (internationaliztion, default apps etc...) and now the websphere server went from bad to worse. Not only doesn't it take horrifying startup time, it keeps freezing every now and then for upto 2 minutes. :-( Sounds like, optimization is not such a good thing, always!</p>\n"},{"tags":["c++","performance"],"answer_count":6,"favorite_count":0,"up_vote_count":11,"down_vote_count":0,"view_count":186,"score":11,"question_id":5528569,"title":"repeated calling - coding practice","body":"<p>which one do you prefer? (of course getSize doesn't make any complicated counting, just returning member value)</p>\n\n<pre><code>void method1(Object &amp; o)\n{\n    int size = o.getSize();\n\n    someAction(size);\n    someOtherAction(size);\n}\n</code></pre>\n\n<p>or</p>\n\n<pre><code>void method2(Object &amp; o)\n{\n    someAction(o.getSize());\n    someOtherAction(o.getSize());\n}\n</code></pre>\n\n<p>I know I can measure which one is faster but I want some comments... Not just executing time related... eg. if you are prefer method2, how many times maximally do you use o.getSize and what is the number what make you use method1 way?\nAny best practices? (imagine even different types then int)\nTY</p>\n"},{"tags":["ios","performance","image-processing","opengl-es"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":174,"score":1,"question_id":12052151,"title":"Image analysis through GPU (with OpenGL ES) or CPU?","body":"<p>I'd like to analyze the constantly updating image feed that comes from an iPhone camera to determine a general \"lightness coefficient\". Meaning: if the coefficient returns 0.0, the image is completely black, if it returns 1.0 the image is completely white. Of course all values in between are the ones that I care about the most (background info: I'm using this coefficient to calculate the intensity of some blending effects in my fragment shader).</p>\n\n<p>So I'm wondering if I should run a for loop over my pixelbuffer and analyze the image every frame (30 fps) and send the coeff as a uniform to my fragment shader or is there a way to analyze my image in OpenGL. If so, how should I do that?</p>\n"},{"tags":["ios","performance","opengl-es","texture"],"answer_count":2,"favorite_count":2,"up_vote_count":1,"down_vote_count":0,"view_count":224,"score":1,"question_id":10296149,"title":"(iPhone, OpenGL) direct texture data storage in files","body":"<p>At this moment I use this scenario to load OpenGL texture from PNG:</p>\n\n<ul>\n<li>load PNG via UIImage</li>\n<li>get pixels data via bitmap context</li>\n<li>repack pixels to new format (currently RGBA8 -> RGBA4, RGB8 -> RGB565, using ARM NEON instructions)</li>\n<li>create OpenGL texture with data</li>\n</ul>\n\n<p>(this approach is commonly used in Cocos2d engine)</p>\n\n<p>It takes much time and seems to do extra work that may be done once per build. So I want to save repacked pixels data back into file and load it directly to OpenGL on second time.</p>\n\n<p>I would know the practical advantages. Does anyone tried it? Is it worth to compress data via zip (as I know, current iDevices have bottleneck in file access)? Would be very thankful for real experience sharing.</p>\n"},{"tags":["ios","performance","opengl-es"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":241,"score":-1,"question_id":9255118,"title":"What is the fastest drawing method on iOS?","body":"<p>I'm developing a 2D game on iOS, but I'm finding it difficult getting drawing to run fast (60 FPS on Retina display).</p>\n\n<p>I've first used UIKit for drawing, which is of course not suitable for a game. I coulnd't draw a couple of sprites without slowdown.</p>\n\n<p>Then I moved on to OpenGL, because I read it's the closest I can get to the GPU (which I think it means it's the fastest possible). I was using glDrawArrays(). When I ran it on the Simulator, FPS dropped when I was reaching over 200 triangles. People said it was because the Simulator or the computer are not optimized to run iOS OpenGL. Then I tested it on a real device, and to my surprise, the performance difference was really small. It still couldn't run that few triangles smoothly - and I know other games on iOS use a lot more polygons, shaders, 3D graphics, etc.</p>\n\n<p>When I ran it through Instruments to check OpenGL performance, it told me I could speed it up by using VBOs. So I rewrote my code to use VBO instead, updating all vertices each frame. Performance increased very little, and I still can't surpass 200 triangles at consistent 60 FPS. And that is 2D drawing alone without context changes/transformations. I also didn't write the game yet - there are no objects making no CPU-intensive tasks.</p>\n\n<p>Everyone I ask says OpenGL is top performance. What could I possibly be doing wrong? I am assuming OpenGL can handle LOTS of polygons that are updated each frame - is that right? Which method other games use that I see they run fine, like Infinity Blade which is 3D, or even Angry Birds which has lots of ever-updating sprites? What is recommended when making a game?</p>\n"},{"tags":["c++","performance","opengl","optimization","opengl-es"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":107,"score":2,"question_id":12894397,"title":"Opengl performance issue when drawing polygon meshes","body":"<p>I'm using the following code to draw some polygon meshes in a 3D game.</p>\n\n<pre><code>void drawModelFace(const MeshFace *face, float *vertices, float *vertNormals, float *textureVerts)\n{   \n    glBegin(GL_POLYGON);\n    for (int i = 0; i &lt; face-&gt;_numVertices; i++) \n    {\n       glNormal3fv(&amp;vertNormals[3 * face-&gt;_vertices[i]]);\n\n       if (face-&gt;_texVertices)\n       {\n           glTexCoord2fv(&amp;textureVerts[2 * face-&gt;_texVertices[i]]);\n       }\n\n       glVertex3fv(&amp;vertices[3 * face-&gt;_vertices[i]]);\n    }\n    glEnd();\n}\n</code></pre>\n\n<p>My problem is that I'm experiencing some performance issue ingame when this function is called a lot of time. </p>\n\n<p>This function is called on average 50000 times per second which gives a constant 60fps but on some places it's called 100000 times per second which gives a 15fps. (I'm using a today's computer underclocked to 1Ghz to simulate the performance of today's phone)</p>\n\n<p>I heard that immediate mode could be slow that's why I tried using glDrawArrays instead. Here's the code:</p>\n\n<pre><code>void drawModelFace(const MeshFace *face, float *vertices, float *vertNormals, float *textureVerts)\n{   \n    GLfloat vert[3*face-&gt;_numVertices];\n    GLfloat normal[3*face-&gt;_numVertices];\n    GLfloat tex[2*face-&gt;_numVertices];\n\n    glEnableClientState(GL_VERTEX_ARRAY);\n    glEnableClientState(GL_TEXTURE_COORD_ARRAY);\n    glEnableClientState(GL_NORMAL_ARRAY);\n    glVertexPointer(3, GL_FLOAT, 0, vert);\n    glTexCoordPointer(2, GL_FLOAT, 0, tex);\n    glNormalPointer(GL_FLOAT, 0, normal);\n\n    for (int i = 0; i &lt; face-&gt;_numVertices; i++) \n    {\n        vert[0 + (i*3)] = vertices[3 * face-&gt;_vertices[i]];\n        vert[1 + (i*3)] = vertices[3 * face-&gt;_vertices[i]+1];\n        vert[2 + (i*3)] = vertices[3 * face-&gt;_vertices[i]+2];\n\n        normal[0 + (i*3)] = vertNormals[3 * face-&gt;_vertices[i]];\n        normal[1 + (i*3)] = vertNormals[3 * face-&gt;_vertices[i]+1];\n        normal[2 + (i*3)] = vertNormals[3 * face-&gt;_vertices[i]+2];\n\n            if (face-&gt;_texVertices)\n            {\n                tex[0 + (i*2)] = textureVerts[2 * face-&gt;_texVertices[i]];\n                tex[1 + (i*2)] = textureVerts[2 * face-&gt;_texVertices[i]+1];\n            }\n    }\n\n    glDrawArrays(GL_TRIANGLE_FAN ,0, face-&gt;_numVertices);\n    glDisableClientState(GL_VERTEX_ARRAY);\n    glDisableClientState(GL_TEXTURE_COORD_ARRAY);\n    glDisableClientState(GL_NORMAL_ARRAY); \n}\n</code></pre>\n\n<p>But the performance results are exactly the same.</p>\n\n<p>How can I optimize my code to gain some fps?</p>\n\n<p>Note that my final goal is to use this code an android devices thus glBegin and glEnd are not allowed anymore. </p>\n"},{"tags":["python","performance","numpy","scipy","cython"],"answer_count":1,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":115,"score":4,"question_id":13044515,"title":"Cython code 3-4 times slower than Python / Numpy code?","body":"<p>I am trying to convert my Python / Numpy code to Cython code for speedup purposes. However, Cython is MUCH slower (3-4 times) than the Python / Numpy code. Am I using Cython correctly? Am I passing arguments correctly to myc_rb_etc() in my Cython code? What about when I call the integrate function? Thank you in advance for your help.\nHere is my Python / Numpy code: </p>\n\n<pre><code>from pylab import * \nimport pylab as pl\nfrom numpy import *\nimport numpy as np\nfrom scipy import integrate\n\ndef myc_rb_e2f(y,t,k,d):\n\n    M = y[0]\n    E = y[1]\n    CD = y[2]\n    CE = y[3]\n    R = y[4]\n    RP = y[5] \n    RE = y[6]\n\n    S = 0.01\n    if t &gt; 300:\n        S = 5.0\n    #if t &gt; 400\n        #S = 0.01\n\n    t1 = k[0]*S/(k[7]+S);\n    t2 = k[1]*(M/(k[14]+M))*(E/(k[15]+E));\n    t3 = k[5]*M/(k[14]+M);\n    t4 = k[11]*CD*RE/(k[16]+RE);\n    t5 = k[12]*CE*RE/(k[17]+RE);\n    t6 = k[2]*M/(k[14]+M);\n    t7 = k[3]*S/(k[7]+S);\n    t8 = k[6]*E/(k[15]+E);\n    t9 = k[13]*RP/(k[18]+RP);\n    t10 = k[9]*CD*R/(k[16]+R);\n    t11 = k[10]*CE*R/(k[17]+R);\n\n    dM = t1-d[0]*M\n    dE = t2+t3+t4+t5-k[8]*R*E-d[1]*E\n    dCD = t6+t7-d[2]*CD\n    dCE = t8-d[3]*CE\n    dR = k[4]+t9-k[8]*R*E-t10-t11-d[4]*R\n    dRP = t10+t11+t4+t5-t9-d[5]*RP\n    dRE = k[8]*R*E-t4-t5-d[6]*RE\n\n    dy = [dM,dE,dCD,dCE,dR,dRP,dRE]\n\n    return dy\n\nt = np.zeros(10000)\nt = np.linspace(0.,3000.,10000.)\n\n# Initial concentrations of [M,E,CD,CE,R,RP,RE]\ny0 = np.array([0.,0.,0.,0.,0.4,0.,0.25])\nE_simulated = np.zeros([10000,5000])\nE_avg = np.zeros([10000])\nk = np.zeros([19])\nd = np.zeros([7])\n\nfor i in range (0,5000):\n    k[0] = 1.+0.1*randn(1)\n    k[1] = 0.15+0.05*randn(1)\n    k[2] = 0.2+0.05*randn(1)\n    k[3] = 0.2+0.05*randn(1)\n    k[4] = 0.35+0.05*randn(1)\n    k[5] = 0.001+0.0001*randn(1)\n    k[6] = 0.5+0.05*randn(1)\n    k[7] = 0.3+0.05*randn(1)\n    k[8] = 30.+5.*randn(1)\n    k[9] = 18.+3.*randn(1)\n    k[10] = 18.+3.*randn(1)\n    k[11] = 18.+3.*randn(1)\n    k[12] = 18.+3.*randn(1)\n    k[13] = 3.6+0.5*randn(1)\n    k[14] = 0.15+0.05*randn(1)\n    k[15] = 0.15+0.05*randn(1)\n    k[16] = 0.92+0.1*randn(1)\n    k[17] = 0.92+0.1*randn(1)\n    k[18] = 0.01+0.001*randn(1)\n    d[0] = 0.7+0.05*randn(1)\n    d[1] = 0.25+0.025*randn(1)\n    d[2] = 1.5+0.05*randn(1)\n    d[3] = 1.5+0.05*randn(1)\n    d[4] = 0.06+0.01*randn(1)\n    d[5] = 0.06+0.01*randn(1)\n    d[6] = 0.03+0.005*randn(1)\n    r = integrate.odeint(myc_rb_e2f,y0,t,args=(k,d))\n    E_simulated[:,i] = r[:,1]\n\nfor i in range(0,10000):\n    E_avg[i] = sum(E_simulated[i,:])/5000.\n\npl.plot(t,E_avg,'-ro')\npl.show()\n</code></pre>\n\n<p>Here is the code converted into Cython:</p>\n\n<pre><code>cimport numpy as np\nimport numpy as np\nfrom numpy import *\nimport pylab as pl\nfrom pylab import * \nfrom scipy import integrate\n\ndef myc_rb_e2f(y,t,k,d):\n\n    cdef double M = y[0]\n    cdef double E = y[1]\n    cdef double CD = y[2]\n    cdef double CE = y[3]\n    cdef double R = y[4]\n    cdef double RP = y[5] \n    cdef double RE = y[6]\n\n    cdef double S = 0.01\n    if t &gt; 300.0:\n        S = 5.0\n    #if t &gt; 400\n        #S = 0.01\n\n    cdef double t1 = k[0]*S/(k[7]+S)\n    cdef double t2 = k[1]*(M/(k[14]+M))*(E/(k[15]+E))\n    cdef double t3 = k[5]*M/(k[14]+M)\n    cdef double t4 = k[11]*CD*RE/(k[16]+RE)\n    cdef double t5 = k[12]*CE*RE/(k[17]+RE)\n    cdef double t6 = k[2]*M/(k[14]+M)\n    cdef double t7 = k[3]*S/(k[7]+S)\n    cdef double t8 = k[6]*E/(k[15]+E)\n    cdef double t9 = k[13]*RP/(k[18]+RP)\n    cdef double t10 = k[9]*CD*R/(k[16]+R)\n    cdef double t11 = k[10]*CE*R/(k[17]+R)\n\n    cdef double dM = t1-d[0]*M\n    cdef double dE = t2+t3+t4+t5-k[8]*R*E-d[1]*E\n    cdef double dCD = t6+t7-d[2]*CD\n    cdef double dCE = t8-d[3]*CE\n    cdef double dR = k[4]+t9-k[8]*R*E-t10-t11-d[4]*R\n    cdef double dRP = t10+t11+t4+t5-t9-d[5]*RP\n    cdef double dRE = k[8]*R*E-t4-t5-d[6]*RE\n\n    dy = [dM,dE,dCD,dCE,dR,dRP,dRE]\n\n    return dy\n\n\ndef main():\n    cdef np.ndarray[double,ndim=1] t = np.zeros(10000)\n    t = np.linspace(0.,3000.,10000.)\n    # Initial concentrations of [M,E,CD,CE,R,RP,RE]\n    cdef np.ndarray[double,ndim=1] y0 = np.array([0.,0.,0.,0.,0.4,0.,0.25])\n    cdef np.ndarray[double,ndim=2] E_simulated = np.zeros([10000,5000])\n    cdef np.ndarray[double,ndim=2] r = np.zeros([10000,7])\n    cdef np.ndarray[double,ndim=1] E_avg = np.zeros([10000])\n    cdef np.ndarray[double,ndim=1] k = np.zeros([19])\n    cdef np.ndarray[double,ndim=1] d = np.zeros([7])\n    cdef int i\n    for i in range (0,5000):\n        k[0] = 1.+0.1*randn(1)\n        k[1] = 0.15+0.05*randn(1)\n        k[2] = 0.2+0.05*randn(1)\n        k[3] = 0.2+0.05*randn(1)\n        k[4] = 0.35+0.05*randn(1)\n        k[5] = 0.001+0.0001*randn(1)\n        k[6] = 0.5+0.05*randn(1)\n        k[7] = 0.3+0.05*randn(1)\n        k[8] = 30.+5.*randn(1)\n        k[9] = 18.+3.*randn(1)\n        k[10] = 18.+3.*randn(1)\n        k[11] = 18.+3.*randn(1)\n        k[12] = 18.+3.*randn(1)\n        k[13] = 3.6+0.5*randn(1)\n        k[14] = 0.15+0.05*randn(1)\n        k[15] = 0.15+0.05*randn(1)\n        k[16] = 0.92+0.1*randn(1)\n        k[17] = 0.92+0.1*randn(1)\n        k[18] = 0.01+0.001*randn(1)\n        d[0] = 0.7+0.05*randn(1)\n        d[1] = 0.25+0.025*randn(1)\n        d[2] = 1.5+0.05*randn(1)\n        d[3] = 1.5+0.05*randn(1)\n        d[4] = 0.06+0.01*randn(1)\n        d[5] = 0.06+0.01*randn(1)\n        d[6] = 0.03+0.005*randn(1)\n        r = integrate.odeint(myc_rb_e2f,y0,t,args=(k,d))\n        E_simulated[:,i] = r[:,1]\n    for i in range(0,10000):\n        E_avg[i] = sum(E_simulated[i,:])/5000.\n    pl.plot(t,E_avg,'-ro')\n    pl.show()\n</code></pre>\n\n<p>Here are some pstats from cProfile on my Python / Numpy code:</p>\n\n<p><code>ncalls tottime   percall  cumtime  percall</code></p>\n\n<p><code>5000   82.505    0.017  236.760    0.047 {scipy.integrate._odepack.odeint}</code></p>\n\n<p><code>1    1.504    1.504  238.949  238.949 myc_rb_e2f.py:1(&lt;module&gt;)</code></p>\n\n<p><code>5000    0.025    0.000  236.855    0.047 C:\\Python27\\lib\\site-packages\\scipy\\integrate\\odepack.py:18(odeint)</code></p>\n\n<p><code>12291237  154.255    0.000  154.255    0.000 myc_rb_e2f.py:7(myc_rb_e2f)</code></p>\n"},{"tags":["performance","hadoop","shuffle","reduce"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":223,"score":2,"question_id":12717659,"title":"Hadoop reduce shuffle merge in memory","body":"<p>I'm having some performance issues with Reduce Merge phase and I wonder if someone can take a look. I have a 6 GB dataset (text), evenly distributed on the cluster, The dataset has two keys that I then GroupBy into two reducers (I'm using cascading). So each reducer has 3GB of data. I give each reducer 12 GB of memory, but I'm still seeing a 20 minute merge phase.</p>\n\n<p>Two questions: Shouldn't this merge be done entirely in memory (if I have 12 GB of Heap). Even without an in memory merge, 20 minutes seems like way way too long to merge 3GB, especially with 12 disks(JBOD) and 12 cores on a node. I'm wondering if I'm writing the partial merge data to the wrong place (HDFS, vs local?).</p>\n\n<p>the MAPRFS_BYTES_READ, and MAPRFS_BYTES_WRITTEN are interesting. The initial dataset is 6GB (which it shows in the Map column). Somehow sorting increases it to 17GB, which seems odd. Then in the reduce Phase it's reading 23GB from MapRfs, and writing 17GB. Should the reduce phase merge data be written to MapRFS or to the local FS? WHy would the size grow so much over the initial dataset (no compression is used, it's straight text)</p>\n\n<hr>\n\n<pre><code>    Counter     Map     Reduce  Total\nJob Counters    Aggregate execution time of mappers(ms)     0   0   29,887,359\nLaunched reduce tasks   0   0   2\nRack-local map tasks    0   0   4\nLaunched map tasks  0   0   353\nData-local map tasks    0   0   311\ncascading.flow.SliceCounters    Read_Duration   329,399     366,004     695,403\nTuples_Read     252,000,000     67,896,295  319,896,295\nTuples_Written  252,000,000     0   252,000,000\nProcess_End_Time    476,294,761,317,139     0   476,294,761,317,139\nWrite_Duration  2,713,840   0   2,713,840\nProcess_Begin_Time  476,294,753,764,176     2,698,557,228,678   478,993,310,992,854\nFileSystemCounters  MAPRFS_BYTES_READ   6,651,978,400   21,721,014,791  28,372,993,191\nMAPRFS_BYTES_WRITTEN    17,044,716,578  17,044,701,398  34,089,417,976\nFILE_BYTES_WRITTEN  19,046,005  107,748     19,153,753\nMap-Reduce Framework    Map input records   252,000,000     0   252,000,000\nReduce shuffle bytes    0   16,980,659,887  16,980,659,887\nSpilled Records     252,000,000     0   252,000,000\nMap output bytes    16,540,701,046  0   16,540,701,046\nCPU_MILLISECONDS    18,861,020  7,640,360   26,501,380\nMap input bytes     6,644,947,675   0   6,644,947,675\nCombine input records   0   0   0\nSPLIT_RAW_BYTES     97,428  0   97,428\nReduce input records    0   67,896,295  67,896,295\nReduce input groups     0   2   2\nCombine output records  0   0   0\nPHYSICAL_MEMORY_BYTES   324,852,019,200     15,041,486,848  339,893,506,048\nReduce output records   0   0   0\nVIRTUAL_MEMORY_BYTES    626,863,038,464     26,729,230,336  653,592,268,800\nMap output records  252,000,000     0   252,000,000\nGC time elapsed (ms)    1,568,523   76,636  1,645,159\ncascading.flow.StepCounters     Tuples_Read     252,000,000     0   252,000,000\n</code></pre>\n\n<hr>\n\n<pre><code>name    value\nfs.s3n.impl org.apache.hadoop.fs.s3native.NativeS3FileSystem\nmapreduce.heartbeat.100 1000\nmapred.task.cache.levels    2\nhadoop.tmp.dir  /tmp/hadoop-${user.name}\nhadoop.native.lib   true\nmap.sort.class  org.apache.hadoop.util.QuickSort\nmapreduce.jobtracker.recovery.dir   /var/mapr/cluster/mapred/jobTracker/recovery\nmapreduce.heartbeat.1000    10000\nipc.client.idlethreshold    4000\nmapred.system.dir   /var/mapr/cluster/mapred/jobTracker/system\nmapreduce.cluster.reduce.userlog.retain-size    10485760\nmapred.job.tracker.persist.jobstatus.hours  0\nio.skip.checksum.errors false\nfs.default.name maprfs:///\nmapred.cluster.reduce.memory.mb -1\nmapred.child.tmp    ./tmp\nfs.har.impl.disable.cache   true\nmapred.jobtracker.jobhistory.lru.cache.size 5\nmapred.skip.reduce.max.skip.groups  0\ncascading.flow.step.num 1\nmapred.jobtracker.instrumentation   org.apache.hadoop.mapred.JobTrackerMetricsInst\nmapr.localvolumes.path  /var/mapr/local\nmapred.tasktracker.dns.nameserver   default\nio.sort.factor  50\nmapred.output.value.groupfn.class   cascading.tuple.hadoop.util.GroupingComparator\nmapreduce.use.maprfs    true\nmapred.task.timeout 600000\nmapred.max.tracker.failures 4\nhadoop.rpc.socket.factory.class.default org.apache.hadoop.net.StandardSocketFactory\nmapred.mapoutput.key.class  cascading.tuple.io.TuplePair\nfs.hdfs.impl    org.apache.hadoop.hdfs.DistributedFileSystem\nmapred.queue.default.acl-administer-jobs    \nmapred.output.key.class org.apache.hadoop.io.Text\nmapred.skip.map.auto.incr.proc.count    true\nmapred.map.runner.class cascading.flow.hadoop.FlowMapper\nmapreduce.job.complete.cancel.delegation.tokens true\nmapreduce.tasktracker.heapbased.memory.management   false\nio.mapfile.bloom.size   1048576\ntasktracker.http.threads    2\nmapred.job.shuffle.merge.percent    0.70\ncascading.flow.id   853276BF02049D394C31880B08C9E6CC\nmapred.child.renice 10\nfs.ftp.impl org.apache.hadoop.fs.ftp.FTPFileSystem\nuser.name   jdavis\nmapred.fairscheduler.smalljob.max.inputsize 10737418240\nmapred.output.compress  false\nio.bytes.per.checksum   512\nmapred.healthChecker.script.timeout 600000\ntopology.node.switch.mapping.impl   org.apache.hadoop.net.ScriptBasedMapping\nmapred.reduce.slowstart.completed.maps  0.95\nmapred.reduce.max.attempts  4\nfs.ramfs.impl   org.apache.hadoop.fs.InMemoryFileSystem\nmapr.localoutput.dir    output\nmapred.skip.map.max.skip.records    0\nmapred.jobtracker.port  9001\nmapred.cluster.map.memory.mb    -1\nmapreduce.tasktracker.prefetch.maptasks 1.0\nhadoop.security.group.mapping   org.apache.hadoop.security.ShellBasedUnixGroupsMapping\nmapreduce.tasktracker.task.slowlaunch   false\nmapred.job.tracker.persist.jobstatus.dir    /var/mapr/cluster/mapred/jobTracker/jobsInfo\nmapred.jar  /var/mapr/cluster/mapred/jobTracker/staging/jdavis/.staging/job_201210022148_0086/job.jar\nfs.s3.buffer.dir    ${hadoop.tmp.dir}/s3\njob.end.retry.attempts  0\nfs.file.impl    org.apache.hadoop.fs.LocalFileSystem\ncascading.app.name  omeg\nmapred.local.dir.minspacestart  0\nmapred.output.compression.type  RECORD\nfs.mapr.working.dir /user/$USERNAME/\nfs.maprfs.impl  com.mapr.fs.MapRFileSystem\nfs.https.impl   cascading.tap.hadoop.io.HttpFileSystem\ntopology.script.number.args 100\nio.mapfile.bloom.error.rate 0.005\nmapred.cluster.max.reduce.memory.mb -1\nmapred.max.tracker.blacklists   4\nmapred.task.profile.maps    0-2\nmapred.userlog.retain.hours 24\nmapred.job.tracker.persist.jobstatus.active false\nhadoop.security.authorization   false\nlocal.cache.size    10737418240\nmapred.min.split.size   0\nmapred.map.tasks    353\nmapred.tasktracker.task-controller.config.overwrite true\ncascading.app.appjar.path   /home/jdavis/tmp/omeg.jar\nmapred.output.value.class   org.apache.hadoop.io.Text\nmapred.partitioner.class    cascading.tuple.hadoop.util.GroupingPartitioner\nmapreduce.maprfs.use.compression    true\nmapred.job.queue.name   default\nmapreduce.tasktracker.reserved.physicalmemory.mb.low    0.90\ncascading.group.comparator.size 3\nipc.server.listen.queue.size    128\ngroup.name  common\nmapred.inmem.merge.threshold    0\njob.end.retry.interval  30000\nmapred.fairscheduler.smalljob.max.maps  10\nmapred.skip.attempts.to.start.skipping  2\nfs.checkpoint.dir   ${hadoop.tmp.dir}/dfs/namesecondary\nmapred.reduce.tasks 2\nmapred.merge.recordsBeforeProgress  10000\nmapred.userlog.limit.kb 0\nmapred.job.reduce.memory.mb -1\nwebinterface.private.actions    true\nio.sort.spill.percent   0.99\nmapred.job.shuffle.input.buffer.percent 0.80\nmapred.job.name [853276BF02049D394C31880B08C9E6CC/DCB7B555F1FC65C767B8E2CD716607AA] copyr/(1/1) /user/jdavis/ctest/end\nmapred.map.tasks.speculative.execution  false\nhadoop.util.hash.type   murmur\nmapred.map.max.attempts 4\nmapreduce.job.acl-view-job\n\nmapred.job.tracker.handler.count    10\nmapred.input.format.class   cascading.tap.hadoop.io.MultiInputFormat\nmapred.tasktracker.expiry.interval  600000\nmapred.jobtracker.maxtasks.per.job  -1\nmapred.jobtracker.job.history.block.size    3145728\nkeep.failed.task.files  false\nmapred.output.format.class  org.apache.hadoop.mapred.TextOutputFormat\nipc.client.tcpnodelay   false\nmapred.task.profile.reduces 0-2\nmapred.output.compression.codec org.apache.hadoop.io.compress.DefaultCodec\nio.map.index.skip   0\nmapred.working.dir  /user/jdavis\nipc.server.tcpnodelay   false\nhadoop.proxyuser.root.hosts \nmapred.reducer.class    cascading.flow.hadoop.FlowReducer\ncascading.app.id    A593B4669179BB6F06771249E7ADFA48\nmapred.used.genericoptionsparser    true\njobclient.progress.monitor.poll.interval    1000\nmapreduce.tasktracker.jvm.idle.time 10000\nmapred.job.map.memory.mb    -1\nhadoop.logfile.size 10000000\nmapred.reduce.tasks.speculative.execution   false\nmapreduce.job.dir   maprfs:/var/mapr/cluster/mapred/jobTracker/staging/jdavis/.staging/job_201210022148_0086\nmapreduce.tasktracker.outofband.heartbeat   true\nmapreduce.reduce.input.limit    -1\nmapred.tasktracker.ephemeral.tasks.ulimit   4294967296&gt;\nfs.s3n.block.size   67108864\nfs.inmemory.size.mb 200\nmapred.fairscheduler.smalljob.max.reducers  10\nhadoop.security.authentication  simple\nfs.checkpoint.period    3600\ncascading.flow.step.id  DCB7B555F1FC65C767B8E2CD716607AA\nmapred.job.reuse.jvm.num.tasks  -1\nmapred.jobtracker.completeuserjobs.maximum  5\nmapreduce.cluster.map.userlog.retain-size   10485760\nmapred.task.tracker.task-controller org.apache.hadoop.mapred.LinuxTaskController\nmapred.output.key.comparator.class  cascading.tuple.hadoop.util.GroupingSortingComparator\nfs.s3.maxRetries    4\nmapred.cluster.max.map.memory.mb    -1\nmapred.mapoutput.value.class    cascading.tuple.Tuple\nmapred.map.child.java.opts  -XX:ErrorFile=/opt/cores/mapreduce_java_error%p.log\nmapred.job.tracker.history.completed.location   /var/mapr/cluster/mapred/jobTracker/history/done\nmapred.local.dir    /tmp/mapr-hadoop/mapred/local\nfs.hftp.impl    org.apache.hadoop.hdfs.HftpFileSystem\nfs.trash.interval   0\nfs.s3.sleepTimeSeconds  10\nmapred.submit.replication   10\nfs.har.impl org.apache.hadoop.fs.HarFileSystem\nmapreduce.heartbeat.10  300\ncascading.version   Concurrent, Inc - Cascading 2.0.5\nmapred.map.output.compression.codec org.apache.hadoop.io.compress.DefaultCodec\nmapred.tasktracker.dns.interface    default\nhadoop.proxyuser.root.groups    root\nmapred.job.tracker  maprfs:///\nmapreduce.job.submithost    c10-m001.wowrack.upstream.priv\nmapreduce.tasktracker.cache.local.numberdirectories 10000\nio.seqfile.sorter.recordlimit   1000000\nmapreduce.heartbeat.10000   100000\nmapred.line.input.format.linespermap    1\nmapred.jobtracker.taskScheduler org.apache.hadoop.mapred.FairScheduler\nmapred.tasktracker.instrumentation  org.apache.hadoop.mapred.TaskTrackerMetricsInst\nmapred.tasktracker.taskmemorymanager.killtask.maxRSS    false\nmapred.child.taskset    true\njobclient.completion.poll.interval  5000\nmapred.fairscheduler.smalljob.max.reducer.inputsize 1073741824\nmapred.local.dir.minspacekill   0\nio.sort.record.percent  0.28\nmapr.localspill.dir spill\nio.compression.codec.lzo.class  com.hadoop.compression.lzo.LzoCodec\nfs.kfs.impl org.apache.hadoop.fs.kfs.KosmosFileSystem\nmapred.tasktracker.reduce.tasks.maximum (CPUS &gt; 2) ? (CPUS * 0.70): 1\nmapred.temp.dir ${hadoop.tmp.dir}/mapred/temp\nmapred.tasktracker.ephemeral.tasks.maximum  1\nfs.checkpoint.edits.dir ${fs.checkpoint.dir}\nmapred.tasktracker.tasks.sleeptime-before-sigkill   5000\nmapred.job.reduce.input.buffer.percent  0.0\nmapred.tasktracker.indexcache.mb    10\nmapreduce.task.classpath.user.precedence    false\nmapreduce.job.split.metainfo.maxsize    -1\nhadoop.logfile.count    10\nfs.automatic.close  true\nmapred.skip.reduce.auto.incr.proc.count true\nmapreduce.job.submithostaddress 10.100.0.99\nmapred.child.oom_adj    10\nio.seqfile.compress.blocksize   1000000\nfs.s3.block.size    67108864\nmapred.tasktracker.taskmemorymanager.monitoring-interval    3000\nmapreduce.tasktracker.volume.healthcheck.interval   60000\nmapred.cluster.ephemeral.tasks.memory.limit.mb  200\nmapreduce.jobtracker.staging.root.dir   /var/mapr/cluster/mapred/jobTracker/staging\nmapred.acls.enabled false\nmapred.queue.default.state  RUNNING\nmapred.fairscheduler.smalljob.schedule.enable   false\nmapred.queue.names  default\nfs.hsftp.impl   org.apache.hadoop.hdfs.HsftpFileSystem\nmapred.fairscheduler.eventlog.enabled   false\nmapreduce.jobtracker.recovery.maxtime   480\nmapred.task.tracker.http.address    0.0.0.0:50060\nmapreduce.jobtracker.inline.setup.cleanup   false\nmapred.reduce.parallel.copies   40\nio.seqfile.lazydecompress   true\nmapred.tasktracker.ephemeral.tasks.timeout  10000\nmapred.output.dir   maprfs:/user/jdavis/ctest/end\nmapreduce.tasktracker.group root\nhadoop.workaround.non.threadsafe.getpwuid   false\nio.sort.mb  512\nmapred.reduce.child.java.opts   -Xmx12000m\nipc.client.connection.maxidletime   10000\nmapred.compress.map.output  false\nhadoop.security.uid.cache.secs  14400\nmapred.task.tracker.report.address  127.0.0.1:0\nmapred.healthChecker.interval   60000\nipc.client.kill.max 10\nipc.client.connect.max.retries  10\nfs.http.impl    cascading.tap.hadoop.io.HttpFileSystem\nfs.s3.impl  org.apache.hadoop.fs.s3.S3FileSystem\nmapred.fairscheduler.assignmultiple true\nmapred.user.jobconf.limit   5242880\nmapred.input.dir    maprfs:/user/jdavis/ctest/mid\nmapred.job.tracker.http.address 0.0.0.0:50030\nio.file.buffer.size 131072\nmapred.jobtracker.restart.recover   true\nio.serializations   cascading.tuple.hadoop.TupleSerialization,org.apache.hadoop.io.serializer.WritableSerialization\nmapreduce.use.fastreduce    false\nmapred.reduce.copy.backoff  300\nmapred.task.profile false\nmapred.jobtracker.retiredjobs.cache.size    300\njobclient.output.filter FAILED\nmapred.tasktracker.map.tasks.maximum    (CPUS &gt; 2) ? (CPUS * 0.80) : 1\nio.compression.codecs   org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec\nfs.checkpoint.size  67108864\ncascading.sort.comparator.size  3\n</code></pre>\n\n<hr>\n\n<pre><code>2012-10-02 19:30:50,676 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processName=SHUFFLE, sessionId=\n2012-10-02 19:30:50,737 INFO org.apache.hadoop.mapreduce.util.ProcessTree: setsid exited with exit code 0\n2012-10-02 19:30:50,742 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: /proc/&lt;pid&gt;/status does not have information about swap space used(VmSwap). Can not track swap usage of a task.\n2012-10-02 19:30:50,742 INFO org.apache.hadoop.mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.mapreduce.util.LinuxResourceCalculatorPlugin@27b62aab\n2012-10-02 19:30:50,903 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 9115 may have finished in the interim.\n2012-10-02 19:31:01,663 INFO org.apache.hadoop.mapred.Merger: Merging 37 sorted segments\n2012-10-02 19:31:01,672 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 36 segments left of total size: 1204882102 bytes\n2012-10-02 19:31:03,079 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 7596 may have finished in the interim.\n2012-10-02 19:31:15,487 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 4803 may have finished in the interim.\n2012-10-02 19:31:15,489 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 11069 may have finished in the interim.\n2012-10-02 19:33:37,821 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 20846 may have finished in the interim.\n2012-10-02 19:33:59,274 INFO org.apache.hadoop.mapred.Merger: Merging 35 sorted segments\n2012-10-02 19:33:59,275 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 35 segments left of total size: 1176895576 bytes\n2012-10-02 19:34:02,131 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 21791 may have finished in the interim.\n2012-10-02 19:34:29,927 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 22847 may have finished in the interim.\n2012-10-02 19:36:32,181 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 30438 may have finished in the interim.\n2012-10-02 19:37:18,243 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 3852 may have finished in the interim.\n2012-10-02 19:37:26,292 INFO org.apache.hadoop.mapred.Merger: Merging 37 sorted segments\n2012-10-02 19:37:26,293 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 37 segments left of total size: 1233203028 bytes\n2012-10-02 19:39:07,695 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 9813 may have finished in the interim.\n2012-10-02 19:39:10,764 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 10045 may have finished in the interim.\n2012-10-02 19:39:56,829 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 17383 may have finished in the interim.\n2012-10-02 19:40:18,295 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 19584 may have finished in the interim.\n2012-10-02 19:40:32,307 INFO org.apache.hadoop.mapred.Merger: Merging 58 sorted segments\n2012-10-02 19:40:32,308 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 44 segments left of total size: 1206978885 bytes\n2012-10-02 19:41:35,154 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 26361 may have finished in the interim.\n2012-10-02 19:43:53,644 INFO org.apache.hadoop.mapred.Merger: Merging 56 sorted segments\n2012-10-02 19:43:53,645 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 56 segments left of total size: 1217287352 bytes\n2012-10-02 19:46:55,246 INFO org.apache.hadoop.mapred.Merger: Merging 44 sorted segments\n2012-10-02 19:46:55,246 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 44 segments left of total size: 1221163604 bytes\n2012-10-02 19:49:57,894 INFO org.apache.hadoop.mapred.Merger: Merging 85 sorted segments\n2012-10-02 19:49:57,895 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 62 segments left of total size: 1229975233 bytes\n2012-10-02 19:52:09,914 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 25247 may have finished in the interim.\n2012-10-02 19:52:52,620 INFO org.apache.hadoop.mapred.Merger: Merging 1 sorted segments\n2012-10-02 19:52:52,620 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32065409 bytes\n2012-10-02 19:52:53,327 INFO org.apache.hadoop.mapred.Merger: Merging 8 sorted segments\n2012-10-02 19:52:53,345 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 8 segments left of total size: 8522450575 bytes\n2012-10-02 19:52:53,366 INFO cascading.flow.hadoop.FlowReducer: cascading version: Concurrent, Inc - Cascading 2.0.5\n</code></pre>\n"},{"tags":["mysql","performance","perl","security","cgi"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":68,"score":0,"question_id":13043648,"title":"Perl: Handing $dbh off to module. Security and performance?","body":"<p>So I have a cgi script,</p>\n\n<pre><code>#!/usr/bin/perl -T\n\nuse strict;\nuse warnings;\nuse DBI;\nuse WebEngine;\n\n\nmy $dbh = DBI-&gt;connect('DBI:mysql:database', $username, $password)\n    || die \"Could not connect to database: $DBI::errstr\";\n\nmy $we = WebEngine-&gt;new($dbh)\n    or die(\"Failed to instantiate WebEngine object:\\n$!\\n\");\n\n$userID = $we-&gt;register(\"MyUsername\", $dbh);\n</code></pre>\n\n<p>This script creates a database handler and then uses a module I made to deal with most of the back-end of the site to register a username and return a userID number.</p>\n\n<p>I have three questions about this.</p>\n\n<ul>\n<li><p>Does creating this $dbh in this script increase performance by keeping a database connection open?  </p></li>\n<li><p>Could I put the $dbh in my module and not fear being inefficient? </p></li>\n<li><p>Is there a security benefit to keeping the $dbh (and the associated info(I keep my pass in plain text in the code; is that bad?)) in the module that is not directly interacted with through my website?</p></li>\n</ul>\n"},{"tags":["performance","algorithm","puzzle"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":97,"score":1,"question_id":13037826,"title":"brute forcing a brain puzzle","body":"<p>I was given a brain puzzle from lonpos.cc as a present. I was curius of how many different solutions there were, and I quite enjoy writing algorithms and code, so I started writing an application to brute force it.</p>\n\n<p>The puzzle looks like this : <a href=\"http://www.lonpos.cc/images/LONPOSdb.jpg\" rel=\"nofollow\">http://www.lonpos.cc/images/LONPOSdb.jpg</a> / <a href=\"http://cdn100.iofferphoto.com/img/item/191/498/944/u2t6.jpg\" rel=\"nofollow\">http://cdn100.iofferphoto.com/img/item/191/498/944/u2t6.jpg</a></p>\n\n<p>It's a board of 20x14 \"points\". And all puzzle pieces can be flipped and turned. I wrote an application where each piece (and the puzzle) is presented like this:</p>\n\n<pre><code>01010\n00100\n01110\n01110\n11111\n01010\n</code></pre>\n\n<p>Now my application so far is reasonably simple.</p>\n\n<p>It takes the list of pieces and a blank board, pops of piece #0\nflips it in every direction, and for that piece tries to place it for every x and y coordinate. If it successfully places a piece it passes a copy of the new \"board\" with some pieces taken to a recursive function, and tries all combinations for their pieces.</p>\n\n<p>Explained in pseudocode:</p>\n\n<pre><code>bruteForce(Board base, List pieces) {\n    for (Piece in pieces.pop, piece.pop.flip, piece.pop.flip2...) {\n        int x,y = 0;\n        if canplace(piece, x, y) {\n            Board newBoard = base.clone();\n            newBoard.placePiece(piece, x, y);\n            bruteForce(newBoard, pieces);\n        }\n        ## increment x until x &gt; width, then y\n    }\n}\n</code></pre>\n\n<p>Now I'm trying to find out ways to make this quicker. Things I've thought of so far:</p>\n\n<ol>\n<li>Making it solve in parallel - Implemented, now using 4 threads.</li>\n<li>Sorting the pieces, and only trying to place the pieces that will fit in the x,y space we're trying to fit. (Aka if we're on the bottom row, and we only have 4 \"points\" from our position to the bottom, dont try the ones that are 8 high).</li>\n<li>Not duplicating the board, instead using placePiece and removePiece or something like it.</li>\n<li>Checking for \"invalid\" boards, aka if a piece is impossible to reach (boxed in completely).</li>\n</ol>\n\n<p>Anyone have any creative ideas on how I can do this quicker? Or any way to mathematically calculate how many different combinations there are? </p>\n"},{"tags":["php","sql","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":49,"score":1,"question_id":13044162,"title":"SQL Efficiency and speed","body":"<p>I have to delete records that contain the id in an array.</p>\n\n<p>I just don't know which is better to use. Which is faster and more efficient?</p>\n\n<p><strong>code 1</strong></p>\n\n<pre><code>$array = array('123','456','789' ...)//over 900 entries\n$id = implode(',', $array);\n$sql = 'DELETE FROM email WHERE id IN ('.$id.')';\n//execute sql\n</code></pre>\n\n<p>or</p>\n\n<p><strong>code2</strong></p>\n\n<pre><code>$array = array('123','456','789' ...)//over 900 entries\nfor($x=0;$x&lt;count($array);$x++){\n  $sql='DELETE FROM email WHERE id = '.$array[$x].' ';\n  //execute sql\n}\n</code></pre>\n\n<p>Which from the two is faster and more efficient when executed?</p>\n"},{"tags":["performance","sockets","web-applications"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":13044133,"title":"Is Socket Programing in combination with Web Application, Good?","body":"<p>I have to develop a Service which is to interact with a Server Socket at remote for activating a subscriber request, i have achieved the it with help of plain socket programming. now, my Service is to be available for other application to initiate the requests sent by the subscriber, i wanted to create a web application for this so that using a URL hit they can initiate the request. but, i m under perception that web application in combination with Socket programming is not that good, my concern is with performance as of now.</p>\n\n<p>Would request to guide me a proper way so as to make my service available either as web application or a Standalone app.</p>\n"},{"tags":["performance","optimization","profiling"],"answer_count":17,"favorite_count":17,"up_vote_count":38,"down_vote_count":16,"view_count":12395,"score":22,"question_id":266373,"title":"One could use a profiler, but why not just halt the program?","body":"<p>If something is making a single-thread program take, say, 10 times as long as it should, you could run a profiler on it. You could also just halt it with a \"pause\" button, and you'll see exactly what it's doing. </p>\n\n<p>Even if it's only 10% slower than it should be, if you halt it more times, before long you'll see it repeatedly doing the unnecessary thing. Usually the problem is a function call somewhere in the middle of the stack that isn't really needed. This doesn't measure the problem, but it sure does find it.</p>\n\n<p>Edit: The objections mostly assume that you only take 1 sample. If you're serious, take 10. Any line of code causing some percentage of wastage, like 40%, will appear on the stack on that fraction of samples, on average. Bottlenecks (in single-thread code) can't hide from it.</p>\n\n<p>EDIT: To show what I mean, many objections are of the form \"there aren't enough samples, so what you see could be entirely spurious\" - vague ideas about chance. But if something of <em>any recognizable description</em>, not just being in a routine or the routine being active, is in effect for 30% of the time, then the probability of seeing it on any given sample is 30%. </p>\n\n<p>Then suppose only 10 samples are taken. The number of times the problem will be seen in 10 samples follows a <a href=\"http://en.wikipedia.org/wiki/Binomial_distribution\" rel=\"nofollow\">binomial distribution</a>, and the probability of seeing it 0 times is .028. The probability of seeing it 1 time is .121. For 2 times, the probability is .233, and for 3 times it is .267, after which it falls off. Since the probability of seeing it less than two times is .028 + .121 = .139, that means the probability of seeing it two or more times is 1 - .139 = .861. The general rule is if you see something you could fix on two or more samples, it is worth fixing. </p>\n\n<p>In this case, the chance of seeing it in 10 samples is 86%. If you're in the 14% who don't see it, just take more samples until you do. (If the number of samples is increased to 20, the chance of seeing it two or more times increases to more than 99%.) So it hasn't been precisely measured, but it has been precisely found, and it's important to understand that it could easily be something that a profiler could not actually find, such as something involving the state of the data, not the program counter.</p>\n"},{"tags":["networking","network-programming","performance","ping"],"answer_count":7,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":2948,"score":1,"question_id":4575537,"title":"Calculate upload/download speed by ping","body":"<p>How to calculate the speed of an internet connection by some average ping rates.What are the calculations involved in it.IS it possible to calculate upload/download limit by ping rate</p>\n\n<p><strong>EDIT</strong>\nIf ping is not a solution what else is?</p>\n"},{"tags":["c#","performance","math","matrix","usability"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":88,"score":0,"question_id":13035464,"title":"Fast and useful way of storing matrix values in C#","body":"<p>I need to create a 4x4 matrix class for a 3D engine in C#.\nI have seen some other engines storing the matrix values in single float member variables / fields like this:</p>\n\n<pre><code>float m11, m12, m13, m14\nfloat m21, m22, m23, m24\nfloat m31, m32, m33, m34\nfloat m41, m42, m43, m44\n</code></pre>\n\n<p>However, I thought storing them in a two dimensional array would be more useful for transformations / calculations with the matrices:</p>\n\n<pre><code>float[4][4];\n</code></pre>\n\n<p>I also thought of a one dimensional array - but it looks less self explanatory and wouldn't give me an advantage over the first option:</p>\n\n<pre><code>float[16];\n</code></pre>\n\n<p>In C++, I always used the \"union\" keyword to have all of the above storing possibilites at once. However, C# does not seem to have this keyword, so I have to decide which one I want to use.</p>\n\n<p>What is the fastest way of storing the 4x4 matrix when applying transformations etc.?\nWhich option would you choose when thinking about usability?</p>\n"},{"tags":["c++","c","performance","math","sin"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":154,"score":3,"question_id":13042110,"title":"Create a Fast Sin() function to improve fps ? Fast sin() function?","body":"<p>I am rendering 500x500 points in real-time. \nI have to compute the position of points using atan() and sin() functions. By using atan() and sin() I am getting <strong>24 fps</strong> (frames per second).</p>\n\n<pre><code>float thetaC = atan(value);\nfloat h = (value) / (sin(thetaC)));\n</code></pre>\n\n<p>If I don't use sin() I am getting <strong>52 fps</strong>.</p>\n\n<p>and if I dont use atan() I am <strong>30 fps</strong>.</p>\n\n<p>So, the big problem is with sin(). How can I use Fast Sin version. Can I create a Look Up Table for that ? I don't have any specific values to create LUT. what can I do in this situation ? </p>\n\n<p><em><strong>PS: I have also tried fast sin function of ASM but not getting any difference.</em></strong></p>\n\n<p>Thanks.</p>\n"},{"tags":["performance","time-complexity","asymptotic-complexity"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":90,"score":-1,"question_id":13040950,"title":"Time complexity, binary (search) tree","body":"<p>assume I have a complete binary tree up-to a certain depth <em>d</em>. What would the time complexity be to traverse (pre-order traversal) this tree.</p>\n\n<p>I am confused because I know that the amount of nodes in the tree is 2^d, so therefore the time complexity would be <code>BigO(2^d)</code> ? because the tree is growing exponentially. </p>\n\n<p>But, upon research on the internet, Everyone states that's traversal is <code>BigO(n)</code> where n is the number of elements (which would be <code>2^d</code> in this case), not <code>BigO(2^d)</code>, what am I missing?</p>\n\n<p>thanks  </p>\n"},{"tags":["sql-server","performance","sql-server-2000","time-series"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":58,"score":1,"question_id":13037557,"title":"SQL Server : time-series data performance","body":"<p>I have a table of a little over 1 billion rows of time-series data with fantastic insert performance but (sometimes) awful select performance.</p>\n\n<p>Table <code>tblTrendDetails</code> (PK is ordered as shown):</p>\n\n<pre><code>PK  TrendTime    datetime\nPK  CavityId     int\nPK  TrendValueId int\n    TrendValue   real\n</code></pre>\n\n<p>The table is continuously pulling in new data and purging old data, so insert and delete performance needs to remain snappy.</p>\n\n<p>When executing a query such as the following, performance is poor (30 sec):</p>\n\n<pre><code>SELECT * \nFROM tblTrendDetails\nWHERE TrendTime BETWEEN @inMinTime AND @inMaxTime\n  AND CavityId = @inCavityId\n  AND TrendValueId = @inTrendId\n</code></pre>\n\n<p>If I execute the same query again (with similar times, but any <code>@inCavityId</code> or <code>@inTrendId</code>), performance is very good (1 sec). Performance counters show that disk access is the culprit the first time the query is run.  </p>\n\n<p>Any recommendations regarding how to improve performance without (significantly) adversely affecting the insert or delete performance?  Any suggestions (including completely changing the underlying database) are welcome.</p>\n"},{"tags":["java","performance","spring","rest","scalability"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":156,"score":1,"question_id":12787275,"title":"Tomcat Performance with Huge String Responses","body":"<p>I've a springRest web service endpoint that returns a string text of size 4MB. As we do load test of this endpoint we constantly see heap spikes and ultimately the system crashes. I'm thinking - as we make requests each request is serviced by a thread separately. My hypothesis is: Because the string is saved in a global static variable, each thread takes a copy of the 4MB and after around 3000 requests the heap is all consumed and the system crashes because 3000 threads taking each 4MB is around 12GB and hence the system goes out of memory. But this is my hypothesis.</p>\n\n<p>My question: doesn't tomcat reclaim the memory after each thread that processes a request has done it's job?\nIs this related to GC (garbage collection)?\nIn the request life cycle - as a request comes, a thread is created (per that request) does the thread get it's own copy of the response or it just references the response? if that huge string response is copied to each thread then may be that's why the heap spike is showing.\nWhen the response is given back to the client how does tomcat reclaim the resources of that thread? when does it do it? is claiming request threads related to GC?</p>\n\n<p>Another aspect that i observed is: delay on the method socketWrite0() - this takes from 70-95% of the response time. It is a bottle neck i think. So in the flow of request response - who writes to the socket? the thread? or the thread hands the response to tomcat and tomcat writes it?</p>\n\n<p>If any of you could give me a hint or an aspect to look at that relates memory spikes with huge string responses, i'd really appreciate it. thanks guys!</p>\n\n<p>rose</p>\n"},{"tags":["java","performance","reflection","interface"],"answer_count":3,"favorite_count":0,"up_vote_count":7,"down_vote_count":1,"view_count":4633,"score":6,"question_id":1122506,"title":"Interpreting Java reflection performance: Why is it surprisingly very fast?","body":"<p>I've seen other threads saying java reflection performance is 10-100x slower than when using non-reflection calls.</p>\n\n<p>My tests in 1.6 have shown that this is not the case but I found some other interesting things that I need someone to explain to me.</p>\n\n<p>I have objects that implement my interface.  I did three things 1) using a reference to an Object I cast that object to the interface and call the method through the interface 2) using a reference to the actual object call the method directly and 3) call the method through reflection.  I saw that #1 interface call was fastest followed closely by #3 reflection but I noticed that the direct method call was the slowest by a good margin.</p>\n\n<p>I don't understand that, I would've expected the direct call to be fastest, then the interface, then reflection would be much much more slow.</p>\n\n<p>Blah and ComplexClass are in a different package from the main class and both have a doSomething(int x) method that implements the interface and just prints the integer x.</p>\n\n<p>Here are my results (times in ms, results very similar w/ multiple trials):\ncalling a method directly: 107194\ncalling a method directly from an object cast to an interface: 89594\ncalling a method through reflection: 90453</p>\n\n<p>Here is my code:</p>\n\n<pre><code>public class Main\n{\n\n    /**\n     * @param args the command line arguments\n     */\n    public static void main(String[] args)\n    {\n        Blah x = new Blah();\n        ComplexClass cc = new ComplexClass();\n        test((Object) x, cc);\n    }\n\n    public static void test(Object x, ComplexClass cc)\n    {\n        long start, end;\n        long time1, time2, time3 = 0;\n        int numToDo = 1000000;\n        MyInterface interfaceClass = (MyInterface) x;\n\n        //warming up the cache\n        for (int i = 0; i &lt; numToDo; i++)\n        {\n            cc.doSomething(i); //calls a method directly\n        }\n\n        start = System.currentTimeMillis();\n        for (int i = 0; i &lt; numToDo; i++)\n        {\n            cc.doSomething(i); //calls a method directly\n        }\n        end = System.currentTimeMillis();\n        time1 = end - start;\n\n        start = System.currentTimeMillis();\n        for (int i = 0; i &lt; numToDo; i++)\n        {\n            interfaceClass.doSomething(i); //casts an object to an interface then calls the method\n        }\n        end = System.currentTimeMillis();\n        time2 = end - start;\n\n\n        try\n        {\n            Class xClass = x.getClass();\n            Class[] argTypes =\n            {\n                int.class\n            };\n            Method m = xClass.getMethod(\"doSomething\", argTypes);\n            Object[] paramList = new Object[1];\n            start = System.currentTimeMillis();\n            for (int i = 0; i &lt; numToDo; i++)\n            {\n                paramList[0] = i;\n                m.invoke(x, paramList); //calls via reflection\n            }\n            end = System.currentTimeMillis();\n            time3 = end - start;\n\n        } catch (Exception ex)\n        {\n        }\n\n        System.out.println(\"calling a method directly: \" + time1);\n        System.out.println(\"calling a method directly from an object cast to an interface: \" + time2);\n        System.out.println(\"calling a method through reflection: \" + time3);\n    }\n</code></pre>\n"},{"tags":["linux","performance","libevent","c10k"],"answer_count":7,"favorite_count":9,"up_vote_count":31,"down_vote_count":0,"view_count":2386,"score":31,"question_id":3129608,"title":"Is there any modern review of solutions to the 10000 client/sec problem","body":"<p>(Commonly called the C10K problem)</p>\n\n<p>Is there a more contemporary review of solutions to the <a href=\"http://www.kegel.com/c10k.html\" rel=\"nofollow\">c10k</a> problem (Last updated: 2 Sept 2006), specifically focused on Linux (epoll, signalfd, eventfd, timerfd..) and libraries like libev or libevent?</p>\n\n<p>Something that discusses all the solved and still unsolved issues on a modern Linux server?</p>\n"},{"tags":["asp.net","performance","security","https","asp.net-mvc-4"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":45,"score":0,"question_id":13040442,"title":"implications of having a site in full https","body":"<p>I am currently developing an MVC4 web application for eCommerce. The site will contain a login and users can visit the site, input their details and submit orders etc. This is a traditional eCommerce site.</p>\n\n<p>To boost the security of the site, I am looking to set up the entire site in https. As the user will be supplying their log in credentials and storing personal information in cookies, I would like the site to be fully secured. </p>\n\n<p>I have concerns though, these being if I set up the site in https, will it detriment performance? Will it impact negatively on search engine optimization? Are there any other implications of having an entire site in https?</p>\n\n<p>I use output caching to cache the content of my views - with https will these still get cached?</p>\n\n<p>I have been reviewing security guidelines and documentation, such as <a href=\"http://asafaweb.com/OWASP%20Top%2010%20for%20.NET%20developers.pdf\" rel=\"nofollow\">this</a> from OWASP and they recommend this. Also, I see that sites such as twitter are fully https.</p>\n"},{"tags":["android","performance","quicksearch"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":204,"score":1,"question_id":6499347,"title":"Populating Quick Search Box via a web service","body":"<p>I am thinking of supporting Quick Search Box in my Android app.  I would take what the user has typed and run a query on a remote server via web services.  Is this pattern efficient enough or is it expected that any data I am querying should be displayed from locally accessible data?</p>\n"},{"tags":["performance","uiimageview","uicollectionview"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":105,"score":0,"question_id":12994964,"title":"Making UICollectionView With Images From URLs More Efficient","body":"<p>I have a UICollection View with cells that contain images from the web. I use the EGOImageView class to load the images in the background, however with really large images there is about a half a second freeze on the iPhone and on the iPad it is very noticeable. Is there anything I can do to make it more efficient? Is there a way to download a compressed version of large images?</p>\n"},{"tags":["sql","performance","oracle","plsql","data-warehouse"],"answer_count":1,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":108,"score":5,"question_id":12991346,"title":"Refresh strategy for materialized views in a data warehouse","body":"<p>I have a system that has a materialized view that contains roughly 1 billion items, on a consistent two hour basis I need to update about 200 million (20% of the records).  My question is what should the refresh strategy on my materialized view be?  As of right now it is refresh with an interval.  I am curious as to the performance impacts between refreshing on an interval vice refresh never and rename/replace the old materialized view with the new one.  The underlying issue is the indices that are used by Oracle which creates a massive amount of redo.  Any suggestions are appreciated.</p>\n\n<p><strong>UPDATE</strong><br>\nSince some people seem to think this is off topic my current view point is to do the following:  </p>\n\n<p>Create an Oracle Schedule Chain that invokes a series of PL/SQL (programming language I promise) functions to refresh materialized view in a pseudo-parallel fashion.    However, being as though I fell into the position of a DBA of sorts, I am looking to solve a data problem with an algorithm and/or some code.</p>\n"},{"tags":["performance","apache","caching","memcached","centos"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":33,"score":0,"question_id":13039691,"title":"Google PageSpeed - Setting an expiry date","body":"<p>I'm using a CentOS 5 32bit and I've just scanned my site on Google for page speed and it gave me the following:</p>\n\n<p>\"Setting an expiry date or a maximum age in the HTTP headers for static resources instructs the browser to load previously downloaded resources from local disk rather than over the network.\"</p>\n\n<p>Can someone please let me know how can I enable this within my Apache server?</p>\n"},{"tags":["asp.net","performance","viewstate"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":1690,"score":3,"question_id":600919,"title":"Running ASP.NET without viewstate turned on","body":"<p>We're about to start rebuilding one of our ASP.NET projects and I would like to try developing it without viestate turned on (disabled in web.config).</p>\n\n<p>I know about the upsides and downsides of viewstate and generally speaking what it keeps track of in comparison to control state, however I would like to know:</p>\n\n<ol>\n<li><p>What are the principle development process differences? Ie how differently do you structure your Page_Load etc?</p></li>\n<li><p>Is there any functionality in the standard ASP.NET controls that really will just not work without viewstate turned on?</p></li>\n</ol>\n\n<p>Also, are there any detailed articles on the workflow differences between working with and without VS?</p>\n"},{"tags":["python","linux","performance","osx","benchmarking"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":86,"score":0,"question_id":13038745,"title":"Speed of Python on Linux vs Mac","body":"<p>Out of curiosity: If I run a program on MacOS 10.8 (Python 2.7.2) it takes 21 seconds to finish. The same code running under a virtual Linux machine on the same computer takes 13 seconds (Python 2.7.3).</p>\n\n<p>Anyone knows why? I'm not asking about this specific code - why does it happen at all? I would expect the speed to be roughly the same as it's the same computer or a bit slower on Linux as it runs virtualized...</p>\n\n<p>Edit: the code calculates some statistics over data in RAM. Think of calculating the mean of some integers or similar.</p>\n\n<p>Edit 2:</p>\n\n<pre><code>from time import time\nt = time()\nx = 0\nl = list(range(1000000))\nr = list(range(1000000))\nfor i in range(100):\n    l = map(lambda a,b: a+b, l, r)\n    x = x+sum(l)\nprint time()-t\n</code></pre>\n\n<p>MacOS: 22s, Linux VM: 15s..?</p>\n"},{"tags":["c","performance","optimization","data-structures","computational-geometry"],"answer_count":5,"favorite_count":1,"up_vote_count":8,"down_vote_count":0,"view_count":1097,"score":8,"question_id":78045,"title":"Spatial Data Structures in C","body":"<p>I do work in theoretical chemistry on a high performance cluster, often involving molecular dynamics simulations.  One of the problems my work addresses involves a static field of N-dimensional (typically N = 2-5) hyper-spheres, that a test particle may collide with.  I'm looking to optimize (read: overhaul) the the data structure I use for representing the field of spheres so I can do rapid collision detection.  Currently I use a dead simple array of pointers to an N-membered struct (doubles for each coordinate of the center) and a nearest-neighbor list.  I've heard of oct- and quad- trees but haven't found a clear explanation of how they work, how to efficiently implement one, or how to then do fast collision detection with one.  Given the size of my simulations, memory is (almost) no object, but cycles are.</p>\n"},{"tags":["performance","sql-server-2008-r2","recoverymodel"],"answer_count":0,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":27,"score":0,"question_id":13038686,"title":"Performance slow down in SQL Server 2008 R2 after setting reovery to simple","body":"<p>I have a SQL Server 2008 R2 database which applications write to it intensively (I guess not less than 20 write operations per second).</p>\n\n<p>It was 3 months ago when I changed my recovery model to <code>Simple</code>. But after that the database becomes slower and slower. </p>\n\n<p>Now I set the recovery model to <code>Full</code>. But can I get the performance gain right now? Or do I have to wait till the Log file grows once again to reserve enough space to work in? </p>\n\n<p>I am not even sure why setting my recovery mode to  <code>Simple</code> reduces my performance (I am thankful if someone enlighten me on this too). </p>\n\n<p>Thanks a lot</p>\n"},{"tags":["php","performance","file-rename"],"answer_count":1,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":299,"score":7,"question_id":12944728,"title":"Renaming a 900kb pdf file takes long time","body":"<p>I am trying to <code>rename()</code> a 900 KiB PDF file in PHP. It is taking a long time to rename it for some reason. I thought it should be instant.</p>\n\n<p>This is on a CentOS server. While the file is being renamed I can get properties and it seems like <code>rename()</code> is copying and replacing the old file with new renamed file.</p>\n\n<p>The old name and new name paths are in the same directory.</p>\n\n<p>Has anyone stumbled upon this issue before?</p>\n\n<p><hr /></p>\n\n<h2>Code:</h2>\n\n<pre><code>    //If exists change name and then return path\n    $pieces = explode(\"@\", $filename);\n    $newName = $pieces[0].' '.$pieces[2];\n\n    rename($uidPath.$filename, $uidPath.$newName);\n\n    if (preg_match('/pdf/', $pieces[2]))\n    {\n        $result['status'] = '1';\n        $result['path'] = 'path to file';\n    } \n    else \n    {\n        $result['status'] = '1';\n        $result['path'] = 'path to file';\n    }\n</code></pre>\n"},{"tags":["performance","algorithm","r","index","sets"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":53,"score":2,"question_id":13037560,"title":"What is an efficient way to convert sets to a column index in R?","body":"<p><strong>Overview</strong></p>\n\n<p>Give a large (nrows > 5,000,000+) data frame, <strong>A</strong>, with string row names and a list of disjoint sets (n = 20,000+), <strong>B</strong>, where each set consists of row names from <strong>A</strong>, what is the best way to create a vector representing the sets in <strong>B</strong> via a unique value?</p>\n\n<p><strong>Illustration</strong></p>\n\n<p>Below is an example illustrating this problem:</p>\n\n<pre><code># Input\nA &lt;- data.frame(d = rep(\"A\", 5e6), row.names = as.character(sample(1:5e6)))\nB &lt;- list(c(\"4655297\", \"3177816\", \"3328423\"), c(\"2911946\", \"2829484\"), ...) # Size 20,000+\n</code></pre>\n\n<p>The desired result would be:</p>\n\n<pre><code># An index of NA represents that the row is not part of any set in B.\n&gt; A[,\"index\", drop = F]\n        d index\n4655297 A     1\n3328423 A     1\n2911946 A     2\n2829484 A     2\n3871770 A    NA\n2702914 A    NA\n2581677 A    NA\n4106410 A    NA\n3755846 A    NA\n3177816 A     1\n</code></pre>\n\n<p><strong>Naive Attempt</strong></p>\n\n<p>Something like this can be achieved using the following method.</p>\n\n<pre><code>n &lt;- 0\nA$index &lt;- NA\nlapply(B, function(x){\n  n &lt;&lt;- n + 1\n  A[x, \"index\"] &lt;&lt;- n\n})\n</code></pre>\n\n<p><strong>Problem</strong></p>\n\n<p>However this is unreasonably slow (several hours) due to indexing A multiple times and is not very R-esque or elegant.</p>\n\n<p>How can the desired result be generated in a quick and efficient manner?</p>\n"},{"tags":["c#","performance","linq"],"answer_count":7,"favorite_count":8,"up_vote_count":68,"down_vote_count":0,"view_count":1253,"score":68,"question_id":7499384,"title":"Does the order of LINQ functions matter?","body":"<p>Basically, as the question states... does the order of LINQ functions matter in terms of <b>performance</b>? Obviously the results would have to be identical still...</p>\n\n<p>Example:</p>\n\n<pre><code>myCollection.OrderBy(item =&gt; item.CreatedDate).Where(item =&gt; item.Code &gt; 3);\nmyCollection.Where(item =&gt; item.Code &gt; 3).OrderBy(item =&gt; item.CreatedDate);\n</code></pre>\n\n<p>Both return me the same results, but are in a different LINQ order. I realize that reordering some items will result in different results, and I'm not concerned about those. What my main concern is in knowing if, in getting the same results, ordering can impact performance. And, not just on the 2 LINQ calls I made (OrderBy, Where), but on any LINQ calls.</p>\n"},{"tags":["python","performance","recursion"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":88,"score":3,"question_id":13036272,"title":"python - compare two directories recursively and flag equivalent structure","body":"<p>I have read many questions in stackoverflow, related to python compare directory. However, my current question is bit different.<br>\nI have two directories, which contains two different version release package contents. Now I want to compare to ensure the contents are same. <strong>However</strong> few files have version name embedded into them. Now which is the best possible way to compare them and conclude (except the version difference, all files match).  </p>\n\n<hr>\n\n<p>For example:<br>\nVersion <strong>V1R1C1</strong> contains directory structure as below  </p>\n\n<pre><code>pmt&gt; find . -name \"*\"\n.\n./c1\n./c1/c2\n./c1/c1_V1R1C1.cfg\n./a1\n./a1/a1_V1R1C1.cfg\n./a1/a2\n./a1/a2/a1a2_V1R1C1.cfg\n./b1/a_best_file.txt\n./b1/b2/a_test_file.txt\n./b1/b2/b1b2_V1R1C1.cfg\n./a_V1R1C1.cfg\n</code></pre>\n\n<p>Version <strong>V2R3C1</strong> may contain below structure    </p>\n\n<pre><code>pmt&gt; find . -name \"*\"\n.\n./c1\n./c1/c2\n./c1/c1_V2R3C1.cfg\n./a1\n./a1/a1_V2R3C1.cfg\n./a1/a2\n./a1/a2/a1a2_V2R3C1.cfg\n./b1/a_best_file.txt\n./b1/b2/a_test_file.txt\n./b1/b2/b1b2_V2R3C1.cfg\n./a_V2R3C1.cfg\n</code></pre>\n\n<hr>\n\n<p>In the above case, the program must flag it as <strong>equivalent</strong> structure.  </p>\n\n<p>I can think of few solutions - like for example, read both the directory structure recursively into cache (dict), rip the version information and compare etc. But looks like not a completely effective mechanism because of two reason 1. It does not utilize the inbuilt directory compare 2.The multiple read/rip/compare is bound to cost (especially with huge directory tree structure).  </p>\n\n<p>I am looking for ideas, which are simple and efficient than the one above. </p>\n\n<hr>\n\n<p>PS :<br>\n1. In case of any difference (except the version unlike the above example), I would like to use the left/right etc to get diffed list.<br>\n2. We can assume before hand which is the version name in both directories (like V1R1C1 in first case and V2R3C1 in second case).</p>\n"},{"tags":["performance","orient-db"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":44,"score":1,"question_id":13034247,"title":"OrientDB and mmap (performance issue)","body":"<p>If I iterate over a cluster, using the following code:</p>\n\n<pre><code>for (ODocument document : m_database.browseCluster(clusterName)) {\n    // ...\n}\n</code></pre>\n\n<p>I observe very poor performance, (ca. 10 records per seconds are fetched), but only if sufficient memory is allocated to Java process. If I restrict memory to 64mb - everything runs very fast.</p>\n\n<p>As I can see from profiler, OFileMMap.map is called VERY often in the case when performance is poor. And channel.map(....) consumes most of the time.</p>\n\n<p>If I disable mmap completely by setting file.mmap.strategy=4 - everything is getting fast, but not as fast, as with mmap and restricted memory allocation.</p>\n\n<p>Any ideas? </p>\n"},{"tags":["c++","python","performance","simulation","simpy"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":53,"score":0,"question_id":13035296,"title":"Python (SimPy - Simulation) speed performance","body":"<p>I opt to implement a very simple open-source discrete event simulation package to model enterprise (manufacturing/logistics etc) procedures. As I said I want it to be simple, but expandable. I spent some days in learning about SimPy and I really like the way things are constructed with it (a great tutorial here-> <a href=\"http://heather.cs.ucdavis.edu/~matloff/simcourse.html\" rel=\"nofollow\">http://heather.cs.ucdavis.edu/~matloff/simcourse.html</a>). </p>\n\n<p>Nevertheless my concerns have to do with speed: since python is generally much slower than c and discrete event simulations are notorious for requiring much time and resources, should I expect that if I manage to model big systems with SimPy the performance of my application will be crappy? Are there means of making python work at better speed (eg Numpy and Scipy) and will that be equivalent to the effectiveness of c?</p>\n\n<p>Also is there some other open-source framework (maybe based in c or c++?) that you believe will be better for my needs? (I have done a quick overview by searching in the web, but still I am confused). Of course I can just use c or c++, but that seems too complicated. </p>\n\n<p>Thank you for any insight!</p>\n"},{"tags":["python","multithreading","performance","concurrency","parallel-processing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":69,"score":0,"question_id":13031886,"title":"Python concurrency performance","body":"<p>I'm facing concurrency in Python for the first time with the goal in mind to optimize my script. Basically I have a script which invokes third party jar tools using os.system against some files. My first \"procedural\" version took about 135 seconds to complete, after using threads (<strong>threading.Thread</strong> and <strong>threading.Queue</strong>) 127 and now switching to multiprocess (<strong>multiprocessing.Process</strong> and <strong>multiprocessing.JoinableQueue</strong>) 113 seconds... but it's still a lot of time... could you give me some feedback and/or point me to an article that may solve my problem?</p>\n\n<p>(I'm using Python 2.7.1 and I would like to avoid 3d party modules)</p>\n"},{"tags":["html","css","performance","web"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":51,"score":-1,"question_id":13034407,"title":"Custom fonts on website, why not?","body":"<p>I would like to ask, whether it is advisable, using custom fonts on a website. \nSure, it maybe a nicer optical solution e.g. using SeroWeb-Light instead of Arial. But what are the reasons against it?\nMy idea: it needs more request and more traffic. Websites with performance problems may have more problems, is this right? I am interested in the opinion of professionals, what do you thin? Please help me to find arguments!</p>\n\n<p>Many Thanks!</p>\n"},{"tags":["performance","jsf","configuration","myfaces","mojarra"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":27,"score":1,"question_id":13033070,"title":"What is the default for FACELETS_REFRESH_PERIOD?","body":"<p>In a JSF application, the parameter javax.faces.FACELETS_REFRESH_PERIOD can be used to enable/disable automatic reloading of XHTML files.</p>\n\n<p>I am currently researching the right configuration for production deployments, and accidentally found out that we currently run with FACELETS_REFRESH_PERIOD=1 even in production, which is obviously not a good idea.</p>\n\n<p>This lead to the question: What is the default value for this parameter?</p>\n\n<p>Ideally, I'd like to just omit FACELETS_REFRESH_PERIOD from our production config for simplicity's sake, and hoped it would use a \"safe\" default value of -1. However, this does not seem to be the case, because without the parameter, refreshing seems to be enabled (with both Mojarra and MyFaces).</p>\n\n<p>I checked the JSF spec, and while it describes the parameter, it does not give a default. Is this a deliberate omission in the spec?</p>\n"},{"tags":["mysql","performance","left-join"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":41,"score":1,"question_id":13034163,"title":"MySQL join slow even after indexing","body":"<p>I have two tables members and dep whose description are as follows:</p>\n\n<pre><code>TABLE members:\n+-----------------+------------+------+-----+---------+-------+\n| Field           | Type       | Null | Key | Default | Extra |\n+-----------------+------------+------+-----+---------+-------+\n| MemberID_M      | varchar(8) | YES  | MUL | NULL    |       |\n| Age             | varchar(5) | YES  |     | NULL    |       |\n| Sex             | varchar(1) | YES  |     | NULL    |       |\n| SomeInfo        | int(11)    | YES  |     | NULL    |       |\n+-----------------+------------+------+-----+---------+-------+\nTABLE dep:\n+-----------------+------------+------+-----+---------+-------+\n| Field           | Type       | Null | Key | Default | Extra |\n+-----------------+------------+------+-----+---------+-------+\n| MemberID_t      | int(11)    | YES  | MUL | NULL    |       |\n| YEAR            | varchar(2) | NO   |     |         |       |\n| Days            | tinyint(4) | YES  |     | NULL    |       |\n| train           | bigint(20) | NO   |     | 0       |       |\n+-----------------+------------+------+-----+---------+-------+\n</code></pre>\n\n<p>I want to perform the following query:</p>\n\n<pre><code>CREATE TABLE table2 \nSELECT a.*,b.* \nFROM dep AS a LEFT OUTER JOIN members AS b \n          ON  a.MemberID_t = b.Memberid_M;\n</code></pre>\n\n<p>Intially, the ids in both the tables were not indexed and the query did not return for hours. Now, even after indexing it is taking a lot of time.</p>\n\n<pre><code>EXPLAIN for the SELECT part of the query is:\n+----+-------------+-------+------+---------------+------+---------+------+--------+-------+\n| id | select_type | table | type | possible_keys | key  | key_len | ref  | rows   | Extra |\n+----+-------------+-------+------+---------------+------+---------+------+--------+-------+\n|  1 | SIMPLE      | a     | ALL  | NULL          | NULL | NULL    | NULL | 436689 |       |\n|  1 | SIMPLE      | b     | ALL  | memid2        | NULL | NULL    | NULL | 226127 |       |\n+----+-------------+-------+------+---------------+------+---------+------+--------+-------+\n</code></pre>\n"},{"tags":["java","performance","playframework","playframework-1.x"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":262,"score":2,"question_id":11288712,"title":"CPU load with play framework","body":"<p>Since a few days, on a system which has been in development for about a year, I have a constant CPU load from the play! server. I have two servers, one active and one as a hot spare. In the past, the hot-spre server showed no load, or a neglectable load. But now it consumes a constant 50-110% CPU (using top on Linux).</p>\n\n<p>Is there an easy way to find out what the cause it? I don't see this behavior on my MacBook when debugging (usually 0.1-1%).This is something that only happened in the past few days as far as I am aware.</p>\n\n<p>This is a status print of the hot-spare. As can be seen no controllers are queried apart from the scheduled tasks (which do not perform on this server due to a flag, but are launched):</p>\n\n<pre><code>~        _            _ \n~  _ __ | | __ _ _  _| |\n~ | '_ \\| |/ _' | || |_|\n~ |  __/|_|\\____|\\__ (_)\n~ |_|            |__/   \n~\n~ play! 1.2.4, http://www.playframework.org\n~ framework ID is prod-frontend\n~\n~ Status from http://localhost:xxxx/@status,\n~\nJava:\n~~~~~\nVersion: 1.6.0_26\nHome: /usr/lib/jvm/java-6-sun-1.6.0.26/jre\nMax memory: 64880640\nFree memory: 11297896\nTotal memory: 29515776\nAvailable processors: 2\n\nPlay framework:\n~~~~~~~~~~~~~~~\nVersion: 1.2.4\nPath: /opt/play\nID: prod-frontend\nMode: PROD\nTmp dir: /xxx/tmp\n\nApplication:\n~~~~~~~~~~~~\nPath: /xxx/server\nName: iDoms Server\nStarted at: 07/01/2012 12:05\n\nLoaded modules:\n~~~~~~~~~~~~~~\nsecure at /opt/play/modules/secure\npaginate at /xxx/server/modules/paginate-0.14\n\nLoaded plugins:\n~~~~~~~~~~~~~~\n0:play.CorePlugin [enabled]\n100:play.data.parsing.TempFilePlugin [enabled]\n200:play.data.validation.ValidationPlugin [enabled]\n300:play.db.DBPlugin [enabled]\n400:play.db.jpa.JPAPlugin [enabled]\n450:play.db.Evolutions [enabled]\n500:play.i18n.MessagesPlugin [enabled]\n600:play.libs.WS [enabled]\n700:play.jobs.JobsPlugin [enabled]\n100000:play.plugins.ConfigurablePluginDisablingPlugin [enabled]\n\nThreads:\n~~~~~~~~\nThread[Reference Handler,10,system] WAITING\nThread[Finalizer,8,system] WAITING\nThread[Signal Dispatcher,9,system] RUNNABLE\nThread[net.sf.ehcache.CacheManager@449278d5,5,main] WAITING\nThread[Timer-0,5,main] TIMED_WAITING\nThread[com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread-#0,5,main] TIMED_WAITING\nThread[com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread-#1,5,main] TIMED_WAITING\nThread[com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread-#2,5,main] TIMED_WAITING\nThread[jobs-thread-1,5,main] TIMED_WAITING\nThread[jobs-thread-2,5,main] TIMED_WAITING\nThread[jobs-thread-3,5,main] TIMED_WAITING\nThread[New I/O server boss #1 ([id: 0x7065ec20, /0:0:0:0:0:0:0:0:9001]),5,main] RUNNABLE\nThread[DestroyJavaVM,5,main] RUNNABLE\nThread[New I/O server worker #1-3,5,main] RUNNABLE\n\nRequests execution pool:\n~~~~~~~~~~~~~~~~~~~~~~~~\nPool size: 0\nActive count: 0\nScheduled task count: 0\nQueue size: 0\n\nMonitors:\n~~~~~~~~\ncontrollers.ReaderJob.doJob(), ms.         -&gt;      114 hits;      4.1 avg;      0.0 min;    463.0 max;\ncontrollers.MediaCoderProcess.doJob(), ms. -&gt;     4572 hits;      0.1 avg;      0.0 min;    157.0 max;\ncontrollers.Bootstrap.doJob(), ms.         -&gt;        1 hits;      0.0 avg;      0.0 min;      0.0 max;\n\nDatasource:\n~~~~~~~~~~~\nJdbc url: jdbc:mysql://xxxx\nJdbc driver: com.mysql.jdbc.Driver\nJdbc user: xxxx\nJdbc password: xxxx\nMin pool size: 1\nMax pool size: 30\nInitial pool size: 3\nCheckout timeout: 5000\n\nJobs execution pool:\n~~~~~~~~~~~~~~~~~~~\nPool size: 3\nActive count: 0\nScheduled task count: 4689\nQueue size: 3\n\nScheduled jobs (4):\n~~~~~~~~~~~~~~~~~~~~~~~~~~\ncontrollers.APNSFeedbackJob run every 24h. (has never run)\ncontrollers.Bootstrap run at application start. (last run at 07/01/2012 12:05:32)\ncontrollers.MediaCoderProcess run every 15s. (last run at 07/02/2012 07:10:46)\ncontrollers.ReaderJob run every 600s. (last run at 07/02/2012 07:05:36)\n\nWaiting jobs:\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\ncontrollers.MediaCoderProcess will run in 2 seconds\ncontrollers.APNSFeedbackJob will run in 17672 seconds\ncontrollers.ReaderJob will run in 276 seconds\n</code></pre>\n"},{"tags":["windows","performance"],"answer_count":5,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":302,"score":3,"question_id":3202337,"title":"How to simulate a reboot in Win7 for testing application cold-start time","body":"<p>In order to measure application' cold-start time, I have to reboot my machine every time, which is really time-consuming. I understand it is mission impossible to simulate a real reboot, but what I want is something rough, ex, put out cache in standby list as many as possible so the warm start won't be so warm anymore.</p>\n\n<p>Any ideas on this?</p>\n\n<p>Thanks</p>\n"},{"tags":["javascript","performance","apache","networking"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":583,"score":1,"question_id":10938682,"title":"How to reduce server \"Wait\" time?","body":"<p>I am trying to optimize my site's speed and I'm using the great tool at <a href=\"http://tools.pingdom.com/fpt/\" rel=\"nofollow\">pingdom.com</a>.  Right now, over 50% of the time it takes to load the page is \"Wait\" time as shown in the screenshot below.  What can I do to reduce this?  Also, how typical is this figure?  are there benchmarks on this?  Thanks!</p>\n\n<p><img src=\"http://i.stack.imgur.com/RgsFY.png\" alt=\"high server wait time\"></p>\n\n<p><strong>EDIT:</strong>\nOk.. let me clarify a few things.  There are no server side scripts or database calls going on. Just HTML, CSS, JS, and images.  I have already done some things like push js to the end of the body tag to get parallel downloads.  I am aware that the main.html and templates.html are adding to the overall wait time by being done synchronously after js.js downloads, that's not the problem.  I am just surprised at how much \"wait\" time there is for each request.  Does server distance affect this?  what about being on a shared server, does that affect the wait time?  Is there any low-hanging fruit to remedy those issues?</p>\n\n<p><img src=\"http://i.stack.imgur.com/rsBlu.png\" alt=\"enter image description here\"></p>\n"},{"tags":["sql","sql-server","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":6,"down_vote_count":0,"view_count":126,"score":6,"question_id":12976486,"title":"SQL Server uses scan instead of seek when using a window function and predicate contains a variable","body":"<p>Take a look at this fiddle: <a href=\"http://sqlfiddle.com/#!6/18324/2\" rel=\"nofollow\">http://sqlfiddle.com/#!6/18324/2</a></p>\n\n<p>Expand the very first execution plan, for the queries against view <code>B</code>.<br>\nNotice that the first query executes using index seek, while the second one - using index scan. In my real setup, with thousands of rows, this produces a performance hit that is quite considerable.  </p>\n\n<p>WTF???</p>\n\n<p>The queries are equivalent, aren't they? Why does a literal produce seek and a variable - scan?<br>\nBut more importantly: <strong>how can I work around this</strong>?  </p>\n\n<p><a href=\"http://stackoverflow.com/questions/4459425/sql-server-query-fast-with-literal-but-slow-with-variable\">This post</a> comes closest to the problem, and the solution that works from there is using <code>option(recompile)</code> (thank you, Martin Smith). However, that does not work for me, because my queries are being generated by my ORM library (which is Entity Framework) and I cannot amend them manually.<br>\nRather what I'm looking for is a way to reformulate the <code>B</code> view so that the problem would not occur.</p>\n\n<p>While fiddling with this problem, I have noticed that it is always the \"Segment\" block in the execution plan that loses the predicate. To verify this, I reformulated the query in terms of a subquery with <code>min</code> function (see view <code>D</code>). And voila! - both queries against the <code>D</code> view produce identical plans.</p>\n\n<p>The bad news, however, is that I cannot use this <code>min</code>-powered trick, because in my real setup, the column <code>Y</code> is actually <strong>several columns</strong>, so that I can order by them, but I cannot take a <code>min()</code> of them.<br>\nSo the second question would be: <strong>can anyone come up with a trick that is similar to min-powered subquery, but works for several columns?</strong></p>\n\n<p><strong>NOTE 1</strong>: this is definitely not related to the tipping point, because there are just 2 records in the table.<br>\n<strong>NOTE 2</strong>: it also doesn't have to do with the presence of a view. See an example with view <code>C</code>: the server is happily using seek in that case.</p>\n"},{"tags":["mysql","performance","database-performance"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":75,"score":3,"question_id":13029946,"title":"Mysql self inner join query?","body":"<p>When i run the below query in mysql it took 78 sec to display record. Is there other way to write this query. Here is my \nmysql query -> \"</p>\n\n<pre><code>select distinct nuqta1.post_id from wp_postmeta as nuqta1 \ninner join wp_postmeta as nuqta2 on (nuqta1.post_id = nuqta2.post_id) \ninner join wp_postmeta as nuqta4 on (nuqta1.post_id = nuqta4.post_id) \ninner join wp_postmeta as nuqta5 on (nuqta1.post_id = nuqta5.post_id) \ninner join wp_postmeta as nuqta6 on (nuqta1.post_id = nuqta6.post_id) \ninner join wp_postmeta as nuqta7 on (nuqta1.post_id = nuqta7.post_id) \ninner join wp_postmeta as nuqta8 on (nuqta1.post_id = nuqta8.post_id) \ninner join wp_postmeta as nuqta9 on (nuqta1.post_id = nuqta9.post_id) \ninner join wp_postmeta as nuqta10 on (nuqta1.post_id = nuqta10.post_id) \ninner join wp_postmeta as nuqta11 on (nuqta1.post_id = nuqta11.post_id) \ninner join wp_postmeta as nuqta12 on (nuqta1.post_id = nuqta12.post_id) \nwhere (nuqta2.meta_key = 'checkin' and nuqta2.meta_value LIKE '%10/31/2012%') \nand (nuqta4.meta_key = 'guests' and nuqta4.meta_value ='1') \nand (nuqta5.meta_key = 'roomtype' and nuqta5.meta_value LIKE '%Entire home/apt%') \nand (nuqta6.meta_key = 'price' and cast(nuqta6.meta_value as signed) BETWEEN '10' and '99999') \nand (nuqta7.meta_key = 'amenities' and nuqta7.meta_value LIKE '%Wireless Internet%') \nand (nuqta8.meta_key = 'amenities' and nuqta8.meta_value LIKE '%TV%') \nand (nuqta9.meta_key = 'amenities' and nuqta9.meta_value LIKE '%Kitchen%') \nand (nuqta10.meta_key = 'amenities' and nuqta10.meta_value LIKE '%Wireless Internet%') \nand (nuqta11.meta_key = 'amenities' and nuqta11.meta_value LIKE '%TV%') \nand (nuqta12.meta_key = 'amenities' and nuqta12.meta_value LIKE '%Kitchen%') \nand 1=1 order by nuqta1.post_id asc\n</code></pre>\n\n<p>\".\nAnd and i am using wordpress table wp_postmeta to run this query</p>\n"},{"tags":["java","performance","graphics2d"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":50,"score":1,"question_id":13030891,"title":"Graphics2D Drawing Performance","body":"<p>I am trying some things out with manually drawing \"things\" with a Java <code>Graphics2D</code> object within a Swing component and as I reach about >2000 squares that I order the object to draw it gets really slow.</p>\n\n<p>I have no clue whether or not this is \"common\". Are 2000 objects to render really \"a lot\"? Is the <code>Graphics2D</code> object just not very performant? Should I just stop where I am now and rather switch to JOGL before I try out more complex stuff and it is too late?</p>\n"},{"tags":["java","performance","visualvm"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":77,"score":1,"question_id":13030317,"title":"Debugging a slow Java method","body":"<p>VisualVM is showing me that a particular method is taking a long time to execute.</p>\n\n<p>Are there any widely used strategies for looking at the performance (in regards to time) of a Java method?</p>\n\n<p>My gut feeling is that the sluggish response time will come from a method that is somewhere down the call hierarchy from the one VisualVM is reporting but I think getting some hard numbers is better than fishing around in the code based on an assumption when it comes to performance.</p>\n"},{"tags":["sql","performance","between"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":246,"score":1,"question_id":10834194,"title":"SQL query multiple ranges without using multiple OR clauses (nesting LIKE, BETWEEN)","body":"<p>I commonly have to query an extremely large table for multiple variables with many conditions per variable.  Often times, a variable will need to be queried for multiple ranges.  For example, I could need all records of VAR_1 where VAR_1 is between 200-300, 350-400, 450-500.</p>\n\n<p>Normally I would write this as follows, but have been told that using <code>IN()</code> instead of the multiple ORs would be much more efficient.\n</p>\n\n<pre><code>SELECT * FROM table\nWHERE VAR_1 BETWEEN '200' AND '300' OR\n      VAR_1 BETWEEN '350' AND '400' OR\n      VAR_1 BETWEEN '450' AND '500'\n</code></pre>\n\n<p>Is there any way to condense this information and get rid of the <code>OR</code>s by nesting <code>LIKE</code> or <code>BETWEEN</code> clauses within an <code>IN()</code>? \nSomething along the lines of:\n</p>\n\n<p><code>WHERE VAR_1 IN (BETWEEN '200' AND '300', BETWEEN '350' AND '400', BETWEEN '450' AND '500')</code></p>\n\n<p>or</p>\n\n<p><code>WHERE VAR_1 IN ('[200-300]','[350-400]','[450-500]')</code></p>\n\n<p>I have tried things like these, but the syntax is clearly incorrect. Any ideas or directions you can point me in would be great, still very new to SQL.</p>\n"},{"tags":["css","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":34,"score":1,"question_id":13028891,"title":"how to bench a css pseudo-elements","body":"<p>Is there a way of measuring the performance (of displaying) for a web page with a lots of <code>:after</code> and <code>:before</code> in the CSS ?</p>\n\n<p>I think there is a small impact on rendering time of each element, and there is no problem on a modern computer but on a slow mobile.  </p>\n"},{"tags":["performance","node.js","loggly"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":47,"score":0,"question_id":13020546,"title":"What is the maximum throughput of Loggly?","body":"<p>How many requests per second from a client can Loggly handle? I am only able to get around 10–20 requests processed per second and I am wondering if this is normal.</p>\n"},{"tags":["c++","performance","loops","macros","overloading"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":135,"score":0,"question_id":13027550,"title":"C++ repeating codes with a loops","body":"<p>Im not sure if this is a stupid question, so shoot me if it is!</p>\n\n<p>I am having this \"dilemna\" which I encounter very often. I have say two overloaded functions in C++</p>\n\n<p>say we have this two overloads of <em>F</em> (just a pseudocode below)</p>\n\n<pre><code>void F(A a, .../*some other parameters*/)\n{ \n  //some code part\n  //loop Starts here\n  G1(a,.../* some other parameter*/)\n  //loop ends here\n  //some code part\n}\n\n\nvoid F(B b, .../*some other parameters*/)\n{\n  //some code part\n  //loop Starts here\n  G2(b,.../* some other parameter*/)\n  //loop ends here\n  //some code part\n}\n</code></pre>\n\n<p>where <em>A</em> and <em>B</em> are different types and <em>G1</em> and <em>G2</em> are different functions doing different things. The code part of the overloads except for <em>G1</em> and <em>G2</em> lines are the same and they are sometimes very long and extensive. Now the question is.. how can I write my code more efficiently. Naturally I want NOT to repeat the code (even if it's easy to do that, because its just a copy paste routine). A friend suggested macro... but that would look dirty. Is this simple, because if it is Im quite stupid to know right now. Would appreciate any suggestions/help.</p>\n\n<p><strong>Edit:</strong> Im sorry for those wanting a code example. The question was really meant to be abstract as I encounter different \"similar\" situation in which I ask myself how I am able to make the code shorter/cleaner. In most cases codes are long otherwise I wouldn't bother asking this in the first place. As KilianDS pointed out, it's also good to make sure that the function itself isn't very long. But sometimes this is just unavoidable. Many cases where I encounter this, the loop is even nested (i.e. several loops within each other) and the beginning of <em>F</em> we have the start of a loop and the end of <em>F</em> we end that loop.</p>\n\n<p>Jose </p>\n"},{"tags":["mysql","performance","database-performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":28,"score":0,"question_id":13027911,"title":"MySQL Query Execution Time Comparison","body":"<p>i am doing one select qry in my mysql database which contains \" order by field desc\".\nI get the resultset within 16 minutes.</p>\n\n<p>Now in the same qry i put  \"order by field desc limit  5\" to get the first 5 records..\nBut my qry is running for more than 20 minutes.</p>\n\n<p>How can i make the execution faster?\nAny Ideas?</p>\n\n<p>Thanks-\njithkaran</p>\n"},{"tags":["python","performance","matplotlib","scientific-computing"],"answer_count":4,"favorite_count":4,"up_vote_count":6,"down_vote_count":0,"view_count":1739,"score":6,"question_id":5854515,"title":"Large plot: ~20 million samples, gigabytes of data","body":"<p>I have got a problem (with my RAM) here: it's not able to hold the data I want to plot. I do have sufficient HD space. Is there any solution to avoid that \"shadowing\" of my data-set?</p>\n\n<p>Concretely I deal with Digital Signal Processing and I have to use a high sample-rate. My framework (GNU Radio) saves the values (to avoid using too much disk space) in binary. I unpack it. Afterwards I need to plot. I need the plot zoomable, and interactive. And that is an issue. </p>\n\n<p>Is there any optimization potential to this, or another software/programming language (like R or so) which can handle larger data-sets? Actually I want much more data in my plots. But I have no experience with other software. GNUplot fails, with a similar approach to the following. I don't know R (jet).</p>\n\n<pre><code>import matplotlib.pyplot as plt\nimport matplotlib.cbook as cbook\nimport struct\n\n\"\"\"\nplots a cfile\n\ncfile - IEEE single-precision (4-byte) floats, IQ pairs, binary\ntxt - index,in-phase,quadrature in plaintext\n\nnote: directly plotting with numpy results into shadowed functions\n\"\"\"\n\n# unpacking the cfile dataset\ndef unpack_set(input_filename, output_filename):\n    index = 0   # index of the samples\n    output_filename = open(output_filename, 'wb')\n\n    with open(input_filename, \"rb\") as f:\n\n        byte = f.read(4)    # read 1. column of the vector\n\n        while byte != \"\":\n        # stored Bit Values\n            floati = struct.unpack('f', byte)   # write value of 1. column to a variable\n            byte = f.read(4)            # read 2. column of the vector\n            floatq = struct.unpack('f', byte)   # write value of 2. column to a variable\n            byte = f.read(4)            # next row of the vector and read 1. column\n            # delimeter format for matplotlib \n        lines = [\"%d,\" % index, format(floati), \",\",  format(floatq), \"\\n\"]\n            output_filename.writelines(lines)\n            index = index + 1\n    output_filename.close\n    return output_filename.name\n\n# reformats output (precision configuration here)\ndef format(value):\n    return \"%.8f\" % value            \n\n# start\ndef main():\n\n    # specify path\n    unpacked_file = unpack_set(\"test01.cfile\", \"test01.txt\")\n    # pass file reference to matplotlib\n    fname = str(unpacked_file)\n    plt.plotfile(fname, cols=(0,1)) # index vs. in-phase\n\n    # optional\n    # plt.axes([0, 0.5, 0, 100000]) # for 100k samples\n    plt.grid(True)\n    plt.title(\"Signal-Diagram\")\n    plt.xlabel(\"Sample\")\n    plt.ylabel(\"In-Phase\")\n\n    plt.show();\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>\n\n<p>Something like plt.swap_on_disk() could cache the stuff on my SSD ;)</p>\n"},{"tags":["javascript","performance","browser","pagespeed"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":51,"score":1,"question_id":13015421,"title":"JavaScript browser parsing speed testing","body":"<p>I'm looking into the speed of JavaScript parsers in web browsers, importantly it needs to be easy to demonstrate. I came up with a simple test - the idea being that each script block is parsed and executed individually, so a large block of script could be timed:</p>\n\n<pre><code>&lt;script&gt;var start = new Date().getTime();&lt;/script&gt;\n\n&lt;script&gt;\n    /*! jQuery v1.8.2 jquery.com | jquery.org/license */\n    ...\n&lt;/script&gt;\n\n&lt;script&gt;alert ( new Date().getTime() - start );&lt;/script&gt;\n</code></pre>\n\n<p>Superficially this appears to work, removing the middle script block will result in a negligible time.</p>\n\n<p>However I'm not certain that my logic is not fundamentally flawed.</p>\n"},{"tags":["c++","c","performance","optimization","compiler-optimization"],"answer_count":6,"favorite_count":1,"up_vote_count":32,"down_vote_count":2,"view_count":1333,"score":30,"question_id":12919184,"title":"Why doesn't a compiler optimize floating-point *2 into an exponent increment?","body":"<p>I've often noticed gcc converting multiplications into shifts in the executable. Something similar might happen when multiplying an <code>int</code> and a <code>float</code>. For example, <code>2 * f</code>, might simply increment the exponent of <code>f</code> by 1, saving some cycles. Do the compilers, perhaps if one requests them to do so (e.g. via <code>-ffast-math</code>), in general, do it?</p>\n\n<p>Are compilers generally smart enough to do this, or do I need to do this myself using the <code>scalb*()</code> or <code>ldexp()/frexp()</code> function family?</p>\n"},{"tags":["java","performance","eclipse-plugin","eclipse-rcp","jface"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":61,"score":0,"question_id":12918603,"title":"Eclipse plugin performance degrades over time","body":"<p>I have 2 <code>views</code> A and B, each has a <code>treeViewer</code>. \nB has a <code>listener</code> to tree <code>selectionChanged</code> in <code>view</code> A.\nso each time I select  a <code>tree</code> item in A , the <code>selectionChanged</code> action is:</p>\n\n<p>1- <code>setInput</code> to <code>tree</code> B</p>\n\n<p>2- apply filter </p>\n\n<p>3- expand all elements</p>\n\n<p>The problem here is each time I click on a tree item in <code>View</code> A, the time it takes for <code>View</code> B to show up the tree items is increasing every time for the same selection in tree A.and items show up slower each time.</p>\n\n<p>which part of the <code>selectionChanged</code> action could result in this performance problem !?</p>\n\n<p>After some investigations, I found out that the method <code>TreeViewer.expandAll()</code> is the one causes that big delay each time. \nIs this an <code>Eclipse</code> issue or should I modify it ?</p>\n"},{"tags":["java","performance","multiplication"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":46,"score":-1,"question_id":13023017,"title":"Need a more optimized karatsuba code java","body":"<p>Here is the code that I have submitted for fast multiplication problem using Karatsuba algorithm but still it gives TLE.The code deploys recursive Karatsuba.Kindly suggest optimizations.The karatsuba function takes in two BigInteger variables as parameter and returns a BigInteger as a result for multiplication.If the bitlength is less than 2000 ,it returns the result of simple multtiplication.Or else it goes for recursion</p>\n\n<pre><code>import java.math.BigInteger;\nimport java.util.Scanner;\n\n\npublic class Main {\n\n     public static BigInteger karatsuba(BigInteger x, BigInteger y) {\n\n            // cutoff to brute force\n            int N = Math.max(BitLength(x), BitLength(y));\n         //   System.out.println(N);\n\n        if (N &lt;= 2000) return x.multiply(y);                                        \n\n        // number of bits divided by 2, rounded up\n        N = (N / 2) + (N % 2);\n\n        // x = a + 2^N b,   y = c + 2^N d\n        BigInteger b = x.shiftRight(N);\n      //  System.out.println(b);\n\n        BigInteger a = x.subtract(b.shiftLeft(N));\n      //  System.out.println(a);\n        BigInteger d = y.shiftRight(N);\n      //  System.out.println(d);\n        BigInteger c = y.subtract(d.shiftLeft(N));\n     //   System.out.println(c);\n\n        // compute sub-expressions\n        BigInteger ac    = karatsuba(a, c);\n        BigInteger bd    = karatsuba(b, d);\n        BigInteger abcd  = karatsuba(a.add(b), c.add(d));\n\n            return ac.add(abcd.subtract(ac).subtract(bd).shiftLeft(N)).add(bd.shiftLeft(2*N));\n    }\n\n public static int BitLength(BigInteger n)\n {\n     byte[] Data = n.toByteArray();\n     int result = (Data.length - 1) * 8;\n     byte Msb = Data[Data.length - 1];\n     while (Msb != 0) {\n         result += 1;\n         Msb &gt;&gt;= 1;\n     }\n     return result;\n }\n\n\npublic static void main(String[] args) {\n    BigInteger A,B,C;\n    int  t;\n    Scanner sc = new Scanner(System.in);\n\n    t = sc.nextInt();\n    while(t &gt; 0){\n\n\n         A = sc.nextBigInteger();\n         B = sc.nextBigInteger();\n         C =  karatsuba(A, B);\n         System.out.println(C);\n         --t;\n\n    }\n    sc.close();\n}\n</code></pre>\n\n<p>}</p>\n"},{"tags":["iphone","objective-c","ios","performance","cocoa-touch"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":53,"score":2,"question_id":12990722,"title":"How do I test the load time of my application?","body":"<p>I would like to benchmark the load time of my application but am not sure how to do this. I could start by inserting a couple of <code>NSLog</code>s and comparing the time stamps. But where should I place these?</p>\n\n<p>Should I place the second log in my root view controller's <code>viewDidAppear</code> since this is the first time the application is available to the user? And what about the first log? I was considering applicationDidFinishLaunchingWithOptions: but the name suggests this is a little late.</p>\n\n<p>Any references to tutorials, instruments, or anything else would be appreciated.</p>\n"},{"tags":["linux","performance","cpu","performancecounter","perf"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":13004300,"title":"perf record what is relation between -c option and overflow events?","body":"<p>I am trying to use perf record to record 15 hardware and trace point events.</p>\n\n<p>I want to understand the following:</p>\n\n<ol>\n<li><p>event based sampling: from the docs I understood perf record will sample whenever 64-bit counter corresponding to that events will over flow. Is that right? The counter will overflow after ~ 2^64 such events?</p></li>\n<li><p>When I have more events to measure than the number of PMUs/counters, do I have to pass any specific switch to use multiplexing. How does overflow-event behave in the presence of multiplexing.</p></li>\n<li><p>What is the purpose/use of the switch \"-c\" ? Can I make the counters to overflow every n-events using this switch?</p></li>\n</ol>\n\n<p>Please help. </p>\n"},{"tags":["objective-c","ios","performance","cocoa-touch","core-data"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":29,"score":0,"question_id":13022065,"title":"Indexing a Core Data property to improve NSSortDescriptor performance","body":"<p>I have a core data relationship that is unordered. For various reasons I can not use an NSOrderedSet, but I do provide an additional method on the <code>NSManagedObject</code> that uses an <code>NSSortDescriptor</code> to order the items by date.</p>\n\n<p>I am hitting a performance issue and have identified that this sort descriptor is the root cause. I have read that <code>NSSortDescriptor</code> is not particularly fast. I am considering adding and maintaining an index on the relationship since my insert/update/delete performance is much less of an issue.</p>\n\n<p>Should I expect a noticeable improvement if my <code>NSSortDescriptor</code> sorts by index instead of date? Or is the performance issue more related to the <code>NSSortDescriptor</code> itself and not the attribute being sorted on?</p>\n\n<p>If it might help, how do I do this? Do I simply select the 'Indexed' check box on my data model editor and re-generate the <code>NSManagedObject</code>? What kind of code will it add to the class? Or do I need to do some manual work also?</p>\n"},{"tags":["php","performance","button","reload","counter"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":270,"score":1,"question_id":10528638,"title":"PHP Button Counter Issue","body":"<p>I'm fairly new to PHP, but have managed to set up a script that simply displays the amount of times a button is clicked each time a user clicks on it.\n<br>Here is the PHP:</p>\n\n<pre><code>&lt;?php\n$f = fopen('counter.txt', 'r+');\nflock($f, LOCK_EX);\n$total = (int) fread($f, max(1, filesize('counter.txt')));\nif (isset($_POST['submit'])) {\nrewind($f);\nfwrite($f, ++$total);\n}\nfclose($f);\n?&gt;\n</code></pre>\n\n<p>Here is the HTML:</p>\n\n<pre><code>&lt;form action='' method=\"post\"&gt;\n&lt;input type=\"submit\" name=\"submit\" value=\"click\" /&gt;\n&lt;/form&gt;\nThis button has been clicked &lt;?php echo $total; ?&gt; times.\n</code></pre>\n\n<p>The counter works and everything, but there are three issues I hope you guys can help me with:</p>\n\n<ol>\n<li><p>The counter increases each time the page is reloaded. I only want the counter to increase when the button is clicked. Is there a way I can fix this?</p></li>\n<li><p>Each time I refresh the page, Firefox asks me to confirm that I want the page to be reloaded. I know there is an option in my browser settings to prevent this, but was wondering if there was a way to refine my php so that this message does not occur for the user as well.</p></li>\n<li><p>If you click the button a couple of times and then try to use the Back button, it takes you through each one of the previous clicks. Again, is there a way to fix my code so that it does not do this and instead goes to the previous page?</p></li>\n</ol>\n\n<p>HUGE thanks!!</p>\n"},{"tags":["objective-c","ios","performance","cocoa","core-data"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":33,"score":0,"question_id":13020684,"title":"Persisting a subset of Core Data objects","body":"<p>This is a little strange, but I want to store an array of <code>NSManagedObjects</code> outside of Core Data. </p>\n\n<p>Core Data is managing all of my model's properties and relationships. One such relationship is to all of an <code>Author</code>'s <code>Books</code>. Sometimes it is useful to know that same list in a certain order, so I have added <code>booksByDate</code> (for example) to the <code>NSManagedObject</code> Author.</p>\n\n<p>Because the sort descriptor proved expensive, I implemented a cache, using an iVar on <code>Author</code>. This helped a lot with some laggy UI issues I was experiencing. But the cache was only useful after it was first loaded, so when my application launches, I now go and tell each <code>Author</code> to cache its <code>booksByDate</code>. This adds a few seconds to my launch time, but drastically speeds up the performance once the app is running.</p>\n\n<p>I would like to reduce that launch time. One area I am experimenting with is somehow storing each <code>Author</code>'s cached <code>booksByDate</code>. At launch, instead of telling each <code>Author</code> to generate its cache using the expensive sort descriptor, I could just pass out each stored cache to it's correct <code>Author</code>.</p>\n\n<p>How might I store these caches so they persist between executions? </p>\n"},{"tags":["database","performance","hibernate","jpa"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":12898638,"title":"Database commit performance","body":"<p>I develop an App and recently I've caught me on the thought. What of the following has better performance?</p>\n\n<ol>\n<li>Perform single commit of 1000 entities.</li>\n<li>Perform 20 commits with 50 entities at each commit.</li>\n</ol>\n\n<p>And how do you think will it be implementation dependable?</p>\n"},{"tags":[".net","performance","memory-management","garbage-collection"],"answer_count":17,"favorite_count":10,"up_vote_count":37,"down_vote_count":0,"view_count":12125,"score":37,"question_id":118633,"title":"What's so wrong about using GC.Collect()?","body":"<p>Although I do understand the serious implications of playing with this function (or at least that's what I think), I fail to see why it's becoming one of these things that respectable programmers wouldn't ever use, even those who don't even know what it is for.</p>\n\n<p>Let's say I'm developing an application where memory usage varies extremely depending on what the user is doing. The application life cycle can be divided into two main stages: editing and real-time processing. During the editing stage, suppose that billions or even trillions of objects are created; some of them small and some of them not, some may have finalizers and some may not, and suppose their lifetimes vary from a very few milliseconds to long hours. Next, the user decides to switch to the real-time stage. At this point, suppose that performance plays a fundamental role and the slightest alteration in the program's flow could bring catastrophic consequences. Object creation is then reduced to the minimum possible by using object pools and the such but then, the GC chimes in unexpectedly and throws it all away, and someone dies.</p>\n\n<p>The question: In this case, wouldn't it be wise to call GC.Collect() before entering the second stage?</p>\n\n<p>After all, these two stages never overlap in time with each other and all the optimization and statistics the GC could have gathered would be of little use here...</p>\n\n<p>Note: As some of you have pointed out, .NET might not be the best platform for an application like this, but that's beyond the scope of this question. The intent is to clarify whether a GC.Collect() call can improve an application's overall behaviour/performance or not. We all agree that the circumstances under which you would do such a thing are extremely rare but then again, the GC tries to guess and does it perfectly well most of the time, but it's still about guessing.</p>\n\n<p>Thanks.</p>\n"},{"tags":["performance","optimization","memory","global-variables"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":52,"score":1,"question_id":6838602,"title":"Are there any benefits to declaring a commonly declared variable as a global?","body":"<p>For example, I have:</p>\n\n<pre><code>char query[512];\n</code></pre>\n\n<p>declared about 27 times in my application which connects to a mysql database.</p>\n\n<p>The same size each time, and declared in many different functions.</p>\n\n<p>This application will never use threads.</p>\n\n<p>The query is ALWAYS executed immediately after the query is set with snprintf.\nThere is no function in between the setting and the execution of the query to mess it up.</p>\n\n<p>Are their any benefits or performance gains to declaring this as a global variable?</p>\n"},{"tags":["c#","arrays","winforms","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":116,"score":1,"question_id":10820108,"title":"Difference in speed while accessing an array","body":"<p><strong>Is any difference (in speed of my program - execution time) between?? <br/></strong></p>\n\n<p><strong>1 st option:</strong></p>\n\n<pre><code> private void particleReInit(int loop)\n        {\n            this.particle[loop].active = true;\n            this.particle[loop].life = 1.0f;\n            this.particle[loop].fade = 0.3f * (float)(this.random.Next(100)) / 1000.0f + 0.003f; \n            this.particle[loop].r = colors[this.col][0];    // Select Red Rainbow Color\n            this.particle[loop].g = colors[this.col][1];    // Select Red Rainbow Color\n            this.particle[loop].b = colors[this.col][2];    // Select Red Rainbow Color\n            this.particle[loop].x = 0.0f;\n            this.particle[loop].y = 0.0f;\n            this.particle[loop].xi = 10.0f * (this.random.Next(120) - 60.0f);\n            this.particle[loop].yi = (-50.0f) * (this.random.Next(60)) - (30.0f);\n            this.particle[loop].xg = 0.0f; \n            this.particle[loop].yg = 0.8f; \n            this.particle[loop].size = 0.2f;\n            this.particle[loop].center = new PointF(particleTextures[0].Width / 2, particleTextures[0].Height / 2);\n\n    }\n</code></pre>\n\n<p><strong>2 nd option: <br/></strong></p>\n\n<pre><code>Particle p = particle[loop];\np.active = true;\np.life = 1.0f;\n...\n</code></pre>\n\n<p><br/>\nWhere <code>Particle particle[] = new Particle[NumberOfParticles];</code> is just an array of Particles which have some properties like life, position.</p>\n\n<p><br/>\nI'm doing it in Visual Studio 2010 Like WFA (Windows Form Aplication) and need to enhance performance (we're not able to use OpenGL, so for more particles my program tends to be slow).</p>\n"},{"tags":["sql-server","performance","stored-procedures"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":82,"score":0,"question_id":13005076,"title":"SQL Server stored procedure a lot slower than straight query","body":"<p>I have a table with over 100MM records in it. The table has a clustered index and a nonclustered index.</p>\n\n<p>I can run a basic count using T-SQL on the table and it takes 1 second to run. When I put the same exact count query inside of a stored procedure it then takes 12 seconds to run.</p>\n\n<p>I have looked at the execution plan for both the standard query and the stored procedure and they both are using the nonclustered index.</p>\n\n<p>I am not sure why the stored procedure is so slow compared to the standard query.</p>\n\n<p>I have read some stuff about reindexing in a situation like this but I am not sure why I need to do that. Also, it takes a few hours to reindex so I want to make sure that will work.</p>\n\n<p>Any help on this would be great.</p>\n\n<p>Thanks</p>\n\n<p>UPDATE </p>\n\n<p>Here is the stored procedure:</p>\n\n<pre><code>SET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\n\nALTER PROCEDURE quickCount \n\n@sYID INT,\n@eYID INT\n\nAS\nBEGIN\n\nSET NOCOUNT ON;\n\n\n    SELECT COUNT(leadID)\n    FROM dbo.leads\n    WHERE yearID &gt;= @sYID\n    AND yearID &lt;= @eYID\n\nEND\nGO\n</code></pre>\n\n<p>and here is the standard query:</p>\n\n<pre><code>SELECT COUNT(leadID)\nFROM leads\nWHERE yearID &gt;= 0\nAND yearID &lt;= 99\n</code></pre>\n\n<p>I did try to run it with no parameters and the SP runs way faster (1 second). So I am assuming that it has something to do with the parameters.</p>\n"},{"tags":["c++","database","performance","compression","bit-manipulation"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":33,"score":0,"question_id":11924954,"title":"Modifying WAH Compress bitmap performence","body":"<p>I have developing word align bitmap compression algorithm for data indexing.algorithm is based on the WAH compression research paper.compression bitmap perform well on bit-wise operation and it's very space efficient. but modifying the compressed bitmap not very efficient ,because modifying need splitting compressed word size block and several memmove cause performance bottleneck. </p>\n\n<blockquote>\n  <p>please look at the following example.</p>\n  \n  <p>example: data set -\n  [1000000,34,9,23456,6543,10000000,23440004,100,345]</p>\n</blockquote>\n\n<p>performance reduce  due to the random nature of the data set , in the real application scenario this can happened.</p>\n\n<ol>\n<li>can anyone give me a hint on how to overcome this performance problem?.</li>\n</ol>\n"},{"tags":["performance","algorithm","open","bigdata"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":53,"score":1,"question_id":13016866,"title":"BigData analysis: scalability but quick development","body":"<p>Which do you consider is the best language to processing (getting statistical results) of analysing some GB of data information, taking into account these limitations:</p>\n\n<ul>\n<li>Open source code.</li>\n<li>Data can be analyzed in matrixes.</li>\n<li>Developing time limited.</li>\n<li>Cost of processing also limited.</li>\n</ul>\n\n<p>For, example, Octave, Fortran, C++, C, Python, etc.</p>\n\n<p>Thanks.</p>\n"},{"tags":["performance","matlab","time","for-loop"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":80,"score":0,"question_id":13012914,"title":"How can i make this code more time efficient?","body":"<p>This is a part of the Profiler report, and shows how these lines are eating up the time. Can it be improved upon?      </p>\n\n<pre><code>         434 %clean up empty cells in subPoly\n         228  435 if ~isempty(subPoly) \n         169  436     subPoly(cellfun(@isempty,subPoly)) = []; \n              437 \n              438     %remove determined subpoly points from the hull polygon\n         169  439     removeIndex = zeros(size(extendedPoly,1),1); \n         169  440     for i=1:length(subPoly) \n         376  441         for j=1:size(subPoly{i}(:,1)) \n       20515  442             for k=1:size(extendedPoly,1) \n6.12 5644644  443                 if extendedPoly(k,:)==subPoly{i}(j,:) \n       30647  444                     removeIndex(k,1)=1; \n       30647  445                 end \n1.08 5644644  446             end \n0.02   20515  447         end \n         376  448     end \n         169  449     extendedPoly = extendedPoly(~removeIndex(:,1),:);  \n         169  450 end \n</code></pre>\n"},{"tags":["java","php","c","performance","code-efficiency"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":4,"view_count":101,"score":-3,"question_id":13016603,"title":"Programming Challenge: Finding 2 Similar arrays in Minimum O(n) Time","body":"<p>I think its quite a challenging programming issue..</p>\n\n<p>I Have a group of arrays in DB. </p>\n\n<p>Each Array has a list of ID numbers.</p>\n\n<p>I get one array and I need to find the most identical array to this specific one.  </p>\n\n<p>Each array has 20 IDs. </p>\n\n<p>How do I find it in minimum time? The fast answer will be compare each cell .. but, what the content of the cell are not in the same location?</p>\n\n<p>Example:</p>\n\n<pre><code>Array MyArray = [**1122, 2233, 4455**, 6677, **8899..**. , N]\n\nArray OtherArray1 = [3454, 2233, 678, 6677, 8899... , N]\n\nArray OtherArray2 = [1122, 2233, 4455, 6677, 8899... , N]\n\nArray OtherArray3 = [**1122, 2233, 4455,** 678, **8899...** , N] ***&lt;-- THE CHOSEN ONE***\n\nArray OtherArray4 = [1122, 678, 4455, 6677, 8899... , N]\n\nArray OtherArray5 = [68, 2233, 4455, 678, 8899... , N]\n...\n..\n\nArray OtherArrayN = [2223, 7777, 5573, 7776, 8899... , N]\n</code></pre>\n\n<hr>\n\n<p>Thank You!</p>\n\n<p>Elik</p>\n"},{"tags":["c++","performance","boost","c++11","parameter-passing"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":85,"score":0,"question_id":13016139,"title":"When should I use boost call_traits::param_type versus a universal reference?","body":"<p>In C++98 I got used to using <a href=\"http://www.boost.org/doc/libs/1_51_0/libs/utility/call_traits.htm\" rel=\"nofollow\">call_traits</a> in my templated functions to automatically pick the best way to pass parameters, e.g.:</p>\n\n<pre><code>template&lt;class T&gt;\nvoid foo(typename boost::call_traits&lt;T&gt;::param_type arg)\n{\n    // .. do stuff with arg ..\n}\n</code></pre>\n\n<p>The advantage being that for primitives it would pass by value and for more complex objects it would pass by reference, so I'd have the least amount of overhead possible. C++11 now has a concept of 'universal references':</p>\n\n<pre><code>template&lt;class T&gt;\nvoid foo(T&amp;&amp; arg)\n{\n    // .. do stuff with arg ..\n}\n</code></pre>\n\n<p>As I understand it I need to use a universal reference in order to get perfect forwarding with std::forward, so if I plan to use that the choice is clear. But when I don't plan to, which should I prefer? Will a universal reference always be as good or better?</p>\n"},{"tags":["java","performance","oracle","stored-procedures"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":44,"score":0,"question_id":13015991,"title":"Processing a text file from PL/SQL vs Java","body":"<p>I need to implement a store procedure in an Oracle database that will do the following:</p>\n\n<ul>\n<li>Read an external file that needs to be processed (extract data from file and validate)</li>\n<li>Call another store procedure in the database in charge to validate/insert the data.</li>\n<li>Manage exceptions.</li>\n<li>Write to another file with the results of the operations executed.</li>\n</ul>\n\n<p>I know I can do all these things with PL/SQL or Java (store procedure), but which will be more efficient/faster or better? most of the operations are reading/writing a file, and the database operations are done in a store procedure already. </p>\n\n<p>I have read other posts about PL/SQL vs Java (like <a href=\"http://stackoverflow.com/questions/6821841/java-stored-procedure-vs-pl-sql-stored-procedure\">this</a> and <a href=\"http://www.coderanch.com/t/550834/Oracle-OAS/Native-Java-Oracle-vs-PL\" rel=\"nofollow\">this</a>) but none talks about this. </p>\n"},{"tags":["performance","io","fortran","implied-do"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":80,"score":0,"question_id":12567087,"title":"Implied do. vs explicit loop with IO","body":"<p>I realize this question has been asked <a href=\"http://stackoverflow.com/questions/4563918/are-implied-do-loops-inneficient\">before</a>, but not in the context of IO.  Is there any reason to believe that:</p>\n\n<pre><code>!compiler can tell that it should write the whole array at once?\n!but perhaps compiler allocates/frees temporary array?\nwrite(UNIT) (/( arr(i), i=1,N )/)\n</code></pre>\n\n<p>would be any more efficient than:</p>\n\n<pre><code>!compiler does lots of IO here?\ndo i=1,N\n   write(UNIT) arr(i)\nenddo\n</code></pre>\n\n<p>for a file which is opened as:</p>\n\n<pre><code>open(unit=UNIT,access='STREAM',file=fname,status='UNKNOWN')\n</code></pre>\n\n<p>There is a possibly that this will be used with compiler options to turn off buffered writing as well ...</p>\n"},{"tags":["performance","hashtable"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":62,"score":1,"question_id":12631413,"title":"How to select a good hashing function (for a hashtable)","body":"<p>I was wondering how one would best approach the task of deciding upon the operations a hashing function should perform on it's input, based on the probable input format of course.</p>\n\n<p>Are there any rule(book)s i have yet to find?</p>\n\n<p>How could i estimate the cost of such a function?</p>\n\n<p>Can i somehow foresee the likelihood of collisions knowing the charset used for inputs?</p>\n\n<p>Thanks for your food for my thought in advance. :)</p>\n"},{"tags":["wcf","performance","concurrency","instance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":33,"score":1,"question_id":13015000,"title":"Single Instance VS PerCall in WCF","body":"<p>There are a lot of posts saying that <code>SingleInstance</code> is a bad design. But I think it is the best choice in my situation.<br>\nIn my service I have to return a list of currently logged-in users (with additional data). This list is identical for all clients. I want to retrieve this list from database every 5 seconds (for example) and return a copy of it to the client, when needed.<br>\nIf I use <code>PerCall</code> instancing mode, I will retrieve this list from database every single time. This list is supposed to contain ~200-500 records, but can grow up to 10 000 in the future. Every record is complex and contains about 10 fields. </p>\n\n<p>So what about performance? Is it better to use \"bad design\" and get list once or to use \"good approach\" and get list from database on every call?</p>\n"},{"tags":["c#",".net","performance","linq","optimization"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":98,"score":0,"question_id":13014357,"title":"Optimizing A LINQ To Objects Query","body":"<p>I'm trying to optimize the below LINQ query to improve it's speed performance.  The number of objects it's searching against could be in the tens of thousands.</p>\n\n<pre><code>var lQuery = from o in oEvents\nwhere (o.oSalesEvent != null &amp;&amp; o.oSalesEvent.OccurDate &lt; oCalcMgr.OccurDate &amp;&amp; (\n                (oCalcMgr.InclTransTypes == Definitions.TransactionTypes.SalesAll) ?\n                   (o.oSalesEvent.EventStateID == ApprovedID || o.oSalesEvent.EventStateID == PendingID) : \n                   o.oSalesEvent.EventStateID == ApprovedID)) &amp;&amp;\n      ((oCalcMgr.InclTransTypes == Definitions.TransactionTypes.SalesAll) ? \n                (o.oSalesMan.oEmployment.EventStateID == ApprovedID || o.oSalesMan.oEmployment.EventStateID == PendingID) : \n                 o.oSalesMan.oEmployment.EventStateID == ApprovedID)\nselect new { SaleAmount = o.SaleAmount.GetValueOrDefault(), CompanyID = o.oSalesEvent.CompanyID };\n</code></pre>\n\n<p>The query basically says, give me the sales amounts and company ids from all sale events that occurred prior to a certain date.  The sale event's status and the salesman's employment status should either always be \"approved\" or they can be also \"pending\" if specified.</p>\n\n<p>As you can see there's a date comparison and a couple of integer comparisons.  Which integer comparison used is based on whether or not a property matches a certain Enum value.</p>\n\n<p>I have some ideas of my own on ways to go about the optimization, but I want to hear others thoughts, who might have more insight into how LINQ would translate this query behind the scenes.</p>\n\n<p>Thanks</p>\n"},{"tags":["web-services","performance","rest","soap"],"answer_count":7,"favorite_count":4,"up_vote_count":6,"down_vote_count":0,"view_count":4816,"score":6,"question_id":4163066,"title":"Rest vs. Soap. Has REST a better performance?","body":"<p>I read some questions already posted here regarding Soap and Rest\nand I didn't find the answer I am looking for.\nWe have a system which has been built using Soap web services.\nThe system is not very performant and it is under discussion\nto replace all Soap web services for REST web services.\nSomebody has argued that Rest has a better performance.\nI don't know if this is true. (This was my first question)\nAssuming that this is true, is there any disadvantage using \nREST instead of Soap? (Are we loosing something?)</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["javascript","jquery","performance","single-page-application"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":84,"score":0,"question_id":7752781,"title":"<script> tag at bottom causing $(document).ready to fail","body":"<p>I was following the snippets from html5boilerplate.com (as well as something that is recommended by yahoo) that we should put the scripts at the bottom of the page (of my site's main page).</p>\n\n<p>I am also following SPI (Single page interface) - meaning I will load the content area only via ajax for any new page visit.</p>\n\n<p>My page structure is..</p>\n\n<pre><code>&lt;body&gt;\n&lt;header /&gt;\n&lt;div id=\"content\" /&gt;\n&lt;footer /&gt;\n&lt;/body&gt;\n</code></pre>\n\n<p>Now issue is ..\nI load JQuery at the end of main page. But I need some javascript to get executed on the child page which would be loaded in content div. So if I write \n    <code>$(document).ready(..)</code>\nin the child page, it will crib about \"$ is not defined\".</p>\n\n<p>Any suggestions?</p>\n"},{"tags":["java","xml","performance","parsing"],"answer_count":10,"favorite_count":5,"up_vote_count":10,"down_vote_count":0,"view_count":15733,"score":10,"question_id":530064,"title":"Fastest XML parser for small, simple documents in Java","body":"<p>I have to objectify very simple and small XML documents (less than 1k, and it's almost SGML: no namespaces, plain UTF-8, you name it...), read from a stream, in Java.</p>\n\n<p>I am using JAXP to process the data from my stream into a Document object. I have tried Xerces, it's way too big and slow... I am using Dom4j, but I am still spending way too much time in org.dom4j.io.SAXReader.</p>\n\n<p>Does anybody out there have any suggestion on a faster, more efficient implementation, keeping in mind I have very tough CPU <em>and</em> memory constraints?</p>\n\n<p>[Edit 1] Keep in mind that my documents are very small, so the overhead of <em>staring</em> the parser can be important. For instance I am spending as much time in org.xml.sax.helpers.XMLReaderFactory.createXMLReader as in org.dom4j.io.SAXReader.read</p>\n\n<p>[Edit 2] The result <em>has to</em> be in Dom format, as I pass the document to decision tools that do arbitrary processing on it, like switching code based on the value of arbitrary XPaths, but also extracting lists of values packed as children of a predefined node.</p>\n\n<p>[Edit 3] In any case I eventually need to load/parse the complete document, since all the information it contains is going to be used at some point.</p>\n\n<p>(This question is related to, but different from, <a href=\"http://stackoverflow.com/questions/373833/best-xml-parser-for-java\">http://stackoverflow.com/questions/373833/best-xml-parser-for-java</a> )</p>\n"},{"tags":["objective-c","ios","performance","cocoa-touch","core-data"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":36,"score":0,"question_id":13014474,"title":"Using a cache, populating it at launch, and optimizing launch time","body":"<p>I have an <code>NSManagedObject</code> 'Recipe' with an unordered, to-many relationship to <code>items</code>. I often need the recipe's items in a specific order, so I added some methods to <code>Recipe</code> - <code>itemsByName</code>, <code>itemsByDate</code>, etc. These use a <code>sortDescriptor</code> to sort everything in the <code>items</code> set and return an <code>NSArray</code>.</p>\n\n<p>Accessing <code>sortedArrayUsingDescriptors</code> during runtime was proving to be a noticeable bottleneck in my UI, so I added a caching mechanism. <code>Recipe</code> now has an iVar for each sorted array: <code>_cachedItemsByName</code>, <code>_cachedItemsByDate</code>, etc. If the cache exists (and is not flagged as dirty), the <code>itemsByName</code> method returns the iVar instead of re-sorting the set each time.</p>\n\n<p>This worked fine once my application had been running a while, but still lead to noticeable UI issues each time a recipe's ordered items was requested for the first time (since the cache had to be created using <code>sortedArrayUsingDescriptors</code>). So, I decided that when the application launches, it should update the cache for all of my recipes immediately - sacrificing load time for a better in-app experience. This has worked very well and my UI issues are now resolved.</p>\n\n<p>However, I would really like to chip away at that initial load time. But I'm not sure where to turn. I have considered the following options:</p>\n\n<ul>\n<li>On launch, update all of the caches in a background thread. After a few hours of research into GCD this seems like a non-starter. I've read that I can't interact with the NSManagedObjectContext or any of it's objects in a different thread (my processing just involves iterating through each <code>Recipe</code> object and telling it to update it's iVar. This involves sorting through it's <code>items</code> relationship).</li>\n<li>Just make <code>items</code> an ordered relationship so that the order doesn't have to be calculated at run time. But I have multiple orders in which I need to return the items, so I would need multiple relationships to the same objects, just in different orders. This seems icky - I would be responsible for keeping each <code>NSOrderedSet</code> up to date. For example, when an item is added, I would have to insert it multiple times into different relationships. Also, I have read that the NSOrderedSet introduces it's own performance overhead.</li>\n<li>Some other way to persist the cache - I don't really know where to start on this one. Perhaps write each Recipe's cache to file somehow, and then try to match them up at load time? I'm not sure this would be any quicker than re-sorting each recipe on launch, but I would be willing to try it out if someone could suggest how and where to store this.</li>\n</ul>\n\n<p>I would love some feedback on each of the options above, and even better, some alternatives. I am pleased with my solution for improving in-app performance, but would really like to speed up my initial launch.</p>\n"},{"tags":["sql","sql-server","performance","query"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":63,"score":1,"question_id":12558483,"title":"SQL query slow calculating cumulative totals","body":"<p>We are storing data readings at 5 minute intervals for a large number of gauges.</p>\n\n<p>The data tables involved are:</p>\n\n<pre><code>Table1 - GaugeData\nColumns - \n    GaugeID (int, primary key)\n    Timestamp (datetime, primary key)\n    Value (decimal)\n\nTable2 - GaugeSummaryData\nColumns - \n    GaugeID (int, primary key)\n    DayTimestamp (date, primary key)\n    DayTotal (decimal) - total for the current date/day\n    CumulativeTotal (decimal) - total up to and including the current date\n</code></pre>\n\n<p>Without changing the table structure in any way, what would be the most efficient way to copy and aggregate data from GaugeData into GaugeSummaryData?</p>\n\n<p>I have attempted this two ways already. Using a cursor takes 40 minutes to copy all data from GaugeData to GaugeSummaryData. Using insert/update statements took 2hrs+.</p>\n\n<p>Could somebody please suggest the most efficient way? Pseudocode or SQL appreciated.</p>\n"},{"tags":["mysql","database","performance","optimization","subquery"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":50,"score":2,"question_id":13012989,"title":"Slow MySQL query with AS and subquery","body":"<p>I have a problem with this slow query that runs for 10+ seconds:</p>\n\n<pre><code>SELECT DISTINCT siteid,\n                storyid,\n                added,\n                title,\n                subscore1,\n                subscore2,\n                subscore3,\n                ( 1 * subscore1 + 0.8 * subscore2 + 0.1 * subscore3 ) AS score\nFROM   articles\nWHERE  added &gt; '2011-10-23 09:10:19'\n       AND ( articles.feedid IN (SELECT userfeeds.siteid\n                                 FROM   userfeeds\n                                 WHERE  userfeeds.userid = '1234')\n              OR ( articles.title REGEXP '[[:&lt;:]]keyword1[[:&gt;:]]' = 1\n                    OR articles.title REGEXP '[[:&lt;:]]keyword2[[:&gt;:]]' = 1 ) )\nORDER  BY score DESC\nLIMIT  0, 25 \n</code></pre>\n\n<p>This outputs a list of stories based on the sites that a user added to his account. The ranking is determined by score, which is made up out of the subscore columns. </p>\n\n<p>The query uses filesort and uses indices on PRIMARY and feedid.\nResults of an EXPLAIN:</p>\n\n<pre><code>1   PRIMARY articles    \nrange   \nPRIMARY,added,storyid   \nPRIMARY  729263 rows    \nUsing where; Using filesort\n\n2   DEPENDENT SUBQUERY  \nuserfeeds   \nindex_subquery  storyid,userid,siteid_storyid   \nsiteid  func    \n1 row   \nUsing where\n</code></pre>\n\n<p>Any suggestions to improve this query? Thank you.</p>\n"},{"tags":["c++","c","performance","optimization"],"answer_count":12,"favorite_count":2,"up_vote_count":14,"down_vote_count":1,"view_count":771,"score":13,"question_id":12982884,"title":"C/C++ optimizing away checks to see if a function has already been run before","body":"<p>Let's say you have a function in C/C++, that behaves a certain way the first time it runs. And then, all other times it behaves another way (see below for example). After it runs the first time, the if statement becomes redundant and could be optimized away if speed is important.  Is there any way to make this optimization?</p>\n\n<pre><code>bool val = true; \n\nvoid function1() {\n\n   if (val == true) {\n      // do something\n      val = false; \n   }\n   else {\n      // do other stuff, val is never set to true again \n   }\n\n}\n</code></pre>\n"},{"tags":["css","performance","netbeans","syntax-highlighting","bug"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":27,"score":1,"question_id":13013247,"title":"Netbeans Syntax Highlighting Lag","body":"<p>I'm working on a pretty big project that uses a CMS and whenever I'm editing a file with a lot of lines of code (especially in a CSS file), the syntax highlighting takes a couple of seconds after I'm finished typing to change colors.</p>\n\n<p>Netbeans feels a lot faster than Eclipse, but why is this bug persisting.</p>\n\n<p>Anyone else having this issue?</p>\n"},{"tags":["android","performance","andengine"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":95,"score":0,"question_id":12998860,"title":"Andengine - Always keep the same speed of the body","body":"<p>I am developing a game where you are on side scroll on screen middle (player) and enemyes appears from left or right continusly.</p>\n\n<p>I create a enemy, set a Linearvelocity to right (or left) on this way:\ngetBody.setLinearVelocity(v*this.getDireccion(), 0);</p>\n\n<p>Then, when the enemy collides with another enemy or player, sometimes change direction (he slides back) or get more slow or fast. I need that always have the same LinearVelocity, and if this enemy is colliding with a player or another monster, dont stop, dont slow, dont fast... same velocity and direction.</p>\n\n<p>Anybody can help me with this? \nSorry my english.</p>\n"},{"tags":["c#","performance","cpu-usage","wait","performance-testing"],"answer_count":4,"favorite_count":3,"up_vote_count":3,"down_vote_count":0,"view_count":134,"score":3,"question_id":13001578,"title":"I need a slow C# function","body":"<p>For some testing I'm doing I need a C# function that takes around 10 seconds to execute. It will be called from an ASPX page, but I need the function to eat up CPU time on the server, not rendering time. A slow query into the Northwinds database would work, or some very slow calculations. Any ideas?</p>\n"},{"tags":["c#",".net","performance","optimization","file-size"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":190,"score":4,"question_id":7583688,"title":"Is there a way to get the size of a file in .NET using a static method?","body":"<p>I know the normal way of getting the size of a file would be to use a FileInfo instance:</p>\n\n<pre><code>using System.IO;\nclass SizeGetter\n{\n  public static long GetFileSize ( string filename )\n  {\n    FileInfo fi = new FileInfo ( filename );\n    return fi.Length;\n  }\n}\n</code></pre>\n\n<p>Is there a way to do the same thing without having to create an instance of FileInfo, using a static method? Maybe I'm trying to be overly stingy with creating a new instance every time I want a file size, but take for example trying to calculate the total size of a directory containing 5000+ files. As optimized as the GC may be, shouldn't there be a way to do this without having to tax it unnecessarily?</p>\n"},{"tags":["visual-studio-2010","performance","stackframe"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":26,"score":0,"question_id":13006371,"title":"Does omitting the frame pointers really have a positive effect on performance and a negative effect on debug-ability?","body":"<p>As was advised long time ago, I always build my release executables without frame pointers (which is the default if you compile with /Ox).</p>\n\n<p>However, now I read in the paper <a href=\"http://research.microsoft.com/apps/pubs/default.aspx?id=81176\" rel=\"nofollow\">http://research.microsoft.com/apps/pubs/default.aspx?id=81176</a>, that frame pointers don't have much of an effect on performance.  So optimizing it fully (using /Ox) or optimizing it fully with frame pointers (using /Ox /Oy-) doesn't really make a difference on peformance.</p>\n\n<p>Microsoft seems to indicate that adding frame pointers (/Oy-) makes debugging easier, but is this really the case?</p>\n\n<p>I did some experiments and noticed that:</p>\n\n<ul>\n<li>in a simple test executable (compiled using /Ox /Ob0) the omission of frame pointers does increase performance (with about 10%).  But this test executable only performs some function calls, nothing else.</li>\n<li>in my own application the adding/removing of frame pointers don't seem to have a big effect.  Adding frame pointers seems to make the application about 5% faster, but that could be within the error margin.</li>\n</ul>\n\n<p>What is the general advice regarding frame pointers?</p>\n\n<ul>\n<li>should they be omitted (/Ox) in a release executable because they really have a positive effect on performance?</li>\n<li>should they be added (/Ox /Oy-) in a release executable because they improve debug-ablity (when debugging with a crash-dump file)?</li>\n</ul>\n\n<p>Using Visual Studio 2010.</p>\n"},{"tags":["ios","performance","uiscrollview","photo-gallery"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":57,"score":1,"question_id":13011128,"title":"UIScrollView: Bad performance with large images","body":"<p><strong>TL:DR</strong></p>\n\n<p>What technique does Apple use to make Photo.app so fast, even with large images?</p>\n\n<p><strong>Long Version</strong></p>\n\n<p>I watched Apple's WWDC 2010 video about scroll views to learn how to replicate Photo.app pagination behavior and low memory utilization (PhotoScroller Demo). It works well, but since images are loaded only when they are needed, when I try to paginate to another image, the app locks while the JPEG is being decompressed.</p>\n\n<p>The same video shows a tiling technique to get better performance, but since I'm using photos taken from the camera and stored in the app, that doesn't seem feasible (having multiple copies of each photo, in different resolutions, would consume too much space - 4MB vs 27MB). Also, using iExplorer I noticed Photo.apps has only a copy of each photo (it doesn't even have a small thumbnail copy for the gallery).</p>\n\n<p>What technique did Apple use to make Photos.app so fast? How can I get that same performance in my app?</p>\n\n<blockquote>\n  <p>I'm a bit confused if this should be here or on Programmers,\n  since there's no code in the question, but F.A.Q. says that algorithm\n  questions are part of Stackoverflow, and the tags here match it\n  better.</p>\n</blockquote>\n"},{"tags":["database","performance","ssis","etl"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":2445,"score":0,"question_id":809119,"title":"How to Handling Incremental Load with large datasets ssis","body":"<p>I have 2 tables (~ 4 million rows) that I have to do insert/update actions on matching and unmatching records. I am pretty confused about the method I have to use for incremental load. Should I use Lookup component or new sql server merge statement? and will there be too much performance differences?</p>\n"},{"tags":["performance","function","parameters","call"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":4,"view_count":38,"score":-3,"question_id":13008832,"title":"what is faster: one call to a function with many parameters or more calls to functions with fewer parameters?","body":"<p>I have 9 buffers that I need to send over CAN. I have a function that has 9 parameters, representing the code for which variables I want to have in each buffer. The function contains 9 switch-case. The question is: will this be faster if I have 3 functions each with 3 parameters? So instead of calling one function with 9 parameters, it's faster to call 3 functions with 3 parameters?</p>\n"},{"tags":["iphone","performance","graphics","opengl-es"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":1484,"score":2,"question_id":2412169,"title":"How do you represent a normal or texture coordinate using GLshorts?","body":"<p>A lot of suggestions on improving the performance in iPhone games revolve around sending less data to the GPU. The obvious suggestion is to use GLshorts instead of GLfloat wherever possible, such as for vertices, normals, or texture coordinates.</p>\n\n<p>What are the specifics when using a GLshort for a normal or a texture coordinate? Is it possible to represent a GLfloat texture coordinate of 0.5 when using a GLshort? If so, how do you do that? Would it just be SHRT_MAX/2? That is, does the range of 0 to 1 for a GLfloat map to 0 to SHRT_MAX when using a GLshort texture coordinate?</p>\n\n<p>What about normals? I've always created normals with GLfloats and normalized them to unit length. When using a GLshort for a normal, are you sending a non-normalized vector to the GPU? If so, when and how is it normalized? By dividing by all components by SHRT_MAX?</p>\n"},{"tags":["performance","algorithm","data-structures"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":63,"score":2,"question_id":13009179,"title":"Data Structure for storing ranges of values that allows efficient comparision operation","body":"<p>I am looking for a data structure that allows storing non-overlapping ranges of integers\nand comparing whether a certain range exists[is covered] in[by] the data structure.</p>\n\n<p>For example, after I store (0,9),(10,19),(30,29), \nat some point later I want to check if the range (1,11) is covered, in which case the\nalgorithm gives a \"yes\" whereas with the range (15,25) the algorithm gives a \"no\" as the given range is not covered.</p>\n\n<p>Many thanks in advance.</p>\n"},{"tags":["php","performance","oop","parameters","instance-variables"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":64,"score":1,"question_id":13007658,"title":"PHP OOP Performance : Parameter or Instance Variable?","body":"<p>Im curious about getting an answer for a OOP question but can't find any info so far.</p>\n\n<p>Here goes, writing classes and methods is it faster to pass in parameters for each method or use instance/field variables and $this->x;</p>\n\n<p>Which would be faster at run time?</p>\n\n<pre><code>class ExampleByParameter(){\n\n function SomeMethod($a,$b){\n echo $a.\" \".$b;\n return;\n }\n\n}\n</code></pre>\n\n<p>or</p>\n\n<pre><code>class ExampleByInstance(){\n\n function __construct($a,$b){\n $this-&gt;a=$a;\n $this-&gt;b=$b;\n }\n\n function SomeMehtod(){\n $a=$this-&gt;a;\n $b=$this-&gt;b;\n echo $a.\" \".$b;\n return;\n }\n\n}\n</code></pre>\n\n<p>I would imagine their would be no difference with the examples above but im thinking there might be a significant difference with more complicated code. </p>\n"},{"tags":[".net","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":36,"score":1,"question_id":13008833,"title":"Impact of declaring multiple classes","body":"<p>In my .Net application (MVVM application) i have let's say 10 drop down lists. So i created 10 different classes(Models) containing just Name , Value pair. The reason of creating 10 different classes is just because these drop down lists are functionally independent.</p>\n\n<p>What are the pros and cons (Including memory impact) of 10 different classes instead of just 1 class (Which contains Name Value Pair and Bound to View through ViewModel)</p>\n"},{"tags":["asp.net",".net","performance","sql-server-2005"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":52,"score":0,"question_id":13007716,"title":"Performance of web application .net 2.0","body":"<p>I have just started working over a website written in .net 2.0. Pages take long to load and response time is quite low, not sure where to start from in order to improve performance of the same.</p>\n\n<p>Hardware is not a problem as there is enough memory and processor is also good enough.</p>\n\n<p>Any Idea where should I start from and to improve the performance.</p>\n"},{"tags":["performance","http","file-upload","upload","ftp"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":41,"score":0,"question_id":13006726,"title":"Why am I getting extremely slow upload speeds via http vs ftp?","body":"<p>I'm getting 10mps upload speeds to my server using ftp but when I use http protocol my upload speeds are around 300kpbs, which is about 3% of the ftp file upload speed.  I've heard that http protocol upload speed is about the same as ftp protocol upload speed so why is there such a huge difference when I try to use a script to upload files through a web browser?  </p>\n\n<p>I wanted to create a user content generated site where users can upload massive files, like 1GB or more and I don't want to put my users through a lot of pain by making them use an ftp client, so I'd rather have them upload files through my site after they login to their member area.  Is this an issue with the server itself that is limiting the speed of file uploads when the http protocol is used?  Please help.</p>\n"},{"tags":["c#","asp.net","performance","iis","iis-7.5"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":60,"score":-1,"question_id":13006499,"title":"Measure ASP.NET Website Server Side Executing Time","body":"<p>I've a website hosted on windows hosting IIS 7.5. How can I measure server side (C#) executing time?.. Is it possible to get it without IIS?</p>\n\n<p>I need some tools, software or online tools</p>\n"},{"tags":["performance","drupal","views","drupal-theming"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":1850,"score":1,"question_id":9914220,"title":"How to theme views fields in Drupal 7 correctly","body":"<p>I need to theme views in Drupal 7. There is a content type 'Book' and I need to list 5 books and theme them in special manner(preview image, title and author).</p>\n\n<p>When I override views-view-field.tpl.php and print raw SQL result, I see that all fields are displayed. This code</p>\n\n<pre><code>echo \"&lt;pre&gt;\";\nprint_r($row);\necho \"&lt;/pre&gt;\";\n</code></pre>\n\n<p>gives</p>\n\n<pre><code>[entity] =&gt; stdClass Object\n (\n  [title] =&gt; ...\n  ....\n  [nid] =&gt; 34\n  ...\n  [body] =&gt; Array\n  ...\n</code></pre>\n\n<p>But I don't want pass [body] from database to php side, because it can be huge and cause a performance issue. I haven't selected [body] in view settings. </p>\n\n<p>Is there a way to pass only certain fields to views-view-field.tpl.php?</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["performance","big-o","worst-case"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":36,"score":1,"question_id":13002478,"title":"Nested for loops running time calculating","body":"<p>How can I calculate worst case running time of this loop?</p>\n\n<pre><code>for(int i=1 ; i * i &lt; n ; i*=2)\n\n{\n     //do something\n}\n</code></pre>\n"},{"tags":["performance","actionscript-3","flash","variables","definition"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":55,"score":1,"question_id":12986990,"title":"Does defining variables within a loop matter?","body":"<p>This is a piece of code I am writing.</p>\n\n<pre><code>var cList:XMLList = xml.defines.c;\nvar className:String;\nvar properties:XMLList;\nvar property:XML;\nvar i:int,l:int;\nvar c:XML;\n\nfor each(c in cList)\n{\n    className = String(c.@name);\n\n    if(cDict[className])\n    {\n        throw new Error('class name has been defined' + className);\n    }\n\n    if(className)\n    {\n        cDict[className] = c;\n    }\n\n    properties = c.property;\n\n    i = 0,\n    l = properties.length();\n\n    if(l)\n    {\n        propertyDict[className] = new Dictionary();\n\n        for(;i&lt;l;i++)\n        {\n            // ...\n        }\n    }\n}\n</code></pre>\n\n<p>As you can see, I defined all variables outside of loops. I am always worried, that if I defined them inside the loop, it might slow down the process speed, though I don't have proof - it's just a feeling.</p>\n\n<p>I also don't like that the as3 grammar allows using a variable name before the defintion. So I always define vars at the very beginning of my functions. </p>\n\n<p>Now I am worried these habits might backfire on me someday. Or is it just a matter of personal taste?</p>\n"},{"tags":["php","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":60,"score":1,"question_id":13003340,"title":"Will the code after false \"if\" statements be compiled","body":"<p>if I have code like that</p>\n\n<pre><code>if($abc==true) {\n   //code code code\n} else {\n   // other code, other code\n}\n</code></pre>\n\n<p>So if <code>$abc</code> is <code>true</code> will the \"other code\" be compiled?</p>\n\n<p>So is this irrelevant if there is a lot of \"code\" performance wise?</p>\n"},{"tags":["sql-server","performance","entity-framework","multi-tenant","parameter-sniffing"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":81,"score":1,"question_id":12979493,"title":"Multi-tenant SQL Server databases and parameter sniffing","body":"<p>I have a multi-tenant database in SQL Server 2012 where each tenant's rows are identified by a <code>tenant_id</code> column (aka the <a href=\"http://msdn.microsoft.com/en-us/library/aa479086.aspx\" rel=\"nofollow\">Shared Database, Shared Schema</a> approach). Some tenants, particularly the newer ones, have very few rows, while others have many.</p>\n\n<p>SQL Server's query optimizer normally builds a query plan based on the parameters provided during its first execution, then re-uses this plan for all future queries even if different parameters are provided. This is known as <a href=\"http://www.sommarskog.se/query-plan-mysteries.html\" rel=\"nofollow\">parameter sniffing</a>.</p>\n\n<p>The problem we have with our database is that SQL Server sometimes builds these plans based on parameters that point to a smaller tenant, which works fine for that tenant, but then when it reapplies the cached plan to a larger tenant it fails catastrophically (usually timing out, in fact). Typically we find out about this situation only when one of our larger tenants contacts us about experiencing time-out errors, then we have to get into the system and manually flush all the query plans to correct it.</p>\n\n<p>There is a query hint you can use to prevent SQL Server from caching query plans (<a href=\"http://www.benjaminnevarez.com/2010/06/how-optimize-for-unknown-works/\" rel=\"nofollow\"><code>OPTIMIZE FOR UNKNOWN</code></a>) but this results in some extra overhead since the query plan is being regenerated every time the query is called. An additional problem is that we're using Entity Framework which offers no ability to specify the <code>OPTIMIZE FOR UNKNOWN</code> hint on queries.</p>\n\n<p>So the question is -- what is the best practice for multi-tenant databases with regard to parameter sniffing? Is there a way to disable parameter sniffing database-wide without having to specify it on every query? If so, is that even the best approach? Should I be partitioning the data in some other way? Is there some other approach I'm not thinking of?</p>\n"},{"tags":["java","string","performance","file-io","concatenation"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":275,"score":1,"question_id":9107097,"title":"Java efficient file writing: String concatentation versus extra call to write()","body":"<p>In the code below, which case (1 or 2) is more \"efficient\"?</p>\n\n<pre>\nstatic final String NEWLINE = System.getProperty(\"line.separator\");\nVector text_vec = ...;\nFileWriter file_writer = new FileWriter(path);\nBufferedWriter buffered_writer = new BufferedWriter(file_writer);\ntry {\n    for (String text: text_vec) {\n\n        // Case 1: String concatenation\n        buffered_writer.write(text + NEWLINE);\n\n        // Case 2: Extra call to write()\n        buffered_writer.write(text);\n        buffered_writer.write(NEWLINE);\n    }\n}\nfinally {\n    buffered_writer.close();\n}\n</pre>\n\n<p>In case #1, as I understand, the String concatenation is handled by the Java compiler by automatically allocating a StringBuilder object.  Since the String values are not known at compile time, it is not possible to concatenate \"early\" (during compile time).</p>\n\n<p>So the question stands: Which one is more efficient (CPU/memory/wall clock time)?</p>\n\n<p>I am leaving the exact definition of \"efficient\" to those whom answer.  I am not an expert on the Java virtual machine.  I hope others can enlighten!</p>\n\n<p>Thanks.</p>\n"},{"tags":["performance","cocos2d","collision"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":12996761,"title":"Cocos2d fast collision detection with many sprites","body":"<p>I have a LOT of sprites (shooting game) and I need to test collision with many other objects in the level (Player, walls, crates etc...) and it's working fine if I brute force the CGRectIntersectsRect method. The problem is very apparent with this method as I'm checking every bullet against every object every frame. I know a bit about how to speed things up but I wanted to get some more experience game dev's insight (perhaps cocos2d specific) before I spend a couple days implementing some sort of spacial partitioning hierarchy.</p>\n\n<p>Any help would be greatly appreciated!</p>\n"},{"tags":["eclipse","tips-and-tricks","performance"],"answer_count":23,"favorite_count":226,"up_vote_count":292,"down_vote_count":0,"view_count":142198,"score":292,"question_id":316265,"title":"Tricks to speed up Eclipse","body":"<p>Which tricks do you know to make the experience with Eclipse faster?  </p>\n\n<p>For instance: I disable the all the plugins I don't need (Mylyn, Subclipse, &hellip;).</p>\n\n<p>Instead of using a plugin for Mercurial I configure TortoiseHG as an external tool.</p>\n"},{"tags":["javascript","css","performance","image","sprite"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":92,"score":1,"question_id":13000650,"title":"Any good alternatives to CSS sprites?","body":"<p>I'm working on an app that needs to load a lot of icons. At the moment I'm using <code>&lt;img&gt;</code> tags with the <code>src</code>'s set to the right URLs. That triggers a lot of request. Even if caching is properly set up there are still a lot of HEAD requests.</p>\n\n<p>I'm looking for a solution that will trigger only a minimum of requests (best would be 1) to receive all of the needed icons. But I don't like the concept of spriting, since it's harder to change/replace/add icons in this concept.</p>\n\n<p>Are there any other solutions?</p>\n\n<p>For example, would it be to slow to base64-encode all images to one file on the server, send them to the browser, split them with JavaScript and set base64 src's to the img tags?</p>\n\n<p>Does anyone test this or different approaches?</p>\n"},{"tags":["iphone","xcode","performance","unity3d"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":437,"score":0,"question_id":4558888,"title":"Xcode debugging is slow!","body":"<p>I'm trying to debug an Unity3D iPhone app using Xcode and it's ultra slow. </p>\n\n<p>The application is running smoothly but the Xcode environment crawls to a halt. I can barely get to the debug menu and turn terminate it. I'm running this on a 2GHz, 3GB RAM machine, and Xcode isn't even hogging any cpu in the activity monitor. Any ideas to what might be causing this?</p>\n\n<p>Regards/Per</p>\n"},{"tags":["c++","performance","boost","c++11","std-function"],"answer_count":2,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":270,"score":7,"question_id":11126238,"title":"Have the ideas behind the Fast Delegate (et al) been used to optimize std::function?","body":"<p>There have been proposals for C++ \"delegates\" which have lower overhead than <code>boost::function</code>:</p>\n\n<ul>\n<li><a href=\"http://www.codeproject.com/Articles/7150/Member-Function-Pointers-and-the-Fastest-Possible\" rel=\"nofollow\">Member Function Pointers and the Fastest Possible C++ Delegates</a></li>\n<li><a href=\"http://www.codeproject.com/Articles/13287/Fast-C-Delegate\" rel=\"nofollow\">Fast C++ Delegate</a></li>\n<li><a href=\"http://www.codeproject.com/Articles/11015/The-Impossibly-Fast-C-Delegates\" rel=\"nofollow\">The Impossibly Fast C++ Delegates</a></li>\n</ul>\n\n<p>Have any of those ideas been used to implement <code>std::function</code>, resulting in better performance than <code>boost::function</code>? Has anyone compared the performance of <code>std::function</code> vs <code>boost::function</code>?</p>\n\n<p><strong>I want to know this specifically for the GCC compiler and libstdc++ on Intel 64-bit architectures</strong>, but information on other compilers is welcome (such as Clang).</p>\n"},{"tags":["arrays","performance","scala","map","for-loop"],"answer_count":1,"favorite_count":0,"up_vote_count":5,"down_vote_count":1,"view_count":153,"score":4,"question_id":12999680,"title":"Poor performance of Array.map(f: A => B) in Scala","body":"<p>Please can someone explain to me, why Array.map(f: A=> B) method is implemented in a such way that it is more than 5 times slower than this code:</p>\n\n<pre><code>val list = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)\nval newList = new Array[Int](size)\n\nvar j = 0  \nwhile (j &lt; size) {\n  newList(j) = list(j)\n  j += 1\n}\n</code></pre>\n\n<p>The method map(f: A=> B) in the Array class, which is provided by TraversableLike trait, uses Scala 'for loop' for iterating over the elements of input Array object, which of course is much slower than using 'while loop'.</p>\n\n<p>Scala version: 2.9.2\nJava: jdk1.6.0_23 64bit windows</p>\n"},{"tags":["php","mysql","performance","comparison"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":206,"score":2,"question_id":8473756,"title":"Compare string with values from mysql","body":"<p>I have a database containing the name of bands and other artists related to music. Now i want to check a string containing an artist name against this database and find similar or equal artists to avoid different kind of spelling.</p>\n\n<p>I found the php function 'similar_text' and i am sure, it is no problem to build a script to do this comparison during a loop.</p>\n\n<p>What would be the best and fastest way to do such a comparison?</p>\n"},{"tags":["python","regex","performance","python-2.6","substitution"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":85,"score":0,"question_id":12982805,"title":"Python Speeding Up Retrieving data from extremely large string","body":"<p>See Down farther:</p>\n\n<p>I have a list I converted to a very very long string as I am trying to edit it, as you can gather it's called tempString. It works as of now it just takes way to long to operate, probably because it is several different regex subs. They are as follow:</p>\n\n<pre><code>tempString = ','.join(str(n) for n in coords)\ntempString = re.sub(',{2,6}', '_', tempString)\ntempString = re.sub(\"[^0-9\\-\\.\\_]\", \",\", tempString)\ntempString = re.sub(',+', ',', tempString)\nclean1 = re.findall(('[-+]?[0-9]*\\.?[0-9]+,[-+]?[0-9]*\\.?[0-9]+,'\n                 '[-+]?[0-9]*\\.?[0-9]+'), tempString)\ntempString = '_'.join(str(n) for n in clean1)\ntempString = re.sub(',', ' ', tempString)\n</code></pre>\n\n<p>Basically it's a long string containing commas and about 1-5 million sets of 4 floats/ints (mixture of both possible),:</p>\n\n<pre><code>-5.65500020981,6.88999986649,-0.454999923706,1,,,-5.65500020981,6.95499992371,-0.454999923706,1,,,\n</code></pre>\n\n<p>The 4th number in each set I don't need/want, i'm essentially just trying to split the string into a list with 3 floats in each separated by a space.</p>\n\n<p>The above code works flawlessly but as you can imagine is quite time consuming on large strings.</p>\n\n<p>I have done a lot of research on here for a solution but they all seem geared towards words, i.e. swapping out one word for another.</p>\n\n<hr>\n\n<p><strong>EDIT:</strong>\nOk so this is the solution i'm currently using:</p>\n\n<pre><code>def getValues(s):\n    output = []\n    while s:\n        # get the three values you want, discard the 3 commas, and the \n        # remainder of the string\n        v1, v2, v3, _, _, _, s = s.split(',', 6)\n        output.append(\"%s %s %s\" % (v1.strip(), v2.strip(), v3.strip()))         \n    return output\ncoords = getValues(tempString)\n</code></pre>\n\n<p>Anyone have any advice to speed this up even farther? After running some tests It still takes much longer than i'm hoping for.</p>\n\n<p>I've been glancing at numPy, but I honestly have absolutely no idea how to the above with it, I understand that after the above has been done and the values are cleaned up i could use them more efficiently with numPy, but not sure how NumPy could apply to the above.</p>\n\n<p>The above to clean through 50k sets takes around 20 minutes, I cant imagine how long it would be on my full string of 1 million sets. I'ts just surprising that the program that originally exported the data took only around 30 secs for the 1 million sets</p>\n"},{"tags":["performance","views","multiple-databases","cross-join"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":12983389,"title":"Need some advice on best way to write Large reporting query from a fairly complex source","body":"<p>After searching through other answers to see if this particular question has already been asked (and other flavours have been asked - just not covering off everything I need to enquire about), I would like to pose the following scenario and ask for advise on the most efficient way to create this reporting query. This is a verbose post and I am not allowed to post any of the T-SQL code unfortunately - my employer has expressly forbid that. </p>\n\n<p>One thing I have learned over the years is that there are loads more people out there that know a heck of a lot more than me - so after searching and not finding what you need, just ask someone :)</p>\n\n<p>All tables have primary keys and if there are tables that link to child tables, there are foreign keys in place. The database and report server is SQL 2008 R2. The server runs 128GB RAM and is an 4CPU Quad core hyper threaded beastie.</p>\n\n<p>First, I have a set of tables that contain Locations, Sections and Areas. A Location is linked through a mapping table to a Section - there are multiple locations to a section, sections are linked to Areas through a mapping table and there are multiple sections to an Area. These table are all located in Database \"A\". I have created a view in a separate Database \"B\" that represents the Area - Section - Location linkages in a nice result set that executes quickly. I call this the LocationSectionArea View. </p>\n\n<p>In database \"B\", thousands of users are entering their daily statistics regarding all sorts of categories of their work. There are at present 91 categories. Each category will contain subcategories which vary from about 3 to as many as 25. The number of categories and subcategories can change at any time with more being added or removed through an administration interface. I have created a View that represents these links and returns a result set called the CategorySubCategoryFields view.</p>\n\n<p>The information being collected for each of these categories/Sub category links varies - some require about 8 different fields, while some require 3, and some only 1. These fields are linked to their appropriate sub categories. This is again contained in database \"B\".</p>\n\n<p>I have constructed a view that nicely pulls this data together in this database which ends up with a large matrix of results that presents the data in a way that can be reported on quite easily, and the views execution so far seems to be quite acceptable in speed. I call this the UserStatsView. This is contained in database \"B\".</p>\n\n<p>Now - the users entering the data are assigned to one of the locations mentioned earlier. The results are required to show all Locations within a Section, and all sections within an area, as well as for each location show each of the categories and sub categories for that category. This then needs to be linked to the results from the UserStatsView so you end up with a rather large matrix of these results intermingled with a lot of 0's where there are no results - but they are created for reporting purposes.</p>\n\n<p>The reports have to be allow a user to , for example, select a Section, and then produce a report of all Locations contained within that section, along with a tally of the results entered by users for that location - for each Category-SubCategory combination there is. It must present all locations for the section and all Categories-SubCategories regardless of if there are actual results entered by users or not. So there may be a lot of 0's on some of the reports.</p>\n\n<p>To achieve this I created a results view that cross joins the LocationSectionArea  with the CategorySubCategoryFields view. This created the basic matrix that I want. I then left join this matrix to the results of the UserStatsView, joining on the LocationId, CategoryId and SubCategoryId to insert the results for the users for the locations. This is all created in database \"B\".</p>\n\n<p>Now this all works and is \"ok\" provided you supply enough filters in the final where clause to reduce the number of records. As you can imaging - if they try to run a report with a larger results set - it starts to get very slow. (ie, date range covering 6 months for all locations takes longer than 30 seconds)</p>\n\n<p>Part of my problem is I believe that the Location information is linked in from database \"A\" and the view constructed is contained in database \"B\" along with all the views tables and records for all other information required for this report. </p>\n\n<p>Another part is I believe the very large and complex cross join created that is then linked to the view of users results. The cross join is a Cartesian result with no indexes or related data until it is joined with the UserStatsView result set.</p>\n\n<p>And the third problem I believe is the fact that data is being \"created\" to fill in all the blanks in order to produce the results structure that can then be fed into SSRS. I have seen that table spooling and hash joins take up a lot of the execution time, as well as lookups from the database with the locations, sections and Areas. These were all shown through the query execution estimated plan.</p>\n\n<p>What I am asking is if anyone knows if there is a better way of generating this result set given the criteria outlined above. Have I completely missed a really simple and faster way of doing this, which is highly probable :). If someone can suggest what I should research I will happily go off and do it - just not sure what to research at this stage?</p>\n\n<p>Cheers</p>\n\n<p>Rod.</p>\n\n<p>Update: the following is the table structure that defines the two databases,. Obviously there is more in the real tables, but this is the crux of how they hang together. Please excuse such a large post - I have removed everything except the key fields and some data fields so you can see how the tables are structured. The table names and field names have been changed to generic names with a lot of extra fields removed so I can post the code.</p>\n\n<hr>\n\n<p>Database A</p>\n\n<pre><code>**Location Table**\nLocationId BIGINT PK\nLocationName Varchar(100)\n\n\n**Section Table**\nSectionId Bigint PK\nSectionName varchar(100)\n\n\n\n**Area Table**\nAreaId Bigint PK\nAreaName varchar(100)\n\n\n\n**LocationSectionMap Table**\nLocSecId Bigint PK\nLocationId Bigint FK Index to Location Table\nSectionId BigInt FK Index to Section Table\n\n\n\n\n**SectionAreaMap Table**\nSecAreaId Bigint PK \nSectionId Bigint FK Index to Section Table\nAreaId Bigint FK Index to Area Table\n</code></pre>\n\n<hr>\n\n<p>Database B</p>\n\n<pre><code>**Categories Table** \nCategoryId Bigint PK\ncategoryName varchar(100)\n\n\n\n**SubCategories Table**\nSubCategoryId Bigint\nCategoryId Bigint FK to Categories Table \nSubCategoryType Int\nFieldTypeId Int (1, 2, 3 or 4)\n\n\n**UserStats Table**\nUserStatId bigint PK\nUserId Bigint\nStartDate DateTime\nEndDate DateTime\nLocationId Bigint --&gt; this is the location ID in Location table in database A\nSectionId Bigint --&gt; this is the Section ID in Section Table in database A\nAreaId Bigint --&gt; this is the Area ID of the Area Table in database A\n\n\n\n**FieldType1 Table**\nFieldType1Id bigint PK\nUserStatId  Bigint FK to UserStats Table\nSubCategoryId Bigint FK to Subcategories Table\nValue1 int\nValue2 int\n\n\n\n\n\n**FieldType2 Table**\nFieldType2Id bigint PK\nUserStatId  Bigint FK to UserStats Table\nSubCategoryId Bigint FK to Subcategories Table\nValue1 int \nValue2 int \nValue4 int\nValue5 int\nValue6 int\nValue7 int\nValue8 int\nValue9 int\nValue10 int\n\n\n**FieldType3 Table**\nFieldType3Id bigint PK\nUserStatId  Bigint FK to UserStats Table\nSubCategoryId Bigint FK to Subcategories Table\nValue1 int\nValue2 int\nValue11 int\nValue12 int\nValue13 int\nValue14 int\nValue15 int\nValue16 int\n\n\n**FieldType4 Table**\nFieldType4Id bigint PK\nUserStatId  Bigint FK to UserStats Table\nSubCategoryId Bigint FK to Subcategories Table\nCombinedValue1And2 int\n\n\n\n**SubCategoryAssociations Table**\nSubCategoryAssociationId Int PK\nReportOfSubCategoryId bigint FK to Subcategories Table\nIncludeValuesFromSubcategoryId Bigint FK to Subcategories Table\n</code></pre>\n\n<p>Views:\nArea-Section-Location View in Database A  (vwAreasSectionsAndLocations)</p>\n\n<pre><code>SELECT     A.AreaId, A.AreaName, C.SectionId, C.SectionName, E.LocationId, E.LocationName\nFROM         DatabaseA.dbo.tblAreas AS A INNER JOIN\n                      DatabaseA.dbo.tblAreaSections AS B ON A.AreaId = B.AreaId INNER JOIN\n                      DatabaseA.dbo.tblSections AS C ON B.SectionId = C.SectionId INNER JOIN\n                      DatabaseA.dbo.tblSectionLocations AS D ON C.SectionId = D.SectionId INNER JOIN\n                      DatabaseA.dbo.tblLocations AS E ON D.LocationId = E.LocationId\n</code></pre>\n\n<p>Category-SubCategory View in Database B   (vwCategoryAndSubCategory)</p>\n\n<pre><code>SELECT     TOP (100) PERCENT dbo.tblCategories.CategoryId, \n          dbo.tblCategories.CategoryName, \n                      dbo.tblSubCategory.SubCategoryId, \n                      dbo.tblSubCategory.SubCategoryname, \n              dbo.tblSubCategory.SubCategoryTypeId\nFROM         dbo.tblCategories INNER JOIN\n                      dbo.tblSubCategory ON dbo.tblCategories.CategoryId = dbo.tblSubCategory.CategoryId\nORDER BY dbo.tblCategories.CategoryName, dbo.tblSubCategory.SubCategoryname\n</code></pre>\n\n<p>View that Cross Joins the two views to create the required results matrix (without Results)  (vwAreaSectionLocationCategorySubCategory)</p>\n\n<pre><code>SELECT     dbo.vwCategoryAndSubCategory.CategoryId, dbo.vwCategoryAndSubCategory.CategoryName, \n                      dbo.vwCategoryAndSubCategory.CategoryPlacementOrder, dbo.vwCategoryAndSubCategory.IsStandardDaybookEntryCategory, \n                      dbo.vwCategoryAndSubCategory.SubCategoryId, dbo.vwCategoryAndSubCategory.SubCategoryname, dbo.vwCategoryAndSubCategory.SubCategoryTypeId, \n                      dbo.vwCategoryAndSubCategory.SubCategoryPlacementOrder, dbo.vwAreasSectionsAndLocations.AreaId, dbo.vwAreasSectionsAndLocations.AreaName, \n                      dbo.vwAreasSectionsAndLocations.SectionId, dbo.vwAreasSectionsAndLocations.SectionName, dbo.vwAreasSectionsAndLocations.LocationId, \n                      dbo.vwAreasSectionsAndLocations.LocationName\nFROM         dbo.vwCategoryAndSubCategory CROSS JOIN\n                      dbo.vwAreasSectionsAndLocations\n</code></pre>\n\n<p>View that Creates the Use Stats Results  (vwUserStatsResults)</p>\n\n<pre><code>SELECT      dbo.tblUserStats.UserStatId, \n    dbo.tblUserStats.UserId, \n    dbo.tblUserStats.LocationId, \n        dbo.tblUserStats.SectionId, \n    dbo.tblUserStats.AreaId,                       \n            dbo.tblUserStats.StartDate, \n    dbo.tblUserStats.EndDate, \n    dbo.tblFieldType1.CategoryId, \n    dbo.tblFieldType1.SubCategoryId, \n            1 AS FieldTypeId, \n    (dbo.tblFieldType1.Value1 + ISNULL\n                      ((SELECT     SUM(A.Value1)\n                          FROM         dbo.tblFieldType1 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType1.UserStatId = A.UserStatId), 0) \n                  + ISNULL\n                      ((SELECT     SUM(A.Value1)\n                          FROM         dbo.tblFieldType2 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType1.UserStatId = A.UserStatId), 0) \n                  + ISNULL\n                      ((SELECT     SUM(A.Value1)\n                          FROM         dbo.tblFieldType3 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType1.UserStatId = A.UserStatId), 0)) \n                  AS Value1, (dbo.tblFieldType1.Value2 + ISNULL\n                      ((SELECT     SUM(A.Value2)\n                          FROM         dbo.tblFieldType1 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType1.UserStatId = A.UserStatId), 0) \n                  + ISNULL\n                      ((SELECT     SUM(A.Value2)\n                          FROM         dbo.tblFieldType2 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType1.UserStatId = A.UserStatId), 0) \n                  + ISNULL\n                      ((SELECT     SUM(A.Value2)\n                          FROM         dbo.tblFieldType3 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType1.UserStatId = A.UserStatId), 0)) \n                  AS Value2, Value1 + Value2 AS CombinedValue1And2, 0 AS Value3, 0 AS Value4, \n                  0 AS Value5, 0 AS Value6, 0 AS Value7, 0 AS Value8, \n                  0 AS Value9, 0 AS Value10, 0 AS Value11, 0 AS Value12, 0 AS Value13, \n                  0 AS Value14, 0 AS Value15, 0 AS Value16\nFROM         dbo.tblUserStats INNER JOIN\n                  dbo.tblFieldType1 ON dbo.tblUserStats.UserStatId = dbo.tblFieldType1.UserStatId \n\nUNION ALL\n\nSELECT      dbo.tblUserStats.UserStatId, \n    dbo.tblUserStats.UserId, \n    dbo.tblUserStats.LocationId, \n            dbo.tblUserStats.SectionId, \n    dbo.tblUserStats.AreaId, \n            dbo.tblUserStats.StartDate, \n    dbo.tblUserStats.EndDate, \n    dbo.tblFieldType2.CategoryId, \n    dbo.tblFieldType2.SubCategoryId, \n            2 AS FieldTypeId, \n    dbo.tblFieldType2.Value1 + ISNULL\n                      ((SELECT     SUM(Value1) AS Expr1\n                          FROM         dbo.tblFieldType1 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType2.UserStatId = UserStatId)), 0) \n                  + ISNULL\n                      ((SELECT     SUM(Value1) AS Expr1\n                          FROM         dbo.tblFieldType2 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType2.UserStatId = UserStatId)), 0) \n                  + ISNULL\n                      ((SELECT     SUM(Value1) AS Expr1\n                          FROM         dbo.tblFieldType3 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType2.UserStatId = UserStatId)), 0) \n                  AS Value1, dbo.tblFieldType2.Value2 + ISNULL\n                      ((SELECT     SUM(Value2) AS Expr1\n                          FROM         dbo.tblFieldType1 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType2.UserStatId = UserStatId)), 0) \n                  + ISNULL\n                      ((SELECT     SUM(Value2) AS Expr1\n                          FROM         dbo.tblFieldType2 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType2.UserStatId = UserStatId)), 0) \n                  + ISNULL\n                      ((SELECT     SUM(Value2) AS Expr1\n                          FROM         dbo.tblFieldType3 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType2.UserStatId = UserStatId)), 0) \n                  AS Value2, dbo.tblFieldType2.Value1 + dbo.tblFieldType2.Value2 AS CombinedValue1And2, \n                  ISNULL(dbo.tblFieldType2.Value3, 0), ISNULL(dbo.tblFieldType2.Value4, 0), \n                  ISNULL(dbo.tblFieldType2.Value5, 0), ISNULL(dbo.tblFieldType2.Value6, 0), \n                  ISNULL(dbo.tblFieldType2.Value7, 0), ISNULL(dbo.tblFieldType2.Value8, 0), \n                  ISNULL(dbo.tblFieldType2.Value9, 0), ISNULL(dbo.tblFieldType2.Value10, 0), \n                  0 AS Value11, 0 AS Value12, 0 AS Value13, 0 AS Value14, 0 AS Value15, \n                  0 AS Value16\nFROM         dbo.tblUserStats INNER JOIN\n                  dbo.tblFieldType2 ON dbo.tblUserStats.UserStatId = dbo.tblFieldType2.UserStatId \nUNION ALL\nSELECT      dbo.tblUserStats.UserStatId, \n    dbo.tblUserStats.UserId, \n    dbo.tblUserStats.LocationId, \n    dbo.tblUserStats.SectionId, \n    dbo.tblUserStats.AreaId, \n            dbo.tblUserStats.StartDate, \n    dbo.tblUserStats.EndDate, \n    dbo.tblFieldType3.CategoryId, \n    dbo.tblFieldType3.SubCategoryId, \n            3 AS FieldTypeId, \n    dbo.tblFieldType3.Value1 + ISNULL\n                      ((SELECT     SUM(Value1) AS Expr1\n                          FROM         dbo.tblFieldType1 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType3.UserStatId = UserStatId)), 0) \n                  + ISNULL\n                      ((SELECT     SUM(Value1) AS Expr1\n                          FROM         dbo.tblFieldType2 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType3.UserStatId = UserStatId)), 0) \n                  + ISNULL\n                      ((SELECT     SUM(Value1) AS Expr1\n                          FROM         dbo.tblFieldType3 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType3.UserStatId = UserStatId)), 0) \n                  AS Value1, dbo.tblFieldType3.Value2 + ISNULL\n                      ((SELECT     SUM(Value2) AS Expr1\n                          FROM         dbo.tblFieldType1 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType3.UserStatId = UserStatId)), 0) \n                  + ISNULL\n                      ((SELECT     SUM(Value2) AS Expr1\n                          FROM         dbo.tblFieldType2 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType3.UserStatId = UserStatId)), 0) \n                  + ISNULL\n                      ((SELECT     SUM(Value2) AS Expr1\n                          FROM         dbo.tblFieldType3 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType3.UserStatId = UserStatId)), 0) \n                  AS Value2, dbo.tblFieldType3.Value1 + dbo.tblFieldType3.Value2 AS CombinedValue1And2, 0 AS Value3, \n                  0 AS Value4, 0 AS Value5, 0 AS Value6, 0 AS Value7, \n                  0 AS Value8, 0 AS Value9, 0 AS Value10, \n                  ISNULL(dbo.tblFieldType3.Value11, 0), ISNULL(dbo.tblFieldType3.Value12, 0), \n                  ISNULL(dbo.tblFieldType3.Value13, 0), ISNULL(dbo.tblFieldType3.Value14, 0), \n                  ISNULL(dbo.tblFieldType3.Value15, 0), ISNULL(dbo.tblFieldType3.Value16, 0)\nFROM         dbo.tblUserStats INNER JOIN\n                  dbo.tblFieldType3 ON dbo.tblUserStats.UserStatId = dbo.tblFieldType3.UserStatId \nUNION ALL\nSELECT     dbo.tblUserStats.UserStatId, \n    dbo.tblUserStats.UserId, \n    dbo.tblUserStats.LocationId, \n            dbo.tblUserStats.SectionId, \n    dbo.tblUserStats.AreaId, \n            dbo.tblUserStats.StartDate, \n    dbo.tblUserStats.EndDate, \n    dbo.tblFieldType4.CategoryId, \n    dbo.tblFieldType4.SubCategoryId, \n            4 AS FieldTypeId, \n    0 AS Value1, 0 AS Value2, dbo.tblFieldType4.CombinedValue1And2 + (ISNULL\n                      ((SELECT     SUM(A.Value1)\n                          FROM         dbo.tblFieldType1 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType4.UserStatId = A.UserStatId), 0) \n                  + ISNULL\n                      ((SELECT     SUM(A.Value1)\n                          FROM         dbo.tblFieldType2 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType4.UserStatId = A.UserStatId), 0) \n                  + ISNULL\n                      ((SELECT     SUM(A.Value1)\n                          FROM         dbo.tblFieldType3 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType4.UserStatId = A.UserStatId), 0)) \n                  + (ISNULL\n                      ((SELECT     SUM(A.Value2)\n                          FROM         dbo.tblFieldType1 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType4.UserStatId = A.UserStatId), 0) \n                  + ISNULL\n                      ((SELECT     SUM(A.Value2)\n                          FROM         dbo.tblFieldType2 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType4.UserStatId = A.UserStatId), 0) \n                  + ISNULL\n                      ((SELECT     SUM(A.Value2)\n                          FROM         dbo.tblFieldType3 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType4.UserStatId = A.UserStatId), 0)) \n                  AS CombinedValue1And2, 0 AS Value3, 0 AS Value4, 0 AS Value5, 0 AS Value6, \n                  0 AS Value7, 0 AS Value8, 0 AS Value9, 0 AS Value10, \n                  0 AS Value11, 0 AS Value12, 0 AS Value13, 0 AS Value14, 0 AS Value15, \n                  0 AS Value16\nFROM         dbo.tblUserStats INNER JOIN\n                  dbo.tblFieldType4 ON dbo.tblUserStats.UserStatId = dbo.tblFieldType4.UserStatId \n</code></pre>\n\n<p>And finally the view that pulls it all together   (vwReportResults)</p>\n\n<p>Posted in another entry due to character limitations</p>\n"},{"tags":["performance","jmeter","load-testing","visualvm","jvisualvm"],"answer_count":5,"favorite_count":7,"up_vote_count":5,"down_vote_count":0,"view_count":3509,"score":5,"question_id":5645393,"title":"How to do load testing using jmeter and visualVM?","body":"<p>I want to do load testing for 10 million users for my site. The site is a Java based web-app. My approach is to create a Jmeter test plan for all the links and then take a report for the 10 million users. Then use jvisualVM to do profiling and check if there are any bottlenecks.</p>\n\n<p>Is there any better way to do this? Is there any existing demo for doing this? I am doing this for the first time, so any assistance will be very helpful.</p>\n"},{"tags":["c","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":94,"score":0,"question_id":12850912,"title":"Performance down","body":"<p>Can anyone help me with this?</p>\n\n<pre><code>void _vect_mat(float *vect,float **mat){\n  float temp[4];\n\n  temp[0] = vect[0];\n  temp[1] = vect[1];\n  temp[2] = vect[2];\n  temp[3] = vect[3];\n\n  vect[0] = (temp[0] * mat[0][0]) + (temp[1] * mat[1][0]) + (temp[2] * mat[2][0]) + (temp[3] * mat[3][0]);\n  vect[1] = (temp[0] * mat[0][1]) + (temp[1] * mat[1][1]) + (temp[2] * mat[2][1]) + (temp[3] * mat[3][1]);\n  vect[2] = (temp[0] * mat[0][2]) + (temp[1] * mat[1][2]) + (temp[2] * mat[2][2]) + (temp[3] * mat[3][2]);\n  vect[3] = (temp[0] * mat[0][3]) + (temp[1] * mat[1][3]) + (temp[2] * mat[2][3]) + (temp[3] * mat[3][3]);\n}\n\nint main(){\n  int i,j,k;\n\n  float *vect,**mat;\n\n  vect =  (float *)malloc(4 * sizeof(float));\n  mat  = (float **)malloc(4 * sizeof(float *);\n  for(i=0;i&lt;4;i++)mat[i] = (float *)malloc(4 * sizeof(float));\n\n  vect[0] = 1.0;\n  vect[n] = ......etc.\n\n  mat[0][0] = 1.0;\n  mat[n][m] ......etc.     \n\n  while (1){\n    for(i = 0;i &lt; 5;i++){\n      for(j = 0;j&lt; 6;j++){\n        for(k = 0;k &lt; 3600;k++)_vect_mat(vect,mat); \n      }\n    }\n  }\n}\n</code></pre>\n\n<p>when I call the function <code>_vect_mat</code> inside the loop, all performance falls. It is Normal?\nWhat am I doing wrong?</p>\n"},{"tags":["php","arrays","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":50,"score":1,"question_id":12995025,"title":"Is there any advantage in reusing arrays instead of creating new ones?","body":"<p>Is it good practice to try to save a \"generic\" array in the $_SESSION array for the purpose of applyng e.g PHP sorting functions instead of calling a new result set from the server? Seems abit overkill to e.g call a new result set if I want to sort comments based on likes instead of timestamp. Maybe there is another best practice for this \"problem\"? How much efficiency is there to be gained working like this? Is it very much?</p>\n"},{"tags":["java","string","performance","indexof"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":74,"score":1,"question_id":12995895,"title":"Java: Overhead in executing StringBuilder.indexOf() that concatenates a variable and literal","body":"<p>Is there a performance overhead in executing <code>sb.indexOf(c + \"\")</code> \nwhere <code>c</code> is of type <code>Character</code> or <code>char</code> and <code>sb</code> is <code>StringBuilder</code> object?</p>\n"},{"tags":["java","performance","hashset"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":67,"score":0,"question_id":12973867,"title":"Java: Comparing hashsets as efficently as possible","body":"<p>I have 3 hashsets. goodLinkSet, badLinkSet and testLinkSet.</p>\n\n<p>goodLinkSet holds a list of URLs that work and badLinkSet holds a list of URLs that don't work. testLinkSet holds a list of URLs that I need to check if they are good are bad, some of the links in here have been tested already in the other two sets. </p>\n\n<p>What I want to do is remove all the strings/links in testLinkSet that appear in goodLinkSet and badLinkSet so I'm not testing URLs multiple times. I want to do this as efficiently and as fast as possible. A for each loop seems to be a bit slow. </p>\n\n<p>What's the most efficient way of running this? Are there any functions that does this for me? Any advice would be very much appreciated!</p>\n"},{"tags":["java","string","performance","collections"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":102,"score":1,"question_id":12995617,"title":"How can I improve performance of string processing with less memory?","body":"<p>I'm implementing this in Java.</p>\n\n<pre><code>Symbol file     Store data file\n\n1\\item1         10\\storename1\n10\\item20       15\\storename6\n11\\item6        15\\storename9\n15\\item14       1\\storename250\n5\\item5         1\\storename15\n</code></pre>\n\n<p>The user will search store names using wildcards like <code>storename?</code>\nMy job is to search the store names and produce a full string using symbol data. For example:</p>\n\n<p>item20-storename1<br>\nitem14-storename6<br>\nitem14-storename9  </p>\n\n<p>My approach is:</p>\n\n<ol>\n<li>reading the store data file line by line</li>\n<li>if any line contains matching search string (like <code>storename?</code>), I will push that line to an intermediate store result file</li>\n<li>I will also copy the itemno of a matching storename into an arraylist (like 10,15)</li>\n<li>when this arraylist size%100==0 then I will remove duplicate item no's using hashset,  reducing arraylist size significantly</li>\n<li><p>when arraylist size >1000</p>\n\n<ol>\n<li>sort that list using <code>Collections.sort(itemno_arraylist)</code></li>\n<li>open symbol file &amp; start reading line by line</li>\n<li>for each line <code>Collections.binarySearch(itemno_arraylist,itmeno)</code></li>\n<li>if matching then push result to an intermediate symbol result file</li>\n</ol></li>\n<li><p>continue with step1 until EOF of store data file</p></li>\n</ol>\n\n<p>...</p>\n\n<p>After all of this I would combine two result files (symbol result file &amp; store result file) to present actual strings list.</p>\n\n<p>This approach is working but it is consuming more CPU time and main memory.</p>\n\n<p>I want to know a better solution with reduced CPU time (currently 2 min) &amp; memory (currently 80MB). There are many collection classes available in Java. Which one would give a more efficient solution for this kind of huge string processing problem?</p>\n\n<p>If you have any thoughts on this kind of string processing problems that too in Java would be great and helpful.</p>\n\n<p>Note: Both files would be nearly a million lines long.</p>\n"},{"tags":["c++","performance","g++","switch-statement"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":1,"view_count":83,"score":3,"question_id":12992108,"title":"\"crosses initialization of variable\" only when initialization combined with declaration","body":"<p>I've read <a href=\"http://stackoverflow.com/questions/5685471/error-jump-to-case-label\">this question</a> about the \"jump to case label\" error, but I still have some questions. I'm using g++ 4.7 on Ubuntu 12.04.</p>\n\n<p>This code gives an error:</p>\n\n<pre><code>int main() {\n  int foo = 1;\n  switch(foo) {\n  case 1:\n    int i = 0;\n    i++;\n    break;\n  case 2:\n    i++;\n    break;\n  }\n}\n</code></pre>\n\n<p>The error is </p>\n\n<pre><code>jump-to-case-label.cpp: In function ‘int main()’:\njump-to-case-label.cpp:8:8: error: jump to case label [-fpermissive]\njump-to-case-label.cpp:5:9: error:   crosses initialization of ‘int i’\n</code></pre>\n\n<p>However, this code compiles fine,</p>\n\n<pre><code>int main() {\n  int foo = 1;\n  switch(foo) {\n  case 1:\n    int i;\n    i = 0;\n    i++;\n    break;\n  case 2:\n    i++;\n    break;\n  }\n}\n</code></pre>\n\n<p>Is the second code any less dangerous than the first? I'm confused as to why g++ allows it.</p>\n\n<p>Secondly, the fix for this problem is to scope the initialized variable. If the initialized variable is a large object, and the switch statement is in a while loop, won't the constructor and destructor be called each time that scope is entered and left, causing a decrease in efficiency? Or will the compiler optimize this away?</p>\n"},{"tags":["sql-server","performance","sql-server-2008","indexing"],"answer_count":5,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":951,"score":0,"question_id":488258,"title":"Adding a index on my table for this query","body":"<p>This table gets hit with this query the most, so I want add a index to speed things up, this table will have 5 million rows it in.</p>\n\n<p>My query looks like this:</p>\n\n<p>SELECT someID \nFROM someTable \nWHERE \n    myVarChar = @myVarChar AND\n    MyBit = 0 AND MyBit2 = 1 AND MyBit3 = 0</p>\n\n<p>myVarChar is unique also.</p>\n\n<p>What would the best index be for this type of query?</p>\n\n<p>I currently have a single index that covers all of the 4 columns in the above query.</p>\n\n<p>I am using sql server 2008 standard.</p>\n\n<p>Do I have to reindex every so often or its automatic?</p>\n"},{"tags":["java","performance","stringbuilder"],"answer_count":2,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":50,"score":3,"question_id":12994374,"title":"Efficient design for a text-based file that is edited at arbitrary positions?","body":"<p>I'm attempting to develop a simple online editor that allows for real-time collaboration (written in Java). In this editor, I want clients to be able to edit the source code at arbitrary points (e.g. add the letter 'd' to the source code file at row 11, column 20). I'm not sure how to design these source code file objects in an efficient way, while still allowing for letter-by-letter client-server synchronization (similar to how Google Docs works).</p>\n\n<p>I considered using a RandomAccessFile, but after reading <a href=\"http://stackoverflow.com/questions/5786697/how-to-write-content-in-a-specific-position-in-a-file\" title=\"this post\">this post</a>, I don't think that would be an efficient approach. Inserting a letter near the beginning of the file would involve changing everything after it.</p>\n\n<p>My current plan is to represent both the source files on the server and client using a StringBuilder object and its insert/delete/append methods. On the server-side, this StringBuilder would be converted to an actual file as necessary.</p>\n\n<p>I'm curious as to whether there might be a better approach for solving this problem. Any ideas?</p>\n"},{"tags":["sql","performance","mongodb","nosql","document"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":366,"score":3,"question_id":4482972,"title":"Sql database versus document database?","body":"<p>Small introduction:</p>\n\n<ol>\n<li><p>I've tried to develop my project with sql database, entity framework, linq. \nI have table 'Users' and for example i have list of user educations. I ask myself: 'How much educations user can have?  1, 2, 10...' ? And seems at 99% of cases not more than 10. So for such example in sql i need to create referenced table 'Educations'. Right? If i need to display user educations and user i need to join above mentioned tables... But what if user have 10 or even more collections with not more than 10 items in each? I need to create 10 referenced tables in sql? And than join all of them when i need to display? For better performance i've created denormized tables with shape of data that i need to show on ui. And every time when user was updated, i need to update denormilized structure.   </p></li>\n<li><p>Now i redeveloped my project to use document database(<a href=\"http://www.mongodb.org/\" rel=\"nofollow\">MongoDB</a>). And i've created one document for User with all 10 collections inside.</p></li>\n</ol>\n\n<p>May be i've lost something? But seems document database win here. It's very fast and very easy to support. +1 to document database.</p>\n\n<p>So, what is your opinion about what better to use document database or sql database? </p>\n\n<p>When I should use document database and when sql?</p>\n"},{"tags":["ruby-on-rails","performance","optimization","amazon-s3","paperclip"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":61,"score":3,"question_id":12993954,"title":"How to Add Expires headers for amazon s3 images?","body":"<p>In my model, which is using paperclip. I have added <a href=\"http://webonrails.com/2012/06/06/set-cache-control-expires-headers-to-the-content-served-by-s3-using-attachment_fu-or-paperclip/\" rel=\"nofollow\">this</a> line of code but still its not working</p>\n\n<pre><code> has_attached_file :attachment, {\n    :styles =&gt; {\n      :medium =&gt; [\"654x5000000&gt;\", :jpg],\n      :small =&gt; [\"260x50000000&gt;\", :jpg], \n      :thumb =&gt; [\"75x75#\", :jpg],\n      :facebook_meta_tag =&gt;[\"200x200#\", :jpg] \n    },\n    :convert_options =&gt; {\n       :medium =&gt; \"-quality 80 -interlace Plane\",\n       :small =&gt; \"-quality 80 -interlace Plane\",\n       :thumb =&gt; \"-quality 80 -interlace Plane\",\n       :facebook_meta_tag =&gt; \"-quality 80 -interlace Plane\" \n       },\n       :s3_headers =&gt; { 'Cache-Control' =&gt; 'max-age=315576000', 'Expires' =&gt; 10.years.from_now.httpdate } \n    }.merge(PAPERCLIP_STORAGE_OPTIONS)\n</code></pre>\n\n<p>PS: I tested it on <a href=\"http://gtmetrix.com\" rel=\"nofollow\">GTmetrix.com</a> and as per their stats, expiry headers are not there in amazon images.</p>\n"},{"tags":["c#",".net","visual-studio-2010","performance","visual-studio"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":34,"score":2,"question_id":12993112,"title":"Debugging very slow after adding persistent settings","body":"<p>I have a semi-small visual C# solution I am working on in Visual Studio 2010. I recently added in Persistent settings using the <code>settings.Settings</code> file and <code>Properties.Settings.Default</code>. On the Form Load event, I am checking the value using <code>Properties.Settings.Default</code> and am assigning them to a <code>checkbox.Checked</code> variable.</p>\n\n<pre><code>//Load Form1\nprivate void Form1_Load(object sender, System.EventArgs e)\n{\n    cbxShowPass.Checked = Properties.Settings.Default.showFullPassword;\n    checkBox2.Checked = Properties.Settings.Default.showBookmarkFiles;\n}\n</code></pre>\n\n<p>When I start my program in Debug mode, it is extremely slow to start, however, when I remove that line of code, it will start quickly.</p>\n\n<p>What can I do to make my program start up quickly in Debug mode without removing the Settings?</p>\n\n<p>Here are my CheckBox_Changed events, where I assign the value to the Settings.</p>\n\n<pre><code>private void ShowPass_CheckChanged(object sender, EventArgs e)\n{\n    Properties.Settings.Default.showFullPassword = !Properties.Settings.Default.showFullPassword;\n        Properties.Settings.Default.Save();\n        DisplayPassword();\n}\n</code></pre>\n"},{"tags":["python","performance","if-statement","coding-style"],"answer_count":7,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":225,"score":7,"question_id":12680109,"title":"Performance or style difference between \"if\" and \"if not\"?","body":"<p>Is there a performance difference or style preference between these two ways of writing if statements?  It is basically the same thing, the 1 condition will be met only once while the other condition will be met every other time.  Should the condition that is met only once be first or second?  Does it make a difference performance wise?   I prefer the 1st way if the the performance is the same.</p>\n\n<pre><code>data = range[0,1023]\nlength = len(data)\nmax_chunk = 10\n\nfor offset in xrange(0,length,max_chunk):\n    chunk = min(max_chunk,length-offset)\n    if chunk &lt; max_chunk:\n        write_data(data[offset:])\n    else:\n        write_data(data[offset:offset+max_chunk])\n</code></pre>\n\n<p>vs</p>\n\n<pre><code>data = range[0,1023]\nlength = len(data)\nmax_chunk = 10\n\nfor offset in xrange(0,length,max_chunk):\n    chunk = min(max_chunk,length-offset)\n    if not chunk &lt; max_chunk:\n        write_data(data[offset:offset+max_chunk])\n    else:\n        write_data(data[offset:])\n</code></pre>\n"},{"tags":["java","performance","optimization","bit-manipulation"],"answer_count":4,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":462,"score":7,"question_id":9825319,"title":"What's the fastest way to do a right circular bit shift on a byte array","body":"<p>If I have the array:</p>\n\n<pre><code>{01101111,11110000,00001111} // {111, 240, 15}\n</code></pre>\n\n<p>The result for a 1 bit shift is:</p>\n\n<pre><code>{10110111,11111000,00000111} // {183, 248, 7}\n</code></pre>\n\n<p>The array size is not fixed, and the shifting will be from 1 to 7 inclusive. Currently I have the following code (which works fine):</p>\n\n<pre><code>private static void shiftBitsRight(byte[] bytes, final int rightShifts) {\n   assert rightShifts &gt;= 1 &amp;&amp; rightShifts &lt;= 7;\n\n   final int leftShifts = 8 - rightShifts;\n\n   byte previousByte = bytes[0]; // keep the byte before modification\n   bytes[0] = (byte) (((bytes[0] &amp; 0xff) &gt;&gt; rightShifts) | ((bytes[bytes.length - 1] &amp; 0xff) &lt;&lt; leftShifts));\n   for (int i = 1; i &lt; bytes.length; i++) {\n      byte tmp = bytes[i];\n      bytes[i] = (byte) (((bytes[i] &amp; 0xff) &gt;&gt; rightShifts) | ((previousByte &amp; 0xff) &lt;&lt; leftShifts));\n      previousByte = tmp;\n   }\n}\n</code></pre>\n\n<p>Is there a faster way to achieve this than my current approach?</p>\n"},{"tags":["android","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":514,"score":4,"question_id":3048637,"title":"Performance of the Android Virtual Device","body":"<p>The Android virtual device (a simulated Android environment) doesn't run very smoothly on my machine. Scrolling is quite sluggish.</p>\n\n<p>Is that normal?</p>\n\n<p>EDIT: Just noticed that a AVD running Android 1.6 has a significantly better performance compared to the AVDs running on 2.1 and 2.2.</p>\n"},{"tags":["php","mysql","performance","for-loop"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":71,"score":0,"question_id":12992793,"title":"Efficient ways to calculate \"ranks\" for over 350,000 users","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/3333665/mysql-rank-function\">Mysql rank function</a>  </p>\n</blockquote>\n\n\n\n<p>I am making a rank website and each user will have a unique rank based on level. #1 is the best rank and since there's 350,000 users, the worst rank will be #350000.</p>\n\n<p>No user can have the same rank as anyone else. Now, when a new user is added, their level is calculated. After calculation a script will \"rebuild\" the ranks, by going through each user one by one, and calculating their new rank.</p>\n\n<p>Here's an explanation of the queries:</p>\n\n<ul>\n<li>\"Id\" is the member's ID in the database</li>\n<li>\"RankNum\" is their level. It needs to be renamed.</li>\n<li>\"Rank\" is their unique rank, #1 to #350000.</li>\n</ul>\n\n<p>Here's my current script:</p>\n\n<pre><code>function RebuildRanks() {\n    $qD = mysql_query(\"SELECT `Id`,`RankNum` FROM `Members` ORDER BY `Rank` DESC, `Id` ASC\");\n    $rowsD = mysql_num_rows($qD);\n\n    $curRank = 0;\n\n    for($x = 1; $x &lt;= $rowsD; $x++) {\n        $rowD = mysql_fetch_array($qD);\n\n        $curRank++;\n        if($rowD['RankNum'] != $curRank) {\n            if($curRank != 0) {\n                mysql_query(\"UPDATE `Members` SET `RankNum`='$curRank' WHERE `Id`='\".$rowD['Id'].\"'\");\n            }\n        }\n    }\n\n    return true;\n}\n</code></pre>\n\n<p>With 350,000 users, this tends to run very slow. Essentially, in the database the <strong>Rank</strong> (\"ORDER BY `Rank` DESC\") is their level, so the query will order them. Unfortunately, the rest of the process is slow. </p>\n\n<p>It takes about 97 seconds to process all 350,000 users this way. Is there any possible solution to running this more efficiently and quickly?</p>\n"},{"tags":["iphone","performance","ios","viewdidload"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":412,"score":0,"question_id":4963745,"title":"Switching Tabs (initially) very slow - Move Function out of viewDidLoad?","body":"<p>In my iPhone app, I have three tabs laid out using a <code>UITabBarController</code>. The first tab (that loads on app launch) uses local data to load, and is very fast.</p>\n\n<p>The second tab, though, which downloads an XML file from the web and parses it, then displays all the data in a <code>UITableView</code>, takes a long time to load over slower connections (EDGE, 3G). And, since I do call my parser inside <code>viewDidLoad</code>, the app won't switch to my second tab until everything is done—this means it takes a while to load the tab sometimes, and it looks like the app's locked up.</p>\n\n<p>I'd rather be able to have the user switch to that tab, have the view load immediately—even if empty, and then have the data downloaded/parsed/displayed. I have the network activity spinner spinning, so at least the user can know something's happening.</p>\n\n<p>Here's my current <code>viewDidLoad</code>:</p>\n\n<pre><code>// Load in the latest stories when the app is launched.\n- (void)viewDidLoad {\n    [super viewDidLoad];\n    NSLog(@\"Loading news view\");\n\n    articles = [[NSMutableArray alloc] init];\n\n    NSURL *url = [NSURL URLWithString:@\"http://example.com/mobile-app/latest-news.xml\"];\n    NSLog(@\"About to parse URL: %@\", url);\n    parser = [[NSXMLParser alloc] initWithContentsOfURL:url];\n    parser.delegate = self;\n    [parser parse];\n}\n</code></pre>\n\n<p>I found <a href=\"http://www.chrisumbel.com/article/asynchronous_iphone_nsthread_nstimer\" rel=\"nofollow\">this article</a>, which shows how to run threads in the background, but I tried implementing that code and couldn't get the background thread to load the data back into my UITableView - the code was called, but how would I make sure the parsed articles are loaded back into my table view?</p>\n"},{"tags":["java","performance","multithreading"],"answer_count":7,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":221,"score":2,"question_id":878035,"title":"Why don't multiple threads seem to speed up my web application?","body":"<pre><code>class ApplicationContext{\n    private final NetworkObject networkObject = new networkObject();\n\n    public ApplicationContext(){\n      networkObject.setHost(\"host\");\n      networkObject.setParams(\"param\");\n    }\n\n    public searchObjects(ObjectType objType){\n        networkObject.doSearch(buildQuery(objType));\n    }\n}\n\nclass NetworkObject{\n    private final SearchObject searchObject = new SearchObject();\n\n    public doSearch(SearchQuery searchQuery){\n        searchObject.search(searchQuery); //threadsafe, takes 15(s) to return\n    }\n}\n</code></pre>\n\n<p>Consider a webserver running a web application which creates only one ApplicationContext instance (singleton) and uses the same applicationInstance to call searchObjects e.g.</p>\n\n<pre><code> ApplicationContext appInstance = \n                  ApplicationContextFactory.Instance(); //singleton\n</code></pre>\n\n<p>Every new request to a webpage say 'search.jsp' makes a call </p>\n\n<pre><code> appInstance.searchObjects(objectType);\n</code></pre>\n\n<p>I am making 1000 requests to 'search.jsp' page. All the threads are using the same ApplicationContext instance, and searchObject.search() method takes 15 seconds to return. My Question is Do all other threads wait for their turn (15 sec) to execute when one is already executing the searchObject.search() function or All threads will execute searchObject.search() concurrently, Why??</p>\n\n<p>I hope I have made my question very clear??</p>\n\n<p><strong>Update:</strong>\nThanks all for clarifying my doubt. Here is my second Question, what difference in performance should be observe when I do:</p>\n\n<pre><code>public synchronized doSearch(SearchQuery searchQuery){\n    searchObject.search(searchQuery); //threadsafe, takes 15(s) to return\n}\n</code></pre>\n\n<p><strong>OR</strong></p>\n\n<pre><code>public doSearch(SearchQuery searchQuery){\n    searchObject.search(searchQuery); //threadsafe, takes 15(s) to return\n}\n</code></pre>\n\n<p>I believe using the function 'doSearch' without synchronized keyword should be giving more performance. But, when I tested it today, the results came out the other way. The performance was similar or sometimes better when I use synchronized keyword.</p>\n\n<p>Can anyone explain the behavior. How should I debug such cases.</p>\n\n<p>Regards,</p>\n\n<p>Perry</p>\n"},{"tags":["ios","performance","interpolation"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":12992157,"title":"Fast bilinear interpolation on old iOS devices","body":"<p>I've got the following code to do a biliner interpolation from a matrix of 2D vectors, each cell has x and y values of the vector, and the function receives k and l indices telling the bottom-left nearest position in the matrix</p>\n\n<pre><code>// p[1]                     returns the interpolated values\n// fieldLinePointsVerts     the raw data array of fieldNumHorizontalPoints x fieldNumVerticalPoints\n//                          only fieldNumHorizontalPoints matters to determine the index to access the raw data\n// k and l                  horizontal and vertical indices of the point just bellow p[0] in the raw data\n\nvoid interpolate( vertex2d* p, vertex2d* fieldLinePointsVerts, int fieldNumHorizontalPoints, int k, int l ) {\n\n    int index = (l * fieldNumHorizontalPoints + k) * 2;\n\n    vertex2d p11;\n    p11.x = fieldLinePointsVerts[index].x;\n    p11.y = fieldLinePointsVerts[index].y;\n\n    vertex2d q11;\n    q11.x = fieldLinePointsVerts[index+1].x;\n    q11.y = fieldLinePointsVerts[index+1].y;\n\n    index = (l * fieldNumHorizontalPoints + k + 1) * 2;\n\n    vertex2d q21;\n    q21.x = fieldLinePointsVerts[index+1].x;\n    q21.y = fieldLinePointsVerts[index+1].y;\n\n    index = ( (l + 1) * fieldNumHorizontalPoints + k) * 2;\n\n    vertex2d q12;\n    q12.x = fieldLinePointsVerts[index+1].x;\n    q12.y = fieldLinePointsVerts[index+1].y;\n\n    index = ( (l + 1) * fieldNumHorizontalPoints + k + 1 ) * 2;\n\n    vertex2d p22;\n    p22.x = fieldLinePointsVerts[index].x;\n    p22.y = fieldLinePointsVerts[index].y;\n\n    vertex2d q22;\n    q22.x = fieldLinePointsVerts[index+1].x;\n    q22.y = fieldLinePointsVerts[index+1].y;\n\n    float fx = 1.0 / (p22.x - p11.x);\n    float fx1 = (p22.x - p[0].x) * fx;\n    float fx2 = (p[0].x - p11.x) * fx;\n\n    vertex2d r1;\n    r1.x = fx1 * q11.x + fx2 * q21.x;\n    r1.y = fx1 * q11.y + fx2 * q21.y;\n\n    vertex2d r2;\n    r2.x = fx1 * q12.x + fx2 * q22.x;\n    r2.y = fx1 * q12.y + fx2 * q22.y;\n\n    float fy = 1.0 / (p22.y - p11.y);\n    float fy1 = (p22.y - p[0].y) * fy;\n    float fy2 = (p[0].y - p11.y) * fy; \n\n    p[1].x = fy1 * r1.x + fy2 * r2.x;\n    p[1].y = fy1 * r1.y + fy2 * r2.y;\n}\n</code></pre>\n\n<p>Currently this code needs to be run every single frame in old iOS devices, say devices with arm6 processors</p>\n\n<p>I've taken the numeric sub-indices from the wikipedia's equations <a href=\"http://en.wikipedia.org/wiki/Bilinear_interpolation\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Bilinear_interpolation</a></p>\n\n<p>I'd accreciate any comments on optimization for performance, even plain asm code </p>\n"},{"tags":["c++","arrays","performance","stl","vector"],"answer_count":15,"favorite_count":19,"up_vote_count":69,"down_vote_count":3,"view_count":6202,"score":66,"question_id":3664272,"title":"std::vector is so much slower than plain arrays?","body":"<p>I've always thought it's the general wisdom that <code>std::vector</code> is \"implemented as an array,\" blah blah blah. Today I went down and tested it, seems to be not so:</p>\n\n<p>Here's some test results:</p>\n\n<pre><code>UseArray completed in 2.619 seconds\nUseVector completed in 9.284 seconds\nUseVectorPushBack completed in 14.669 seconds\nThe whole thing completed in 26.591 seconds\n</code></pre>\n\n<p>That's about 3 - 4 times slower! Doesn't really justify for the \"<code>vector</code> may be slower for a few nanosecs\" comments.</p>\n\n<p>And the code I used:</p>\n\n<pre><code>#include &lt;cstdlib&gt;\n#include &lt;vector&gt;\n\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n\n#include &lt;boost/date_time/posix_time/ptime.hpp&gt;\n#include &lt;boost/date_time/microsec_time_clock.hpp&gt;\n\nclass TestTimer\n{\npublic:\n    TestTimer(const std::string &amp; name) : name(name),\n        start(boost::date_time::microsec_clock&lt;boost::posix_time::ptime&gt;::local_time())\n    {\n    }\n\n    ~TestTimer()\n    {\n        using namespace std;\n        using namespace boost;\n\n        posix_time::ptime now(date_time::microsec_clock&lt;posix_time::ptime&gt;::local_time());\n        posix_time::time_duration d = now - start;\n\n        cout &lt;&lt; name &lt;&lt; \" completed in \" &lt;&lt; d.total_milliseconds() / 1000.0 &lt;&lt;\n            \" seconds\" &lt;&lt; endl;\n    }\n\nprivate:\n    std::string name;\n    boost::posix_time::ptime start;\n};\n\nstruct Pixel\n{\n    Pixel()\n    {\n    }\n\n    Pixel(unsigned char r, unsigned char g, unsigned char b) : r(r), g(g), b(b)\n    {\n    }\n\n    unsigned char r, g, b;\n};\n\nvoid UseVector()\n{\n    TestTimer t(\"UseVector\");\n\n    for(int i = 0; i &lt; 1000; ++i)\n    {\n        int dimension = 999;\n\n        std::vector&lt;Pixel&gt; pixels;\n        pixels.resize(dimension * dimension);\n\n        for(int i = 0; i &lt; dimension * dimension; ++i)\n        {\n            pixels[i].r = 255;\n            pixels[i].g = 0;\n            pixels[i].b = 0;\n        }\n    }\n}\n\nvoid UseVectorPushBack()\n{\n    TestTimer t(\"UseVectorPushBack\");\n\n    for(int i = 0; i &lt; 1000; ++i)\n    {\n        int dimension = 999;\n\n        std::vector&lt;Pixel&gt; pixels;\n            pixels.reserve(dimension * dimension);\n\n        for(int i = 0; i &lt; dimension * dimension; ++i)\n            pixels.push_back(Pixel(255, 0, 0));\n    }\n}\n\nvoid UseArray()\n{\n    TestTimer t(\"UseArray\");\n\n    for(int i = 0; i &lt; 1000; ++i)\n    {\n        int dimension = 999;\n\n        Pixel * pixels = (Pixel *)malloc(sizeof(Pixel) * dimension * dimension);\n\n        for(int i = 0 ; i &lt; dimension * dimension; ++i)\n        {\n            pixels[i].r = 255;\n            pixels[i].g = 0;\n            pixels[i].b = 0;\n        }\n\n        free(pixels);\n    }\n}\n\nint main()\n{\n    TestTimer t1(\"The whole thing\");\n\n    UseArray();\n    UseVector();\n    UseVectorPushBack();\n\n    return 0;\n}\n</code></pre>\n\n<p>Am I doing it wrong or something? Or have I just busted this performance myth?</p>\n\n<p>Edit: I'm using Release mode in MSVS2005.</p>\n\n<hr>\n\n<p>In MSVC, <code>#define _SECURE_SCL 0</code> reduces <code>UseVector</code> by half (bringing it down to 4 seconds). This is really huge IMO.</p>\n"},{"tags":["java","performance","map"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":82,"score":1,"question_id":12991569,"title":"Efficient java map for values.contains(object) in O(1)?","body":"<p>I've started writing a tool for modelling graphs (nodes and edges, both represented by objects, not adjacency matrices or simply numbers) recently to get some practice in software development next to grad school. I'd like a node to know its neighbors and the edges it's incident with. An obvious way to do this would be to take a HashSet and a HashSet. What I would like to do however is have a method</p>\n\n<pre><code>Node_A.getEdge(Node B)\n</code></pre>\n\n<p>that returns the edge between nodes A and B in O(1). I was thinking of doing this by replacing the two HashSets mentioned above by using one HashMap that maps the neighbors to the edges that connect a node with its neighbors. For example, calling</p>\n\n<pre><code>Node_A.hashmap.get(B)\n</code></pre>\n\n<p>should return the edge that connects A and B. My issue here is whether there's a way to have</p>\n\n<pre><code>HashMap.keySet().contains(Node A);\nHashMap.values().contains(Edge e);\n</code></pre>\n\n<p>both work in O(1)? If that isn't the case with the standard libraries, are there implementations that will give me constant time for add, remove, contains, size for keySet() and values()?</p>\n"},{"tags":["iphone","ios","performance","cocoa-touch","uiviewcontroller"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":53,"score":0,"question_id":12991582,"title":"What can happen between viewWillAppear and viewDidAppear?","body":"<p>I am optimizing a transition that seems to be slow on my device. I am pushing one <code>UIViewController</code> from another when a <code>UITableView</code>'s row is selected. There is a noticeable pause after row selection and before the new view is pushed.</p>\n\n<p>Some logging indicates that all of my code is reasonably quick, from row selection until the pushed controller's <code>viewWillAppear</code>. But then the time between <code>viewWillAppear</code> and <code>viewDidAppear</code> is logged at around 0.7 seconds.</p>\n\n<p>The transition itself (I believe) should only take 0.3 seconds. What could be accounting for the remainder?</p>\n\n<p>I am testing on an iPhone 4, so I'm not expecting the snappiest performance. But I should be able to match the same performance of other similar apps on the same device, no?</p>\n"},{"tags":["php","performance","compression"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":146,"score":-1,"question_id":8222311,"title":"I looking for the best way to compress a file/string with PHP","body":"<p>They are many options on php.net website for compression:</p>\n\n<ul>\n<li>Bzip2</li>\n<li>LZF</li>\n<li>Phar</li>\n<li>Rar</li>\n<li>Zip</li>\n<li>Zlib</li>\n</ul>\n\n<p>I am looking for the best option. In other words witch one is the fastes?</p>\n\n<p>Also I am thinking of splitting the file into multiple ones.</p>\n\n<p>I though about reformatting the files into a giant string and then use a PHP script to split the string after a certain amount of characters. But I am not sure this is the fastes way.</p>\n\n<p>To put the file into a string I was thinking of using base64_encode... Is this the best way?</p>\n\n<p>Thanks in advance for any tip and help.</p>\n"},{"tags":["mysql","performance","relevance","full-text"],"answer_count":3,"favorite_count":4,"up_vote_count":11,"down_vote_count":0,"view_count":1721,"score":11,"question_id":237970,"title":"Full-text search relevance is measured in?","body":"<p>I am making a quiz system, and when quizmakers insert questions into the Question Bank, I am to check the DB for duplicate / very highly similar questions.</p>\n\n<p>Testing MySQL's <a href=\"http://dev.mysql.com/doc/refman/5.0/en/fulltext-search.html#function_match\" rel=\"nofollow\">MATCH() ... AGAINST()</a>, the highest relevance I get is 30+, when I test against a 100% similar string.</p>\n\n<p>So what exactly is the relevance? To quote the <a href=\"http://dev.mysql.com/doc/refman/5.0/en/fulltext-natural-language.html\" rel=\"nofollow\">manual</a>:</p>\n\n<blockquote>\n  <p>Relevance values are non-negative floating-point numbers. Zero relevance means no similarity. Relevance is computed based on the number of words in the row, the number of unique words in that row, the total number of words in the collection, and the number of documents (rows) that contain a particular word. </p>\n</blockquote>\n\n<p>My problem is how to test the relevance value if a string is a duplicate. If it's 100% duplicate, prevent it from being inserter into Question Bank. But if it is only so similar, prompt the quizmaker to verify, insert or not. So how do I do that? 30+ for 100% identical string is not percentage, so I'm stump.</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["ruby-on-rails","database","performance","nosql"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":54,"score":0,"question_id":12971230,"title":"Rails3: Should I switch to NoSQL? Alot of calculations, alot of data for user-matching","body":"<p>I'm at a stage of my project where I'm thinking about moving to NoSQL for performance reasons.  I will definitely have tables with millions of rows, so NoSQL might be useful. But my problem is I'm also doing alot of calculations with this data and I don't know if that would give me that much more performance if rails still has to do all the calculations.</p>\n\n<p>Here's another question of mine where I describe what data I need and how I process it: <a href=\"http://stackoverflow.com/questions/12944813/rails-3-user-matching-algorithm-to-sql-query-complicated/\">Rails 3 user matching-algorithm to SQL Query (COMPLICATED)</a></p>\n\n<p>After I realized most of my code in SQL and match one user with 1000 other users it still took</p>\n\n<pre><code>Completed 200 OK in 104871ms (Views: 2146.0ms | ActiveRecord: 93780.5ms)\n(on my local machine with sqlite)\n</code></pre>\n\n<p>And that's not acceptable for me.\nI would definitely be able to denormalize my tables into one for this to work. But will this give me a performance boost?</p>\n\n<p>I also thought about storing the calculated match percentages IN the database, but that would result in 2.5 billion rows for just 50k users.</p>\n"},{"tags":["performance","entity-framework"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":32,"score":1,"question_id":12990662,"title":"EF5 .Local performance","body":"<p>I'm doing this on a table with ~43k rows:</p>\n\n<pre><code>MyDbContext.Stores.Load();\nMyDbContext.Stores.Local.Count.Dump(); //horrible performance!\n</code></pre>\n\n<p>I can see through the profiler that the first instruction fires up the select statement to fetch all rows. Actually the second instruction returns the correct value but after ~12 seconds, and it is not what I was expecting considering that all data should be in memory.\nWhat is wrong (or what is its real purpose) with .Local in Entity Framework?</p>\n"},{"tags":["ios","performance","transparency"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":12990321,"title":"iOS Performance troubles with transparency","body":"<p>I just generated a gradient with transparency programmatically by adding a solid color and a gradient to an image mask. I then applied the resulting image to my UIView.layer.content. The visual is fine, but when I scroll object under the transparency, the app gets chunky. Is there a way to speed up?</p>\n\n<p>My minital thought was caching the resulting gradient. Another thought was to create a gradient that is only one pixel wide and stretch it to cover the desired area. Will either of these approaches help the performance?</p>\n\n<p>Joe </p>\n"},{"tags":["php","string","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":6,"view_count":158,"score":-5,"question_id":8304341,"title":"Which is faster, concatenating a string or imploding an array of strings?","body":"<p>Got into a discussion about the performance issues of combing a string.</p>\n\n<p>Example</p>\n\n<pre><code>$var1 = 'abc';\n$var2 = 'def';\n$var3 = 'hij';\n</code></pre>\n\n<p>Would it be faster to combine these by doing</p>\n\n<pre><code>implode('', array($var1, $var2, $var3));\n</code></pre>\n\n<p>Or would it be faster to do</p>\n\n<pre><code>$var1.$var2.$var3;\n</code></pre>\n"},{"tags":["mysql","performance","postgresql","database-performance","postgresql-performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":63,"score":1,"question_id":12986656,"title":"Postgres equivalent of MySQL's BENCHMARK() function","body":"<p>I am using Postgresql. I want to test how much time a function takes to execute. Since the fucntion takes only a few milliseconds, I would like to call it in a loop 1000s of times to get an accurate figure. </p>\n\n<p>MySQL has a BENCHMARK() function to do this. Is there an equivalent or do I have to write a procedure with a loop to do this?</p>\n"},{"tags":["asp.net","performance","iis","cassini"],"answer_count":1,"favorite_count":2,"up_vote_count":0,"down_vote_count":0,"view_count":81,"score":0,"question_id":12954150,"title":"Why is IIS slower than ASP.NET Development Server?","body":"<p>I have an ASPX webpage that does some complex operations and database calls. When I view the webpage by running ASP.NET Development Server (Cassini), it takes about <em>200ms</em>.</p>\n\n<p>Then, <em>without any code changes and configuration changes</em>, I deploy the website to my local machine IIS 7 and view the same web page again. It takes <em>2.0sec</em>, which is <strong>10 times slower</strong>.</p>\n\n<p>I thought IIS should be faster than (or at least as fast as) Cassini.</p>\n\n<p>To investigate further, I created a new page, test1.aspx, which contains nothing but an empty for-loop that runs for 90 million times in the Page_Load. In Cassini, it takes about 200ms. In IIS, it takes 300ms (50% slower).</p>\n\n<p>What could be the reason that makes IIS slower than Cassini? Or, perhaps an even better question, how can I make IIS run at least as fast as Cassini? </p>\n"},{"tags":["php","performance","memory-management","apc"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":222,"score":1,"question_id":2884413,"title":"Does PHP's APC reclaim memory after apc_delete() is called?","body":"<p>More generally, does anyone know where the way APC works internally is documented?</p>\n"},{"tags":["php","performance","apc"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":33,"score":1,"question_id":12989424,"title":"PHP APC doesn't cache mixed php/html files","body":"<p>I seen, through the APC manager, which APC caches only \"pure\" php files, while files with php and raw html mixed aren't cached. Does a solution exist for this question?</p>\n\n<p>This a example of mixed code:</p>\n\n<pre><code>&lt;html&gt;\n&lt;p&gt;fgsfsdfs &lt;?php echo \"test\" ?&gt;&lt;/p&gt;\n&lt;/html&gt;\n</code></pre>\n\n<p>Could I resolve, including all raw html in php \"echo function\"? How would be performance in this case?</p>\n"},{"tags":["algorithm","performance","sorting","quicksort"],"answer_count":6,"favorite_count":0,"up_vote_count":12,"down_vote_count":0,"view_count":1927,"score":12,"question_id":4146843,"title":"When should we use Radix sort?","body":"<p>It seems Radix sort has a very good average case performance, i.e. <em>O(kN)</em>: <a href=\"http://en.wikipedia.org/wiki/Radix_sort\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Radix_sort</a></p>\n\n<p>but it seems most people still are using Quick Sort, don't they?</p>\n"},{"tags":["performance","ms-access"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":2679,"score":2,"question_id":2405226,"title":"How to populate an array with recordset data","body":"<p>I am attempting to move data from a recordset directly into an array. I know this is possible, but specifically I want to do this in VBA as this is being done in MS Access 2003.</p>\n\n<p>Typically I would do something like the following to achieve this:</p>\n\n<pre><code>    Dim vaData As Variant \n    Dim rst As ADODB.Recordset\n\n    ' Pull data into recordset code here...\n\n    ' Populate the array with the whole recordset.\n    vaData = rst.GetRows \n</code></pre>\n\n<p>What differences exist between VB and VBA which makes this type of operation not work?</p>\n\n<p>What about performance concerns? Is this an \"expensive\" operations?</p>\n"},{"tags":["mysql","sql","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":73,"score":3,"question_id":12988199,"title":"MySQL database structure: more columns or more rows","body":"<p>I collect how people tag topics with categories in table like:</p>\n\n<pre><code>ID | topic_id | votes_Category_1 | votes_Category_2 |.......... | votes_Category_12\n</code></pre>\n\n<p>I dump this table every hour for history reasons.\nLets say table contain 2 million rows. dumped every hour in history tables.</p>\n\n<p>This solution is not flexible if I want to add column Category_13, so I'm thinking about this one:</p>\n\n<pre><code>ID | topic_id | Category_id | vote_count\n</code></pre>\n\n<p>This solution will create 12 rows per topic, its better structured and more flexible but I'll have to dump every hour 24 million rows.</p>\n\n<p>I need the best 10 topics in each category!\nI wonder in Case 2 if using Max on votes (where category_id=x and topic_id=y) will be slower than in case 1: Order by categoy_x where topic_id=y</p>\n\n<p>Which one would be better JUST!!! from performance stand point:</p>\n\n<ol>\n<li>To have 2 million rows with 14 columns</li>\n<li>To have 24 million rows with 4 columns</li>\n</ol>\n\n<p>Thank you</p>\n"},{"tags":["c#",".net","performance","logging"],"answer_count":6,"favorite_count":7,"up_vote_count":10,"down_vote_count":0,"view_count":2327,"score":10,"question_id":1348643,"title":"How performant is StackFrame?","body":"<p>I am considering using something like <code>StackFrame stackFrame = new StackFrame(1)</code> to log the executing method, but I don't know about its performance implications. Is the stack trace something that is build anyway with each method call so performance should not be a concern or is it something that is only build when asked for it? Do you recommend against it in an application where performance is very important? If so, does that mean I should disable it for the release?</p>\n"},{"tags":["c#","performance","xna"],"answer_count":3,"favorite_count":2,"up_vote_count":6,"down_vote_count":0,"view_count":183,"score":6,"question_id":5432073,"title":"What books, articles, blog posts about XNA performance?","body":"<p>Do you know some books, articles, blogs about how to write code to keep good performance in XNA?<br>\nI know that there are principles in XNA, strict rules what to do and what to avoid, what solutions are better.  </p>\n"},{"tags":["c++","performance","linker","exe"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":58,"score":0,"question_id":12755482,"title":"EXE global slowdown","body":"<p>I have an exe containing Fortran and C++ code that was recently modified to link against some custom static libraries.  The exe now runs significantly slower (~ factor of 2) than before in every function, even though it is not calling any new code in the test run.  The call graph and function hit count has been checked in a profiler and validates the assertion that the new code is not being called.</p>\n\n<p>At link time there are now numerous \"multiple definition\" warnings caused by the new code.  However, optimisations are still enabled and no other compiler or environment settings have been modified.  The exe is not significantly larger than before and the memory footprint is the same in both runs.</p>\n\n<p>Any ideas what might cause this?</p>\n"},{"tags":["c++","performance","matrix","io","binary"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":75,"score":0,"question_id":12987796,"title":"Reading big binary files in c++, performance difference compiling with makefile or not","body":"<p>It's being years I read this forum but this is my first question. I have a program which reads a really big matrix (~500 megabytes) of doubles stored in binary. If I compile my program #including all the libraries etc.. the matrices get read in few seconds, if I use a makefile to link all the object files, the program read the matrix in more than 45 seconds!.</p>\n\n<p>I have changed nothing on the program except for compiling it with a makefile.</p>\n\n<p>I do numerical computation and performance is important. Is there an explanation and hopefully a way out?</p>\n\n<p>I'm using a mac, runing lion, g++ 4.2 </p>\n"},{"tags":["performance","category","opencart"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":1907,"score":1,"question_id":9915941,"title":"Opencart extremely slow loading speed","body":"<p>I'm running Opencart 1.5.2 on my server and after importing a large number of products I got a massive speed down. I tried installing a vq mod <a href=\"http://www.opencart.com/index.php?route=extension/extension/info&amp;extension_id=3444\" rel=\"nofollow\">http://www.opencart.com/index.php?route=extension/extension/info&amp;extension_id=3444</a> which had to speed up the site... it didn't.</p>\n\n<p>Store url: <a href=\"http://abvbooks.com/alba-books/\" rel=\"nofollow\">http://abvbooks.com/alba-books/</a></p>\n\n<p>I know I have some elements on the site which are relatively big but before the import it was running fine.</p>\n\n<p>I appreciate the help.</p>\n"},{"tags":["performance","methods","log4j"],"answer_count":3,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":234,"score":4,"question_id":9979044,"title":"log4j: fastest way to display method name?","body":"<p>In my logging messages, I need to insert the name of the method where the messages were produced. I've looked at Log4J documentation and \"M\" and \"l\" conversion chars that also have warning like \"WARNING Generating caller location information is extremely slow and should be avoided unless execution speed is not an issue\". So I have (at least) two options:</p>\n\n<ol>\n<li>Use these chars but slow down my code</li>\n<li>Manually insert method name into messages, i.e. something like this <code>log.info(\"myMethod:  message\");</code> which will be faster but not as elegant</li>\n</ol>\n\n<p>Are there any other options that would not slow down my code?</p>\n\n<p>Thanks!</p>\n"},{"tags":["performance","postgresql","index"],"answer_count":3,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":2144,"score":3,"question_id":4009062,"title":"Very slow bitmap heap scan in Postgres","body":"<p>I have the following simple table that contains traffic measurement data:</p>\n\n<pre><code>CREATE TABLE \"TrafficData\"\n(\n  \"RoadID\" character varying NOT NULL,\n  \"DateID\" numeric NOT NULL,\n  \"ExactDateTime\" timestamp NOT NULL,\n  \"CarsSpeed\" numeric NOT NULL,\n  \"CarsCount\" numeric NOT NULL\n)\nCREATE INDEX \"RoadDate_Idx\" ON \"TrafficData\" USING btree (\"RoadID\", \"DateID\");\n</code></pre>\n\n<p>The column RoadID uniquely identifies the road whose data is being recorded, while DateID identifies the day of the year (1..365) of the data - basically a rounded off representation of ExactDateTime.</p>\n\n<p>I have about 100.000.000 rows; there are 1.000 distinct values in the column \"RoadID\" and 365 distinct values in the column \"DateID\".</p>\n\n<p>I then run the following query:</p>\n\n<pre><code>SELECT * FROM \"TrafficData\"\nWHERE \"RoadID\"='Station_1'\nAND \"DateID\"&gt;20100610 AND \"DateID\"&lt;20100618;\n</code></pre>\n\n<p>This takes up to three mind-boggling seconds to finish, and I cannot for the life of me figure out WHY.</p>\n\n<p>EXPLAIN ANALYZE gives me the following output:</p>\n\n<pre><code>Bitmap Heap Scan on \"TrafficData\"  (cost=104.84..9743.06 rows=2496 width=47) (actual time=35.112..2162.404 rows=2016 loops=1)\n  Recheck Cond: (((\"RoadID\")::text = 'Station_1'::text) AND (\"DateID\" &gt; 20100610::numeric) AND (\"DateID\" &lt; 20100618::numeric))\n  -&gt;  Bitmap Index Scan on \"RoadDate_Idx\"  (cost=0.00..104.22 rows=2496 width=0) (actual time=1.637..1.637 rows=2016 loops=1)\n        Index Cond: (((\"RoadID\")::text = 'Station_1'::text) AND (\"DateID\" &gt; 20100610::numeric) AND (\"DateID\" &lt; 20100618::numeric))\nTotal runtime: 2163.985 ms\n</code></pre>\n\n<p>My specs:</p>\n\n<ul>\n<li>Windows 7</li>\n<li>Postgres 9.0</li>\n<li>4GB RAM</li>\n</ul>\n\n<p>I'd greatly appreciate any helpful pointers!</p>\n"},{"tags":["mysql","xml","performance","caching"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":45,"score":0,"question_id":12981893,"title":"Is it a good idea to cache/read data with xml instead of reading from MySQL database?","body":"<p>I'v been running my own blog system for a while, and I noticed that for most of the pages, several (5-10) same databse queries need to be performed everytime the page gets visited.</p>\n\n<p>Now I'm thinking of caching data into a xml file. Here's my plan of doing this:</p>\n\n<p>--- When posting/editing a blog, insert/update data into MySQL database, and generate the xml file</p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;data&gt;\n    &lt;id&gt;32&lt;/id&gt;\n    &lt;title&gt;caching data with xml&lt;/title&gt;\n    &lt;date&gt;2012-10-01&lt;/date&gt;\n    &lt;content&gt;&lt;![CDATA[&lt;p&gt;blah...blah...blah...&lt;/p&gt;]]&gt;&lt;/content&gt;\n&lt;/data&gt;\n</code></pre>\n\n<p>--- When the page gets visited, check for file existence first (just in case), and then parse the xml file and format/output data.</p>\n\n<pre><code>if(file_exists(\"path/to/blog/32.xml\")) {\n    $data = simplexml_load_file(\"path/to/blog/32.xml\");\n    echo '&lt;h2'&gt;.$data-&gt;title.'&lt;/h2&gt;';\n    echo '&lt;p'&gt;.$data-&gt;date.'&lt;/p&gt;';\n    echo $data-&gt;content;\n}\nelse {\n    mysql_query(...);\n}\n</code></pre>\n\n<p>By doing this MySQL could do much less work, but I'm not quite sure if this is gonna cause any problems later, like 100 or 300 visitors are visiting the same page at the same time. Can PHP handle that? Am I doing it right with this method?</p>\n\n<p>Thank in advance for any info and tips.</p>\n\n<p>BTW, I'm not thinking of using those templates yet.</p>\n"},{"tags":["c#",".net","performance","algorithm","optimization"],"answer_count":20,"favorite_count":18,"up_vote_count":63,"down_vote_count":1,"view_count":51747,"score":62,"question_id":228038,"title":"Best way to reverse a string","body":"<p>I've just had to write a string reverse function in C# 2.0 (i.e. LINQ not available) and came up with this:</p>\n\n<pre><code>public string Reverse(string text)\n{\n    char[] cArray = text.ToCharArray();\n    string reverse = String.Empty;\n    for (int i = cArray.Length - 1; i &gt; -1; i--)\n    {\n        reverse += cArray[i];\n    }\n    return reverse;\n}\n</code></pre>\n\n<p>Personally I'm not crazy about the function and am convinced that there's a better way to do it. Is there?</p>\n"},{"tags":["c#","performance","caching","sortedlist","mmo"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":126,"score":2,"question_id":12872801,"title":"Improving Game Performance C#","body":"<p>We are all aware of the popular trend of MMO games. where players face each other live. However during gameplay there is a tremendous flow of SQL inserts and queries, as given below</p>\n\n<ul>\n<li>There are average/minimum 100 tournaments online per 12 minutes or 500 players / hour</li>\n<li>In Game Progress table, We are storing each player move \n<ul>\n<li>12 round tournament of 4 player there can be 48 records</li>\n<li>plus around same number for spells or special items</li>\n<li>a total of 96 per tournament or 48000 record inserts per hour (500 players/hour)</li>\n</ul></li>\n</ul>\n\n<p>In reponse to my previous question ( <a href=\"http://stackoverflow.com/questions/12668680/improve-mmo-game-performance\">Improve MMO game performance</a> ), I changed the schema and we are not writing directly to database. </p>\n\n<p>Instead accumulating all values in a <code>DataTable</code>. The process then whenever the <code>DataTable</code> has more than 100k rows (which can sometimes be even within the hour) writes to a text file in csv format. Another background application which frequently scans the folder for CSV files, reads any available CSV file and stores the information into server database.</p>\n\n<p><strong>Questions</strong></p>\n\n<ol>\n<li><p>Can we access the datatable present in the game application from another application, directly (it reads the datatable and clears records that have read). So that the in place of writing and reading from disk, we read and write directly from memory.</p></li>\n<li><p>Is there any method that is quicker that DataTable, that can hold large data and yet be fairly quicker in sorting and updating operation. Because we have to frequenly scan for userids, update game status (almost at every insert). It can be a cache utility OR a fast\nScan/Search algorithm OR even a CollectionModel. Right now, we use a foreach loop to go through all records in a <code>DataTable</code> and update rows if user is present. If not then we create a new row. I tried using <code>SortedList</code> and classes, but then it not only doubles the effort, memory usage increases tremendously slowing down overall game performance.</p></li>\n</ol>\n\n<p>thanks</p>\n\n<p>arvind</p>\n"},{"tags":["performance","cpu","low-level"],"answer_count":3,"favorite_count":1,"up_vote_count":1,"down_vote_count":1,"view_count":57,"score":0,"question_id":12977050,"title":"What has a better performance: multiplication or division?","body":"<p>Which version is faster ?\n<code>x * 0.5\nor\nx / 2</code></p>\n\n<p>Ive had a course at the university called computer systems some time ago. From back then i remember that multiplying two values can be achieved with comparably \"simple\" logical gates but division is not a \"native\" operation and requires a sum register that is in a loop increased by the divisor and compared to the dividend.</p>\n\n<p>Now i have to optimise an algorithm with a lot of divisions. Unfortunately its not just dividing by two so binary shifting is no option. Will it make a difference to change all divisions to multiplications ?</p>\n\n<p>update:</p>\n\n<p>I have changed my code and didnt notice any difference. You're probably right about compiler optimisations. Since all the answers were great ive upvoted them all. I chose rahul's answer because of the great link. </p>\n"},{"tags":["c++","windows","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":12984782,"title":"Improving Delay of Context Switch Monitor","body":"<p>So I've written a program that continually polls the operating system to see if a specific process has experienced any context switches. In addition, this program sends a signal of either \"Process Sleeping\" if it has detected 0 context switches since last polling, or \"Process Context Switching\" if it has detected more than 0 context switches since last polling. It sends this signal to a receiver program running on the same machine. Here is the code I use to do this:</p>\n\n<pre><code>#include \"stdafx.h\"\n#include &lt;iostream&gt;\n#include &lt;windows.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;conio.h&gt;\n#include &lt;pdh.h&gt;\n#include &lt;pdhmsg.h&gt;\n#include &lt;iomanip&gt;\n#include &lt;string.h&gt;\n#pragma comment(lib, \"pdh.lib\")\n\nusing namespace std;\nCONST ULONG SAMPLE_INTERVAL_MS    = 0;\nCONST PWSTR BROWSE_DIALOG_CAPTION = L\"Select a counter to monitor.\";\nstatic SOCKET m_socket;\n\nstatic void sendSignal(char * sendbuf){\n    int bytesSent = send(m_socket, sendbuf, strlen(sendbuf), 0);\n}\n\nstatic int initializeConnection(){\n    WORD wVersionRequested;\n    WSADATA wsaData;\n    int wsaerr;\n\n    // Using MAKEWORD macro, Winsock version request 2.2\n    wVersionRequested = MAKEWORD(2, 2);\n\n    wsaerr = WSAStartup(wVersionRequested, &amp;wsaData);\n    if (wsaerr != 0)\n    {\n        /* Tell the user that we could not find a usable WinSock DLL.*/\n        printf(\"Server: The Winsock dll not found!\\n\");\n        return 0;\n    }\n    else\n    {\n        printf(\"Server: The Winsock dll found!\\n\");\n        printf(\"Server: The status: %s.\\n\", wsaData.szSystemStatus);\n    }\n\n    /* Confirm that the WinSock DLL supports 2.2.*/\n    /* Note that if the DLL supports versions greater    */\n    /* than 2.2 in addition to 2.2, it will still return */\n    /* 2.2 in wVersion since that is the version we      */\n    /* requested.                                        */\n    if (LOBYTE(wsaData.wVersion) != 2 || HIBYTE(wsaData.wVersion) != 2 )\n    {\n        /* Tell the user that we could not find a usable WinSock DLL.*/\n        printf(\"Server: The dll do not support the Winsock version %u.%u!\\n\", LOBYTE(wsaData.wVersion), HIBYTE(wsaData.wVersion));\n        WSACleanup();\n        return 0;\n    }\n    else\n    {\n        printf(\"Server: The dll supports the Winsock version %u.%u!\\n\", LOBYTE(wsaData.wVersion), HIBYTE(wsaData.wVersion));\n        printf(\"Server: The highest version this dll can support: %u.%u\\n\", LOBYTE(wsaData.wHighVersion), HIBYTE(wsaData.wHighVersion));\n    }\n\n    //////////Create a socket////////////////////////\n    //Create a SOCKET object called m_socket.\n    //SOCKET m_socket;\n\n    // Call the socket function and return its value to the m_socket variable.\n    // For this application, use the Internet address family, streaming sockets, and\n    // the TCP/IP protocol.\n    // using AF_INET family, TCP socket type and protocol of the AF_INET - IPv4\n    m_socket = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);\n\n    // Check for errors to ensure that the socket is a valid socket.\n    if (m_socket == INVALID_SOCKET)\n    {\n        printf(\"Server: Error at socket(): %ld\\n\", WSAGetLastError());\n        WSACleanup();\n        return 0;\n    }\n    else\n    {\n        printf(\"Server: socket() is OK!\\n\");\n    }\n\n    ////////////////bind//////////////////////////////\n    // Create a sockaddr_in object and set its values.\n    sockaddr_in service;\n\n    // AF_INET is the Internet address family.\n    service.sin_family = AF_INET;\n    // \"127.0.0.1\" is the local IP address to which the socket will be bound.\n    service.sin_addr.s_addr = inet_addr(\"127.0.0.1\");\n    // 55555 is the port number to which the socket will be bound.\n    service.sin_port = htons(55555);\n\n    // Call the bind function, passing the created socket and the sockaddr_in structure as parameters.\n    // Check for general errors.\n    if (bind(m_socket, (SOCKADDR*)&amp;service, sizeof(service)) == SOCKET_ERROR)\n    {\n        printf(\"Server: bind() failed: %ld.\\n\", WSAGetLastError());\n        closesocket(m_socket);\n        return 0;\n    }\n    else\n    {\n        printf(\"Server: bind() is OK!\\n\");\n    }\n\n    // Call the listen function, passing the created socket and the maximum number of allowed\n    // connections to accept as parameters. Check for general errors.\n    if (listen(m_socket, 10) == SOCKET_ERROR)\n        printf(\"Server: listen(): Error listening on socket %ld.\\n\", WSAGetLastError());\n    else\n    {\n        printf(\"Server: listen() is OK, I'm waiting for connections...\\n\");\n    }\n\n    // Create a temporary SOCKET object called AcceptSocket for accepting connections.\n    SOCKET AcceptSocket;\n\n    // Create a continuous loop that checks for connections requests. If a connection\n    // request occurs, call the accept function to handle the request.\n    printf(\"Server: Waiting for a client to connect...\\n\" );\n    printf(\"***Hint: Server is ready...run your client program...***\\n\");\n    // Do some verification...\n    while (1)\n    {\n        AcceptSocket = SOCKET_ERROR;\n        while (AcceptSocket == SOCKET_ERROR)\n        {\n            AcceptSocket = accept(m_socket, NULL, NULL);\n        }\n        // else, accept the connection...\n        // When the client connection has been accepted, transfer control from the\n        // temporary socket to the original socket and stop checking for new connections.\n        printf(\"Server: Client Connected!\\n\");\n        m_socket = AcceptSocket;\n        break;\n    }\n    return 0;\n}\n\n\n\nvoid wmain(void)\n{\n    PDH_STATUS Status;\n    HQUERY Query = NULL;\n    HCOUNTER Counter;\n    PDH_FMT_COUNTERVALUE DisplayValue;\n    DWORD CounterType;\n    SYSTEMTIME SampleTime;\n    PDH_BROWSE_DLG_CONFIG BrowseDlgData;\n    WCHAR CounterPathBuffer[PDH_MAX_COUNTER_PATH] = L\"\\\\\\\\HP1040915\\\\Thread(tiny/0)\\\\Context Switches/sec\";\n\n    initializeConnection();\n\n\n    //\n    // Create a query.\n    //\n    Status = PdhOpenQuery(NULL, NULL, &amp;Query);\n\n    if (Status != ERROR_SUCCESS) \n    {\n        wprintf(L\"\\nPdhOpenQuery failed with status 0x%x.\", Status);\n        goto Cleanup;\n    }\n\n    //\n    // Initialize the browser dialog window settings.\n    //\n\n\n\n    ZeroMemory(&amp;BrowseDlgData, sizeof(PDH_BROWSE_DLG_CONFIG));\n\n    BrowseDlgData.bIncludeInstanceIndex = FALSE;\n    BrowseDlgData.bSingleCounterPerAdd = TRUE;\n    BrowseDlgData.bSingleCounterPerDialog = TRUE;\n    BrowseDlgData.bLocalCountersOnly = FALSE;\n    BrowseDlgData.bWildCardInstances = TRUE;\n    BrowseDlgData.bHideDetailBox = TRUE;\n    BrowseDlgData.bInitializePath = FALSE;\n    BrowseDlgData.bDisableMachineSelection = FALSE;\n    BrowseDlgData.bIncludeCostlyObjects = FALSE;\n    BrowseDlgData.bShowObjectBrowser = FALSE;\n    BrowseDlgData.hWndOwner = NULL;\n    BrowseDlgData.szReturnPathBuffer = CounterPathBuffer;\n    BrowseDlgData.cchReturnPathLength = PDH_MAX_COUNTER_PATH;\n    BrowseDlgData.pCallBack = NULL;\n    BrowseDlgData.dwCallBackArg = 0;\n    BrowseDlgData.CallBackStatus = ERROR_SUCCESS;\n    BrowseDlgData.dwDefaultDetailLevel = PERF_DETAIL_WIZARD;\n    BrowseDlgData.szDialogBoxCaption = BROWSE_DIALOG_CAPTION;\n\n    //\n    // Add the selected counter to the query.\n    //\n\n    wprintf(L\"\\nCounter selected: %s\\n\", CounterPathBuffer);\n\n    Status = PdhAddCounter(Query, CounterPathBuffer, 0, &amp;Counter);\n    if (Status != ERROR_SUCCESS) \n    {\n        wprintf(L\"\\nPdhAddCounter failed with status 0x%x.\", Status);\n        goto Cleanup;\n    }\n\n    //\n    // Most counters require two sample values to display a formatted value.\n    // PDH stores the current sample value and the previously collected\n    // sample value. This call retrieves the first value that will be used\n    // by PdhGetFormattedCounterValue in the first iteration of the loop\n    // Note that this value is lost if the counter does not require two\n    // values to compute a displayable value.\n    //\n\n    Status = PdhCollectQueryData(Query);\n    while(Status != ERROR_SUCCESS)\n        Status = PdhCollectQueryData(Query);\n    if (Status != ERROR_SUCCESS) \n    {\n        wprintf(L\"\\nPdhCollectQueryData failed with 0x%x.\\n\", Status);\n        goto Cleanup;\n    }\n\n    //\n    // Print counter values until a key is pressed.\n    //\n\n    while (!_kbhit()) \n    {\n        Sleep(SAMPLE_INTERVAL_MS);\n\n        GetLocalTime(&amp;SampleTime);\n\n        Status = PdhCollectQueryData(Query);\n        if (Status != ERROR_SUCCESS) \n        {\n            wprintf(L\"\\nPdhCollectQueryData failed with status 0x%x.\", Status);\n        }\n\n        FILETIME ft;\n        GetSystemTimeAsFileTime(&amp;ft);\n        unsigned long long tt = ft.dwHighDateTime;\n        tt &lt;&lt;=32;\n        tt |= ft.dwLowDateTime;\n        tt /=10;\n        tt -= 11644473600000000ULL;\n        cout&lt;&lt; \"time is \" &lt;&lt; tt / 1000000&lt;&lt;\"\\n\";*/\n\n        wprintf(L\"\\n\\\"%2.2d/%2.2d/%4.4d %2.2d:%2.2d:%2.2d.%3.3d\\\"\",\n            SampleTime.wMonth,\n            SampleTime.wDay,\n            SampleTime.wYear,\n            SampleTime.wHour,\n            SampleTime.wMinute,\n            SampleTime.wSecond,\n            SampleTime.wMilliseconds);\n\n        //\n        // Compute a displayable value for the counter.\n        //\n\n\n    Status = PdhGetFormattedCounterValue(Counter,\n            PDH_FMT_DOUBLE,\n            &amp;CounterType,\n            &amp;DisplayValue);\n\n\n        if (Status != ERROR_SUCCESS) \n        {\n            wprintf(L\"\\nPdhGetFormattedCounterValue failed with status 0x%x.\", Status);\n            goto Cleanup;\n        }\n\n\n        wprintf(L\",\\\"%.20g\\\"\", DisplayValue.doubleValue);\n        if(DisplayValue.doubleValue == 0)\n            sendSignal(\"Process Sleeping\\n\");\n        else{\n            sendSignal(\"Process Context Switching\\n\");\n            getchar();\n        }\n    }\n\nCleanup:\n\n    //\n    // Close the query.\n    //\n\n    if (Query) \n    {\n        PdhCloseQuery(Query);\n    }\n\n    int x;\n    cin &gt;&gt;x;\n}\n</code></pre>\n\n<p>In order to measure the delay between a context switch happening to the process and my program polling it, I have written another program that gets the time immediately before it programmatically runs the specific process for which I am measuring context switches. I then manually compare this time to the time printed by my context switch program above. Here is the code for that program:</p>\n\n<pre><code>// TheOpener.cpp : Defines the entry point for the console application.\n//\n\n#include \"stdafx.h\"\n#include \"PreciseTimer.h\"\n\n#include &lt;fstream&gt;\n#include &lt;iostream&gt;\n#include &lt;windows.h&gt;\n#include &lt;iomanip&gt;\n#include &lt;stdio.h&gt;\n\n#include &lt;string&gt;\n#include &lt;algorithm&gt;\n\nusing namespace std;\nint _tmain(int argc, _TCHAR* argv[])\n{\n    SYSTEMTIME st;\n    GetLocalTime(&amp;st);\n    wprintf(L\"\\n\\\"%2.2d/%2.2d/%4.4d %2.2d:%2.2d:%2.2d.%3.3d\\\"\",\n            st.wMonth,\n            st.wDay,\n            st.wYear,\n            st.wHour,\n            st.wMinute,\n            st.wSecond,\n            st.wMilliseconds);\n\n    system(\"\\\"\\\"c:\\\\program files\\\\tiny.exe\\\"\");\n\n    int x;\n    cin&gt;&gt;x;\n    return 0;\n}\n</code></pre>\n\n<p>From this experiment, I get an average delay of about 50 milliseconds. My actual question is twofold: Is this a valid way of measuring this delay? If so, is there a way to minimize this delay to the order of nanoseconds? If not, could you suggest a more valid way of measuring the delay? The process in question here is called \"tiny.exe\", as indicated in the above code.</p>\n"},{"tags":["c#","performance","fileinfo"],"answer_count":7,"favorite_count":4,"up_vote_count":12,"down_vote_count":0,"view_count":19002,"score":12,"question_id":6061957,"title":"Get all files and directories in specific path fast","body":"<p>I am creating a backup application where c# scans a directory. Before I use to have something like this in order to get all the files and subfiles in a directory:</p>\n\n<pre><code>DirectoryInfo di = new DirectoryInfo(\"A:\\\\\");\nvar directories= di.GetFiles(\"*\", SearchOption.AllDirectories);\n\nforeach (FileInfo d in directories)\n{\n       //Add files to a list so that later they can be compared to see if each file\n       // needs to be copid or not\n}\n</code></pre>\n\n<p>The only problem with that is that sometimes a file could not be accessed and I get several errors. an example of an error that I get is:<img src=\"http://i.stack.imgur.com/5YH4D.png\" alt=\"error\"></p>\n\n<p>As a result I created a recursive method that will scan all files in the current directory. If there where directories in that directory then the method will be called again passing that directory. The nice thing about this method is that I could place the files inside a try catch block giving me the option to add those files to a List if there where no errors and adding the directory to another list if I had errors.</p>\n\n<pre><code>try\n{\n    files = di.GetFiles(searchPattern, SearchOption.TopDirectoryOnly);               \n}\ncatch\n{\n     //info of this folder was not able to get\n     lstFilesErrors.Add(sDir(di));\n     return;\n}\n</code></pre>\n\n<p>So this method works great the only problem is that when I scan a large directory it takes to much times. How could I speed up this process?   My actual method is this in case you need it. </p>\n\n<pre><code>private void startScan(DirectoryInfo di)\n{\n    //lstFilesErrors is a list of MyFile objects\n    // I created that class because I wanted to store more specific information\n    // about a file such as its comparePath name and other properties that I need \n    // in order to compare it with another list\n\n    // lstFiles is a list of MyFile objects that store all the files\n    // that are contained in path that I want to scan\n\n    FileInfo[] files = null;\n    DirectoryInfo[] directories = null;\n    string searchPattern = \"*.*\";\n\n    try\n    {\n        files = di.GetFiles(searchPattern, SearchOption.TopDirectoryOnly);               \n    }\n    catch\n    {\n        //info of this folder was not able to get\n        lstFilesErrors.Add(sDir(di));\n        return;\n    }\n\n    // if there are files in the directory then add those files to the list\n    if (files != null)\n    {\n        foreach (FileInfo f in files)\n        {\n            lstFiles.Add(sFile(f));\n        }\n    }\n\n\n    try\n    {\n        directories = di.GetDirectories(searchPattern, SearchOption.TopDirectoryOnly);\n    }\n    catch\n    {\n        lstFilesErrors.Add(sDir(di));\n        return;\n    }\n\n    // if that directory has more directories then add them to the list then \n    // execute this function\n    if (directories != null)\n        foreach (DirectoryInfo d in directories)\n        {\n            FileInfo[] subFiles = null;\n            DirectoryInfo[] subDir = null;\n\n            bool isThereAnError = false;\n\n            try\n            {\n                subFiles = d.GetFiles();\n                subDir = d.GetDirectories();\n\n            }\n            catch\n            {\n                isThereAnError = true;                                                \n            }\n\n            if (isThereAnError)\n                lstFilesErrors.Add(sDir(d));\n            else\n            {\n                lstFiles.Add(sDir(d));\n                startScan(d);\n            }\n\n\n        }\n\n}\n</code></pre>\n\n<p>Ant the problem if I try to handle the exception with something like:</p>\n\n<pre><code>DirectoryInfo di = new DirectoryInfo(\"A:\\\\\");\nFileInfo[] directories = null;\n            try\n            {\n                directories = di.GetFiles(\"*\", SearchOption.AllDirectories);\n\n            }\n            catch (UnauthorizedAccessException e)\n            {\n                Console.WriteLine(\"There was an error with UnauthorizedAccessException\");\n            }\n            catch\n            {\n                Console.WriteLine(\"There was antother error\");\n            }\n</code></pre>\n\n<p>Is that if an exception occurs then I get no files.</p>\n"},{"tags":["c#",".net","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":103,"score":0,"question_id":12983385,"title":"More efficient way to calculate CPU Usage","body":"<p>So I'm writing a task manager clone, and right now I'm using this to calculate the CPU usage %s of each process. The problem is this is very slow; I was just wondering if there is a way to speed this up.</p>\n\n<p>Also, I'm not allowed to use PerformanceCounter's methods and/or WMI.</p>\n\n<pre><code>//Omitted:\n// - Process[] processes just holds the currently running processes\n// - rows[] is a list of the rows I have in a table which shows the tables data\n// - rows[2] is the column for CPU usage\n// - rows[0] is the column for PID\n//========\n//Do CPU usages\ndouble totalCPUTime = 0;\nforeach (Process p in processes)\n{\n    try\n    {\n        totalCPUTime += p.TotalProcessorTime.TotalMilliseconds;\n    }\n    catch (System.ComponentModel.Win32Exception e)\n    {\n        //Some processes do not give access rights to view their time.\n    }\n}\nforeach (Process p in processes)\n{\n    double millis = 0;\n    try\n    {\n        millis = p.TotalProcessorTime.TotalMilliseconds;\n    }\n    catch (System.ComponentModel.Win32Exception e)\n    {\n        //Some processes do not give access rights to view their time.\n    }\n    double pct = 100 * millis / totalCPUTime;\n    for (int i = 0; i &lt; rows.Count; i++)\n    {\n        if(rows[i].Cells[0].Value.Equals(p.Id))\n        {\n            rows[i].Cells[2].Value = pct;\n            break;\n        }\n    }\n}\n</code></pre>\n"},{"tags":["asp.net-mvc-3","performance","linq-to-sql"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":54,"score":0,"question_id":12973604,"title":"Using LINQ-to-SQL containers","body":"<p>I'm using MVC3 and currently i'm following a practice such that I declare one instance of DB Container for every controller. I use that container instance for every request coming to that controller. If I need to go to my models for a query or sth, I send that instance as a parameter to the model's function. So for the whole application, I create and use 4-5 different instances of DB Container class. My question is, does this have a good or bad effect on my database operations? Does it matter to create a seperate container instance? What is the proper way to use container classes?</p>\n\n<p>I believe the mentioned class was called DBContext before.</p>\n"},{"tags":["android","performance","sprite","collision"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":12983306,"title":"Android laggs when more than 1 sprite is present","body":"<p>I have a \"simple\" shooting game where I have a cannon in the center bottom that can shoot bitmaps to enemies (sprites). </p>\n\n<p>Now I don't know if its the collision detection or the sprite itself but if I \"spawn\" more than one enemy it starts to lagg, after 15 its completly unplayable.</p>\n\n<p>The only thing the sprite does is moves itself in a direction and changes the picture from a spritesheet and holds some variables, nothing much.</p>\n\n<p>Collision detection is by going through an array of enemies and within that I go through a second array of bullets and for each enemy I check if a bullet has hit it by making a Rect by them both and check for overlaps.</p>\n\n<p>Its a bit of code so I don't know if I should post some of it or not but you can ask me to post it if you need.</p>\n"},{"tags":["php","mysql","query","performance"],"answer_count":10,"favorite_count":1,"up_vote_count":11,"down_vote_count":1,"view_count":1270,"score":10,"question_id":561900,"title":"How many MySQL queries should I limit myself to on a page? PHP / MySQL","body":"<p>Okay, so I'm sure plenty of you have built crazy database intensive pages...  </p>\n\n<p>I am building a page that I'd like to pull all sorts of unrelated database information from.  Here are some sample different queries for this one page:</p>\n\n<ul>\n<li>article content and info</li>\n<li>IF the author is a registered user, their info</li>\n<li>UPDATE the article's view counter</li>\n<li>retrieve comments on the article</li>\n<li>retrieve information for the authors of the comments</li>\n<li>if the reader of the article is signed in, query for info on them</li>\n<li>etc...</li>\n</ul>\n\n<p>I know these are basically going to be pretty lightning quick, and that I could combine some; but I wanted to make sure that this isn't abnormal?</p>\n\n<p>How many fairly normal and un-heavy queries would you limit yourself to on a page?</p>\n"},{"tags":["android","performance","opengl-es","activity"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":139,"score":0,"question_id":10565240,"title":"Android, performance degrading if multiple activities","body":"<p>I have an OpenGL based app, all is working fine so far. The app consists out ouf 2 activities, the main activity containing the OpenGL view and an additional activity that contains a movie player for some cut-scenes.</p>\n\n<p>On some low-end devices (e.g. LG P690, Android 2.3.4) performance degrades significantly after the movie-activity ran. After that the main activity runs so slow that it only reaches about 30 fps instead of 50 fps.</p>\n\n<p>You can \"fix\" it to become fast again, if you leave the app by pushing the home-button and restart it. Then all is fine again - until that movie-view-activity is shown again...</p>\n\n<p>I did some tests and this effect is also happening regardless of what activity #2 actually does, even if it is just a splash-screen or whatever. If I never invoke a second activity then the performance is always high.</p>\n\n<p>Note: the OpenGL activity is <em>not</em> instanciated multiple times, which would probably lead to a similar effect.</p>\n\n<p>Has somebody experienced the same or a similar issue and knows a remedy?</p>\n"},{"tags":["mysql","performance","greatest-n-per-group","filesort"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":83,"score":3,"question_id":12980823,"title":"Improve performance or redesign 'greatest-n-per-group' mysql query","body":"<p>I'm using MySQL5 and I currently have a query that gets me the info I need but I feel like it could be improved in terms of performance.</p>\n\n<p>Here's the query I built (roughly following <a href=\"http://murrayhopkins.wordpress.com/2008/10/28/mysql-left-join-on-last-or-first-record-in-the-right-table/\" rel=\"nofollow\" title=\"http://murrayhopkins.wordpress.com/2008/10/28/mysql-left-join-on-last-or-first-record-in-the-right-table/\">this guide</a>) :</p>\n\n<pre><code>SELECT d.*, dc.date_change, dc.cwd, h.name as hub\nFROM livedata_dom AS d\n      LEFT JOIN ( SELECT dc1.*\n        FROM livedata_domcabling as dc1\n        LEFT JOIN livedata_domcabling AS dc2\n        ON dc1.dom_id = dc2.dom_id AND dc1.date_change &lt; dc2.date_change\n        WHERE dc2.dom_id IS NULL\n        ORDER BY dc1.date_change desc) AS dc ON (d.id = dc.dom_id)\n      LEFT JOIN livedata_hub AS h ON (d.id = dc.dom_id AND dc.hub_id = h.id)\nWHERE d.cluster = 'localhost'\nGROUP BY d.id;\n</code></pre>\n\n<p>EDIT: Using ORDER BY + GROUP BY to avoid getting multiple dom entries in case 'domcabling' has an entry with null date_change and another one with a date for the same 'dom'.</p>\n\n<p>I feel like I'm killing a mouse with a bazooka. This query takes more than 3 seconds with only about 5k entries in 'livedata_dom' and 'livedata_domcabling'. Also, EXPLAIN tells me that 2 filesorts are used:</p>\n\n<pre><code>+----+-------------+------------+--------+-----------------------------+-----------------------------+---------+-----------------+------+----------------------------------------------+\n| id | select_type | table      | type   | possible_keys               | key                         | key_len | ref             | rows | Extra                                        |\n+----+-------------+------------+--------+-----------------------------+-----------------------------+---------+-----------------+------+----------------------------------------------+\n|  1 | PRIMARY     | d          | ALL    | NULL                        | NULL                        | NULL    | NULL            |    3 | Using where; Using temporary; Using filesort |\n|  1 | PRIMARY     | &lt;derived2&gt; | ALL    | NULL                        | NULL                        | NULL    | NULL            |    3 |                                              |\n|  1 | PRIMARY     | h          | eq_ref | PRIMARY                     | PRIMARY                     | 4       | dc.hub_id       |    1 |                                              |\n|  2 | DERIVED     | dc1        | ALL    | NULL                        | NULL                        | NULL    | NULL            |    4 | Using filesort                               |\n|  2 | DERIVED     | dc2        | ref    | livedata_domcabling_dc592d9 | livedata_domcabling_dc592d9 | 4       | live.dc1.dom_id |    2 | Using where; Not exists                      |\n+----+-------------+------------+--------+-----------------------------+-----------------------------+---------+-----------------+------+----------------------------------------------+ \n</code></pre>\n\n<p>How could I change this query to make it more efficient?</p>\n\n<p>Using the dummy data (provided below), this is the expected result:</p>\n\n<pre><code>+-----+-------+---------+--------+----------+------------+-----------+---------------------+------+-----------+\n| id  | mb_id | prod_id | string | position | name       | cluster   | date_change         | cwd  | hub       |\n+-----+-------+---------+--------+----------+------------+-----------+---------------------+------+-----------+\n| 249 | 47    | 47      |     47 |       47 | SuperDOM47 | localhost | NULL                | NULL | NULL      |\n| 250 | 48    | 48      |     48 |       48 | SuperDOM48 | localhost | 2014-04-16 05:23:00 | 32A  | megahub01 |\n| 251 | 49    | 49      |     49 |       49 | SuperDOM49 | localhost | NULL                | 22B  | megahub01 |\n+-----+-------+---------+--------+----------+------------+-----------+---------------------+------+-----------+\n</code></pre>\n\n<p>Basically I need 1 row for every 'dom' entry, with </p>\n\n<ol>\n<li>the 'domcabling' record with the highest date_change\n<ul>\n<li>if record does not exist, I need null fields</li>\n<li>ONE entry may have a null date_change field per dom (null datetime field considered older than any other datetime)</li>\n</ul></li>\n<li>the name of the 'hub', when a 'domcabling' entry is found, null otherwise</li>\n</ol>\n\n<p>CREATE TABLE + dummy INSERT for the 3 tables:</p>\n\n<p><strong>livedata_dom</strong> (about 5000 entries)</p>\n\n<pre><code>CREATE TABLE `livedata_dom` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `mb_id` varchar(12) NOT NULL,\n  `prod_id` varchar(8) NOT NULL,\n  `string` int(11) NOT NULL,\n  `position` int(11) NOT NULL,\n  `name` varchar(30) NOT NULL,\n  `cluster` varchar(9) NOT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `mb_id` (`mb_id`),\n  UNIQUE KEY `prod_id` (`prod_id`),\n  UNIQUE KEY `name` (`name`),\n  UNIQUE KEY `livedata_domgood_string_7bff074107b0e5a0_uniq` (`string`,`position`,`cluster`)\n) ENGINE=InnoDB AUTO_INCREMENT=5485 DEFAULT CHARSET=latin1;\n\nINSERT INTO `livedata_dom` VALUES (251,'49','49',49,49,'SuperDOM49','localhost'),(250,'48','48',48,48,'SuperDOM48','localhost'),(249,'47','47',47,47,'SuperDOM47','localhost');\n</code></pre>\n\n<p><strong>livedata_domcabling</strong> (about 10000 entries and growing slowly)</p>\n\n<pre><code>CREATE TABLE `livedata_domcabling` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `dom_id` int(11) NOT NULL,\n  `hub_id` int(11) NOT NULL,\n  `cwd` varchar(3) NOT NULL,\n  `date_change` datetime DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `livedata_domcabling_dc592d9` (`dom_id`),\n  KEY `livedata_domcabling_4366aa6e` (`hub_id`),\n  CONSTRAINT `dom_id_refs_id_73e89ce0c50bf0a6` FOREIGN KEY (`dom_id`) REFERENCES `livedata_dom` (`id`),\n  CONSTRAINT `hub_id_refs_id_179c89d8bfd74cdf` FOREIGN KEY (`hub_id`) REFERENCES `livedata_hub` (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=5397 DEFAULT CHARSET=latin1;\n\nINSERT INTO `livedata_domcabling` VALUES (1,251,1,'22B',NULL),(2,250,1,'33A',NULL),(6,250,1,'32A','2014-04-16 05:23:00'),(5,250,1,'22B','2013-05-22 00:00:00');\n</code></pre>\n\n<p><strong>livedata_hub</strong> (about 100 entries)</p>\n\n<pre><code>CREATE TABLE `livedata_hub` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `name` varchar(14) NOT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `name` (`name`)\n) ENGINE=InnoDB AUTO_INCREMENT=98 DEFAULT CHARSET=latin;\n\nINSERT INTO `livedata_hub` VALUES (1,'megahub01');\n</code></pre>\n"},{"tags":["c#","asp.net-mvc","performance","iis7","httpmodule"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":56,"score":0,"question_id":12981298,"title":"How to analyze the performance of requests in ASP.NET MVC application?","body":"<p>I would like to capture the hit time, processing time, memory consumption and response time of requests in ASP.NET MVC application.</p>\n\n<p>Is there any way or tool to perform this?</p>\n"},{"tags":["c++","performance","matrix","sse","simd"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":85,"score":0,"question_id":12965291,"title":"How to improve sse based matrix multiplication","body":"<pre><code>float** matrix::mult(float** matrix1){\n  float** result=new float *[n];\n  int i,j,k;\n  for(i=0;i&lt;n;i++){\n    result[i]=new float [n];\n  }\n  vect v1;\n  vect v2;\n  vect v3;\n  vect total;\n  clock_t start, end;\n  start = clock();\n  float result_ij=0;\n  for(i=0;i&lt;n;i++){   \n    for(j=0;j&lt;n;j++){\n      result_ij=0;\n      total.v=_mm_set1_ps(0);\n      for(k=0;k&lt;n;k=k+4){\n        v1.v=_mm_set_ps(user_matrix[k][j],user_matrix[k+1][j],user_matrix[k+2][j],user_matrix[k+3][j]);\n        v2.v=_mm_set_ps(matrix1[i][k],matrix1[i][k+1],matrix1[i][k+2],matrix1[i][k+3]);\n        v3.v=_mm_mul_ps(v1.v,v2.v);\n        total.v=_mm_add_ps(total.v,v3.v);\n      }\n      result[i][j]=total.a[1]+total.a[0]+total.a[2]+total.a[3];\n    }\n  }\n  end = clock();\n  cout&lt;&lt;(double)(end-start)/CLOCKS_PER_SEC&lt;&lt;endl;\n  return result;\n}\n</code></pre>\n\n<p>This code is about exactly the same speed as the scalar code. I can't see why this would be so slow, it was compiled with g++and the vect type is a union.</p>\n\n<pre><code>union vect {\n__m128 v;    \nfloat a[4];  \n} ;\n</code></pre>\n\n<p>For the matrix as a multidimensional array, what is also the fastest way to load that into the sse register?</p>\n"},{"tags":["ruby-on-rails","ruby","performance","rubygems"],"answer_count":2,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":65,"score":4,"question_id":12979171,"title":"How number of gems influences on Rails app performance?","body":"<p>I'm new to Rails and this maybe a dumb question, but I wonder how number of gems influences on Rails app performance? Does it become slower the more gems you add? Are all gems get called on every request? </p>\n\n<p>I'm asking this question because, for example, in Django, you import every needed class/method/library in every .py file which calls it. In Rails you're not doing this, everything is \"autoloaded\", but I wonder, what is the cost of such \"autoloading\"? </p>\n\n<p>Does it mean all gems get's called on every request?</p>\n"},{"tags":["c++","performance","sse"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":130,"score":3,"question_id":12971139,"title":"SSE cumulative summing","body":"<p>I have a simple problem. Having a starting uint_32 value (say 125) and a __m128i of operands to add, for example (+5,+10,-1,-5). What I would like to get as fast as possible is a vector (125 + 5, 125 + 5 + 10, 125 + 5 + 10 - 1, 125 + 5 + 10 - 1 - 5), i.e. cumulatively add values from the operands to the starting value. So far the only solution I can think of is doing an addition of 4 __m128i variables. For the example, they would be</p>\n\n<pre><code>/* pseudoSSE code... */\n__m128i src =     (125,125,125,125)\n__m128i operands =(5,10,-1,-5)\n\n/*  Here I omit the partitioning of operands into add1,..add4 for brevity  */\n\n__m128i add1 =    (+05,+05,+05,+05)\n__m128i add2 =    (+00,+10,+10,+10)\n__m128i add3 =    (+00,+00,-01,-01)\n__m128i add4 =    (+00,+00,+00,-05)\n__m128i res1 = _mm_add_epu32( add1, add2 )\n__m128i res2 = _mm_add_epu32( add3, add4 )\n__m128i res3 = _mm_add_epu32( res1, add2 )\n__m128i res  = _mm_add_epu32( res3, src  )\n</code></pre>\n\n<p>Like this, I get what I wanted. For this solution I am going to need to set all add_ variables and then perform 4 additions. What Im really asking is whether this can be done faster. Either via some different algo or maybe using some specialized SSE functions that I do not know yet (something like _mm_cumulative_sum()). Many thanks.</p>\n"},{"tags":["performance","html5","animation","canvas"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":667,"score":3,"question_id":5387706,"title":"What is the fastest way of animating multiple sprites under html5?","body":"<p>I'm trying to animate 50 or so sprites at the same time, using a setInterval of 30ms.</p>\n\n<p>What gives bettter performance?\nUsing canvas? Or using -webkit-transform and divs? Does anyone have any tips on making animations for html5?</p>\n"},{"tags":["java","performance","jvm","hotspot"],"answer_count":2,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":102,"score":4,"question_id":12980029,"title":"Oracle Hotspot JVM: generally, what operations are especially CPU-costly?","body":"<p>I'd like to understand of what types of operations contribute disproportionately to CPU load as well as develop an intuition on relative cost of common operations. To minimize generalizations, please assume Oracle 7 HotSpot JVM. </p>\n\n<p>For example:</p>\n\n<ul>\n<li>does constructing lots of objects cost CPU (I understand it costs memory :-) )?</li>\n<li>does contenting for a monitor cost CPU? ie if we have multiple threads attempting to enter the same synchronized block, do blocked threads also consume CPU cycles? </li>\n<li>relative cost of above operations? For example, \"new'ing a single object costs the same CPU as iterating over a X-element array\"</li>\n</ul>\n\n<p>Any tips on developing an intuition of relative CPU cost of typical operations? </p>\n\n<p>Any good reads on the subject you could recommend?</p>\n\n<p>thank you,</p>\n\n<p><strong>CLARIFICATION</strong></p>\n\n<p>thanks for early responses, but please note I:</p>\n\n<ul>\n<li>am NOT asking 'why is my app slow'</li>\n<li>understand that using a profiler will help identify problems in a specific app and that for example, GC can eat up CPU or that GC'ing tenured generation is more costly than Eden space</li>\n<li>understand that most ops become costly only if executed a lot (ie virtually no op is expensive if used sparingly)</li>\n</ul>\n\n<p>Instead, I am looking for guidance of <em>relative</em> CPU cost, especially w.r.t. above operations (let's assume a 'web-scale' app uses all ops mentioned equal amount - a lot). </p>\n\n<p>For example I already now that:</p>\n\n<ul>\n<li>long method call chains do not contribute significantly to CPU load (so it's generally OK to use method delegation liberally)</li>\n<li>throwing exceptions is more expensive than using conditionals (thus latter is generally preferred for flow-control in highly performance-sensitive code)</li>\n</ul>\n\n<p>...but what about instantiating new objects or contenting for a monitor? Would either of these ops be significant (dominant?) contributors to CPU load (let's say I don't care about latency of heap size) at scale?</p>\n"},{"tags":["javascript","jquery","performance","httprequest","cdn"],"answer_count":16,"favorite_count":44,"up_vote_count":118,"down_vote_count":1,"view_count":28975,"score":117,"question_id":1447184,"title":"Microsoft CDN for jQuery or Google CDN?","body":"<p>Does it actually matter which CDN you use to link to your jquery file or any javascript file for that matter.  Is one potentially faster than the other?  What other factors could play a role in which cdn you decide to use?  I know that Microsoft, Yahoo, and Google all have CDN's now.</p>\n"},{"tags":["ajax","performance","magento"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":46,"score":2,"question_id":12978579,"title":"Is it relevant enough to create a direct .php file as an AJAX target in Magento (instead of standard router path)","body":"<p>I will explain a bit of background here. The AJAX I'm going to use must work very fast, implementing some logic on the backend and returning JSON as response.</p>\n\n<p>I'm not new in Magento development, but I've struggled to create a <em>cheap</em> request using standard Magento request flow. Using profiler I've discovered, that only the <strong>routing</strong> work (including <code>match</code>, <code>preDispatch</code>, <code>rewrite</code> and more light-weighted, but numerous small routing-related functions)  takes almost a second.</p>\n\n<p>Have no doubts, I'm loading all the necessary data to bootstrap application, my script looks very much like scripts in the <code>shell</code> directory.</p>\n\n<p>So using my own <em>.php</em> file instead of Magento router I'm already saving almost a second per request - without even touching the logic. My benchmarking may be not very accurate, but the point is - I'm definitely saving some time, when time is very essential for me.</p>\n\n<p>So the questions are: <strong>is this 1 sec worth neglecting of Magento architecture</strong>? Has anyone implemented something similar? And where is the best place to put such file into, considering Magento Module approach?</p>\n\n<p>I'll be glad, if anyone can point me in the right direction.</p>\n"},{"tags":["php","performance","xdebug","profiler"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":29,"score":0,"question_id":12979830,"title":"PHP performance Target with Xdebug","body":"<p>I begin to work with Xdebug and WinCacheGrind to understand more about the code I wrote.</p>\n\n<p>I'm currently testing a Shopping Cart Object that uses MySQL to store as a persistent session.</p>\n\n<p>Here are the steps the object does on a typical Add to Cart Action:</p>\n\n<ol>\n<li>Construct a Cart Session filled with default values</li>\n<li>Check for an existing Cart Session In MYSQL with\n$_COOKIE['session_id'] AND $_SERVER['REMOTE_ADDR']. If so, mysql\nrow populates the Cart Session  <em>(12ms)</em></li>\n<li>Set Country Code and State Code in the Cart for furter shipping calculation.</li>\n<li>Add an item</li>\n<li>Add item options</li>\n<li>Get Shipping Options (Regular, Express, NextDay) from MYSQL based on Country Code and State Code <em>(9.1ms)</em></li>\n<li>Calculate Shipping Cost for each Options Based on Weight items in the Cart</li>\n<li>Set Discount <em>(0.1ms)</em></li>\n<li>Set User Prefered Shipping Option ex. regular;</li>\n<li>Save Cart Session in MYSQL <em>(93ms)</em>, using php function serialize for cart content.</li>\n<li>Display Cart values in the VIEW.</li>\n</ol>\n\n<p>The only call to db are on step 2, 6, 11.</p>\n\n<p>There will be of course extra DB call to get Item Details, Item Options and Discount Code. But for the example, I keep it minimal.</p>\n\n<p>For This PHP Request, XDebug give a result of \n<strong>Cumulative Time : 130ms.</strong></p>\n\n<p>Is it bad?</p>\n\n<p>And my real question would be, How Fast should a Request should be in \"ms\"? I heard about YouTube who target 200ms Total but, I'm not Google and don't have this team of ultra super genius laser Intelligent 2055 back from the future engineers...</p>\n\n<p>Thanks for the help.</p>\n\n<p>C.</p>\n"},{"tags":["mysql","performance","entity-framework"],"answer_count":3,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":5403,"score":3,"question_id":1233245,"title":"How to optimize Entity Framework Queries","body":"<p>I am using Linq-To-Entities to do a query which is returning only 947 rows but taking 18 seconds to run. I have done a \"ToTraceString\" to get the underlying sql out and ran the same thing directly on the database and get the same timing. </p>\n\n<p>I have used the tuning advisor and created a couple of indexes although with little impact.</p>\n\n<p>Looking at the query execution plan there are a couple of nested loops which are taking up 95% of the time but these are already working on the indexes?</p>\n\n<p>Does anyone have any ideas on how to force some optimisation into the EF query??</p>\n\n<p>EDIT: Supplying additional information</p>\n\n<p>A basic ER diagram with for the three tables is as follows:</p>\n\n<pre><code>People &gt;----People_Event_Link ----&lt; Events\nP_ID        P_ID                    E_ID\n            E_ID\n</code></pre>\n\n<p>The linq that I am running is designed to get all Events back for a particular Person (using the P_ID):</p>\n\n<pre><code>        var query = from ev in genesisContext.Events\n                    join pe in genesisContext.People_Event_Link\n                    on ev equals pe.Event\n                    where pe.P_ID == key\n                    select ev;\n        return query;\n</code></pre>\n\n<p>Here is the generated SQL (deep breath!):</p>\n\n<pre><code>SELECT \n1 AS [C1], \n[Extent1].[E_ID] AS [E_ID], \n[Extent1].[E_START_DATE] AS [E_START_DATE], \n[Extent1].[E_END_DATE] AS [E_END_DATE], \n[Extent1].[E_COMMENTS] AS [E_COMMENTS], \n[Extent1].[E_DATE_ADDED] AS [E_DATE_ADDED], \n[Extent1].[E_RECORDED_BY] AS [E_RECORDED_BY], \n[Extent1].[E_DATE_UPDATED] AS [E_DATE_UPDATED], \n[Extent1].[E_UPDATED_BY] AS [E_UPDATED_BY], \n[Extent1].[ET_ID] AS [ET_ID], \n[Extent1].[L_ID] AS [L_ID]\nFROM  [dbo].[Events] AS [Extent1]\nINNER JOIN [dbo].[People_Event_Link] AS [Extent2] ON  EXISTS (SELECT \n    1 AS [C1]\n    FROM    ( SELECT 1 AS X ) AS [SingleRowTable1]\n    LEFT OUTER JOIN  (SELECT \n    \t[Extent3].[E_ID] AS [E_ID]\n    \tFROM [dbo].[Events] AS [Extent3]\n    \tWHERE [Extent2].[E_ID] = [Extent3].[E_ID] ) AS [Project1] ON 1 = 1\n    LEFT OUTER JOIN  (SELECT \n    \t[Extent4].[E_ID] AS [E_ID]\n    \tFROM [dbo].[Events] AS [Extent4]\n    \tWHERE [Extent2].[E_ID] = [Extent4].[E_ID] ) AS [Project2] ON 1 = 1\n    WHERE ([Extent1].[E_ID] = [Project1].[E_ID]) OR (([Extent1].[E_ID] IS NULL) AND ([Project2].[E_ID] IS NULL))\n)\nWHERE [Extent2].[P_ID] = 291\n</code></pre>\n"},{"tags":["performance","html5","canvas","android-browser","frame-rate"],"answer_count":1,"favorite_count":2,"up_vote_count":6,"down_vote_count":0,"view_count":98,"score":6,"question_id":12921018,"title":"Canvas rendering performance","body":"<p>I am modifying the HTML5 port of the game Jump'n'Bump to run on Apple and Android-based mobile devices. I use a cheap 1 GHz Cortex-A8 Android 4.0.3 tablet for testing. I have encountered strange behaviour in the system's Browser. I normally get a very low frame-rate of about 1 FPS (entire screen is re-drawn every frame, setTimeout is used...). However, when I add a &lt;div&gt; which has a position:fixed CSS attribute before the &lt;canvas&gt; tag, the frame-rate skyrockets and the game becomes playable.</p>\n\n<p>Could someone please explain this odd phenomenon? Are there some rendering modes in the Android Browser which influence canvas performance? Is this a cross-platform issue? How do I make sure the page works efficiently in the user's browser?</p>\n\n<p>An outline of the code I'm working on:</p>\n\n<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;title&gt;Jump'n'Bump - HTML5&lt;/title&gt;\n&lt;meta http-Equiv=\"Cache-Control\" Content=\"no-cache\"&gt;\n&lt;meta http-Equiv=\"Pragma\" Content=\"no-cache\"&gt;\n&lt;meta http-Equiv=\"Expires\" Content=\"0\"&gt;\n&lt;meta name=\"viewport\" content = \"width=400px, user-scaleable=no\"&gt;\n\n&lt;!-- javascript files included here --&gt;\n&lt;script type=\"text/javascript\" src=\"main.js\"&gt;&lt;/script&gt;\n\n&lt;style type=\"text/css\"&gt;\n  body { margin: 0px 0px 0xp 0px }\n  canvas { border: 0px solid black; }\n  img.resource { display:none; }\n  #fixed_div { position: fixed; width: 10px; height: 10px; left: 0px; top: 0px; }\n  #gameArea { position: absolute; left: 0px; top: 0px; width: 400px; height: 256px; background: red; }\n  canvas {\n    image-rendering: optimizeSpeed;             // Older versions of FF\n    image-rendering: -moz-crisp-edges;          // FF 6.0+\n    image-rendering: -webkit-optimize-contrast; // Webkit\n    image-rendering: optimize-contrast;         // Possible future browsers.\n    -ms-interpolation-mode: nearest-neighbor;   // IE\n  }  \n&lt;/style&gt;\n&lt;body onload=\"init()\" text=\"#FFFFFF\" bgcolor=\"#000000\"&gt;\n\n&lt;!-- image resources like this here: --&gt;\n&lt;img class=\"resource\" id='rabbits' src='rabbit.png'/&gt;\n\n&lt;!-- *** remove the line below and the game slows down *** --&gt;\n&lt;div id='fixed_div'&gt;&lt;/div&gt;\n\n&lt;div id=\"gameArea\"&gt;&lt;canvas id=\"screen\" width=\"400\" height=\"256\"&gt;&lt;/canvas&gt;&lt;/div&gt; \n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n"},{"tags":[".net","performance","64bit","32bit-64bit","infragistics"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":81,"score":0,"question_id":12979774,"title":".Net 4.0 app slower on 64 bit than 32 bit (Profiling and possible solutions) (app is using NetAdvantage)","body":"<p>We have got .NET app written in VB .NET 4.0 / VS2010, compiled with all projects set to the AnyCPU setting for both Debug and Release configuration. We have noticed that when this app is run on a 64 bit environment (tested on Windows Server 2003 R2 and 2008 R2) that the app then takes at least double as long (in absolute terms about 25 secs) as opposed to around 6-12 seconds on a 32 bit environment (Win XP and 7) to start up.</p>\n\n<p>I should add that the 64 bit systems are powerful servers, definitely more powerful than the other tested 32 bit systems. All other apps were faster on 64 bit, but not our poor app ;) (And we did test the apps at different times, under different load, and the results are always pretty much the same.)</p>\n\n<p>As said above, the app is built using AnyCPU and it does run as 64 bit assembly under 64 bit OS (checked via TaskManager). The app itself is a WinForms app, making use of NetAdvantage Forms v10.3 and is regularly querying and writing to MS SQL Server 2008.</p>\n\n<p>The different target machines are all on the same network, so the path to the database (same database was used for the performance tests) for example is the same, I don't think the problem is around the database or the network itself. </p>\n\n<p>One thing I did notice, which seemed odd to me, was that when I built in different \"profiling steps\" using a stopwatch during the startup of our MainForm that the InitializeComponent method took twice as long on 64 bit, around 4 seconds opposed to 1.5 on 32 bit.</p>\n\n<p>It's the very same app we deploy on both systems, no different config.</p>\n\n<p>So I have got two questions:</p>\n\n<p>Any idea what the cause of this could be?</p>\n\n<p>And: What's the best way of determining the \"offending\" pieces of code? Currently I use stopwatches and try to narrow it down. But it looks to me like everything is slower on the 64 bit machines as far as our app is concerned, so I am not too sure I can break it down to specific statements.</p>\n\n<p>Thanks all for your help, very much appreciated...</p>\n"},{"tags":["php","mysql","database","performance","overhead"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":61,"score":0,"question_id":12931446,"title":"MySQL connections and PHP","body":"<p>I have a somewhat general question regarding what's best when programming in PHP using also database connections. I am building a project which includes several modules and each module needs to connect to the MySQL sometimes. The module files are included in the main index.php depending on the action selected from the menu by the user. I guess, most projects work this way anyway.</p>\n\n<p>So far what I do is always open the connection at the start of each module file and close it after the queries have run.</p>\n\n<p>My question is this: is it better to open the connection to the database in the beginning of the index.php and close it in the end so to have 1 connection open, or do multiple connections which stay open for less time? What's best for speed and overhead?</p>\n"},{"tags":["java","networking","tcp","sockets","performance"],"answer_count":10,"favorite_count":6,"up_vote_count":7,"down_vote_count":0,"view_count":13552,"score":7,"question_id":1169739,"title":"Java TCP socket: data transfer is slow","body":"<p>I set up a server with a ServerSocket, connect to it with a client machine.  They're directly networked through a switch and the ping time is &lt;1ms.</p>\n\n<p>Now, I try to push a \"lot\" of data from the client to the server through the socket's output stream.  It takes 23 minutes to transfer 0.6Gb.  I can push a much larger file in seconds via scp.</p>\n\n<p>Any idea what I might be doing wrong?  I'm basically just looping and calling writeInt on the socket.  The speed issue doesn't matter where the data is coming from, even if I'm just sending a constant integer and not reading from disk.</p>\n\n<p>I tried setting the send and receive buffer on both sides to 4Mb, no dice.  I use a buffered stream for the reader and writer, no dice.</p>\n\n<p>Am I missing something?</p>\n\n<p>EDIT: code</p>\n\n<p>Here's where I make the socket</p>\n\n<pre><code>System.out.println(\"Connecting to \" + hostname);\n\n\tserverAddr = InetAddress.getByName(hostname);\n\n\t// connect and wait for port assignment\n\tSocket initialSock = new Socket();\n\tinitialSock.connect(new InetSocketAddress(serverAddr, LDAMaster.LDA_MASTER_PORT));\n\tint newPort = LDAHelper.readConnectionForwardPacket(new DataInputStream(initialSock.getInputStream()));\n\tinitialSock.close();\n\tinitialSock = null;\n\n\tSystem.out.println(\"Forwarded to \" + newPort);\n\n\t// got my new port, connect to it\n\tsock = new Socket();\n\tsock.setReceiveBufferSize(RECEIVE_BUFFER_SIZE);\n\tsock.setSendBufferSize(SEND_BUFFER_SIZE);\n\tsock.connect(new InetSocketAddress(serverAddr, newPort));\n\n\tSystem.out.println(\"Connected to \" + hostname + \":\" + newPort + \" with buffers snd=\" + sock.getSendBufferSize() + \" rcv=\" + sock.getReceiveBufferSize());\n\n\t// get the MD5s\n\ttry {\n\t\tbyte[] dataMd5 = LDAHelper.md5File(dataFile),\n\t\t\t   indexMd5 = LDAHelper.md5File(indexFile);\n\n\t\tlong freeSpace = 90210; // ** TODO: actually set this **\n\n\t\toutput = new DataOutputStream(new BufferedOutputStream(sock.getOutputStream()));\n\t\tinput  = new DataInputStream(new BufferedInputStream(sock.getInputStream()));\n</code></pre>\n\n<p>Here's where I do the server-side connection:</p>\n\n<pre><code>\tServerSocket servSock = new ServerSocket();\n\tservSock.setSoTimeout(SO_TIMEOUT);\n\tservSock.setReuseAddress(true);\n\tservSock.bind(new InetSocketAddress(LDA_MASTER_PORT));\n\n\tint currPort = LDA_START_PORT;\n\n\twhile (true) {\n\t\ttry {\n\t\t\tSocket conn = servSock.accept();\n\t\t\tSystem.out.println(\"Got a connection.  Sending them to port \" + currPort);\n\t\t\tclients.add(new MasterClientCommunicator(this, currPort));\n\t\t\tclients.get(clients.size()-1).start();\n\n\t\t\tThread.sleep(500);\n\n\t\t\tLDAHelper.sendConnectionForwardPacket(new DataOutputStream(conn.getOutputStream()), currPort);\n\n\t\t\tcurrPort++;\n\t\t} catch (SocketTimeoutException e) {\n\t\t\tSystem.out.println(\"Done listening.  Dispatching instructions.\");\n\t\t\tbreak;\n\t\t}\n\t\tcatch (IOException e) {\n\t\t\te.printStackTrace();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n</code></pre>\n\n<p>Alright, here's where I'm shipping over ~0.6Gb of data.</p>\n\n<pre><code>public static void sendTermDeltaPacket(DataOutputStream out, TIntIntHashMap[] termDelta) throws IOException {\n\tlong bytesTransferred = 0, numZeros = 0;\n\n\tlong start = System.currentTimeMillis();\n\n\tout.write(PACKET_TERM_DELTA); // header\t\t\n\tout.flush();\n\tfor (int z=0; z &lt; termDelta.length; z++) {\n\t\tout.writeInt(termDelta[z].size()); // # of elements for each term\n\t\tbytesTransferred += 4;\n\t}\n\n\tfor (int z=0; z &lt; termDelta.length; z++) {\n\t\tfor (int i=0; i &lt; termDelta[z].size(); i++) {\n\t\t\tout.writeInt(1);\n\t\t\tout.writeInt(1);\n\t\t}\n\t}\n</code></pre>\n\n<p>It seems pretty straightforward so far...</p>\n"},{"tags":["performance","flash","animation","adobe"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":49,"score":1,"question_id":12977553,"title":"most effieciant way to run a flash animation/application for a long period of time","body":"<p>I need to run a flash animation, (swf file) for about 12 hours. This is for a exhibition.</p>\n\n<p>Due to constraint outside of my control Im running this from a laptop (currently windows vista(!) but I can reinstall all but mac os's onto it, through a VGA, at a 1024x768 resolution.</p>\n\n<p>Does anyone have any previus experience of best practices/advice on this?</p>\n\n<p>would any os be better exquiped, am I better with a comandline linux running only a flash player? would there be any benifit converting the swf to native code and running it?</p>\n\n<p>any pitfalls you can think of?</p>\n\n<p>any advice greatly appriciated.</p>\n\n<p>N.b I hve seen the question <a href=\"http://stackoverflow.com/questions/1504580/running-the-flash-player-over-long-time-period\">Running the Flash Player over long time period</a> however I'm looking for best practices and tips etc on a wide range of topics</p>\n\n<p>If anyone feels another 'overflow' would get me better advice please let me know</p>\n\n<p>thanks for any suggestions</p>\n\n<p>Andrew</p>\n\n<p>EDIT:</p>\n\n<p>just come across this:\n<a href=\"http://www.adobe.com/devnet/flash/articles/efficiency-tips.html\" rel=\"nofollow\">http://www.adobe.com/devnet/flash/articles/efficiency-tips.html</a>\nSome good tips here on optimizing the file contents.</p>\n"},{"tags":["c++","linux","performance","g++","intel"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":34,"score":1,"question_id":12977128,"title":"fmaf weird performance","body":"<p>I'm experiencing an huge performance decrease using the <code>fmaf</code> function over the usage of <code>*</code> and <code>+</code>. I'm on two Linux machines and using g++ 4.4.3 and g++ 4.6.3</p>\n\n<p>On two different machines the following code runs faster if the <code>myOut</code> vector is filled without the usage of <code>fmaf</code>.</p>\n\n<p>server with g++ 4.6.3 and Intel(R) Xeon(R) CPU E5-2650 @ 2.00GHz</p>\n\n<pre><code>$ ./a.out fmaf\nTime: 1.55008 seconds.\n$ ./a.out muladd\nTime: 0.403018 seconds.\n</code></pre>\n\n<p>server with g++ 4.4.3 and Intel(R) Xeon(R) CPU X5650  @ 2.67GHz</p>\n\n<pre><code>$ ./a.out fmaf\nTime: 0.547544 seconds.\n$ ./a.out muladd\nTime: 0.34955 seconds.\n</code></pre>\n\n<p>Shouldn't the <code>fmaf</code> version (apart to avoid an extra roundup and then be more precise) be faster?</p>\n\n<pre><code>#include &lt;stddef.h&gt;\n#include &lt;iostream&gt;\n#include &lt;math.h&gt;\n#include &lt;string.h&gt;\n#include &lt;stdlib.h&gt;\n\n#include &lt;sys/time.h&gt;\n\nint main(int argc, char** argv) {\n  if (argc != 2) {\n    std::cout &lt;&lt; \"missing parameter: 'muladd' or 'fmaf'\"\n              &lt;&lt; std::endl;\n    exit(-1);\n  }\n  struct timeval start,stop,result;\n  const size_t mySize = 1e6*100;\n\n  float* myA = new float[mySize];\n  float* myB = new float[mySize];\n  float* myC = new float[mySize];\n  float* myOut = new float[mySize];\n\n  gettimeofday(&amp;start,NULL);\n  if (!strcmp(argv[1], \"muladd\")) {\n    for (size_t i = 0; i &lt; mySize; ++i) {\n      myOut[i] = myA[i]*myB[i]+myC[i];\n    }\n  } else if (!strcmp(argv[1], \"fmaf\")) {\n    for (size_t i = 0; i &lt; mySize; ++i) {\n      myOut[i] = fmaf(myA[i], myB[i], myC[i]);\n    }\n  } else {\n    std::cout &lt;&lt; \"specify 'muladd' or 'fmaf'\" &lt;&lt; std::endl;\n    exit(-1);\n  }\n\n  gettimeofday(&amp;stop,NULL);\n  timersub(&amp;stop,&amp;start,&amp;result);\n  std::cout &lt;&lt; \"Time: \" &lt;&lt;  result.tv_sec + result.tv_usec/1000.0/1000.0\n            &lt;&lt; \" seconds.\" &lt;&lt; std::endl;\n\n  delete []myA;\n  delete []myB;\n  delete []myC;\n  delete []myOut;\n}\n</code></pre>\n"},{"tags":["performance","oracle","query","database-design","data-warehouse"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":1,"view_count":64,"score":0,"question_id":12973618,"title":"Oracle explain plan over simple select performs multiple hash joins when multiple columns are indexed in a table","body":"<p>I am currently running into an issue with my Oracle instance.  I have two simple select statements:  </p>\n\n<pre><code>select * from dog_vets  \n</code></pre>\n\n<p>and  </p>\n\n<pre><code>select * from dog_statuses\n</code></pre>\n\n<p>and the following <a href=\"http://sqlfiddle.com/#!4/37754\" rel=\"nofollow\">fiddle</a></p>\n\n<p>My explain plan on <code>dog_vets</code> is as follows:  </p>\n\n<pre><code> 0 | Select Statement  \n 1 | Table Access Full Scan dog_vets\n</code></pre>\n\n<p>my explain plan on <code>dog_statuses</code>  is as follows:  </p>\n\n<pre><code>ID|Operation | Name | Rows |Bytes | cost | time    \n0 | Select Statement |  | 20G | 500M | 100000 | 999:99:17  \n1 | View  | index%_join_001 | 20G | 500M | 100000 | 999:99:17  \n2 | Hash Join  |  | | | |   \n3 | Hash Join  |  | | | |   \n4 | Index fast full scan dog_statuses_check_up  |  | 20G | 500M | 100000 | 32:15:00  \n5 | Index fast full scan dog_statuses_sick|  | 20G | 500M | 100000 | 35:19:00  \n</code></pre>\n\n<p>To get this type of output execute the following statement:  </p>\n\n<pre><code>explain plan for   \nselect * from dog_vets;\n</code></pre>\n\n<p>OR</p>\n\n<pre><code> explain plan for   \n    select * from dog_statuses;\n</code></pre>\n\n<p>and then  </p>\n\n<pre><code>select * from table(dbms_xplan.display);\n</code></pre>\n\n<p>Now my question is, why do multiple indexes imply a view (materialized  I assume) being created in my above statements and further what type of performance hit am I suffering on this type of query?  As it stands now <code>dog_vets</code> has ~300 million records and <code>dog_Statuses</code> has about 500 million.  I have yet to be able to get <code>select * from dog_statuses</code> to return in under 10 hours.  This is primarily because the query dies before it completes.  </p>\n\n<p><strong>DDL</strong>  </p>\n\n<p>In case sql fiddle dies:  </p>\n\n<pre><code>create table dog_vets\n(\n     name varchar2(50),\n     founded timestamp,\n     staff_count number\n  );\n\ncreate table dog_statuses\n(\n      check_up timestamp,\n      sick varchar2(1)\n  );\n\n\ncreate index dog_vet_name\non dog_vets(name);\n\ncreate index dog_status_check_up\non dog_statuses(check_up);\n\ncreate index dog_status_sick\non dog_statuses(sick);\n</code></pre>\n"},{"tags":["performance","parallel-processing","data-modeling","simulation","hpc"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":39,"score":0,"question_id":12907569,"title":"Meaning of axis of figures of simulation or performance modeling papers","body":"<p>I am reading some papers on simulation and performance modeling. The Y axis in some figures is labeled \"Seconds per Simulation Day\". I am not sure what it actually means. It span from 0, 20, 40 to 120.</p>\n\n<p>Another label is \"Simulation years per day\". I guess it means the guest OS inside simulation environment thinks it has passed several years while actually it just passed a day in the real world? But I guess simulation should slow down the execution, so I guess inside simulation environment passed several hours while actually it just passed a day in the real world would be more reasonable.</p>\n\n<p>Thanks.</p>\n"},{"tags":["performance","optimization","jsoup"],"answer_count":2,"favorite_count":0,"up_vote_count":6,"down_vote_count":0,"view_count":125,"score":6,"question_id":11291414,"title":"A way to estimate or predict Jsoup processing time of a chunk of HTML?","body":"<p>Some web pages that I process in Jsoup are heavy. By \"heavy\" I mean the page either contains lots of HTML (let's assume the page has already been downloaded), or it requires several iterations on the <em>same</em> document (created only once via <a href=\"http://jsoup.org/cookbook/input/parse-document-from-string\">Jsoup.parse()</a>).</p>\n\n<p>For that reason, I would like to present to the user a progress bar with a guesstimate of how much time is left.</p>\n\n<p>One approach is to just measure the volume of HTML (in KB or MB) and come up with a speed factor (unfortunately, totally dependent on speed of the system this code runs on).</p>\n\n<p>Another approach is to count the <a href=\"http://stackoverflow.com/a/11291133/1357272\">number of nodes</a>?</p>\n\n<p>Due to the obvious in-deterministic nature of this, am I calling for trouble?</p>\n\n<p>Ideas of better ways to handles this?</p>\n"},{"tags":["ajax","performance","jsf","jquery-ajax","datatable"],"answer_count":1,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":69,"score":4,"question_id":12976215,"title":"How to write <table> markup without JSF tag libraries (h:datatable or ui:repeat) but still use JSF for controlling page flow","body":"<p>I have various tables with the following size : 12 columns and up to 1800 rows. It takes 8 seconds to render it to the user. I currently use <code>h:dataTable</code>. I tried <code>ui:repeat</code> to get the row data from a Java List object, managed by JSF.  Although this works fine, the 8 seconds to render the table is unacceptable. I'm trying to find other ways to do this, but need to keep <code>JSF</code> as my <code>controller</code> for action buttons on the page.  In other words I want to create the 'table markup<code>to send to the</code>page myself<code>and then still associate actions on</code>h:commandButtons` to the managed bean methods. Is there a way to do this?  </p>\n\n<p>The only ways I can think of is to use <code>jquery</code> or <code>ajax</code> to create the table markup, although I am new to technologies other than JSF for UI development.Maybe then I would somehow pass that to the client for render.  The only problem is I don't know how to generate the markup from my list, and second how I would inject it between <code>h:commandButtons</code> that are in my <code>XHTML</code> file currently. </p>\n\n<p>Does any one know how I can solve this without having to completely rip OFF JSF?  One main problem I have is that the  <strong>business requirement that says we can't page the datatable (i.e: Next / Back buttons displaying 100 at a time for example)</strong>.  So, possibly I was thinking I could do this by Ajax calls to the server and get 100 rows at a time after page ready, and append new rows behind the scenes to the user.  This would be a \"perceived\" speed of load, but I don't know how to do this at all.</p>\n"},{"tags":["performance","r"],"answer_count":4,"favorite_count":2,"up_vote_count":6,"down_vote_count":0,"view_count":110,"score":6,"question_id":12973963,"title":"Using sapply vs. for to efficiently write to preallocated data structures","body":"<p>Assume I have a preallocated data structure that I want to write into for the sake of performance vs. growing the data structure over time. First I tried this using sapply:</p>\n\n<pre><code>set.seed(1)\ncount &lt;- 5\npre &lt;- numeric(count)\n\nsapply(1:count, function(i) {\n  pre[i] &lt;- rnorm(1)\n})\npre\n# [1] 0 0 0 0 0\n\n\nfor(i in 1:count) {\n  pre[i] &lt;- rnorm(1)\n}\npre\n# [1] -0.8204684  0.4874291  0.7383247  0.5757814 -0.3053884\n</code></pre>\n\n<p>I assume this is because the anonymous function in <code>sapply</code> is in a different scope (or is it environment in R?) and as a result the <code>pre</code> object isn't the same. The for loop exists in the same scope/environment and so it works as expected.</p>\n\n<p>I've generally tried to adopt the R mechanisms for iteration with apply functions vs. for, but I don't see a way around it here. Is there something different I should be doing or a better idiom for this type of operation?</p>\n\n<p>As noted, my example is highly contrived, I have no interested in generaring normal deviates. Instead my actual code is dealing with a 4 column 1.5 million row dataframe. Previously I was relying on growing and merging to get a final dataframe and decided to try to avoid merges and preallocate based on benchmarking.</p>\n"},{"tags":["asp.net","database","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":6,"view_count":27,"score":-6,"question_id":12975862,"title":"Free performance monitoring tool in asp.net","body":"<blockquote>\n  <p>That tool should not consume bulk space while monitoring.  </p>\n  \n  <p>Suggest free one.  </p>\n  \n  <p>Thanks in advance</p>\n</blockquote>\n"},{"tags":["c++","performance","c++-amp"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":81,"score":0,"question_id":12974389,"title":"When should I use C++ AMP","body":"<p>When should I use the C++ AMP (or shouldn't use it)?</p>\n\n<p>What is an overhead of AMP? How long it takes to copy data to GPU memory and back? What is a minimal data size when AMP starts to decrease performance?</p>\n"},{"tags":["c++","c","performance","algorithm","profiling"],"answer_count":6,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":202,"score":4,"question_id":12964396,"title":"What is the most efficient method to find x contiguous values of y in an array?","body":"<p>Running my app through callgrind revealed that this line dwarfed everything else by a factor of about 10,000. I'm probably going to redesign around it, but it got me wondering; Is there a better way to do it?</p>\n\n<p>Here's what I'm doing at the moment:</p>\n\n<pre><code>int i = 1;\nwhile\n(\n    (\n        (*(buffer++) == 0xffffffff &amp;&amp; ++i) || \n        (i = 1)\n    )\n    &amp;&amp;\n    i &lt; desiredLength + 1\n    &amp;&amp;\n    buffer &lt; bufferEnd\n);\n</code></pre>\n\n<p>It's looking for the offset of the first chunk of desiredLength 0xffffffff values in a 32 bit unsigned int array.</p>\n\n<p>It's significantly faster than any implementations I could come up with involving an inner loop. But it's still too damn slow.</p>\n"},{"tags":["windows","performance","splash-screen","launcher"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":35,"score":0,"question_id":12974184,"title":"writing fast launcher for windows","body":"<p>I'm writing WPF application </p>\n\n<p>application targets all sort of windows and low performance computers\nso I want to write launcher/splash screen for it which will be displayed before application loads</p>\n\n<p>I'm not sure what language to use or what technology\nI want it to be very fast and lightweight </p>\n\n<p>can you suggest anything ?</p>\n"},{"tags":["performance","internet-explorer","internet-explorer-9","ie-compatibility-mode"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":30,"score":0,"question_id":12974055,"title":"Internet explorer compatibility view performance","body":"<p>I use IE9 and I know that the compatibility mode uses IE7 render engine but what about performances ? If the compatibility mode is enabled, does it slow the page load and the javascript ?</p>\n\n<p>I don't see major differences when switching from one to another.</p>\n"},{"tags":["eclipse","performance","jsp","indexing","cpu-usage"],"answer_count":2,"favorite_count":2,"up_vote_count":1,"down_vote_count":0,"view_count":2088,"score":1,"question_id":4320088,"title":"Eclipse indexing takes forever","body":"<p>I have Eclipse Helios SR1 installed on my Ubuntu 10.04 desktop and I'm having to work with a huge set of web projects in it.</p>\n\n<p>When I import the projects eclipse builds the workspace successfully but it keeps consuming 100% of the CPU invariably.</p>\n\n<p>After checking what could be happening I found that in Eclipse's progress tab there is a couple of endless tasks:</p>\n\n<ul>\n<li>System: Java indexing... n files to index</li>\n<li>System: Updating JSP index</li>\n<li>System: Persisting JSP translations</li>\n</ul>\n\n<p>These tasks seem to never end and makes my Eclipse unusable.</p>\n\n<p>I doubt it is a memory issue, I have 2GB in this machine and Eclipse's heap size does not get greater than 350MB and Xmx is set currently to 1024MB.</p>\n\n<p>Also tried running Eclipse with different VM versions: Sun's 1.6, Sun's 1.5, and Open JDK 1.6. No changes.</p>\n\n<p>I have an Athlon X2 2.2GHz processor and a 7200 rpm Samsung hard drive.</p>\n\n<p>The source code is shared via SVN.</p>\n\n<p>Does anyone have any idea of what could be going on?</p>\n\n<p>This is my eclipse.ini just in case:</p>\n\n<pre><code>-startup\nplugins/org.eclipse.equinox.launcher_1.1.0.v20100507.jar\n--launcher.library\nplugins/org.eclipse.equinox.launcher.gtk.linux.x86_1.1.1.R36x_v20100810\n-product\norg.eclipse.epp.package.jee.product\n--launcher.defaultAction\nopenFile\n-showsplash\norg.eclipse.platform\n--launcher.XXMaxPermSize\n256m\n--launcher.defaultAction\nopenFile\n-vmargs\n-Dosgi.requiredJavaVersion=1.5\n-XX:MaxPermSize=256m\n-Xms40m\n-Xmx1024m\n-Djava.library.path=/usr/lib/jni\n</code></pre>\n\n<p>Thanks a lot.</p>\n"},{"tags":["asp.net","performance","c#-4.0"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":53,"score":0,"question_id":11031233,"title":"Assembly.GetExecutingAssembly() performance","body":"<p>I'm looking into creating a automated way to check the running version of our system(s). Whenever we release code to test we version is using </p>\n\n<pre><code>[assembly: AssemblyVersion(\"1.21.0.13329\")]\n[assembly: AssemblyFileVersion(\"1.21.0.13329\")]\n</code></pre>\n\n<p>I now want to extend our ASP.Net <strong>Web forms</strong> and <strong>Web Services</strong> to display this version number. So the Web forms app will put it somewhere on the home page and the web service will probably have a web method that returns the version number.</p>\n\n<p>I was thinking of using </p>\n\n<pre><code>Version version = Assembly.GetExecutingAssembly().GetName().Version;\n</code></pre>\n\n<p>to do this. But I'm concerned that because this is using reflection it could add a performance overhead to my code. It could be getting called several times, especially on the home page and I don't want this to slow response times down. </p>\n\n<p>I have another solution so I don't want alternatives, I just want to know if anyone knows if the performance overhead of this is going to be relatively significant?</p>\n"},{"tags":["performance","algorithm","sorting","data-structures","quicksort"],"answer_count":3,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":92,"score":1,"question_id":12970688,"title":"Quicksort recursion depth Stack space of O(n) doesnot cause stackoverflow?","body":"<p>In Worst case Quicksort recursion Depth requires Stack  space  of  O(n). Why it doesn't cause a stack overflow for large set in the worst case? (reversed sequence)</p>\n"},{"tags":["c#",".net","performance","linq"],"answer_count":4,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":155,"score":5,"question_id":12819792,"title":"LINQ query is slow","body":"<p>During application profiling I found that function checking for pattern match is very slow. It is written using LINQ. Simple replacement of this LINQ expression with loop makes really huge difference. What is it? Is LINQ really such a bad thing and works so slow or I misunderstand something?</p>\n\n<pre><code>private static bool PatternMatch1(byte[] buffer, int position, string pattern)\n{\n    int i = 0;\n\n    foreach (char c in pattern)\n    {\n        if (buffer[position + i++] != c)\n        {\n            return false;\n        }\n    }\n\n    return true;\n}    \n</code></pre>\n\n<p>version 2 with LINQ (suggested by Resharper)</p>\n\n<pre><code>private static bool PatternMatch2(byte[] buffer, int position, string pattern)\n{\n    int i = 0;\n    return pattern.All(c =&gt; buffer[position + i++] == c);\n}\n</code></pre>\n\n<p>version 3 with LINQ</p>\n\n<pre><code>private static bool PatternMatch3(byte[] buffer, int position, string pattern)\n{\n    return !pattern.Where((t, i) =&gt; buffer[position + i] != t).Any();\n}\n</code></pre>\n\n<p>version 4 using lambda</p>\n\n<pre><code>private static bool PatternMatch4(byte[] buffer, int position, string pattern, Func&lt;char, byte, bool&gt; predicate)\n{\n    int i = 0;\n\n    foreach (char c in pattern)\n    {\n        if (predicate(c, buffer[position + i++]))\n        {\n            return false;\n        }\n    }\n\n    return true;\n}\n</code></pre>\n\n<p>and here is the usage with large buffer</p>\n\n<pre><code>const int SIZE = 1024 * 1024 * 50;\n\nbyte[] buffer = new byte[SIZE];\n\nfor (int i = 0; i &lt; SIZE - 3; ++i)\n{\n    if (PatternMatch1(buffer, i, \"xxx\"))\n    {\n        Console.WriteLine(i);\n    }\n}\n</code></pre>\n\n<p>Calling <code>PatternMatch2</code> or <code>PatternMatch3</code> is phenomenally slow. It takes about 8 seconds for <code>PatternMatch3</code> and about 4 seconds for <code>PatternMatch2</code>, while calling <code>PatternMatch1</code> takes about 0.6. As far as I understand it is the same code and I see no difference. Any ideas?</p>\n"},{"tags":["java","arrays","list","performance"],"answer_count":21,"favorite_count":23,"up_vote_count":69,"down_vote_count":0,"view_count":48568,"score":69,"question_id":716597,"title":"Array or List in Java. Which is faster?","body":"<p>I have to keep thousands of strings in memory to be accessed serially in Java. Should I store them in an array or should I use some kind of List ?</p>\n\n<p>Since arrays keep all the data in a contiguous chunk of memory (unlike Lists), would the use of an array to store thousands of strings cause problems ?</p>\n\n<p><strong>Answer:</strong> The common consensus is that the performance difference is minor. List interface provides more flexibility. </p>\n"},{"tags":["android","performance","view","touch"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":50,"score":1,"question_id":12972222,"title":"Slow response onTouchEvent in Android View","body":"<p>I created a View that listens for touch events and then draw a circle whenever you touched. Currently, it only tracks vertical position of the touch. </p>\n\n<p>There are the interesting code parts: </p>\n\n<p>onTouchEvent:</p>\n\n<pre><code>public boolean onTouchEvent(MotionEvent event) {\n\n    int actionType = event.getAction();\n    if (actionType == MotionEvent.ACTION_MOVE || actionType == MotionEvent.ACTION_DOWN ) {\n\n        int px = getMeasuredWidth() / 2;\n        int py = getMeasuredHeight() / 2;\n\n        touchY = (event.getY() - py);\n\n        if (touchY &lt; 25 &amp;&amp; touchY &gt; -25) {\n            touchY = 0;\n        }\n        invalidate();\n    }\n    return true;\n}\n</code></pre>\n\n<p>Also, the onDraw:</p>\n\n<pre><code>@Override\nprotected void onDraw(Canvas canvas) {\n    int px = getMeasuredWidth() / 2;\n    int py = getMeasuredHeight() / 2;\n    int radius = px - innerPadding;\n\n    canvas.drawCircle(px, touchY+py, radius - innerPadding, circlePaint);\n    canvas.save();\n}\n</code></pre>\n\n<p>It works but I find that the circle following the finger is quite slow considering I have a Galaxy S3. It is smooth but it has a delay.</p>\n\n<p>Any advice of how to implement this better?</p>\n\n<p>Thank you. </p>\n"},{"tags":["c#",".net","performance","stream","filestream"],"answer_count":3,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":156,"score":2,"question_id":12969555,"title":"File I/O best practice - byte[] or FileStream?","body":"<p>I'm currently working with a lot of different file types (txt, binary, office, etc). I typically use a <code>byte[]</code> or <code>string</code> to hold the file data in memory (while it is being written/parsed) and in order to read/write it into files I write the entire data using a <code>FileStream</code> after the data has been completely processed. </p>\n\n<ul>\n<li>Should I be using a <code>TextStream</code> instead of a <code>string</code> while generating data for a text file?</li>\n<li>Should I be using a <code>FileStream</code> instead of a <code>byte[]</code> while generating data for a binary file?</li>\n<li>Would using streams give me better performance instead of calculating the entire data and outputting it in one go at the end?</li>\n<li>Is it a general rule that File I/O should always use streams or is my approach fine in some cases?</li>\n</ul>\n"},{"tags":["performance","nginx","control"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":30,"score":2,"question_id":12969958,"title":"nginx speed control based on cookies [or any other variable I can set on the fly]?","body":"<p>I have this config in my Nginx server:</p>\n\n<pre><code>limit_rate 500k;\nlocation ~ \\.mp4$ {\n    mp4;\n    limit_rate_after 4m;\n    limit_rate 90k;\n    limit_req zone=one burst=5;\n    limit_conn addr 2;\n}\n</code></pre>\n\n<p>I want the speed and the burst controlled on the fly by a mean of cookies (can't alter URL as I need it in certain length/structure)</p>\n\n<p>Is there a way to put something like this?</p>\n\n<pre><code>if($cookie_burst){\n    limit_rate_after {$cookie_burst}m;//how can I use cookie value here along with m[Megabytes]?\n}\n</code></pre>\n\n<p>Edit: I cahnged the URL requesting the mp4 file, adding a burst argument, now using $arg_burst in Nginx config file.</p>\n\n<p>However, I can test for $arg_burst but I can't use it in a line like this:</p>\n\n<p>This works:</p>\n\n<pre><code>if ($arg_burst = \"1m\"){\n   limit_rate_after 1m;\n}\n</code></pre>\n\n<p>This doesn't :( :</p>\n\n<pre><code>if ($arg_burst != \"\"){\n   limit_rate_after $arg_burst;\n}\n</code></pre>\n\n<p>Any workaround to put whatever value I want there, on the fly?</p>\n"},{"tags":["mysql","performance","amazon-ec2","amazon-elasticache"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":470,"score":1,"question_id":11335366,"title":"Using ElastiCache with RDS for improving read/write performance","body":"<p>I am using RDS on amazon with a MySQL interface. My application runs on EC2 nodes and read/update the database, but the number of reads and writes are too many and that reduces performance. Most of the time the number of connections exceed the allowed limit. \nI was considering using Elasticache to improve performance, however I did not find resources on web, how to configure database to use this effectively.\nIs this the best way to improve my read/write performance?\nAny suggestions?</p>\n"},{"tags":["html","css","performance","html5","optimization"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":40,"score":-1,"question_id":12970857,"title":"CSS/HTML Optimizations tips","body":"<p>Can someone overview the code on our web site and offer some tips to optimize it further, we want to be able to achieve a grade A or B in yslow performance. </p>\n\n<p>Website at <a href=\"http://www.wirenine.com/\" rel=\"nofollow\">http://www.wirenine.com/</a></p>\n"},{"tags":["mysql","performance","query","join"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":47,"score":0,"question_id":12970402,"title":"Slow MySQL query with multiple sub-select joins","body":"<p>This has been bothering me all day:</p>\n\n<p>I have this query which works fine but is quite slow in some cases. The explain result tells me mysql is scanning 40000 rows 3 times. The query joins on approximatily similar sub-selects but there all a little different.</p>\n\n<p>How can I improve this?</p>\n\n<pre><code>SET @lastchecktime = (SELECT max(tresults.StartTime) FROM tresults);\nSELECT tresults.shopID\n , tshops.OfficialName\n , tresults.Price, tresults.starttime\n , Sub2.minprice\n , Sub2.StartTime\n , Sub3.daystoolow\n , (sub2.minprice/tsupplierproducts.lowestprice) -1 as afwijking\n , If(maxstarttime = @lastchecktime,'yes' ,'no') as notavailable     \nFROM\n  tresults\nINNER JOIN tshops\nON tshops.shopID = tresults.shopID\ninner JOIN (SELECT tresults.shopID\n                 , max(tresults.StartTime) AS MaxStartTime\n            FROM\n            tresults\n            WHERE\n              tresults.pID = 7\n              AND tresults.websiteID = 1\n              AND tresults.StartTime BETWEEN \"2012-08-01\" AND \"2012-12-01\"              \n            GROUP BY\n              tresults.shopID) Sub1\nON tresults.StartTime = Sub1.MaxStartTime AND Sub1.shopID = tshops.shopID\nINNER JOIN (SELECT tresults.shopID\n             , tresults.StartTime\n             , min(tresults.Price) AS minprice\n        FROM\n          tresults\n        WHERE\n          tresults.pID = 7\n          AND tresults.websiteID = 1\n          AND tresults.StartTime BETWEEN \"2012-08-01\" AND \"2012-12-01\"\n        GROUP BY\n          tresults.shopID) Sub2\nON Sub2.shopID = tshops.shopID\nINNER JOIN (SELECT tresults.shopID\n             , round(count(tresults.StartTime)/3,0) AS daystoolow\n        FROM\n          tsupplierproducts\n        INNER JOIN tresults\n        ON tsupplierproducts.pID = tresults.pID AND tresults.Price &lt; tsupplierproducts.LowestPrice\n        WHERE\n          tresults.pID = 7\n          AND tresults.websiteID = 1\n          AND tresults.StartTime BETWEEN \"2012-08-01\" AND \"2012-12-01\"\n          AND tsupplierproducts.supplierID = 2\n        GROUP BY\n          tresults.shopID) Sub3\nON Sub3.shopID = tshops.shopID\nINNER JOIN tsupplierproducts\nON tsupplierproducts.pID = tresults.pID AND tsupplierproducts.supplierID = 2\nWHERE\n  tresults.pID = 7\n  AND tresults.websiteID = 1\nORDER BY\nnotavailable desc, tresults.Price DESC\n</code></pre>\n\n<p>The explain result:</p>\n\n<pre><code>1, PRIMARY, tsupplierproducts, const, PRIMARY,fk_SupplierID,fk_pID, PRIMARY, 8, const,const, 1, Using temporary; Using filesort\n1, PRIMARY, &lt;derived4&gt;, ALL, , , , , 27, \n1, PRIMARY, &lt;derived2&gt;, ALL, , , , , 43, Using where; Using join buffer\n1, PRIMARY, &lt;derived3&gt;, ALL, , , , , 43, Using where; Using join buffer\n1, PRIMARY, tshops, eq_ref, PRIMARY, PRIMARY, 4, Sub1.shopID, 1, Using where\n1, PRIMARY, tresults, eq_ref,     PRIMARY,idxPID,idxWebsite,idxStartTimeASC,idxStartTimeDESC,fk_shopID, PRIMARY, 20, Sub1.MaxStartTime,const,Sub3.shopID,const, 1, \n4, DERIVED, tsupplierproducts, const, PRIMARY,fk_SupplierID,fk_pID, PRIMARY, 8, , 1, Using temporary; Using filesort\n4, DERIVED, tresults, ref, PRIMARY,idxPID,idxWebsite,idxStartTimeASC,idxStartTimeDESC,     idxPID, 4, , 42048, Using     where\n3, DERIVED, tresults, ref, PRIMARY,idxPID,idxWebsite,idxStartTimeASC,idxStartTimeDESC,     idxPID, 4, , 42048,     Using     where; Using temporary; Using filesort\n2, DERIVED, tresults, ref, PRIMARY,idxPID,idxWebsite,idxStartTimeASC,idxStartTimeDESC, idxPID, 4, , 42048, Using where; Using index; Using temporary; Using filesort\n</code></pre>\n\n<p>Any help would be appreciated!</p>\n"},{"tags":["c#","performance","silverlight"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":31,"score":0,"question_id":12953722,"title":"Making image-source swap faster","body":"<p>I have 4 image controls and a button, which swaps all those images within those image-controls. But this is going too slow.</p>\n\n<p>The image swap happens after a 1.5s Storyboard animation. So imagine those four-image controls making a move down and then calling this method:</p>\n\n<pre><code> BLOCK4.Source = stack[3];\n BLOCK3.Source = stack[2];\n BLOCK2.Source = stack[1];\n BLOCK1.Source = stack[0];\n</code></pre>\n\n<p>stack is a private <code>BitmapImage[] stack;</code> array which contains random images after every animation-call.</p>\n\n<p>Do you see a way to tune this code in order to make the swap seemingly faster?</p>\n\n<p><strong>This is what happens:</strong> Animation starts -> stops -> I can see old images -> milliseconds pass by -> I can see new images.</p>\n"},{"tags":["arrays","performance","matlab","structure"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":47,"score":2,"question_id":12949665,"title":"Rearrange structure arrays of uneven lengths to single 1d array","body":"<p>I've got an struct array, with three fields - an array, the array's length, and a number.</p>\n\n<pre><code>N = 5;\ndata = struct;\nfor i=1:N\n    n = ceil(rand * 3);\n    data(i).len = n;\n    data(i).array = rand(1,n);\n    data(i).number = i;\nend\n</code></pre>\n\n<p>The data looks like this:</p>\n\n<pre><code>data = \n1x5 struct array with fields:\n    len    = [ 1 3 3 1 1 ]\n    array  = [[0.8]; [0.7 0.9 0.4]; [0.7 0 0.3]; [0.1]; [0.3]]\n    number = [ 1 2 3 4 5 ]\n</code></pre>\n\n<p>I can return array as a 1x9 array in several ways:</p>\n\n<pre><code>&gt;&gt;&gt; [data.array] \n&gt;&gt;&gt; cat(2,data.array)\n[0.8 | 0.7 0.9 0.4 | 0.7 0 0.3 | 0.1 | 0.3]     %  | shows array separation\n</code></pre>\n\n<p>I'd like to repeat the number (<code>data.number</code>) <code>len</code> times, to produce the same length array as the concatenated array. </p>\n\n<p>I'm currently doing this with <code>arrayfun</code> then <code>cell2mat</code>:</p>\n\n<pre><code>&gt;&gt; x = arrayfun(@(x) repmat(x.number, 1, x.len), data, 'UniformOutput', false)\nx = \n    [1]    [1x3 double]    [1x3 double]    [4]    [5]\n&gt;&gt; cell2mat(x)\n[ 1 2 2 2 3 3 3 4 5]\n</code></pre>\n\n<p>This makes the numbers line up with the arrays. </p>\n\n<pre><code>arrays =  [ 0.8 | 0.7 0.9 0.4 | 0.7 0 0.3 | 0.1 | 0.3 ] \nnumbers = [ 1   | 2   2   2   | 3   3   3 | 4   | 5   ]\n</code></pre>\n\n<hr>\n\n<p>The idea behind this is to feed the data to the GPU for processing - but rearranging the data takes orders of magnitude longer than the actual processing. </p>\n\n<p><code>Arrayfun</code> takes ~5 seconds when N=100,000, and a for loop calling <code>repmat</code> takes ~4 seconds. </p>\n\n<p>Is there a faster way to rearrange data from uneven arrays in structures into matching length 1d arrays? I'm open to using a different data structure. </p>\n\n<hr>\n\n<p><strong>Testing vectorised method:</strong></p>\n\n<pre><code>data = struct;\ndata(1).len = 1;\ndata(1).array = [1 2 3];\ndata(1).number = 11;\ndata(2).len = 0;\ndata(2).array = [];\ndata(2).number = 12;\ndata(3).len = 2;\ndata(3).array = [4 5 6; 7 8 9];\ndata(3).number = 13;\n\nlist_of_array = cat(1,data.array)\n\nidx = zeros(1,size(list_of_array,1));\n% Set start of each array to 1\nlen = cumsum([data.len])\nidx(len) = 1\n% Flat indices\nidx = cumsum([1 idx(1:end-1)])\n\nnf = [data.number]\nrepeated_num_faces = nf(idx)\n</code></pre>\n\n<p>Gives the output:</p>\n\n<pre><code>list_of_array =\n     1     2     3\n     4     5     6\n     7     8     9\nlen =\n     1     1     3    % Cumulative lengths\nidx =\n     1     0     1    % Ones at start\nidx =\n     1     2     2    % Flat indexes - should be [1 3 3]\nnf =\n    11    12    13    % Numbers expanded\nrepeated_num_faces =\n    11    12    12    % Wrong .numbers - should be [11 13 13]\n</code></pre>\n"},{"tags":["python","performance","memory-management","coding-style"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":79,"score":3,"question_id":12953125,"title":"Python: How to improve performance of a script to obtain the Bounding Box of a set of points","body":"<p>I have a set of points (x and y) and i wish to know the maximum and Minimum values of X and Y (the Bounding Box). I coded these lines where i read all points with list comprehension and after i use max and min on X and Y. In the end i delete the points.</p>\n\n<p>This solution is not memory efficency because i need to read all points</p>\n\n<pre><code>points = [(p.x,p.y) for p in lasfile.File(inFile,None,'r')] # read in list comprehension\nX_Max = max(zip(*points)[0])\nX_Min = min(zip(*points)[0])\nY_Max = max(zip(*points)[1])\nY_Min = min(zip(*points)[1])\ndel points\n</code></pre>\n\n<p>I am asking a suggetion to avoid this step (store all points in the memory).\nThanks in advance\nGianni</p>\n"},{"tags":["javascript","json","performance","node.js"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":60,"score":0,"question_id":12969597,"title":"Javascript: Removing an object from array by checking it's attribute","body":"<p>I might have written a pretty confusing title but my question is rather simple. </p>\n\n<p>I'm looking for an efficient way to remove an item from an array. But my array is full objects that has been <b>stringified</b> (I'm writing my app on Node.js and I'm using <b>JSON.stringify</b> method). So my array is like this;</p>\n\n<pre><code>\"{\\\"userID\\\":\\\"15\\\",\n  \\\"possibleFollowers\\\":[\n      {\\\"followerID\\\":\\\"201\\\",\\\"friends\\\":716},\n      {\\\"followerID\\\":\\\"202\\\",\\\"friends\\\":11887},\n      {\\\"followerID\\\":\\\"203\\\",\\\"friends\\\":11887}],\n  \\\"name\\\":\\\"John\\\",\n  \\\"lon\\\":\\\"Doe\\\"}\"\n</code></pre>\n\n<p><P>My question is on Javascript(or Node). If I wanted to remove the from possibleFollowers with \"followerID: 202\", how would I be able to do that <b>efficiently</b>?</p></p>\n"},{"tags":["performance","assembly","pic","pic18","8bit"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":84,"score":1,"question_id":12960652,"title":"fast 8x8bit multiplication assembly pic18","body":"<p>Im trying to multiply two 8 bit numbers and store them in a 16 bit location for results larger than 255. The fastest way to accomplish this is through shifting, which I have tried to implement via the rrcf function and using bcf to clear unwanted carries. </p>\n\n<p>This is what ive came up with. Ive tried to comment all the code so you are able to see my thought process. Im  fairly new to both the PIC18 and programming in ASM in general. Please keep that in mind when (hopefully) providing help. I need to get put in a better spot than I am in, when I run MPLAB SIM, all I get is the counter decrementing...? </p>\n\n<p>I think this is due to the last bit of multiplier being repeatedly tested, which will be zeros and and therefore skip my add instruction every time. Can you help me create a loop to move BTFSC progressively from bit 0-7? I think this is the problem, but i can't figure out the code. I could essentailly write main 8 times, but im trying to save code space</p>\n\n<pre><code>            LIST P=18F4550\n            #include &lt;P18F4550.INC&gt;\n\n            ;equates\n            counter equ 0x00 ;set counter\n            multiplicand equ 0x01 ;set multiplicand\n            multiplier equ 0x02 ;set multiplier\n            resultHigh equ 0x03 ;set resultHigh\n            resultLow equ 0x04 ;set resultLow\n\n            ;test program\n\n            movlw d'100' ;move literal d'100' to wreg\n            movwf multiplicand ;set multiplicand\n            movlw d'400'       ;move literal d'400'\n            movlw multiplier   ;set multiplier\n            movlw d'8'         ;counter\n            movwf counter      ;set counter\n\n            main:\n            btfsc multiplier, 0          ;test LSB if 0,skip next if 0\n            addwf multiplier, resultLow  ;add if 1 to resultLow\n            bcf STATUS,C                 ;clear carry flag\n            rlcf multiplier              ;rotate multiplier left\n            bcf STATUS,C                 ;clear carry\n            rlcf resultLow               ;rotate resultLow w/ carry\n            rlcf resultHigh              ;rotate resultHigh \n                                                                  ;w/carry from resultLow\n\n            decf counter                 ;dec counter\n            goto main                    ;repeat for 8 bits\n            end\n</code></pre>\n"},{"tags":["ios","performance","uitableview","uitableviewcell"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":12969346,"title":"Improving UITableView scrolling","body":"<p>i stuck with some issue of UITableView scrolling. I know that there are numbers of this question and some solutions. However from technical view like (how exactly to do that) i can't find solutions.</p>\n\n<p>Well in my case scrolling is not so smooth because of images in it. It takes lots of time to load it from file. Also i create custom UITableViewCell and using XiB. </p>\n\n<p>These images is using not 1 time but several, the good idea would be to keep them in memory and to load them just once from file. However in my case there could lots of images and app don't have so many memory to keep them. Maybe there are other way to load these images ?</p>\n\n<p>Also images is PNG, maybe i should change type to JPG ?</p>\n\n<p>I would appreciate if someone would give other solutions to improve this scrolling.</p>\n\n<p>Thanks.  </p>\n"},{"tags":["performance","cpu","scaling","ondemand"],"answer_count":0,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":47,"score":4,"question_id":12969081,"title":"Performance governor doesn't locks the CPU frequency at max","body":"<p>I'm running real-time application on the dedicated X3440 server and wondering why the application performance is worse than my former VPS box. Then I download <a href=\"http://i7z.googlecode.com/svn/trunk/i7z_64bit\" rel=\"nofollow\">http://i7z.googlecode.com/svn/trunk/i7z_64bit</a> and execute it to see that under normal usage (in top around 10% cpu), all CPU core only stays around 900Mhz ~ 1200Mhz, and it fluctuates rapidly and inconsistent.</p>\n\n<p>Then I try to set governor from ondemand to performance (echo performance > /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor), and my application performs better because now all CPU core stays around 1700 ~ 1900Mhz (still fluctuating), but the application performance still doesn't meet my standard yet.</p>\n\n<p>I have trying to disable any scaling completely by typing \"service cpuspeed stop\", and it's still showing the same result as performance governor. I was wondering why Performance governor didn't lock the CPU frequency at max.</p>\n\n<p>Then I try to run 4 separate dummy nice low-priority tasks in each of the cores:\nyum install gcc</p>\n\n<p>nano dummy.c</p>\n\n<p>content:</p>\n\n<pre><code>int main() {\n    while(1);\n}\n</code></pre>\n\n<p>gcc dummy.c -o dummy</p>\n\n<pre><code>nice taskset -c 0 ./dummy &amp; nice taskset -c 1 ./dummy &amp; nice taskset -c 2 ./dummy &amp; nice taskset -c 3 ./dummy &amp;\n</code></pre>\n\n<p>This way, all 4 cores will stay at 2533mhz and never fluctuates anymore (regardless of the governor settings), and now my application performs very stellar and no complain whatsoever. But, I don't like the way these tasks waste resource, even though it's not disturbing the main application since it's on low priority.</p>\n\n<p>My question: \nIn my home desktop windows computer, somehow my CPU speed is always going stable at max frequency. But why it doesn't happened on this CentOS 6 2.6.32 x86_64 dedicated server? Is there any way to set all cpu cores to max frequency without using any nice low-priority tasks?</p>\n"},{"tags":["c","performance","numerical"],"answer_count":2,"favorite_count":1,"up_vote_count":4,"down_vote_count":2,"view_count":91,"score":2,"question_id":12937412,"title":"How does the size of a binary influence the execution speed","body":"<p>How does the size of a binary influence the execution speed? Specifically I am talking about code written in ANSI-C translated into machine language using the gnu or intel compiler. The target platform for the binary are modern computers with intel or AMD multi-core CPU's running a Linux operating system. The code performs numerical computations possibly in parallel using openMP and the binary could have several mega bytes. </p>\n\n<p>Note that the execution time will in any case be much larger than the time needed to load code and libraries. I think of very specific codes used to solve large systems of ordinary differential equations for simulations of kinetic equations which are typically CPU-bound for a moderate system size but can also become memory-bound.</p>\n\n<p>I am asking whether small binary size should be a design criterion for highly efficient code or if I can always give preference to explicit code (which eventually repeats code blocks which could be implemented as functions) and compiler optimizations such as loop unrolling etc.</p>\n\n<p>I am aware of profiling technics and how I can apply them to specific problems, but I wonder to which extent general statements can be made.</p>\n"},{"tags":["database","performance","google-app-engine","gae-datastore"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":56,"score":1,"question_id":12966253,"title":"Which database model to store data in?","body":"<p>I am writing an application in Google App Engine with python and I want to sort users and user posts into groups.  Users will be able to tag a post with a group ID and then that post will be displayed on the group page.  </p>\n\n<p>I would also like to relate the users to the groups so that only members of a group can tag a post with that group ID and so that I can display all the users of a group on the side.  I am wondering if it would be more efficient to have a property on the user which will have all of the groups listed (I am thinking max 10 or so) or would it be better to have a property on the Group model which lists all of the users (possibly a few hundred).</p>\n\n<p>Is there much of a difference here?</p>\n"},{"tags":["performance","ubuntu","symfony2","virtualbox"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":98,"score":1,"question_id":12905404,"title":"Symfony2 Slow Initialization Time","body":"<p>I have Symfony2 running on an Ubuntu Server 12.04 (64-bit) VM (VirtualBox).  The host is a MacBook pro.  For some reason I am getting really long request times in development mode (app_dev.php).  I know its slower in dev mode, but I'm talking 5-7 seconds per request (sometimes even slower).  On my Mac I get request times of 200ms or so in development mode.</p>\n\n<p>After looking at my timeline in the Symfony2 profiler, I noticed that ~95% of the request time is \"initialization time\".  What is this?  What are some reasons it could be so slow?</p>\n\n<p>This issue only applies to Symfony2 in dev mode, not any other sites I'm running on the VM, and not even to Symfony2 in production mode.</p>\n\n<p>I saw this (http://stackoverflow.com/questions/11162429/whats-included-in-the-initialization-time-in-the-symfony2-web-profiler), but it doesn't seem to answer my questions.</p>\n"},{"tags":["json","performance","web-applications","web"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":26,"score":0,"question_id":12966871,"title":"Performance impact of using short key in JSON file","body":"<p>In a <strong>web server application</strong>, I use some JSON configuration file. Does anyone have any idea, if using shorter key, what's the performance benefit?</p>\n\n<p>For instance, a long key is \"country_code\", which value is a country code, an int.\nA shorter key is \"cc\". </p>\n\n<p><strong>How much performance benefit can we get from using the shorter key? considering network bandwidth, memory...etc</strong></p>\n\n<p>Thanks!</p>\n"},{"tags":["performance","memory","opencl"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":61,"score":0,"question_id":12938520,"title":"Maximize OpenCL memory output","body":"<p>Using OpenCL, I can't seem to pull more than 7MB/sec of data off of a Radeon 7970 into the main memory of my i5 Desktop.</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;Windows.h&gt;\n#include &lt;CL/cl.h&gt;\n\nint main(int argc, char ** argv)\n{\n    cl_platform_id platform;\n    clGetPlatformIDs(1, &amp;platform, NULL);\n    cl_device_id device;\n    clGetDeviceIDs(platform, CL_DEVICE_TYPE_GPU, 1, &amp;device, NULL);\n    cl_context context = clCreateContext(NULL, 1, &amp;device, NULL, NULL, NULL);\n    cl_command_queue queue = clCreateCommandQueue(context, device, 0, NULL);\n    const char *source =\n    \"__kernel void copytest(__global short* dst) {\\n\"\n    \"    __local short buff[1024];\\n\"\n    \"    for (int i = 0; i &lt; 1024; i++) {\\n\"\n    \"        for (int j = 0; j &lt; 1024; j++)\\n\"\n    \"            buff[j] = j;\\n\"\n    \"        (void)async_work_group_copy(&amp;dst[i*1024], buff, 1024, 0);\\n\"\n    \"    }\\n\"\n    \"}\\n\";\n    cl_program program = clCreateProgramWithSource(context, 1, &amp;source, NULL, NULL);\n    clBuildProgram( program, 1, &amp;device, NULL, NULL, NULL);\n    cl_kernel kernel = clCreateKernel( program, \"copytest\", NULL);\n    cl_mem buf = clCreateBuffer(context, CL_MEM_WRITE_ONLY, 1024 * 1024 * 2, NULL, NULL);\n    const size_t global_work_size = 1;\n    clSetKernelArg(kernel, 0, sizeof(buf), (void*)&amp;buf);\n    LARGE_INTEGER pcFreq = {}, pcStart = {}, pcEnd = {};\n    QueryPerformanceFrequency(&amp;pcFreq);\n    QueryPerformanceCounter(&amp;pcStart);\n    clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &amp;global_work_size, NULL, 0, NULL, NULL);\n    clFinish(queue);\n    QueryPerformanceCounter(&amp;pcEnd);\n    std::cout &lt;&lt; 2.0 * pcFreq.QuadPart / (pcEnd.QuadPart-pcStart.QuadPart) &lt;&lt; \"MB/sec\";\n}\n</code></pre>\n\n<p>As you can see, it's operating all just on a single work unit.  I tried replacing the async_work_group_copy() with a loop distributed amongst multiple (64) work units, but that did not help.</p>\n\n<p>Is there some way to pull memory off faster from the Radeon than 7MB/sec?  I'm interested in the hundreds of MB/sec.  Would NVidia be faster?</p>\n"},{"tags":["multithreading","performance","qt","timer","signals"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":77,"score":1,"question_id":12924960,"title":"Timer makes UI unresponsive?","body":"<p>I use a timer to emit a signal and call a slot function which updates the UI according to new data.</p>\n\n<p>I've calculated the time to run that function as being quite short. When the timer is set to 1000ms, the UI responds quite slow. </p>\n\n<p>I tried to move that functionality to a thread but I'm finding it difficult since a lot of the functionality needs to access the UI class's protected values.</p>\n\n<p>I'm going to try moving the timer to another thread and leave the update functionality in the UI class (main window) but I don't know if it will help.</p>\n\n<p>Why is the timer causing the UI to be slow and unresponsive? Will a thread be lighter and consume less CPU time? How can I fix this?</p>\n\n<pre><code>    initTimer()\n    {\n        refreshTimer = new QTimer(this);\n        connect(refreshTimer, SIGNAL(timeout()), this, SLOT(refreshDisplay));\n        refreshTimer-&gt;start(1000);\n    }\n</code></pre>\n\n<p>refresh slot function called every 1000ms:</p>\n\n<pre><code>    void refreshDisplay()\n    {\n        ui-&gt;tableWidget-&gt;setUpdatesEnabled(false);\n\n        for(int queue_i = size_1, index = 0; queue_i &gt;= 0; queue_i--, index++)\n        {\n            LogInfoItem* logItem = (LogInfoItem*)logDisplayQueue.at(queue_i);\n\n            QString BITS_str = bits2Hexs(logItem-&gt;BITS);\n\n\n            ui-&gt;tableWidget-&gt;item(index, 0)-&gt;setText(logItem-&gt;time);//time\n            ui-&gt;tableWidget-&gt;item(index, 1)-&gt;setText(logItem-&gt;name);//name\n            ui-&gt;tableWidget-&gt;item(index, 2)-&gt;setText(BITS_str);//BITS\n\n            if(queue_i == oldRowItemNo)ui-&gt;tableWidget-&gt;selectRow(index);\n        }\n\n        ui-&gt;tableWidget-&gt;setUpdatesEnabled(true);\n\n        Q_FOREACH(Page* p, PageInfoList)\n        {\n            p-&gt;refresh();\n        }\n\n        Q_FOREACH(IconLabel* icl, iconLabelList)\n        {\n            icl-&gt;refresh();\n        }\n    }\n</code></pre>\n\n<p>What 'refresh()' does is just changing icons and texts in the ui according to the data inside.   Besides, i've made the data static, but still cannot fix it.\n    (I've tested the function, almost no time consuming...)</p>\n"},{"tags":["c++","windows","linux","performance","qt"],"answer_count":4,"favorite_count":0,"up_vote_count":9,"down_vote_count":0,"view_count":233,"score":9,"question_id":12878980,"title":"Speed performance of a Qt program: Windows vs Linux","body":"<p>I've already posted this question <a href=\"http://qt-project.org/forums/viewthread/21092/\" rel=\"nofollow\">here</a>, but since it's maybe not that Qt-specific, I thought I might try my chance here as well. I hope it's not inappropriate to do that (just tell me if it is).</p>\n\n<p>I’ve developed a small scientific program that performs some mathematical computations. I’ve tried to optimize it so that it’s as fast as possible. Now I’m almost done deploying it for Windows, Mac and Linux users. But I have not been able to test it on many different computers yet.</p>\n\n<p>Here’s what troubles me: To deploy for Windows, I’ve used a laptop which has both Windows 7 and Ubuntu 12.04 installed on it (dual boot). I compared the speed of the app running on these two systems, and I was shocked to observe that <strong>it’s at least twice as slow on Windows!</strong> I wouldn’t have been surprised if there were a small difference, but how can one account for such a difference?</p>\n\n<p>Here are a few precisions:</p>\n\n<ul>\n<li>The computation that I make the program do are just some brutal and stupid mathematical calculations, basically, it computes products and cosines in a loop that is called a billion times. On the other hand, the computation is multi-threaded: I launch something like 6 QThreads.</li>\n<li>The laptop has two cores @1.73Ghz. At first I thought that Windows was probably not using one of the cores, but then I looked at the processor activity, according to the small graphic, both cores are running 100%.</li>\n<li>Then I thought the C++ compiler for Windows didn’t the use the optimization options (things like -O1 -O2) that the C++ compiler for Linux automatically did (in release build), but apparently it does.</li>\n</ul>\n\n<p>I’m bothered that the app is so mush slower (2 to 4 times) on Windows, and it’s really weird. On the other hand I haven’t tried on other computers with Windows yet. Still, do you have any idea why the difference?</p>\n\n<p><strong>Additional info:</strong> some data…</p>\n\n<p>Even though Windows seems to be using the two cores, I’m thinking this might have something to do with threads management, here’s why:</p>\n\n<p>Sample Computation n°1 (this one launches 2 QThreads):</p>\n\n<ul>\n<li>PC1-windows: 7.33s</li>\n<li>PC1-linux: 3.72s</li>\n<li>PC2-linux: 1.36s</li>\n</ul>\n\n<p>Sample Computation n°2 (this one launches 3 QThreads):</p>\n\n<ul>\n<li>PC1-windows: 6.84s</li>\n<li>PC1-linux: 3.24s</li>\n<li>PC2-linux: 1.06s</li>\n</ul>\n\n<p>Sample Computation n°3 (this one launches 6 QThreads):</p>\n\n<ul>\n<li>PC1-windows: 8.35s</li>\n<li>PC1-linux: 2.62s</li>\n<li>PC2-linux: 0.47s</li>\n</ul>\n\n<p>where:</p>\n\n<ul>\n<li>PC1-windows = my 2 cores laptop (@1.73Ghz) with Windows 7</li>\n<li>PC1-linux = my 2 cores laptop (@1.73Ghz) with Ubuntu 12.04</li>\n<li>PC2-linux = my 8 cores laptop (@2.20Ghz) with Ubuntu 12.04</li>\n</ul>\n\n<p>(Of course, it's not shocking that PC2 is faster. What's incredible to me is the difference between PC1-windows and PC1-linux).</p>\n\n<p>Note: I've also tried running the program on a recent PC (4 or 8 cores @~3Ghz, don't remember exactly) under Mac OS, speed was comparable to PC2-linux (or slightly faster).</p>\n\n<p>EDIT: I'll answer here a few questions I was asked in the comments.</p>\n\n<ul>\n<li><p>I just installed Qt SDK on Windows, so I guess I have the latest version of everything (including MinGW?). The compiler is MinGW. Qt version is 4.8.1.</p></li>\n<li><p>I use no optimization flags because I noticed that they are automatically used when I build in release mode (with Qt Creator). It seems to me that if I write something like QMAKE_CXXFLAGS += -O1, this only has an effect in debug build.</p></li>\n<li><p>Lifetime of threads etc: this is pretty simple. When the user clicks the \"Compute\" button, 2 to 6 threads are launched simultaneously (depending on what he is computing), they are terminated when the computation ends. Nothing too fancy. Every thread just does brutal computations (except one, actually, which makes a (not so) small\"computation every 30ms, basically checking whether the error is small enough).</p></li>\n</ul>\n\n<p><strong>EDIT: latest developments and partial answers</strong></p>\n\n<p>Here are some new developments that provide answers about all this:</p>\n\n<ul>\n<li><p>I wanted to determine whether the difference in speed really had something to do with threads or not. So I modified the program so that the computation only uses 1 thread, that way we are pretty much comparing the performance on \"pure C++ code\". It turned out that now Windows was only slightly slower than Linux (something like 15%). So I guess that <strong>a small (but not unsignificant) part of the difference is intrinsic to the system, but the largest part is due to threads management</strong>.</p></li>\n<li><p>As someone (Luca Carlon, thanks for that) suggested in the comments, I tried building the application with the compiler for Microsoft Visual Studio (MSVC), instead of MinGW. And suprise, the computation (with all the threads and everything) was now \"only\" 20% to 50% slower than Linux! I think I'm going to go ahead and be content with that. I noticed that weirdly though, the \"pure C++\" computation (with only one thread) was now even slower (than with MinGW), which must account for the overall difference. So as far as I can tell, <strong>MinGW is slightly better than MSVC except that it handles threads like a moron</strong>.</p></li>\n</ul>\n\n<p>So, I’m thinking either there’s something I can do to make MinGW (ideally I’d rather use it than MSVC) handle threads better, or it just can’t. I would be amazed, how could it not be well known and documented ? Although I guess I should be careful about drawing conclusions too quickly, I’ve only compared things on one computer (for the moment).</p>\n"},{"tags":["sql","ruby-on-rails","performance"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":85,"score":0,"question_id":12944813,"title":"Rails 3 user matching-algorithm to SQL Query (COMPLICATED)","body":"<p>I'm currently working on an app that matches users based on answered questions.\nI realized my algorithm in normal RoR and ActiveRecord queries but it's waaay to slow to use it. To match one user with 100 other users takes</p>\n\n<pre><code>Completed 200 OK in 17741ms (Views: 106.1ms | ActiveRecord: 1078.6ms)\n</code></pre>\n\n<p>on my local machine. But still...\nI now want to realize this in raw SQL in order to gain some more performance. But I'm really having trouble getting my head around SQL queries inside of SQL queries and stuff like this plus calculations etc. My head is about to explode and I don't even know where to start.</p>\n\n<p>Here's my algorithm:</p>\n\n<pre><code>def match(user)\n  @a_score = (self.actual_score(user).to_f / self.possible_score(user).to_f) * 100\n  @b_score = (user.actual_score(self).to_f / user.possible_score(self).to_f) * 100\n\n  if self.common_questions(user) == []\n    0.to_f\n  else\n    match = Math.sqrt(@a_score * @b_score) - (100 / self.common_questions(user).count)\n    if match &lt;= 0\n      0.to_f\n    else\n      match\n    end\n  end\nend\n\ndef possible_score(user)\n  i = 0\n  self.user_questions.select(\"question_id, importance\").find_each do |n|\n    if user.user_questions.select(:id).find_by_question_id(n.question_id)\n      i += Importance.find_by_id(n.importance).value\n    end\n  end\n  return i\nend\n\ndef actual_score(user)\n  i = 0\n  self.user_questions.select(\"question_id, importance\").includes(:accepted_answers).find_each do |n|\n    @user_answer = user.user_questions.select(\"answer_id\").find_by_question_id(n.question_id)\n    unless @user_answer == nil\n      if n.accepted_answers.select(:answer_id).find_by_answer_id(@user_answer.answer_id)\n        i += Importance.find_by_id(n.importance).value\n      end\n    end\n  end\n  return i\nend\n</code></pre>\n\n<p>So basically a user answers a questions, picks what answers he accepts and how important that question is to him. The algorithm then checks what questions 2 users have in common, if user1 gave an answer user2 accepts, if yes then the importance user2 gave for each question is added which makes up the score user1 made. Also the other way around for user2. Divided by the possible score gives the percentage and both percentages applied to the geometric mean gives me one total match percentage for both users. Fairly complicated I know. Tell if I didn't explain it good enough. I just hope I can express this in raw SQL. Performance is everything in this.</p>\n\n<p>Here are my database tables:</p>\n\n<pre><code>CREATE TABLE \"users\" (\"id\" INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, \"username\" varchar(255) DEFAULT '' NOT NULL); (left some unimportant stuff out, it's all there in the databse dump i uploaded)\n\nCREATE TABLE \"user_questions\" (\"id\" INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, \"user_id\" integer, \"question_id\" integer, \"answer_id\" integer(255), \"importance\" integer, \"explanation\" text, \"private\" boolean DEFAULT 'f', \"created_at\" datetime);\n\nCREATE TABLE \"accepted_answers\" (\"id\" INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, \"user_question_id\" integer, \"answer_id\" integer);\n</code></pre>\n\n<p>I guess the top of the SQL query has to look something like this?</p>\n\n<pre><code>SELECT u1.id AS user1, u2.id AS user2, COALESCE(SQRT( (100.0*actual_score/possible_score) * (100.0*actual_score/possible_score) ), 0) AS match\nFROM \n</code></pre>\n\n<p>But since I'm not an SQL master and can only do the usual stuff my head is about to explode.\nI hope someone can help me figure this out. Or atleast improve my performance somehow! Thanks so much!</p>\n\n<h1>EDIT:</h1>\n\n<p>So based on Wizard's answer I've managed to get a nice SQL statement for \"possible_score\"</p>\n\n<pre><code>SELECT SUM(value) AS sum_id \nFROM user_questions AS uq1\nINNER JOIN importances ON importances.id = uq1.importance\nINNER JOIN user_questions uq2 ON uq1.question_id = uq2.question_id AND uq2.user_id = 101\nWHERE uq1.user_id = 1\n</code></pre>\n\n<p>I've tried to get the \"actual_score\" with this but it didn't work. My database manager crashed when I executed this.</p>\n\n<pre><code>SELECT SUM(imp.value) AS sum_id \nFROM user_questions AS uq1\nINNER JOIN importances imp ON imp.id = uq1.importance\nINNER JOIN user_questions uq2 ON uq2.question_id = uq1.question_id AND uq2.user_id = 101\nINNER JOIN accepted_answers as ON as.user_question_id =  uq1.id AND as.answer_id = uq2.answer_id\nWHERE uq1.user_id = 1\n</code></pre>\n\n<h1>EDIT2</h1>\n\n<p>Okay I'm an idiot! I can't use \"as\" as an alias of course. Changed it to aa and it worked! W00T!</p>\n"},{"tags":["javascript","jquery","performance","browser","rendering"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":1129,"score":1,"question_id":5190296,"title":"How to test rendering time (not DomContentLoaded nor onLoad)?","body":"<p>I need to have a way or tools to test the actual perceived rendering time for the browser to render the entire page to users. Any suggestions? </p>\n\n<p>The reason I ask is because firbug and Yslow only reports the DomContentLoaded and OnLoad time. </p>\n\n<p>For instance, my application reports 547ms (onLoad:621ms) for the contents. But the actual content is rendered around 3 seconds. I know so because I actually counted 1, 2, 3 slowly  from the moment I hit enter in the url field of the browser to the moment when content appears in front of my eyes. So I know 547ms nor 621ms DOES NOT represents the actual time it takes for the page to load. </p>\n\n<p>Not sure if this helps. But my application </p>\n\n<ol>\n<li><p>renders json data on the server side, save the data as a javascript variable along with the rest of the page's html before server returns the entire html to browser</p></li>\n<li><p>page loads Jquery 1.5 and Jquery template</p></li>\n<li><p>jquery code grabs the json data from the variable defined at step 1</p></li>\n<li><p>use jquery template to render the page. </p></li>\n</ol>\n\n<p>Technically, no Ajax involved here and images on the page are all cached. I don't see firebug downloads any of them. </p>\n\n<p>[Edit]</p>\n\n<p>What i'm trying to figure out is after the firebug reported onLoad time which in my case is 621ms, to the time the page is completed and loaded in my eyes (which is at least 3 seconds), what happened to the 2.4s in between? What took place there? Browser is doing something? Something is blocking? Network? what is it?</p>\n"},{"tags":["performance","matlab","optimization","vectorization"],"answer_count":1,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":129,"score":4,"question_id":12949936,"title":"Vectorization of matlab code","body":"<p>i'm kinda new to vectorization. Have tried myself but couldn't. Can somebody help me vectorize this code as well as give a short explaination on how u do it, so that i can adapt the thinking process too. Thanks. </p>\n\n<pre><code>function [result] = newHitTest (point,Polygon,r,tol,stepSize)\n%This function calculates whether a point is allowed.\n\n%First is a quick test is done by calculating the distance from point to \n%each point of the polygon. If that distance is smaller than range \"r\", \n%the point is not allowed. This will slow down the algorithm at some \n%points, but will greatly speed it up in others because less calls to the \n%circleTest routine are needed.\npolySize=size(Polygon,1);\ntestCounter=0;\n\nfor i=1:polySize\nd = sqrt(sum((Polygon(i,:)-point).^2));\n\nif d &lt; tol*r\n    testCounter=1;\n    break\nend\nend\n\nif testCounter == 0\ncircleTestResult = circleTest (point,Polygon,r,tol,stepSize);\ntestCounter = circleTestResult;\nend\n\nresult = testCounter;\n</code></pre>\n"},{"tags":["java","multithreading","performance","parallel-processing","fork-join"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":122,"score":2,"question_id":11809652,"title":"Performance problems using new fork join framework of jdk 7","body":"<p>I'm using the new forkjoin framework of jdk 7.\nI got a task, which has to be performed multiple times with different parameters.</p>\n\n<p>This task extends <code>RecursiveTask</code>. there are more than 100 tasks to perform, which can performed concurrently. the tasks are independent, so there should be no need for any synchronisation. \nTherefore I created at first the needed tasks and passed them to forkjoin thread pool.\nbut the application becomes slower, than running it without any parallelism.</p>\n\n<p>My first thought was, that i create to much threads..  Thats why i tried to recycle the threads to reduce object creation overhead, but this has no effect on the performance. for recycling im using the reinitialize() method. Also with recycling the performance is slower than running it without any parallelism.</p>\n\n<p>The operations performed in the tasks are not trivial, the duration of running threads are from 5 to 150 ms.   The application runs on a dualcore machine and im using ubuntu and oracle jdk 7.</p>\n"},{"tags":["java","regex","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":54,"score":0,"question_id":12962091,"title":"Java RegEx Performance with Pattern.CASE_INSENSITIVE","body":"<p>I got a pretty simple regular expression I am using </p>\n\n<pre><code>%%(products?)%%\n</code></pre>\n\n<p>Now I want it to be able to match both products? and Products?. The obvious answer is to use the CASE_INSENSITIVE tag when compiling a pattern: </p>\n\n<pre><code>Pattern.compile(\"%%(products?)%%\", Pattern.CASE_INSENSITIVE)\n</code></pre>\n\n<p>But on the <a href=\"http://docs.oracle.com/javase/1.4.2/docs/api/java/util/regex/Pattern.html#CASE_INSENSITIVE\" rel=\"nofollow\">documentation</a> it says \"Specifying this flag may impose a slight performance penalty.\" I therefore thought of an alternative, without the flag:</p>\n\n<pre><code>Pattern.compile(\"%%([Pp]roducts?)%%\")\n</code></pre>\n\n<p>My question is: Which one would have better performance?</p>\n"},{"tags":["ruby-on-rails","ruby","multithreading","performance"],"answer_count":2,"favorite_count":3,"up_vote_count":7,"down_vote_count":0,"view_count":601,"score":7,"question_id":5426540,"title":"How to speed up the processing of a large CSV using ruby","body":"<p>For a project I need to parse some pretty large CSV files. The contents of some of the entries is stored in a MySQL database. I am trying to speed this up using multithreading, but up to now this only slows things down.</p>\n\n<p>I parse a CSV file (up to 10GB), and some of these records (aprox. 5M out of a 20M+ record CSV) need to be inserted into a MySQL database. To determine which record needs to be inserted we use a Redis server with sets that contain the correct id's / references.</p>\n\n<p>Since we process around 30 of these files at any given time, and there are some dependencies, we store each file on a Resque queue and have multiple servers handling these (prioritized) queues.</p>\n\n<p>In a nutshell:</p>\n\n<pre><code>class Worker\n  def self.perform(file)\n    CsvParser.each(file) do |line|\n      next unless check_line_with_redis(line)\n      a = ObjectA.find_or_initialize_by_reference(line[:reference])\n      a.object_bs.destroy_all\n      a.update_attributes(line)\n    end\n  end\n</code></pre>\n\n<p>This works, scales nice horizontally (more CSV files = more servers), but larger CSV files pose a problem. We currently have files that take over 75 hours to parse this way. There are a number of optimizations I thought of already:</p>\n\n<p>One is cutting down on the MySQL queries; we instantiate AR objects while an insert with plain SQL, if we know the objects Id, is much faster. This way we can probably get rid of most of AR and maybe even Rails to remove overhead this way. We can't use a plain MySQL load data since we have to map the CSVs records to other entities that might have different Ids by now (we combine a dozen legacy databases into a new database).</p>\n\n<p>The other is trying to do more in the same time. There is some IO wait time, network wait time for both Redis and MySQL, and even while MRI uses green threads this might allow us to schedule our MySQL queries at the same time as the IO reads etc. But using the following code:</p>\n\n<pre><code>class Worker\n  def self.perform(file)\n    CsvParser.each(file) do |line|\n      next unless check_line_with_redis(line)\n      create_or_join_thread(line) do |myLine|\n        a = ObjectA.find_or_initialize_by_reference(myLine[:reference])\n        a.object_bs.destroy_all\n        a.update_attributes(myLine)\n      end\n    end\n\n    def self.create_or_join_thread(line)\n      @thread.join if @thread.present?\n      @thread = Thread.new(line) do |myLine|\n        yield myLine\n      end\n    end\n  end\n</code></pre>\n\n<p>This slowly slows down the process. When I <code>ps au</code> it starts off at 100% CPU, but as time progresses it goes down to just 2-3%. At that moment it does not insert new records at all, it just appears to hang.</p>\n\n<p>I have <code>strace</code>d the process, and at first I see MySQL queries pass by, after a while it appears it is not executing my ruby code at all. Could be a deadlock (it hung after parsing the <em>last</em> line of the CSV, but the process kept on running at 5% CPU and did not quit), or something I read here: <a href=\"http://timetobleed.com/ruby-threading-bugfix-small-fix-goes-a-long-way/\" rel=\"nofollow\">http://timetobleed.com/ruby-threading-bugfix-small-fix-goes-a-long-way/</a></p>\n\n<p>I am using Rails 2.3.8, REE, 1.8.7-2010.02 on Ubuntu 10.10. Any insights on how to handle large numbers of threads (or maybe why not to use threads here at all) is greatly appreciated!</p>\n"},{"tags":["c#","performance","optimization"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":70,"score":1,"question_id":12963203,"title":"Why is this List.Except so slow in some cases (and how to speed it up)?","body":"<p>I have the following two lists, which are pairs of strings.  One is what I expect and the other is what I found.  I want to find out what is missing.  The code works, but some cases are much, much slower than others.</p>\n\n<ul>\n<li>When n = 1, it takes 21 seconds for the <code>.Except()</code> call.  </li>\n<li>When n = 10, it takes 2 seconds for the <code>.Except()</code> call.  </li>\n</ul>\n\n<p>In both cases, it is the same number of elements.  Is this just some hash table collisions? What can I do to make all cases equally quick?</p>\n\n<pre><code>List&lt;KeyValuePair&lt;string, string&gt;&gt; FoundItems = new List&lt;KeyValuePair&lt;string, string&gt;&gt;();\nList&lt;KeyValuePair&lt;string, string&gt;&gt; ExpectedItems = new List&lt;KeyValuePair&lt;string, string&gt;&gt;();\n\nint n = 1;\nfor (int k1 = 0; k1 &lt; n; k1 ++)\n{\n    for (int k2 = 0; k2 &lt; 3500/n; k2++)\n    {\n        ExpectedItems.Add(new KeyValuePair&lt;string, string&gt;( k1.ToString(), k2.ToString()));\n        if (k2 != 0)\n        {\n            FoundItems.Add(new KeyValuePair&lt;string, string&gt;(k1.ToString(), k2.ToString()));\n        }\n    }\n}\n\nStopwatch sw = new Stopwatch();\nsw.Start();\n\n//!!!! This is the slow line.\nList&lt;KeyValuePair&lt;string, string&gt;&gt; MissingItems = ExpectedItems.Except(FoundItems).ToList();\n//!!!! \n\nstring MatchingTime = \"Matching Time: \" + sw.ElapsedMilliseconds.ToString() + \" (\" + sw.ElapsedMilliseconds / 1000 + \" sec)\";\nMessageBox.Show(MatchingTime + \", \" + ExpectedItems.Count() + \" items\");\n</code></pre>\n\n<p>My data really are strings, I just use integers in this test case because it's easy.</p>\n"},{"tags":["jquery","performance","jquery-validate"],"answer_count":3,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":653,"score":4,"question_id":5542014,"title":"jQuery validate large forms - script running slowly","body":"<p>I'm using jQuery Validate plugin 1.8.0 with jQuery 1.5. Works great for small to medium sized forms. For larger forms the performance degrades significantly (even in IE8 and FF4), sometimes causing the \"script is running too slowly\" message. It appears that the plugin scans the entire DOM within the form looking for attributes and classes to validate, even if you specified custom rules. Anyone know how to turn this off completely? There is an ignore option as well, but it still would scan the DOM, skipping those with the ignore attr.</p>\n\n<p>Here is what ASP.NET renders, except there are about 120 rows of data. Paging the results is not an option, unfortunately.</p>\n\n<pre><code>&lt;table id=\"GridView1\"&gt;\n    &lt;tbody&gt;\n        &lt;tr&gt;\n            &lt;th scope=\"col\"&gt;Header 1&lt;/th&gt;\n            &lt;th scope=\"col\"&gt;Header 2&lt;/th&gt;\n            &lt;th scope=\"col\"&gt;Header 3&lt;/th&gt;\n            &lt;th scope=\"col\"&gt;Header 4&lt;/th&gt;\n            &lt;th scope=\"col\"&gt;Header 5&lt;/th&gt;\n            &lt;th scope=\"col\"&gt;Header 6&lt;/th&gt;\n            &lt;th style=\"width: 60px; white-space: nowrap\" scope=\"col\"&gt;Header 7&lt;/th&gt;\n            &lt;th style=\"width: 60px; white-space: nowrap\" scope=\"col\"&gt;Header 8&lt;/th&gt;\n        &lt;/tr&gt;        \n        &lt;tr class=\"gridRow\" jquery1507811088779756411=\"3\"&gt;\n            &lt;td style=\"width: 50px\" align=\"middle\"&gt;\n                &lt;span id=\"GridView1_ctl03_Label1\"&gt;XXX&lt;/span&gt;\n            &lt;/td&gt;\n            &lt;td&gt;\n                &lt;span id=\"GridView1_ctl03_Label2\"&gt;YYY&lt;/span&gt;\n            &lt;/td&gt;\n            &lt;td style=\"width: 50px\" align=\"middle\"&gt;\n                &lt;span id=\"GridView1_ctl03_Label3\"&gt;ZZZ&lt;/span&gt;\n            &lt;/td&gt;\n            &lt;td align=\"middle\"&gt;\n                &lt;select style=\"width: 70px\" id=\"GridView1_ctl03_Dropdown4\" name=\"GridView1$ctl03$Dropdown4\"&gt;\n                    &lt;option selected value=\"Y\"&gt;Y&lt;/option&gt;\n                    &lt;option value=\"N\"&gt;N&lt;/option&gt;\n                &lt;/select&gt;\n            &lt;/td&gt;\n            &lt;td style=\"width: 50px\" align=\"middle\"&gt;\n                &lt;input id=\"GridView1_ctl03_hidId1\" value=\"100\" type=\"hidden\" name=\"GridView1$ctl03$hidId1\" /&gt;\n                &lt;input id=\"GridView1_ctl03_hidId2\" value=\"100\" type=\"hidden\" name=\"GridView1$ctl03$hidId2\" /&gt;\n                &lt;input id=\"GridView1_ctl03_hidId3\" value=\"100\" type=\"hidden\" name=\"GridView1$ctl03$hidId3\" /&gt;\n                &lt;input id=\"GridView1_ctl03_hidId4\" value=\"100\" type=\"hidden\" name=\"GridView1$ctl03$hidId4\" /&gt;\n                &lt;select style=\"width: 40px\" id=\"GridView1_ctl03_Dropdown5\" name=\"GridView1$ctl03$Dropdown5\"&gt;\n                    &lt;option selected value=\"A\"&gt;A&lt;/option&gt;\n                    &lt;option value=\"B\"&gt;B&lt;/option&gt;\n                &lt;/select&gt;\n            &lt;/td&gt;\n            &lt;td style=\"width: 50px\" align=\"middle\"&gt;\n                &lt;span id=\"GridView1_ctl03_Label6\"&gt;101&lt;/span&gt;\n            &lt;/td&gt;\n            &lt;td align=\"middle\"&gt;\n                &lt;input style=\"width: 60px\" id=\"GridView1_ctl03_Textbox8\" class=\"date required\"\n                    title=\"Please enter a valid start date.\" type=\"text\" name=\"GridView1$ctl03$Textbox8\"\n                    jquery1507811088779756411=\"122\" /&gt;\n            &lt;/td&gt;\n            &lt;td align=\"middle\"&gt;\n                &lt;input style=\"width: 60px\" id=\"GridView1_ctl03_Textbox9\" class=\"date\"\n                    title=\"Please enter a valid end date.\" type=\"text\" name=\"GridView1$ctl03$Textbox9\"\n                    jquery1507811088779756411=\"123\" /&gt;\n            &lt;/td&gt;\n        &lt;/tr&gt;\n    &lt;/tbody&gt;\n&lt;/table&gt;\n</code></pre>\n"},{"tags":["javascript","c++","performance","windows-8"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":56,"score":0,"question_id":12962177,"title":"Windows 8 project type performance differences","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/7466303/c-c-sharp-and-javascript-on-winrt\">C++, C# and JavaScript on WinRT</a>  </p>\n</blockquote>\n\n\n\n<p>I am wanting to create an app for Windows 8 and am equally well versed in C++ and JavaScript. I was wondering what (if any) performance differences there would be between a JavaScript/HTML or C++/XAML. Do they both get compiled to the same source or is the JavaScript version interpreted?</p>\n\n<p>Thanks!</p>\n"},{"tags":["iphone","ios","performance","uitableview","memory"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":35,"score":1,"question_id":12962051,"title":"UITableView Dynamic vs. Static Cell Allocation Performance / Memory Usage","body":"<p>Could anyone tell me the tradeoff in performance/memory usage between using static and dynamic cells in a UITableView?</p>\n\n<p>Here's my situation: I have a TableView with 6 different sections. The first section is the only section in my tableView that holds a different number of cells each time the view loads, depending on the current state of the app. i.e. I have declared 12 static cells for that section in interface builder, however I only display a certain number of those cells depending on the user's interaction with the app thus far. The other 5 tableView sections all contain UISwitches and textFields that never change.</p>\n\n<p>So say I statically allocated 50 cells for that first section, but still only displayed maybe just half of them depending on the state of the app. I would want to be able to display up to 50 cells though. How would this affect the speed or performance of my app? Would doing the entire tableView dynamically and redrawing the switches and textFields for the other sections each time lead to a better application performance?</p>\n"},{"tags":["sql-server","performance","sql-server-2008","tsql","insert"],"answer_count":3,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":117,"score":1,"question_id":12956286,"title":"Optimal insert/update batch for SQL Server","body":"<p>I'm making frequent inserts and updates in large batches from c# code and I need to do it as fast as possible, please help me find all ways to speed up this process.</p>\n\n<ol>\n<li>Build command text using <code>StringBuilder</code>, separate statements with <code>;</code> </li>\n<li>Don't use <code>String.Format</code> or <code>StringBuilder.AppendFormat</code>, it's slower then multiple <code>StringBuilder.Append</code> calls</li>\n<li>Reuse <code>SqlCommand</code> and <code>SqlConnection</code></li>\n<li>Don't use <code>SqlParameter</code>s (limits batch size)</li>\n<li>Use <code>insert into table values(..),values(..),values(..)</code> syntax (1000 rows per statement)</li>\n<li>Use as few indexes and constraints as possible</li>\n<li>Use simple recovery model if possible</li>\n<li>?</li>\n</ol>\n\n<p>Here are questions to help update the list above</p>\n\n<ol>\n<li>What is optimal number of statements per command (per one <code>ExecuteNonQuery()</code> call)?</li>\n<li>Is it good to have inserts and updates in the same batch, or it is better to execute them separately?</li>\n</ol>\n\n<p>My data is being received by tcp, so please don't suggest any Bulk Insert commands that involve reading data from file or external table. </p>\n\n<p>Insert/Update statements rate is about 10/3.</p>\n"},{"tags":["jquery","performance","table"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":41,"score":0,"question_id":12961526,"title":"How to show table rows after they've been hidden","body":"<p>I have a large table and allow the user to hide rows based upon a lot of complicated rules.\nI also have a 'reset' button that is to reveal all the rows back again, but it is running very slowly:</p>\n\n<pre><code>$('#myTable tbody tr').show('fast');\n</code></pre>\n\n<p>Q: Is there a fast way to show table rows that have been previously hidden with the hide method?</p>\n\n<p>Perhaps I should add class=\"hide\" and removeClass instead.</p>\n"},{"tags":[".net","performance"],"answer_count":3,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":163,"score":5,"question_id":1637903,"title":"Odd performance degradation in application","body":"<p>We have an application that is mixed .NET 2.0 and native C++. In our testing, we have a mode which can automatically loop through a set of projects. A project opens, runs, closes, repeat. Each of these steps requires creation/destruction of windows (winforms to be precise). Recently we've experienced some odd behavior in performance. After running for a few hours the opening and closing parts slow down (blocking the gui thread and showing half drawn screens etc). Now it would be easy to chock this up to a resource leak...but we're tracking handles and memory, and while memory grows slightly there's nothing to indicate this level of problem. Handles are stable. So maybe dangling event handlers...still need to investigate that. But the kicker, which perplexes me, is that shutting down the application and restarting it doesn't bring back the initial performance. It's still slow until I reboot the OS (win XP) and then performance starts out snappy again. This really perplexes me as I assume shutting down the application will reclaim all resources. Any thoughts?</p>\n"},{"tags":["android","performance","android-layout"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":47,"score":0,"question_id":12960967,"title":"Can my layout be improved for efficiency?","body":"<p>I have a layout as shown below. It is inflated by code and added to a HorizontalScrollView, sometimes a few hundred times, and causing getting memory issues.</p>\n\n<p>I'm wondering if there's anything that can be done to make it more efficient? Originally I used LinearLayouts, and replacing that with RelativeLayout made a huge difference to the scrolling. Now I'm wondering if it can be further improved?</p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"156dp\"\n    android:layout_height=\"254dp\"\n    android:paddingLeft=\"7dip\"\n    android:paddingRight=\"7dip\"&gt;\n\n    &lt;FrameLayout android:id=\"@+id/button_frame\"\n    android:layout_width=\"156dp\"\n    android:layout_height=\"218dp\"&gt;\n\n    &lt;ImageView android:id=\"@+id/button_bg\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"\n        android:src=\"@drawable/image_bg\"\n        /&gt;\n\n        &lt;ImageView android:id=\"@+id/button_image\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"\n        android:layout_gravity=\"bottom|right\"\n        /&gt;\n\n        &lt;TextView android:id=\"@+id/button_select\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"\n        android:layout_marginLeft=\"2dip\"\n        android:layout_marginRight=\"2dip\"\n        android:background=\"@drawable/btn_selector_bg_selected\"\n        /&gt;\n\n        &lt;TextView android:id=\"@+id/button_selected\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"wrap_content\"\n        android:layout_gravity=\"bottom\"\n        android:layout_marginLeft=\"2dip\"\n        android:layout_marginRight=\"2dip\"\n        android:background=\"@drawable/title_bg_selected\"/&gt;\n\n    &lt;/FrameLayout&gt;\n\n    &lt;TextView\n    android:id=\"@+id/button_title\"\n    android:layout_width=\"156dp\"\n    android:layout_height=\"36dp\"\n    android:textColor=\"#ffffff\"\n    android:singleLine=\"true\"\n    android:textSize=\"13sp\"\n    android:textStyle=\"bold\"\n    android:gravity=\"center_vertical|left\"\n    android:ellipsize=\"end\"\n    android:paddingLeft=\"30dip\"\n    android:paddingRight=\"5dip\"\n    android:layout_below=\"@id/button_frame\"\n    android:background=\"@drawable/title_bg\"/&gt;\n\n&lt;/RelativeLayout&gt;\n</code></pre>\n\n<p>The ImageView button_image is populated using AQuery image caching, so I'm not sure if there's much more I can do to improve the way the image is handled. But any tips on improvements greatly appreciated.</p>\n"},{"tags":["performance","matlab","nested-loops","matrix-multiplication"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":123,"score":1,"question_id":12944026,"title":"MATLAB - Help speed up the code","body":"<p>I need your help to speed up my MATLAB code. Line 17 is the most expensive part. It is because of the two nested loops. I need help with removing the loops and rewrite it into just one matrix multiplication expression. Note that I have taken dKdx as a cell, which is causing problems to replace nested loops with simple matrix multiplication term. Any ideas?\nBelow if a simplified code. May be dKdx doesn't need to be an cell? Idea behind cell was to be able to store lot of matrices of size [2*(nelx+1)<em>(nely+1),2</em>(nelx+1)*(nely+1)].</p>\n\n<pre><code>    clc\n    nelx = 16; nely = 8;\n    dKdx = cell(2*(nelx+1)*(nely+1),1);\n    Hess = zeros(nelx*nely,nelx*nely);\n    U = rand(2*(nelx+1)*(nely+1),1);\n    dUdx = rand(2*(nelx+1)*(nely+1),nelx*nely);\n\n    for elx = 1:nelx\n        for ely = 1:nely\n            elm = nely*(elx-1)+ely;               \n            dKdx{elm,1} = rand(2*(nelx+1)*(nely+1),2*(nelx+1)*(nely+1));\n        end\n    end\n\n    for i = 1:nelx*nely    \n        for j = i:nelx*nely\n            Hess(i,j) = U'*dKdx{j,1}*dUdx(:,i);\n            if i ~= j\n                Hess(j,i) = Hess(i,j);\n            end\n        end\n    end\n</code></pre>\n"},{"tags":["java","performance","profiling","cpu"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":43,"score":0,"question_id":12960167,"title":"Can CPU timing in Java VisualVM show total method durations?","body":"<p>I am not sure if Java VisualVM does not do this, or if I am not using the correct terminology, or seeing the menu option.</p>\n\n<p>Say one has done a CPU profile of an app with 3 methods.</p>\n\n<pre><code>time 0001: method A called\ntime 0002:   method B called\ntime 0004:   method B exits\ntime 0005:   method B called\ntime 0007:   method B exits\ntime 0008: method A exits\ntime 0009: method C called\ntime 0010:   method B called\ntime 0012:   method B exits\ntime 0013: method C exits\n</code></pre>\n\n<p>The only view I can get to shows me:</p>\n\n<pre><code>method B duration 6\nmethod A duration 2\nmethod C duration 2\n</code></pre>\n\n<p>But one should be able to flip the perspective (and does this have a standard name?) to see:</p>\n\n<pre><code>method A duration 7\nmethod B duration 6\nmethod C duration 4\n</code></pre>\n\n<p>How can I get this second view of the CPU timing out of Java VisualVM? Am I not seeing the option, or is it not there?</p>\n\n<p>I see this: <a href=\"http://stackoverflow.com/questions/1892038/total-method-time-in-java-visualvm\">Total method time in Java VisualVM</a></p>\n\n<p>But I do not see anything in the Call Stack that is from any of my objects. Would I have to be comparing two snapshots? This is incredibly non-obvious....</p>\n"},{"tags":["c#","wpf","performance","mediaelement"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":69,"score":0,"question_id":12957121,"title":"WPF - limitation for playing mediaelement at once","body":"<p>I have function for searching videos in folder. For every video file in that folder I am adding MediaElement and starts playing. When I have cca 10 videos it was all right. Then I added some videos, change view element from grid to canvas (because of performance) and now some mediaelement didn´t show (there is blank place where they should be). It is not one and same video. Mostly these happens to videos that are later procesing but not always. Does anyone know where could be problem? I think it is not problem with performance because the rest of videos are playing all right and application working fine. So is there some limitations? Or what I am doing wrong? Testing videos are .wmv and low quality (320x240).</p>\n\n<p>View:</p>\n\n<pre><code>&lt;Canvas x:Name=\"mainCanvas\"&gt;\n            &lt;ScrollViewer HorizontalScrollBarVisibility=\"Auto\" Width=\"1680\" Height=\"750\"&gt;\n                &lt;Canvas x:Name=\"videoCanvas\"&gt;\n                &lt;/Canvas&gt;\n            &lt;/ScrollViewer&gt;\n&lt;/Canvas&gt;\n</code></pre>\n\n<p>and code from MainWindow.xaml.cs</p>\n\n<pre><code> public MainWindow()\n        {\n            InitializeComponent();\n\n            this.WindowStyle = WindowStyle.None;\n            this.WindowState = WindowState.Maximized;\n\n            getAllVideosFromFolder(System.IO.Path.GetFullPath(@\"Videos\\\"));\n        }\n\n        private void getAllVideosFromFolder(string path)\n        {\n            try\n            {\n                var videoFiles = DirectoryHelper.GetFilesByExtensions(new DirectoryInfo(path), \".wmv\", \".mp4\", \".avi\", \".mov\");\n                int i = 0, j = 0;\n\n                foreach (var item in videoFiles)\n                {\n                    MediaElement melem = createMediaElementForPreview(item.FileName);\n\n                    Canvas.SetTop(melem, i * 250);\n                    Canvas.SetLeft(melem, j * 340);\n                    videoCanvas.Children.Add(melem);\n\n                    i++;\n                    if (i &gt; 2)\n                    {\n                        i = 0;\n                        j++;\n                    }\n\n                    videos.Add(videoClass);\n                }\n                videoCanvas.Width = j * 340;\n            }\n            catch (Exception ex)\n            {\n                MessageBox.Show(ex.Message);\n            }\n        }\n\n        private MediaElement createMediaElementForPreview(string sourcePath)\n        {\n            MediaElement melem = new MediaElement();\n            melem.LoadedBehavior = MediaState.Manual;\n            melem.Source = new Uri(sourcePath, UriKind.Relative);\n        melem.Width = 320;\n        melem.Height = 240;\n        melem.Volume = 0;\n        melem.Play();\n        melem.MouseDown += melem_MouseDown;\n        return melem;\n    }\n</code></pre>\n"},{"tags":["javascript","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":12957540,"title":"javascript optimization to avoid function context","body":"<p>I have the following code (trucated for clarity):</p>\n\n<pre><code>if ( typeof CPM == \"undefined\" || !CPM) {var CPM={};}\n\n(function(){\n  CPM.process = {\n    createStyles: function() {\n      //Do style processing\n      alert(\"processing styles\");\n    },\n    createClasses = {\n      //Do style processing\n      alert(\"processing classes\");      \n    },\n    init: function(){CPM.process.createStyles()}\n  };\n})();\n</code></pre>\n\n<p>init is a convention used in the company and must be there for other reasons. What I am trying to avoid is an additional function call for init. Is there an alternative way to write the same init and avoid creation of the function context? Like:</p>\n\n<pre><code>init: CPM.process.createStyles\n</code></pre>\n\n<p>The above fails, but am trying to see if there is any other way of achieving the optimization</p>\n"},{"tags":["performance","matlab","optimization","for-loop","vectorization"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":55,"score":-1,"question_id":12957703,"title":"Need to optimize this matlab code..vectorization will help or not?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/12895609/optimization-a-recurring-matlab-code\">Optimization a recurring matlab code</a>  </p>\n</blockquote>\n\n\n\n<p>Is vectorization a good option in optimizing this piece of code? What criteria decides, whether we vectorize a code or not? What else can be done?</p>\n\n<pre><code>function [oddNodes] = pointInPolygon (point,thePolygon)\n% determine if a point is in the polygon (faster than matlab \"inpolygon\"\n% command\n\n polyPoints=size(thePolygon,1);     %number of polygon points\n oddNodes = false;\n\nj=polyPoints;\nx=point(1); y=point(2);\n\nfor i=1:polyPoints\nif (thePolygon(i,2)&lt;y &amp;&amp; thePolygon(j,2)&gt;=y ||  thePolygon(j,2)&lt;y &amp;&amp; thePolygon(i,2)&gt;=y)\n    if (thePolygon(i,1)+(y-thePolygon(i,2))/(thePolygon(j,2)-thePolygon(i,2))*(thePolygon(j,1)-thePolygon(i,1))&lt;x)\n        oddNodes=~oddNodes;\n    end\nend\nj=i; \nend\n</code></pre>\n"},{"tags":["c#",".net","windows","performance","filesystems"],"answer_count":7,"favorite_count":7,"up_vote_count":8,"down_vote_count":0,"view_count":13860,"score":8,"question_id":1602578,"title":"C#: What is the fastest way to generate a unique filename?","body":"<p>I've seen several suggestions on naming files randomly, including using </p>\n\n<pre><code>System.IO.Path.GetRandomFileName()\n</code></pre>\n\n<p>or using a </p>\n\n<pre><code>System.Guid\n</code></pre>\n\n<p>and appending a file extension.  </p>\n\n<p>My question is:  <strong>What is the fastest way to generate a unique filename?</strong></p>\n"},{"tags":["database","performance","latency"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":1666,"score":1,"question_id":605648,"title":"Database Network Latency","body":"<p>I am currently working on an n-tier system and battling some database performance issues. \nOne area we have been investigating is the latency between the database server and the application server. In our test environment the\naverage ping times between the two boxes is in the region of 0.2ms however on the clients site its more in the region of 8.2 ms. Is that\nsomthing we should be worried about?</p>\n\n<p>For your average system what do you guys consider a resonable latency and how would you go about testing/measuring the latency?</p>\n\n<p>Karl</p>\n"},{"tags":["objective-c","ios","c","performance","zooming"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":12954741,"title":"Math Graph Zoom-In - coordinates and gridlines (for-loop get too slow)","body":"<p>maybe someone could help ?</p>\n\n<p>I develop a graphic calculator for iphone and have a problem\nwith the zoom In funktion.</p>\n\n<p>I have some coordinate system width gridlines and x-axis and y-axis.\ni can move the graph left, right, up, down.\nAnd i can zoom in and out.</p>\n\n<p>If i pan left and then i zoom in, the centre is in the middle and the gridlines go\nto left and right side. Also the Y-axis pan left out.</p>\n\n<p>I realized the gridlines with a for-loop, which starts from the y-axis and end to the bounds of the screen.</p>\n\n<p>The Problem is, if i zoom in very deep, the for-loop have to handle a lot of iterations and the performance get very slow.\nAbove all, if the y-axis is panned very left Side. So the iterations get very big.</p>\n\n<p>Hope someone understand my problem and can help.\nI try to post some code here:</p>\n\n<pre><code>- (void)drawRect:(CGRect)rect\n{\n\n    CGContextRef context = UIGraphicsGetCurrentContext();\n\n\n    // Pan Left and Right\n    x_Global = 160.0 + (lastTouch.x - FirstTouch.x);\n    y_Global = 210.0 + (lastTouch.y - FirstTouch.xy);\n\n    // Zoom\n    x_GlobalAfterZoom = 160.0 - ((160.0 - x_Global) * factor_x_Zoom);\n    y_GlobalAfterZoom = 160.0 - ((160.0 - y_Global) * factor_y_Zoom);\n\n\n    // draw axis\n    CGContextSetLineWidth(context, 0.5);\n    CGContextSetStrokeColorWithColor(context, [UIColor blackColor].CGColor);\n    CGContextMoveToPoint(context, x_MinusBounds, y_GlobalAfterZoom);\n    CGContextAddLineToPoint(context, x_PlusBounds, y_GlobalAfterZoom);\n    CGContextMoveToPoint(context, x_GlobalAfterZoom, y_MinusBounds);\n    CGContextAddLineToPoint(context, x_GlobalAfterZoom, y_PlusBounds);\n    CGContextSetAllowsAntialiasing(context, true);\n    CGContextStrokePath(context);\n\n\n    //draw Gidlines Positive\n    double gitterStepSize = gridLines Distance * gridlinesZoomFactor; //     gridlinesZoomFactor depends on factor_x_Zoom\n\n    for (x_Gridlines = x_GlobalAfterZoom; x_Gridlines &lt;= x_PlusBounds; x_Gridlines +=  GitterStepSize)\n    {\n        if (x_Gridlines &gt;= x_MinusBounds &amp;&amp; x_Gridlines &lt;= x_PlusBounds)\n        {\n              CGContextSetLineWidth(context, 0.15);\n              CGContextSetStrokeColorWithColor(context, [UIColor blackColor].CGColor);\n              CGContextMoveToPoint(context, x_Gridlines, y_MinusBounds);\n              CGContextAddLineToPoint(context, x_Gridlines, y_PlusBounds);\n              CGContextSetAllowsAntialiasing(context, true);\n              CGContextStrokePath(context);\n        }\n    }\n\n}\n</code></pre>\n"},{"tags":["ruby-on-rails","performance","windows-7","64bit","ubuntu-11.10"],"answer_count":3,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":416,"score":3,"question_id":9383308,"title":"Slow rails server inside VMware using RVM","body":"<p>I'm running Windows 7 with a Ubuntu 11.10 x86_64/desktop virtual machine (2gb ram) where I use rvm to manage my rails application (rvm is using 1.9.2-p290).  Myself and another co-worker are both experiencing brutally slow server responses - some of the simple pages are taking anywhere from 2 to 7 minutes to load, and I'm connecting from both inside the VM and on my host computer, so there isn't a network issue.\nI was running Ubuntu 11.10 32-bit server, and rails was running very well with no slow down on the same host.  Unfortunately for me, because of the work that I'm doing, I need to be using 64 bit Ubuntu, which is unusable slow.</p>\n\n<p>Does anyone have any suggestions</p>\n"},{"tags":["android","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":31,"score":0,"question_id":12957730,"title":"Canvas.drawBitmap extremely high cpu consumption","body":"<p>I have a simple Android application which consists of one View, that holds a Gallery widget.\nI load up 10 images to the Gallery object upon view creation.</p>\n\n<p>The FPS during the image transitions (when I swipe right for next image, left for previous) is <em>low</em> - it's just not smooth.</p>\n\n<p>I've used the Method profiling function built in eclipse to profile the application, and noticed that <code>Canvas.drawBitmap</code> consumes a lot of CPU.</p>\n\n<p>I'm currently targeting Android 2.3.3, so <code>hardwareacceleration</code> is irrelevant.</p>\n\n<p>Furthermore, downsampling the bitmaps did not change anything - even when downsampling by a large scale.</p>\n\n<p>Currently using Samsung Galaxy TAB</p>\n\n<p>Any insights on this?</p>\n"},{"tags":["java","performance","logging","slf4j"],"answer_count":6,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":269,"score":4,"question_id":8444266,"title":"Even with slf4j, should you guard your logging?","body":"<p>Help me out in a debate here.. :)</p>\n\n<p>The slf4j site here <a href=\"http://www.slf4j.org/faq.html#logging_performance\" rel=\"nofollow\">http://www.slf4j.org/faq.html#logging_performance</a> indicates that because of the parameterized logging, logging guards are not necessary.  I.e. instead of writing:</p>\n\n<pre><code>if(logger.isDebugEnabled()) {\n  logger.debug(\"Entry number: \" + i + \" is \" + String.valueOf(entry[i]));\n}\n</code></pre>\n\n<p>You can get away with:</p>\n\n<pre><code>Object entry = new SomeObject();\nlogger.debug(\"The entry is {}.\", entry);\n</code></pre>\n\n<p>Is this really okay, or does it incur the (albeit lower) cost of creating the static string that's passed to the trace method..?</p>\n"},{"tags":["java","performance","arraylist"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":75,"score":0,"question_id":12956503,"title":"Java merge arraylist is slow","body":"<p>So I have an application that I start on 2PC. Each instance launches an HSQLDB in server mode.</p>\n\n<p>I'm trying to get the sales of different products.</p>\n\n<p>So I query the local database and fill an arraylist with the name of the product and the number of sales.</p>\n\n<p>Then I execute the same query on the other database on the other PC.</p>\n\n<p>For 1 product, I so have two lines (each one correspond to one database).\nHere, results are false but the execution time is ok.</p>\n\n<p>In order to manage that, I did the following : </p>\n\n<pre><code>ResultSet rs2 = state2.executeQuery(produitsQuery);\nwhile (rs2.next()) {\n   for (int i = 0; i &lt; produits.size(); i++) {\n       obj = ((Object[]) produits.get(i));\n       idpdt = (Integer) obj[1];\n\n       if (idpdt == rs2.getInt(1)) {\n           nb = (Integer) obj[3];\n           valo = (Double) obj[4];\n\n           nb += rs2.getDouble(4);\n           valo += rs2.getDouble(5);\n           produits.set(i, new Object[]{\n               rs2.getString(\"famille\"),\n               rs2.getInt(\"id_pdt\"),\n               rs2.getString(\"nom_pdt\"),\n               nb,\n               valo,\n               s2.getString(\"sous_famille\")});\n           k = 1;\n       }\n    }\n    if (k == 0) \n        produits.add(new Object[]{\n            rs2.getString(\"famille\"),\n            rs2.getInt(\"id_pdt\"),\n            rs2.getString(\"nom_pdt\"),\n            rs2.getInt(\"nb\"),\n            rs2.getDouble(\"valo\"),\n            rs2.getString(\"sous_famille\")});\n\n}\n</code></pre>\n\n<p>Results are perfect but the execution time is very very slow and that's a problem. I think it's because I loop the entire arraylist at every row of the resultset.</p>\n\n<p>What others solutions might I use to make the execution time faster?</p>\n"},{"tags":["ios","performance","optimization","uiimage","nsurlconnection"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":36,"score":0,"question_id":12940176,"title":"iOS optimize image download speed from own server","body":"<p>I have a gallery and detailed view controllers in my application, when a user taps a thumbnail in the gallery I redirect them to the detailed controller and begin to asynchronously download original-sized image, which is 640x640 px and it takes like 5-6 seconds to download. Is there a way to optimize the image download speed?</p>\n\n<p>For example Instagram does this in around 0.2-1.0 second, like incredibly fast. My thinking is that they use some kickass compression on the server side and then unarchive the image they got in the Application. Are there any ways to do something like that?</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["multithreading","performance","d","thread-local-storage"],"answer_count":6,"favorite_count":7,"up_vote_count":16,"down_vote_count":0,"view_count":5119,"score":16,"question_id":506093,"title":"Why is thread local storage so slow?","body":"<p>I'm working on a custom mark-release style memory allocator for the D programming language that works by allocating from thread-local regions.  It seems that the thread local storage bottleneck is causing a huge (~50%) slowdown in allocating memory from these regions compared to an otherwise identical single threaded version of the code, even after designing my code to have only one TLS lookup per allocation/deallocation.  This is based on allocating/freeing memory a large number of times in a loop, and I'm trying to figure out if it's an artifact of my benchmarking method.  My understanding is that thread local storage should basically just involve accessing something through an extra layer of indirection, similar to accessing a variable via a pointer.  Is this incorrect?  How much overhead does thread-local storage typically have?</p>\n\n<p>Note:  Although I mention D, I'm also interested in general answers that aren't specific to D, since D's implementation of thread-local storage will likely improve if it is slower than the best implementations.</p>\n"},{"tags":["python","performance","bash","sed","logging"],"answer_count":4,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":370,"score":5,"question_id":9653507,"title":"trim big log file","body":"<p>i perform performance tests for a few java applications. Applications produce very big log files( it can be 7-10 GB) during the test . I need to trim these log files between specific dates and time. currently, i use python script, which parses log timestamps in datetime python object and print only matched strings. But this solution is very slow. 5 GB log is parsed about 25 minutes\nObviously entries in log file is sequentially and i don't need to read all file line by line. \nI thought about reading file from the start and from the end, until condition is matched and print files between matched number of lines. But i don't know how can i read file from the backwards, without downloading it to the memory.</p>\n\n<p>Please, can you suggest me any suitibale solution for this case.</p>\n\n<p>here is part of python script:</p>\n\n<pre><code>      lfmt = '%Y-%m-%d %H:%M:%S'\n      file = open(filename, 'rU')\n      normal_line = ''\n      for line in file:\n        if line[0] == '[':\n          ltimestamp = datetime.strptime(line[1:20], lfmt)\n\n          if ltimestamp &gt;= str and ltimestamp &lt;= end:\n            normal_line = 'True'\n        else:\n          normal_line = ''\n\n      if normal_line:\n        print line,\n</code></pre>\n"},{"tags":["performance","big-o","quicksort","worst-case"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":12946837,"title":"How to prove Quicksort is O(n*lg n) with special conditions?","body":"<p>This is one the questions on my Data Structures homework:</p>\n\n<p>Suppose that we have a linear-time procedure that is guaranteed to find a pivot element for Quicksort such that\nat least 1% of the array is less than or equal to the pivot and at least 1% is greater than or equal to the pivot.\nShow that Quicksort will then have worst-case complexity O(n lg n).</p>\n\n<p>I know that worst-case complexity for quicksort in general is O(n^2). I read that this happens when all the values of the pivot chosen is either the largest or smallest of the taken set.</p>\n\n<p>My guess is that, because of the given condition that at least 1% of the array be bigger and at least 1% of the array be smaller than the pivot, this eliminates the situation where the pivot is the smallest or largest element in the set. Thus it can never be O(n^2)</p>\n\n<p>Does this sound correct? </p>\n"},{"tags":[".net","performance","iis7","64bit"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":1343,"score":0,"question_id":6182667,"title":".Net Application running slow in IIS7 / Server 2008 64bit","body":"<p>We have a ASP .Net Application (built for 3.5 SP1) which when compiled is done so in Visual Studio 2008 using the \"All CPU's\" option.  It is currently hosted in a Windows 2003 (32bit) IIS6 environment (Virtual server) and connects to a SQL 2008 (64bit) Server. The current Application Server is running x2 Xeon  E5520 CPU's @2.27GHz with 4GM RAM.</p>\n\n<p>With this current setup, the application performs as well as it should.  Recently I have setup a new virtual server running Windows Server 2008 R2 (64bit) and IIS7 running on x2 Xeon E5530 CPU's running @ 2.4GHz with 6GB RAM.  I have setup our existing .Net application on this new server which is still connecting through to the same database server.</p>\n\n<p>Unfortunately though, for reasons beyond my understanding our application performs really poorly on this new server (which when looking at the specs should operate better than the old server)???  Pages seem to take twice as long to load (possibly taking longer to query the DB..?) etc..</p>\n\n<p>Could anyone provide any insight for me that might indicate why this might be?  Our networking guys profess that the new server is setup exactly the same as the old one, so I can't see it being an issue when the application server not being able to access the sql server on the same ports etc.. as the old 32bit server.. all very strange :s</p>\n\n<p>Cheers</p>\n\n<p>Greg</p>\n"},{"tags":["performance","computer-science","performance-testing","measurement","perf"],"answer_count":0,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":38,"score":1,"question_id":12930276,"title":"How to measure interference between processes","body":"<p>In parallel systems every process has an impact onto other processes, because they all compete for several scarce resources like cpu-caches, memory, disk I/O, network, etc.</p>\n\n<p>What method is best suited for measuring interference between processes? Such as Process A &amp; B each access the disk heavily. So running them parallel will probably slower then running sequential (individual runtime). Because the bottleneck is the hard drive.</p>\n\n<p>If I don't know exactly the behaviour of a process (disk-, memory- or cpu- intensive), what method would be best to analyse that?</p>\n\n<p>Measure individual runtime and compare the relative share of each parallel process?</p>\n\n<p>Like process A runs on average 30s alone, when 100% parallel with B 45s, when 20% parallel 35s.. etc ??</p>\n\n<p>Would it be better to compare several indicators like L1 &amp; LLC cache misses, page faults, etc.??</p>\n"},{"tags":["performance","r","mean"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":82,"score":1,"question_id":12759937,"title":"Why are `colMeans()` and `rowMeans()` functions faster than using the mean function with `lapply()`?","body":"<p>What I want to ask is, algorithmically, what do the <code>rowMeans()</code> and <code>colMeans()</code> functions do to optimize speed?</p>\n"},{"tags":["python","regex","performance","perl","text-processing"],"answer_count":6,"favorite_count":1,"up_vote_count":15,"down_vote_count":0,"view_count":468,"score":15,"question_id":12793562,"title":"text processing - python vs perl performance","body":"<p>Here is my perl and python script to do some simple text processing from about 21 log files each about 300KB to 1MB (max) x 5 times repeated (total of 125 files, due to the <em>log</em> repeated 5 times).</p>\n\n<p><strong>Python Code</strong> (code modified to use compiled re and using re.I)</p>\n\n<pre><code>#!/usr/bin/python\n\nimport re\nimport fileinput\n\nexists_re = re.compile(r'^(.*?) INFO.*Such a record already exists', re.I)\nlocation_re = re.compile(r'^AwbLocation (.*?) insert into', re.I)\n\nfor line in fileinput.input():\n    fn = fileinput.filename()\n    currline = line.rstrip()\n\n    mprev = exists_re.search(currline)\n\n    if(mprev):\n        xlogtime = mprev.group(1)\n\n    mcurr = location_re.search(currline)\n\n    if(mcurr):\n        print fn, xlogtime, mcurr.group(1)\n</code></pre>\n\n<p><strong>Perl Code</strong></p>\n\n<pre><code>#!/usr/bin/perl\n\nwhile (&lt;&gt;) {\n    chomp;\n\n    if (m/^(.*?) INFO.*Such a record already exists/i) {\n        $xlogtime = $1;\n    }\n\n    if (m/^AwbLocation (.*?) insert into/i) {\n        print \"$ARGV $xlogtime $1\\n\";\n    }\n\n}\n</code></pre>\n\n<p>And, on my PC both code generates exactly the same result file of 10,790 lines. And, here is the timing done on cygwin perl and python</p>\n\n<pre><code>User@UserHP /cygdrive/d/tmp/Clipboard\n# time /tmp/scripts/python/afs/process_file.py *log* *log* *log* *log* *log* &gt;\nsummarypy.log\n\nreal    0m8.185s\nuser    0m8.018s\nsys     0m0.092s\n\nUser@UserHP /cygdrive/d/tmp/Clipboard\n# time /tmp/scripts/python/afs/process_file.pl *log* *log* *log* *log* *log* &gt;\nsummarypl.log\n\nreal    0m1.481s\nuser    0m1.294s\nsys     0m0.124s\n</code></pre>\n\n<p>Originally, It took 10.2 secs using Python and only 1.9 secs using Perl for this simple text processing.</p>\n\n<p><strong>(UPDATE) but, after the compiled re version of python, it now takes 8.2 seconds in python and 1.5 seconds in perl. Still perl is much faster.</strong></p>\n\n<p>Is there anyway to improve the speed of Python at all OR it is obvious you expert guys that Perl will be the speedy one for simple text processing.</p>\n\n<p>By the way this was not the only test I did for simple text processing... And, each different way I make the source code, always always Perl wins by a large margin. And, not once did Python performed better for simple <code>m/regex/</code> match and print stuff.</p>\n\n<p>Thanks for your input.</p>\n\n<blockquote>\n  <p>Please do not suggest to use C, C++, Assembly, other flavours of\n  Python, etc.</p>\n  \n  <p>I am looking for a solution using Standard Python with its built-in\n  modules compared against Standard Perl (not even using the modules).\n  Boy, I wish to use Python for all my tasks due to its readability, but\n  to give up speed, I don't think so.</p>\n  \n  <p>So, please suggest how can the code be improved to have comparable\n  results with perl.</p>\n</blockquote>\n\n<p><strong>UPDATE: 18OCT2012</strong> </p>\n\n<p>As other users suggested, Perl has its place and Python has its.</p>\n\n<p>So, for this question, one can safely conclude that for simple regex match on each line for hundreds or thousands of text files and writing the results to a file (or printing to screen), <strong>Perl will always, always WIN in performance for this job, as simple as that.</strong></p>\n\n<p>Please note that when I say Perl wins in performance.. only standard Perl and Python is compared... not resorting to some obscure modules (obscure for a normal user like me) and also not calling C, C++, Assembly libraries from python or perl. We don't have time to learn all these extra steps and installation for a simple text matching job.</p>\n\n<p>So, Perl rocks for text processing and regex.</p>\n\n<p>Python has its place to rock in other places.</p>\n"},{"tags":["mysql","performance","query","order"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":55,"score":1,"question_id":12952903,"title":"MySQL ORDER BY - Slowing Down Query","body":"<p>Question pertains to a listing site that holds listing data and lead data in two tables.  When doing a query for listings and total number of leads for each listing, the results are extremely slow after adding an ORDER BY. Without the ORDER BY, the results are retrieved very fast.  Any advice or help with restructuring the query below would be awesome!! Fyi, there are 20k listings and 100k leads.</p>\n\n<pre><code>SELECT ls.*, IFNULL(ld.total_leads, 0) AS total_leads\nFROM listing ls \nLEFT JOIN (SELECT listing_id, COUNT(listing_id) AS total_leads \n            FROM lead GROUP BY listing_id) ld\nON (ls.listing_id = ld.listing_id)\nORDER BY ls.listing_id DESC LIMIT 0,20\n</code></pre>\n"},{"tags":["asp.net-mvc","asp.net-mvc-3","performance","mini-profiler"],"answer_count":3,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":82,"score":0,"question_id":11154966,"title":"Strange MVC3 performance issue","body":"<p>I have a strange issue that is occurring before my code appears to reach my controller.</p>\n\n<p>From using the SO Mini-MVC-Profiler, I've found that a certain request is taking 500ms (500ms!!!), and for what I know it is doing; it's excessively high!</p>\n\n<p><img src=\"http://i.stack.imgur.com/QUz2B.png\" alt=\"enter image description here\"></p>\n\n<p>We do use unity IoC to create our dependencies, and in the ase of the below EntityController being created, there is a AppServices dependently to be created, which does require some service classes to be instantiated, but I wouldn't expect this takes 500ms, it's simply constructors.</p>\n\n<p>Is there any way I could debug to find out specifically where time is being taken up?</p>\n\n<p>Thanks,</p>\n"},{"tags":["performance","64bit","32bit"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":55,"score":0,"question_id":12944500,"title":"32 bit/64 bit Compiling and Application Speed","body":"<p>I just got a new 64 bit computer and I am still trying to understand the differences between 32 bit and 64 bit. I understand that applications built using 64-bit dependencies can only run on a 64 bit, but applications built with 32-bit dependencies can run on both 32 and 64 bit systems. </p>\n\n<p>However, is there any other differences? I know some programs have two different windows versions you can download, one for 64 bit and one for 32 bit. Why do they provide the two different types? Is there a speed increase for compiling a program with 64 bit dependencies for a program to run on a 64 bit system?</p>\n"},{"tags":["django","performance","templates","render","nodes"],"answer_count":0,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":31,"score":1,"question_id":12952707,"title":"Is there a performance hit for Django tags using takes_context?","body":"<p>I sometimes access the context by supplying 'takes_context=True' for Django tags; Usually to acess the request.</p>\n\n<p>But are there performance implications.</p>\n\n<p>My mental model of how templates worke is that the tag function assembles the nodes representing the template, and that thereafter these nodes can render content without recreating the node, or reparsing the template.</p>\n\n<p>But surely, if the tag function can be made to return different nodes, depending on\nSomething in the context, then the nodes will have to be recreated everytime the context is different (I.E every time).</p>\n\n<p>Either that, or you are stuck with what nodes you get first time round, in which case you shouldn't return nodes based on anything in the context (in which case, what's the point?).</p>\n\n<p>Can someone clear this up for me? I' using Django 1.4.2.</p>\n"},{"tags":["mysql","datatable","table","performance"],"answer_count":3,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":2148,"score":3,"question_id":2887619,"title":"Mysql medium int vs. int performance","body":"<p>I have a simple users table, I guess the maximum users I am going to have is 300,000.</p>\n\n<p>Currently I am using:</p>\n\n<pre><code> CREATE TABLE users\n (\n         id INT UNSIGNED AUTOINCREMENT PRIMARY KEY,\n         ....\n</code></pre>\n\n<p>Of course I have many other tables for which users(id) is a FOREIGN KEY. </p>\n\n<p>I read that since the id is not going to use the full maximum of INT it is better to use:\nMEDIUMINT and it will give better performance.</p>\n\n<p>Is it true?</p>\n\n<p>(I am using mysql on Windows Server 2008)</p>\n"},{"tags":["performance","matlab","mex"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":109,"score":4,"question_id":12921705,"title":"Matlab: Does calling the same mex function repeatedly from a loop incur too much overhead?","body":"<p>I have some Matlab code which needs to be speeded up. Through profiling, I've identified a particular function as the culprit in slowing down the execution. This function is called hundreds of thousands of times within a loop. </p>\n\n<p>My first thought was to convert the function to mex (using Matlab Coder) to speed it up. However, common programming sense tells me the interface between Matlab and the mex code would lead to some overhead, which means calling this mex function thousands of times might not be a good idea. Is this correct? Or does Matlab do some magic when it's the same mex being called repeatedly to remove the overhead? </p>\n\n<p>If there <em>is</em> significant overhead, I'm thinking of restructuring the code so as to add the loop to the function itself and <em>then</em> creating a mex of that. Before doing that, I would like to validate my assumption to justify the time spent on this. </p>\n\n<p>Update: </p>\n\n<p>I tried @angainor's suggestion, and created donothing.m with the following code: </p>\n\n<pre><code>function nothing = donothing(dummy) %#codegen\nnothing = dummy;\nend\n</code></pre>\n\n<p>Then, I created a mex function from this as donothing_mex, and tried the following code: </p>\n\n<pre><code>tic;\nfor i=1:1000000\n    donothing_mex(5);\nend\ntoc;\n</code></pre>\n\n<p>The result was that a million calls to the function took about 9 seconds. This is not a significant overhead for our purposes, so for now I think I will convert the called function alone to mex. However, calling a function from a loop that executes about a million times does seem a pretty stupid idea in retrospect, considering this is performance critical code, so moving the loop to the mex function is still in the books, but with much lesser priority. </p>\n"},{"tags":["iphone","performance","optimization","opengl-es","rendering"],"answer_count":6,"favorite_count":8,"up_vote_count":8,"down_vote_count":0,"view_count":7948,"score":8,"question_id":2785640,"title":"optimizing iPhone OpenGL ES fill rate","body":"<p>I have an Open GL ES game on the iPhone. My framerate is pretty sucky, ~20fps. Using the Xcode OpenGL ES performance tool on an iPhone 3G, it shows:</p>\n\n<p>Renderer Utilization: 95% to 99%</p>\n\n<p>Tiler Utilization: ~27%</p>\n\n<p>I am drawing a lot of pretty large images with a  lot of blending. If I reduce the number of images drawn, framerates go from ~20 to ~40, though the performance tool results stay about the same (renderer still maxed). I think I'm being limited by the fill rate of the iPhone 3G, but I'm not sure.</p>\n\n<p>My questions are: How can I determine with more granularity where the bottleneck is? That is my biggest problem, I just don't know what is taking all the time. If it is fillrate, is there anything I do to improve it besides just drawing less?</p>\n\n<p>I am using texture atlases. I have tried to minimize image binds, though it isn't always possible (drawing order, not everything fits on one 1024x1024 texture, etc). Every frame I do 10 image binds. This seem pretty reasonable, but I could be mistaken.</p>\n\n<p>I'm using vertex arrays and glDrawArrays. I don't really have a lot of geometry. I can try to be more precise if needed. Each image is 2 triangles and I try to batch things were possible, though often (maybe half the time) images are drawn with individual glDrawArrays calls. Besides the images, I have ~60 triangles worth of geometry being rendered in ~6 glDrawArrays calls. I often glTranslate before calling glDrawArrays.</p>\n\n<p>Would it improve the framerate to switch to VBOs? I don't think it is a huge amount of geometry, but maybe it is faster for other reasons?</p>\n\n<p>Are there certain things to watch out for that could reduce performance? Eg, should I avoid glTranslate, glColor4g, etc?</p>\n\n<p>I'm using glScissor in a 3 places per frame. Each use consists of 2 glScissor calls, one to set it up, and one to reset it to what it was. I don't know if there is much of a performance impact here.</p>\n\n<p>If I used PVRTC would it be able to render faster? Currently all my images are GL_RGBA. I don't have memory issues.</p>\n\n<p>One of my fullscreen textures is 256x256. Would it be better to use 480x320 so the phone doesn't have to do any scaling? Are there any other general performance advice for texture sizes?</p>\n\n<p>Here is a rough idea of what I'm drawing, in this order:</p>\n\n<p>1) Switch to perspective matrix.\n2) Draw a full screen background image\n3) Draw a full screen image with translucency (this one has a scrolling texture).\n4) Draw a few sprites.\n5) Switch to ortho matrix.\n6) Draw a few sprites.\n7) Switch to perspective matrix.\n8) Draw sprites and some other textured geometry.\n9) Switch to ortho matrix.\n10) Draw a few sprites (eg, game HUD).</p>\n\n<p>Steps 1-6 draw a bunch of background stuff. 8 draws most of the game content. 10 draws the HUD.</p>\n\n<p>As you can see, there are many layers, some of them full screen and some of the sprites are pretty large (1/4 of the screen). The layers use translucency, so I have to draw them in back-to-front order. This is further complicated by needing to draw various layers in ortho and others in perspective.</p>\n\n<p>I will gladly provide additional information if reqested. Thanks in advance for any performance tips or general advice on my problem!</p>\n\n<p><strong>Edit:</strong></p>\n\n<p>I added some logging to see how many glDrawArrays calls I am doing, and with how much data. I do about 20 glDrawArray calls per frame. Usually about 1 to up to 6 of these has about 40 vertices each. The rest of the calls are usually just 2 vertices (one image). I'm just using glVertexPointer and glTexCoordPointer.</p>\n"},{"tags":["mysql","performance","hadoop"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":70,"score":1,"question_id":12935771,"title":"Which technology should I use to handle 1 million * 1 million calculation per 30 seconds","body":"<p>I am doing a Project. I have developed a GPS application where all the devices (Moving on the road) send their coordinates to the server in every 30 seconds. Now I have to calculate the distance between these devices so if any device comes in the range of another device then both the devices get a notification. </p>\n\n<p>I know how to calculate the distance between two coordinates (thanks to google) but I am not sure how to implement it because if we  have 1 million devices simultaneously sending data to the server then the server needs to execute distance calculation 1 million * (1 million -1) times in every 30 seconds. </p>\n\n<p>Please let me how to implement it. Do I need to use anything like hadoop or a mysql database procedure to do the job. </p>\n\n<p>Calculation is not a problem here but handling and calculating this much data is a problem.</p>\n\n<p>Regards,</p>\n"},{"tags":["c++","performance","qt","xml-parsing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":61,"score":0,"question_id":12948152,"title":"Fastest way of parsing XML files for specified Tags using the Qt Framework","body":"<p>As far es I know there a 4 ways of parsing XML files using C++ with Qt.</p>\n\n<pre><code>QDom\nQSax\nQXMLStreamReader\nQXMLQuery\n</code></pre>\n\n<p>I search in my file for a node with a specific attribute, if I've found it, I abort the parsing save the file name to a list and parse the next file. \nI accomplished that using QDom, but since i search up to 10k files with each about 400lines. it takes some time to parse them all.</p>\n\n<p>My question is whether anyone of you knows about the performance of this different approaches?\nOr if you have any tips to improve the performance of such a program?</p>\n\n<p>I appreciate any information!</p>\n"},{"tags":[".net","performance","timer","timing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":12931733,"title":"Is the timer resolution of the System.Diagnostics.Stopwatch class stable?","body":"<p>.Net has support for high resolution timing using the <code>System.Diagnostics.Stopwatch</code> class. I understand that the specific resolution that this class uses differs depending on the underlying hardware and can be obtained via the static property <code>Stopwatch.Frequency</code>.</p>\n\n<p>This frequency appears to be related to CPU frequency and <code>Stopwatch</code> reads this value and stores it in a static class variable within a static initializer/constructor. Hence I'm now wondering if this class will report incorrect timings if the CPU clock changes? e.g. in systems that alter the CPU clock depending system load.</p>\n"},{"tags":["android","performance","ice-cream-sandwich","ormlite"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":141,"score":0,"question_id":12909489,"title":"Ormlite performance on 4.0.3 (ICS)","body":"<p>I experienced a huge performance difference between android 2.3.4 and 4.0.3 on HTC Sensation. </p>\n\n<p>Some additional information:</p>\n\n<ul>\n<li>ormlite version 4.42</li>\n<li>to getting dao I use DaoManager and a dao singleton.</li>\n<li>using batch task to insert</li>\n<li>I'm trying createorupdate 30 objects (only creating takes the same time)</li>\n<li>These are single objects (without relations), but have long string fields.</li>\n</ul>\n\n<p>Time logs:</p>\n\n<p>ICS (4.0.3)</p>\n\n<pre><code>10-16 09:17:06.206: 1 getting dao\n10-16 09:17:06.206: 2 got dao\n10-16 09:17:06.206: 2 start call batch task\n10-16 09:17:06.216: 3 start initializing batch_task\n10-16 09:17:06.326: 121 finished initializing batchtask  \n10-16 09:17:06.836: 623 end batch task\n</code></pre>\n\n<p>2.3.4</p>\n\n<pre><code>10-16 09:20:00.355: 0 getting dao\n10-16 09:20:00.355: 1 got dao\n10-16 09:20:00.355: 1 start call batch task\n10-16 09:20:00.355: 1 start initializing batch_task\n10-16 09:20:00.435: 87 finished initializing batchtask  \n10-16 09:20:00.445: 96 end batch task \n</code></pre>\n\n<p>As you can see on ICS takes creating much more time.</p>\n\n<p>What should I do to get the similar performance on ICS?</p>\n"},{"tags":["php","performance","profiling","production-environment"],"answer_count":2,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":57,"score":2,"question_id":12364622,"title":"PHP: How to run sampling profiler in production?","body":"<p><strong>Production environment:</strong> A load balancer / HTTP reverse proxy in front of cluster of worker machines running Apache 2.2 with <code>mod_php</code> 5.3 on 64 bit Linux. All worker machines are running identical fully custom PHP code and speak to single backend PostgreSQL database. The PHP code is optimized to spend CPU over talking to the database. The database machine has been verified to still have lots of idle.</p>\n\n<p><strong>What I'm looking for:</strong> sampling profiler that can attach to PHP process by PID and periodically stop the process (e.g. with <code>SIGSTOP</code>), collect PHP stack via memory inspection and the continue the process (e.g. with <code>SIGCONT</code>). The stopping period should be adjustable but I think stopping interval should be around 1-10 ms.</p>\n\n<p>A single worker machine PHP process is expected to run a single request always in less than 100 ms. I'm mostly interested collecting profile data for those processes that take more than 100 ms. The best case scenario would be a sampling profiler that would be notified at the start of the request and if the PHP process handling the request is still running 100 ms later, start collecting samples at 1 ms intervals. This way any normally running process would be run to the end without interrupts and I would still get profiles for problematic cases.</p>\n\n<p><strong>Does this kind of sampling profiler for PHP exist?</strong> The intent is to not use instrumenting profiler because the overhead is too high and the instrumentation messes the statistics (been there, done that).</p>\n\n<p>I'm already aware of XHProf and Xdebug but as far as I know, both are instrumenting profilers and affect the actual opcodes of PHP program. I'd highly prefer something that runs the normal PHP opcodes instead.</p>\n\n<p>The closest I know would work is to run PHP code with HipHop and use sampling profiler for C/C++ code but I'm not yet ready to port the software to HipHop. And in that case, the profiling result would be representative only for HipHop, not for mod_php.</p>\n"},{"tags":["performance","xna","2d","sprite"],"answer_count":4,"favorite_count":3,"up_vote_count":1,"down_vote_count":0,"view_count":3286,"score":1,"question_id":1637086,"title":"XNA - Merge sprites for better drawing performance?","body":"<p>I read somewhere that when rendering a lot of 3D objects one could merge these to form a giant mesh so that only one draw call would be made. Therefore letting the, quote: \"GPU do its magic\", while the <em>CPU is free for other calls</em> than draw.</p>\n\n<p>So to my question, would this be feasible to do in a <strong>2D environment</strong> with <strong>performance</strong> in mind?</p>\n\n<p>For example, say we have a simple tile-system and instead of making a draw call for each tile in view, one would instead <em>merge all tiles</em> to form a large sprite and then call a draw on it.</p>\n\n<p>Any insight into the matter - either tips, links or whatnot - is greatly appreciated since I have no prior experience in graphics performance.</p>\n\n<p><strong>Edit:</strong> Sorry for my poor explanation. I am creating a tileengine for personal use and want it to be as versatile as possible. Therefore I want to optimize just in case I have to draw lots of tiles in the near future.</p>\n\n<p>I <em>do</em> use a tilesheet but what I meant with the question is if merging all tiles that are to be drawn from the sheet into a new Texture2D will gain performance. For example:</p>\n\n<p>We have 128x72 tiles to draw on the screen. Instead of looping through all tiles and calling draw for each tile to be drawn, we merge all tiles into a new sprite 1280x720 in size and draws it. This way the draw() method will only be called <em>once per frame</em>. My <em>question</em> was if this will improve performance as it would merging 3d objects into a single mesh when doing 3D.</p>\n\n<p>Because the info I have gathered is that calling draw() is a performance hog and should be called as little as possible. Anyone to confirm or deny? :)</p>\n"},{"tags":["xcode","multithreading","performance","instruments","xnu"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":31,"score":1,"question_id":12709995,"title":"XNU Thread States color-coded in Xcode/Instruments","body":"<p>I am doing some analysis of a multi-threaded application using the Apple Instruments tools, which give a lot of information I'm trying to make sense out of. I am trying to find a good resource to describe the thread states which are color-coded in the tool. I've been looking into XNU Kernel documentation and books but without much luck.</p>\n\n<p>There is a lot of yellow and purple which correspond to the \"preempted\" and \"supervisor\" modes (the full color chart is described in the upper right pop up in the attached image). Given I'm spending so much time in these states as opposed to the \"running\" state (in blue), I would be particularly interested in knowing what they refer to and whether it is possible/desirable to minimise the time spent in these states.</p>\n\n<p><img src=\"http://i.stack.imgur.com/I1auN.png\" alt=\"enter image description here\"></p>\n"},{"tags":["android","performance","background","onpause"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":63,"score":0,"question_id":12944603,"title":"Android - Resume application from background programmatically","body":"<p>i need to resume my app as soon as the application goes in background.</p>\n\n<p>I don't like my solution cause has a few bugs and is not too performant, this is what i try:</p>\n\n<pre><code>    @Override\nprotected void onPause() {\n       super.onPause();\n       Intent intent = new Intent(this, MainActivity.class);\n       intent.setFlags(Intent.FLAG_ACTIVITY_NEW_TASK | Intent.FLAG_ACTIVITY_CLEAR_TOP);\n       intent.setAction(Intent.ACTION_MAIN);\n       startActivity(intent);\n}\n</code></pre>\n\n<p>This is a single Activity application which load a webview with many dynamic and flash content  while update his state every few seconds and every few second get information from web.\nSo is not to simple and fast every time it goes in background to recreate the activity.\nTo relaunch application needs about 3-4 seconds, too much for me. If users between that seconds clicks the settings icon in the home, application doesn't start again. I don't know why and i'm writing here after a lot of googling :) Help me please!</p>\n"},{"tags":["php","mysql","performance","insert"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":12944132,"title":"PHP/MySQL: What is a reasonable execution time for inserts?","body":"<p><strong>Same result with PDO and standard MySQL.</strong></p>\n\n<p>I understand if you're going to do mass inserts, it's better to do it in bulk. I know that doing it individually is a terrible idea. I don't even have a need to do mass-inserts so I'm just wondering if the results I'm getting are to be expected.</p>\n\n<p><strong>Standard (deprecated) mysql code (I'm using PDO in my site):</strong></p>\n\n<p><strong>493.9</strong> seconds for <strong>10,000</strong> inserts.</p>\n\n<pre><code>$c = mysql_connect ( 'localhost', 'root', 'password', true ) or die ( mysql_error () );\nmysql_select_db ( 'testdb', $c );\nset_time_limit ( 600 );\n$iterations = 10000;\n$store = 0;\n$amount = 0;\n$startTime = microtime ( true );\n$endTime = 0.00;\n\nfor ( $i = 1;   $i &lt;= $iterations;  $i ++ ) :\n    $store = rand ( 1, 2 );\n    $amount = rand ( -5000, 5000 );\n\n    mysql_query ( \"INSERT INTO checkbook (employee_id,store_id,amount,balance) VALUES\n    (6,$store,$amount,0.00)\", $c ) or die ( mysql_error () );\n\nendfor;\n\n$endTime = microtime ( true ) - $startTime;\n\necho '&lt;p&gt;&lt;strong&gt;', $endTime, '&lt;/strong&gt;&lt;/p&gt;';\n</code></pre>\n"},{"tags":["c#","wpf","performance","richtextbox"],"answer_count":1,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":1360,"score":4,"question_id":6418643,"title":"Storing and displaying rich text efficiently","body":"<p>I need to store significant amounts of rich text in a SQL database, retrieve it and display it.</p>\n\n<p>One font throughout is OK but I need different font sizes/bold/colors.</p>\n\n<p>For now I am using a RichTextBox (WPF) to display it, and XamlWriter.Save/XamlReader.Parse to serialize it to strings to store in the DB. It works well but the RichTextBox is so HUGELY SLOW at displaying the text that it's basically unusable.</p>\n\n<p>Is there a quick way to do this with acceptable performance?</p>\n\n<p>I'm considering doing it with GlyphRun objects, drawing each character as a bitmap and computing all the alignment requirements to fit the destination image etc... But reinventing the wheel on simple colored/sizable text seems really strange in 2011.</p>\n\n<p><strong>EDIT</strong>:\nThanks for the answers, didn't see them until now, sorry.</p>\n\n<p>Text is entered by the user from <code>RichTextBox</code>es as well, basically I just save the resulting <code>string</code> <code>XamlWriter.Save(richTextBox.Document)</code> in the database. Other fields (double/int etc) are also entered by the user from <code>TextBox</code>es.</p>\n\n<p>As the user queries the database, pages of read-only rich text with colors and formatting is generated from scratch using the fields in the database, including the saved rich text fields above: these are converted from <code>FlowDocument</code>s to <code>Span</code>s and some replacement is done on them (<code>InlineUIContainer</code>s which host a class derived from <code>UIElement</code> which references a database entry, inlined in the text, like \"see [thisbook]\" where [thisbook] references some database entry's ID). MSDN says all that is far too much text for a <code>TextBlock</code>.</p>\n\n<p>That text rendering is the really slow part but there is no way around it, I need that formatting and it's just how the WPF <code>RichTextBox</code>es are: even when entering a little simple text in the <code>RichTextBox</code>es, there is a delay between typing and the character appearing on the screen...</p>\n\n<p>For now I still use <code>RichTextBox</code>es but I keep lots of rendered layouts in memory (the <code>Paragraph</code>/<code>Section</code>/<code>Span</code> objects) and I am careful to rerender only the least amount of formatted text possible when changes/queries are made or different views of the database data are requested by the user.</p>\n\n<p>It's still not fast but it's OK, changing the whole structure (AvalonEdit or <code>FormattedText</code> or <code>GlyphRun</code>) doesn't seem worth it right now, too much work, the whole serialization API with <code>XamlWriter.Save</code> and <code>XamlReader.Parse</code> simplifies much (for <code>FormattedText</code> and <code>GlyphRun</code>, I'd have to come up with a file format myself to save the formatted text to the database).</p>\n\n<p>There is also the possibility of using the OpenXML SDK to create Microsoft Word .docx documents but google says rendering performance isn't great either, and I don't know if embedding an <code>UIElement</code> in the text within an <code>InlineUIContainer</code> and serializing that to be saved in the database would be possible (same problem with AvalonEdit).</p>\n"},{"tags":[".net","sql","sql-server","performance","normalization"],"answer_count":5,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":50,"score":2,"question_id":12943776,"title":"more performant to have normalized or denormalized tables","body":"<p>I am currently developing an mvc application to read from an existing sql server database. The database is denormalized - and I was looking at modifying some tables to normalize it to a degree.</p>\n\n<p>This led to a discussion with a fellow developer as the most preformant way to read the data, or if the structure should change or not. The data will be read via ado.net with a stored procedure. Question I have is, is it more performant to have numerous fields in a table (denormalized) OR have several tables with inner joins (normalized) to retrieve the data?</p>\n\n<p>I should have mentioned, the actions on the tables will be 95% read, 5% write.</p>\n"},{"tags":["c","performance","fortran"],"answer_count":17,"favorite_count":29,"up_vote_count":90,"down_vote_count":3,"view_count":28481,"score":87,"question_id":146159,"title":"Is Fortran faster than C?","body":"<p>From time to time I read that Fortran is or can be faster then C for heavy calculations. Is that really true? I must admit that I hardly know Fortran, but the Fortran code I have seen so far did not show that the language has features that C doesn't have.</p>\n\n<p>If it is true, please tell me why. Please don't tell me what languages or libs are good for number crunching, I don't intend to write an app or lib to do that, I'm just curious.</p>\n"},{"tags":["sql-server","performance","blob","insert-into","poeaa"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":30,"score":1,"question_id":12938247,"title":"Should I Replace Multiple Float Columns with a BLOB?","body":"<p>How would a single BLOB column in SQL Server compare (performance wise), to ~20 REAL columns (20 x 32-bit floats)?</p>\n\n<p>I remember Martin Fowler recommending using BLOBs for persisting large object graphs (in Patterns of Enterprise Application Architecture) to remove multiple joins in queries, but does it make sense to do something like this for a table with 20 fixed columns (which are never used in queries)?</p>\n\n<p>This table is updated really often, around 100 times per second, and <code>INSERT</code> statements get rather large with all the columns specified in the query.</p>\n\n<p>I presume the first answer is going to be \"profile it yourself\", but I'd like to know if someone already has experience with this stuff.</p>\n"},{"tags":["asp.net","html","performance","image","optimization"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":25,"score":0,"question_id":12941712,"title":"Find Images in the Website Which are used Never","body":"<p>How can I detect image that is never used in the website, to improve loading speed of the whole content?.. Thanks in advance...</p>\n"},{"tags":["performance","r","matrix","sparse-matrix"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":38,"score":1,"question_id":12937043,"title":"Efficiency problems building a \"signature matrix\" using data from two (sparse) matrices in R","body":"<p>I don't know if the \"signature matrix\" I am trying to build has a proper pre-existing name or definition in any fields, but the following code appears to generate the correct result on some toy matrices. I have trouble explaining what exactly I am trying to do without causing confusion, but if the code I have provided isn't sufficient to deduce what I am trying to do, I'd be happy to give it a shot.</p>\n\n<p>When I run this code with my actual data (two integer matrices that are both approximately 300 by 20,000 elements in size) it appears to be working, but after hours and hours it still doesn't finish.</p>\n\n<p>I understand that the iteration might be the biggest problem here, but I haven't been able to work out how to remove it.</p>\n\n<p>The code:</p>\n\n<pre><code># Load required library\nlibrary(Matrix)\n\n# Load in the test data\nmut &lt;- matrix(data=c(1,1,1,0,0,0,0,1,0,1,1,0,0,1,1,0,0,0,1,0),\n              nrow=5,ncol=4,\n              dimnames=list(c(\"p1\",\"p2\",\"p3\",\"p4\",\"p5\"),c(\"GA\",\"GB\",\"GC\",\"GD\")))\n\noute &lt;- matrix(data=c(1,1,0,1,0,1,0,0,1,1,1,1,1,0,0,1,1,0,0,1),\n              nrow=5,ncol=4,\n              dimnames=list(c(\"p1\",\"p2\",\"p3\",\"p4\",\"p5\"),c(\"GQ\",\"GW\",\"GE\",\"GR\")))\n\npatOutMatrix &lt;- Matrix(data=oute,sparse=TRUE)\npatMutMatrix &lt;- Matrix(data=mut,sparse=TRUE)\n\ntransposePatMutMatrix &lt;- t(patMutMatrix)\n\n# Build the empty matrix (with row and col names)\nsigMatrix &lt;- Matrix(0,nrow=ncol(patMutMatrix), ncol=ncol(patOutMatrix),sparse=TRUE)\nrownames(sigMatrix) &lt;- colnames(patMutMatrix)\ncolnames(sigMatrix) &lt;- colnames(patOutMatrix)\n\n# Populate sigMatrix\nfor (mgene in rownames(transposePatMutMatrix))\n{\n  a &lt;- patOutMatrix[which(transposePatMutMatrix[mgene, ] == 1, arr.ind = T), ]\n\n  # Using an IF here to get around a problem with colSums() not working on single rows\n  sigMatrix[mgene,] &lt;- if (dim(as.matrix(a))[2] == 1) {\n    a\n  } else {\n    colSums(patOutMatrix[which(transposePatMutMatrix[mgene, ] == 1, arr.ind = T), ])\n  }\n}\n</code></pre>\n\n<p>Does anyone know how I could change anything here to make this perform faster?</p>\n"},{"tags":["java","performance","time","classloader"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":38,"score":1,"question_id":12368559,"title":"Measure how much time the ClassLoader takes to load class or jar","body":"<p>How to know how much time a specific .class or .jar takes to be loaded by the ClassLoader?</p>\n\n<p>The JVM option <code>-verbose:class</code> allows to know in which order the ClassLoader loads .class and jar. But it doesn't tell me if a specific .jar take a lot of time or not to be loaded.</p>\n"},{"tags":["iphone","performance","camera","iso","shutter"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":2226,"score":0,"question_id":10347515,"title":"It is possible to change the iso or shutter speed fot iphone camera?","body":"<p>I am tring to control the iso and the shutter speed for the iphone, but it is no API in AVFoundation. It can only change the exposure, wb for iphone. \nHow can i control the iso or shutter speed?</p>\n"},{"tags":["javascript","jquery","performance","jquery-ui"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":34,"score":0,"question_id":12939538,"title":"Is it a good idea to have two versions of jquery-ui library : One with core and other with all components?","body":"<p>We are trying to adapt to using widgets from jquery-ui for most of our functionality. It gives lot of OOP like features and ease of extension. The downside is the jquery-ui library comes with other baggage and in-built widgets that we don't need in some pages. No doubt our project uses  widgets from jquery-ui but in some cases we just need Core and Widget components to write our custom widgets. So my question : Is it worthwhile to have two versions of jquery-ui library one just with Core and other will Core + components we use across the project ? The idea is to just use a lighter jquery-ui-core in pages that have our custom widgets.</p>\n"},{"tags":["performance","apache","caching","http-headers","mod-expires"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":36,"score":0,"question_id":12939273,"title":"Changes to Expires Headers not respected by Pagespeed and YSlow","body":"<p>I added the following code to an htaccess file:</p>\n\n<pre><code>&lt;IfModule mod_expires.c&gt;\n\n# Enable expirations\nExpiresActive On\n\n# Default directive\nExpiresDefault \"access plus 1 month\"\n\n# Images\nExpiresByType image/gif \"access plus 1 month\"\nExpiresByType image/png \"access plus 1 month\"\nExpiresByType image/jpg \"access plus 1 month\"\n\n# CSS\nExpiresByType text/css \"access 1 month”\n\n# Javascript\nExpiresByType application/javascript \"access plus 1 year\"\n\n&lt;/IfModule&gt;\n</code></pre>\n\n<p>but these changes are not reflected by Google's PageSpeed and the YSlow Addon for Chrome. </p>\n\n<p>Based on the above code, can someone explain why I continue to receive an F grade for expires headers for PageSpeed and YSlow? More importantly, why does Google's PageSpeed indicate that the defined filetypes expire in 4 hours instead of the 1 month as defined in the htaccess file?</p>\n\n<p>Here are my response headers:</p>\n\n<pre><code>Date: Wed, 17 Oct 2012 15:29:36 GMT\nContent-Type: text/html\nServer: Nginx / Varnish\nX-Powered-By: PHP/5.2.17\nCache-Control: max-age=2592000\nExpires: Fri, 16 Nov 2012 15:29:36 GMT\nVary: Accept-Encoding\nContent-Encoding: gzip\nAge: 0\n\n200 OK\n</code></pre>\n"},{"tags":["performance","magento-admin","slowdown","slow-load"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":85,"score":0,"question_id":12763020,"title":"Magento admin backend is very slow even with fresh installation","body":"<p>The Magento admin backend is very slow with fresh installation of the same version 1.7.0.2.</p>\n\n<p>With my previous installation, it was working perfectly fine with reasonably good speed on the same server with same hosting company and no additional tweaks at all.</p>\n\n<p>But suddenly I messed up with it due to another custom theme that I installed. So I reinstalled it after removing it. Then I found more problems in even accessing it.</p>\n\n<p>Hence, I created new <code>public_html</code> folder and rename the previous one to <code>public_html.old</code>.</p>\n\n<p>Then I was able to reinstall the Magento successfully on the root folder. But this time it is opening very very slow, in fact, every step is slow.</p>\n\n<p>Can anybody help me to trace the actual reason. What could be the possibilities. It was working fine earlier but why not this time.</p>\n"},{"tags":["ruby-on-rails","performance","haml","benchmarking","slim-lang"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":256,"score":1,"question_id":12088818,"title":"How much faster is SLIM compared to HAML?","body":"<p>I'm trying to convince my team members to use SLIM instead of HAML because I like SLIM's syntax a lot more. I was promised that we would change to SLIM if it was really a lot faster than HAML as I read <a href=\"https://github.com/stonean/slim#benchmarks\" rel=\"nofollow\">here</a> and <a href=\"https://gist.github.com/626215\" rel=\"nofollow\">here</a>, but I don't know how recent these benchmarks are, so I wanted to know whether anybody has some real life experience with this topic?</p>\n\n<p>I am also not really sure which of the benchmarks (compiled, tilt compiled, cached, uncached) does mean the most, as I don't know what exactly will be used when a Rails application is in production mode (where the speed is the most needed).</p>\n\n<p>Thanks a lot for more information about this topic!</p>\n"},{"tags":["javascript","performance","firefox","dom-manipulation"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":66,"score":1,"question_id":12937905,"title":"Most efficient way to add remove classes with JavaScript","body":"<p>I am curious if anyone knows which of these is more efficient, I am only concerned with Firefox as a browser and do not need to know that this code doesn't work in IE, etc...</p>\n\n<p>Basically I am showing and hiding DOM elements based on an input field's value, an instant search if you will. I need to show or hide a \"nothing found\" element if no search results are shown. I am curious if it is cheaper (more efficient) to check if the \"nothing found\" element is in the proper state before modifying its class attribute, or to just modify the class attribute. </p>\n\n<p>Question: should I remove/add the hidden class every time the function is run, even if there is no change in the element's class attribute? </p>\n\n<pre><code>if (shown_count &gt; 0) {\n    element.classList.add('hidden');\n}\nelse {\n    element.classList.remove('hidden');\n}\n</code></pre>\n\n<p>or should I check to see if the element needs its class attribute updated before actually updating it?</p>\n\n<pre><code>if (shown_count &gt; 0) {\n    if (element.classList.contains('hidden') == false) {\n         element.classList.add('hidden');\n    }\n}\nelse {\n    if (element.classList.contains('hidden')) {\n         element.classList.remove('hidden');\n    }\n}\n</code></pre>\n"},{"tags":["java","performance","jvm","garbage-collection"],"answer_count":6,"favorite_count":3,"up_vote_count":0,"down_vote_count":0,"view_count":3774,"score":0,"question_id":2743106,"title":"Tuning JVM (GC) for high responsive server application","body":"<p>I am running an application server on Linux 64bit with 8 core CPUs and 6 GB memory.</p>\n\n<p>The server must be highly responsive.</p>\n\n<p>After some inspection I found that the application running on the server creates rather a huge amount of short-lived objects, and has only about 200~400 MB long-lived objects(as long as there is no memory leak)</p>\n\n<p>After reading <a href=\"http://java.sun.com/javase/technologies/hotspot/gc/gc_tuning_6.html\" rel=\"nofollow\">http://java.sun.com/javase/technologies/hotspot/gc/gc_tuning_6.html</a>\nI use these JVM options</p>\n\n<pre><code>-server -Xms2g -Xmx2g -XX:MaxPermSize=256m -XX:NewRatio=1 -XX:+UseConcMarkSweepGC\n</code></pre>\n\n<p>Result: the minor GC takes 0.01 ~ 0.02 sec, the major GC takes 1 ~ 3 sec\nthe minor GC happens constantly. </p>\n\n<p>How can I further improve or tune the JVM?</p>\n\n<p>larger heap size? but will it take more time for GC?</p>\n\n<p>larger NewSize and MaxNewSize (for young generation)?</p>\n\n<p>other collector? parallel GC? </p>\n\n<p>is it a good idea to let major GC take place more often? and how?</p>\n"},{"tags":["c++","iphone","objective-c","performance","optimization"],"answer_count":7,"favorite_count":30,"up_vote_count":51,"down_vote_count":0,"view_count":35204,"score":51,"question_id":926728,"title":"Will my iPhone app take a performance hit if I use Objective-C for low level code?","body":"<p>When programming a CPU intensive or GPU intensive application on the iPhone or other portable hardware, you have to make wise algorithmic decisions to make your code fast.</p>\n\n<p>But even great algorithm choices can be slow if the language you're using performs more poorly than another.</p>\n\n<p>Is there any hard data comparing Objective-C to C++, specifically on the iPhone but maybe just on the Mac desktop, for performance of various similar language aspects?  I am very familiar with <a href=\"http://stackoverflow.com/questions/826281/better-performance-with-libxml2-or-nsxmlparser-on-the-iphone\">this article comparing C and Objective-C</a>, but this is a larger question of comparing two object oriented languages to each other.</p>\n\n<p>For example, is a C++ vtable lookup really faster than an Obj-C message?  How much faster?  Threading, polymorphism, sorting, etc.  Before I go on a quest to build a project with duplicate object models and various test code, I want to know if anybody has already done this and what the results where.  This type of testing and comparison is a project in and of itself and can take a considerable amount of time.  Maybe this isn't one project, but two and only the outputs can be compared.</p>\n\n<p>I'm looking for <strong>hard data</strong>, not evangelism.  Like many of you I love and hate both languages for various reasons.  Furthermore, if there is someone out there actively pursuing this same thing I'd be interesting in pitching in some code to see the end results, and I'm sure others would help out too.  My guess is that they both have strengths and weaknesses, my goal is to find out precisely what they are so that they can be avoided/exploited in real-world scenarios.</p>\n"},{"tags":["performance","cdn","browser-cache"],"answer_count":5,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":90,"score":3,"question_id":12916430,"title":"Is better use Cache or CDN?","body":"<p>I was studying about the browser performance when loading static files and this doubt has come.</p>\n\n<blockquote>\n  <p>Some people say that use CDN static files (i.e. Google Code, jQuery\n  latest, AJAX CDN,...) is better for performance, because it requests\n  from another domain than the whole web page.</p>\n  \n  <p>Other manner to improve the performance is to set the <code>Expires</code> header\n  equal to some months later, forcing the browser to cache the static\n  files and cutting down the requests.</p>\n  \n  <p>I'm wondering which manner is the best, thinking about performance and\n  if I may combine both.</p>\n</blockquote>\n\n<p>Thank you in advance guys, I love StackOverflow.</p>\n"},{"tags":["javascript","jquery","performance","jstree"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":119,"score":1,"question_id":12692741,"title":"jstree performance issues","body":"<p>i am using a jstree with around 1500 nodes, nested a max of 4 levels deep (most are only 1 level deep), and i'm getting internet explorer's \"this script is running slowly\" error.  i began with just a straight html_data &lt;li&gt; structure, generated by ASP.NET.  The tree wouldn't finish loading at all.  then i tried xml_data and json_data, which was a little better but eventually errored out.  my last-stitch effort was async loading.  clearly, this fixed the initial load problem, but now i get IE's error when i expand one of the larger branches.</p>\n\n<p>some more details:  i'm using the checkbox plugin, and i will also need the ability to search.  unfortunately, when searching, the user could potentially enter as little as one character so i'm looking at some large search results.</p>\n\n<p>has anybody done something similar with such a large dataset?  any suggestions on speeding up jstree?  or, am i better off exploring other options for my gui?</p>\n\n<p>i realize i haven't posted any code, but any general techniques/gotcha's are welcome.</p>\n\n<p>thanks,</p>\n\n<p>mike</p>\n"},{"tags":["java","performance","eclipse-plugin","eclipse-rcp"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":35,"score":0,"question_id":12936587,"title":"Performance of TreeViewer.expandAll method","body":"<p>In my application, an action is to expand all elements of a certain tree.</p>\n\n<p>each time I execute this action, it takes longer time than before.</p>\n\n<pre><code>long startTime = System.currentTimeMillis();\ngetTreeViewer().expandAll();\nlong endTime = System.currentTimeMillis()-startTime;\nSystem.out.println(\"expandAll time :  \" + endTime );\n</code></pre>\n\n<p>and this is a sample from the output.</p>\n\n<p>expandAll time : 200\nexpandAll time : 800\nexpandAll time : 1800\nexpandAll time : 3200</p>\n\n<p>-- This slowdown my plugin so much. Is this an issue with the method ?</p>\n"},{"tags":["mysql","performance"],"answer_count":4,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":3297,"score":3,"question_id":5060366,"title":"MySQL: Fastest way to count number of rows","body":"<p>Which way to count a number of rows should be faster in MySQL?</p>\n\n<p>This:</p>\n\n<pre><code>SELECT COUNT(*) FROM ... WHERE ...\n</code></pre>\n\n<p>Or, the alternative:</p>\n\n<pre><code>SELECT 1 FROM ... WHERE ...\n\n// and then count the results with a built-in function, e.g. in PHP mysql_num_rows()\n</code></pre>\n\n<p>One would think that the first method should be faster, as this is clearly database territory and the database engine should be faster than anybody else when determining things like this internally.</p>\n"},{"tags":["java","json","performance","struts","tomcat7"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":119,"score":2,"question_id":12728860,"title":"Java Performance Issue - Tomcat WebappClassLoader locked","body":"<p>I have been facing this strange issue from past few days while doing the performance test of my product, i am using java 6,struts 3 framework and tomcat 7 server</p>\n\n<p>During performance test we start a load of thousands of UI requests hitting the server initialy it runs fine but after couple of hours the requests start getting blocked followed by a spike in CPU usage to 100% and even the UI becomes inaccessible.when i took the thread dumps to analyse the issue below is what i am getting consistently.</p>\n\n<pre><code>\"\"http-nio-8443\"-exec-1305\" daemon prio=10 tid=0x00007f0458538000 nid=0x56dd waiting for monitor entry [0x00007f04345c9000]\n   java.lang.Thread.State: BLOCKED (on object monitor)\n    at java.lang.Class.forName0(Native Method)\n    at java.lang.Class.forName(Class.java:247)\n    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1663)\n    - locked &lt;0x00000000a162f0c0&gt; (a org.apache.catalina.loader.WebappClassLoader)\n    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1521)\n    at java.beans.Introspector.instantiate(Introspector.java:1448)\n    at java.beans.Introspector.findExplicitBeanInfo(Introspector.java:431)\n    at java.beans.Introspector. (Introspector.java:380)\n    at java.beans.Introspector.getBeanInfo(Introspector.java:232)\n    at java.beans.Introspector.getBeanInfo(Introspector.java:218)\n    at com.googlecode.jsonplugin.JSONWriter.bean(JSONWriter.java:169)\n    at com.googlecode.jsonplugin.JSONWriter.process(JSONWriter.java:152)\n    at com.googlecode.jsonplugin.JSONWriter.value(JSONWriter.java:120)\n    at com.googlecode.jsonplugin.JSONWriter.write(JSONWriter.java:88)\n    at com.googlecode.jsonplugin.JSONUtil.serialize(JSONUtil.java:90)\n    at com.googlecode.jsonplugin.JSONResult.execute(JSONResult.java:119)\n    at com.opensymphony.xwork2.DefaultActionInvocation.executeResult(DefaultActionInvocation.java:348)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:253)\n    at com.facetime.imauditor.sreach.action.CustomRequestInterceptorJson.intercept(CustomRequestInterceptorJson.java:69)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.DefaultWorkflowInterceptor.doIntercept(DefaultWorkflowInterceptor.java:221)\n    at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.validator.ValidationInterceptor.doIntercept(ValidationInterceptor.java:150)\n    at org.apache.struts2.interceptor.validation.AnnotationValidationInterceptor.doIntercept(AnnotationValidationInterceptor.java:48)\n    at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.ConversionErrorInterceptor.intercept(ConversionErrorInterceptor.java:123)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.ParametersInterceptor.doIntercept(ParametersInterceptor.java:167)\n    at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.StaticParametersInterceptor.intercept(StaticParametersInterceptor.java:105)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at org.apache.struts2.interceptor.CheckboxInterceptor.intercept(CheckboxInterceptor.java:83)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at org.apache.struts2.interceptor.FileUploadInterceptor.intercept(FileUploadInterceptor.java:207)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.ModelDrivenInterceptor.intercept(ModelDrivenInterceptor.java:74)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.ScopedModelDrivenInterceptor.intercept(ScopedModelDrivenInterceptor.java:127)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at org.apache.struts2.interceptor.ProfilingActivationInterceptor.intercept(ProfilingActivationInterceptor.java:107)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at org.apache.struts2.interceptor.debugging.DebuggingInterceptor.intercept(DebuggingInterceptor.java:206)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.ChainingInterceptor.intercept(ChainingInterceptor.java:115)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.I18nInterceptor.intercept(I18nInterceptor.java:143)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.PrepareInterceptor.doIntercept(PrepareInterceptor.java:121)\n    at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at org.apache.struts2.interceptor.ServletConfigInterceptor.intercept(ServletConfigInterceptor.java:170)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.AliasInterceptor.intercept(AliasInterceptor.java:123)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.ExceptionMappingInterceptor.intercept(ExceptionMappingInterceptor.java:176)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at org.apache.struts2.impl.StrutsActionProxy.execute(StrutsActionProxy.java:50)\n    at org.apache.struts2.dispatcher.Dispatcher.serviceAction(Dispatcher.java:504)\n    at org.apache.struts2.dispatcher.FilterDispatcher.doFilter(FilterDispatcher.java:419)\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n    at com.facetime.imcoreserver.registration.AbsoluteSendRedirectFilter.doFilter(AbsoluteSendRedirectFilter.java:48)\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:240)\n    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:164)\n    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:462)\n    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:164)\n    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:100)\n    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)\n    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:403)\n    at org.apache.coyote.http11.Http11NioProcessor.process(Http11NioProcessor.java:369)\n    at org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process(Http11NioProtocol.java:317)\n    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1532)\n    - locked &lt;0x00000000abb53778&gt; (a org.apache.tomcat.util.net.SecureNioChannel)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n    at java.lang.Thread.run(Thread.java:662)\n</code></pre>\n\n<p>And below is the statistics of threads.</p>\n\n<pre><code>Threads locking monitor=1\nThreads sleeping on monitor=0\nThreads waiting to lock monitor=1097\n</code></pre>\n\n<p>Any help in this regards would really be helpful, thanks in advance.</p>\n\n<p>-Vinay</p>\n"},{"tags":["html","css","performance","wordpress","pagespeed"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":62,"score":0,"question_id":12935331,"title":"What is best practice for images in HTML and CSS","body":"<p>I creating a WordPress theme, and I have implement a function that allowing the images to be served embeded into the HTML document and the CSS files.</p>\n\n<p>What I mean is that instead of adding images in my web site as:</p>\n\n<pre><code>&lt;!-- In HMTL --&gt;\n&lt;img src=\"http://www.some-url.ext/img/my_image.jpg\" /&gt;\n\n/* In CSS */\nselector\n{\n    background-image: url(http://www.some-url.ext/img/my_image.jpg);\n}\n</code></pre>\n\n<p>to add the image in my site in the following form:</p>\n\n<pre><code>&lt;!-- In HMTL --&gt;\n&lt;img src=\"data:image/gif;base64,R0lG....\" /&gt;\n\n/* In CSS */\nselector\n{\n    background-image: url(data:image/gif;base64,R0lG....);\n}\n</code></pre>\n\n<p>The processed images are stored in cache files for better performance.</p>\n\n<p>My current theme also has a full width slider, that contains images that are large.</p>\n\n<p>The issue is that the processed document has the size of 1.83MB because of the embeded images.</p>\n\n<p>Also the document while is loading very fast, anything bellow the slideshow is getting slower to be displayed :(</p>\n\n<p>So, is it better to embed the images into the document or is it better to use the normal way with URLs ?</p>\n"},{"tags":["performance","google-chrome","network-protocols"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":136,"score":1,"question_id":12097425,"title":"Chrome Developer Tools: How to Read the Network Panel?","body":"<p>I am trying to make sense of the Chrome Developer Tools when I run performence tests on my websites. If you select Network on the tools meny it will look like this:</p>\n\n<p><img src=\"http://i.stack.imgur.com/ejvhD.png\" alt=\"enter image description here\"></p>\n\n<p>Then if I select the performance file I will have this information:</p>\n\n<p><img src=\"http://i.stack.imgur.com/ppZjE.png\" alt=\"enter image description here\"></p>\n\n<p>My question is this:</p>\n\n<ol>\n<li><em>What is the meaning of DNS Lookup, Connecting, Sending, Waiting and Receving? What is happening between the server, network and browser at each stage?</em> </li>\n<li><em>On the first image, the red line reads \"Load event fired\" and the blue one reads \"DOMContent event fired\". What is the meaning of this and why is it the DOMContent event is fired after all the content has been loaded?</em></li>\n</ol>\n"},{"tags":["c#","sql","performance","linq","linq-to-sql"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":74,"score":2,"question_id":12933794,"title":"Linq to SQL one to many relationships","body":"<p>Last couple of days i was struggling with a linq querys performance: </p>\n\n<pre><code>LinqConnectionDataContext context = new LinqConnectionDataContext();\nSystem.Data.Linq.DataLoadOptions options = new System.Data.Linq.DataLoadOptions();\noptions.LoadWith&lt;Question&gt;(x =&gt; x.Answers);\noptions.LoadWith&lt;Question&gt;(x =&gt; x.QuestionVotes);\noptions.LoadWith&lt;Answer&gt;(x =&gt; x.AnswerVotes);\ncontext.LoadOptions = options;\nvar query =( from c in context.Questions\n            where c.UidUser == userGuid\n            &amp;&amp; c.Answers.Any() == true\n            select new\n            {\n                c.Uid,\n                c.Content,\n                c.UidUser,\n                QuestionVote = from qv in c.QuestionVotes where qv.UidQuestion == c.Uid &amp;&amp; qv.UidUser == userGuid select new {qv.UidQuestion, qv.UidUser },\n                Answer = from d in c.Answers\n                         where d.UidQuestion == c.Uid\n                         select new\n                         {\n                             d.Uid,\n                             d.UidUser,\n                             d.Conetent,\n                             AnswerVote = from av in d.AnswerVotes where av.UidAnswer == d.Uid &amp;&amp; av.UidUser == userGuid select new { av.UidAnswer, av.UidUser }\n                         }\n            }).ToList();\n</code></pre>\n\n<p>Query have to run through 5000 rows, and it takes up to 1 minute. How can i improve performance of this query? </p>\n\n<p><strong>Update:</strong></p>\n\n<p><img src=\"http://i.stack.imgur.com/O61M5.png\" alt=\"enter image description here\"></p>\n"},{"tags":["python","regex","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":112,"score":3,"question_id":12726961,"title":"What is the fastest way to compare a text with a large number of regexp with python?","body":"<p>I have a list of regular expressions and I would like to match with tweets that as they arive so I can associate them with a specific account. With a small number of rules as above it goes really fast, but as soon as you increase the amount of rules, it becomes slower and slower.</p>\n\n<pre><code>import string, re2, datetime, time, array\n\nrules = [\n    [[1],[\"(?!.*ipiranga).*((?=.*posto)(?=.*petrobras).*|(?=.*petrobras)).*\"]],\n    [[2],[\"(?!.*brasil).*((?=.*posto)(?=.*petrobras).*|(?=.*petrobras)).*\"]],\n]\n\n#cache compile\ncompilled_rules = []\nfor rule in rules:\n    compilled_scopes.append([[rule[0][0]],[re2.compile(rule[1][0])]])\n\ndef get_rules(text):\n    new_tweet = string.lower(tweet)\n    for rule in compilled_rules:\n        ok = 1\n        if not re2.search(rule[1][0], new_tweet): ok=0\n        print ok\n\ndef test():\n    t0=datetime.datetime.now()\n    i=0\n    time.sleep(1)\n    while i&lt;1000000:\n        get_rules(\"Acabei de ir no posto petrobras. Moro pertinho do posto brasil\")\n        i+=1\n        t1=datetime.datetime.now()-t0\n        print \"test\"\n        print i\n        print t1\n        print i/t1.seconds\n</code></pre>\n\n<p>When I have tested with 550 rules, I couldn't do more then 50 reqs/s. Is there a better way for doing this? I need at least 200 reqs/s</p>\n\n<p>EDIT:\nafter tips from Jonathan I could improve about speed 5 times just but nesting a bit my rules. See the code below:</p>\n\n<pre><code>scope_rules = {\n    \"1\": {\n        \"termo 1\" : \"^(?!.*brasil)(?=.*petrobras).*\",\n        \"termo 2\" : \"^(?!.*petrobras)(?=.*ipiranga).*\",\n        \"termo 3\" : \"^(?!.*petrobras)(?=.*ipiranga).*\",\n        \"termo 4\" : \"^(?!.*petrobras)(?=.*ipiranga).*\",\n        },\n    \"2\": {\n        \"termo 1\" : \"^(?!.*ipiranga)(?=.*petrobras).*\",\n        \"termo 2\" : \"^(?!.*petrobras)(?=.*ipiranga).*\",\n        \"termo 3\" : \"^(?!.*brasil)(?=.*ipiranga).*\",\n        \"termo 4\" : \"^(?!.*petrobras)(?=.*ipiranga).*\",\n        }\n    }\ncompilled_rules = {}\nfor scope,rules in scope_rules.iteritems():\n    compilled_rules[scope]={}\n    for term,rule in rules.iteritems():\n        compilled_rules[scope][term] = re.compile(rule)\n\n\ndef get_rules(text):\n    new_tweet = string.lower(text)\n    for scope,rules in compilled_rules.iteritems():\n        ok = 1\n        for term,rule in rules.iteritems():\n            if ok==1:\n                if re.search(rule, new_tweet):\n                    ok=0\n                    print \"found in scope\" + scope + \" term:\"+ term\n\n\ndef test():\n    t0=datetime.datetime.now()\n    i=0\n    time.sleep(1)\n    while i&lt;1000000:\n        get_rules(\"Acabei de ir no posto petrobras. Moro pertinho do posto ipiranga da lagoa\")\n        i+=1\n        t1=datetime.datetime.now()-t0\n        print \"test\"\n        print i\n        print t1\n        print i/t1.seconds\n\ncProfile.run('test()', 'testproof')\n</code></pre>\n"},{"tags":["performance","oracle","postgresql"],"answer_count":4,"favorite_count":3,"up_vote_count":3,"down_vote_count":0,"view_count":1779,"score":3,"question_id":3316812,"title":"How big is the performance difference between Oracle and PostgreSQL?","body":"<p>I'm wondering about how to scale a database. Currently it uses PostgreSQL. Would switching to Oracle be worthwhile inspite of the coding pain and expense? Or is PostgreSQL + more boxes a better/cheaper approach?</p>\n"},{"tags":["android","performance","android-emulator","qemu"],"answer_count":39,"favorite_count":277,"up_vote_count":539,"down_vote_count":1,"view_count":189248,"score":538,"question_id":1554099,"title":"Slow Android emulator","body":"<p>I have a 2.67&nbsp;GHz Celeron processor, 1.21&nbsp;GB of RAM on a x86 Windows XP Professional machine. My understanding is that the Android emulator should start fairly quickly on such a machine, but for me it does not. I have followed all instructions in setting up the IDE, SDKs, JDKs and such and have had some success in staring the emulator quickly but is very particulary. How can I, if possible, fix this problem?</p>\n\n<p>Even if it starts and loads the home screen, it is very sluggish. I have tried the Eclipse IDE in Galileos, and Ganymede.</p>\n"},{"tags":["jquery","ajax","performance","click"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":46,"score":0,"question_id":12932237,"title":"Slowing process down by clicking multiple times in jquery","body":"<p>I'm working on my own CMS system. It works partly in Ajax and jQuery, but the problem is that I'm using a lot of click events. So, when I keep clicking on different items in my website, it slows the system down. Eventually it doesn't do anything anymore. Am I right about the click event and how do I need to use it in the good way? I used the .bind and .on event handlers. </p>\n\n<pre><code>$(document).ready(function(){\n\n\n//Standards\nvar windowWidth = $(window).width();\nvar windowHeight = $(window).height();\n$('#wrapper').css('width',windowWidth);\n$('#content').css('height',windowHeight);\n\n\n//Click related items\n$('.listItem').bind('click',function() {\n    var itemID = $(this).attr('rel');\n    $('#content').load('showitems.php',{newID:itemID});\n});\n\n\n//Click on tab\n$('.liBase a').on('click', function() {\n    $('.liBase a').parent().removeClass('activeList');\n    $('#imageShow').removeClass('activeList');\n    $(this).parent().addClass('activeList');\n});\n\n\n//Click pages\n\n$('.page').on('click', function() {\n    var pageID = $(this).attr('rel');\n    $('.liBase').parent().parent().removeClass('activeList');\n    $('#imageShow').removeClass('activeList');\n    $(this).parent().parent().addClass('activeList');\n    $('#content').load('showpages.php',{newID:pageID});\n});\n\n$('.item').on('click', function() {\n    var pageID = $(this).attr('rel');\n    $('.liBase').parent().parent().removeClass('activeList');\n    $('#imageShow').removeClass('activeList');\n    $(this).parent().parent().addClass('activeList');\n    $('#content').load('showitems.php',{newID:pageID});\n});\n\n$('.editItem').on('click', function() {\n    var newID = $(this).attr('rel');\n    $('.editPage').parent().parent().removeClass('activeList');\n    $('#imageShow').removeClass('activeList');\n    $(this).parent().parent().addClass('activeList');\n    $('#content').load('edititem.php',{itemID:newID});\n});\n\n$('.editPage').on('click',function() {\n    var newID = $(this).attr('rel');\n    $('.liBase').parent().parent().removeClass('activeList');\n    $('#imageShow').removeClass('activeList');\n    $(this).parent().parent().addClass('activeList');\n    $('#content').load('editpage.php',{pageID:newID});\n});\n\n$('.deleteItem').on('click', function() {\n    var newID = $(this).attr('rel');\n    $('.liBase').parent().parent().removeClass('activeList');\n    $('#imageShow').removeClass('activeList');\n    $(this).parent().parent().addClass('activeList');\n    $('#content').load('../control/deleteRecords.php',{postID:newID,tblName:'items',tblID:'itemID'});\n});\n\n$('.deletePage').on('click',function() {\n\n    var newID = $(this).attr('rel');\n    $('.liBase').parent().parent().removeClass('activeList');\n    $('#imageShow').removeClass('activeList');\n    $(this).parent().parent().addClass('activeList');\n    $('#content').load('../control/deleteRecords.php',{postID:newID,tblName:'pages',tblID:'pageID'});\n});\n\n$('#addPage').on('click', function() {\n    $('#content').load('addpage.php');\n});\n\n$('#addItem').on('click', function() {\n    $('#content').load('additem.php');\n});\n\n\n$('#imageShow a').on('click', function() {\n    var pageID = $(this).attr('rel');\n    $('.liBase').parent().parent().removeClass('activeList');\n    $(this).parent().addClass('activeList');\n    $('#content').load('showimages.php');\n\n});\n\n$('#imageAdd').on('click', function() {\n    $('#content').load('addimage.php');\n});\n\n\n\n});\n</code></pre>\n"},{"tags":["c++","performance","visual-studio","gcc","boost"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":115,"score":2,"question_id":12932348,"title":"Why std::make_shared<>() has much better performance than boost::make_shared()?","body":"<p>I have been doing some field performance test on </p>\n\n<pre><code>1&gt;std::shared_ptr, std::make_shared based on 'gcc 4.7.2' &amp; 'VC10 implementation' \n2&gt;boost::shared_ptr, boost::make_shared based on boost 1.47\n</code></pre>\n\n<p>The test result is somewhat interesting.</p>\n\n<p>1>In general <code>std</code> version performs better but especially <code>std::make_shared</code>. Why? Can I increase the <code>boost</code> version performance since C++ 11 is not available for some old project yet as they are using old version of Visual studio?</p>\n\n<p>Below is my code snippet used to test those. \nNB. you need to manually switch between boost &amp; std. \nNB. \"SimpleMSTimer.hpp\" is my timer wrapper for boost ptime, a bit too long to post here. But feel free to use your own timer.Any portable time would do.</p>\n\n<pre><code>#include \"stdafx.h\"\n#include &lt;vector&gt;\n#include &lt;iostream&gt;\n#include &lt;boost/shared_ptr.hpp&gt;\n#include &lt;boost\\make_shared.hpp&gt;\n\n#include \"SimpleMSTimer.hpp\"//my timer wrapper for boost ptime\n\nusing namespace std;\nusing namespace boost;\n\nclass Thing\n{\npublic:\n    Thing()\n    {\n    }\n\n    void method (void)\n    {\n        int i = 5;\n    }\n};\n\ntypedef boost::shared_ptr&lt;Thing&gt; ThingPtr;\n\nvoid processThing(Thing* thing)\n{\n    thing-&gt;method();\n}\n\n//loop1 and loop2 test shared_ptr in the vector container\nvoid loop1(long long num)\n{\n    cout &lt;&lt; \"native raw pointer: \";\n    vector&lt;Thing&gt; thingPtrs;\n    YiUtil::MSSegmentTimer segTimer(YiUtil::MSSegmentTimer::MLSEC, std::cout);\n    for(int i=0; i&lt; num; i++) {\n        Thing thing;\n        thingPtrs.push_back(thing);\n    }\n    thingPtrs.clear();\n}\n\nvoid loop2(long long num)\n{\n    cout &lt;&lt; \"native boost::shared_ptr: \";\n    vector&lt;ThingPtr&gt; thingPtrs;\n    YiUtil::MSSegmentTimer segTimer(YiUtil::MSSegmentTimer::MLSEC, std::cout);\n    for(int i=0; i&lt; num; i++) {\n        ThingPtr p1(new Thing);\n        thingPtrs.push_back(p1);\n    }\n}\n\nvoid loop3(long long num)\n{\n    cout &lt;&lt; \"optimized boost::shared_ptr: \";\n    vector&lt;ThingPtr&gt; thingPtrs;\n\n    YiUtil::MSSegmentTimer segTimer(YiUtil::MSSegmentTimer::MLSEC, std::cout);\n    for(int i=0; i&lt; num; i++) {\n        ThingPtr p1 = boost::make_shared&lt;Thing&gt;();\n        thingPtrs.push_back(p1);\n    }\n}\n\n\n//loop3 and loop4 test shared_ptr in loop\nvoid loop4(long long num)\n{\n    cout &lt;&lt; \"native raw pointer: \";\n    YiUtil::MSSegmentTimer segTimer(YiUtil::MSSegmentTimer::MLSEC, std::cout);\n    for(int i=0; i&lt; num; i++) {\n        Thing* p1 = new Thing();\n        processThing(p1);\n        delete p1;\n    }\n}\n\nvoid loop5(long long num)\n{\n    cout &lt;&lt; \"native boost::shared_ptr: \";\n    YiUtil::MSSegmentTimer segTimer(YiUtil::MSSegmentTimer::MLSEC, std::cout);\n    for(int i=0; i&lt; num; i++) {\n        ThingPtr p1(new Thing);\n        processThing(p1.get());\n    }\n}\n\nvoid loop6(long long num)\n{\n    cout &lt;&lt; \"optimized boost::shared_ptr: \";\n    YiUtil::MSSegmentTimer segTimer(YiUtil::MSSegmentTimer::MLSEC, std::cout);\n    for(int i=0; i&lt; num; i++) {\n        ThingPtr p1 = boost::make_shared&lt;Thing&gt;();\n        processThing(p1.get());\n    }\n}\n\nint main() {\n    long long num = 10000000;\n    cout &lt;&lt; \"test 1\" &lt;&lt; endl;\n    loop1(num);\n    loop2(num);\n    loop3(num);\n\n    cout &lt;&lt; \"test 2\"&lt;&lt; endl;\n    loop4(num);\n    loop5(num);\n    loop6(num);\n\n    return 0;\n}\n</code></pre>\n\n<p>VC10 compiler under release mode, gcc compiled with flag '-O3' for max optimization.\nTest result:</p>\n\n<pre><code>//VS2010 release mode\n//boost\ntest 1\nnative raw pointer: SegmentTimer: 15 milliseconds/n\nnative boost::shared_ptr: SegmentTimer: 3312 milliseconds/n\noptimized boost::shared_ptr: SegmentTimer: 3093 milliseconds/n\ntest 2\nnative raw pointer: SegmentTimer: 921 milliseconds/n\nnative boost::shared_ptr: SegmentTimer: 2359 milliseconds/n\noptimized boost::shared_ptr: SegmentTimer: 2203 milliseconds/n\n\n//std\ntest 1\nnative raw pointer: SegmentTimer: 15 milliseconds/n\nnative std::shared_ptr: SegmentTimer: 3390 milliseconds/n\noptimized std::shared_ptr: SegmentTimer: 2203 milliseconds/n\ntest 2\nnative raw pointer: SegmentTimer: 937 milliseconds/n\nnative std::shared_ptr: SegmentTimer: 2359 milliseconds/n\noptimized std::shared_ptr: SegmentTimer: 1343 milliseconds/n\n==============================================================================\ngcc 4.72 release mode\n//boost\ntest 1\nnative raw pointer: SegmentTimer: 15 milliseconds/n\nnative boost::shared_ptr: SegmentTimer: 4874 milliseconds/n\noptimized boost::shared_ptr: SegmentTimer: 3687 milliseconds/n\ntest 2\nnative raw pointer: SegmentTimer: 1109 milliseconds/n\nnative boost::shared_ptr: SegmentTimer: 2546 milliseconds/n\noptimized boost::shared_ptr: SegmentTimer: 1578 milliseconds/n\n\n//std\ntest 1\nnative raw pointer: SegmentTimer: 15 milliseconds/n\nnative std::shared_ptr: SegmentTimer: 3374 milliseconds/n\noptimized std::shared_ptr: SegmentTimer: 2296 milliseconds/n\ntest 2\nnative raw pointer: SegmentTimer: 1124 milliseconds/n\nnative std::shared_ptr: SegmentTimer: 2531 milliseconds/n\noptimized std::shared_ptr: SegmentTimer: 1468 milliseconds/n\n</code></pre>\n"},{"tags":["performance","excel","aggregate-functions","lookup"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":24,"score":0,"question_id":12930500,"title":"excel performance: Lookup Vs Getpivotdata","body":"<p>I build an Excel 2007 spreadsheet which contains a larger table with source data (about 500,000 rows and 10 columns). I need to extract data from this large table for my analysis. To extract and aggregate data I usually use sumif, vlookup/hlookup and index+match functions. </p>\n\n<p>I recently learned about the existence of the function getpivotdata, which makes it possible to extract data from a pivot table. To be able to use it, I first need to convert my large source table to a pivot table and after that I can extract data using the function getpivotdata.</p>\n\n<p>Would you expect a performance improvement if I would use getpivotdata to extract and aggregate data instead? I would expect that within the underlying Pivot object aggregated values are pre-calculated and therefore performance would be better. </p>\n\n<p>If performance would be better, are there any reasons not to follow this approach? To be clear, there is no need to refresh the pivot table because it contains source data (which is located in the beginning of the calculation chain).</p>\n"},{"tags":["mysql","performance","query-optimization"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":89,"score":4,"question_id":12930331,"title":"Why is MAX() 100 times slower than ORDER BY ... LIMIT 1?","body":"<p>I have a table <code>foo</code> with (among 20 others) columns <code>bar</code>, <code>baz</code> and <code>quux</code> with indexes on <code>baz</code> and <code>quux</code>. The table has ~500k rows.</p>\n\n<p>Why do the following to queries differ so much in speed? Query A takes 0.3s, while query B takes 28s.</p>\n\n<p><strong>Query A</strong></p>\n\n<pre><code>select baz from foo\n    where bar = :bar\n    and quux = (select quux from foo where bar = :bar order by quux desc limit 1)\n</code></pre>\n\n<p><strong>Explain</strong></p>\n\n<pre><code>id  select_type table   type    possible_keys   key     key_len ref     rows    Extra\n1   PRIMARY     foo     ref     quuxIdx         quuxIdx 9       const   2       \"Using where\"\n2   SUBQUERY    foo     index   NULL            quuxIdx 9       NULL    1       \"Using where\"\n</code></pre>\n\n<p><strong>Query B</strong></p>\n\n<pre><code>select baz from foo\n    where bar = :bar\n    and quux = (select MAX(quux) from foo where bar = :bar)\n</code></pre>\n\n<p><strong>Explain</strong></p>\n\n<pre><code>id  select_type table   type    possible_keys   key     key_len ref     rows    Extra\n1   PRIMARY     foo     ref     quuxIdx         quuxIdx 9       const   2       \"Using where\"\n2   SUBQUERY    foo     ALL     NULL            NULL    NULL    NULL    448060  \"Using where\"\n</code></pre>\n\n<p>I use MySQL 5.1.34.</p>\n"},{"tags":["c++","performance","memory","pointers"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":204,"score":0,"question_id":12904621,"title":"Is there any tricks for fast-memory access?","body":"<p>I am very new to the world of C++ programming, so sorry for my amatuerish question:</p>\n\n<p>I get a large block of data stored in the main memory (1-D array), and I need to access some of the data there frequently, my way of doing this is:</p>\n\n<pre><code>float *x=new float[20];//array to store x;\nint *indlistforx=new int[20];//array to store the index of x;\nfloat *databank=new float[100000000];//a huge array to store data\n\n/... fill data to databank.../\n\n\nfor (int i=0;i&lt;N;i++)//where N is a very large number;\n {\n  /... write index to indlistforx.../\n  getdatafromdatabank(x, indlistforx, databank);\n  //Based on the index provided by indlistforx, read data from databank then pass them to x\n\n  /...do something with x.../\n  };\n</code></pre>\n\n<p>Is there any efficient/fast way to access these data(the index for x are not aligned, and it is impossible to be aligned)?</p>\n\n<p>Many thanks in advance!</p>\n"},{"tags":["performance","apache","dns","varnish"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":2073,"score":3,"question_id":3728066,"title":"Varnish: cache only specific domain","body":"<p>I have been Googling aggressively, but without luck.</p>\n\n<p>I'm using Varnish with great results, but I would like to host multiple websites on a single server (Apache), without Varnish caching all of them.</p>\n\n<p>Can I specify what websites by URL to cache?</p>\n\n<p>Thanks</p>\n"},{"tags":["iphone","ios","performance","core-image"],"answer_count":3,"favorite_count":5,"up_vote_count":27,"down_vote_count":5,"view_count":9563,"score":22,"question_id":6625888,"title":"Are the Core Image filters in iOS 5.0 fast enough for realtime video processing?","body":"<p>Now that Apple has ported the Core Image framework over to iOS 5.0, I'm wondering: is Core Image is fast enough to apply live filters and effects to camera video?</p>\n\n<p>Also, what would be a good starting point to learn the Core Image framework for iOS 5.0?</p>\n"},{"tags":["xcode","performance","xcode4"],"answer_count":13,"favorite_count":64,"up_vote_count":74,"down_vote_count":0,"view_count":19780,"score":74,"question_id":6355667,"title":"Xcode 4 - slow performance","body":"<p>I have an issue with Xcode 4 really responding very slowly to user interactions, e.g. editing code, scrolling areas etc. This particularly happens with larger scale projects with many controllers/view files etc.</p>\n\n<p>I completely wiped the hard disk and re-installed Snow Leopard and Xcode the other week but steadily it ground to a frustrating response time again (over a number of days) disrupting workflow considerably.</p>\n\n<p>I have also on occasion removed the project's \"derived data\" via the Organiser -> Projects and this has had little effect.</p>\n\n<p>I'm wondering if there is anything I can do to improve performance other than get a higher specced machine in the first instance.</p>\n\n<p>FYI I'm running MacBook with Intel Core 2 Duo processors at 2GHz and 4GB of RAM.</p>\n\n<p>In case we need to upgrade I'd also like to know if people are experiencing this poor performance from Xcode 4 on well specced machines (which would make our hardware upgrade rather pointless as it's only Xcode that has any performance issue on the MacBook).</p>\n\n<p>If anybody has any suggestions or recommendations or could even let us know how improved hardware effects Xcode's performance on larger project trees then that would be extremely helpful and also a valuable resource for other devs in a similar position.</p>\n"},{"tags":["xcode","performance","cpu","xcode4.3"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":1345,"score":2,"question_id":10522936,"title":"Xcode 4.3.2 and 100% CPU constantly in the idle time","body":"<p>My Xcode started to behave very heavily from yesterday when working on medium size project (around 200 source files). Project compiles correctly and runs in both simulator and device. I do not use any 3rd party libraries, except few widely used includes (like JSON or facebook ios sdk).</p>\n\n<p>It constantly uses CPU(s) at full speed, even if it is in idle state (no indexing, no compiling, no editing). The usage of RAM is relatively normal (300-50MB).</p>\n\n<p>My machine uses: Core 2 Duo 3.04Ghz CPU, 8GB of RAM and Vertex OCZ 3 SSD drive.</p>\n\n<p>I have tried every suggested solution found at stackoverflow:</p>\n\n<ol>\n<li>Cleaned project</li>\n<li>Cleaned Derived Data in Organizer</li>\n<li>Cleaned repositories in Organizer</li>\n<li>Cleaned xcodeproject bundle from workspace and userdata files as suggested here: <a href=\"http://stackoverflow.com/a/8165886/229229\">http://stackoverflow.com/a/8165886/229229</a> (it is helping just for a moment and starts again after minute or so).</li>\n<li>Restarted Xcode many times (with the same effect as in 4).</li>\n<li>Disabled \"Live issues\"</li>\n<li>even Reinstalled Xcode</li>\n</ol>\n\n<p>Nothing helps. In most cases, Xcode indexes the project for a moment, then comes back to the normal performance, but after a while becomes unusable again. CPU jumps back to 95-100% for both cores, intelligence hangs, etc... </p>\n\n<p>I am attaching screenshots of how the Xcode processes are seen by the Instruments:</p>\n\n<p><img src=\"http://i.stack.imgur.com/nuEqj.png\" alt=\"enter image description here\">\n<img src=\"http://i.stack.imgur.com/ktR3m.png\" alt=\"enter image description here\">\n<img src=\"http://i.stack.imgur.com/bIhVX.png\" alt=\"enter image description here\">\n<img src=\"http://i.stack.imgur.com/NUdPx.jpg\" alt=\"enter image description here\">\n<img src=\"http://i.stack.imgur.com/AbsTj.png\" alt=\"enter image description here\"></p>\n\n<p><strong>UPDATE:</strong>\nAfter a moment of hope that I solved the problem by moving around few</p>\n\n<p><code>#import \"header.h\"</code> </p>\n\n<p>statements from headers to the implementation files and exchanging them with forward declarations ... the problem came back again after a while.\nI am adding the console log. \nThe strange thing is that the logs related to Xcode show up after I quit it, not during the run itsef.</p>\n\n<p>Console logs:</p>\n\n<pre><code>5/11/12 9:27:03.777 AM [0x0-0x45045].com.apple.dt.Xcode: com.apple.dt.instruments.backgroundinstruments: Already loaded\n5/11/12 9:27:05.571 AM Xcode: Performance: Please update this scripting addition to supply a value for ThreadSafe for each event handler: \"/Library/ScriptingAdditions/SIMBL.osax\"\n5/11/12 9:27:58.168 AM Xcode: ERROR: Failed to create an alert for ID \"enabled\" based on defaults: 1\n</code></pre>\n"},{"tags":["mysql","performance","partitioning"],"answer_count":1,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":39,"score":3,"question_id":12929624,"title":"Partitions and UPDATE","body":"<p>I'm diving deeper and deeper into MySQL Features, and the next one I'm trying out is table partitions</p>\n\n<p>There's basically only one question about them, where I couldnt find a clear answer yet:</p>\n\n<p>If you UPDATE a row, will the row be moved to another partition automatically, if the partition conditions of another partition is met? (if for example, the partitions are split up by region, and the region changes from region A to region B)</p>\n\n<p>And if that doesnt happen automatically, what do I need to do in order to move the row from partition A to partition B? (and will there be a performance hit by doing so?)</p>\n\n<p>What I would like to do, is to move 'deleted' (a flag) informations into a separate partition of the table, since those will rarely be called. Would that usually be a good idea or would it be better to just leave everything in the same (probably someday huge - multiple million rows) table?</p>\n"},{"tags":["mysql","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":33,"score":1,"question_id":12929964,"title":"MySQL column comments and performance","body":"<p>I have to add new column to database table but its name is relatively ambiguous and I thought a comment on the column would be perfectly utilised and would give good insight for the developers down the line.</p>\n\n<p>But the questions is, does the column comment have any impact on the SQL queries and their performance generally?</p>\n"},{"tags":["asp.net-mvc-3","performance","debugging","entity-framework-4.1"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":39,"score":0,"question_id":12914089,"title":"Count maximum and opened EF connections","body":"<p>I have ASP.NET MVC app which suffer from performance bottleneck. I would like to profile EF connections and find out maximum allowed EF connections and current opened connections. Any suggestions how to do that? </p>\n\n<p>My Repository code</p>\n\n<pre><code>    using EntityFramework.Patterns;\n\n\n     public class ServiceRepository : IServiceRepository\n        {\n            private readonly IRepository&lt;User&gt; _userRepository;\n            private readonly IUnitOfWork _unitOfWork;\n\n\n            public ServiceRepository(DbContext dbContext)\n            {\n                var dbContextAdapter = new DbContextAdapter(dbContext);\n                _unitOfWork  = new UnitOfWork(dbContextAdapter);            \n                _userRepository = new Repository&lt;User&gt;(dbContextAdapter);\n            }\n\n            public IQueryable&lt;User&gt; GetUsersByProduct(int productId)\n            {\n                return _userRepository.AsQueryable().Where(p =&gt; p.Orders.Any(o =&gt; o.ProductId == productId));\n            }\n//Skip code\n\n        public void Commit()\n        {\n            _unitOfWork.Commit();\n        }\n\n    }\n</code></pre>\n\n<p>Injection with Ninject</p>\n\n<pre><code>private static void RegisterServices(IKernel kernel)\n{\n       var connectionString = ConfigurationManager.ConnectionStrings[\"Entities\"].ConnectionString;\n       kernel.Bind(typeof(DbContext)).ToMethod(context =&gt; new DbContext(connectionString)).InRequestScope();           \n       kernel.Bind&lt;IServiceRepository&gt;().To&lt;ServiceRepository&gt;().InRequestScope();\n}\n</code></pre>\n"},{"tags":["javascript","performance","algorithm"],"answer_count":1,"favorite_count":2,"up_vote_count":7,"down_vote_count":1,"view_count":88,"score":6,"question_id":12926975,"title":"Closest Pair Algorithm in JavaScript","body":"<p>I'm trying to implement a divide and conquer algorithm to find the closest pair of points in a randomly-generated set of points using JavaScript.  This algorithm should be running in O(n log n) time, but it is taking considerably longer to run than a simple brute force algorithm, which should be O(n^2).</p>\n\n<p>I've created two jsfiddles that time the algorithms for an array of 16000 points:</p>\n\n<ul>\n<li><a href=\"http://jsfiddle.net/kaangs10/rfSz5/2/\">Divide and Conquer</a></li>\n<li><a href=\"http://jsfiddle.net/kaangs10/eFgSA/2/\">Brute Force</a></li>\n</ul>\n\n<p>My hypothesis is that the divide and conquer is so slow because JavaScript arrays are actually hash tables.  Is it possible to significantly speed up the algorithm in JavaScript?  If so, what would be the best way to go about doing this?</p>\n"},{"tags":["performance","algorithm","r"],"answer_count":2,"favorite_count":0,"up_vote_count":10,"down_vote_count":0,"view_count":104,"score":10,"question_id":12913446,"title":"Efficiently create dataframe from strings containing key-value pairs","body":"<p>I would like to ask you for efficiency suggestions for a specific coding problem in R. I have a string vector in the following style:</p>\n\n<pre><code>[1] \"HGVSc=ENST00000495576.1:n.820-1G&gt;A;INTRON=1/1;CANONICAL=YES\"\n[2] \"DISTANCE=2179\"                                              \n[3] \"HGVSc=ENST00000466430.1:n.911C&gt;T;EXON=4/4;CANONICAL=YES\"    \n[4] \"DISTANCE=27;CANONICAL=YES;common\"\n</code></pre>\n\n<p>In each element of the vector, the single entries are separated with a <code>;</code> and MOST of the single entries have the format <code>KEY=VALUE</code>. However, there are also some entries, which only have the format <code>KEY</code> (see \"common\" in [4]). In this example, there are 15 different keys and not every key appears in each element of the vector. The 15 different keys are:</p>\n\n<pre><code>names &lt;- c('ENSP','HGVS','DOMAINS','EXON','INTRON', 'HGVSp', 'HGVSc','CANONICAL','GMAF','DISTANCE', 'HGNC', 'CCDS', 'SIFT', 'PolyPhen', 'common')\n</code></pre>\n\n<p>From this vector I would like to create a dataframe that looks like this:</p>\n\n<pre><code>ENSP HGVS DOMAINS EXON INTRON HGVSp                        HGVSc CANONICAL\n1    -    -       -    -    1/1     - ENST00000495576.1:n.820-1G&gt;A       YES\n2    -    -       -    -      -     -                            -         -\n3    -    -       -  4/4      -     -   ENST00000466430.1:n.911C&gt;T       YES\n4    -    -       -    -      -     -                            -       YES\nGMAF DISTANCE HGNC CCDS SIFT PolyPhen common\n1    -        -    -    -    -        -      -\n2    -     2179    -    -    -        -      -\n3    -        -    -    -    -        -      -\n4    -       27    -    -    -        -    YES\n</code></pre>\n\n<p>I wrote this function to solve the problem:</p>\n\n<pre><code>unlist.info &lt;- function(names, column){\n  info.mat &lt;- matrix(rep('-', length(column)*length(names)), nrow=length(column), ncol=length(names), dimnames=list(c(), names))\n  info.mat &lt;- as.data.frame(info.mat, stringsAsFactors=F)\n\n  for (i in 1:length(column)){\n    info &lt;- unlist(strsplit(column[i], \"\\\\;\"))\n    for (e in info){\n      e &lt;- unlist(strsplit(e, \"\\\\=\"))\n      j &lt;- which(names == e[1])\n      if (length(e) &gt; 1){\n        # KEY=VALUE. The value might contain a = as well\n        value &lt;- paste(e[2:length(e)], collapse='=')\n        info.mat[i,j] &lt;- value\n      }else{\n        # only KEY\n        info.mat[i,j] &lt;- 'YES'\n      }\n    }\n  }\n  return(info.mat)\n}\n</code></pre>\n\n<p>And then I call:</p>\n\n<pre><code>mat &lt;- unlist.info(names, vector)\n</code></pre>\n\n<p>Even though this works, it is really slow. Also I am handling vectors with over 100.000 entries. Now I realize that looping is inelegant and inefficient in R and I am familiar with the concept of applying functions to data frames. However, since every entry of the vector contains a different subset of <code>KEY=VALUE</code> or <code>KEY</code> entries I could not come up with a more efficient function.</p>\n"},{"tags":["performance","postgresql","database-design","postgresql-performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":77,"score":1,"question_id":12915209,"title":"How to understand an EXPLAIN ANALYZE","body":"<p>I am not very familiar with looking at EXPLAIN ANALYZE results, I have a huge problem with  my queries being too slow. I have tried to read up on how to interpret results from an explain queries, but I still don't know what I should be looking for, and what might be wrong. I have a feeling that there is some big red light flashing somewhere, I just don't see it.</p>\n\n<p>So the query is pretty simple, it looks like this:</p>\n\n<pre><code>EXPLAIN ANALYZE SELECT \"cars\".* FROM \"cars\" WHERE \"cars\".\"sales_state\" = 'onsale' AND \"cars\".\"brand\" = 'BMW' AND \"cars\".\"model_name\" = '318i' AND \"cars\".\"has_auto_gear\" = TRUE  LIMIT 25 OFFSET 0\n</code></pre>\n\n<p>And the result like this:</p>\n\n<pre><code>Limit  (cost=0.00..161.07 rows=25 width=1245) (actual time=35.232..38.694 rows=25 loops=1)\n  -&gt;  Index Scan using index_cars_onsale_on_brand_and_model_name on cars  (cost=0.00..1179.06 rows=183 width=1245) (actual time=35.228..38.652 rows=25 loops=1)\n        Index Cond: (((brand)::text = 'BMW'::text) AND ((model_name)::text = '318i'::text))\n        Filter: has_auto_gear\"\nTotal runtime: 38.845 ms\n</code></pre>\n\n<p>A little background:\nI'm on Postgresql 9.1.6, running on Herokus dedicated databases. My db has aprox 7,5Gb RAM, the table cars contains 3,1M rows and an aprox 2,0M of the rows has sales_state = 'onsale'. The table has 170 columns. The index that it uses looks something like this:</p>\n\n<pre><code>CREATE INDEX index_cars_onsale_on_brand_and_model_name\n  ON cars\n  USING btree\n  (brand COLLATE pg_catalog.\"default\" , model_name COLLATE pg_catalog.\"default\" )\n  WHERE sales_state::text = 'onsale'::text;\n</code></pre>\n\n<p>Anyone seeing some big obvious issue?</p>\n\n<p>EDIT:</p>\n\n<pre><code>SELECT pg_relation_size('cars'), pg_total_relation_size('cars');\n</code></pre>\n\n<p>pg_relation_size: 2058444800\npg_total_relation_size: 4900126720</p>\n\n<pre><code>SELECT pg_relation_size('index_cars_onsale_on_brand_and_model_name');\n</code></pre>\n\n<p>pg_relation_size: 46301184</p>\n\n<pre><code>SELECT avg(pg_column_size(cars)) FROM cars limit 5000;\n</code></pre>\n\n<p>avg: 636.9732567210792995</p>\n\n<p>WITHOUT THE LIMIT:</p>\n\n<pre><code>EXPLAIN ANALYZE SELECT \"cars\".* FROM \"cars\" WHERE \"cars\".\"sales_state\" = 'onsale' AND \"cars\".\"brand\" = 'BMW' AND \"cars\".\"model_name\" = '318i' AND \"cars\".\"has_auto_gear\" = TRUE\n\nBitmap Heap Scan on cars  (cost=12.54..1156.95 rows=183 width=4) (actual time=17.067..55.198 rows=2096 loops=1)\n  Recheck Cond: (((brand)::text = 'BMW'::text) AND ((model_name)::text = '318i'::text) AND ((sales_state)::text = 'onsale'::text))\n  Filter: has_auto_gear\n  -&gt;  Bitmap Index Scan on index_cars_onsale_on_brand_and_model_name  (cost=0.00..12.54 rows=585 width=0) (actual time=15.211..15.211 rows=7411 loops=1)\"\n        Index Cond: (((brand)::text = 'BMW'::text) AND ((model_name)::text = '318i'::text))\nTotal runtime: 56.851 ms\n</code></pre>\n"},{"tags":["jquery","performance","caching","jquery-selectors","chaining"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":43,"score":1,"question_id":12926467,"title":"Cached vs chained selectors in jquery?","body":"<p>I am wondering if there is a performance difference between using a cached selector, and using chained selectors?</p>\n\n<p>If I understand it correctly the chaining works because each function returns the jquery object, which is exactly the same as what is contained in the cached selector. So there would be no difference performance wise in the two examples below is there?\n<hr />\n<strong>Cached Selector</strong></p>\n\n<pre><code>$(function(){\n\n    $.on('click', '.disabled', function(){\n        $toggle = $(this);\n        $toggle.attr('title', 'Object Enabled');\n        $toggle.toggleClass('disabled enabled');\n        $toggle.html('Enabled');\n    });\n});\n</code></pre>\n\n<p><hr />\n<strong>Chained Selector</strong></p>\n\n<pre><code>$(function(){\n\n    $.on('click', '.disabled', function(){\n        $(this)\n            .attr('title', 'Object Enabled')\n            .toggleClass('disabled enabled')\n            .html('Enabled');\n    });\n});\n</code></pre>\n"},{"tags":["python","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":140,"score":2,"question_id":12842717,"title":"Why is processing a sorted array not faster than an unsorted array in Python?","body":"<p>In this post <a href=\"http://stackoverflow.com/questions/11227809/why-is-processing-a-sorted-array-faster-than-an-unsorted-array\">Why is processing a sorted array faster than random array</a>, it says that branch predicton is the reason of the performance boost in sorted arrays.</p>\n\n<p>But I just tried the example using Python; and I think there is no difference between sorted and random arrays (I tried both bytearray and array; and use line_profile to profile the computation).</p>\n\n<p>Am I missing something?</p>\n\n<p>Here is my code:</p>\n\n<pre><code>from array import array\nimport random\narray_size = 1024\nloop_cnt = 1000\n# I also tried 'array', and it's almost the same\na = bytearray(array_size)\nfor i in xrange(array_size):\n    a.append(random.randint(0, 255))\n#sorted                                                                         \na = sorted(a)\n@profile\ndef computation():\n    sum = 0\n    for i in xrange(loop_cnt):\n        for j in xrange(size):\n            if a[j] &gt;= 128:\n                sum += a[j]\n\ncomputation()\nprint 'done'\n</code></pre>\n"},{"tags":["mysql","performance","primary-key","innodb","myisam"],"answer_count":9,"favorite_count":8,"up_vote_count":27,"down_vote_count":0,"view_count":13220,"score":27,"question_id":332300,"title":"Is there a REAL performance difference between INT and VARCHAR primary keys?","body":"<p>Is there a measurable performance difference between using INT vs. VARCHAR as a primary key in MySQL? I'd like to use VARCHAR as the primary key for reference lists (think US States, Country Codes) and a coworker won't budge on the INT AUTO_INCREMENT as a primary key for all tables. </p>\n\n<p>My argument, as detailed <a href=\"http://database-programmer.blogspot.com/2008/01/database-skills-sane-approach-to.html#rule1\" rel=\"nofollow\">here</a>, is that the performance difference between INT and VARCHAR is negligible, since every INT foreign key reference will require a JOIN to make sense of the reference, a VARCHAR key will directly present the information.</p>\n\n<p>So, does anyone have experience with this particular use-case and the performance concerns associated with it?</p>\n"},{"tags":[".net","regex","performance","multiple","backreference"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":39,"score":0,"question_id":12925890,"title":"Optimization of multiple expressions to one","body":"<p><strong>BACKGROUND</strong></p>\n\n<p>I have a scenario where I must repeatedly find certain words in text, over and over.\nI have currently used a series of regular Expressions in a format like this...</p>\n\n<pre><code>\"((^)|(\\W))(?&lt;c&gt;Word1)((\\W)|($))\"\n\n\"((^)|(\\W))(?&lt;c&gt;NextWord)((\\W)|($))\"\n\n\"((^)|(\\W))(?&lt;c&gt;AnotherWord)((\\W)|($))\"\n</code></pre>\n\n<p>...</p>\n\n<p>This list of Regex objects is them looped through with a chunk of data and the matches are pulled out (one loop for one regex.matches(data) call)</p>\n\n<p>I have done everything I can to optimize them, such as Compiling them before hand.</p>\n\n<p>However the list is growing longer and I decided to start making larger compiled regular expressions to optimize the process. such as...</p>\n\n<pre><code>\"((^)|(\\W))(?&lt;c&gt;((Word1)|(NextWord)|(AnotherWord)))((\\W)|($))\"\n</code></pre>\n\n<p>This provides a HUGE speed imporvement, however there is a side effect I cannot figure out how to correct.</p>\n\n<p>When the words are in the data side by side (such as space delimited. eg. \"Word1 NextWord AnotherWord\") the second word is missed in the capture because the regex for \"Word1\" also includes the trailing space. The match that could occur for \"NextWord\" no longer has the Leading space because it's part of the previous match.</p>\n\n<p><strong>QUESTION</strong></p>\n\n<p>Can anyone alter this Regular expression (.net format)</p>\n\n<pre><code>Pattern = \"((^)|(\\W))(?&lt;c&gt;((Word1)|(NextWord)|(AnotherWord)))((\\W)|($))\"\n</code></pre>\n\n<p>to work to capture all the words in this list below with a single call to \".matches(data)\"\nWhere </p>\n\n<pre><code>data = \"Word1 NextWord AnotherWord\" \n</code></pre>\n\n<p>? (without sacrificing the efficiency gain)</p>\n\n<p><strong>RESULTS</strong></p>\n\n<p>Just thought I would mention this. After applying the suggested answer/correction with the look ahead and look behind, which I now know how to use :) , the code I just modified has improved in speed by 347x (0.00347% of old testing speed). Which is definitly something to remember when you get into multiple expressions. Very happy.</p>\n"},{"tags":["javascript","html","performance"],"answer_count":4,"favorite_count":3,"up_vote_count":8,"down_vote_count":1,"view_count":216,"score":7,"question_id":2661770,"title":"Downloading javascript Without Blocking","body":"<p>The context: My question relates to improving web-page loading performance, and in particular the effect that javascript has on page-loading (resources/elements below the script are blocked from downloading/rendering).</p>\n\n<p>This problem is usually avoided/mitigated by placing the scripts at the bottom (eg, just before the  tag).  </p>\n\n<p>The code i am looking at is for web analytics. Placing it at the bottom reduces its accuracy; and because this script has no effect on the page's content, ie, it's not re-writing any part of the page--i want to move it inside the head. Just how to do that without ruining page-loading performance is the crux.</p>\n\n<p>From my research, i've found six techniques (w/ support among all or most of the major browsers) for downloading scripts so that they don't block down-page content from loading/rendering:</p>\n\n<p><strong>(i)</strong> XHR + <em>eval()</em>;</p>\n\n<p><strong>(ii)</strong> XHR + <em>inject</em>;</p>\n\n<p><strong>(iii)</strong> download the HTML-wrapped script as in iFrame;</p>\n\n<p><strong>(iv)</strong> setting the script tag's <em>async</em> flag to TRUE (HTML 5 only); </p>\n\n<p><strong>(v)</strong> setting the script tag's <em>defer</em> attribute; and </p>\n\n<p><strong>(vi)</strong> 'Script DOM Element'.</p>\n\n<p>It's the last of these i don't understand. The javascript to implement the pattern (vi) is: </p>\n\n<pre><code>(function() {\n  var q1 = document.createElement('script');\n  q1.src = 'http://www.my_site.com/q1.js'\n  document.documentElement.firstChild.appendChild(q1)\n})();\n</code></pre>\n\n<p>Seems simple enough: and anonymous function is created then executed in the same block. Inside this anonymous function:</p>\n\n<ul>\n<li><p>a script element is created</p></li>\n<li><p>its <em>src</em> element is set to it's location, then</p></li>\n<li><p>the script element is added to the DOM</p></li>\n</ul>\n\n<p>But while each line is clear, it's still not clear to me <em>how exactly this pattern allows script loading without blocking down-page elements/resources from rendering/loading</em>?</p>\n"},{"tags":["sql","performance","postgresql","index","postgresql-performance"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":72,"score":2,"question_id":12923558,"title":"Index for a WHERE clause with datetime, and more","body":"<p>I'm using Postgres 9.1 and have a horribly slow performing query.</p>\n\n<h3>The Query:</h3>\n\n<pre><code>Explain Analyze SELECT COUNT(DISTINCT email) FROM \"invites\" WHERE (\n created_at &lt; '2012-10-10 21:08:05.259200'\n AND invite_method = 'email' \n AND accept_count = 0 \n AND reminded_count &lt; 3 \n AND (last_reminded_at IS NULL OR last_reminded_at &lt; '2012-10-10 21:08:05.261483'))\n</code></pre>\n\n<h3>Results:</h3>\n\n<pre><code>Aggregate  (cost=19828.24..19828.25 rows=1 width=21) (actual time=11395.903..11395.903 rows=1 loops=1)\n  -&gt;  Seq Scan on invites  (cost=0.00..18970.57 rows=343068 width=21) (actual time=0.036..353.121 rows=337143 loops=1)\n        Filter: ((created_at &lt; '2012-10-10 21:08:05.2592'::timestamp without time zone) AND (reminded_count &lt; 3) AND ((last_reminded_at IS NULL) OR (last_reminded_at &lt; '2012-10-10 21:08:05.261483'::timestamp without time zone)) AND ((invite_method)::text = 'email'::text) AND (accept_count = 0))\nTotal runtime: 11395.970 ms\n</code></pre>\n\n<p>As you can see this is taking about 11 seconds. How would I go about adding an index to optimize this queries performance?</p>\n"},{"tags":["c#","performance","entity-framework","sql-server-2008"],"answer_count":4,"favorite_count":1,"up_vote_count":8,"down_vote_count":0,"view_count":1279,"score":8,"question_id":4355474,"title":"How do I speed up DbSet.Add()?","body":"<p>I have to import about 30k rows from a CSV file to my SQL database, this sadly takes 20 minutes.</p>\n\n<p>Troubleshooting with a profiler shows me that <strong>DbSet.Add is taking the most time, but why?</strong></p>\n\n<p>I have these Entity Framework Code-First classes:</p>\n\n<pre><code>public class Article\n{\n    // About 20 properties, each property doesn't store excessive amounts of data\n}\n\npublic class Database : DbContext\n{\n    public DbSet&lt;Article&gt; Articles { get; set; }\n}\n</code></pre>\n\n<p>For each item in my for loop I do:</p>\n\n<pre><code>db.Articles.Add(article);\n</code></pre>\n\n<p>Outside the for loop I do:</p>\n\n<pre><code>db.SaveChanges();\n</code></pre>\n\n<p>It's connected with my local SQLExpress server,   but I guess there isn't anything written till SaveChanges is being called so I guess the server won't be the problem....</p>\n"},{"tags":["performance","hibernate","commit","flush"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":3196,"score":1,"question_id":5523101,"title":"Hibernate Performance flush v commit","body":"<p>Im creating a hibernate component to interact with large incoming data to persist, both save(create) and update data with volumes in the million of rows.</p>\n\n<p>I am aware of the main differences around flush v commit, for example flush syncing the \"dirty\" data into the persistable underlying data, and that flush allows you to sync with the underlying persistable data without actually committing so that the transaction can be rolled back if required. Commit essentially commits all persistable data to the database.</p>\n\n<p>Im creating a hibernate component to interact with large incoming data to persist, both save(create) and update data with volumes in the million of rows.</p>\n\n<p>I am aware of the main differences around flush v commit, for example flush syncing the \"dirty\" data into the persistable underlying data, and that flush allows you to sync with the underlying persistable data without actually committing so that the transaction can be rolled back if required. Commit essentially commits all persistable data to the database.</p>\n\n<p>Whats a reasonable size to do a batch insert? IS 50 the max amount for reasonable performance so something like:</p>\n\n<p>for (i &lt; 1000000)</p>\n\n<p>if(i % 50 ) {\nsession.flush()\n}</p>\n\n<p>I gather 50 should match the value in the hibernate.jdbc.batch_size 50</p>\n"},{"tags":["performance","internet-explorer","internet-explorer-8","profiling"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":580,"score":0,"question_id":2098870,"title":"The kinds of rendering in dynatrace~~","body":"<p>I use Dynatrace to find problems in my company site. I want to say,wow, this is a beautiful tool for page performance.</p>\n\n<p>But I found there are many kinds of rendering in dynatrace. For example:</p>\n\n<ol>\n<li>Calculating generic layout</li>\n<li>Calculating flow layout</li>\n<li>Scheduling layout task</li>\n</ol>\n\n<p>What's the difference between these?</p>\n"},{"tags":["performance","daemon","vala"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":64,"score":0,"question_id":12561695,"title":"Efficient daemon in Vala","body":"<p>i'd like to make a daemon in Vala which only executes a task every X seconds.\nI was wondering which would be the best way:</p>\n\n<ol>\n<li>Thread.usleep() or Posix.sleep()</li>\n<li>GLib.MainLoop + GLib.Timeout</li>\n<li>other?</li>\n</ol>\n\n<p>I don't want it to eat too many resources when it's doing nothing..</p>\n"},{"tags":["python","performance","memory-profiling"],"answer_count":4,"favorite_count":6,"up_vote_count":42,"down_vote_count":0,"view_count":9751,"score":42,"question_id":33978,"title":"Find out how much memory is being used by an object in Python","body":"<p>How would you go about finding out how much memory is being used by an object in python?</p>\n\n<p>I know it is possible to find out how much is used by a block of code, but not by an instantiated object anytime in its life, which is what I want.</p>\n\n<p>EDIT:\ndecided to go with the stats that task manager gives me. Based on the strong evidence that what I want to do is impossible without modifying python, both from fserb and from the heapy documentation.</p>\n"},{"tags":["memory","linux-kernel","driver","performance","kernel-programming"],"answer_count":4,"favorite_count":3,"up_vote_count":1,"down_vote_count":0,"view_count":2072,"score":1,"question_id":4452400,"title":"Memory access after ioremap very slow","body":"<p>I'm working on a Linux kernel driver that makes a chunk of physical memory available to user space. I have a working version of the driver, but it's currently very slow. So, I've gone back a few steps and tried making a small, simple driver to recreate the problem.</p>\n\n<p>I reserve the memory at boot time using the kernel parameter <code>memmap=2G$1G</code>. Then, in the driver's <code>__init</code> function, I <code>ioremap</code> some of this memory, and initialize it to a known value. I put in some code to measure the timing as well:</p>\n\n<pre><code>#define RESERVED_REGION_SIZE    (1 * 1024 * 1024 * 1024)   // 1GB\n#define RESERVED_REGION_OFFSET  (1 * 1024 * 1024 * 1024)   // 1GB\n\nstatic int __init memdrv_init(void)\n{\n    struct timeval t1, t2;\n    printk(KERN_INFO \"[memdriver] init\\n\");\n\n    // Remap reserved physical memory (that we grabbed at boot time)\n    do_gettimeofday( &amp;t1 );\n    reservedBlock = ioremap( RESERVED_REGION_OFFSET, RESERVED_REGION_SIZE );\n    do_gettimeofday( &amp;t2 );\n    printk( KERN_ERR \"[memdriver] ioremap() took %d usec\\n\", usec_diff( &amp;t2, &amp;t1 ) );\n\n    // Set the memory to a known value\n    do_gettimeofday( &amp;t1 );\n    memset( reservedBlock, 0xAB, RESERVED_REGION_SIZE );\n    do_gettimeofday( &amp;t2 );\n    printk( KERN_ERR \"[memdriver] memset() took %d usec\\n\", usec_diff( &amp;t2, &amp;t1 ) );\n\n    // Register the character device\n    ...\n\n    return 0;\n}\n</code></pre>\n\n<p>I load the driver, and check dmesg. It reports:</p>\n\n<pre><code>[memdriver] init\n[memdriver] ioremap() took 76268 usec\n[memdriver] memset() took 12622779 usec\n</code></pre>\n\n<p>That's 12.6 seconds for the memset. That means the memset is running at <em><strong>81 MB/sec</strong></em>. Why on earth is it so slow?</p>\n\n<p>This is kernel 2.6.34 on Fedora 13, and it's an x86_64 system.</p>\n\n<p>EDIT:</p>\n\n<p>The goal behind this scheme is to take a chunk of physical memory and make it available to both a PCI device (via the memory's bus/physical address) and a user space application (via a call to <code>mmap</code>, supported by the driver). The PCI device will then continually fill this memory with data, and the user-space app will read it out. If <code>ioremap</code> is a bad way to do this (as Ben suggested below), I'm open to other suggestions that'll allow me to get any large chunk of memory that can be directly accessed by both hardware and software. I can probably make do with a smaller buffer also.</p>\n\n<hr>\n\n<p>See my eventual solution below.</p>\n"},{"tags":["java","performance","jvm"],"answer_count":4,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":114,"score":6,"question_id":12864943,"title":"JAVA: reference defined within a loop","body":"<p>I'm just curious.<br>\nsuppose I define a reference inside a while/for loop.  </p>\n\n<p>does the JVM define this reference every iteration, or it's optimized to define it only once? </p>\n"},{"tags":["c++","c","performance","floating-point"],"answer_count":3,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":164,"score":3,"question_id":12920700,"title":"floating point conversions and performance","body":"<p>I am aware of the errors that can occur when doing conversions between floating point numbers and integers, but what about performance (please disregard the accuracy issues)?</p>\n\n<p>Does performance, in general, suffer if I do n-ary operations on operands of differing arithmetic types, that is, on differing floating point types (e.g. <code>float</code> and <code>double</code>) and floating point/integer type combinations (e.g. <code>float</code> and <code>int</code>)? Do there exist rules of thumb, such as, to keep all operands the same type?</p>\n\n<p>P.S.: I am asking because I'm writing an expression template library and would like to know whether to allow binary operations on vectors containing values of differing arithmetic types.</p>\n"},{"tags":["python","performance","memory","memory-management","writing"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":69,"score":0,"question_id":12923935,"title":"Python: improve memory efficiency of a script","body":"<p>I am using <a href=\"http://www.liblas.org/tutorial/python.html\" rel=\"nofollow\">Liblas</a> and <a href=\"http://code.google.com/p/pyshp/\" rel=\"nofollow\">shapefile</a> module to read *.las file and save a point shapefile.</p>\n\n<p>I have the following memory problem: when i arrive around 91% the memory is full and the script became really slow. I had try to figurate with a buffer or save in streaming way, but in the shapefile module i didn't find a solution to resolve this problem.</p>\n\n<p>thanks in advance for help and suggestions\nGianni </p>\n\n<pre><code>inFile =\"mypoints.las\"\noutFile =\"myshape.shp\"\n\ndef LAS2SHP(inFile,outFile):\n    w = shapefile.Writer(shapefile.POINT)\n    w.field('Z','C','10')\n    pbar = ProgressBar(len(lasfile.File(inFile,None,'r')))\n    i = 0\n    for p in lasfile.File(inFile,None,'r'):\n        i +=1\n        pbar.update(i)\n        w.point(p.x,p.y)\n        w.record(float(p.z))\n    w.save(outFile)\n</code></pre>\n"},{"tags":["java","performance","exception","checked","unchecked"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":90,"score":4,"question_id":12923302,"title":"Good practices for Java exceptions handling","body":"<p>I have some questions regarding handling exceptions in Java. I read a bit about it and got some contradicting guidelines.</p>\n\n<p><a href=\"http://onjava.com/pub/a/onjava/2003/11/19/exceptions.html?page=1\" rel=\"nofollow\">Best Practices for Exception Handling</a></p>\n\n<p>Let's go through the mentioned article:</p>\n\n<p>It states that one should generally avoid using checked exceptions <em>if \"Client code cannot do anything\"</em>. But what does it exactly mean? Is displaying error message in GUI sufficient reason for bubbling up checked exception? But it would force GUI programmer to remember to catch RuntimeExceptions and their descendants to display potential error info.</p>\n\n<p>Second view presented in this article is that one should evade inventing own exception classes unless I want to implement some customs field/methods in them. \nI generally disagree with this, my practice up today was just the opposite: I wrapped exceptions in my own exception structure to reflex goals realized by classes I write, even if they just extend Exception without adding any new methods. I think it helps to handle them more flexibly in the higher layers when thrown plus it's generally more clear and comprehensible for programmer who will use these classes.</p>\n\n<p>I implemented some code today 'new way' presented in the article throwing RuntimeException here and there, then I let <a href=\"http://www.sonarsource.org/\" rel=\"nofollow\">Sonar</a> analyze it. To confuse me even more Sonar marked my RuntimeExceptions as Major errors with a message like <em>\"Avoid throwing root type exceptions, wrap'em in your own types\".</em></p>\n\n<p>So it looks quite controversional, what do you think?</p>\n\n<p>I also heard from one of tech-leads today that just wrapping exceptions is bad, 'because it's a really costly operation for JVM'. For me, on the other side throwing SQLExceptions or IOExceptions everywhere looks like a bit of breaking encapsulation..</p>\n\n<p><strong>So what is your general attitude to questions I presented here?</strong> </p>\n\n<ol>\n<li><p><strong>When to wrap exceptions in my own types, when I shouldn't do this?</strong></p></li>\n<li><p><strong>Where is that point of <em>'client cannot do anything about this, throw\nruntime exception?</strong>'</em></p></li>\n<li><p><strong>What about performance issues?</strong></p></li>\n</ol>\n"},{"tags":["encryption","compression","performance","aes","zlib"],"answer_count":5,"favorite_count":3,"up_vote_count":13,"down_vote_count":0,"view_count":3314,"score":13,"question_id":4676095,"title":"When compressing and encrypting, should I compress first, or encrypt first?","body":"<p>If I were to AES-encrypt a file, and then ZLIB-compress it, would the compression be less efficient than if I first compressed and then encrypted?</p>\n\n<p>In other words, should I compress first or encrypt first, or does it matter?</p>\n"},{"tags":["ruby-on-rails-3","performance","passenger"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":24,"score":0,"question_id":12923373,"title":"Rails startup performance: how to trace and debug","body":"<p>Our Rails 3.2.8 app is taking around 30 seconds to load after restarting, with similar load time for <code>rails console</code> -- this is on an Amazon EC2 <code>m1.small</code> instance with Ubuntu 11, 64-bits.  The server seems perfectly suitable under load (we have several).  The startup time, which effectively occurs only after a deploy is a problem.</p>\n\n<p>Is there a way to get a timed trace of what is loading as the Rails instance loads?</p>\n"},{"tags":["c#","performance","reflection","expression-trees"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":71,"score":1,"question_id":12914990,"title":"GetCustomAttributes performance issue (expression trees is the solution??)","body":"<p>I have a performance problem because I use reflection and GetCustomAttributes for my data access. The performance profiler detected it. I have an extension method like this:</p>\n\n<pre><code>public static class DataRowExtensions\n{\n    /// &lt;summary&gt;\n    /// Maps DataRow objecto to entity T depending on the defined attributes. \n    /// &lt;/summary&gt;\n    /// &lt;typeparam name=\"T\"&gt;Entity to map.&lt;/typeparam&gt;\n    /// &lt;param name=\"rowInstance\"&gt;DataRow instance.&lt;/param&gt;\n    /// &lt;returns&gt;Instance to created entity.&lt;/returns&gt;\n    public static T MapRow&lt;T&gt;(this DataRow rowInstance) where T : class, new()\n    {\n        //Create T item\n        T instance = new T();\n\n        IEnumerable&lt;PropertyInfo&gt; properties = typeof(T).GetProperties();\n        MappingAttribute map;\n        DataColumn column;\n\n        foreach (PropertyInfo item in properties)\n        {\n            //check if custom attribute exist in this property\n            object[] definedAttributes = item.GetCustomAttributes(typeof(MappingAttribute), false);\n\n            // Tiene atributos\n            if (definedAttributes != null &amp;&amp; definedAttributes.Length == 1)\n            {\n                //recover first attribute\n                map = definedAttributes.First() as MappingAttribute;\n\n                column = rowInstance.Table.Columns.OfType&lt;DataColumn&gt;()\n                                          .Where(c =&gt; c.ColumnName == map.ColumnName)\n                                          .SingleOrDefault();\n\n                if (column != null)\n                {\n                    object dbValue = rowInstance[column.ColumnName];\n                    object valueToSet = null;\n\n                    if (dbValue == DBNull.Value)//if value is null\n                        valueToSet = map.DefaultValue;\n                    else\n                        valueToSet = dbValue;\n\n                    //Set value in property \n                    setValue&lt;T&gt;(instance, item, valueToSet);\n                }\n            }\n        }\n\n        return instance;\n    }\n\n    /// &lt;summary&gt;\n    /// Set \"item\" property.\n    /// &lt;/summary&gt;\n    /// &lt;typeparam name=\"T\"&gt;Return entity type&lt;/typeparam&gt;\n    /// &lt;param name=\"instance\"&gt;T type instance&lt;/param&gt;\n    /// &lt;param name=\"item\"&gt;Property name to return value&lt;/param&gt;\n    /// &lt;param name=\"valueToSet\"&gt;Value to set to the property&lt;/param&gt;\n    private static void setValue&lt;T&gt;(T instance, PropertyInfo item, object valueToSet) where T : class, new()\n    {\n        if (valueToSet == null)\n        {\n            CultureInfo ci = CultureInfo.InvariantCulture;\n\n            if (item.PropertyType.IsSubclassOf(typeof(System.ValueType)))\n            {\n                //if is a value type and is nullable\n                if (item.PropertyType.FullName.Contains(\"System.Nullable\"))\n                {\n                    item.SetValue(instance, null, BindingFlags.Public, null, null, ci);\n                }\n                else\n                {\n                    item.SetValue(instance, Activator.CreateInstance(item.PropertyType, null), BindingFlags.Public, null, null, ci);\n                }\n            }\n            else //property type is reference type\n            {\n                item.SetValue(instance, null, BindingFlags.Public, null, null, ci);\n            }\n        }\n        else // set not null value\n        {\n            //if is a value type and is nullable\n            if (item.PropertyType.FullName.Contains(\"System.Nullable\"))\n            {\n                item.SetValue(instance, Convert.ChangeType(valueToSet, Nullable.GetUnderlyingType(item.PropertyType)), null);\n            }\n            else\n            {\n                item.SetValue(instance, Convert.ChangeType(valueToSet, item.PropertyType), null);\n            }\n        }\n    }\n}\n</code></pre>\n\n<p>What I do here, in essence, is to map the domain entities with the database fields, and a data helper attacks the tables automatically. An example of one of these entities is:</p>\n\n<pre><code>public class ComboBox\n    {\n    /// &lt;summary&gt;\n    /// Represents a ComboBox item.\n    /// &lt;/summary&gt;\n    [Mapping(\"CODE\", DefaultValue = 0, DBType = DbParametersTypes.Varchar2, IsKey = true, IdentifierFK = \"\")]\n    public string Code { get; set; }\n\n    /// &lt;summary&gt;\n    /// Represents Text.\n    /// &lt;/summary&gt;\n    [Mapping(\"DESCRIPTION\", DefaultValue = \"\", DBType = DbParametersTypes.Varchar2, IsKey = false, IdentifierFK = \"\")]\n    public string Description { get; set; }\n\n    }\n</code></pre>\n\n<p>And the attribute class I use:</p>\n\n<pre><code>public sealed class MappingAttribute : Attribute\n    {\n        public string ColumnName { get; set; }\n\n        public object DefaultValue { get; set; }\n\n        public DbParametersTypes DBType { get; set; }\n\n        public bool IsKey { get; set; }\n\n        public string IdentifierFK { get; set; }\n\n        public bool IsParameter { get; set; } \n\n        public MappingAttribute(string columnName)\n        {\n            if (String.IsNullOrEmpty(columnName))\n                throw new ArgumentNullException(\"columnName\");\n\n            ColumnName = columnName;\n        }               \n    }\n</code></pre>\n\n<p>I read <a href=\"http://blogs.microsoft.co.il/blogs/alon_nativ/archive/2011/01/15/using-expression-trees-to-optimize-your-code.aspx\" rel=\"nofollow\">here</a> that a possible improvement could be an expression tree, but, first, I'm not an expression tress expert, and second, I have to solve this with .NET 3.5...(in the sample .NET 4 or 4.5 is used...)</p>\n\n<p>¿Suggestions?</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["c","performance","pointers","struct"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":102,"score":3,"question_id":12921758,"title":"Pointer to struct or struct itself?","body":"<p>Consider this code:</p>\n\n<pre><code>struct s { /* ... */ };\n\nvoid f(struct s x) { /* ... */) /* (1) */\n/* or */\nvoid f(const struct s *x) { /* ... */ } /* (2) */\n</code></pre>\n\n<p>When <code>struct s</code> has a decent size, in which case should we prefer the first form?</p>\n"},{"tags":["c++","c","performance","floating-point","arbitrary-precision"],"answer_count":4,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":287,"score":7,"question_id":11798640,"title":"Floats vs rationals in arbitrary precision fractional arithmetic (C/C++)","body":"<p>Since there are two ways of implementing an AP fractional number, one is to emulate the storage and behavior of the <code>double</code> data type, only with more bytes, and the other is to use an existing integer APA implementation for representing a fractional number as a rational i.e. as a pair of integers, numerator and denominator, which of the two ways are more likely to deliver efficient arithmetic in terms of performance? (Memory usage is really of minor concern.)</p>\n\n<p>I'm aware of the existing C/C++ libraries, some of which offer fractional APA with \"floats\" and other with rationals (none of them features fixed-point APA, however) and of course I could benchmark a library that relies on \"float\" implementation against one that makes use of rational implementation, but the results would largely depend on implementation details of those particular libraries I would have to choose randomly from the nearly ten available ones. So it's more <em>theoretical</em> pros and cons of the two approaches that I'm interested in (or three if take into consideration fixed-point APA).</p>\n"},{"tags":["java","eclipse","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":63,"score":0,"question_id":12920887,"title":"Eclipse Indigo Speed Issues","body":"<p>After using Eclipse Indigo for around a year, suddenly the speed of an application ran in Eclipse has decreased intensely. For example, my Java game was getting 64 FPS, then suddenly 1 FPS.</p>\n\n<p>Creating a new Workspace and adding all the files from the old project seems to fix it, but how would I fix this without making a new Workspace? (I've already tried deleting the Workspace's .metadata, made no difference.)</p>\n\n<p>I've also tried running Eclipse with the -clean argument, no help at all.\nNote that the performance issues are only when running a Java Application from Eclipse, not within Eclipse itself.</p>\n"},{"tags":["javascript","performance","application","documentation","drive"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":43,"score":0,"question_id":12920616,"title":"The execution speed is very slow Javascript for app google","body":"<p>I have a Javascript app development Drive, I use a spreadsheet to manage information with an interface, but the execution speed is very slow, is there way to improve performance?</p>\n"},{"tags":["sql","sql-server","performance","analytics"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":43,"score":2,"question_id":12920454,"title":"Select distinct group of measurements in a specified time point","body":"<p>I have a SQL table with a following structure:</p>\n\n<pre><code>CREATE TABLE dataLog (\ntstamp datetime NOT NULL, \nidentifier smallint NOT NULL, \npayload binary(46) NOT NULL, \nPRIMARY KEY (tstamp, identifier));\n</code></pre>\n\n<p>In this table logging infrastructure logs data for many devices. The device identifier ranges from eg. 1 to 125. That mean column \"identifier\" has values from 1-125. The payload column holds binary data from each device (logging information, temperatures etc.) The tstamp column holds current time information.</p>\n\n<p>How can one build a query to get a 'snapshot' information for each device in a given time point. E.g. I want to know what was the payload column value for each identifier (125) in eg. 2012-06-12 12:00:00. The data in the table were written when the device sends it and that's why the data are not with the exact timestamp given above but eg. 2 devices sent data on 2012-06-12 11:59:59, 10 devices on 2012-06-12 11:15:30 etc. The data should be detected backwards in time.</p>\n\n<p>The expected result: 125 rows with each identifier, timestamp of each measurement and payload value for each identifier.</p>\n\n<p>The data is needed to draw eg. a plot of the temperature across all of the devices from 1 to 125 at a given timestamp and next iterate through the data in eg. 5 minutes steps.</p>\n"},{"tags":["c#","performance","memory"],"answer_count":5,"favorite_count":0,"up_vote_count":10,"down_vote_count":0,"view_count":1881,"score":10,"question_id":1152573,"title":"Does \"readonly\" (C#) reduce memory usage?","body":"<p>In C#, does setting a field as readonly reduce memory usage?</p>\n\n<p>i.e.</p>\n\n<pre><code>DBRepository _db = new DBRepository();\n</code></pre>\n\n<p>vs</p>\n\n<pre><code>readonly DBRepository _db = new DBRepository();\n</code></pre>\n\n<p>Just curious. Thanks.</p>\n"},{"tags":["mysql","performance","innodb","sequence","uuid"],"answer_count":7,"favorite_count":7,"up_vote_count":19,"down_vote_count":0,"view_count":6670,"score":19,"question_id":2365132,"title":"UUID performance in MySQL?","body":"<p>We're considering using UUID values as primary keys for our MySQL database. The data being inserted is generated from dozens, hundreds, or even thousands of remote computers and being inserted at a rate of 100-40,000 inserts per second, and we'll never do any updates.</p>\n\n<p>The database itself will typically get to around 50M records before we start to cull data, so not a massive database, but not tiny either. We're also planing to run on InnoDB, though we are open to changing that if there is a better engine for what we're doing.</p>\n\n<p>We were ready to go with Java's Type 4 UUID, but in testing have been seeing some strange behavior. For one, we're storing as varchar(36) and I now realize we'd be better off using binary(16) - though how much better off I'm not sure.</p>\n\n<p>The bigger question is: how badly does this random data screw up the index when we have 50M records? Would we be better off if we used, for example, a type-1 UUID where the leftmost bits were timestamped? Or maybe we should ditch UUIDs entirely and consider auto_increment primary keys?</p>\n\n<p>I'm looking for general thoughts/tips on the performance of different types of UUIDs when they are stored as an index/primary key in MySQL. Thanks!</p>\n"},{"tags":["performance","sqlite"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":65,"score":1,"question_id":12677761,"title":"Sqlite appending data performance linear degradation, is this solvable?","body":"<p>I have a test set up to write rows to a database.\nEach transaction inserts 10,000 rows, no updates.\nEach step takes a linear time longer then the last.\nThe first ten steps took the following amount of time in ms to perform a commit</p>\n\n<p>568, 772, 942, 1247, 1717, 1906, 2268, 2797, 2922, 3816, 3945</p>\n\n<p>By the time it reaches adding 10,00 rows to a table of 500,000 rows, it takes 37149 ms to commit!</p>\n\n<ul>\n<li><p>I have no foreign key constraints.</p></li>\n<li><p>I have found using WAL, improves performance (gives figures above), but still linear degradation</p></li>\n<li><p>PRAGMA Synchronous=OFF has no effect</p></li>\n<li><p>PRAGMA locking_mode=EXCLUSIVE has no effect</p></li>\n<li><p>Ran with no additional indexes and additional indexes.  Made a roughly constant time difference, so was still a linear degradation.</p></li>\n</ul>\n\n<p>Some other settings I have</p>\n\n<ul>\n<li>setAutocommit(false)</li>\n<li>PRAGMA page_size = 4096</li>\n<li>PRAGMA journal_size_limit = 104857600</li>\n<li>PRAGMA count_changes = OFF</li>\n<li>PRAGMA cache_size = 10000</li>\n<li>Schema has Id INTEGER PRIMARY KEY ASC, insertion of which is incremental and generated by Sqlite</li>\n</ul>\n\n<p>Full Schema as follows (I have run both with and without indexes, but have included)</p>\n\n<pre><code>create table if not exists [EventLog] (\nId INTEGER PRIMARY KEY ASC, \nDocumentId TEXT NOT NULL, \nEvent TEXT NOT NULL, \nContent TEXT NOT NULL, \nTransactionId TEXT NOT NULL, \nDate INTEGER NOT NULL, \nUser TEXT NOT NULL)\n\ncreate index if not exists DocumentId ON EventLog (DocumentId)\n\ncreate index if not exists TransactionId ON EventLog (TransactionId)\n\ncreate index if not exists Date ON EventLog (Date)\n</code></pre>\n\n<p>This is using sqlite-jdbc-3.7.2 running in a windows environment</p>\n"},{"tags":["c#","performance","azure"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":93,"score":0,"question_id":12915585,"title":"azure queue performance","body":"<p>For the windows azure queues the scalability target per storage is supposed to be around 500 messages / second (<a href=\"http://msdn.microsoft.com/en-us/library/windowsazure/hh697709.aspx\" rel=\"nofollow\">http://msdn.microsoft.com/en-us/library/windowsazure/hh697709.aspx</a>). I have the following simple program that just writes a few messages to a queue. The program takes 10 seconds to complete (4 messages / second). I am running the program from inside a virtual machine (on west-europe) and my storage account also is located in west-europe. I don't have setup geo replication for my storage. My connection string is setup to use the http protocol.</p>\n\n<pre><code>       // http://blogs.msdn.com/b/windowsazurestorage/archive/2010/06/25/nagle-s-algorithm-is-not-friendly-towards-small-requests.aspx\n        ServicePointManager.UseNagleAlgorithm = false;\n\n        CloudStorageAccount storageAccount=CloudStorageAccount.Parse(ConfigurationManager.AppSettings[\"DataConnectionString\"]);\n\n        var cloudQueueClient = storageAccount.CreateCloudQueueClient();\n\n        var queue = cloudQueueClient.GetQueueReference(Guid.NewGuid().ToString());\n\n        queue.CreateIfNotExist();\n        var w = new Stopwatch();\n        w.Start();\n        for (int i = 0; i &lt; 50;i++ )\n        {\n            Console.WriteLine(\"nr {0}\",i);\n            queue.AddMessage(new CloudQueueMessage(\"hello \"+i));    \n        }\n\n        w.Stop();\n        Console.WriteLine(\"elapsed: {0}\", w.ElapsedMilliseconds);\n        queue.Delete();\n</code></pre>\n\n<p>Any idea how I can get better performance?</p>\n\n<p>EDIT:</p>\n\n<p>Based on Sandrino Di Mattia's answer I re-analyzed the code I've originally posted and found out that it was not complete enough to reproduce the error. In fact I had created a queue just before the call to ServicePointManager.UseNagleAlgorithm = false; The code to reproduce my problem looks more like this:</p>\n\n<pre><code>        CloudStorageAccount storageAccount=CloudStorageAccount.Parse(ConfigurationManager.AppSettings[\"DataConnectionString\"]);\n\n        var cloudQueueClient = storageAccount.CreateCloudQueueClient();\n\n        var queue = cloudQueueClient.GetQueueReference(Guid.NewGuid().ToString());\n\n        //ServicePointManager.UseNagleAlgorithm = false; // If you change the nagle algorithm here, the performance will be okay.\n        queue.CreateIfNotExist();\n        ServicePointManager.UseNagleAlgorithm = false; // TOO LATE, the queue is already created without 'nagle'\n        var w = new Stopwatch();\n        w.Start();\n        for (int i = 0; i &lt; 50;i++ )\n        {\n            Console.WriteLine(\"nr {0}\",i);\n            queue.AddMessage(new CloudQueueMessage(\"hello \"+i));    \n        }\n\n        w.Stop();\n        Console.WriteLine(\"elapsed: {0}\", w.ElapsedMilliseconds);\n        queue.Delete();\n</code></pre>\n\n<p>The suggested solution from Sandrino to configure the ServicePointManager using the app.config file has the advantage that the ServicePointManager is initialized when the application starts up, so you don't have to worry about time dependencies.</p>\n"},{"tags":["performance","oracle","materialized-views"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":4,"view_count":43,"score":-4,"question_id":12911392,"title":"Fast refresh taking longer than complete refresh","body":"<p>Which one is better complete or fast?</p>\n\n<pre><code> create materialized view mv_sales_at_point\n    build immediate\n    refresh fast on demand\n    enable query rewrite\n    as\n    select  year ,month ,week,t.company_id,t.branch_id,t.customer_id,\n            sum(total_sales_txn),sum(total_spend),sum(total_discount)\n    from txn_agg1 t \n    group by year,month,week,t.company_id,t.branch_id,t.customer_id;\n</code></pre>\n\n<p>My Base Table is TXN_AGG1 Table having 285 Rows.\nComplete Refresh Takes  0.219 Seconds   and Fast refresh take 0.531 seconds.\nWhich one is better to use, since this this table (TXN_AGG1) will grow eventually?</p>\n"},{"tags":["javascript","html","performance","social-networking","webpage"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":52,"score":1,"question_id":12916045,"title":"The purpose of loading External Javascript of social media into webpages","body":"<p>When adding social media resources in webpage, the traditional method results in loading much external JS from other domains, just to load an iframe or an anchor with a brand image background. Probably the below are the most transferred files over the web   (after the <a href=\"http://www.google-analytics.com/ga.js\" rel=\"nofollow\">http://www.google-analytics.com/ga.js</a> 36.35KB, which is somehow inevitable for many)</p>\n\n<p><a href=\"http://connect.facebook.net/en_US/all.js\" rel=\"nofollow\">http://connect.facebook.net/en_US/all.js</a> 181.30KB (59.06KB gzipped)<br>\n<a href=\"https://platform.twitter.com/widgets.js\" rel=\"nofollow\">https://platform.twitter.com/widgets.js</a> 75.19KB (24.42KB gzipped)<br>\n<a href=\"https://apis.google.com/js/plusone.js\" rel=\"nofollow\">https://apis.google.com/js/plusone.js</a> 16.71KB<br>\n<a href=\"http://assets.pinterest.com/js/pinit.js\" rel=\"nofollow\">http://assets.pinterest.com/js/pinit.js</a> (well this is small, but still unneeded connection)  </p>\n\n<blockquote>\n  <p>For example, <a href=\"http://connect.facebook.net/en_US/all.js\" rel=\"nofollow\">http://connect.facebook.net/en_US/all.js</a> does only one\n  thing: adding an iFrame <code>&lt;iframe src=\"//www.facebook.com/plugins/likebox.php?href=http%3A%2F%2Fwww.facebook.com%2Flavishdream&amp;amp;width=292&amp;amp;height=180&amp;amp;colorscheme=light&amp;amp;show_faces=true&amp;amp;border_color&amp;amp;stream=false&amp;amp;header=false\" scrolling=\"no\" frameborder=\"0\" style=\"border:none; overflow:hidden; width:292px; height:180px;\" allowtransparency=\"true\"&gt;&lt;/iframe&gt;</code></p>\n  \n  <p>Twitter and Google Plus scripts does very similar tasks, only adding\n  small HTML chunks into the page.</p>\n</blockquote>\n\n<p>Why not only writing those iFrames, images and anchors HTML ?</p>\n"},{"tags":["performance","media","player"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":12916451,"title":"Fast Start Play WMP","body":"<p>I am developing an application in C# that has a form with Windows Media Player. </p>\n\n<p>There is an event called \"Buffering\" in AXWindowsMediaPlayer class that signals when media player finishes buffering the content. But I could not achieve this with a single \nAXWindowsMediaPlayer object. Whatever I did, I could not continue to play the \nfirst content while buffering the second.</p>\n\n<p>I want to, Fast Start and buffering disabled for local videos.</p>\n\n<pre><code>private void button1_Click(object sender, EventArgs e)\n{\ntimer1.Interval = 100;\ntimer1.Enabled = true;\ntimer1.Start();\naxWindowsMediaPlayer1.URL=(@\"d:\\\\adobes\\\\3.avi\");\naxWindowsMediaPlayer1.Ctlcontrols.play();\n}\n\nprivate void timer1_Tick(object sender, EventArgs e)\n{\n// nearest whole number.\ndouble t = Math.Round(axWindowsMediaPlayer1.Ctlcontrols.curre ntPosition,2);\n\n\nlabel1.Text = (t.ToString());\nif (t &gt; 5 )\n{\n\naxWindowsMediaPlayer1.URL = (@\"d:\\\\adobes\\\\2.avi\");\n\n}\n\n\n\n}\n</code></pre>\n"},{"tags":[".net","winforms","performance","events","treeview"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":55,"score":3,"question_id":12915525,"title":".NET Treeview: How to prevent unchanged child nodes from being redrawn?","body":"<p>I'm working with WinForms in .NET 2010. I've created a user control that inherits from Treeview and I'm using owner-drawing for the text part of my treenodes.</p>\n\n<p>Now I had to solve a strange performance problem:</p>\n\n<p>When the text of a treenode (I call it \"parentnode\") gets changed, the treeview control fires the DrawNode-event for each of \"parentnodes\"'s child nodes, whether they are visible or not!!!</p>\n\n<p>This is causing a big performance problem for my application. How can I prevent the treeview control from firing the DrawNode event for each child node?</p>\n\n<p>Thx a lot in advance for you help!</p>\n"},{"tags":["xml","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":3,"view_count":33,"score":-3,"question_id":12915365,"title":"Performance for an XML","body":"<p>I have an xml that I will need to very frequently update. What is teh best way to achieve this such that performance of the system does not go for a toss? My xml size would not be too big, maybe 5-10 KB. Only thing is that at every statement in the xml, I will need to edit something and persist it back. Any suggestions how this can be done?</p>\n"},{"tags":["jquery","performance","code-efficiency"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":46,"score":0,"question_id":12911543,"title":"What is the most efficient way to select a fallback element if original element is not present in DOM with jQuery?","body":"<p>In a jQuery Plugin I am writing, I want to check for specific elements inside the DOM object the plugin is called on. The code looks kind of like this: </p>\n\n<pre><code>//the object that you call the plugin on, stored in the variable \"o\"\no = this;\n\n//Store children of a child element inside \"o\" in the variable \"elInsideO\"\nelInsideO = o.find('selector').children('childrenSelector');\n\n/* check if elInsideO is empty in case 'selector' is not present and then look for \nalternative element 'selector2' */\nif (elInsideO.length == 0) {\n    elInsideO = o.find('selector2').children('childrenSelector');\n}\n</code></pre>\n\n<p>Is there a more efficent way to make this selection? One more possibility I can think of is this: </p>\n\n<pre><code>if (o.find('selector').length != 0) {\n    elInsideO = o.find('selector').children('childrenSelector');\n} else {\n    elInsideO = o.find('selector2').children('childrenSelector');\n}\n</code></pre>\n\n<p>Which one of these solutions is more efficient (meaning performs better)? Is there another way that is even better?</p>\n\n<p>Thx for any help!</p>\n"},{"tags":["ruby-on-rails","performance","activerecord","asynchronous","future"],"answer_count":1,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":123,"score":5,"question_id":12900121,"title":"Is it possible to transparently implement the Future pattern for ActiveRecord queries in Ruby 1.9?","body":"<p>I'm working on an existing Rails 2 site with a large codebase that recently updated to Ruby 1.9.2 and the mysql2 gem.  I've noticed that this setup allows for non-blocking database queries; you can do <code>client.query(sql, :async =&gt; true)</code> and then later call <code>client.async_result</code>, which blocks until the query completes.</p>\n\n<p>It seems to me that we could get a performance boost by having all <code>ActiveRecord</code> queries that return a collection decline to block until a method is called on the collection.  e.g.</p>\n\n<pre><code>@widgets = Widget.find(:all, :conditions=&gt; conditions) #sends the query\ndo_some_stuff_that_doesn't_require_widgets\n@widgets.each do #if the query hasn't completed yet, wait until it does, then populate @widgets with the result. Iterate through @widgets\n...\n</code></pre>\n\n<p>This could be done by monkey-patching <code>Base::find</code> and its related methods to create a new database client, send the query asynchronously, and then immediately return a Delegator or other proxy object that will, when any method is called on it, call <code>client.async_result</code>, instantiate the result using <code>ActiveRecord</code>, and delegate the method to that.  <code>ActiveRecord</code> association proxy objects already work similarly to implement ORM.</p>\n\n<p>I can't find anybody who's done this, though, and it doesn't seem to be an option in any version of Rails.  I've tried implementing it myself and it works in console (as long as I append <code>; 1</code> to the line calling everything so that <code>to_s</code> doesn't get called on the result). But it seems to be colliding with all sorts of other magic and creating various problems.</p>\n\n<p>So, is this a bad idea for some reason I haven't thought of? If not, why isn't it the way <code>ActiveRecord</code> already works? Is there a clean way to make it happen?</p>\n"},{"tags":["java","performance","java-3d","netbeans-platform"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":69,"score":0,"question_id":12693924,"title":"How to display Java3D image faster in NetBeans platform app?","body":"<p>I made a desktop app with the NetBeans platform using Java Swing technology. In that app I do image processing on a 3D image using PointArray[] objects.</p>\n\n<p>When my app runs on a PC which has windows 7, a graphics card, and good RAM capacity (4GB RAM), my 3D image is created and displayed within  5 to 6 sec after clicking on the 3DImage button. But when my app runs on a Windows XP, low graphics configuration PC, my 3D image takes up to three minutes to render or some time it doesn't display image. So how can I solve that problem?</p>\n\n<p>My code is below.</p>\n\n<pre><code>import com.sun.j3d.utils.behaviors.mouse.MouseRotate;\nimport com.sun.j3d.utils.behaviors.mouse.MouseWheelZoom;\nimport com.sun.j3d.utils.universe.SimpleUniverse;\nimport java.awt.BorderLayout;\nimport java.awt.GraphicsConfiguration;\nimport java.awt.image.BufferedImage;\nimport javax.media.j3d.Appearance;\nimport javax.media.j3d.BoundingSphere;\nimport javax.media.j3d.BranchGroup;\nimport javax.media.j3d.Canvas3D;\nimport javax.media.j3d.ColoringAttributes;\nimport javax.media.j3d.GeometryArray;\nimport javax.media.j3d.PointArray;\nimport javax.media.j3d.PointAttributes;\nimport javax.media.j3d.Shape3D;\nimport javax.media.j3d.TransformGroup;\nimport javax.media.jai.JAI;\nimport javax.media.jai.PlanarImage;\nimport javax.swing.JButton;\nimport javax.swing.JFrame;\nimport javax.swing.JPanel;\nimport javax.swing.JScrollPane;\nimport javax.vecmath.Color3f;\nimport javax.vecmath.Point3d;\nimport javax.vecmath.Point3f;\n\n\npublic final class FinalDImage extends JPanel {\n\n    private static int s = 0, count = 0, r = 0, g = 0, b = 0, medHeight = 0, medWidth = 0;\n    private static float divisior1 = 300000.0f;\n    private static Shape3D plShape;\n    private static TransformGroup objRotate;\n    private static BranchGroup scene;\n    private static JButton btn;\n    private static Canvas3D canvas3D;\n    private static SimpleUniverse simpleU;\n    private static GraphicsConfiguration gc;\n    private static BufferedImage histImage;\n    private static JPanel jPanel1 = new JPanel();\n\n    public FinalDImage(BufferedImage histImage) {\n        FinalDImage.histImage = histImage;\n        medWidth = (int) (histImage.getWidth() / 2.0f);\n        medHeight = (int) (histImage.getHeight() / 2.0f);\n        this.add(jPanel1);\n        initComponents();\n    }\n\n    public void initComponents() {\n        setLayout(new BorderLayout());\n        btn = new JButton(\"Intensity\");\n        gc = SimpleUniverse.getPreferredConfiguration();\n        canvas3D = new Canvas3D(gc);//See the added gc? this is a preferred config\n        add(\"Center\", canvas3D);\n        add(btn, BorderLayout.SOUTH);\n        scene = createSceneGraph(FinalDImage.histImage);\n        scene.setCapability(BranchGroup.ALLOW_DETACH);\n        scene.compile();\n        simpleU = new SimpleUniverse(canvas3D);\n        simpleU.getViewingPlatform().setNominalViewingTransform();\n        simpleU.addBranchGraph(scene);\n    }\n\n    public BranchGroup createSceneGraph(BufferedImage histImage) {\n        count = 0;\n        BranchGroup lineGroup = new BranchGroup();\n        Appearance app = new Appearance();\n        ColoringAttributes ca = new ColoringAttributes(new Color3f(204.0f, 204.0f, 204.0f), ColoringAttributes.SHADE_FLAT);\n        app.setColoringAttributes(ca);\n\n        Point3f plaPts;\n        Color3f color;\n        PointArray pla = new PointArray(histImage.getWidth() * histImage.getHeight(), GeometryArray.COLOR_3 | GeometryArray.COORDINATES);\n\n        if (histImage.getType() == 11) {\n            for (int i = histImage.getWidth() - 3; i &gt;= 3; i--) {\n                for (int j = histImage.getHeight() - 3; j &gt;= 3; j--) {\n                    s = histImage.getRaster().getSample(i, j, 0);\n                    plaPts = new Point3f((medWidth - i) / 1500.0f,\n                                         (medHeight - j) / 1500.0f,\n                                         s / divisior1);\n                    color = new Color3f(s / 60000.0f, s / 60000.0f, s / 60000.0f);\n                    pla.setCoordinate(count, plaPts);\n                    pla.setColor(count, color);\n                    count++;\n                }\n            }\n        } else {\n            for (int i = 2; i &lt; histImage.getWidth() - 2; i++) {\n                for (int j = 2; j &lt; histImage.getHeight() - 2; j++) {\n                    r = histImage.getRaster().getSample(i, j, 0);\n                    g = histImage.getRaster().getSample(i, j, 1);\n                    b = histImage.getRaster().getSample(i, j, 2);\n                    s = (r + g + b) / 3;\n                    plaPts = new Point3f((medWidth - i) / 1200.0f,\n                                         (medHeight - j) / 1200.0f,\n                                         s / (divisior1/600.0f));\n                    color = new Color3f(r / 300.0f, g / 300.0f, b / 300.0f);\n                    pla.setCoordinate(count, plaPts);\n                    pla.setColor(count, color);\n                    count++;\n                }\n            }\n        }\n\n\n        PointAttributes a_point_just_bigger = new PointAttributes();\n        a_point_just_bigger.setPointSize(4.0f);//10 pixel-wide point\n        a_point_just_bigger.setPointAntialiasingEnable(true); //now points are sphere-like (not a cube)\n        app.setPointAttributes(a_point_just_bigger);\n        plShape = new Shape3D(pla,app);\n        objRotate = new TransformGroup();\n        objRotate.setCapability(TransformGroup.ALLOW_TRANSFORM_WRITE);\n        objRotate.addChild(plShape);\n        lineGroup.addChild(objRotate);\n        MouseRotate myMouseRotate = new MouseRotate();\n        myMouseRotate.setFactor(0.015, 0.015);\n        myMouseRotate.setTransformGroup(objRotate);\n        myMouseRotate.setSchedulingBounds(new BoundingSphere());\n        myMouseRotate.setSchedulingBounds(new BoundingSphere(\n                new Point3d(0.0, 0.0, 0.0), 100));\n\n        lineGroup.addChild(myMouseRotate);\n        MouseWheelZoom mz = new MouseWheelZoom();\n        mz.setFactor(0.01);\n        mz.setTransformGroup(objRotate);\n        mz.setSchedulingBounds(new BoundingSphere());\n        lineGroup.addChild(mz);\n        return lineGroup;\n    }\n\n    public static void main(String[] args) {\n        PlanarImage plImg3 = JAI.create(\"fileload\", \"E:\\\\Data\\\\office\\\\teeth3.tiff\");\n        BufferedImage histImage = plImg3.getAsBufferedImage();\n        JFrame frame = new JFrame();\n        frame.add(new JScrollPane(new FinalDImage(histImage)));\n        frame.setSize(800, 700);\n        frame.setVisible(true);\n        frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\n    }\n}\n</code></pre>\n"},{"tags":["performance","amazon-s3"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":38,"score":0,"question_id":12914307,"title":"Number of objects in Amazon S3 (most efficient way)","body":"<p>Right now I am migrating to Amazon S3 from standard server. In current server, I have to set structure of directories in order to stay more efficient. For example, current structure of directory is: category_id/article_id/year/month/day/article_image.jpg and so on. </p>\n\n<p>So, do I need to create such structure in the bucket of Amazon S3. Does it affect speed of requests? </p>\n"},{"tags":["ajax","performance","magento"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":46,"score":0,"question_id":12914084,"title":"How to improve Magento Ajax Performance?","body":"<p>I'm currently developing an ajax plugin for Magento and wonder how I can improve it's performance. Let's start with an example. I want to show the number of items in the shopping cart on an external page. Or a page that has been rendered via full page caching and needs a to update the cart info via a JavaScript Ajax request. </p>\n\n<p><em>For that I see two ways of doing it.</em> </p>\n\n<p><strong>Making use of a Magento Controller stripped down to the minimum</strong></p>\n\n<pre><code>class AFCustom_CartInfo_AjaxController extends Mage_Core_Controller_Front_Action {\n\n    public function indexAction() {\n      header('Cache-Control: no-cache, must-revalidate');\n      header('Expires: Mon, 26 Jul 1997 05:00:00 GMT');\n      header('Content-Type: text/html; charset=utf-8');\n\n      $out = $this-&gt;__('My Cart');\n      if (Mage::helper('core')-&gt;isModuleOutputEnabled('Mage_Checkout')) {\n        $count = Mage::getSingleton('checkout/cart')-&gt;getSummaryQty() ? Mage::getSingleton('checkout/cart')-&gt;getSummaryQty()\n            : MAGE::helper('checkout/cart')-&gt;getSummaryCount();\n        if ($count == 1) {\n          $out = $this-&gt;__('My Cart (%s item)', $count);\n        } elseif ($count &gt; 0) {\n          $out = $this-&gt;__('My Cart (%s items)', $count);\n        } else {\n          $out = $this-&gt;__('My Cart');\n        }\n      }\n      echo $out;\n      exit;\n    }\n}\n</code></pre>\n\n<p>As you can see I exit the code in the controller and don't user render layout. However the requests are still taking quite long. I assume because it has to load the whole Framework. Are there ways to just load the minimum requirements of Magento to execute the controller? Would it be able to disable the layout engine as it is not needed? How would I do that? What other modules could I switch off?</p>\n\n<p><strong>Observer to Update Session</strong>\nA second solution would be to have an observer to listen for changes in the shopping cart (checkout_cart_save_after) and update an PHP session variable. Which could be read out in a small custom PHP script. However, I'm not sure how I could easily patch into the same session mechanism as Magento uses? I figure that it might not be advisable to directly use $_SESSION as an Magento session might be handled elsewhere. </p>\n\n<p>What would you do? Any pointers are appreciated? </p>\n\n<p>Many thanks!</p>\n"},{"tags":["c++","perl","optimization","performance"],"answer_count":20,"favorite_count":17,"up_vote_count":58,"down_vote_count":5,"view_count":10637,"score":53,"question_id":885908,"title":"while (1) Vs. for (;;) Is there a speed difference?","body":"<p>Long version...</p>\n\n<p>A co-worker asserted today after seeing my use of <code>while (1)</code> in a Perl script that <code>for (;;)</code> is faster.  I argued that they should be the same hoping that the interpreter would optimize out any differences. I set up a script that would run 1,000,000,000 for loop iterations and the same number of while loops and record the time between. I could find no appreciable difference. My co-worker said that a professor had told him that the <code>while (1)</code> was doing a comparison <code>1 == 1</code> and the <code>for (;;)</code> was not.  We repeated the same test with the 100x the number of iterations with C++ and the difference was negligible. It was however a graphic example of how much faster compiled code can be vs. a scripting language.</p>\n\n<p>Short version...</p>\n\n<p>Is there any reason to prefer a <code>while (1)</code> over a <code>for (;;)</code> if you need an infinite loop to break out of?</p>\n\n<p><strong>Note:</strong> If it's not clear from the question.  This was purely a fun academic discussion between a couple of friends.  I am aware this is not a super important concept that all programmers should agonize over.  Thanks for all the great answers I (and I'm sure others) have learned a few things from this discussion.</p>\n\n<p><strong>Update:</strong> The aforementioned co-worker weighed in with a response below.</p>\n\n<p>Quoted here in case it gets buried.</p>\n\n<blockquote>\n  <p>It came from an AMD assembly programmer. He stated that C programmers\n  (the poeple) don't realize that their code has inefficiencies. He said\n  today though, gcc compilers are very good, and put people like him out\n  of business. He said for example, and told me about the <code>while 1</code> vs\n  <code>for(;;)</code>. I use it now out of habit but gcc and especially interpreters\n  will do the same operation (a processor jump) for both these days,\n  since they are optimized.</p>\n</blockquote>\n"},{"tags":["asp.net","asp.net-mvc","performance","asp.net-webforms"],"answer_count":13,"favorite_count":27,"up_vote_count":73,"down_vote_count":1,"view_count":18813,"score":72,"question_id":43743,"title":"ASP.NET MVC Performance","body":"<p>I found some wild remarks that ASP.NET MVC is 30x faster than ASP.NET WebForms. What real performance difference is there, has this been measured and what are the performance benefits.</p>\n\n<p>This is to help me consider moving from ASP.NET WebForms to ASP.NET MVC.</p>\n"},{"tags":["mysql","performance","query"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":48,"score":2,"question_id":12912890,"title":"Simple MySQL query with performance issues","body":"<p>I have the following simple MySQL query:</p>\n\n<pre><code>SELECT SQL_NO_CACHE mainID\nFROM tableName \nWHERE otherID3=19\nAND dateStartCol &gt;= '2012-08-01' \nAND dateStartCol &lt;= '2012-08-31';\n</code></pre>\n\n<p>When I run this it takes 0.29 seconds to bring back 36074 results. When I increase my date period to bring back more results (65703) it runs in 0.56. When I run other similar SQL queries on the same server but on different tables (some tables are larger) the results come back in approximately 0.01 seconds.</p>\n\n<p>Although 0.29 isn't slow - this is a basic part for a complex query and this timing means that it is not scalable.</p>\n\n<p>See below for the table definition and indexes.  </p>\n\n<p>I know it's not server load as I have the same issue on a development server which has very little usage.</p>\n\n<pre><code>+---------------------------+--------------+------+-----+---------+----------------+\n| Field                     | Type         | Null | Key | Default | Extra          |\n+---------------------------+--------------+------+-----+---------+----------------+\n| mainID                    | int(11)      | NO   | PRI | NULL    | auto_increment |\n| otherID1                  | int(11)      | NO   | MUL | NULL    |                |\n| otherID2                  | int(11)      | NO   | MUL | NULL    |                |\n| otherID3                  | int(11)      | NO   | MUL | NULL    |                |\n| keyword                   | varchar(200) | NO   | MUL | NULL    |                |\n| dateStartCol              | date         | NO   | MUL | NULL    |                |\n| timeStartCol              | time         | NO   | MUL | NULL    |                |\n| dateEndCol                | date         | NO   | MUL | NULL    |                |\n| timeEndCol                | time         | NO   | MUL | NULL    |                |\n| statusCode                | int(1)       | NO   | MUL | NULL    |                |\n| uRL                       | text         | NO   |     | NULL    |                |\n| hostname                  | varchar(200) | YES  | MUL | NULL    |                |\n| IPAddress                 | varchar(25)  | YES  |     | NULL    |                |\n| cookieVal                 | varchar(100) | NO   |     | NULL    |                |\n| keywordVal                | varchar(60)  | NO   |     | NULL    |                |\n| dateTimeCol               | datetime     | NO   | MUL | NULL    |                |\n+---------------------------+--------------+------+-----+---------+----------------+\n\n\n+--------------------+------------+-------------------------------+--------------+---------------------------+-----------+-------------+----------+--------+------+------------+---------+\n| Table              | Non_unique | Key_name                      | Seq_in_index | Column_name               | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment |\n+--------------------+------------+-------------------------------+--------------+---------------------------+-----------+-------------+----------+--------+------+------------+---------+\n| tableName          |          0 | PRIMARY                       |            1 | mainID                    | A         |      661990 |     NULL | NULL   |      | BTREE      |         |\n| tableName          |          1 | idx_otherID1                  |            1 | otherID1                   | A         |      330995 |     NULL | NULL   |      | BTREE      |         |\n| tableName          |          1 | idx_otherID2                  |            1 | otherID2                   | A         |          25 |     NULL | NULL   |      | BTREE      |         |\n| tableName          |          1 | idx_otherID3                  |            1 | otherID3                   | A         |          48 |     NULL | NULL   |      | BTREE      |         |\n| tableName          |          1 | idx_dateStartCol              |            1 | dateStartCol               | A         |         187 |     NULL | NULL   |      | BTREE      |         |\n| tableName          |          1 | idx_timeStartCol              |            1 | timeStartCol               | A         |       73554 |     NULL | NULL   |      | BTREE      |         |\n|tableName          |          1 | idx_dateEndCol                 |            1 | dateEndCol                 | A         |         188 |     NULL | NULL   |      | BTREE      |         |\n|tableName          |          1 | idx_timeEndCol                 |            1 | timeEndCol                 | A         |       73554 |     NULL | NULL   |      | BTREE      |         |\n| tableName          |          1 | idx_keyword                   |            1 | keyword                    | A         |       82748 |     NULL | NULL   |      | BTREE      |         |\n| tableName           |          1 | idx_hostname                 |            1 | hostname                   | A         |        2955 |     NULL | NULL   | YES  | BTREE      |         |\n| tableName           |          1 | idx_dateTimeCol              |            1 | dateTimeCol                | A         |      220663 |     NULL | NULL   |      | BTREE      |         |\n| tableName           |          1 | idx_statusCode               |            1 | statusCode                 | A         |           2 |     NULL | NULL   |      | BTREE      |         |\n+--------------------+------------+-------------------------------+--------------+---------------------------+-----------+-------------+----------+--------+------+------------+---------+\n</code></pre>\n\n<p>Explain Output:</p>\n\n<pre><code>+----+-------------+-----------+-------+----------------------------------+-------------------+---------+------+-------+----------+-------------+\n| id | select_type | table     | type  | possible_keys                    | key               | key_len | ref  | rows  | filtered | Extra       |\n+----+-------------+-----------+-------+----------------------------------+-------------------+---------+------+-------+----------+-------------+\n|  1 | SIMPLE      | tableName | range | idx_otherID3,idx_dateStartCol | idx_dateStartCol | 3       | NULL | 66875 |    75.00 | Using where |\n+----+-------------+-----------+-------+----------------------------------+-------------------+---------+------+-------+----------+-------------+\n</code></pre>\n"},{"tags":["asp.net-mvc","database","performance","design-patterns","architecture"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":44,"score":0,"question_id":12913249,"title":"Some approach to work with big data cases","body":"<p>At the company that I work we have some problems with performance loading the data of database.\nOur ERP works with a big database and we need to make some complex queries.\nWe are using C# following the DDD pattern and in the front-end we are using ASP.NET MVC.</p>\n\n<p>The example that I'm talking is that:\nWe have alot of lists (grids) in the system where the data is loaded by ajax. But everytime that a user enter in some page like that (an html page), we need to make a query in the database.</p>\n\n<p>Its important to say that this data aren't only for \"query\". This data is constantly changed by the users.</p>\n\n<p>My doubt is if you have any sugestion to how we can minimize this problem. I already have searching some solutions of cache in the server side, but I don't have any experience working with cache in high complex scenarios.</p>\n\n<p>Very thanks,\nRenan Cunha.</p>\n"},{"tags":["c++","performance","huffman"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":993,"score":2,"question_id":807979,"title":"Efficient Huffman tree search while remembering path taken","body":"<p>As a follow up question related to my <a href=\"http://stackoverflow.com/questions/759707/efficient-way-of-storing-huffman-tree\">question regarding efficient way of storing huffman tree's</a> I was wondering what would be the fastest and most efficient way of searching a binary tree (based on the Huffman coding output) and storing the path taken to a particular node.</p>\n\n<p>This is what I currently have:</p>\n\n<ul>\n<li>Add root node to queue</li>\n<li>while queue is not empty, pop item off queue\n<ul>\n<li>check if it is what we are looking\n<ul>\n<li>yes:\nFollow a head pointer back to the root node, while on each node we visit checking whether it is the left or right and making a note of it.</li>\n<li>break out of the search</li>\n</ul></li>\n<li>enqueue left, and right node</li>\n</ul></li>\n</ul>\n\n<p>Since this is a Huffman tree, all of the entries that I am looking for will exist. The above is a breadth first search, which is considered the best for Huffman trees since items that are in the source more often are higher up in the tree to get better compression, however I can't figure out a good way to keep track of how we got to a particular node without backtracking using the head pointer I put in the node.</p>\n\n<p>In this case, I am also getting all of the right/left paths in reverse order, for example, if we follow the head to the root, and we find out that from the root it is right, left, left, we get left, left, right. or 001 in binary, when what I am looking for is to get 100 in an efficient way.</p>\n\n<p>Storing the path from root to the node as a separate value inside the node was also suggested, however this would break down if we ever had a tree that was larger than however many bits the variable we created for that purpose could hold, and at that point storing the data would also take up huge amounts of memory.</p>\n"},{"tags":["java","performance","methods"],"answer_count":3,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":75,"score":1,"question_id":12912543,"title":"How to know if method is N or N^2","body":"<p>I often see you guys talking about N methods and N^2 methods, which, correct me if I'm wrong, indicate how fast a method is. My question is: how do you guys know which methods are N and which are N^2? And also: are there other speed indications of methods then just N and N^2?</p>\n"},{"tags":["performance","glassfish","monitoring"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":18,"score":0,"question_id":12912495,"title":"What is the performance impact for GlassFish 2.1 of asadmin get --monitor command?","body":"<p>We are monitoring GlassFish by setting the monitoring levels of monitored elements to LOW or HIGH:</p>\n\n<pre><code>asadmin set server.monitoring-service.module-monitoring-levels.jvm=LOW\nasadmin set server.monitoring-service.module-monitoring-levels.http-service=HIGH\n</code></pre>\n\n<p>And then we use <code>asadmin</code> command to gather the metrics during one hour with a 10-seconds precision:</p>\n\n<pre><code>asadmin get --monitor --interval 10 --iterations 360 \\\n        server.http-service.server.http-listener-2.requestcount-count \\\n        server.jvm.heapsize-current \\\n        server.jvm.memory.usedheapsize-count \\\n        server.jvm.memory.usednonheapsize-count\n</code></pre>\n\n<p>It outputs the metrics every ten seconds during one hour. Each batch of metrics are separated by two empty line. We analyze this output to push these metrics in a statsd/graphite server (nice tool by the way).</p>\n\n<p>What is the performance impact of using the <code>asadmin get --monitor</code> command during a 1-hour period? I assume that it has been designed to be efficient but I do not find any note about this. Is it the good way to gather GlassFish metrics or should we be using JMX or something else?</p>\n\n<p>We are using GlassFish 2.1.1</p>\n"},{"tags":["java","performance","atomic"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":63,"score":3,"question_id":12910981,"title":"AtomicLong vs Long performance","body":"<p>Context:\nI am using netty and have defined a handler in order to count and categorize incoming/outgoing traffic. For this I have used an enumMap that looks like this:</p>\n\n<pre><code>EnumMap&lt;MyEnum, AtomicLong&gt;\n</code></pre>\n\n<p>However now I have realized that there is only one thread that is manipulating the values (previously I thought it was more than one, netty seems to guarantee that one thread per channel). This means that AtomicLong is not necessary. <strong>However, as AtomicLong is a wrapper for a primitive long meanwhile Long is an immutable type, I have a reason to think that just swapping AtomicLong to Long will be less performant.</strong></p>\n\n<p><strong>Any ideas on this?</strong></p>\n\n<p>What I probably should do is to move to int and remove the whole enumMap thing..</p>\n\n<p>BR\nSebastian</p>\n"},{"tags":["performance","optimization","opencl"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":86,"score":1,"question_id":12900835,"title":"How can I optimize this OpenCL Sobel Filter Kernel?","body":"<p>I wrote an OpenCL kernel for a 3x3 Sobel filter, and currently it's running in about 17 millis on a 2k x 2k image. This isn't as a fast as I had hoped; does anyone have any suggestions for how to improve the speed? I've followed most of the suggestions on the <a href=\"http://software.intel.com/en-us/articles/tips-and-tricks-for-kernel-development/\" rel=\"nofollow\">checklist for optimizing kernels</a>. My processor is an Intel i5-3450. The workgroup size is 8x8, and the number of workitems is height x width / 16, which is 2048 x 128 on the images I'm running on.</p>\n\n<pre><code>__kernel void localCacheSobelFilter(\n  const __global char16* src, \n  __write_only __global float16* angle, \n  __write_only __global float16* mag, \n  const int width, \n  const int height)\n{\n  // Cache the data we're looking at in __local space\n  const int row = get_global_id(0);\n  const int col = get_global_id(1);\n\n  const int cacheRow = get_local_id(0) + 1;\n  const int cacheCol = get_local_id(1) + 1;\n\n  __local char16 cache[BLOCK_SIZE + 2][BLOCK_SIZE + 2];\n\n\n  cache[cacheRow][cacheCol] = src[ indexOf(row, col) ];\n\n  // --- Deal with the boundary conditions\n  // This adds in the rows above and below the local block,\n  // ignoring the corners.\n  const bool atTopRow = (cacheRow == 1);\n  const bool atBottomRow = (cacheRow == BLOCK_SIZE);\n\n  if(atTopRow) {\n    cache[0][cacheCol] = src[ indexOf(row - 1, col) ];\n\n  } else if (atBottomRow) {\n    cache[BLOCK_SIZE + 1][cacheCol] = src[ indexOf(row + 1, col) ];\n  }\n\n\n  // This adds in the columns to the left and right of the local block,\n  // ignoring the corners.\n  const bool atLeftCol = (cacheCol == 1);\n  const bool atRightCol = (cacheCol == BLOCK_SIZE);\n\n  if(atLeftCol) { \n    cache[cacheRow][0].sf = src[ indexOf(row, col - 1) ].sf;\n\n  } else if (atRightCol) {\n    cache[cacheRow][BLOCK_SIZE + 1].s0 = src[ indexOf(row, col + 1) ].s0;\n  }\n\n\n  // Now finally check the corners\n  const bool atTLCorner = atTopRow &amp;&amp; atLeftCol;\n  const bool atTRCorner = atTopRow &amp;&amp; atRightCol;\n  const bool atBLCorner = atBottomRow &amp;&amp; atLeftCol;\n  const bool atBRCorner = atBottomRow &amp;&amp; atRightCol;\n\n\n  if(atTLCorner) {\n    cache[0][0].sf = src[ indexOf(row - 1, col - 1) ].sf;\n\n  } else if (atTRCorner) { \n    cache[0][BLOCK_SIZE + 1].s0 = src[ indexOf(row - 1, col + 1) ].s0;\n\n  } else if (atBLCorner) {\n    cache[BLOCK_SIZE + 1][0].sf = src[ indexOf(row + 1, col - 1) ].sf;\n\n  } else if (atBRCorner) {\n    cache[BLOCK_SIZE + 1][BLOCK_SIZE + 1].s0 = src[ indexOf(row + 1, col + 1) ].s0;\n  }\n\n  barrier(CLK_LOCAL_MEM_FENCE); \n\n  //===========================================================================\n  // Do the calculation\n\n  //  [..., pix00]  upperRow  [pix02, ...]\n  //  [..., pix10]  centerRow [pix12, ...]\n  //  [..., pix20]  lowerRow  [pix22, ...]\n  const char pix00 = cache[cacheRow - 1][cacheCol - 1].sf;\n  const char pix10 = cache[cacheRow    ][cacheCol - 1].sf;\n  const char pix20 = cache[cacheRow + 1][cacheCol - 1].sf;\n\n  const char16 upperRow  = cache[cacheRow - 1][cacheCol];    \n  const char16 centerRow = cache[cacheRow    ][cacheCol];\n  const char16 lowerRow  = cache[cacheRow + 1][cacheCol];\n\n  const char pix02 = cache[cacheRow - 1][cacheCol + 1].s0;\n  const char pix12 = cache[cacheRow    ][cacheCol + 1].s0;              \n  const char pix22 = cache[cacheRow + 1][cacheCol + 1].s0;\n\n\n\n  // Do the calculations for Gy\n  const char16 upperRowShiftLeft = (char16)(upperRow.s123456789abcdef, pix02);\n  const char16 upperRowShiftRight = (char16)(pix00, upperRow.s0123456789abcde);\n\n  const char16 lowerRowShiftLeft = (char16)(lowerRow.s123456789abcdef, pix22);\n  const char16 lowerRowShiftRight = (char16)(pix20, lowerRow.s0123456789abcde);\n\n  const float16 Gy = convert_float16(\n    (upperRowShiftLeft + 2 * upperRow + upperRowShiftRight)\n    - (lowerRowShiftLeft + 2 * lowerRow + lowerRowShiftRight));\n\n\n  // Do the calculations for Gx\n  const char16 centerRowShiftLeft = (char16)(centerRow.s123456789abcdef, pix12);\n  const char16 centerRowShiftRight = (char16)(pix10, centerRow.s0123456789abcde);\n\n  const float16 Gx = convert_float16(\n    (upperRowShiftRight + 2 * centerRowShiftRight + lowerRowShiftRight)\n    - (upperRowShiftLeft + 2 * centerRowShiftLeft + lowerRowShiftLeft));\n\n\n  // Find the angle and magnitude\n  angle[ indexOf(row, col) ] = 0.0; //atan2(Gy, Gx);\n  mag[ indexOf(row, col) ] = ALPHA * max(Gx, Gy) + BETA * min(Gx, Gy);\n}\n</code></pre>\n\n<p>Any help would be greatly appreciated. Thanks!</p>\n"},{"tags":[".net","performance","sqlite","non-admin"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":45,"score":2,"question_id":12908960,"title":"System.Data.SQLite slow connect for non-admin users","body":"<p>I have a .NET 4 application (in mixed mode) with System.Data.Sqlite (1.0.82) for database access to an encrypted database. </p>\n\n<p>When I install the application to \"c:\\program files\\myfolder\" the connect to the sqlite database file is slow. Log files show that it's the sqlite connect statement that is delayed by a few seconds. </p>\n\n<p>The problem does not occur when I do the following:</p>\n\n<ul>\n <li>Run the application with admin privileges</li>\n <li>Install any other place than c:\\program files\\</li>\n <li>Install the application to c:\\program files\\, but move the database to another folder.</li>\n</ul>\n\n<p>I have no clue what can be the cause of this...</p>\n"},{"tags":["performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":3,"view_count":21,"score":-3,"question_id":12909476,"title":"Youtube streaming status test [KT research center]","body":"<p>Dear Youtube developer</p>\n\n<p>I'm working in the KT research center in korea.\nAsk something for the youtube function. \nI'd like to check the ssid and streaming speed (mbps) everytime, when youtube is continually running.</p>\n\n<p>if you can could you please send me a api or guide to me ?</p>\n\n<p>Thanks and bestRegards</p>\n\n<p>-Woong-</p>\n"},{"tags":["linux","performance","file-io","rsync","mv"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":25,"score":0,"question_id":12909053,"title":"What is the fastest way to move a million images from one directory to another?","body":"<p>I have a million images totally 30gb of disk space that need to move from one local directory to another local directory.</p>\n\n<p>What will be the most efficient way?  mv?  cp?  rsync?  Something else?  Tips?</p>\n\n<pre><code>/path/to/old-img-dir/*\n                     00000000.jpg\n                     --------.jpg  ## nearly 1M of them! ##\n                     ZZZZZZZZ.jpg\n</code></pre>\n\n<p>Move them to here:</p>\n\n<pre><code>/path/to/new/img/dir/\n</code></pre>\n"},{"tags":["c#","performance","entity-framework","sql-server-2008"],"answer_count":7,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":4379,"score":3,"question_id":3890974,"title":"Entity Framework 4 : Bad performance with SQL Server 2008","body":"<p>I'am developing a software based on Entity Framework to handle data in a MS SQL Server 2008 database.</p>\n\n<p><strong>[Trouble 1]</strong></p>\n\n<p>I've just tried to insert some small data (<strong>about 2 Mb</strong>) from my progam to the database : the performance are very bad ! It takes <strong>more than 1 minute</strong> to insert these datas !</p>\n\n<p>I've try to generate pre-compiled views, I've got the same results :-(</p>\n\n<p>All my code use a business layer (automatically generated from .edmx file with T4 template) to manage data in a service layer. It is very pratical to navigate in the relations of objects.</p>\n\n<p><strong>How can I improve the performance of these inserts with Entity Framework ?</strong></p>\n\n<p><strong>[Trouble 2]</strong></p>\n\n<p>Also, before inserting data in database with SaveChanges() method, I fill my object context with AddObject() method. I add about 100 000 small objects (about 2 Mb) to my object context with AddObject() : it takes a very long time (more than 10 minutes) !</p>\n\n<p><strong>How can I decrease this time ?</strong></p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>My program must save more than 50 Mb in database in less than 2-3 minutes ? Do you think it will be possible with EF ?</p>\n"},{"tags":["php","performance","optimization","if-statement"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":22,"score":-1,"question_id":12908780,"title":"php ideal way to check function returns true","body":"<p>Which of the following way of checking whether a  function returns True or FALSE is best, in terms of efficiency and code readability </p>\n\n<p>I was told by a friend that Method B is a good practice, but I believe Method A is better since it checks whether function returns TRUE then Assigns</p>\n\n<p><strong>Method A:</strong>  (Checks whether function is TRUE and <strong>then</strong> assigns </p>\n\n<pre><code>  if( $result = $db-&gt;getResult($id)){\n      echo 'pass';\n   }else{\n      echo 'fail';\n   }\n</code></pre>\n\n<p><strong>Method B</strong>  (first <strong>assigns</strong> the value, then <strong>checks</strong> , '<strong>TWO OPERATIONS'</strong></p>\n\n<pre><code>  $result = $db-&gt;getResults(90);\n if($result){\n      echo 'pass';\n  }else{\n      echo 'fail';\n   }\n</code></pre>\n\n<hr>\n\n<pre><code>   public function getResults($no){\n     if($no&gt;85){\n       return TRUE;}\n      else{\n       return FALSE;}\n</code></pre>\n"},{"tags":["css","performance","internet-explorer","profiler"],"answer_count":4,"favorite_count":17,"up_vote_count":32,"down_vote_count":0,"view_count":2078,"score":32,"question_id":5173122,"title":"CSS Performance Profiler?","body":"<p>I'm currently working on a site, and somewhere in my mass of stylesheets, something is killing performance in IE.  Are there any good CSS profilers out there?  I'd like a tool that can pinpoint rules that are killing performance.</p>\n\n<p>Before you ask, I've disabled JavaScript, opacity, and box-shadow/text-shadow rules.  The page is still jumpy.  :/  If I disable all CSS, it runs great.  </p>\n\n<p>I need a tool that can profile the page and report where the CSS bottlenecks are.</p>\n"},{"tags":["performance","tinymce","loading","moodle"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":69,"score":0,"question_id":11982538,"title":"Moodle 2.2.3 TinyMCE loads slowly","body":"<p>In Moodle 2.2.3, after 10-12 seconds TinyMCE (TinyMCE HTML editor; editor_tinymce; Standard;    2012030300) buttons will show up (loads very slowly). Where problem lies? Can't figure out how to speed up TinyMCE HTML editor loading time.</p>\n\n<p>I have Moodle 2.2.3+ (Build: 20120519).</p>\n"},{"tags":["ruby-on-rails","database","performance"],"answer_count":1,"favorite_count":2,"up_vote_count":0,"down_vote_count":0,"view_count":56,"score":0,"question_id":12907264,"title":"What are the common and useful techniques to optimize database performance?","body":"<p>I was wondering how we could usually optimize the database performance for a data intensive web application. Are there any common requirements like what queries to use, how to deal with relationships fast, whether to avoid writing to database as much as possible, etc. Anything related works.</p>\n\n<p>Or more specifically, any techniques to make the Ruby on Rails database more efficient?</p>\n\n<p>Thanks a lot!</p>\n"},{"tags":["performance","magento"],"answer_count":5,"favorite_count":5,"up_vote_count":4,"down_vote_count":0,"view_count":736,"score":4,"question_id":9216743,"title":"Tweaking magento for performance","body":"<p>i'm looking on performance (server load time) of magento site and i'm trying to tune search result pages. I realized that when I disabled all heavy things like top navigation, lev layered navigation and product listing and I cleared all cache then after this magento core does like 60 SQL queries agains a database. Does anyone have any procedure how to rid of them or how to reduce them to some acceptable amount?</p>\n\n<p>Also can I somehow reduce a time spent during creating of blocks?</p>\n\n<p>Thank you very much,\nJaro.</p>\n"},{"tags":["mysql","performance","cpu"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":353,"score":0,"question_id":6710563,"title":"MYSQL suddenly uses CPU even when no one user is connected to Mysql","body":"<p>Site is an ecommerce site: 5 Million records in table\nTwo servers: one for webserver and other contains mysql\nSearch is happenning through Sphinx server. So search queries do not come to MySQL</p>\n\n<p>Mysql configuration: Dual Quad Core Zeoo 2.0, 146 GB, 16 GB RAM.\nWebserver configuration: Dual Quad Core Zeoo 2.0, 146 GB, 16 GB RAM.</p>\n\n<p>For past four days I find MySQL is using CPU continuously for at least 6-7 hours in a day. It becomes normal after that. Even if I restart, it doesnt stop. It again uses CPU in 2 to 3 mins. I even tried stopping Apache and made sure no one is connecting to Mysql.</p>\n"},{"tags":["performance","sql-server-2008","stored-procedures","jdbc","clr"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":74,"score":0,"question_id":12869062,"title":"SQL Server 2008 x64 Express Batch Insert Throughput Performance to CLR 'Instead Of Insert' Trigger","body":"<p>I have a simple NO-OP CLR Trigger being fired on a six field table with schema:</p>\n\n<pre><code> datetime (PK)\n char (PK)\n int (PK, FK)\n varchar(1)\n tinyint\n real\n</code></pre>\n\n<p>which should be a total of 19 bytes / row inserted.</p>\n\n<p>The FK in the above points to a table with schema:</p>\n\n<pre><code> int (PK)\n nchar(10)\n ncahr(10)\n nchar(10)\n nchar(10)\n</code></pre>\n\n<p>I am performing batch inserts using a prepared statement every 1000 rows. I am getting for 150K rows inserted an execution time of about 11s. This corresponds to a throughput of ~.24MB/s (150,000*19B / 11s). Why so slow? I am not even talking to the disk?</p>\n\n<p>I know that my client side code doing the sending is much faster. If I comment out the:</p>\n\n<pre><code> executeBatch()\n</code></pre>\n\n<p>method in the client I get upwards of 5MB/s. What else is going on with the JDBC/ODBC connection and processing in SQL Server to make things so slow?</p>\n\n<p>Is this really the throughput one can expect from SQL Server and an JDBC/ODBC connection in this case?</p>\n\n<p>---EDIT---</p>\n\n<p>The actual CLR code is very simple, literally an empty method:</p>\n\n<pre><code> public class Triggers\n {\n     [SqlTrigger(Name = \"InsertHook\", Target = ConfigConstants.TABLE_NAME, Event =   \"INSTEAD OF INSERT\")]\n     public static void InsertHook()\n     {\n       //empty\n     }\n }\n</code></pre>\n\n<blockquote>\n  <p>No-op in T-SQL Stored Procedure Total Measured Time was:3475297036ns\n  or 3.475297036sec INSERT COUNT:150000\n  0.783276657MB/s\n  43165.47 Rows/s</p>\n  \n  <p>INSERTING NORMAL into SQL Server Total Measured Time was:4884190118ns\n  or 4.884190118sec INSERT COUNT:150000 MB Sent:2.7179718017578125\n  0.556MB/s\n  30712.53 Rows/s</p>\n</blockquote>\n\n<p>Sadly, I do not have control over the schemas I am working with here. I guess I am still shocked that even with one FK constraint that this is the best SQL Server can do?</p>\n\n<p>In summary, if I had to rank the performance. It would be:</p>\n\n<blockquote>\n  <p>1) T-SQL NO-OP Instead of Trigger (Fastest) - 0.78MB/s  </p>\n  \n  <p>2) Inserting Normal - 0.56MB/s </p>\n  \n  <p>3) CLR NO-OP Stored Procedure Instead of Trigger - .24MB/s</p>\n</blockquote>\n"},{"tags":["java","jvm","performance","jvm-arguments"],"answer_count":6,"favorite_count":13,"up_vote_count":15,"down_vote_count":0,"view_count":13843,"score":15,"question_id":564039,"title":"JVM performance tuning for large applications","body":"<p>The default JVM parameters are not optimal for running large applications. Any insights from people who have tuned it on a real application would be helpful. We are running the application on a 32-bit windows machine, where the client JVM is used <a href=\"http://java.sun.com/docs/hotspot/gc5.0/ergo5.html#0.0.%20Garbage%20collector,%20heap,%20and%20runtime%20compiler|outline\" rel=\"nofollow\">by default</a>. We have added -server and changed the NewRatio to 1:3 (A larger young generation).</p>\n\n<p>Any other parameters/tuning which you have tried and found useful?</p>\n\n<p>[Update] The specific type of application I'm talking about is a server application that are rarely shutdown, taking at least -Xmx1024m. Also assume that the application is profiled already. I'm looking for general guidelines in terms of <strong>JVM performance</strong> only.</p>\n"},{"tags":["sql","sql-server","performance","database-design","foreign-keys"],"answer_count":9,"favorite_count":3,"up_vote_count":5,"down_vote_count":0,"view_count":7639,"score":5,"question_id":599159,"title":"Should I use foreign keys?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/83147/whats-wrong-with-foreign-keys\">What’s wrong with foreign keys?</a>  </p>\n</blockquote>\n\n\n\n<p>I use MS Sql Server with a large database about 4 GB data.</p>\n\n<p>I search around the web why I should use foreign keys. \nby now I only indexed the keys used to join tables. \nPerformance is all fine, dataintegrety is no problem.</p>\n\n<p>Should I use foreign keys? Will I get even more performance with foreign keys?</p>\n"},{"tags":["linux","performance","x86","x86-64","linux-x32-abi"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":84,"score":4,"question_id":12903049,"title":"What <4GB workloads would have worse performance in the Linux x32 ABI than x64?","body":"<p>There is a relatively <a href=\"http://en.wikipedia.org/wiki/X32_ABI\" rel=\"nofollow\">new Linux ABI referred to as x32</a>, where the x86-64 processor is run in 32-bit mode, so pointers are still only 32-bits, but the 64-bit architecture specific registers are still used. So you're still limited to 4GB max memory use as in normal 32-bit, but your pointers use up less cache space than they do in 64-bit, you can do 64-bit arithmetic efficiently, and you get access to more registers (16) than you would in vanilla 32-bit (8).</p>\n\n<p>Assuming you have a workload the fits nicely within 4GB, is there any way the performance of x32 could be worse than on x86-64?</p>\n\n<p>It seems to me that if you don't need the extra memory space nothing is lost -- you should always get the same perf (when you already fit in cache) or better (when the pointer space savings lets you fit more in cache). But it wouldn't surprise me if there are paging/TLB/etc. details that I don't know about.</p>\n"},{"tags":[".net","vb.net","performance","events"],"answer_count":4,"favorite_count":0,"up_vote_count":6,"down_vote_count":0,"view_count":1867,"score":6,"question_id":250494,"title":"VB.NET: Are events raised even if there are no event handlers?","body":"<p>I have a class that downloads, examines and saves some large XML files. Sometimes I want the UI to tell me what's going on, but sometimes I will use the class and ignore the events. So I have placed lines of code like this in a dozen places:</p>\n\n<pre><code>RaiseEvent Report(\"Sending request: \" &amp; queryString)\n\nRaiseEvent Report(\"Saving file: \" &amp; fileName)\n\nRaiseEvent Report(\"Finished\")\n</code></pre>\n\n<p>My question is this - will these events slow down my code if nothing is listening for them? Will they even fire?</p>\n"},{"tags":["performance","algorithm","time","complexity"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":118,"score":0,"question_id":12877674,"title":"How to find time complexity of an algorithm when given number of iterations and total time?","body":"<p>I understand how to find the time complexity of an algorithm when I've been presented with an algorithm, but I can't seem to get my head around how to work it out when I've been given the number of times the algorithm is executed, and the time taken.</p>\n\n<p>I can sometimes get it, when it's obvious things like O(n), O(n) or O(n^2) but take this question for example:</p>\n\n<p>An algorithm runs a given input of size n.\nIf n is 4096 the run time is 512 milliseconds.\nIf n is 16384 the run time is 1024 milliseconds.\nIf n is 36864 the run time is 1536 milliseconds.</p>\n\n<p>What is the time complexity?</p>\n\n<p>I see that as n * 2, t * 1.5, but I'm not quite sure how to work it out.</p>\n\n<p>Thank you for your help :)</p>\n"},{"tags":["javascript","jquery","performance","image","firebug"],"answer_count":6,"favorite_count":3,"up_vote_count":3,"down_vote_count":0,"view_count":106,"score":3,"question_id":12825845,"title":"How to trace slow JS or JQuery code","body":"<p>I created a web page for viewing images. This page has some other code that gets included that I did not write. The page loads 40 small images upon load. Then the user will scroll down and additional pages of 40 images can be loaded via ajax. Once I get to  15-20 pages, I notice the page begins to slow significantly. I check app counters and it can go up to 100% cpu and memory can go over 3GB. Then I will inevitably get the modal that JQuery is taking too long to execute, asking me if I want to stop executing the script. Now I realize that a page with up to 800 images is a big load, but the issue with JQuery suggests to me that some code may also be iterating over this larger and larger group of dom objects. It almost appears to get exponentially slower as I pass 15 pages or so. Once I get to 20 pages it becomes almost unusable.</p>\n\n<p>First of all, is it even possible to run a page efficiently, even with minimal JS, when you have this many images? Secondly, is there a recommended way to \"trace\" JS and see what kinds of functions are getting executed to help determine what is the most likely culprit? This is most important to me - is there a good way to do in Firebug?</p>\n\n<p>Thanks :)</p>\n\n<p>EDIT - I found my answer. I had some older code which was being used to replace images that failed to load with a generic image. This code was using Jquery's .each operator and thus was iterating over the entire page and each new ajax addition every time the page loaded. I am going to set a class for the images that need to be checked in CSS so that the ajax-loaded images are unaffected.</p>\n"},{"tags":["javascript","jquery","performance","css3"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":80,"score":1,"question_id":12902323,"title":"How do I optimize this CSS3/many images for performance?","body":"<p>Here is the <a href=\"http://pineapple.io/\" rel=\"nofollow\">link to site</a></p>\n\n<p>There's a few things going on here I'd love to tweak, I'm not quite sure what else I can do though.</p>\n\n<p>Thumbnails and favicons are pulled via external services, and I'd rather not setup reverse proxy to cache them. Normally, the images are supposed to fade in smoothly when they load (in a sort of 'random' order, it gives it a nice effect).</p>\n\n<p>Youll notice when the page loads, it seems to be a bit choppy though, I think because the twitter script loading at the same  time as all the thumbnails fadein.</p>\n\n<p>I have tried looking at several performance tools but I'm at a bit of a loss as to how I can improve this.</p>\n\n<p>The only thing I can think of would be to maybe have twitter offset by a second or two.</p>\n"},{"tags":["java","performance","garbage-collection","jvm","tuning"],"answer_count":5,"favorite_count":4,"up_vote_count":4,"down_vote_count":0,"view_count":1657,"score":4,"question_id":9792590,"title":"GC Tuning - preventing a Full GC","body":"<p>I'm trying to avoid the Full GC (from gc.log sample below)\nrunning a Grails application in Tomcat in production.\nAny suggestions on how to better configure the GC?</p>\n\n<p><strong>14359.317: [Full GC 14359.317: [CMS: 3453285K->3099828K(4194304K), 13.1778420 secs] 4506618K->3099828K(6081792K), [CMS Perm : 261951K->181304K(264372K)] icms_dc=0 , 13.1786310 secs] [Times: user=13.15 sys=0.04, real=13.18 secs]</strong> </p>\n\n<p><strong>My VM params are as follow:</strong><br>\n-Xms=6G<br>\n-Xmx=6G<br>\n-XX:MaxPermSize=1G <br>\n-XX:NewSize=2G <br>\n-XX:MaxTenuringThreshold=8 <br>\n-XX:SurvivorRatio=7<br>\n-XX:+UseConcMarkSweepGC <br>\n-XX:+CMSClassUnloadingEnabled <br>\n-XX:+CMSPermGenSweepingEnabled <br>\n-XX:+CMSIncrementalMode<br> \n-XX:CMSInitiatingOccupancyFraction=60 <br>\n-XX:+UseCMSInitiatingOccupancyOnly <br>\n-XX:+HeapDumpOnOutOfMemoryError <br>\n-XX:+PrintGCDetails <br>\n-XX:+PrintGCTimeStamps <br>\n-XX:+PrintTenuringDistribution <br>\n-Dsun.reflect.inflationThreshold=0 <br></p>\n\n<pre>\n    14169.764: [GC 14169.764: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 8 (max 8)\n    - age   1:   15584312 bytes,   15584312 total\n    - age   2:   20053704 bytes,   35638016 total\n    - age   3:   13624872 bytes,   49262888 total\n    - age   4:   14469608 bytes,   63732496 total\n    - age   5:   10553288 bytes,   74285784 total\n    - age   6:   11797648 bytes,   86083432 total\n    - age   7:   12591328 bytes,   98674760 total\n    : 1826161K->130133K(1887488K), 0.1726640 secs] 5216326K->3537160K(6081792K) icms_dc=0 , 0.1733010 secs] [Times: user=0.66 sys=0.03, real=0.17 secs] \n    14218.712: [GC 14218.712: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 8 (max 8)\n    - age   1:   25898512 bytes,   25898512 total\n    - age   2:   10308160 bytes,   36206672 total\n    - age   3:   16927792 bytes,   53134464 total\n    - age   4:   13493608 bytes,   66628072 total\n    - age   5:   14301832 bytes,   80929904 total\n    - age   6:   10448408 bytes,   91378312 total\n    - age   7:   11724056 bytes,  103102368 total\n    - age   8:   12299528 bytes,  115401896 total\n    : 1807957K->147911K(1887488K), 0.1664510 secs] 5214984K->3554938K(6081792K) icms_dc=0 , 0.1671290 secs] [Times: user=0.61 sys=0.00, real=0.17 secs] \n    14251.429: [GC 14251.430: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 7 (max 8)\n    - age   1:   25749296 bytes,   25749296 total\n    - age   2:   20111888 bytes,   45861184 total\n    - age   3:    7580776 bytes,   53441960 total\n    - age   4:   16819072 bytes,   70261032 total\n    - age   5:   13209968 bytes,   83471000 total\n    - age   6:   14088856 bytes,   97559856 total\n    - age   7:   10371160 bytes,  107931016 total\n    - age   8:   11426712 bytes,  119357728 total\n    : 1825735K->155304K(1887488K), 0.1888880 secs] 5232762K->3574222K(6081792K) icms_dc=0 , 0.1895340 secs] [Times: user=0.74 sys=0.06, real=0.19 secs] \n    14291.342: [GC 14291.343: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 7 (max 8)\n    - age   1:   25786480 bytes,   25786480 total\n    - age   2:   21991848 bytes,   47778328 total\n    - age   3:   16650000 bytes,   64428328 total\n    - age   4:    7387368 bytes,   71815696 total\n    - age   5:   16777584 bytes,   88593280 total\n    - age   6:   13098856 bytes,  101692136 total\n    - age   7:   14029704 bytes,  115721840 total\n    : 1833128K->151603K(1887488K), 0.1941170 secs] 5252046K->3591384K(6081792K) icms_dc=0 , 0.1947390 secs] [Times: user=0.82 sys=0.04, real=0.20 secs] \n    14334.142: [GC 14334.143: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 6 (max 8)\n    - age   1:   31541800 bytes,   31541800 total\n    - age   2:   20826888 bytes,   52368688 total\n    - age   3:   19155264 bytes,   71523952 total\n    - age   4:   16422240 bytes,   87946192 total\n    - age   5:    7235616 bytes,   95181808 total\n    - age   6:   16549000 bytes,  111730808 total\n    - age   7:   13026064 bytes,  124756872 total\n    : 1829427K->167467K(1887488K), 0.1890190 secs] 5269208K->3620753K(6081792K) icms_dc=0 , 0.1896630 secs] [Times: user=0.80 sys=0.03, real=0.19 secs] \n    14359.317: [Full GC 14359.317: [CMS: 3453285K->3099828K(4194304K), 13.1778420 secs] 4506618K->3099828K(6081792K), [CMS Perm : 261951K->181304K(264372K)] icms_dc=0 , 13.1786310 secs] [Times: user=13.15 sys=0.04, real=13.18 secs]\n    14373.287: [GC [1 CMS-initial-mark: 3099828K(4194304K)] 3100094K(6081792K), 0.0107380 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \n    14373.298: [CMS-concurrent-mark-start]\n    14472.579: [GC 14472.579: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 8 (max 8)\n    - age   1:   42849392 bytes,   42849392 total\n    : 1677824K->86719K(1887488K), 0.1056680 secs] 4777652K->3186547K(6081792K) icms_dc=0 , 0.1063280 secs] [Times: user=0.61 sys=0.00, real=0.11 secs] \n    14506.980: [GC 14506.980: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 8 (max 8)\n    - age   1:   42002904 bytes,   42002904 total\n    - age   2:   35733928 bytes,   77736832 total\n    : 1764543K->96136K(1887488K), 0.0982790 secs] 4864371K->3195964K(6081792K) icms_dc=0 , 0.0988960 secs] [Times: user=0.53 sys=0.01, real=0.10 secs] \n    14544.285: [GC 14544.286: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 8 (max 8)\n    - age   1:   26159736 bytes,   26159736 total\n    - age   2:   37842840 bytes,   64002576 total\n    - age   3:   33192784 bytes,   97195360 total\n    : 1773960K->130799K(1887488K), 0.1208590 secs] 4873788K->3230628K(6081792K) icms_dc=0 , 0.1215900 secs] [Times: user=0.59 sys=0.02, real=0.13 secs] \n    14589.266: [GC 14589.266: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 4 (max 8)\n    - age   1:   28010360 bytes,   28010360 total\n    - age   2:   21136704 bytes,   49147064 total\n    - age   3:   35081376 bytes,   84228440 total\n    - age   4:   32468056 bytes,  116696496 total\n    : 1808623K->148284K(1887488K), 0.1423150 secs] 4908452K->3248112K(6081792K) icms_dc=0 , 0.1429440 secs] [Times: user=0.70 sys=0.02, real=0.14 secs] \n    14630.947: [GC 14630.947: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 8 (max 8)\n    - age   1:   28248240 bytes,   28248240 total\n    - age   2:   20712320 bytes,   48960560 total\n    - age   3:   18217168 bytes,   67177728 total\n    - age   4:   34834832 bytes,  102012560 total\n    : 1826108K->140347K(1887488K), 0.1784680 secs] 4925936K->3275469K(6081792K) icms_dc=0 , 0.1790920 secs] [Times: user=0.98 sys=0.03, real=0.18 secs] \n    14664.779: [GC 14664.779: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 5 (max 8)\n    - age   1:   25841000 bytes,   25841000 total\n    - age   2:   22264960 bytes,   48105960 total\n    - age   3:   17730104 bytes,   65836064 total\n    - age   4:   17988048 bytes,   83824112 total\n    - age   5:   34739384 bytes,  118563496 total\n    : 1818171K->147603K(1887488K), 0.1714160 secs] 4953293K->3282725K(6081792K) icms_dc=0 , 0.1720530 secs] [Times: user=0.82 sys=0.11, real=0.17 secs] \n    14702.488: [GC 14702.489: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 8 (max 8)\n    - age   1:   26887368 bytes,   26887368 total\n    - age   2:   21403352 bytes,   48290720 total\n    - age   3:   18732224 bytes,   67022944 total\n    - age   4:   17640576 bytes,   84663520 total\n    - age   5:   17942952 bytes,  102606472 total\n    : 1825427K->142695K(1887488K), 0.2118320 secs] 4960549K->3312168K(6081792K) icms_dc=0 , 0.2124630 secs] [Times: user=1.13 sys=0.14, real=0.21 secs] \n</pre>\n\n<p>The strategy I was aiming at:\nI want to limit to the minimum what gets Tenured, I'm serving requests and expect that beyond a certain amount of shared objects, every other objects are useful only to the request at hand. Therefore by using a big NewSize and an increased TenuringThreshold and was hoping to have none of these single serving objects stick around.</p>\n\n<p>The following are there to support my strategy:<br>\n-Xms=6G<br>\n-Xmx=6G<br>\n-XX:NewSize=2G // big space so that ParNew doesn't occur to often and let time for objects to expire<br>\n-XX:MaxTenuringThreshold=8 // to limit the tenuring some more<br>\n-XX:SurvivorRatio=7 // based on examples\n-XX:CMSInitiatingOccupancyFraction=60<br> // to prevent a Full GC caused by promotion allocation failed<br>\n-XX:+UseCMSInitiatingOccupancyOnly<br> // to go with the one above based on example</p>\n\n<p>MaxPermSize=1G and \"-Dsun.reflect.inflationThreshold=0\" are related to another issue I'd rather keep separated.</p>\n\n<p>\"-XX:+CMSClassUnloadingEnabled\" and \"-XX:+CMSPermGenSweepingEnabled\" are there because of grails which rely heavily and extra classes for closures and reflexion</p>\n\n<p>-XX:+CMSIncrementalMode is an experiment which hasn't yield much success</p>\n"},{"tags":["java","performance","garbage-collection","jvm","permanent-generation"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":436,"score":2,"question_id":6260771,"title":"JVM performance tuning: young copy vs old generation gc","body":"<p>Hi: I have a multi thread Java application. There are many temporary objects.<br>\n<code>-XX:MaxTenuringThreshold=1</code>, we put above parameter when starting JVM. This means all the objects would be survive once during gc, then it would be promoted to old generation. Could we put this <code>-XX:MaxTenuringThreshold=10</code> for example, so that object would be promoted to old JVM old generation after 10 times gc. But will that cause unnecessary copy operation during young gc (since objects are copied 'from 'eden' to 'from', from 'from' to 'to', 'from','to' are two survivor buffer)?</p>\n\n<p>The questions might also mean if a) there are multiple times copy in young generation,less old generation gc, b) long old generation garbage collection but few young generation copy, which one is better for good performance?</p>\n"},{"tags":["jvm","performance"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":209,"score":1,"question_id":2391135,"title":"Resources for JVM Tuning","body":"<p>Anybody knows a good book or two (or resources) for JVM Tuning?</p>\n\n<p>I am struggling to find any. I stumbled upon Apress Java EE 5 Performance Management and Optimization, but there was not much in there.</p>\n"},{"tags":["performance","multithreading","file","file-io","parallel-processing"],"answer_count":6,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":437,"score":1,"question_id":993038,"title":"Directory walker on modern operating systems slower when it's multi-threaded?","body":"<p>Once I had the theory that on modern operating systems multithreaded \nread access on the HDD should perform better.</p>\n\n<p>I thought that:<br />\n<em>the operating system queues all read requests,\nand rearranges them in such a way, that it could read from the HDD more\nsequentially. The more requests it would get, the better it could rearrange them\nto optimize the read sequence.</em><br />\nI was very sure that I read it somewhere few times.</p>\n\n<p>But I did some benchmarking, and had to find out, that multithreaded\nread access mostly perform much worst, and never performs better.</p>\n\n<p>I had the experience under Windows and Linux. I benchmarked pure\nsearching of files using the operating system's tools, and also\nhad written own little benchmarks.</p>\n\n<p>Am I missing something?<br />\nCan someone explain to me the secrets of this topic?<br />\nThank you!</p>\n"},{"tags":["java","performance","delay","xuggler"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":79,"score":1,"question_id":12785995,"title":"Xuggler live streaming delay and high cpu usage","body":"<p>I'm currently using Xuggler to receive the video stream of an AR.Drone. The stream format is H.264 720p. I can decode and display the video using the following code, but the processor usage is very high (100% on dual-core 2ghz) and there is a huge delay in the stream that keeps increasing.</p>\n\n<pre><code>        final IMediaReader reader = ToolFactory.makeReader(\"http://192.168.1.1:5555\");\n        reader.setBufferedImageTypeToGenerate(BufferedImage.TYPE_3BYTE_BGR);\n\n        MediaListenerAdapter adapter = new MediaListenerAdapter()\n        {\n            public void onVideoPicture(IVideoPictureEvent e)\n            {\n                currentframe = e.getImage();\n                //Draw frame\n            }\n\n            public void onOpenCoder(IOpenCoderEvent e) {\n                videostreamopened = true;\n            }\n        };\n\n        reader.addListener(adapter);\n\n        while (!stop) {\n            try {\n                reader.readPacket();\n            } catch(RuntimeException re) {\n                // Errors happen relatively often\n            }\n        }\n</code></pre>\n\n<p>Using the Xuggler sample application resolves none of the problems, so I think my approach is correct. Also, when I decrease the resolution to 360p the stream is real-time and everything works OK. Does anybody know if this performance issues are normal or what I have to do to avoid this? I am <em>very</em> new to this, and I have not been able to find information, so does anybody have suggestions? </p>\n\n<p>By the way, I tried changing the bitrate without success. Calling <code>reader.getContainer().getStream(0).getStreamCoder().setBitRate(bitrate);</code> seems to be ignored...</p>\n\n<p>Thanks in advance!</p>\n\n<p><strong>UPDATE:</strong>\nI get many of these errors:</p>\n\n<pre><code>9593 [Thread-7] ERROR org.ffmpeg - [h264 @ 0x7f12d40e53c0] mmco: unref short failure\n39593 [Thread-7] ERROR org.ffmpeg - [h264 @ 0x7f12d40e53c0] number of reference frames (0+2) exceeds max (1; probably corrupt input), discarding one\n39593 [Thread-15] ERROR org.ffmpeg - [h264 @ 0x7f12d40e53c0] reference overflow\n39593 [Thread-15] ERROR org.ffmpeg - [h264 @ 0x7f12d40e53c0] decode_slice_header error\n</code></pre>\n\n<p><strong>UPDATE 2:</strong> Changing the codec solves the above errors, but performance is still poor.</p>\n"},{"tags":["ruby-on-rails","ruby","performance","testing"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":39,"score":0,"question_id":12902064,"title":"How to measure file upload performance in Rails 2?","body":"<p>We have a rails 2 application with an uploader which currently has not bee performing to expectations in terms of speed. We would like to try a few things to speed it up.</p>\n\n<p>The first task is to define what \"slow\" means and try to measure that it terms of time it takes for an upload. I looked into new relic and now looking into munin, but from what I have seen I am not sure these are the right tools for the job.</p>\n\n<p>How would you recommend we should approach this problem?</p>\n"},{"tags":["performance","algorithm","math","interview"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":100,"score":1,"question_id":12867219,"title":"Given a point inside a rectangle, determine the side that's closest to the point","body":"<p>I was curious if there was an elegant way to do this, aside from just calculating the distance from the point to each side and finding the minimum.</p>\n\n<p>Some things I've thought about:\nIf it's a square, we can just draw the diagonals and figure out which of the 4 regions the point falls on. Each of these region corresponds to a closest side.</p>\n\n<p>Perhaps we can divide up the rectangle into squares and go somewhere from there?</p>\n\n<p>It seems an alternative solution would be too complicated and not worth looking for.</p>\n"},{"tags":["java","performance","jasper-reports","itext"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":75,"score":0,"question_id":12900968,"title":"What is the expected performance of JasperReports 4.7/iText 2.7.1","body":"<p>I know after reading the title many will think \"well it depends...\" but that's exactly my question: on what does it depend?</p>\n\n<p>In my particular case, i'm generating a ~84Mb file which contains about 1000 pages of a single table(band). That is ~24,000 rows and 18 columns, and a summary element. Using a compiled (.jasper) file.</p>\n\n<p>When i run it on a dev pc (2.5 Ghz CPU, 3 Gb RAM) it takes about 90 seconds to generate.\nI profiled it using JVisualVM and the output was (cumulative times):</p>\n\n<pre><code>* JRPdfExporter.exportReport() 90,389 ms\n* JRPdfExporter.exportElements() 85,559 ms\n* JRPdfExporter.exportText() 50,338 ms\n* SimplePdfTextRenderer.render() 43,520 ms\n* SimplePdfTextRenderer.getPhrase() 22,278 ms\n* JRDataUtils.getLocale() 14,094 ms\n</code></pre>\n\n<p>I think this getLocale() method is taking way too long (15% of total time) considering i set it to a constant when passing it to the JasperExportManager.exportReportToPdfFile() method like this:</p>\n\n<pre><code>params.put(JRParameter.REPORT_LOCALE, MX_LOCALE);//MX_LOCALE is a Locale constant\n</code></pre>\n\n<p>Am i doing this wrong?</p>\n\n<p>I'm using a custom JRDataSource implementation, which esentially iterates over a List. There's nothing terribly complicated to it, but I can post it if it helps.</p>\n\n<p>I hope i'm not asking too many questions into one, or including way too much information, but id like to know if this is a reasonable time for this kind of report, or there´s anything i can do to make it faster.  Thanks in advance</p>\n"},{"tags":["android","performance","sqlite","ipc"],"answer_count":1,"favorite_count":6,"up_vote_count":12,"down_vote_count":0,"view_count":836,"score":12,"question_id":4426616,"title":"Android ContentProvider Performance","body":"<p>I'm curious if anyone has done any performance testing on querying a <code>ContentProvider</code> via <code>ContentResolver</code> vs querying a <code>SQLiteDatabase</code> object in the same process. I'm guessing that a <code>ContentResolver</code> query passes back a Cursor that communicates with the database through a Binder (Android IPC). This means that if I read the contents of 100 records through the <code>Cursor</code> that would result 100 Binder method calls. Are my guesses correct and if so would that be significantly slower than accessing the database in the same process?</p>\n"},{"tags":["java","regex","performance","algorithm","string"],"answer_count":5,"favorite_count":0,"up_vote_count":11,"down_vote_count":2,"view_count":1032,"score":9,"question_id":2667015,"title":"Is regex too slow? Real life examples where simple non-regex alternative is better","body":"<p>I've seen people here made comments like \"regex is too slow!\", or \"why would you do something so simple using regex!\" (and then present a 10+ lines alternative instead), etc.</p>\n\n<p>I haven't really used regex in industrial setting, so I'm curious if there are applications where regex is demonstratably just too slow, <strong>AND</strong> where a <em>simple</em> non-regex alternative exists that performs significantly (maybe even asymptotically!) better.</p>\n\n<p>Obviously many highly-specialized string manipulations with sophisticated string algorithms will outperform regex easily, but I'm talking about cases where a simple solution exists and <em>significantly</em> outperforms regex.</p>\n\n<p>What counts as simple is subjective, of course, but I think a reasonable standard is that if it uses only <code>String</code>, <code>StringBuilder</code>, etc, then it's probably simple.</p>\n\n<hr>\n\n<p><em>Note</em>: I would very much appreciate answers that demonstrate the following:</p>\n\n<ol>\n<li>a beginner-level regex solution to a non-toy real-life problem that performs horribly</li>\n<li>the simple non-regex solution</li>\n<li>the expert-level regex rewrite that performs comparably</li>\n</ol>\n"},{"tags":["performance","magento","indexing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":66,"score":0,"question_id":12899701,"title":"Magento - slow url rewrite reindex","body":"<p>I'm thinking about improving reindexing of url rewrites in magento, we have 130k simple and configurable products. And reindexing consumes a lot of memory, basicaly I'm not able to finish this job in comammnd line because of memory limit.</p>\n\n<p>So I started with looking for some solution which could speedup whole process. Whole reindex is happening in <code>Mage_Catalog_Model_Url::_refreshProductRewrite</code> because system iterate over all products and categories and stores. And for each iteration it possibly does 1 or 2 <code>insertOnDuplicate</code> actions.</p>\n\n<p>I'm thinking about storing such queries somewhere and merging them into one or more bigger queries which would be much more faster (I guess).</p>\n\n<p>Slow bit for one comibation (product, category, store) looks like</p>\n\n<pre><code>    $this-&gt;getResource()-&gt;saveRewrite($rewriteData, $this-&gt;_rewrite);\n\n    if ($this-&gt;getShouldSaveRewritesHistory($category-&gt;getStoreId())) {\n        $this-&gt;_saveRewriteHistory($rewriteData, $this-&gt;_rewrite);\n    }\n</code></pre>\n\n<p>Have anyone better idea how to improve this?</p>\n"},{"tags":["c","performance","visual-c++","c99"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":96,"score":0,"question_id":12899048,"title":"Efficiency issues when using C99 and C11.","body":"<p>The other day I was converting a program written with C99 standard into C11. Basically the motive was to use the code with MSVC but It was written in Linux and was mostly compiled with default GCC behaviour. During the code conversion, I found out that you can not decalre variables of a function after any statement i.e. you must declare them at the top of the function. </p>\n\n<p>But my question is that wouldn't it be against the efficient programming rule that variables should be declared near their use so that it maximizes the cache hits? For example, In a large function of say 200 LOC, I want to use some big static look up array at nearly the end of the function. Wouldn't declaring and initializing it just before the usage cause more cache hits?  or am I simple missing some basic point of C11 C language standard?</p>\n"},{"tags":["jquery","performance","jquery-deferred"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":73,"score":0,"question_id":12899442,"title":"jQuery performance: $(document).ready(...), $(window).load(...) or during page load?","body":"<p>I'm referring to <a href=\"http://www.artzstudio.com/2009/04/jquery-performance-rules/#defer-to-window-load\" rel=\"nofollow\">this article discussing performance solutions for jQuery</a></p>\n\n<p>It mentions:</p>\n\n<blockquote>\n  <p>There is a temptation among jQuery developers to hook everything into\n  the $(document).ready pseudo event. After all, it is used in most\n  examples you will find. Although $(document).ready is incredibly\n  useful, it occurs during page render while objects are still\n  downloading. If you notice your page stalling while loading, all those\n  $(document).ready functions could be the reason why. You can reduce\n  CPU utilization during the page load by binding your jQuery functions\n  to the $(window).load event, which occurs after all objects called by\n  the HTML (including  content) have downloaded.</p>\n</blockquote>\n\n<pre><code> $(window).load(function(){     // jQuery functions to initialize after\n the page has loaded. });\n</code></pre>\n\n<blockquote>\n  <p>Superfluous functionality such as drag and drop, binding visual\n  effects and animations, pre-fetching hidden images, etc., are all good\n  candidates for this technique.</p>\n</blockquote>\n\n<p>Some things have to be inline imho - otherwise the flicker of unstyled content is too strong. For everything else I'm trying to use lazy loading:\nfor example I'm using deferred event listeners on: auto select on focus for inputs, jquery tooltips, some drop down buttons, datepickers. I'm adding as well default buttons the user actually clicks when he presses enter.</p>\n\n<p>First of all: is this a good idea or do everything inline?\nSecondly: if this is a good idea - is using window.load() the way to go? When would you use which method?</p>\n"},{"tags":["ios","performance","uiscrollview"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":67,"score":0,"question_id":12898259,"title":"UIScrollView Performance Tips?","body":"<p>I have a UIScrollView that is animating a lot of UIViews (probably too many).  Also, these UIViews represent \"pages\", and sometimes multiple pages are stacked on top of each other, resulting in a \"pile\" of pages (setup by adding subviews to a given view).</p>\n\n<p>I know that scrolling an excessive number of UIViews can have poor performance, but I was wondering if anybody had some general tips for me to improve performance?  </p>\n\n<p>For now, doing drawing manually in drawRect is not something I would like to consider because it would mess up various \"page pile\" animations.  I will keep it in mind for a last resort, but I'd definitely like to avoid it if possible.</p>\n\n<p>Update:\nThe cause of the performance hit has been determined and is two fold: I'm using antialiasing and shadows on all my UIViews.  When I toggle them both off, the performance issues are resolved!  However, I obviously don't want to just toggle them off :)</p>\n\n<p>I'm creating my shadows like so:</p>\n\n<pre><code>self.imageView.layer.opaque = YES;\n        self.imageView.layer.masksToBounds = NO;\n        self.imageView.layer.shadowOffset = CGSizeMake(-4, 0);\n        self.imageView.layer.shadowRadius = 2.5;\n        self.imageView.layer.shadowOpacity = 0.15;\n        self.imageView.layer.shadowPath = [UIBezierPath bezierPathWithRect:CGRectMake(\n                                                                                      self.bounds.origin.x,\n                                                                                      self.bounds.origin.y,\n                                                                                      self.bounds.size.width + 8,\n                                                                                      self.bounds.size.height + 2)].CGPath;\n</code></pre>\n\n<p>Any tips to improve the performance?</p>\n\n<p>As far as antialiasing, it is almost a necessity.  The problem is that those \"offset pages\" are slightly rotated AND my pages have a 1 pixel border.  Slightly rotated with a 1 pixel border without antialiasing looks awful.  I am simply enabling antialiasing in the .plist by setting \"Renders with Edge Antialiasing\" to YES.</p>\n\n<p>Any suggestions on how to improve my shadow/antialiasing performance would be appreciated.</p>\n"},{"tags":["java","android","performance","class"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":90,"score":2,"question_id":12897241,"title":"Android: Is Java less effective with many classes","body":"<p>I am writing a game for android, and I am worry about performance and memory.\nIs Java less effective with many classes?</p>\n"},{"tags":["php","mysql","performance","search","tags"],"answer_count":2,"favorite_count":2,"up_vote_count":5,"down_vote_count":0,"view_count":97,"score":5,"question_id":12897817,"title":"PHP, MySQL, Efficient tag-driven search algorithm","body":"<p>I'm currenlty building a webshop. This shop allows users to filter products by <code>category</code>, and a couple optional, additional filters such as <code>brand</code>, <code>color</code>, etc.</p>\n\n<p>At the moment, various properties are stored in different places, but I'd like to switch to a tag-based system. Ideally, my database should store tags with the following data:</p>\n\n<ul>\n<li><code>product_id</code> </li>\n<li><code>tag_url_alias</code> (unique)</li>\n<li><code>tag_type</code> (unique) (category, product_brand, product_color, etc.)</li>\n<li><code>tag_value</code> (not unique)</li>\n</ul>\n\n<h1>First objective</h1>\n\n<p>I would like to search for <code>product_id</code>'s that are associated with anywhere <strong><em>between 1-5 particular tags</em></strong>. The tags are extracted from a SEO-friendly url. So I will be retrieving a unique strings (the <code>tag_url_alias</code>) for each tag, but I won't know the <code>tag_type</code>.\nThe search will be an <strong><em>intersection</em></strong>, so my search should return the <code>product_id</code>'s that match <strong><em>all</em></strong> of the provided <code>tags</code>.</p>\n\n<h1>Second objective</h1>\n\n<p>Besides displaying the products that match the current filter, I would also like to display the product-count for other categories and filters which the user might supply.</p>\n\n<p>For instance, my current search is for products that match the tags: </p>\n\n<pre><code>Shoe + Black + Adidas\n</code></pre>\n\n<p>Now, a visitor of the shop might be looking at the resulting products and wonder which black shoes other brands have to offer. So they might go to the \"brand\" filter, and choose any of the other listed brands. Lets say they have 2 different options (in practice, this will probably have many more), resulting in the following searches:</p>\n\n<pre><code>Shoe + Black + Nike &gt; 103 results\nShoe + Black + K-swiss &gt; 0 results\n</code></pre>\n\n<p>In this case, if they see the brand \"K-swiss\" listed as an available choise in their filter, their search will return 0 results.</p>\n\n<p>This is obviously rather disappointing to the user... I'd much rather know that switching the \"brand\" from \"adidas\" to \"k-swiss\" will 0 results, and simply remove the entire option from the filter.</p>\n\n<p>Same thing goes for categories, colors, etc.</p>\n\n<p>In practice this would mean a single page view would not only return the filtered product list described in my primary objective, but potentially hundreds of similar yet different lists. One for each filter value that could replace another filter value, or be added to the existing filter values.</p>\n\n<h1>Capacity</h1>\n\n<p>I suspect my database will eventually contain:</p>\n\n<blockquote>\n  <p>between 250 and 1.000 unique tags</p>\n</blockquote>\n\n<p>And it will contain:</p>\n\n<blockquote>\n  <p>between 10.000 and 100.000 unique products </p>\n</blockquote>\n\n<h1>Current Ideas</h1>\n\n<p>I did some Google searches and found the following article: <a href=\"http://www.pui.ch/phred/archives/2005/06/tagsystems-performance-tests.html\">http://www.pui.ch/phred/archives/2005/06/tagsystems-performance-tests.html</a></p>\n\n<p>Judging by that article, running hundreds of queries to achieve the 2nd objective, is going to be a painfully slow route. The \"toxy\" example might work for my needs and it might be acceptable for my First objective, but it would be unacceptably slow for the Second objective.</p>\n\n<p>I was thinking I might run individual queries that match 1 <code>tag</code> to it's associated <code>product_id</code>'s, cache those queries, and then calculate intersections on the results. But, do I calculate these intersections in MySQL? or in PHP? If I use MySQL, is there a particular way I should cache these individual queries, or is supplying the right indexes all I need?</p>\n\n<p>I would imagine it's also quite possible to maybe even cache the intersections between two of these <code>tag</code>/<code>product_id</code> sets. The amount of intersections would be limited by the fact that a <code>tag_type</code> can have only one particular value, but I'm not sure how to efficiently manage this type of caching. Again, I don't know if I should do this in MySQL or in PHP. And if I do this in MySQL, what would be the best way to store and combine this type of cached results?</p>\n"},{"tags":["performance","matlab","optimization","if-statement"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":1,"view_count":103,"score":2,"question_id":12895609,"title":"Optimization a recurring matlab code","body":"<p>I'm optimizing a model which takes some weather data and then converts the clouds into polygons, so that they can be further utilized. <br />\nThe code is working, but its kinds slow. By running the profiler I was able to found out the following lines are being called <code>106360430</code> times and takes about 50 secs to process. <br />\nIs there a way I can make these lines more efficient?</p>\n\n<pre><code>function [oddNodes] = pointInPolygon (point,thePolygon)\n% determine if a point is in the polygon (faster than matlab \"inpolygon\"command\n\npolyPoints=size(thePolygon,1);    % number of polygon points\noddNodes = false;\n\nj=polyPoints;\nx=point(1); y=point(2);\n\nfor i=1:polyPoints\nif (thePolygon(i,2)&lt;y &amp;&amp; thePolygon(j,2)&gt;=y ||  thePolygon(j,2)&lt;y &amp;&amp; thePolygon(i,2)&gt;=y)\nif (thePolygon(i,1)+(y-thePolygon(i,2))/(thePolygon(j,2)-thePolygon(i,2))*(thePolygon(j,1)-thePolygon(i,1))&lt;x)\noddNodes=~oddNodes;\nend\nend\nj=i; \nend\n</code></pre>\n"},{"tags":["asp.net-mvc-3","performance","debugging","profiling"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":42,"score":2,"question_id":12897336,"title":"ASP.NET MVC lost in finding botleneck","body":"<p>I have <code>ASP.NET MVC</code> app which accept file uploads and has result pooling using SignalR. The app hosted on Prod server with IIS7, 4 Gb Ram and two cores CPU. </p>\n\n<p>The app on Dev server works perfectly but when I host it on Prod server with about <code>50 000 users per day</code> the app become unrresponsible after five minutes of running. The web page request time increase dramatically and it takes about 30 seconds to load one page. I have tried to record all <code>MvcApplication.Application_BeginRequest</code> event call and got <code>9000</code> hits in <code>5 minutes</code>. Not sure is this acceptable number of hits or not for app like this.</p>\n\n<p>I have used ANTS Performance Profiler(not useful in Prod app profiling, slow and eats all memory) to profile code but profiler do not show any time delay issues in my code/MSSQL queries. </p>\n\n<p>Also I have tried to monitored CPU and RAM spike problems but I didn't find any. CPU percentage sometimes goes to 15% but never up and memory usage is normal.</p>\n\n<p>I suspect that there is something wrong with request or threads limits in ASP.NET/IIS7 but don't know how to profile it.  </p>\n\n<p>Could someone suggest any profiling solutions which could help in this situation? Tried to hunt the problem for two week already without any result :(</p>\n"},{"tags":["performance","oracle","jdbc"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":58,"score":1,"question_id":12896752,"title":"Oracle Perfomance - JDBC Oracle Thin driver: Use parameters or not?","body":"<p>I have two decades SQL experience, but not specifically with Oracle. An 'Oracle expert' assures me that building a SQL query without parameters (like this): </p>\n\n<pre><code>SELECT t.ID, t.Name, t.Address ... FROM Table1 t WHERE t.ID = 'someID' AND t.Name = 'someName'...\n</code></pre>\n\n<p>is at least as fast as using parameters (like this)</p>\n\n<pre><code>SELECT t.ID, t.Name, t.Address ... FROM Table1 t WHERE t.ID = ? AND t.Name = ?\n</code></pre>\n\n<p>The code is executed in a loop.</p>\n\n<p>In most other databases I have experience with, using parameters increases speed. It allows the database to cache the compiled plan that matches the SQL statement. Since the SQL does not change per invocation (although the parameters do) this improves performance. The database simply binds the parameters and continues.</p>\n\n<p>The 'Oracle expert' states that this is not necessary. But obviously, Oracle needs to 'parse out' the parameters, match the remaining string to a cached execution plan, then rebind the parameters as if they were passed along as parameters in the first place.  </p>\n\n<p>Do I have the correct mental picture here? Is there something 'magical' about Oracle that it really does not make a difference how we approach our parameter passing/SQL building strategy?</p>\n\n<p>Are there thoughts about Java / JDBC / Oracle thin driver that I am not aware of that I should be aware of here?</p>\n\n<p>I am looking to either reinforce my understanding or to expand my knowledge.</p>\n\n<p>(Security concerns aside please, I understand that building SQL strings allows for SQL injection attacks, I am looking for more direct ammunition to use against the experts opinion - if it exists).</p>\n\n<p>Other details: <code>Oracle 11gR2, Java 1.6</code></p>\n"},{"tags":[".net","vb.net","performance","assembly"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":121,"score":-1,"question_id":11195759,"title":"Use asm in VB.NET","body":"<p>I wanna call some ASM functions in VB.NET. How can I do it?\nIt's for performance purpose.</p>\n\n<p>I did not find anything on the web.</p>\n\n<p>Thx</p>\n"},{"tags":["asp.net","firefox","localhost","performance","loading"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":3633,"score":1,"question_id":2386299,"title":"Running sites on \"localhost\" is extremely slow","body":"<p>Having real trouble using my localhost to test sites. It runs extremely slowly! Sometimes it takes up to a minute to load a page. I'm using Firefox and the sites I'm testing run fine on other developers in my office's local machines / on the production server.</p>\n\n<p>I've gone through the normal things :-</p>\n\n<ol>\n<li>Disabled IPv6</li>\n<li>Not running in debug mode</li>\n<li>Put the site in the highest app pool (High Isolated) on IIS 6.</li>\n<li>Taking of firewalls etc.</li>\n</ol>\n\n<p>The problem only seems to occur when hitting pages which contain some form of .net code in the code-behind.</p>\n\n<p>Appreciate that this a little bit of a vague topic / stab in the dark but would appreciate any sort of advice - it's horrible waiting a minute each refresh to try out a change!</p>\n\n<p>Cheers, Sean.</p>\n"},{"tags":["python","performance","image","scroll","zoom"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":60,"score":0,"question_id":12895309,"title":"Python - Efficient way to display, scroll and zoom image / array with thousands of columns","body":"<p>I am looking for an efficient way to display, scroll and zoom into an image. I have an array with 256xn values that are calculated elsewhere (height x width). n is in the neighborhod of 15k to 50k. For starters lets assume that the window of the application is 256x1000 (height x with).</p>\n\n<p>The question now is: which library, algorithm or any other trick could I use to achieve the image to be displayable, scrollable and zoomable without the user noticing any lags in Python?</p>\n\n<p>I did some research but could not find anything that looked to promising (e.g. <a href=\"http://stackoverflow.com/questions/602557/image-viewer-standard-gui-controls-bottom-up-or-what/604916#604916\">this</a>). I'm sure I could pull off something, but was wondering if there might be an elegant way of doing so.</p>\n"},{"tags":["performance","redis","lru"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":52,"score":1,"question_id":12892396,"title":"Redis maxmemory-policy: performances of volatile-lru vs allkeys-lru","body":"<p>assuming all keys in a redis instance have an expire set, volatile-lru and allkeys-lru are similar. But is there a significative performance difference between the 2 when a key is removed?</p>\n\n<p>Bonus question:</p>\n\n<p>between 2 distinct instances configured with the allkeys-lru policy, having the same content and same configuration, except:</p>\n\n<ul>\n<li>Instance A has <em>all</em> its keys with an expire set (different values of expire)</li>\n<li>Instance B has <em>none</em> key with an expire set</li>\n</ul>\n\n<p>Aside the overhead of memory in instance A due to the expires bits, is there a performance difference between the 2 when a key is removed by the allkeys-lru algorithm?</p>\n\n<p>In both cases, I'm talking about instances of redis 2.4.x on linux 64 bits with maxmemory = 3Gb with 4-5000 keys when the maxmemory is reached (most of the keys are hashes).</p>\n\n<p>Thanks</p>\n"},{"tags":["c++","performance","optimization","gcc","compiler"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":104,"score":1,"question_id":12849170,"title":"Any areas where the GCC compiler is better than Intel's?","body":"<p>Are there any \"areas\" where the GCC C++ compiler is better than the Intel C++ compiler? </p>\n\n<p>I presumed optimization-wise Intel would win hands down, but wanted to double-check before assuming?</p>\n\n<p>It would also be interesting if there are any areas where both compilers are not particularly good/problems optimizing C++ code.</p>\n"},{"tags":["performance","algorithm"],"answer_count":17,"favorite_count":14,"up_vote_count":27,"down_vote_count":0,"view_count":16397,"score":27,"question_id":3947867,"title":"Find the least number of coins required that can make any change from 1 to 99 cents","body":"<p>Recently I challenged my co-worker to write an algorithm to solve this problem:</p>\n\n<blockquote>\n  <p>Find the least number of coins required that can make any change from 1 to 99 cents. The coins can only be pennies (1), nickels (5), dimes (10), and quarters (25), and you must be able to make every value from 1 to 99 (in 1-cent increments) using those coins.</p>\n</blockquote>\n\n<p>However, I realized that I don't actually know how to do this myself without examining every possible combination of coins. There has to be a better way of solving this problem, but I don't know what the generic name for this type of algorithm would be called, and I can't figure out a way to simplify it beyond looking at every solution.</p>\n\n<p>I was wondering if anybody could point me in the right direction, or offer up an algorithm that's more efficient.</p>\n"},{"tags":["multithreading","performance","optimization"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":74,"score":2,"question_id":12892941,"title":"CPU usage vs Number of threads","body":"<p>In general what is the relation between CPU usage and number of threads in a program.\nAssumptions:</p>\n\n<ul>\n<li>Multi-core CPU</li>\n<li>Threads do the exact same job (assume they fetch identical work items from a queue and process them)</li>\n</ul>\n"},{"tags":["javascript","performance","design-patterns","object","properties"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":38,"score":1,"question_id":12892849,"title":"Method that returns property VS property direct access in javascript","body":"<p>I have found that many javascript developers create methods that simply return a property like this :</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>function Obj (prop) {\n    this.prop = prop; // public\n}\nObj.prototype.getProp = function () {\n    return this.prop;\n};\n</code></pre>\n\n<p>While prop is public and can be accessed like this :</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>var a = obj.prop;\n</code></pre>\n\n<p>Moreover, I found that accessing an object property with a method is 121 times slower than accessing it directly (in Firefox)</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>var a, b,\n    obj = new Obj(1);\n\na = obj.prop;\n// ~6ns on Chrome\n// ~5ns on Firefox\n\nb = obj.getProp();\n// ~6ns on Chrome (no difference)\n// ~730ns on Firefox (122x slower...)\n</code></pre>\n\n<p>So my question is: should we always create methods that return properties or can we access properties directly? Is that antipattern?</p>\n"},{"tags":["android","performance","image","load"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":12686752,"title":"Android read file optimize performance","body":"<p>I have function to read image file az byte array.\nPerformance analisys gives to me interesting facts.\nI have check if file exists and then read as byte array.\nFile.exists spent 69.7% of time in function ?????\nOpen read and close spent only 30,3% of time.</p>\n\n<p>I cant't explaint to mysefl why?</p>\n\n<p>Is this depend on something or it is default behaviour?</p>\n\n<p>If this check is always slow - may be better approach is to open file without check for existing. And of cource use catch to solve case with missing file.</p>\n\n<p>UPDATE:\nFiles are stored in internal SD card. More than 20000 files.\nWhen test with 30 files percent is reduced to 23%.</p>\n"},{"tags":["performance","oracle","pagination"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":67,"score":1,"question_id":12881519,"title":"Oracle data paging optimization","body":"<pre><code>select * from (\n select t_tmp_a.*, rownum t_tmp_id from (\n select  t.*, i.counts  \n from table1 t, (select id, count(id) counts from table2 group by id) i\n where t.id=i.id and t.kindid in (0,1,3) order by t.id desc\n) t_tmp_a where rownum &lt;= 20) t_tmp_b where t_tmp_id &gt;= 11;\n</code></pre>\n\n<p>table1 and table2 have more then 2 million data per table, when execute this query need 18s , before this query execute we should calculation total count need about 7s, so it spends more than 25s， any idea to optimiza it?</p>\n"},{"tags":["performance","cross-platform","scientific-computing","generic-programming"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":45,"score":0,"question_id":12545460,"title":"Cross-platform development solutions for (mobile) performance computing","body":"<p>I am looking for a programming language/platform which is suited for handling around 1 to 2 GB of data, and can run signal processing algorithms fast enough across platforms (including modern mobile platforms). Did someone come across suitable solutions?. Please <strong>share</strong> your <strong>research</strong> and <strong>experience</strong> on cross-platform development.</p>\n\n<p>My current research effort is complicated, owing to</p>\n\n<ul>\n<li>the increased deployment of powerful [mobile] platforms, </li>\n<li>the backing of Java through the popularity of Android (currently the #1 tag on stackoverflow) </li>\n</ul>\n\n<p>I had mixed experience with C# and <a href=\"http://www.mono-project.com/Main_Page\" rel=\"nofollow\">Mono</a> (It is fast, but cross-platform compatibility can take some additional effort).</p>\n\n<p>I am <strong>not</strong> looking for <strong>High performance computing</strong>(HPC), but into <strong>developing code</strong> with a data and computing-intensive background <strong>that performs well</strong> with as little effort as possible. A case study scenario for this would be Python code which is adapted to the Cython compiler.</p>\n\n<p>I am <strong>not</strong> looking for <strong>UI-centric</strong> projects with solutions such as <a href=\"https://build.phonegap.com/\" rel=\"nofollow\">Phonegap</a> and Adobe Air, but I know little about its performance.</p>\n\n<p><strong>Links</strong>:</p>\n\n<ul>\n<li><a href=\"http://www.cs.colostate.edu/saxs/researchexam/GenericProgramming.pdf\" rel=\"nofollow\">Cross-Platform Development of High Performance Applications\nUsing Generic Programming</a></li>\n<li><a href=\"http://research.google.com/pubs/archive/34913.pdf\" rel=\"nofollow\">Native Client: A Sandbox for Portable, Untrusted x86 Native Code</a></li>\n</ul>\n"},{"tags":["java","performance"],"answer_count":6,"favorite_count":4,"up_vote_count":16,"down_vote_count":0,"view_count":3554,"score":16,"question_id":2589741,"title":"How to effectively copy an array in java?","body":"<p>The toArray method in ArrayList , Bloch uses both System.arraycopy and Arrays.copyOf to copy an array .</p>\n\n<pre><code>public &lt;T&gt; T[] toArray(T[] a) {\n  if (a.length &lt; size)\n        // Make a new array of a's runtime type, but my contents:\n        return (T[]) Arrays.copyOf(elementData, size, a.getClass());\n    System.arraycopy(elementData, 0, a, 0, size);\n    if (a.length &gt; size)\n        a[size] = null;\n    return a;\n}\n</code></pre>\n\n<p>How to compare these two copy methods , when to use which ? Thanks.</p>\n"},{"tags":["mvc","iis7","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":584,"score":0,"question_id":2320636,"title":"How can I optimize MVC and IIS pipeline to obtain higher speed?","body":"<p>I am doing performance tweaking of a simple app that uses MVC on IIS 7.5.\nI have a StopWatch starting up in Application_BeginRequest and I take a snapshot at Controller.OnActionExecuting.</p>\n\n<p>So I measure the time spend in the entire IIS pipeline: from request receipt to the moment execution finally gets to my controller.</p>\n\n<p>I obtain 700 microseconds on my 3GHz quad-core (project compiled Release x64), and I wonder where the bottleneck is, especially hearing some people say that one can get up to <a href=\"http://stackoverflow.com/questions/43743/asp-net-mvc-performance/530946\">8000 page loads per second</a> with MVC.</p>\n\n<p>How can I optimize MVC and IIS pipeline to obtain higher speed?</p>\n"},{"tags":["performance","nosql","cassandra","hector"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":49,"score":0,"question_id":12888954,"title":"Cassandra performance slow down with counter column","body":"<p>I have a cluster (4 node ) and a node have 16 core and 24 gb ram: </p>\n\n<pre><code>192.168.23.114  datacenter1 rack1       Up     Normal  44.48 GB        25.00%             \n192.168.23.115  datacenter1 rack1       Up     Normal  44.51 GB        25.00%\n192.168.23.116  datacenter1 rack1       Up     Normal  44.51 GB        25.00%\n192.168.23.117  datacenter1 rack1       Up     Normal  44.51 GB        25.00%\n</code></pre>\n\n<p>We use about 10 column family (counter column) to make some system statistic report.</p>\n\n<p>Problem on here is that When i set replication_factor of this keyspace from 1 to 2 (contain 10 counter column family ), all cpu of node increase from 10% ( when use replication factor=1) to ---> 90%. :( :(</p>\n\n<p>who can help me work around that :( . why counter column consume too much cpu time :(.</p>\n\n<p>thanks all</p>\n"},{"tags":["performance","comparison","integer","range"],"answer_count":7,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":793,"score":5,"question_id":3269434,"title":"What's the most efficient way to test two integer ranges for overlap?","body":"<p>Given two inclusive integer ranges [x1:x2] and [y1:y2], where x1 &lt;= x2 and y1 &lt;= y2, what is the most efficient way to test whether there is any overlap of the two ranges?</p>\n\n<p>A simple implementation is as follows:</p>\n\n<pre><code>bool testOverlap(int x1, int x2, int y1, int y2) {\n  return (x1 &gt;= y1 &amp;&amp; x1 &lt;= y2) ||\n         (x2 &gt;= y1 &amp;&amp; x2 &lt;= y2) ||\n         (y1 &gt;= x1 &amp;&amp; y1 &lt;= x2) ||\n         (y2 &gt;= x1 &amp;&amp; y2 &lt;= x2);\n}\n</code></pre>\n\n<p>But I expect there are more efficient ways to compute this.</p>\n\n<p>What method would be the most efficient in terms of fewest operations.</p>\n"},{"tags":["sql","performance","postgresql","file-io","csv"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":67,"score":0,"question_id":12756413,"title":"Check if records exists in a Postgres table","body":"<p>I have to read a CSV every 20 seconds. Each CSV contains min. of 500 to max. 60000 lines. I have to insert the data in a Postgres table, but before that I need to check if the items have already been inserted, because there is a high probability of getting duplicate item. The field to check for uniqueness is also indexed.</p>\n\n<p>So, I read the file in chunks and use the IN clause to get the items already in the database.</p>\n\n<p>Is there a better way of doing it? </p>\n"},{"tags":["performance","postgresql","strict","sql-function"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":84,"score":2,"question_id":8455177,"title":"Function executes faster without STRICT modifier?","body":"<p>I wonder about a slump in performance when a simple SQL function is declared <code>STRICT</code>. I stumbled upon this phenomenon while <a href=\"http://stackoverflow.com/a/8455172/939860\">answering a question here</a>.</p>\n\n<p>To demonstrate the effect I create two variants of a simple SQL function that orders two elements of an array in ascending order.</p>\n\n<h3>Test setup</h3>\n\n<pre><code>-- temporary table with 10000 random pairs of integer\nCREATE TEMP TABLE arr (i int[]);\n\nINSERT INTO arr \nSELECT ARRAY[(random() * 1000)::int, (random() * 1000)::int]\nFROM   generate_series(1,10000);\n</code></pre>\n\n<p>Function with <code>STRICT</code> modifier:</p>\n\n<pre><code>CREATE OR REPLACE FUNCTION f_sort_array1(int[])  RETURNS int[] AS\n$$\nSELECT CASE WHEN $1[1] &gt; $1[2] THEN ARRAY[$1[2], $1[1]] ELSE $1 END;\n$$ LANGUAGE sql STRICT IMMUTABLE;\n</code></pre>\n\n<p>Function without <code>STRICT</code> modifier (otherwise identical):</p>\n\n<pre><code>CREATE OR REPLACE FUNCTION f_sort_array2(int[])  RETURNS int[] AS\n$$\nSELECT CASE WHEN $1[1] &gt; $1[2] THEN ARRAY[$1[2], $1[1]] ELSE $1 END;\n$$ LANGUAGE sql IMMUTABLE;\n</code></pre>\n\n<h3>Results</h3>\n\n<p>I executed each around 20 times and took the best result from <code>EXPLAIN ANALYZE</code>.</p>\n\n<pre><code>SELECT f_sort_array1(i) FROM arr  -- Total runtime: 103 ms\nSELECT f_sort_array2(i) FROM arr  -- Total runtime:  43 ms (!!!)\n</code></pre>\n\n<p>These are the results from a v9.0.5 server on Debian Squeeze. Similar results on v8.4. Did not test on 9.1, have no cluster at my disposal right now. (Can someone supply additional results for v9.1?)</p>\n\n<p>Edit:\nIn a test with 10000 NULL values both functions perform the same in the same test environment: ~37 ms.</p>\n\n<p>I did some research and found an interesting gotcha. Declaring an SQL function <strong>STRICT disables function-inlining</strong> in most cases. More about that in the <a href=\"http://www.postgresonline.com/journal/archives/163-STRICT-on-SQL-Function-Breaks-In-lining-Gotcha.html\" rel=\"nofollow\">PostgreSQL Online Journal</a> or in the <a href=\"http://archives.postgresql.org/pgsql-performance/2011-11/msg00119.php\" rel=\"nofollow\">pgsql-performance mailing list</a>.</p>\n\n<p>But I am not quite sure how this could be the explanation. How can not inlining the function cause the performance slump in this simple scenario? No index, no disc read, no sorting. Maybe an overhead from the repeated function call that is streamlined away by inlining the function? Can you explain it? Or am I missing something?</p>\n\n<hr>\n\n<h3>Retest with Postgres 9.1</h3>\n\n<p>Ran the same test on the same hardware with PostgreSQL 9.1 an found even bigger differences:</p>\n\n<pre><code>SELECT f_sort_array1(i) FROM arr  -- Total runtime: 107 ms\nSELECT f_sort_array2(i) FROM arr  -- Total runtime:  27 ms (!!!)\n</code></pre>\n"},{"tags":["java","ruby-on-rails","ruby","performance","playframework-2.0"],"answer_count":2,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":205,"score":4,"question_id":12886667,"title":"Play! framework vs Ruby on Rails","body":"<p>It's not going to be a request for general comparisson: </p>\n\n<p><strong>Play! framework is Java based</strong> which means the code is interpreted to bytecode and then compiled by the JVM in runtime. On the other hand, <strong>Ruby is a dynamic language</strong> which means the code is interpreted with every request. This is certainly obvious for every programmer.</p>\n\n<p>Another aspect is the development process and the ease of the language (strong typing vs weak typing).</p>\n\n<p>Currently I'm developing a new website using Play!<br>\nSo, for the questions:  </p>\n\n<ol>\n<li><strong>Performance for an HTTP server</strong> (Play! runs on the JVM, Ruby is dynamic) - does it really matter for a website? would you see a significant differences? </li>\n<li>I feel RoR has much <strong>larger community, sources, tutorials etc</strong>, and it's a little batter me. Or should it?</li>\n</ol>\n"},{"tags":["python","performance","memory","memory-management"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":134,"score":3,"question_id":12872244,"title":"Python: memory problems in a script","body":"<p>I have wrote a script where i read around 4 million of points and 800.000 plots. The script clip the points inside each plot and save a new text file for each plot.</p>\n\n<p>After a certain period of time my PC memory is full. I had tried to dig inside my script but in each loop <code>for i in xrange(len(sr)):</code> the each object is replaced and the points clipped saved in a new txt file.</p>\n\n<p>are there some strategy to use in this case in order to improve memory usage without reduce the performance(the script is already slow)? I am a beginner in python and sorry if the question is simple.</p>\n\n<p>Thanks in advance\nGianni</p>\n\n<pre><code>inFile =\"C://04-las_clip_inside_area//prova//Ku_115_class_Notground_normalize.las\"\npoly =\"C://04-las_clip_inside_area//prova//ku_115_plot_clip.shp\"\nchunkSize = None\nMinPoints = 1\n\nsf = shapefile.Reader(poly) #open shpfile\nsr = sf.shapeRecords()\npoly_filename, ext = path.splitext(poly)\ninFile_filename = os.path.splitext(os.path.basename(inFile))[0]\npbar = ProgressBar(len(sr)) # set progressbar\nif chunkSize == None:\n    points = [(p.x,p.y) for p in lasfile.File(inFile,None,'r')]\n    for i in xrange(len(sr)):\n        pbar.update(i+1) # progressbar\n        verts = np.array(sr[i].shape.points,float)\n        record = sr[i].record[0]\n        index = nonzero(points_inside_poly(points, verts))[0]\n        if len(index) &gt;= MinPoints:\n            file_out = open(\"{0}_{1}_{2}.txt\".format(poly_filename, inFile_filename, record), \"w\")\n            inside_points = [lasfile.File(inFile,None,'r')[l] for l in index]\n            for p in inside_points:\n                file_out.write(\"%s %s %s %s %s %s %s %s %s %s %s\" % (p.x, p.y, p.z, p.intensity,p.return_number,p.number_of_returns,p.scan_direction,p.flightline_edge,p.classification,p.scan_angle,record)+ \"\\n\")\n            file_out.close()\n</code></pre>\n\n<p>this is the origial function</p>\n\n<pre><code>def LAS2TXTClipSplitbyChunk(inFile,poly,chunkSize=1,MinPoints=1):\n    sf = shapefile.Reader(poly) #open shpfile\n    sr = sf.shapeRecords()\n    poly_filename, ext = path.splitext(poly)\n    inFile_filename = os.path.splitext(os.path.basename(inFile))[0]\n    pbar = ProgressBar(len(sr)) # set progressbar\n    if chunkSize == None:\n        points = [(p.x,p.y) for p in lasfile.File(inFile,None,'r')]\n        for i in xrange(len(sr)):\n            pbar.update(i+1) # progressbar\n            verts = np.array(sr[i].shape.points,float)\n            record = sr[i].record[0]\n            index = nonzero(points_inside_poly(points, verts))[0]\n            if len(index) &gt;= MinPoints:\n                file_out = open(\"{0}_{1}_{2}.txt\".format(poly_filename, inFile_filename, record), \"w\")\n                inside_points = [lasfile.File(inFile,None,'r')[l] for l in index]\n                for p in inside_points:\n                    file_out.write(\"%s %s %s %s %s %s %s %s %s %s %s\" % (p.x, p.y, p.z, p.intensity,p.return_number,p.number_of_returns,p.scan_direction,p.flightline_edge,p.classification,p.scan_angle,record)+ \"\\n\")\n                file_out.close()\n    else:\n        for i in xrange(len(sr)):\n            pbar.update(i+1) # progressbar\n            verts = np.array(sr[i].shape.points,float)\n            record = sr[i].record[0]\n            f = lasfile.File(inFile,None,'r')\n            file_out = open(\"{0}_{1}_{2}.txt\".format(poly_filename, inFile_filename, record), \"w\")\n            TotPoints = 0\n            while True:\n                chunk = list(islice(f,chunkSize))\n                if not chunk:\n                    break\n                points = [(p.x,p.y) for p in chunk]\n                index = nonzero(points_inside_poly(points, verts))[0]\n                TotPoints += len(index) #add points to count inside th plot\n                chunk = [chunk[l] for l in index]\n                for p in chunk:\n                    file_out.write(\"%s %s %s %s %s %s %s %s %s %s %s\" % (p.x, p.y, p.z, p.intensity,p.return_number,p.number_of_returns,p.scan_direction,p.flightline_edge,p.classification,p.scan_angle,record)+ \"\\n\")\n            if TotPoints &gt;= MinPoints:\n                file_out.close()\n            else:\n                file_out.close()\n                os.remove(\"{0}_{1}_{2}.txt\".format(poly_filename, inFile_filename, record))\n            f.close()\n</code></pre>\n\n<p>the script by the suggestion of unutbu is:</p>\n\n<pre><code>import shapefile\nimport os\nimport glob\nfrom os import path\nimport numpy as np\nfrom numpy import nonzero\nfrom matplotlib.nxutils import points_inside_poly\nfrom itertools import islice\nfrom liblas import file as lasfile\nfrom shapely.geometry import Polygon\nfrom progressbar import ProgressBar\nimport multiprocessing as mp\n\n\ninFile =\"C://04-las_clip_inside_area//prova//Ku_115_class_Notground_normalize.las\"\npoly =\"C://04-las_clip_inside_area//prova//ku_115_plot_clip.shp\"\nchunkSize = None\nMinPoints = 1\n\ndef pointinside(record):\n    verts = np.array(record.shape.points, float)\n    record = record.record[0]\n    index = nonzero(points_inside_poly(points, verts))[0]\n    if len(index) &gt;= MinPoints:\n        outfile = \"{0}_{1}_{2}.txt\".format(poly_filename, inFile_filename, record)\n        with open(outfile, \"w\") as file_out:\n            inside_points = [lasfile.File(inFile, None, 'r')[l] for l in index]\n            for p in inside_points:\n                fields = (p.x, p.y, p.z, p.intensity, p.return_number,\n                          p.number_of_returns, p.scan_direction, p.flightline_edge,\n                          p.classification, p.scan_angle, record)\n                file_out.write(' '.join(map(str, fields)) + \"\\n\")\n\nsf = shapefile.Reader(poly) #open shpfile\nsr = sf.shapeRecords()\npoly_filename, ext = path.splitext(poly)\ninFile_filename = os.path.splitext(os.path.basename(inFile))[0]\npbar = ProgressBar(len(sr)) # set progressbar\nif chunkSize == None:\n    points = [(p.x,p.y) for p in lasfile.File(inFile,None,'r')]\n    for i in xrange(len(sr)):\n        pbar.update(i+1) # progressbar\n        proc = mp.Process(target = pointinside, args = (sr[i], ))\n        proc.start()\n        proc.join()\n</code></pre>\n"},{"tags":["java","performance","jvm"],"answer_count":4,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":1369,"score":2,"question_id":4687757,"title":"Tools to monitor java thread execution","body":"<p>I've a java web application running on an Tomcat server(Linux). In the production environment I'm facing some performance issue. At random intervals the jsvc process on which tomcat is running starts to run at 90-100% CPU. I'm unable to find out the trigger for this event. The server is a quad core system. Memory conception does not indicate any abnormalities.</p>\n\n<p>How can I monitor which thread(application stack trace) in the application is causing the problem?</p>\n\n<p>I'm checking with <a href=\"http://java.sun.com/developer/technicalArticles/J2SE/jconsole.html\" rel=\"nofollow\">jconsole</a> and <a href=\"http://code.google.com/p/psi-probe/\" rel=\"nofollow\">PSI Probe</a>, but both are not giving any detailed information about what thread inside the application is causing the CPU usage abnormality.</p>\n"},{"tags":["php","performance","content-management-system"],"answer_count":7,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":364,"score":1,"question_id":1159413,"title":"Make PHP code as small as possible while not reducing performance?","body":"<p>I'm writing a Content Management System in PHP, and I want it to be the smallest one in the world. I'm planning to make it available to everyone, just like Drupal and Joomla. But to make it so ultra-tiny, I change code to smaller code.</p>\n\n<p>For example, I change:</p>\n\n<pre><code>$info = parse_ini_file(\"info.scm\"); /* to */ $i=parse_ini_file(\"info.scm\");\n</code></pre>\n\n<p>just to make it smaller. But, I use some functions very often, like preg_replace();. I use it over 30 times. Should I make a function like:</p>\n\n<pre><code>function p($p,$r,$s){preg_replace($p,$r,$s);}\n//and than just use:\np($my_regex, $my_replacement, $my_string);\n</code></pre>\n\n<p>or does this make it all work slower?</p>\n\n<p>Notice that my goal is to make it so tiny as possible.</p>\n"},{"tags":["c#","performance","linked-list","binary-search","linear-search"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":68,"score":0,"question_id":12887872,"title":"Will a binary search on LinkedList to insert a value into the middle of a sorted value list increase performance?","body":"<p>I need to create a sorted list adding one item at a time. So I decided to go with <strong>LinkedList</strong>, Since it is efficient in <strong>insert</strong> operations. But when finding the proper location, it seems to take much longer time. I am using linear search to get the location. If I use binary search to get the proper location using <strong>elementAt()</strong> method, will it increase the performance.  According to <a href=\"http://stackoverflow.com/questions/10164355/how-do-i-get-the-n-th-element-in-a-linkedlistt\">this</a>, still it is a O(n) operation. What do you think? If it is so, is there any other better data structure for the work? Because if I use a different datatype, when inserting a new value to the middle, I will have to shift all the data after that location by one location, which is obviously not a .</p>\n"},{"tags":["python","performance","data-structures","iterate"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":79,"score":3,"question_id":12886611,"title":"High-performance way to eliminate semi-duplicate items from a list","body":"<p>I have a series of puzzles: Strings of morse code with no spaces between the letters or words.  My plan is to do a dictionary attack to find the best solution candidates. My weapon is Python.</p>\n\n<p>I have a list of 17000 English words. I also have a much smaller list of words that are pertinent to the puzzle's theme, and if those words show up they should score higher.</p>\n\n<p>So at the very beginning of my script when I generate the list of words, I use a list of tuples of the form (word, scoremultiplier). Here's a small subset:</p>\n\n<pre><code>[('zoned', 1.0), \n ('zonely', 1.0), \n ('zoner', 1.0), \n ('zones', 1.0), \n ('zoning', 1.0), \n ('zoo', 1.0), \n ('zoom', 1.0), \n ('zoomed', 1.0), \n ('zooming', 1.0), \n ('zooms', 1.0), \n ('zoos', 1.0), \n ('ten', 1.0), \n ('tens', 1.0), \n ('gnash', 1.0), \n ('shag', 1.0), \n ('75th', 2.0), \n ('seventy', 2.0), \n ('fifth', 2.0)]\n</code></pre>\n\n<p>In the file that I parse all that out of, I want to just stick the high-value words at the end, without manually getting rid of any duplicates in the main part of the file. So I need to write something to get rid of the early tuples whose first value is equal to that of a later tuple.</p>\n\n<p>I can do this with brute force:</p>\n\n<pre><code>for firstkey, (firstword, firstfactor) in enumerate(wordlist):\n    for laterkey, (laterword, laterfactor) in enumerate(wordlist[firstkey+1:]):\n        if firstword == laterword:\n            del wordlist[firstkey]\n            break\n</code></pre>\n\n<p>But that part of the script alone takes almost 45 seconds, and my 17000 words isn't even a full dictionary. (That code is also untested other than the time it takes to finish, so it may not even work.) It also seems very un-Pythony, though I'm just now learning Python (and doing some of my first programming at all) with this very project.</p>\n\n<p>Is there a better way to do this? I can't use <code>set()</code> because the duplicate words are part of nonequal tuples. Do I need to restructure my data somehow? Or should I just be prepared to wait a full minute every time I run this?</p>\n"},{"tags":["c++","performance","variables"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":92,"score":2,"question_id":12887342,"title":"Is it better to check a variable before setting its value in C++?","body":"<p>If I have a boolean and some code which maybe changes it, and then I want to set it to <code>true</code>, should I check if it's <code>false</code>?</p>\n\n<p>For example:</p>\n\n<pre><code>bool b = false;\n// Some code\n// Here \"b\" can be true or false\nif (cond) {\n    b = true;\n}\n</code></pre>\n\n<p>vs</p>\n\n<pre><code>bool b = false;\n// Some code\n// Here `b` can be `true` or `false`\nif (cond &amp;&amp; !b){\n    b = true;\n}\n</code></pre>\n\n<p>Which is faster?</p>\n\n<p><strong>Note</strong>:</p>\n\n<p>I ask that because of the following implementation of <a href=\"http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes\" rel=\"nofollow\">Sieve of Eratosthenes</a>: <a href=\"http://bloc.gerardfarras.com/wp-content/uploads/2011/12/erastotenes.txt\" rel=\"nofollow\">http://bloc.gerardfarras.com/wp-content/uploads/2011/12/erastotenes.txt</a></p>\n\n<pre><code>if (( i % divisor == 0 ) &amp;&amp; ( numsprimers[i] == 0 )) {\n    numsprimers[i] = 1;\n}\n</code></pre>\n\n<p>(If <code>numsprimers[i]==1</code> it means that <code>i</code> isn't a prime number. And if it's 0 it can be prime or not)</p>\n"},{"tags":["mysql","performance","innodb","sqlperformance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":33,"score":0,"question_id":12887646,"title":"What should i configure to able insert huge data to my INNODB tables?","body":"<p>I insert big data to my tables.\nSome of the tables not insert the data because to much rows.</p>\n\n<p>How should be number of rows enlarged in INNODB?</p>\n\n<p>(I try ALTER TABLE table MAX_ROWS=1000000000, after i create the empty tables, but after i insert to much rows the table not insert.)</p>\n\n<p>Edit:</p>\n\n<p>Simplify my Case:</p>\n\n<p>I have 10 text files that i insert to INNDOB database + tables(new installation of mysql 5.5 windows server 2008 with):</p>\n\n<pre><code>LOAD DATA LOCAL INFILE\n</code></pre>\n\n<p>Its works for 8 files, but 2 huge text file not success no insert to tables(when i cut them and make them small they are inserted soo it limit table problem).</p>\n\n<p>Thanks</p>\n"},{"tags":["performance","opencl","gpgpu"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":111,"score":0,"question_id":12846809,"title":"Why is this simple OpenCL kernel running so slowly?","body":"<p>I'm looking into OpenCL, and I'm a little confused why this kernel is running so slowly, compared to how I would expect it to run. Here's the kernel: </p>\n\n<pre><code>__kernel void copy(\n  const __global char* pSrc, \n  __global __write_only char* pDst, \n  int length)\n{\n  const int tid = get_global_id(0);\n\n  if(tid &lt; length) {\n    pDst[tid] = pSrc[tid];\n  }\n}\n</code></pre>\n\n<p>I've created the buffers in the following way:</p>\n\n<pre><code>char* out = new char[2048*2048];\ncl::Buffer(\n  context,\n  CL_MEM_USE_HOST_PTR | CL_MEM_WRITE_ONLY,\n  length,\n  out);\n</code></pre>\n\n<p>Ditto for the input buffer, except that I've initialized the in pointer to random values. Finally, I run the kernel this way: </p>\n\n<pre><code>cl::Event event;\nqueue.enqueueNDRangeKernel(\n  kernel, \n  cl::NullRange,\n  cl::NDRange(length),\n  cl::NDRange(1), \n  NULL, \n  &amp;event);\n\nevent.wait();\n</code></pre>\n\n<p>On average, the time is around 75 milliseconds, as calculated by: </p>\n\n<pre><code>cl_ulong startTime = event.getProfilingInfo&lt;CL_PROFILING_COMMAND_START&gt;();\ncl_ulong endTime = event.getProfilingInfo&lt;CL_PROFILING_COMMAND_END&gt;();\nstd::cout &lt;&lt; (endTime - startTime) * SECONDS_PER_NANO / SECONDS_PER_MILLI &lt;&lt; \"\\n\";\n</code></pre>\n\n<p>I'm running Windows 7, with an Intel i5-3450 chip (Sandy Bridge architecture). For comparison, the \"direct\" way of doing the copy takes less than 5 milliseconds. I don't think the event.getProfilingInfo includes the communication time between the host and device. Thoughts? </p>\n\n<p>EDIT: </p>\n\n<p>At the suggestion of ananthonline, I changed the kernel to use float4s instead of chars, and that dropped the average run time to about 50 millis. Still not as fast as I would have hoped, but an improvement. Thanks ananthonline!</p>\n"},{"tags":["java","c++","performance","logic"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":4,"view_count":88,"score":-4,"question_id":12886799,"title":"Is a for loop that ends up not even running once just as fast as calling an if statement to check the size first?","body":"<p>I am trying to optimize a chunk of code where speed is very important and wondered if checking  the <code>int</code> that holds the number of times a for loop is about to loop and not doing the for loop if it is equal to zero was any faster or slower than just letting the for loop execute 0 times.</p>\n\n<p>I realize that any speed improvement would be tiny; it just started to become more of a curiosity. Also would this be different from Java to say C++ or C?</p>\n\n<p>Example:</p>\n\n<pre><code>size=0;\nfor (int i = 0;i&lt;size;i++)\n{\n}\n</code></pre>\n\n<p>or</p>\n\n<pre><code>size=0;\nif (size!=0)\n{ \n    for (int i = 0;i&lt;size;i++)\n    {\n    }\n}\n</code></pre>\n\n<p>Of course, in the real code the size is often not zero, but when it is which would be faster if either?</p>\n"},{"tags":["c#","asp.net","performance","web","web-performance-test"],"answer_count":4,"favorite_count":0,"up_vote_count":3,"down_vote_count":1,"view_count":56,"score":2,"question_id":12637844,"title":"Does space matters on a page for web application","body":"<p>My client complained me that my web page contains so many spaces so it is not good for the performance.</p>\n\n<p>Does the space matters in web pages?</p>\n\n<p><strong>UPDATE :</strong></p>\n\n<p>I mean whitespace for performance and cost issue.</p>\n"},{"tags":["sql","sql-server","performance","client","native"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":113,"score":3,"question_id":12877300,"title":"SQL Native Client 10 Performance miserable (due to server-side cursors)","body":"<p>we have an application that uses ODBC via CDatabase/CRecordset in MFC (VS2010).\nWe have two backends implemented. MSSQL and MySQL.</p>\n\n<p>Now, when we use MSSQL (with the Native Client 10.0), retrieving records with SELECT is dramatically slow via slow links (VPN, for example). The MySQL ODBC driver does not exhibit this nasty behavior.</p>\n\n<p>For example:</p>\n\n<pre><code>CRecordset r(&amp;m_db);\nr.Open(CRecordset::snapshot, L\"SELECT a.something, b.sthelse FROM TableA AS a LEFT JOIN TableB AS b ON a.ID=b.Ref\");\nr.MoveFirst();\nwhile(!r.IsEOF())\n{\n    // Retrieve\n    CString strData;\n    crs.GetFieldValue(L\"a.something\", strData);\n    crs.MoveNext();\n}\n</code></pre>\n\n<p>Now, with the MySQL driver, everything runs as it should. The query is returned, and everything is lightning fast.\nHowever, with the MSSQL Native Client, things slow down, because on every MoveNext(), the driver communicates with the server.</p>\n\n<p>I think it is due to server-side cursors, but I didn't find a way to disable them. I have tried using:</p>\n\n<pre><code>::SQLSetConnectAttr(m_db.m_hdbc, SQL_ATTR_ODBC_CURSORS, SQL_CUR_USE_ODBC, SQL_IS_INTEGER);\n</code></pre>\n\n<p>But this didn't help either. There are still long-running exec's to sp_cursorfetch() et al in SQL Profiler.\nI have also tried a small reference project with SQLAPI and bulk fetch, but that hangs in FetchNext() for a long time, too (even if there is only one record in the resultset).\nThis however only happens on queries with LEFT JOINS, table-valued functions, etc.\n<em>Note that the query doesn't take that long</em> - executing the same SQL via SQL Studio over the same connection returns in a reasonable time.</p>\n\n<p><strong>Question1: Is is possible to somehow get the native client to <del>\"cache\" all results locally </del> use local cursors in a similar fashion as the MySQL driver seems to do it?</strong></p>\n\n<p>Maybe this is the wrong approach altogether, but I'm not sure how else to do this.</p>\n\n<p>All we want is to retrieve all data at once from a SELECT, then never talk the server again until the next query.\nWe don't care about recordset updates, deletes, etc or any of that nonsense. We only want to retrieve data.\nWe take that recordset, get all the data, and delete it.</p>\n\n<p><strong>Question2: Is there a more efficient way to just retrieve data in MFC with ODBC?</strong></p>\n"},{"tags":["javascript","jquery","ajax","performance","post"],"answer_count":5,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":82,"score":1,"question_id":12880539,"title":"Native or jQuery for a large web app","body":"<p>I know that coding in native JavaScript means that your code will execute faster than if you were to code it in jQuery, but how much faster?</p>\n\n<p>In particular, I want to know if the speed increase would make it worth while spending the longer time coding in native JavaScript than jQuery if it was for a very large webapp?</p>\n\n<p>Or is the difference in speed not that great at all?</p>\n\n<p>For instance, to setup an AJAX request in jQuery all you have to do is call <code>$.ajax</code> or <code>$.post</code> and pass a few parameters, but with native JavaScript you have to create <code>XMLHttpRequest</code> or <code>ActiveXObject</code> objects depending on the users browser etc etc.</p>\n"},{"tags":["python","performance","iterator"],"answer_count":4,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":170,"score":6,"question_id":12775449,"title":"Group an iterable by a predicate in Python","body":"<p>I'm parsing a file like this:</p>\n\n<pre>\n--header--\ndata1\ndata2\n--header--\ndata3\ndata4\ndata5\n--header--\n--header--\n...\n</pre>\n\n<p>And I want groups like this:</p>\n\n<pre><code>[ [header, data1, data2], [header, data3, data4, data5], [header], [header], ... ]\n</code></pre>\n\n<p>so I can iterate over them like this:</p>\n\n<pre><code>for grp in group(open('file.txt'), lambda line: 'header' in line):\n    for item in grp:\n        process(item)\n</code></pre>\n\n<p>and keep the detect-a-group logic separate from the process-a-group logic.</p>\n\n<p>But I need an iterable of iterables, as the groups can be arbitrarily large and I don't want to store them.  That is, I want to split an iterable into subgroups every time I encounter a \"sentinel\" or \"header\" item, as indicated by a predicate.  Seems like this would be a common task, but I can't find an efficient Pythonic implementation.</p>\n\n<p>Here's the dumb append-to-a-list implementation:</p>\n\n<pre><code>def group(iterable, isstart=lambda x: x):\n    \"\"\"Group `iterable` into groups starting with items where `isstart(item)` is true.\n\n    Start items are included in the group.  The first group may or may not have a \n    start item.  An empty `iterable` results in an empty result (zero groups).\"\"\"\n    items = []\n    for item in iterable:\n        if isstart(item) and items:\n            yield iter(items)\n            items = []\n        items.append(item)\n    if items:\n        yield iter(items) \n</code></pre>\n\n<p>It feels like there's got to be a nice <code>itertools</code> version, but it eludes me.  The 'obvious' (?!) <code>groupby</code> solution doesn't seem to work because there can be adjacent headers, and they need to go in separate groups.  The best I can come up with is (ab)using <code>groupby</code> with a key function that keeps a counter:</p>\n\n<pre><code>def igroup(iterable, isstart=lambda x: x):\n    def keyfunc(item):\n        if isstart(item):\n            keyfunc.groupnum += 1       # Python 2's closures leave something to be desired\n        return keyfunc.groupnum\n    keyfunc.groupnum = 0\n    return (group for _, group in itertools.groupby(iterable, keyfunc))\n</code></pre>\n\n<p>But I feel like Python can do better -- and sadly, this is even slower than the dumb list version:</p>\n\n<pre>\n# ipython\n%time deque(group(xrange(10 ** 7), lambda x: x % 1000 == 0), maxlen=0)\nCPU times: user 4.20 s, sys: 0.03 s, total: 4.23 s\n\n%time deque(igroup(xrange(10 ** 7), lambda x: x % 1000 == 0), maxlen=0)\nCPU times: user 5.45 s, sys: 0.01 s, total: 5.46 s\n</pre>\n\n<p>To make it easy on you, here's some unit test code:</p>\n\n<pre><code>class Test(unittest.TestCase):\n    def test_group(self):\n        MAXINT, MAXLEN, NUMTRIALS = 100, 100000, 21\n        isstart = lambda x: x == 0\n        self.assertEqual(next(igroup([], isstart), None), None)\n        self.assertEqual([list(grp) for grp in igroup([0] * 3, isstart)], [[0]] * 3)\n        self.assertEqual([list(grp) for grp in igroup([1] * 3, isstart)], [[1] * 3])\n        self.assertEqual(len(list(igroup([0,1,2] * 3, isstart))), 3)        # Catch hangs when groups are not consumed\n        for _ in xrange(NUMTRIALS):\n            expected, items = itertools.tee(itertools.starmap(random.randint, itertools.repeat((0, MAXINT), random.randint(0, MAXLEN))))\n            for grpnum, grp in enumerate(igroup(items, isstart)):\n                start = next(grp)\n                self.assertTrue(isstart(start) or grpnum == 0)\n                self.assertEqual(start, next(expected))\n                for item in grp:\n                    self.assertFalse(isstart(item))\n                    self.assertEqual(item, next(expected))\n</code></pre>\n\n<p>So: how can I subgroup an iterable by a predicate elegantly and efficiently in Python?</p>\n"},{"tags":["performance","interpreter","brainfuck"],"answer_count":4,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":985,"score":5,"question_id":5543360,"title":"Fastest brainfuck interpreter?","body":"<p>Simple question: <strong>What is the fastest brainfuck interpreter available?</strong></p>\n\n<p>I am asking this because I am about to write my own optimizing bf interpreter and I need something to compare it with.</p>\n"},{"tags":["php","performance","mvc","cakephp"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":61,"score":0,"question_id":12885270,"title":"Would there be benefit in CakePHP performance if it dropped support for JS/AJAX engines?","body":"<p>My main Question:</p>\n\n<p><strong>Is there any practical effect on performance of CakePHP (i.e. faster view rendering), if support for JS helpers was dropped from the core completely?</strong></p>\n\n<p>My reasoning: </p>\n\n<p>Right now CakePHP has a few classes that allow a programmer to create basic client-side code using PHP, both for things like effects and AJAX requests.</p>\n\n<p>There is some coupling with the View object, which could be degrading to performance.</p>\n\n<p>Considering that a lot of frameworks are moving to a RESTful model and in general it is hard to keep up with the changes of the client-side frameworks, while coupling them with the server-side framework, like CakePHP.</p>\n\n<p>I am wondering if it's worthwhile to drop support for JS/AJAX and focus on PHP framework patterns. Of course we lose the ability to write some JS code through the helpers, but in my opinion it is still best left to a JS framework. </p>\n\n<p>The benefits are reduced coupling, lighter weight, and possibly improved performance. </p>\n"},{"tags":["performance","plc"],"answer_count":6,"favorite_count":5,"up_vote_count":10,"down_vote_count":0,"view_count":3194,"score":10,"question_id":1361396,"title":"Being a better / more efficient PLC Programmer","body":"<p>The company I am doing my intership/appretinceship in, does mainly PLC programming with Siemens modules.\nComes from the fact that most of the people were electric guys and switched over to engineering.</p>\n\n<p>My problem as newbie there is, that I can't be really efficient and fast when I code PLC software.</p>\n\n<p>Even though I am very efficient when I am coding C# or Java in VS/Eclipse</p>\n\n<p>It really bothers that I can't be really productive with PLC as opposed to the \"real\" programming languages.</p>\n\n<ul>\n<li>Is it the lack of code completion?</li>\n<li>Is it the lack of overall knowledge on the automation side?</li>\n<li>Is it the lack of innovation in PLC as opposed to VS (LINQ, Dynamics, Lambda)</li>\n</ul>\n\n<p>Have you guys any good experience with PLC?\nAnd how did you get productive with it?</p>\n\n<p>Notice: It is my last year at the company, that's also why I want to be very productive.</p>\n\n<p>Looking forward to many great answers!</p>\n"},{"tags":["python","performance","multiprocessing","cpu"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":64,"score":0,"question_id":12883237,"title":"Python: improve the efficency of my script using multiprocessing module (tips and suggestions)","body":"<p>I am a beginner of Python (few weeks) and recently i had read some post in Stackoverflow about <strong>multiprocessing module</strong>. Normally i work with million of points format data (*.las file. This <a href=\"http://vimeo.com/16282938\" rel=\"nofollow\">video</a> to understand the source of my data) and I have interest to understand better how use multiprocessing module.  </p>\n\n<p><strong>I use Python 2.7 on windows 7, intel core i7-3770CPU</strong> </p>\n\n<p>Normally I use this <strong>def</strong> wrote from me as a Benchmark to understand:</p>\n\n<pre><code># load line-by-line the las file, check if the points are inside the polygon\n# if yes save a new *.las file\n\nimport shapefile\nimport numpy\nimport numpy as np\nfrom numpy import nonzero\nfrom matplotlib.mlab import griddata\nfrom matplotlib.nxutils import pnpoly\nfrom liblas import file as lasfile\n\ndef LAS2LASClip(inFile,poly,outFile):\n    f = lasfile.File(inFile,None,'r') # open LAS\n    h = f.header\n    # change the software id to libLAS\n    h.software_id = \"Python 2.7\"\n    file_out = lasfile.File(outFile,mode='w',header= h)\n    f.close()\n    sf = shapefile.Reader(poly) #open shpfile\n    shapes = sf.shapes()\n    for i in xrange(len(shapes)):\n        verts = np.array(shapes[i].points,float)\n        inside_points = [p for p in lasfile.File(inFile,None,'r') if pnpoly(p.x, p.y, verts)]\n        for p in inside_points:\n            file_out.write(p)\n    file_out.close()\n</code></pre>\n\n<p>Thanks in advance\nGianni</p>\n"},{"tags":["mysql","sql","performance","table","entity-attribute-value"],"answer_count":2,"favorite_count":2,"up_vote_count":1,"down_vote_count":0,"view_count":108,"score":1,"question_id":12882531,"title":"Entity attribute value model - Performance alternative?","body":"<p>I work with PHP and mySQL.</p>\n\n<p>I have a page table and a meta table. It looks a little bit like this.</p>\n\n<p><strong>Page table</strong></p>\n\n<pre><code>page_id | headline    | content\n--------------------------\n1       | My headline | My content\n2       | Another one | Another text\n</code></pre>\n\n<p><strong>Meta table</strong></p>\n\n<pre><code>id | page_id | meta_key  | meta_value\n------------------------------------\n1  | 2       | seo_title | Hello world\n2  | 2       | price     | 299\n</code></pre>\n\n<p>I've read that this type of model is called <a href=\"http://en.wikipedia.org/wiki/Entity%E2%80%93attribute%E2%80%93value_model\" rel=\"nofollow\">EAV</a>. I also read that it is <a href=\"http://karwin.blogspot.se/2009/05/eav-fail.html\" rel=\"nofollow\">bad for performance</a>.</p>\n\n<p>My meta table is made for any kind of value connected to a page. I can not created a table with \"static\" columns this time.</p>\n\n<p><strong>Question</strong></p>\n\n<ul>\n<li>How bad is this for 300 pages with 30 meta values on each page? 9000\nrows in the meta table that is.</li>\n<li>Is there a better model for \"dynamic\" data?</li>\n</ul>\n"},{"tags":["directx","performance","fullscreen","directx-10"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":2637,"score":1,"question_id":1006039,"title":"D3D10 (DirectX10) fullscreen performance issue","body":"<p>I have a bit of a problem setting up my DirectX10 (Win32/c++) application for fullscreen mode. The problem is that I want to have my app running in fullscreen right from the start. This can be done by taking the DXGISwapChain::SetFullScreenState function. This works, but i get a small notice in my Visualc++ 2008 debugger which states: </p>\n\n<p><strong>\"DXGI Warning: IDXGISwapChain::Present: Fullscreen presentation inefficiencies incurred due to application not using IDXGISwapChain::ResizeBuffers appropriately, specifying a DXGI_MODE_DESC not available in IDXGIOutput::GetDisplayModeList, or not using DXGI_SWAP_CHAIN_FLAG_ALLOW_MODE_SWITCH.\"</strong></p>\n\n<p>What this means is that DirectX will not take full ownership of the graphicscard and flip the images from front to backbuffer but instead blit them which is much slower.</p>\n\n<p>Now, i do have the DXGI_SWAP_CHAIN_FLAG_ALLOW_MODE_SWITCH enabled and i did try to resize my buffers but i have absolutely no idea what would be the best way to go into fullscreen mode. I have looked on MSDN but there they mostly assume you will only go into Fullscreen by pressing Alt+Enter which lest DXGI do all the work. If someone please could post a bit of code which takes DirectX10 into fullscreen mode and takes full advantage of the \"flipping\" it would be greatly appriciated! </p>\n\n<p>For anybody interested in the code used on resize:</p>\n\n<pre><code>ReleaseCOM(m_pD3DRenderTargetView);\nReleaseCOM(m_pD3DDepthStencilView);\nReleaseCOM(m_pD3DDepthStencilBuffer);\n\nDXGI_MODE_DESC* mod = new DXGI_MODE_DESC;\nmod-&gt;Format = DXGI_FORMAT_R8G8B8A8_UNORM;\nmod-&gt;Height = m_ScreenHeight;\nmod-&gt;Width = m_ScreenWidth;\nmod-&gt;RefreshRate.Denominator = 0;\nmod-&gt;RefreshRate.Numerator = 0;\nmod-&gt;ScanlineOrdering = DXGI_MODE_SCANLINE_ORDER_UNSPECIFIED;\nmod-&gt;Scaling = DXGI_MODE_SCALING_UNSPECIFIED;\ndelete mod; mod = 0;\n\nm_pSwapChain-&gt;ResizeTarget(mod);\n\nHR(m_pSwapChain-&gt;ResizeBuffers(1, m_ScreenWidth, m_ScreenHeight, DXGI_FORMAT_R8G8B8A8_UNORM, DXGI_SWAP_CHAIN_FLAG_ALLOW_MODE_SWITCH))\n\tthrow(Exception(GET_BUFFER_FAIL, AT));\n\n//problem area\nm_pSwapChain-&gt;SetFullscreenState(TRUE, NULL);\n\nID3D10Texture2D* pBackBuffer;\nHR( m_pSwapChain-&gt;GetBuffer(0, __uuidof(ID3D10Texture2D), (LPVOID*)&amp;pBackBuffer))\n\tthrow(Exception(GET_BUFFER_FAIL, AT)); //continues as usual\n</code></pre>\n"},{"tags":["php","performance","sqlite"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":54,"score":1,"question_id":12881778,"title":"SQLite - How to make more smaller databases in order to get better performance?","body":"<p>Is there some class or method to limit the size or the number of records in a SQLite database and automatically create a new DB once first one is full (up to limit)? </p>\n\n<p>My SQLite DB can go up to 500mb in size and on shared hosting it won't work.\nSo, I want to divide it to 10 databases of 50 MB or so.</p>\n\n<p>Will this also allow to easily read from the right database if several of them are created?</p>\n"},{"tags":["c#","asp.net","ajax","performance","drop-down-menu"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":120,"score":0,"question_id":12881803,"title":"how to fill dropdown list from extra huge datatable?","body":"<p>i have table from database this table has 400000 row in my asp page my dropdown list(ddlPlaintiffName) fill from\nthis method </p>\n\n<pre>\n\n private void FillPlaintiff()\n    {\n\n        //declare connection by pass connection string from web.config\n        SqlConnection sqlcon = new SqlConnection\n            (ConfigurationManager.ConnectionStrings[\"SystemConn\"].ConnectionString);\n        //declare sql statment  as astring variable\n\n        SqlCommand sqlcom = new SqlCommand();\n        sqlcom.Connection = sqlcon;\n        sqlcom.CommandType = CommandType.StoredProcedure;\n        sqlcom.CommandText = \"proc_SelectPlaintiff\";\n\n\n\n        DataTable ds = new DataTable();\n        //fill data set with data adabter that contain data from database\n     //   sad.Fill(ds);\n        sqlcon.Open();\n         SqlDataAdapter sad = new SqlDataAdapter(sqlcom);\n\n         sad.Fill(ds);\n\n        ddlPlaintiffName.DataSource = ds;\n        ddlPlaintiffName.DataBind();\n        ddlPlaintiffName.Items.Insert(0, \"--select  --\");\n        sqlcon.Close();\n\n    }\n\n\n</pre> \n\n<p>but every postback my load is very very slow how can i avoid this</p>\n"},{"tags":["database","performance","debugging","interface"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":2,"view_count":37,"score":-2,"question_id":12881707,"title":"Database & Interface speed","body":"<p>I have a project which requires extreme processing speeds. I have budgeted for a multi CPU solution with tons of RAM and SSD drives. My question is now what to code in and what database to use.</p>\n\n<p>The options I have and am comfortable using are:\nOperating System Options\n1) Windows Server 2008 \n2) A Linux distro</p>\n\n<p>Databases:\n1) Oracle\n2) MySQL/Postgres\n3) MYSQL (obviously tied into windows platform)</p>\n\n<p>Coding/Language:\n1) C# (.Net interface)\n2) C++ (.Net again)\n3) C++ (low level compiler like a GNU verison)\n4) Java</p>\n\n<p>Now I need the best combination to give me maximum speed. Now I am not sure if on new systems these even make a difference but I need speed increases in terms of milliseconds. (ie: if turnaround time is 5-20ms faster then it's still a clear winner).</p>\n\n<p>Now this is what I am thinking:</p>\n\n<p>Windows vs Linux: while windows generally takes more resources, once the applications are running the OS shouldnt make a difference.</p>\n\n<p>Database: My experience is as such - \nOracle, huge amounts of spatial data; \nMySQL/Postgres, large websites (forums, etc)\nMYSQL - middle tier for oracle backend for fin trnsactions.</p>\n\n<p>Now my gut feels tells me Oracle as it's the most stable and handles lare amounts of data, but is it fastest?\nThe data processing will be mostly reads (speed very imortant) and writes are not as mission critical, those will happen every 0.5 to 5 seconds. Data size is usually 100-1000 records of 4-8 fields most containing real number values (float/double in C terms).</p>\n\n<p>Language:\n- Java I would throw out right away since the fact that it's a proces srunning in a virtual machine adds another layer and could sslow things down.\n- C#/C++ .NET versions, this is a bit faster then Java and the easiest/nices for me to work in but the .NET framework could slow things down and I'm worried about the speed of the.NETR database interfaces.\n- C++ (GNU flavor), as this is the closest to hardware without going to assembler, this would be my best option. Only drawback is I don't know if there are any interfaces to the databases above and how much of a pain they are to use.</p>\n\n<p>So my own conclusion would be:\nOracle with a GNU C++ language interface. Unsure about windows or linux as yet, don't mind using either.  </p>\n\n<p>My biggest problem with using GNU C++ and linux is the debugging process, in windows C#/C++ I just use VStudio and am golden, are there any decent graphical debuggers/tools for the gnu compilers?</p>\n\n<p>I apologise for the length o the question but any comments and changes to my thinking process and conclusion would be appreciated.</p>\n"},{"tags":["c++","python","performance","code-generation","converter"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":9,"view_count":182,"score":-9,"question_id":12344414,"title":"Should I use a code converter (Python to C++)?","body":"<p>Let me just say right off the bat that i'm not a programmer. I'm just a guy with an idea taking his first steps to make it a reality. I'm no stranger to programming, mind you, but some of the concepts and terminology here are way over my head; so i apologize in advance if this question was answered before (i.e. <a href=\"http://stackoverflow.com/questions/4650243/convert-python-program-to-c-c-code\">Convert Python program to C/C++ code?</a>).</p>\n\n<p>I have an idea to create a simple A.I. network to analyze music data sent from a phone via cloud computing (I got a guy for the cloud stuff). It will require a lot of memory and need to be fast for the hard number-crunching. I had planned on doing it in python, but have since learned that might not be such a good idea (<a href=\"http://stackoverflow.com/questions/801657/is-python-faster-and-lighter-than-c\">Is Python faster and lighter than C++?</a>).</p>\n\n<p>Since python is really the only gun i have in my holster, i was thinking of using a python-to-C++-converter.  But nothing comes without a price:</p>\n\n<ol>\n<li>Is this an advantageous way to keep my code fast?</li>\n<li>What's the give-and-take for using a converter?</li>\n<li>Am i missing anything? I'm still new to this so i'm not even sure what questions to ask.</li>\n</ol>\n\n<p>Thanks in advance.</p>\n"},{"tags":["java","performance","solr"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":650,"score":0,"question_id":6124657,"title":"Calling solr inside solr","body":"<p>I am having multiple solr with seprate schema...</p>\n\n<p>i need to get results each solr and then i will append the results to final query and then i will call solr - to get final result..</p>\n\n<p>How to do this?\nI need to write seperate requesthandler ..?? </p>\n\n<p>Any other way is there???</p>\n\n<p>Ex:\nquery1 AND query2 AND query3 OR query4</p>\n\n<p>All the results i m passing in the next</p>\n\n<p>Query 5 : solr\\select?q=res.query1 AND res.query2 AND res.query3 OR res.query4 </p>\n\n<p>why i am separating the index?\n because i have 100,000 million of datas . so only i spliting the index</p>\n\n<p>Thanks in advance</p>\n"},{"tags":["mysql","sql","performance","query","sql-update"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":59,"score":0,"question_id":12812331,"title":"How to efficiently UPDATE a column when it requires joining 2 large tables in MySQL?","body":"<p>I've got 2 tables, named: <code>csv (a csv dump)</code>, and <code>items (primary data table)</code> with 7M (csv dump) and 15M rows respectively. I need to update a column in <code>items</code> that exists in table <code>csv</code>.</p>\n\n<p>Both tables have a commonly indexed join ID (a <code>VARCHAR(255)</code>).</p>\n\n<p>An UPDATE query with a join on the mutual ID column (indexed) still takes multiple days to run. After researching it  I believe the inefficiency is in MySQL scanning the <code>csv</code> table and making per-row random-access queries against the <code>items</code> table.</p>\n\n<p>Even though there are indexes, those indexes don't fit in memory, so the required 7M random access queries are nose diving performance.</p>\n\n<p>Are there \"typical\" ways of addressing this kind of issue?</p>\n\n<hr>\n\n<p><strong>Update:</strong></p>\n\n<blockquote>\n  <p>We're basically taking multiple catalogs of \"items\" and storing them\n  in our <code>items</code> table (this is bit of a simplification for discussion).\n  Each of say 10 catalogs will have 7M items (some duplicates across catalogs that we\n  normalize to 1 row in our item table). We need to compare and\n  verify changes to those 10 catalogs daily (<code>UPDATES</code> w/ joins between two big\n  tables, or other such mechanism).</p>\n  \n  <p>In reality we have an <code>items</code> table and an <code>items_map</code> table, but no\n  need to discuss that additional level of abstraction here. I'd be\n  happy to find a way to perform an update between the <code>csv</code> dump table\n  and an <code>items</code> table (given that they both have a common ID that's\n  indexed in both tables). But  given that the <code>items</code> table might have\n  20M rows, and the <code>csv</code> table might have 7M rows.</p>\n  \n  <p>In this case indexes don't fit in memory and we're hammering the drive with random seeks I believe</p>\n</blockquote>\n"},{"tags":["performance","actionscript-3","flash"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":59,"score":0,"question_id":12880015,"title":"AS3 Bottlenecks","body":"<p>What are the common bottlenecks or inherently slow actions/functions that I should look out for while developing an app/game/anything in ActioScript3 and flash?</p>\n"},{"tags":["android","performance","cursor"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":33,"score":0,"question_id":12880095,"title":"update single row in Android cursor","body":"<p>In my app I use ListView with CursorAdapter. When some data in some row of database is changed, I recreate cursor for CursorAdapter.</p>\n\n<p>This process is expensive. Is there any way to update just needed rows in Cursor?</p>\n\n<p>Thanks.</p>\n"},{"tags":["android","database","performance","sqlite"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":58,"score":1,"question_id":12831504,"title":"Quick readonly sqlite database","body":"<p>I have a huge database and I want my application to work with it as soon as possible. I'm using android so resources are more restricted. I know that its not a good idea to storage huge  data in the sqlite database, but I need this.</p>\n\n<p>Each database contain only ONE table and I use it READ only.</p>\n\n<p>What advice can you give me to optimize databases as much as possible. I've already read <a href=\"http://stackoverflow.com/questions/784173/what-are-the-performance-characteristics-of-sqlite-with-very-large-database-file\">this</a> post, and except the PRAGMA commands what else can I use?</p>\n\n<p>Maybe there are some special <strong>types</strong> of the tables which are restricted for read only queries, but principally faster then ordinary table types?</p>\n"},{"tags":["javascript","jquery","performance","delay","domready"],"answer_count":3,"favorite_count":3,"up_vote_count":9,"down_vote_count":0,"view_count":127,"score":9,"question_id":12850622,"title":"$.ready() before closing body","body":"<p>This is not a real coding question, more of a real-world statement.</p>\n\n<p>I have previously <a href=\"http://stackoverflow.com/questions/9557846/why-is-jquery-ready-recommended-when-its-so-slow\">noted</a> that <code>DOMReady</code> events are slow, very slow. So, I noticed while browsing the jQuery source that the jQuery domeready event can be trigger using <code>$.ready()</code>. Then I thought, placing this simple execution script just before closing the body should trigger all the \"onDomReady\" listeners that where previoulsy attached. And yes, it works as expected:</p>\n\n<pre><code>     &lt;script&gt;$.ready()&lt;/script&gt;\n&lt;/body&gt;\n</code></pre>\n\n<p>Here are two examples, this one measures the ms spent while waiting for DOMReady:</p>\n\n<p><a href=\"http://jsbin.com/aqifon/10\">http://jsbin.com/aqifon/10</a></p>\n\n<p>As you can see, the DOMReady trigger is very natively slow, the user has to wait for a whole 200-300 milliseconds before the domready script kick in.</p>\n\n<p>Anyway, if we place <code>$.ready()</code> just before closing the <code>BODY</code> tag we get this:</p>\n\n<p><a href=\"http://jsbin.com/aqifon/16\">http://jsbin.com/aqifon/16</a></p>\n\n<p>See the difference? By triggering domready manually, we can cut off 100-300 ms of execution delay. This is a major deal, because we can rely on jQuery to take care of DOM manipulations before we see them.</p>\n\n<p>Now, to a question, I have never seen this being recommended or discussed before, but still it seems like a major performance issue. Everything is about optimizing the code itself, which is good of course, but it is in vain if the execution is delayed for such a long time that the user sees a \"flash of \"unjQueryedContent\".</p>\n\n<p>Any ideas why this is not discussed/recommended more frequently?</p>\n"},{"tags":["iphone","xcode","build","compilation","performance"],"answer_count":8,"favorite_count":7,"up_vote_count":14,"down_vote_count":0,"view_count":4712,"score":14,"question_id":1479085,"title":"How to decrease build times / speed up compile time in XCode?","body":"<p><strong>What strategies can be used in general to decrease build times for any XCode project? I'm mostly interested in XCode specific strategies.</strong> </p>\n\n<p>I'm doing iPhone development using XCode, and my project is slowly getting bigger and bigger. I find the compile / link phases are starting to take more time than I'd like.</p>\n\n<p>Currently, I'm:</p>\n\n<ul>\n<li><p>Using Static Libraries to make it so\nmost of my code doesn't need to be\ncompiled everytime I clean and build\nmy main project</p></li>\n<li><p>Have removed most resources from my\napplication, and test with a hard\ncoded file system path in the iPhone\nsimulator whenever possible so my\nresources don't have to constantly be\npackaged as I make changes to them.</p></li>\n</ul>\n\n<p>I've noticed that the \"Checking Dependencies\" phase seems to take longer than I'd like. Any tips to decrease that as well would be appreciated!</p>\n"},{"tags":["java","performance","collections","guava"],"answer_count":1,"favorite_count":0,"up_vote_count":7,"down_vote_count":5,"view_count":168,"score":2,"question_id":12853375,"title":"ArrayListMultimap Vs ArrayList , which have the high performance?","body":"<p>In my application, I'm using <code>ArrayList(java.util)</code> for storing bulk of customized data and processing. But it causes the process delay when use continuously in a scheduled manner. So I would like to switch to some other.</p>\n\n<p>Does <code>ArrayListMultimap(com.google.common.collect.ArrayListMultimap)</code> have higher performance than <code>ArrayList</code>? Or any other open source collection frameworks that have better performance than <code>ArrayList</code>?</p>\n"},{"tags":["performance","spring","hibernate","unit-testing","dbunit"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":143,"score":2,"question_id":12876278,"title":"Improving performance of database tests using Spring 3.1, Hibernate 4.1, Dbunit, etc","body":"<p>I'm currently starting a new project, and I've got around 190 repository tests. One thing I've noticed - and I am not entirely sure why this happening - is that the integration tests against HSQLDB (2.2.8) are running a lot slower than I think they should be.</p>\n\n<p>I think I've tracked the bottleneck to the insertion of data before each test. For most tests, it ranges from .15 to .38 seconds just to setup the database. This is unacceptable. I would have imagined that an in-memory database would be much faster :(</p>\n\n<p>Here is the database test class that all of my repository tests extend from:</p>\n\n<pre><code>@ContextConfiguration(locations = {\"classpath:applicationContext.xml\"})\n@RunWith(SpringJUnit4ClassRunner.class)\n@TransactionConfiguration(defaultRollback=true)\n@Transactional\npublic abstract class DatabaseTest {\n\n    public static final String TEST_RESOURCES = \"src/test/resources/\";\n\n    @Autowired\n    protected SessionFactory sessionFactory;\n\n    @Autowired\n    protected UserRepository userRepository;\n\n    @Autowired\n    protected DataSource dataSource;\n\n    protected IDatabaseTester databaseTester;\n\n    protected Map&lt;String, Object&gt; jdbcMap;\n    protected JdbcTemplate jdbcTemplate;\n\n    @PostConstruct\n    public void initialize() throws SQLException, IOException, DataSetException {\n        jdbcTemplate = new JdbcTemplate(dataSource);\n\n        setupHsqlDb();\n\n        databaseTester = new DataSourceDatabaseTester(dataSource);\n        databaseTester.setSetUpOperation(DatabaseOperation.CLEAN_INSERT);\n        databaseTester.setTearDownOperation(DatabaseOperation.NONE);\n        databaseTester.setDataSet(getDataSet());\n    }\n\n    @Before\n    public void insertDbUnitData() throws Exception {\n        long time = System.currentTimeMillis();\n\n        databaseTester.onSetup();\n\n        long elapsed = System.currentTimeMillis() - time;\n        System.out.println(getClass() + \" Insert DB Unit Data took: \" + elapsed);\n    }\n\n    @After\n    public void cleanDbUnitData() throws Exception {\n        databaseTester.onTearDown();\n    }\n\n    public IDataSet getDataSet() throws IOException, DataSetException {\n        Set&lt;String&gt; filenames = getDataSets().getFilenames();\n\n        IDataSet[] dataSets = new IDataSet[filenames.size()];\n        Iterator&lt;String&gt; iterator = filenames.iterator();\n        for(int i = 0; iterator.hasNext(); i++) {\n            dataSets[i] = new FlatXmlDataSet(\n                new FlatXmlProducer(\n                    new InputSource(TEST_RESOURCES + iterator.next()), false, true\n                )\n            );\n        }\n\n        return new CompositeDataSet(dataSets);\n    }\n\n    public void setupHsqlDb() throws SQLException {\n        Connection sqlConnection = DataSourceUtils.getConnection(dataSource);\n        String databaseName = sqlConnection.getMetaData().getDatabaseProductName();\n        sqlConnection.close();\n\n        if(\"HSQL Database Engine\".equals(databaseName)) {\n            jdbcTemplate.update(\"SET DATABASE REFERENTIAL INTEGRITY FALSE;\");\n\n            // MD5\n            jdbcTemplate.update(\"DROP FUNCTION MD5 IF EXISTS;\");\n            jdbcTemplate.update(\n                \"CREATE FUNCTION MD5(VARCHAR(226)) \" +\n                    \"RETURNS VARCHAR(226) \" +\n                    \"LANGUAGE JAVA \" +\n                    \"DETERMINISTIC \" +\n                    \"NO SQL \" +\n                    \"EXTERNAL NAME 'CLASSPATH:org.apache.commons.codec.digest.DigestUtils.md5Hex';\"\n            );\n        } else {\n            jdbcTemplate.update(\"SET foreign_key_checks = 0;\");\n        }\n    }\n\n    protected abstract DataSet getDataSets();\n\n    protected void flush() {\n        sessionFactory.getCurrentSession().flush();\n    }\n\n    protected void clear() {\n        sessionFactory.getCurrentSession().clear();\n    }\n\n    protected void setCurrentUser(User user) {\n        if(user != null) {\n            Authentication authentication = new UsernamePasswordAuthenticationToken(user,\n                user, user.getAuthorities());\n\n            SecurityContextHolder.getContext().setAuthentication(authentication);\n        }\n    }\n\n    protected void setNoCurrentUser() {\n        SecurityContextHolder.getContext().setAuthentication(null);\n    }\n\n    protected User setCurrentUser(long userId) {\n        User user = userRepository.find(userId);\n\n        if(user.getId() != userId) {\n            throw new IllegalArgumentException(\"There is no user with id: \" + userId);\n        }\n\n        setCurrentUser(user);\n\n        return user;\n    }\n\n    protected User getCurrentUser() {\n        return (User) SecurityContextHolder.getContext().getAuthentication().getPrincipal();\n    }\n\n}\n</code></pre>\n\n<p>Here is the relevant beans on my application context:</p>\n\n<pre><code>&lt;bean class=\"org.springframework.beans.factory.config.PropertyPlaceholderConfigurer\"&gt;\n    &lt;property name=\"locations\" value=\"classpath:applicationContext.properties\"/&gt;\n&lt;/bean&gt;\n\n&lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"\n      destroy-method=\"close\"&gt;\n    &lt;property name=\"driverClass\" value=\"${database.driver}\"/&gt;\n    &lt;property name=\"jdbcUrl\" value=\"${database.url}\"/&gt;\n    &lt;property name=\"user\" value=\"${database.username}\"/&gt;\n    &lt;property name=\"password\" value=\"${database.password}\"/&gt;\n    &lt;property name=\"initialPoolSize\" value=\"10\"/&gt;\n    &lt;property name=\"minPoolSize\" value=\"10\"/&gt;\n    &lt;property name=\"maxPoolSize\" value=\"50\"/&gt;\n    &lt;property name=\"idleConnectionTestPeriod\" value=\"100\"/&gt;\n    &lt;property name=\"acquireIncrement\" value=\"2\"/&gt;\n    &lt;property name=\"maxStatements\" value=\"0\"/&gt;\n    &lt;property name=\"maxIdleTime\" value=\"1800\"/&gt;\n    &lt;property name=\"numHelperThreads\" value=\"3\"/&gt;\n    &lt;property name=\"acquireRetryAttempts\" value=\"2\"/&gt;\n    &lt;property name=\"acquireRetryDelay\" value=\"1000\"/&gt;\n    &lt;property name=\"checkoutTimeout\" value=\"5000\"/&gt;\n&lt;/bean&gt;\n\n&lt;bean id=\"sessionFactory\"\n      class=\"org.springframework.orm.hibernate4.LocalSessionFactoryBean\"&gt;\n    &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt;\n    &lt;property name=\"mappingResources\"&gt;\n        &lt;list&gt;\n            &lt;value&gt;...&lt;/value&gt;\n        &lt;/list&gt;\n    &lt;/property&gt;\n    &lt;property name=\"namingStrategy\"&gt;\n        &lt;bean class=\"org.hibernate.cfg.ImprovedNamingStrategy\"/&gt;\n    &lt;/property&gt;\n    &lt;property name=\"hibernateProperties\"&gt;\n        &lt;props&gt;\n            &lt;prop key=\"javax.persistence.validation.mode\"&gt;none&lt;/prop&gt;\n\n            &lt;prop key=\"hibernate.dialect\"&gt;${hibernate.dialect}&lt;/prop&gt;\n            &lt;prop key=\"hibernate.hbm2ddl.auto\"&gt;${hibernate.hbm2ddl.auto}\n            &lt;/prop&gt;\n            &lt;prop key=\"hibernate.generate_statistics\"&gt;false&lt;/prop&gt;\n\n            &lt;prop key=\"hibernate.show_sql\"&gt;false&lt;/prop&gt;\n            &lt;prop key=\"hibernate.format_sql\"&gt;true&lt;/prop&gt;\n\n            &lt;prop key=\"hibernate.cache.use_second_level_cache\"&gt;false&lt;/prop&gt;\n            &lt;prop key=\"hibernate.cache.provider_class\"&gt;\n\n            &lt;/prop&gt;\n        &lt;/props&gt;\n    &lt;/property&gt;\n&lt;/bean&gt;\n\n&lt;bean class=\"org.springframework.orm.hibernate4.HibernateExceptionTranslator\"/&gt;\n\n&lt;bean id=\"transactionManager\"\n      class=\"org.springframework.orm.hibernate4.HibernateTransactionManager\"&gt;\n    &lt;property name=\"sessionFactory\" ref=\"sessionFactory\"/&gt;\n&lt;/bean&gt;\n</code></pre>\n\n<p>In order to try and insert less data, I allow each test class to pick a DataSet enum that only loads the data it needs. It's specified like this:</p>\n\n<pre><code>public enum DataSet {\n    NONE(create()),\n    CORE(create(\"core.xml\")),\n    USERS(combine(create(\"users.xml\"), CORE)),\n    TAGS(combine(create(\"tags.xml\"), USERS)),\n</code></pre>\n\n<p>Could this be causing it to run slower rather than faster? The idea is that if I only want the core xml (languages, provinces, etc.), I only have to load those records. I thought this would make the test suite faster, but it's still too slow.</p>\n\n<p><strong>I can save some time by creating a separate xml dataset specifically designed for each test class. This chops out some of the insert statements. But even when I have 20 insert statements in a single xml dataset (thus, the minimum I/O loss other than in-lining the dataset right into java code directly), each test still takes .1 to .15 seconds during the initialization of the database data!</strong> I am in disbelief that it takes .15 seconds to insert 20 records into memory.</p>\n\n<p>In my other project using Spring 3.0 and Hibernate 3.x, it takes 30 milliseconds to insert everything before each test, but it's actually inserting 100 or more rows per test. For the tests that only have 20 inserts, they are flying as if there was no delay at all. This is what I expected. I'm starting to think the problem is with Spring's annotations - or the way I have them setup in my <code>DatabaseTest</code> class. This is basically the only thing different now.</p>\n\n<p>Also, my repositories are using the sessionFactory.getCurrentSession() instead of the HibernateTemplate. This is the first time I started using the annotation-based unit test stuff from Spring, since the Spring test classes are deprecated. Could that be the reason they are going slow?</p>\n\n<p>If there's anything else you need to know to help figure it out, please let me know. I am sort of stumped.</p>\n\n<p>EDIT: I put in the answer. The problem was hsqldb 2.2.x. Reverting to 2.0.0 fixes the problem.</p>\n"},{"tags":["performance","optimization","encoding","bitmap","bitvector"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":56,"score":3,"question_id":12877557,"title":"Efficient way to encode bit-vectors?","body":"<p>Currently using the run length encoding for encoding bit-vectors, and the current run time is \n2log(i), where is the size of the run. Is there another way of doing it to bring it down to log(i)?\nThanks.</p>\n"},{"tags":["sql","performance","standards","aggregate-functions"],"answer_count":1,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":63,"score":5,"question_id":12876873,"title":"Is there a standard for SQL aggregate function calculation?","body":"<p>Is there a standard on SQL implementaton for multiple calls to the same aggregate function in the same query?</p>\n\n<p>For example, consider the following example, based on a popular example schema:</p>\n\n<pre><code>SELECT Customer,SUM(OrderPrice) FROM Orders\nGROUP BY Customer\nHAVING SUM(OrderPrice)&gt;1000\n</code></pre>\n\n<p>Presumably, it takes computation time to calculate the value of SUM(OrderPrice).  Is this cost incurred for each reference to the aggregate function, or is the result stored for a particular query?</p>\n\n<p>Or, is there no standard for SQL engine implementation for this case?</p>\n"},{"tags":["python","performance","redis","generator"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":166,"score":4,"question_id":12868222,"title":"Performance of Redis vs Disk in caching application","body":"<p>I wanted to create a redis cache in python, and as any self respecting scientist I made a bench mark to test the performance.</p>\n\n<p>Interestingly, redis did not fare so well. Either Python is doing something magic (storing the file) or my version of redis is stupendously slow.</p>\n\n<p>I don't know if this is because of the way my code is structured, or what, but I was expecting redis to do better than it did.</p>\n\n<p>To make a redis cache, I set my binary data (in this case, an HTML page) to a key derived from the filename with an expiration of 5 minutes. </p>\n\n<p>In all cases, file handling is done with f.read() (this is ~3x faster than f.readlines(), and I need the binary blob). </p>\n\n<p>Is there something I'm missing in my comparison, or is Redis really no match for a disk? Is Python caching the file somewhere, and reaccessing it every time? Why is this so much faster than access to redis? </p>\n\n<p>I'm using redis 2.8, python 2.7, and redis-py, all on a 64 bit Ubuntu system.</p>\n\n<p>I do not think Python is doing anything particularly magical, as I made a function that stored the file data in a python object and yielded it forever. </p>\n\n<p>I have four function calls that I grouped:</p>\n\n<p>Reading the file X times</p>\n\n<p>A function that is called to see if redis object is still in memory, load it, or cache new file (single and multiple redis instances).</p>\n\n<p>A function that creates a generator that yields the result from the redis database (with single and multi instances of redis).</p>\n\n<p>and finally, storing the file in memory and yielding it forever.</p>\n\n<pre><code>import redis\nimport time\n\ndef load_file(fp, fpKey, r, expiry):\n    with open(fp, \"rb\") as f:\n        data = f.read()\n    p = r.pipeline()\n    p.set(fpKey, data)\n    p.expire(fpKey, expiry)\n    p.execute()\n    return data\n\ndef cache_or_get_gen(fp, expiry=300, r=redis.Redis(db=5)):\n    fpKey = \"cached:\"+fp\n\n    while True:\n        yield load_file(fp, fpKey, r, expiry)\n        t = time.time()\n        while time.time() - t - expiry &lt; 0:\n            yield r.get(fpKey)\n\n\ndef cache_or_get(fp, expiry=300, r=redis.Redis(db=5)):\n\n    fpKey = \"cached:\"+fp\n\n    if r.exists(fpKey):\n        return r.get(fpKey)\n\n    else:\n        with open(fp, \"rb\") as f:\n            data = f.read()\n        p = r.pipeline()\n        p.set(fpKey, data)\n        p.expire(fpKey, expiry)\n        p.execute()\n        return data\n\ndef mem_cache(fp):\n    with open(fp, \"rb\") as f:\n        data = f.readlines()\n    while True:\n        yield data\n\ndef stressTest(fp, trials = 10000):\n\n    # Read the file x number of times\n    a = time.time()\n    for x in range(trials):\n        with open(fp, \"rb\") as f:\n            data = f.read()\n    b = time.time()\n    readAvg = trials/(b-a)\n\n\n    # Generator version\n\n    # Read the file, cache it, read it with a new instance each time\n    a = time.time()\n    gen = cache_or_get_gen(fp)\n    for x in range(trials):\n        data = next(gen)\n    b = time.time()\n    cachedAvgGen = trials/(b-a)\n\n    # Read file, cache it, pass in redis instance each time\n    a = time.time()\n    r = redis.Redis(db=6)\n    gen = cache_or_get_gen(fp, r=r)\n    for x in range(trials):\n        data = next(gen)\n    b = time.time()\n    inCachedAvgGen = trials/(b-a)\n\n\n    # Non generator version    \n\n    # Read the file, cache it, read it with a new instance each time\n    a = time.time()\n    for x in range(trials):\n        data = cache_or_get(fp)\n    b = time.time()\n    cachedAvg = trials/(b-a)\n\n    # Read file, cache it, pass in redis instance each time\n    a = time.time()\n    r = redis.Redis(db=6)\n    for x in range(trials):\n        data = cache_or_get(fp, r=r)\n    b = time.time()\n    inCachedAvg = trials/(b-a)\n\n    # Read file, cache it in python object\n    a = time.time()\n    for x in range(trials):\n        data = mem_cache(fp)\n    b = time.time()\n    memCachedAvg = trials/(b-a)\n\n\n    print \"\\n%s file reads: %.2f reads/second\\n\" %(trials, readAvg)\n    print \"Yielding from generators for data:\"\n    print \"multi redis instance: %.2f reads/second (%.2f percent)\" %(cachedAvgGen, (100*(cachedAvgGen-readAvg)/(readAvg)))\n    print \"single redis instance: %.2f reads/second (%.2f percent)\" %(inCachedAvgGen, (100*(inCachedAvgGen-readAvg)/(readAvg)))\n    print \"Function calls to get data:\"\n    print \"multi redis instance: %.2f reads/second (%.2f percent)\" %(cachedAvg, (100*(cachedAvg-readAvg)/(readAvg)))\n    print \"single redis instance: %.2f reads/second (%.2f percent)\" %(inCachedAvg, (100*(inCachedAvg-readAvg)/(readAvg)))\n    print \"python cached object: %.2f reads/second (%.2f percent)\" %(memCachedAvg, (100*(memCachedAvg-readAvg)/(readAvg)))\n\nif __name__ == \"__main__\":\n    fileToRead = \"templates/index.html\"\n\n    stressTest(fileToRead)\n</code></pre>\n\n<p>And now the results:</p>\n\n<pre><code>10000 file reads: 30971.94 reads/second\n\nYielding from generators for data:\nmulti redis instance: 8489.28 reads/second (-72.59 percent)\nsingle redis instance: 8801.73 reads/second (-71.58 percent)\nFunction calls to get data:\nmulti redis instance: 5396.81 reads/second (-82.58 percent)\nsingle redis instance: 5419.19 reads/second (-82.50 percent)\npython cached object: 1522765.03 reads/second (4816.60 percent)\n</code></pre>\n\n<p>The results are interesting in that a) generators are faster than calling functions each time, b) redis is slower than reading from the disk, and c) reading from python objects is ridiculously fast.</p>\n\n<p>Why would reading from a disk be so much faster than reading from an in-memory file from redis?</p>\n\n<p>EDIT:\nSome more information and tests.</p>\n\n<p>I replaced the function to </p>\n\n<pre><code>data = r.get(fpKey)\nif data:\n    return r.get(fpKey)\n</code></pre>\n\n<p>The results do not differ much from </p>\n\n<pre><code>if r.exists(fpKey):\n    data = r.get(fpKey)\n\n\nFunction calls to get data using r.exists as test\nmulti redis instance: 5320.51 reads/second (-82.34 percent)\nsingle redis instance: 5308.33 reads/second (-82.38 percent)\npython cached object: 1494123.68 reads/second (5348.17 percent)\n\n\nFunction calls to get data using if data as test\nmulti redis instance: 8540.91 reads/second (-71.25 percent)\nsingle redis instance: 7888.24 reads/second (-73.45 percent)\npython cached object: 1520226.17 reads/second (5132.01 percent)\n</code></pre>\n\n<p>Creating a new redis instance on each function call actually does not have a noticable affect on read speed, the variability from test to test is larger than the gain.</p>\n\n<p>Sripathi Krishnan suggested implementing random file reads. This is where caching starts to really help, as we can see from these results.</p>\n\n<pre><code>Total number of files: 700\n\n10000 file reads: 274.28 reads/second\n\nYielding from generators for data:\nmulti redis instance: 15393.30 reads/second (5512.32 percent)\nsingle redis instance: 13228.62 reads/second (4723.09 percent)\nFunction calls to get data:\nmulti redis instance: 11213.54 reads/second (3988.40 percent)\nsingle redis instance: 14420.15 reads/second (5157.52 percent)\npython cached object: 607649.98 reads/second (221446.26 percent)\n</code></pre>\n\n<p>There is a HUGE amount of variability in file reads so the percent difference is not a good indicator of speedup.</p>\n\n<pre><code>Total number of files: 700\n\n40000 file reads: 1168.23 reads/second\n\nYielding from generators for data:\nmulti redis instance: 14900.80 reads/second (1175.50 percent)\nsingle redis instance: 14318.28 reads/second (1125.64 percent)\nFunction calls to get data:\nmulti redis instance: 13563.36 reads/second (1061.02 percent)\nsingle redis instance: 13486.05 reads/second (1054.40 percent)\npython cached object: 587785.35 reads/second (50214.25 percent)\n</code></pre>\n\n<p>I used random.choice(fileList) to randomly select a new file on each pass through the functions. </p>\n\n<p>The full gist is here if anyone would like to try it out - <a href=\"https://gist.github.com/3885957\" rel=\"nofollow\">https://gist.github.com/3885957</a></p>\n\n<p>Edit edit:\nDid not realize that I was calling one single file for the generators (although the performance of the function call and generator was very similar). Here is the result of different files from the generator as well.</p>\n\n<pre><code>Total number of files: 700\n10000 file reads: 284.48 reads/second\n\nYielding from generators for data:\nsingle redis instance: 11627.56 reads/second (3987.36 percent)\n\nFunction calls to get data:\nsingle redis instance: 14615.83 reads/second (5037.81 percent)\n\npython cached object: 580285.56 reads/second (203884.21 percent)\n</code></pre>\n"},{"tags":["ios","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":54,"score":0,"question_id":12876566,"title":"experiencing heavy performance hit in my iOS app","body":"<p>It is my first iOS app, and I am trying to figure out what I am doing wrong here. My app would go up and would hang for a few seconds until it is responsive. \nThe app would go over the web and bring images from there when it starts up. It then builds views from the images with added text. The function that builds up the views is quite long, but basically it is fetching the images data from the web for every single object.\nI use these methods:</p>\n\n<pre><code>NSURL *url = [NSURL URLWithString: \n   @\"http://mysite.com/images/best_trip_ever.png\"];\nUIImage *image = [UIImage imageWithData: [NSData dataWithContentsOfURL:url]];\n</code></pre>\n\n<p>Since it is happening tens of times when the app starts, I figured this might be one reason for the performance hit.</p>\n\n<p>The views I am creating are made by adding subviews to a view I create. It is happening for every object with the images I fetched from the web.</p>\n\n<p>I also have a <code>table view</code>, for each row I am using the same methods to display a nice row in the table view again from images brought from the web. Something VERY noticeable in the table view is that when I scroll down the cells would get stuck and not move smoothly.</p>\n\n<p>For each object I store the data in an <code>NSData</code> object with <code>encode/decode</code> methods to fetch data and write it back down to the object.</p>\n\n<p>I don't know if it is the bringing of images from the network makes things so slow (which in my opinion shouldn't be THAT slow. It might takes like 7-8 seconds!)\nOr is it the act of building the views from the images.</p>\n\n<p>I don't mind showing the spinner rotating symbol for each image until it is available, providing the app would wake up as fast as possible.</p>\n\n<p>Any ideas?</p>\n"},{"tags":["performance","optimization","dns"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":49,"score":1,"question_id":12874799,"title":"DNS prefetching and page optimization","body":"<p>Today I saw this snippet in the HTML source of a webpage:</p>\n\n<pre><code>&lt;!-- prefetch dns --&gt;\n&lt;link rel=\"dns-prefetch\" href=\"//s3.amazonaws.com\"&gt;\n&lt;link rel=\"dns-prefetch\" href=\"//cdn.api.twitter.com\"&gt;\n&lt;link rel=\"dns-prefetch\" href=\"//graph.facebook.com\"&gt;\n&lt;link rel=\"dns-prefetch\" href=\"//connect.facebook.net\"&gt;\n&lt;link rel=\"dns-prefetch\" href=\"//api.pinterest.com\"&gt;\n&lt;link rel=\"dns-prefetch\" href=\"//google-analytics.com\"&gt;\n</code></pre>\n\n<p>How much can you gain by doing this? I haven't seen this before, nor in the <a href=\"http://developer.yahoo.com/performance/rules.html\" rel=\"nofollow\">Yahoo! Developer Networks guidelines for optimization</a>. The only thing that seems related is \"Reduce DNS Lookups\". </p>\n\n<p>In a similar fashion, why doesn't these services expose an IP address to their services and avoid the DNS look-up altogether? </p>\n"},{"tags":["c++","performance","vector","comparison"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":81,"score":1,"question_id":12874517,"title":"Compare vector of vectors","body":"<p>I have a vector of vectors that store pointers. Currently I iterate over them and compare each pointer and if I find ones that are not equal then vectors also do not equal, but I wonder if it is the right way to do such a thing. </p>\n\n<p>UPD: <code>std::vector&lt;std::vector&lt;Combination*&gt; &gt; combinations;</code> </p>\n"},{"tags":["php","jquery","ajax","performance","firebug"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":77,"score":0,"question_id":12873931,"title":"Ajax request waiting time","body":"<p>I've been working on a project that required me to use Ajax. In the past I have had no problems creating a project with it, but this time every request takes at least 1 second, which is ofcourse way too long.</p>\n\n<p>I can't give you the entire code, but I'll share as much as possible. The requests are as follows:</p>\n\n<pre><code>/* *\n * Update the navigation screen\n */\n    function UpdateNavigation() {\n         $.ajax({\n            type: \"POST\",\n            url: \"application/controllers/LocationController.php\",\n            dataType: \"json\",\n            data: \"action=GetSurroundings\",\n            success: function(data){\n                $(\"#direction-north\").html((typeof data.north != 'undefined' ? data.north : \"\") + '&lt;/br&gt; North');\n                $(\"#direction-west\").html((typeof data.west != 'undefined' ? data.west : \"\") + '&lt;/br&gt; West');\n                $(\"#direction-center\").html((typeof data.center != 'undefined' ? data.center : \"\") + '&lt;/br&gt; Center');\n                $(\"#direction-east\").html((typeof data.east != 'undefined' ? data.east : \"\") + '&lt;/br&gt; East');\n                $(\"#direction-south\").html((typeof data.south != 'undefined' ? data.south : \"\") + '&lt;/br&gt; South');\n            }\n        });\n    }\n\n/* *\n * Update the current location\n */\n    $('#navigation-list :button').click(function(event) {\n        if (event.target.id == \"direction-center\")\n            return;\n\n         $.ajax({\n            type: \"POST\",\n            url: \"application/controllers/LocationController.php\",\n            data: \"action=SetLocation&amp;value=\" + event.target.id,\n            success: function() {\n                UpdateNavigation();\n            }\n        });\n    });\n</code></pre>\n\n<p>As far as I know this could is fine. It might be a good idea to change it a little but it shouldn't cause the delay as far as I know. The LocationController file is pretty big, but it uses a switch for different cases so the actual executed code isn't that big.</p>\n\n<p>Using FireBug I found out that it's waiting for 1.01 seconds for the first request, and 1.00 second for the second request. I did some research and people told me that this might be because the server is too busy to handle your request properly so that's why it's taking so long. But that seems unlikely because the code and database are hosted locally. There shouldn't be a conflict in requests either since they are executed one at a time.</p>\n\n<p>I'm at a loss here. I have no idea how to start debugging this problem. Deleting parts of the code didn't help because it would either stop executing alltogether or just take 1+ second. This leads me to believe the code is not the problem, although I could be wrong.</p>\n\n<p>Any help would be greatly apreciated! If you need more information, please don't hesitate to ask.</p>\n\n<p>Edit: Some more digging around leads me to believe the queries inside the code may be at fault? If so, I'm using the following (pretty ugly) queries:</p>\n\n<pre><code>SELECT \n    character_location.block,\n    character_location.location\nFROM\n    character_location\nWHERE\n    character_location.id = 1\nLIMIT\n    1\n\n\n\nSELECT \n    zones.name,\n    zones.location,\n    zones.block\nFROM\n    zones\nWHERE\n    (\n            `zones`.`location` = (\".$getBlock['location'].\" - 1)\n        AND\n            zones.block = '\".$playerBlock.\"'\n    )\nOR\n    (\n            `zones`.`location` = \".$getBlock['location'].\"\n        AND\n            zones.block = '\".$playerBlock.\"'\n    )\nOR\n    (\n            `zones`.`location` = (\".$getBlock['location'].\" + 1)\n        AND\n            zones.block = '\".$playerBlock.\"'\n    )\nOR\n    (\n            `zones`.`location` = \".$getBlock['location'].\"\n        AND\n            zones.block = '\".$playerBlockDown.\"'\n    )\nOR\n    (\n            `zones`.`location` = \".$getBlock['location'].\"\n        AND\n            zones.block = '\".$playerBlockUp.\"'\n    )\nLIMIT\n    5\n\n\nUPDATE \n    character_location\nSET\n    character_location.block = '\" . $targetBlock . \"',\n    `character_location`.`location` = \" . $targetLocation . \"\nWHERE\n    character_location.id = 1\n</code></pre>\n\n<p>Guess which one I think might be causing the problem?</p>\n"},{"tags":["c++","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":4,"view_count":110,"score":-4,"question_id":12873719,"title":"Which is more optimized code?","body":"<pre><code>// (1)\nfor (int iter = 1; iter &lt;= VERTEX_SIZE; iter ++) {\n    if (visit[iter]) continue;\n    dfs(iter);\n}\n\n// (2)\nfor (int iter = 1; iter &lt;= VERTEX_SIZE; iter ++) {\n    if (!visit[iter]) {\n        dfs(iter);\n    }\n}\n</code></pre>\n\n<p>Which code is more optimized? I'm just curious about it.</p>\n"},{"tags":["c#","performance","dblinq"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":12873685,"title":"New Dblinq DataContext instantion gets slower as the number of DLLs referenced in the project increases","body":"<p>I have been using Dblinq(v0.20.0.0) for SQLite database. But initialization(instantiation) of my datacontext(It has only 15 tables/entities) is becoming slower and slower as i reference new DLLs in my project.(I have reached 5 seconds after referencing 30 DLLs)</p>\n\n<p>I have used visual studio profiler to get the following results :</p>\n\n<p><img src=\"http://i.stack.imgur.com/IbumT.png\" alt=\"Test result summary\">\n<img src=\"http://i.stack.imgur.com/tvXNd.png\" alt=\"Test result hot lines\"></p>\n\n<p>I guess dblinq is using too much reflection, I read from <a href=\"http://www.abhisheksur.com/2010/11/reflection-slow-or-faster-demonstration.html\" rel=\"nofollow\">this</a> post that a call to GetCustomAttributes method is very costly. But why the performance constantly deteriorates when i add reference to new DLLs. Does it mean DBLinq is iterating through all available DLLs to get a certain type. </p>\n\n<p>Is there something i can do about it? I am to give up on DBlinq because of this.<br>\nThanks,</p>\n"},{"tags":["java","performance","optimization"],"answer_count":6,"favorite_count":14,"up_vote_count":28,"down_vote_count":0,"view_count":3197,"score":28,"question_id":4019180,"title":"Obsolete Java Optimization Tips","body":"<p>There are number of performance tips made obsolete by Java compiler and especially <a href=\"http://en.wikipedia.org/wiki/Profile-guided_optimization\" rel=\"nofollow\">Profile-guided optimization</a>. For example, these platform-provided optimizations can drastically (according to sources) reduces the cost of virtual function calls. VM is also capable of method inlining, loop unrolling etc.</p>\n\n<p>What are other performance optimization techniques you came around still being applied but are actually made obsolete by optimization mechanisms found in more modern JVMs?</p>\n"},{"tags":["asp.net",".net","asp.net-mvc","performance","profiling"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":85,"score":2,"question_id":12872041,"title":"Monitoring ASP.NET application memory and disk usage","body":"<p>What is the best way to monitor memory/cpu/disk (reads/sec or total reads) utilisation for an ASP.NET (MVC) application (or and app pool). Are there any perf counters that can do that?</p>\n"},{"tags":["android","performance","webview","prefetch"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":12772262,"title":"Android prefetch Webview","body":"<p>I'm creating a \"Daily ....\" application for Android. (Let's say \"Daily flowers\") The idea is: you get a notification each day and when you click it, it will show you a picture of a flower + some text. I'd like to distribute this from a website.</p>\n\n<p>I can just create a notification that and an Intent to an activity with a WebView that does a loadUrl. However, this is far too slow! (takes between 1 and 5 secs depending on the network.)</p>\n\n<p>So I would like to prefetch the \"entire site\" (1 HTML file, 1 image) and then create the notification so the application can show the daily flower in a snappy way.</p>\n\n<p>Can this be done easily? I could do this the hard way with HttpClient etc, but I'd rather use something like WebView.saveState</p>\n"},{"tags":["iphone","objective-c","performance","audio"],"answer_count":4,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":1350,"score":5,"question_id":2761388,"title":"Is Objective C fast enough for DSP/audio programming","body":"<p>I've been making some progress with audio programming for iPhone. Now I'm doing some performance tuning, trying to see if I can squeeze more out of this little machine. Running Shark, I see that a significant part of my cpu power (16%) is getting eaten up by objc_msgSend. I understand I can speed this up somewhat by storing pointers to functions (IMP) rather than calling them using [object message] notation. But if I'm going to go through all this trouble, I wonder if I might just be better off using C++.</p>\n\n<p>Any thoughts on this? </p>\n"},{"tags":["php","mysql","performance","drupal"],"answer_count":5,"favorite_count":9,"up_vote_count":12,"down_vote_count":0,"view_count":1111,"score":12,"question_id":573276,"title":"Scaling Drupal","body":"<p>I am working on a Drupal based site and notice there are a lot of seperate CSS and js files. Wading though some of the code I can also see quite a few cases where many queries are used too.</p>\n\n<p>What techniques have you tried to improve the performance of Drupal and what modules (if any) do you use to improve the performance of Drupal 'out of the box'?</p>\n"},{"tags":["java","performance","pattern-matching"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":51,"score":0,"question_id":12870372,"title":"Efficiency formula code in Pattern matching","body":"<p>I need a formula that will determine the efficiency of the system in pattern matching\nusing time and the number of comparison factors.</p>\n\n<p>Is there any formula that would produce numeric output using these factors?</p>\n"},{"tags":["iphone","objective-c","ios","performance","audio"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":56,"score":0,"question_id":12869960,"title":"Quicker alternative to AVAudioPlayer?","body":"<p>I have an app in which the frame rate slows down dramatically when a sound is played. I am using <code>AVAudioPlayer</code> to play these sounds, and there are many sounds being played within short spaces of time. These sounds are only a matter of kilobytes. Is there an alternative way to play these sounds with much lower performance costs?</p>\n"},{"tags":["c++","performance","compilation","linker"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":56,"score":-1,"question_id":12869894,"title":"Does network drive space affect C++ linking time on linux?","body":"<p>I recently noticed an interesting phenomenon - my linking time for a project linking with several libraries recently had its link time drop by nearly an order of magnitude. It was not a small change, but literally an order of magnitude. This occurred (and is continuing) on a machine which is a lab machine.</p>\n\n<p>This was not caused by any chances to source code (that I am aware of) nor the build method changing.</p>\n\n<p>I am speculating my particular drop was caused by a network drive the libraries are linked on having increased free space but regardless this has caused me to wonder if other things outside code/build systems can significantly affect linking time. I would like to know what factors -- assuming source code/build/computer remains constant -- affect how long linking takes on Linux? </p>\n\n<p>In other words:</p>\n\n<ul>\n<li>Assuming my project source code and build system remains constant on the same machine, what factors can cause large linking time differences, particularly on Linux?</li>\n</ul>\n"},{"tags":["java","performance","java-ee","distributed-transactions","xa"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":63,"score":0,"question_id":12305900,"title":"Performance Overhead of XA Data Sources - Best Practices","body":"<p>I am trying to understand the Impact of XA Datasources on Performance. </p>\n\n<p>In many applications, it happens that not all the transactions need to participate in Distributed Transactions (meaning only a few transactions require to be distributed/participating with other resources). </p>\n\n<p>Is the trade-off of the performance high enough to have two data sources configured (one each for XA and non-XA)? Again, the answer is, it depends on the scenario, but I am looking for \"Best Practices\".</p>\n"},{"tags":["java","performance","tomcat","solr","config"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":793,"score":2,"question_id":5725329,"title":"\"Connection Reset\" occurs in Solr client side","body":"<p>We are encountering a \"connection reset\" error when we call the SOLR server. And our concurrent load is rather small.</p>\n\n<p>Here is the Tomcat connector config for SOLR:</p>\n\n<pre><code>&lt;Connector port=\"8983\" protocol=\"HTTP/1.1\" \n           connectionTimeout=\"20000\" maxThreads=\"40000\" minSpareThreads=\"400\" maxSpareThreads=\"5000\" maxKeepAliveRequests=\"100\" URIEncoding=\"UTF-8\"\n           redirectPort=\"8943\" /&gt;\n</code></pre>\n\n<p>And  here is we got from our SOLR client:</p>\n\n<pre><code>Caused by: org.apache.solr.client.solrj.SolrServerException: java.net.SocketException: Connection reset\nat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:472)\nat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:243)\nat org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:89)\nat org.apache.solr.client.solrj.SolrServer.query(SolrServer.java:122)\n\n... 36 more\nCaused by: java.net.SocketException: Connection reset\nat java.net.SocketInputStream.read(SocketInputStream.java:168)\nat java.io.BufferedInputStream.fill(BufferedInputStream.java:218)\nat java.io.BufferedInputStream.read(BufferedInputStream.java:237)\nat org.apache.commons.httpclient.HttpParser.readRawLine(HttpParser.java:78)\nat org.apache.commons.httpclient.HttpParser.readLine(HttpParser.java:106)\nat org.apache.commons.httpclient.HttpConnection.readLine(HttpConnection.java:1116)\nat org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnectionAdapter.readLine(MultiThreadedHttpConnectionManager.java:1413)\nat org.apache.commons.httpclient.HttpMethodBase.readStatusLine(HttpMethodBase.java:1973)\nat org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1735)\nat org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:1098)\nat org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:398)\nat org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)\nat org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)\nat org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)\n</code></pre>\n\n<p>After trouble shooting by reading through the SOLR client code, we found this may be due to an improper connection timeout setting in SOLR's Tomcat config. We decide to change it to default (infinite timeout). So, my question is, will it bring　out other performance issues when setting this value to infinite?</p>\n"},{"tags":["performance","aix"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":38,"score":0,"question_id":11028725,"title":"Performance Improvement of AIX application.","body":"<p>I want improve the performance of application running of aix operating system.</p>\n\n<p>please find the details about the application below.</p>\n\n<ol>\n<li>Application coded in c/c++. </li>\n<li>The code is compiled by the gcc  version 4.2.4 compiler.</li>\n<li>The operation system version is AIX 6.1</li>\n</ol>\n\n<p>please suggest some sources for performance tweaks in code  and operating system settings.</p>\n\n<p>note: The cpu usage of the application is very low mostly it is around 30-40.</p>\n\n<p>Thanks in advance.   </p>\n"},{"tags":["sql-server","performance","java-ee","content-management-system"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":55,"score":1,"question_id":12843345,"title":"How to cache a query which is often used?","body":"<p>When there is a web app will query some information frequently, how to improve the performance by cache the query result?\n(The information is like top news in a website and my database is SQL Server 2008, the application is on tomcat.)</p>\n"},{"tags":["java","performance","swing","jtable","java-web-start"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":138,"score":3,"question_id":12864998,"title":"JTable Calls Custom Cell Renderer Method... Continuously","body":"<p>Compilable source can be found at: <a href=\"http://www.splashcd.com/jtable.tar\" rel=\"nofollow\">http://www.splashcd.com/jtable.tar</a></p>\n\n<p>I'm new to the language, so I'm not sure if this is acceptable behavior or not.</p>\n\n<p>I created a JTable to display a row for each message received (it receives about\none every 20 seconds). One of the table columns can contain a large amount of\ntext, so I created a custom cell renderer which word wraps and sets the row\nheight accordingly.</p>\n\n<p>All that works as expected, except that once the table displays its first row,\nit calls the cell renderer about ten times a second... until the user closes the\ntable.</p>\n\n<p>Once I get approx 20 rows in there, the table gets fairly sluggish, taking 2-8\nseconds to resize a column, scoll up or down, or render a selected row with the\nselected background color.</p>\n\n<p>I inserted a print statement inside the renderer, so I can see how many times\nthe getTableCellRendererComponent method is being called.</p>\n\n<p>I disabled tool tips, and disabled all cell editing. I do have a listener that\nscrolls the view to the last row when either a new row is added or the table is\nresized.</p>\n\n<p>Should the getTableCellRendererComponent method be called several times a second\nwhen I'm just viewing the screen (not touching mouse or keyboard)?</p>\n\n<p>TIA</p>\n"},{"tags":["performance","optimization","language-agnostic"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":51,"score":0,"question_id":12868499,"title":"When does it makes sense to move a length getter out of the loop?","body":"<p>Consider the following (not in any particular language):</p>\n\n<pre><code>for (i=0; i&lt;list.length(); i++) { ... }\n</code></pre>\n\n<p>Some people prefer to rewrite it as:</p>\n\n<pre><code>int len = list.length()\nfor (i=0; i&lt;len; i++) { ... }\n</code></pre>\n\n<p>This would make sense if getting the length via <code>list.length()</code> was anything other than O(1). But I don't see any reason why this would be the case. Regardless of the data type, it should be trivial to add a length field somewhere and update it whenever the size changes.</p>\n\n<p>Is there a common data type where getting or updating the length is not O(1)? Or is there another reason why someone would want to do that? </p>\n"},{"tags":["performance","optimization","lua","tips-and-tricks"],"answer_count":4,"favorite_count":4,"up_vote_count":5,"down_vote_count":0,"view_count":2562,"score":5,"question_id":154672,"title":"What can I do to increase the performance of a Lua program?","body":"<p>I asked a question about Lua perfromance, and on of the <a href=\"http://stackoverflow.com/questions/124455/how-do-you-pre-size-an-array-in-lua#152894\">responses</a> asked:</p>\n\n<blockquote>\n  <p>Have you studied general tips for keeping Lua performance high? i.e. know table creation and rather reuse a table than create a new one, use of 'local print=print' and such to avoid global accesses.</p>\n</blockquote>\n\n<p>This is a slightly different question from <a href=\"http://stackoverflow.com/questions/89523/lua-patternstips-and-tricks\">Lua Patterns,Tips and Tricks</a> because I'd like answers that specifically impact performance and (if possible) an explanation of why performance is impacted.</p>\n\n<p>One tip per answer would be ideal.</p>\n"},{"tags":["performance","performancecounter","perfmon","performance-monitor"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":33,"score":1,"question_id":12868345,"title":"Capture All Windows Performance Monitor Counter","body":"<p>I need to monitor the performance of some servers and thus need to create perfmon counters for that.</p>\n\n<p>At the moment I am having to add the counters manually to create the data collector and this is taking a long time to do as the servers have different customised counters.</p>\n\n<p>Is there an easy way to select all of the counters and collect them instead of selecting and adding one by one?</p>\n\n<p>Thanks\nRikesh</p>\n"},{"tags":["mysql","sql","performance","database-design","optimization"],"answer_count":3,"favorite_count":8,"up_vote_count":10,"down_vote_count":0,"view_count":524,"score":10,"question_id":10731333,"title":"Steps to design a well organized and normalized Relational Database","body":"<p>I just started making a database for my website so I am re-reading <code>Database Systems - Design, Implementation and Management (9th Edition)</code>but i notice there is no single step by step process described in the book to create a well organized and normalized database. The book seems to be a little all over the place and although the normalization process is all in one place the steps leading up to it are not. </p>\n\n<p>I thought it be very usefull to have all the steps in one list but i cannot find anything like that online or anywhere else. I realize the answerer explaining all of the steps would be quite an extensive one but anything i can get on this subject will be greatly appreciated; including the order of instructions before normalization and links with suggestions.</p>\n\n<p>Although i am semi familiar with the process i took a long break (about 1 year) from designing any databases so i would like everything described in detail. </p>\n\n<p>I am especially interested in:</p>\n\n<ul>\n<li>Whats a good approach to begin modeling a database (or how to list business rules so its not confusing) </li>\n</ul>\n\n<p>I would like to use ER or EER (extended entity relationship model) and I would like to know</p>\n\n<ul>\n<li>how to model subtypes and supertypes correctly using EER(disjoint and overlapping) (as well as writing down the business rules for it so you know that its a subtype if there is any common way of doing that) </li>\n</ul>\n\n<p>(I allready am familiar with the normalization process but an answer can include tips about it as well)</p>\n\n<p><strong>Still need help with:</strong></p>\n\n<ul>\n<li>Writing down business rules (including business rules for subtypes and super types in EER)</li>\n<li>How to use subtypes and super-types in EER correctly (how to model them) </li>\n</ul>\n\n<p>Any other suggestions will be appreciated. </p>\n"},{"tags":["performance","query","innodb","longtext"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":64,"score":0,"question_id":12863549,"title":"query very slow longtext field innodb table","body":"<p>Well, firts of all, sorry by my english. I try to do a query in a table that the users can include some text, like a blog page. The users can design the content in a html format. In my table it is stored like this:</p>\n\n<pre><code>Estad&amp;amp;iacute;sticas&amp;lt;br /&amp;gt;\n&amp;lt;table border=&amp;quot;0&amp;quot;&amp;gt;\n&amp;lt;tbody&amp;gt;\n&amp;lt;tr&amp;gt;\n&amp;lt;td&amp;gt;Columna 1&amp;lt;/td&amp;gt;\n&amp;lt;td&amp;gt;Columna 2&amp;lt;/td&amp;gt;\n&amp;lt;/tr&amp;gt;\n&amp;lt;tr&amp;gt;\n&amp;lt;td&amp;gt;Columna 3&amp;lt;br /&amp;gt;&amp;lt;/td&amp;gt;\n&amp;lt;td&amp;gt;Columna 4&amp;lt;br /&amp;gt;&amp;lt;/td&amp;gt;\n&amp;lt;/tr&amp;gt;\n&amp;lt;/tbody&amp;gt;\n&amp;lt;/table&amp;gt;\n</code></pre>\n\n<p>I must serch in that content all that user's want. The field 'texto' (that I'm using for it) is a longtext field and the table is innodb. I can't use full text search, 'cause it is only for myisam tables. I made the query as:</p>\n\n<pre><code>\"SELECT * FROM texto WHERE texto like '%$variable%'\"\n</code></pre>\n\n<p>but the query is very, very slow, an it take an eternity. The table has a 849 records, that's isn't big. If I write the same query in a phpmyadmin also take a very, very long time. But there are big records in this field, some records have the video html, tables, images, but it's just that, text like the above.</p>\n\n<p>What I can do??? How can improve the performance of the query??? I appreciate all your help. Thanks a lot. And again, sorry for my english.</p>\n"},{"tags":["mysql","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":47,"score":0,"question_id":12866906,"title":"Need performance improvement suggestions for MySQL 5.5 procedures","body":"<p>I am running MySQL 5.5 - WAMP stack. I am having performance issues. I am new to MySQL hence any guidance will be helpful. I am not too much worried about PHP performance at this time, I am mainly worried about MySQL performance.</p>\n\n<p>I have about 20 procedures(~5000 lines of code) being used in calculating an answer to user question. These procedures mainly select data from 4 main tables(each having around 500 rows) and performs CRUD operations on another 7-8 tables(each having around 100 records). These 7-8 tables' data is deleted after responding user with answer. Due to complex nature of the application, these 20 procedures are called several times recursively (depending upon the type of user input), and CRUD operations are performed on these 7-8 tables for about 1000-2000 times. It is taking around 4 seconds to respond with an answer if run from MySQL Workbench, and around 6 seconds if run from PHP.</p>\n\n<p>Is there a better way to manage this as the procedures are called recursively and same 7-8 tables are updated/selected few thousand times?</p>\n\n<p>I tried tweaking several innodb and other parameters but nothing has shown any improvement yet except setting innodb_flush_log_at_trx_commit to 0.</p>\n"},{"tags":["windows-7","ruby-on-rails","git","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":244,"score":1,"question_id":8169902,"title":"windows 7 very slow console actions","body":"<p>Standard console functions work fast, but when I try to use git  or Ruby on Rails console - actions take too long (like git pull origin master - slow on showing \"Enter passphrase\", RoR - run server and all rake db commands). In average RoR commands take more than 1 minute.</p>\n"},{"tags":["wpf","wcf","performance","startup","datacontractserializer"],"answer_count":3,"favorite_count":1,"up_vote_count":8,"down_vote_count":0,"view_count":931,"score":8,"question_id":12835949,"title":"Using WCF from WPF very slow on first use","body":"<p>I have been struggling for a few days with an issue with our WPF applications and I wonder if someone has come across this before and can help?\nThe problem seems to boil down to the client generating \"on-the-fly\" a serializer to handle the types in that web method call. When that method is called for the first time (the web service itself has been running already), it may take e.g. 8 seconds, subsequent calls may take e.g. 20ms. The CPU on the client WPF process is v. high during this delay.</p>\n\n<p>When using the XmlSerializer, there is a way of pre-generating these serializer assemblies, using svcutil. When (as we are) using the normal WCF DataContractSerializer, this option does not seem to be present.</p>\n\n<p>What I would like is to be able to pre-generate this assembly for all types in all my data contracts (a lot) or, alternatively, to replace this process with a custom one that I can code and passes the data in binary (we own both ends of this webservice/client and they are both .NET 4). I have already used BinaryForamtter and GZip compression and while this speeds up the transfer of data, it always gets restored to XML to be de-serialized by the framework, hence this problem remains.</p>\n\n<p>Any ideas?</p>\n"},{"tags":["sql","performance","postgresql","plpgsql","temporary-tables"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":2,"view_count":64,"score":0,"question_id":12856629,"title":"Copying Rows of Table0 to Table2 Where Same Rows Do Not Exist in Table1 (PostgreSQL)","body":"<p>Could anyone please tell me which one of the following is more efficient? I have tens of millions of rows to process, and performance is critical.</p>\n\n<p>In the second example, <code>table0</code> is a temporary table, which seems to be much faster to create than table0 in the first example. (Why?) I couldn't use a temporary table in the first example because the variable row could not be declared before the table is created. (<code>table0</code> holds all distinct rows of the original table, which is not shown in the code below.)</p>\n\n<p>I guess it'd be a good idea to create hash indices for <code>blah2</code>, <code>blah3</code>, <code>blah4</code> and <code>blah5</code> of <code>table1</code> in the second example, though it would then take longer to write onto the table.</p>\n\n<hr>\n\n<pre><code>FOR row IN SELECT * FROM table0\nLOOP\n  IF NOT EXISTS (SELECT 1 FROM table1\n                   WHERE blah2 = row.blah2 AND blah3 = row.blah3\n                     AND blah4 = row.blah4 AND blah5 = row.blah5) THEN\n    INSERT INTO table2\n      (blah0, blah1, blah2, blah3, blah4, blah5)\n      VALUES (row.blah0, row.blah1, row.blah2, row.blah3, row.blah4, row.blah5);\n  END IF;\nEND LOOP;\n</code></pre>\n\n<hr>\n\n<pre><code>INSERT INTO table2\n  (blah0, blah1, blah2, blah3, blah4, blah5)\n  SELECT blah0, blah1, blah2, blah3, blah4, blah5 FROM table0\n    WHERE NOT EXISTS\n      (SELECT 1 FROM table1\n         WHERE table1.blah2 = table0.blah2\n           AND table1.blah3 = table0.blah3\n           AND table1.blah4 = table0.blah4\n           AND talbe1.blah5 = table0.blah5);\n</code></pre>\n"},{"tags":["mysql","performance","query","group-concat"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":57,"score":0,"question_id":12864693,"title":"Mysql, group_concat performance: split huge table","body":"<p>I've a huge table (231,451,584 rows, 14 columns, 15.1 GiB size). </p>\n\n<p>The last 3 columns have data that will be \"group_concated\" into 3 different tables (resuming the number of rows to ≈2,500 by eliminate the redundancy of the first 11 columns and merging the values of columns #12 or #13 or #14 into a big comma separated, csv formatted text).</p>\n\n<p>This operation (the group_concat insert on each new table) takes a huge time (≈18.000 seconds per new table).</p>\n\n<p>It should be fast if I split my first table into 3 (same 11 columns first, and just a last one with the different values I want to concat, for each new table)?</p>\n\n<p>I'm asking this here because it takes to long to get a benchmark for my case... </p>\n\n<p>Thanks!</p>\n"},{"tags":["javascript","performance","dom","z-index","reflow"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":54,"score":1,"question_id":12828850,"title":"Avoiding zIndex by appending dom elements","body":"<p>So I'm trying to control the stacking of equally-sized  elements via z-index. \nNow an idea that I came across recently to avoid going through z-indices and improve performance times by hopefully to avoiding browser reflows is instead order layers via the order I append things to the parent.</p>\n\n<p>So if I have a container div that holds all the stacking divs, and linked list that mirrors the order, referencing the stacking divs, then I reorder the divs based on user input. Then instead of updating the z-indices, I would recreate the div element and just append everything in order. So something like this:</p>\n\n<pre><code>var from = nodeBeforeFrom; // Input\nvar target = nodeBeforeTarget; // Input\nvar linkedlist = input; // var linkedlist contains all the stacking divs\nlinkedlist.moveElement(div1, div2); //Move div1 to after div2\nvar container = document.createElement('div');\n\nlinkedlist.reorder; // \n\nvar cur = linkedlist.first;\nwhile (cur.next) {\n  container.appendChild(cur)\n  cur = cur.next;\n}\ndocument.removeChild(oldContainer);\ndocument.appendChild(container);\n// This is meant as pseudocode so forgive an errors in regards to the specifics\n</code></pre>\n\n<p>So my questions are the following:</p>\n\n<ol>\n<li>Would this reduce browser reflows from n reflows to just 1 or 2 (where n is the number of divs)? If I understand it right, changing the z-index of a single element should cause either a browser repaint or a reflow.</li>\n<li>Will the second approach work and stack elements in the order you append them? </li>\n<li>Is there a way to move childs around using the DOM's child node structure already so I don't have to create a separate linked list? I only see removeChild and appendChild functions that I can use at the moment.</li>\n</ol>\n\n<p>And yes performance is an issue since I'm planning on using this for graphics and html5 stuff. So where I can save I would like to save.</p>\n"},{"tags":["javascript","performance","benchmarking"],"answer_count":1,"favorite_count":3,"up_vote_count":3,"down_vote_count":0,"view_count":90,"score":3,"question_id":12863861,"title":"JavaScript - string versus integer key","body":"<p>I was running performance benchmarks for jQuery (don't ask) and discovered something interesting. For some reason it seems that <code>this[0] = element</code> is quite slow compared to <code>this.foo = element</code>. Here is the obligatory <a href=\"http://jsperf.com/string-integer-property\" rel=\"nofollow\">jsPerf case</a>.</p>\n\n<p>Can anybody explain why there is such a performance hit? Is there any way to improve the performance apart from the obvious \"use a string key\"?</p>\n"},{"tags":["javascript","performance","knockout.js"],"answer_count":7,"favorite_count":12,"up_vote_count":16,"down_vote_count":0,"view_count":3540,"score":16,"question_id":9709374,"title":"Knockout.js incredibly slow under semi-large datasets","body":"<p>I'm just getting started with Knockout.js (always wanted to try it out, but now I finally have an excuse!) - However, I'm running into some really bad performance problems when binding a table to a relatively small set of data (around 400 rows or so).</p>\n\n<p>In my model, I have the following code:</p>\n\n<pre><code>this.projects = ko.observableArray( [] ); //Bind to empty array at startup\n\nthis.loadData = function (data) //Called when AJAX method returns\n{\n   for(var i = 0; i &lt; data.length; i++)\n   {\n      this.projects.push(new ResultRow(data[i])); //&lt;-- Bottleneck!\n   }\n};\n</code></pre>\n\n<p>The issue is the <code>for</code> loop above takes about 30 seconds to so with around 400 rows.  However, if I change the code to:</p>\n\n<pre><code>this.loadData = function (data)\n{\n   var testArray = []; //&lt;-- Plain ol' Javascript array\n   for(var i = 0; i &lt; data.length; i++)\n   {\n      testArray.push(new ResultRow(data[i]));\n   }\n};\n</code></pre>\n\n<p>Then the <code>for</code> loop completes in the blink of an eye.  In other words, the <code>push</code> method of Knockout's <code>observableArray</code> object is incredibly slow.</p>\n\n<p>Here is my template:</p>\n\n<pre><code>&lt;tbody data-bind=\"foreach: projects\"&gt;\n    &lt;tr&gt;\n       &lt;td data-bind=\"text: code\"&gt;&lt;/td&gt;\n       &lt;td&gt;&lt;a data-bind=\"projlink: key, text: projname\"&gt;&lt;/td&gt;\n       &lt;td data-bind=\"text: request\"&gt;&lt;/td&gt;\n       &lt;td data-bind=\"text: stage\"&gt;&lt;/td&gt;\n       &lt;td data-bind=\"text: type\"&gt;&lt;/td&gt;\n       &lt;td data-bind=\"text: launch\"&gt;&lt;/td&gt;\n       &lt;td&gt;&lt;a data-bind=\"mailto: ownerEmail, text: owner\"&gt;&lt;/a&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n&lt;/tbody&gt;\n</code></pre>\n\n<p><strong>My Questions:</strong></p>\n\n<ol>\n<li>Is this the right way to bind my data (which comes from an AJAX method) to an observable collection?</li>\n<li>I expect <code>push</code> is doing some heavy re-calc every time I call it, such as maybe rebuilding bound DOM objects.  Is there a way to either delay this recalc, or perhaps push in all my items at once?</li>\n</ol>\n\n<p>I can add more code if needed, but I'm pretty sure this is what's relevant.  For the most part I was just following Knockout tutorials from the site.</p>\n\n<p><strong>UPDATE:</strong></p>\n\n<p>Per the advice below, I've updated my code:</p>\n\n<pre><code>this.loadData = function (data)\n{\n   var mappedData = $.map(data, function (item) { return new ResultRow(item) });\n   this.projects(mappedData);\n};\n</code></pre>\n\n<p>However, <code>this.projects()</code> still takes about 10 seconds for 400 rows.  I do admit I'm not sure how fast this would be <em>without</em> Knockout (just adding rows through the DOM), but I have a feeling it would be much faster than 10 seconds.</p>\n\n<p><strong>UPDATE 2:</strong></p>\n\n<p>Per other advice below, I gave <strong>jQuery.tmpl</strong> a shot (which is natively supported by KnockOut), and this templating engine will draw around 400 rows in just over 3 seconds.  This seems like the best approach, short of a solution that would dynamically load in more data as you scroll.</p>\n"},{"tags":["performance","ruby-on-rails-3","windows-7","cygwin","mingw"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":876,"score":2,"question_id":7276993,"title":"Improve Ruby on Rails Performance Windows 7","body":"<p>I'm pursuing Ruby on Rails development but using windows to perform rake and rails tasks is PAINFULLY slow, but I heard it's quite the opposite on Linux.</p>\n\n<p>I'm using a Netbook (Acer Aspire One 722) for development and using VirtualBox to run Ubuntu is out of the question. Doing the whole dual boot thing is also not an option because I run into severe processor load balancing and heating issues that I really do not have the luxury of time to troubleshoot right now.</p>\n\n<p>What I would like to know is: is there anything I can install or any settings I can change that will give me linux-like speed when performing these rake and rails tasks on windows 7?</p>\n\n<p>I've heard that Cygwin and Mingw are \"linux emulators\", is there any way I could leverage them?</p>\n\n<p>Thanks.</p>\n"},{"tags":["performance","optimization","measurement"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":1744,"score":2,"question_id":1346308,"title":"How do you measure site load time in IE6?","body":"<p>I'm looking for something similar to <a href=\"http://stevesouders.com/hammerhead/\" rel=\"nofollow\">Hammerhead</a>. Currently, I write javascript code to test, and I'd rather just use a tool that I can easily share and has a GUI. </p>\n\n<p><strong>Edit: I'm hoping for something that tracks load events if possible and can easily do repeat tests.</strong></p>\n"},{"tags":["mysql","sql","performance","database-performance","sqlperformance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":72,"score":1,"question_id":12856783,"title":"Best practice with mysql innodb to rename huge table when table with same name already exist","body":"<p>I Use Mysql 5.5..  + INNODB and windows server.</p>\n\n<p>The case(make it simple then real case):</p>\n\n<p>I have 2 tables 1GB with name <code>new_car</code> and <code>car</code> table 1GB.</p>\n\n<p>I to replace <code>car</code> table with <code>new_car</code> table every 10 hours not manually(auto by code) - important to do it fast(real website data).</p>\n\n<p>I read(say that drop its problem in innodb) :<a href=\"http://www.mysqlperformanceblog.com/2011/02/03/performance-problem-with-innodb-and-drop-table/\" rel=\"nofollow\">http://www.mysqlperformanceblog.com/2011/02/03/performance-problem-with-innodb-and-drop-table/</a></p>\n\n<p>solution1:</p>\n\n<pre><code>DROP TABLE car;\nRENAME TABLE new_car TO car;\n</code></pre>\n\n<p>Solution2(making drop in the end -maybe it not block the table to access that happen during drop):</p>\n\n<pre><code>RENAME TABLE car TO temp_car;\nRENAME TABLE new_car TO car;\nDROP TABLE temp_car;\n</code></pre>\n\n<p>Solution3(Truncate delete fast the table and create empty table then maybe drop action after should be very fast):</p>\n\n<pre><code>TRUNCATE TABLE car;\nDROP TABLE car;\nRENAME TABLE new_car TO car; \n</code></pre>\n\n<p>Solution4:</p>\n\n<pre><code>RENAME TABLE car TO temp_car;\nRENAME TABLE new_car TO car;\nTRUNCATE TABLE temp_car;\nDROP TABLE temp_car;\n</code></pre>\n\n<p>Which solution is the best and why or please write other better solution?</p>\n\n<p>Thanks</p>\n"},{"tags":["java","android","performance","bitmap"],"answer_count":1,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":4595,"score":2,"question_id":4715840,"title":"improving speed of getpixel() and setpixel() on Android Bitmap","body":"<p>All,</p>\n\n<p>after I noticed how slow getPixel and setPixel are (not sure which one, guess both are not turbocharged) I quickly coded a container for Bitmap that uses int[] array to handle bitmap operations. </p>\n\n<p>Already - its noticeably faster, but this is not enough. Please could you advice how to speed it further? </p>\n\n<p>My idea is to keep track of what is made \"dirty\" by the setPixel functions and update only this part of Bitmap when getBitmap() is called ... not clear how to set the setPixels parameters though (something with offset and stride I guess). </p>\n\n<p>Also - any faster recipe? </p>\n\n<p>Thanks for all help in advance!</p>\n\n<pre><code>import android.graphics.Bitmap;\n\npublic class DrawableBitmapContainer {\nprivate Bitmap image;\nprivate int width, height;\nprivate int[]  pixels;\npublic DrawableBitmapContainer(Bitmap _source ){\n    image = _source;\n    width = image.getWidth();\n    height = image.getHeight();\n    pixels = new int[width*height];\n    image.getPixels(pixels,0,width,0,0,width,height);\n}\npublic int getPixel(int x,int y){\n    return pixels[x+y*width];\n}\npublic void setPixel(int x,int y, int color){\n    pixels[x+y*width]=color;\n}\npublic Bitmap getBimap(){\n    image.setPixels(pixels,0,width,0,0,width,height);\n    return image;\n}\npublic int getWidth(){\n    return image.getWidth();\n}\npublic int getHeight(){\n    return image.getHeight();\n}\n}\n</code></pre>\n"},{"tags":["iphone","objective-c","ios","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":69,"score":3,"question_id":12467009,"title":"How can I speed up the loading of images from a web service?","body":"<p>I'm new to iPhone development. In my application, I had kept a scrollview and I have loaded the images from web service using JSON parsing. Unfortunately, it is taking too much time to load them. Do you have any suggestions on how to speed up the (down)loading?</p>\n"},{"tags":["python","performance","nlp","text-processing","nltk"],"answer_count":3,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":385,"score":1,"question_id":6621666,"title":"Real time text processing using Python","body":"<p>Real time text processing using Python. For e.g. consider this sentance</p>\n\n<pre>\nI am going to schol today\n</pre>\n\n<p>I want to do the following (real time):</p>\n\n<pre>\n1) tokenize \n2) check spellings\n3) stem(nltk.PorterStemmer()) \n4) lemmatize (nltk.WordNetLemmatizer())\n</pre> \n\n<p>Currently I am using <a href=\"http://www.nltk.org/\" rel=\"nofollow\">NLTK</a> library to do these operations, but its not real time (meaning its taking few seconds to complete these operations). I am processing 1 sentence at a time, Is it possible to make it efficient</p>\n\n<p>Update:\nProfiling: </p>\n\n<pre>\nFri Jul  8 17:59:32 2011    srj.profile\n\n         105503 function calls (101919 primitive calls) in 1.743 CPU seconds\n\n   Ordered by: internal time\n   List reduced from 1797 to 10 due to restriction \n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n     7450    0.136    0.000    0.208    0.000 sre_parse.py:182(__next)\n  602/179    0.130    0.000    0.583    0.003 sre_parse.py:379(_parse)\n23467/22658    0.122    0.000    0.130    0.000 {len}\n 1158/142    0.092    0.000    0.313    0.002 sre_compile.py:32(_compile)\n    16152    0.081    0.000    0.081    0.000 {method 'append' of 'list' objects}\n     6365    0.070    0.000    0.249    0.000 sre_parse.py:201(get)\n     4947    0.058    0.000    0.086    0.000 sre_parse.py:130(__getitem__)\n 1641/639    0.039    0.000    0.055    0.000 sre_parse.py:140(getwidth)\n      457    0.035    0.000    0.103    0.000 sre_compile.py:207(_optimize_charset)\n     6512    0.034    0.000    0.034    0.000 {isinstance}\n\n</pre>\n\n<p>timit:</p>\n\n<pre>\nt = timeit.Timer(main)\nprint t.timeit(1000)\n\n=> 3.7256231308\n</pre>\n"},{"tags":["c++","string","performance","if-statement","string-length"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":108,"score":1,"question_id":12856288,"title":"Code performance between if else statements and length of string","body":"<p>I'm writing code that takes a number from a user and prints in back in letters as string. I want to know, which is better performance-wise, to have if statements, like </p>\n\n<pre><code>if (n &lt; 100) {\n    // code for 2-digit numbers\n} else if (n &lt; 1000) {\n    // code for 3-digit numbers\n} // etc..\n</code></pre>\n\n<p>or to put the number in a string and get its length, then work on it as a string.</p>\n\n<p>The code is written in C++.</p>\n"},{"tags":["performance","index","sql-server-2008-r2"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":56,"score":0,"question_id":12859636,"title":"Column index not updating in SQL Server 2008","body":"<p>I had asked a question on stackoverflow:\n<a href=\"http://stackoverflow.com/questions/12460602/large-mssql-database-timing-out-php-web-application\">Large SQL Server database timing out PHP web application</a> </p>\n\n<p>The problem was that the query was fast for when a previous date was chosen and slow for a more current date. We fixed it by recreating the index and hence it worked. For a while I thought the index might have gotten corrupted. </p>\n\n<p>Today the issue occurred again (slow) and by recreating it again it ran fast. This was done on the date column which is of type datetime.</p>\n\n<p>Is there a specific reason for this or is the SQL Server 2008 R2 corrupted?</p>\n"},{"tags":["sql-server","performance","query","large-data"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":60,"score":0,"question_id":12460602,"title":"Large SQL Server database timing out PHP web application","body":"<p>We are running a hospital system which is web based created in PHP. The system was initially fast due to small size of the database but now it has become slow.</p>\n\n<p>The following is an example query</p>\n\n<pre><code>select pa.id, pa.date as date, pa.visitno, pa.receiptno, pa.debitnoteno, pad.id as padid,\n pad.serviceid as serviceid, pad.waitno, pa.paytype, s.id as doctorid, s.fullname as\n doctorname, p.id as patientid, p.name as patient, p.regno, p.age, p.gender, p.doc,\n p.department, p.telno, p.address, pa.ins_prov_id, ip.name as provider,\n pa.sicksheet_billcode as billcode, ds.id as serviceid, ds.name as servicename, ds.departid\n as departid, ds.servicetype as servicetype, pad.charge, pad.status as status, ts.id as\n timeslotid, ts.name as timeslot, pad.treatment, sd.anesthesiologist, sd.hospitalcharge,\n sd.anesthcharge from patientappointments as pa\nINNER JOIN patientappdetails as pad ON pa.id = pad.patappid \nINNER JOIN patients as p ON pa.patid = p.id \nINNER JOIN staffs as s ON pad.doctorid = s.id \nLEFT JOIN departmentalservices as ds ON pad.serviceid = ds.id \nLEFT JOIN insproviders as ip ON pa.ins_prov_id = ip.id \nLEFT JOIN timeslots as ts ON pad.timeslotid = ts.id \nLEFT JOIN surgerydetails as sd ON sd.appdetid = pad.id \nwhere 1 = 1 and pa.date &gt;= '01.Jul.2012' and ds.departgroupid = 16 and pad.charge != 0\n</code></pre>\n\n<p>As you can see the size of our queries (call them un-optimized) which shows the patient, doctor, service taken, what time and which ins company he came from. So now we created indexes that did help for a while but now again the speed has become slow. Running the system on localhost results in around 15 secs for the result to appear while on the live system, <strong>it times out</strong>.</p>\n\n<p>Can you suggest any method to improve the speed and exactly how to implement them. </p>\n\n<p>Just FYI, rows in each table are as follows:</p>\n\n<ul>\n<li>patientappdetails - 195k</li>\n<li>patients - 34k</li>\n<li>staffs - 200</li>\n<li>departmentalservices - 700</li>\n<li>insproviders - 2800</li>\n</ul>\n\n<p>Thank you</p>\n"},{"tags":["performance","query","mongodb","limit"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":25,"score":0,"question_id":12859857,"title":"Find the next \"older\" document (performance)","body":"<p>I have a collection of posts (see the pseudo-json :-) ) : </p>\n\n<pre><code>[\n  {\"title\":\"First\",\"created\":\"2012-10-04 01:00:00\"},\n  {\"title\":\"Second\",\"created\":\"2012-10-04 03:30:00\"},\n  {\"title\":\"Third\",\"created\":\"2012-10-02 02:00:00\"}\n]\n</code></pre>\n\n<p>I am in the \"2012-10-04\" posts page and I want to know if there are older posts, just for displaying a \"next page\" button.</p>\n\n<p>I can think about a query like this:</p>\n\n<pre><code>db.posts.find({\"created\":{$lt:ISODate(\"2012-10-04\")} }).limit(1);\n</code></pre>\n\n<p>I still have to try it, but I think it will work... actually my question is:</p>\n\n<p><em>What about its performance? Will it run at the same speed with billions of documents in the collection?</em></p>\n"},{"tags":["python","performance","coding-style","progress-bar","easy-install"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":57,"score":1,"question_id":12859253,"title":"Python: how to use a progressbar inside my function","body":"<p>I am using the following function: </p>\n\n<pre><code>def LAS2TXTGridClip(inFile,poly,MinPoints=1):\n        sf = shapefile.Reader(poly) #open shpfile\n        sr = sf.shapeRecords()\n        poly_filename, ext = path.splitext(poly)\n        inFile_filename = os.path.splitext(os.path.basename(inFile))[0]\n        for i in xrange(len(sr)):\n            verts = np.array(sr[i].shape.points,float)\n            record = sr[i].record[0]\n            inside_points = [p for p in lasfile.File(inFile,None,'r') if pnpoly(p.x, p.y, verts)]\n            if len(inside_points) &gt;= MinPoints:\n                file_out = open(\"{0}_{1}_{2}.txt\".format(poly_filename, inFile_filename, record), \"w\")\n                for p in inside_points:\n                    file_out.write(\"%s %s %s %s %s %s %s %s %s %s %s\" % (p.x, p.y, p.z, p.intensity,p.return_number,p.number_of_returns,p.scan_direction,p.flightline_edge,p.classification,p.scan_angle,record)+ \"\\n\")\n                file_out.close()\n</code></pre>\n\n<p>where <code>for i in xrange(len(sr)):</code> the function will be process several times. The <code>len(sr)</code> is around half million and I wish a insert a progress bar in order to have an idea of the time I need to wait (it's friday). I have the following question:</p>\n\n<ol>\n<li>Which is the \"best and easy\" progressbar for python 27 on windows OS\n64bit?</li>\n<li>I found <a href=\"http://pypi.python.org/pypi/progressbar/2.2\" rel=\"nofollow\">progressbar module</a> but I have problem to use\neasy_install progressbar after this step.</li>\n<li>where is the best position to insert the progressbar?</li>\n</ol>\n"},{"tags":["performance","drupal","memory","memory-limit","ini-set"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":30,"score":-1,"question_id":12859159,"title":"Why Drupal doesn't use special Memory Limit (ini_set) in Cache Clearing Script?","body":"<p>In Drupal 7, i have found that <code>CLEAR CACHE</code> (on the Web Panel) is using extreme high Memory resource. Giant projects like mine, needs over 350MB to that process.</p>\n\n<ul>\n<li>Then i set <code>400MB</code> inside <code>php.ini</code></li>\n</ul>\n\n<p>So as far as i have investigated, i have found that having high memory limit (Globally) is totally the <code>damage</code> to the overall Performance because every single script is using that limit in memory.</p>\n\n<ul>\n<li>Then i found, we can use separate/runtime setting as <code>ini_set('memory_limit',____)</code> only for the specific scripts.</li>\n</ul>\n\n<p>So my question here is:</p>\n\n<ul>\n<li>Why Drupal by default is <code>NOT HAVING</code> this <code>ini_set('memory_limit',____)</code> separately in Cache Clearing Script (hardcoded or Panel Setting, etc) ?</li>\n</ul>\n"},{"tags":["php","performance","memory","memory-limit"],"answer_count":3,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":86,"score":4,"question_id":12858457,"title":"Is the more the better for PHP memory_limit?","body":"<p>In PHP, i am oftenly facing memory limit problem. Especially with highly resource integrated systems like Drupal, etc.</p>\n\n<p>What i want to know here is:</p>\n\n<ul>\n<li>Is it good to have very high php memory limit like 2GB?</li>\n<li>Is there any major drawback?</li>\n</ul>\n\n<p><strong>Edited:</strong></p>\n\n<blockquote>\n  <p>As a scenario, for example in Drupal, it NEEDS so much memory while we\n  CLEAR the CACHE via Web Panel (Not by Drush or any script). So even for\n  this case only, i am definitely needing high-limit around 512MB currently.</p>\n</blockquote>\n"},{"tags":["performance","azure","disk","infrastructure"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":130,"score":2,"question_id":11844900,"title":"Is there any data on how fast Azure VM local drives are?","body":"<p>I'm experimenting with <code>OnStart()</code> in my Azure role using \"small\" instances. Turns out it takes about two minutes to unpack a 400 megabytes ZIP file that is located in \"local storage\" on drive D into a folder on drive E.</p>\n\n<p>I though maybe I should do it some other way around but I can't find any data about how fast the local disks on Azure VMs typically are.</p>\n\n<p>Are there any test results for how fast Azure VM local disks are?</p>\n"},{"tags":["mysql","sql","performance","query"],"answer_count":2,"favorite_count":2,"up_vote_count":0,"down_vote_count":0,"view_count":72,"score":0,"question_id":12858224,"title":"Why doesn't this query run?","body":"<p>I have this query that isn't finishing (I think the server runs out of memory)</p>\n\n<pre><code>SELECT fOpen.*, fClose.*\nFROM (\n    SELECT of.*\n    FROM fixtures of\n        JOIN (\n            SELECT MIN(id) id\n            FROM fixtures\n            GROUP BY matchId, period, type\n        ) ofi ON ofi.id = of.id\n) fOpen\nJOIN (\n    SELECT cf.*\n    FROM fixtures cf\n        JOIN (\n            SELECT MAX(id) id\n            FROM fixtures\n            GROUP BY matchId, period, type\n        ) cfi ON cfi.id = cf.id\n) fClose ON fClose.matchId = fOpen.matchId AND fClose.period = fOpen.period AND fClose.type = fOpen.type\n</code></pre>\n\n<p>This is the EXPLAIN of it:</p>\n\n<p><img src=\"http://i.stack.imgur.com/EP83o.png\" alt=\"\"></p>\n\n<p>Those 2 subqueries 'of' and 'cf' take about 1.5s to run, if I run them separately.</p>\n\n<p>'id' is a PRIMARY INDEX and there is a BTREE INDEX named 'matchPeriodType' that has those 3 columns in that order.</p>\n\n<p>More info: MySQL 5.5, 512MB of server memory, and the table has about 400k records.</p>\n"},{"tags":["performance","design","sorting","filter","mapreduce"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":12846878,"title":"How to filter and sort large resultsets in-memory","body":"<p>I have two REST services:</p>\n\n<ol>\n<li>Service-one provides a list of all ids that should not be included in the final result set</li>\n<li>Service-two maintains a list of ids that need to sorted</li>\n</ol>\n\n<p>I need to capture the result (all excluded ids) from service-one and then remove from the list obtained by invoking service 2 and then sort the resultant set.</p>\n\n<p>I need perform this in-memory, the result sets could be huge (20k to 100 k rows, but only ids and a few other columns). I need performance that should be good.</p>\n\n<p>I was wondering if I could employ mapreduce to perform this task, but I am not sure if it can provide the necessary performance. (entire operation has to complete quickly)</p>\n\n<p>Any suggestions/clues about how to approach this problem?</p>\n\n<p>I could regularly invoke the services, store the results and then run a SQL query, but wanted to check if there ways of doing this in memory and dynamically without storing the results </p>\n\n<p>Environment: Windows, ASP.Net Web API for REST Services. Can use No-SQLs such as Mongo, but is there a simpler solution?</p>\n"},{"tags":["c#",".net","performance","datetime","stopwatch"],"answer_count":7,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":715,"score":0,"question_id":1566852,"title":"DateTime.AddDays or new DateTime","body":"<p>I'm creating a list of a month's worth of dates.  I'm wondering what will be more efficient</p>\n\n<pre><code>List&lt;DateTime&gt; GetDates(DateTime StartDay) {\n  List&lt;DateTime&gt; dates = new List&lt;DateTime&gt;();\n  int TotalDays=StartDay.AddMonths(1).AddDays(-1).Day;\n  for (int i=1; i&lt;TotalDays; i++) {\n    dates.Add(new DateTime(StartDay.Year, StartDay.Month, i));\n  }\n  return dates;\n}\n</code></pre>\n\n<p>or</p>\n\n<pre><code>List&lt;DateTime&gt; GetDates(DateTime StartDay) {\n  List&lt;DateTime&gt; dates = new List&lt;DateTime&gt;();\n  DateTime NextMonth = StartDay.AddMonths(1);\n  for (DateTime curr=StartDay; !curr.Equals(NextMonth); curr=curr.AddDays(1)) {\n    dates.Add(curr);\n  }\n  return dates;\n}\n</code></pre>\n\n<p>basically, is new DateTime() or DateTime.addDays more efficient.  </p>\n\n<p>UPDATE:</p>\n\n<pre><code>static void Main(string[] args) {\n  System.Diagnostics.Stopwatch sw=new System.Diagnostics.Stopwatch();\n  long t1, t2, total;\n  List&lt;DateTime&gt; l;\n  DateTime begin = DateTime.Now;\n  total = 0L;\n  for (int i=0; i&lt;10; i++) {\n    sw.Start();\n    l = GetDates(begin);\n    sw.Stop();\n\n\n    sw.Stop();\n    t1 = sw.ElapsedTicks;\n    sw.Reset();\n    sw.Start();\n\n    l = GetDates2(begin);\n    sw.Stop();\n    t2=sw.ElapsedTicks;\n    total +=  t1- t2;\n\n    Console.WriteLine(\"Test {0} : {1} {2} : {3}\", i,t1,t2, t1- t2);\n  }\n  Console.WriteLine(\"Total: {0}\", total);\n\n  Console.WriteLine(\"\\n\\nDone\");\n  Console.ReadLine();\n}\n\nstatic List&lt;DateTime&gt; GetDates(DateTime StartDay) {\n  List&lt;DateTime&gt; dates = new List&lt;DateTime&gt;();\n  int TotalDays=StartDay.AddMonths(10000).AddDays(-1).Day;\n  for (int i=1; i&lt;TotalDays; i++) {\n    dates.Add(new DateTime(StartDay.Year, StartDay.Month, i));\n  }\n  return dates;\n}\n\n\nstatic List&lt;DateTime&gt; GetDates2(DateTime StartDay) {\n  List&lt;DateTime&gt; dates = new List&lt;DateTime&gt;();\n  DateTime NextMonth = StartDay.AddMonths(10000);\n  for (DateTime curr=StartDay; !curr.Equals(NextMonth); curr=curr.AddDays(1)) {\n    dates.Add(curr);\n  }\n  return dates;\n}\n</code></pre>\n\n<pre>\nTest 0 : 2203229 63086205 : -60882976\nTest 1 : 63126483 102969090 : -39842607\nTest 2 : 102991588 93487982 : 9503606\nTest 3 : 93510942 69439034 : 24071908\nTest 4 : 69465137 70660555 : -1195418\nTest 5 : 70695702 68224849 : 2470853\nTest 6 : 68248593 63555492 : 4693101\nTest 7 : 63578536 65086357 : -1507821\nTest 8 : 65108190 64035573 : 1072617\nTest 9 : 64066128 64933449 : -867321\nTotal: -62484058\n\nDone\n</pre>\n\n<p>results are consistently negative... way negative, so, looks like the constructor and integer test is the more efficient method.</p>\n"},{"tags":["python","performance","multiprocessing","multicore"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":55,"score":0,"question_id":12855914,"title":"Python: how to run several scripts (or functions) at the same time under windows 7 multicore processor 64bit","body":"<p>sorry for this question because there are several examples in Stackoverflow. I am writing in order to clarify some of my doubts because I am quite new in Python language. </p>\n\n<p>i wrote a function:</p>\n\n<pre><code>def clipmyfile(inFile,poly,outFile):\n... # doing something with inFile and poly and return outFile\n</code></pre>\n\n<p>Normally I do this:</p>\n\n<pre><code>clipmyfile(inFile=\"File1.txt\",poly=\"poly1.shp\",outFile=\"res1.txt\")\nclipmyfile(inFile=\"File2.txt\",poly=\"poly2.shp\",outFile=\"res2.txt\")\nclipmyfile(inFile=\"File3.txt\",poly=\"poly3.shp\",outFile=\"res3.txt\")\n......\nclipmyfile(inFile=\"File21.txt\",poly=\"poly21.shp\",outFile=\"res21.txt\")\n</code></pre>\n\n<p>I had read in this example <a href=\"http://stackoverflow.com/questions/12126655/run-several-python-programs-at-the-same-time\">Run several python programs at the same time</a> and i can use (but probably i wrong)</p>\n\n<pre><code>from multiprocessing import Pool\np = Pool(21)  # like in your example, running 21 separate processes\n</code></pre>\n\n<p>to run the function in the same time and speed my analysis</p>\n\n<p>I am really honest to say that I didn't understand the next step.</p>\n\n<p>Thanks in advance for help and suggestion\nGianni</p>\n"},{"tags":["php","python","performance","testing","load"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":44,"score":0,"question_id":12856487,"title":"Need load testing tool where we configure the URLs","body":"<p>I require the tool for testing the load of server, I need load testing tool where I can configure the URL's used for testing the load.</p>\n"},{"tags":["jquery","performance","user-interface","position","bounce"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":36,"score":0,"question_id":12856861,"title":"Unexpected behavior of changing position with hover and jQuery UI bounce effect","body":"<p>I've got this jQuery code:</p>\n\n<pre><code>$('.cennik-circle').hover(function() {                  \n    $(this).effect('bounce', { times:1, distance:10  }, 200); \n}, \n\n    function() { }\n);\n</code></pre>\n\n<p>What it does, it gives a bounce effect while hover on .cennik-circle element.\nThis is an example: <a href=\"http://goo.gl/z585f\" rel=\"nofollow\">http://goo.gl/z585f</a></p>\n\n<p>When you hover, for example at one of the white circles you'll see an effect of bounce. But if you try to move mouse cursor fast (over and out) you will see that the circle somehow change its position to left: 0; Why does it happen? What is the reason for this issue? How to avoid it?</p>\n\n<p>Regards, \nDave </p>\n"},{"tags":["php","performance"],"answer_count":7,"favorite_count":18,"up_vote_count":41,"down_vote_count":1,"view_count":9529,"score":40,"question_id":482202,"title":"Is there a performance benefit single quote vs double quote in php?","body":"<p>Are there any performance benefits to using single quotes instead of double quotes in php?</p>\n\n<p>In other words, would there be a performance benefit of:</p>\n\n<pre><code>$foo = 'Test';\n</code></pre>\n\n<p>versus</p>\n\n<pre><code>$foo = \"Test\";\n</code></pre>\n"}]}
