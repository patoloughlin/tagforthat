{"total":25592,"page":1,"pagesize":100,"questions":[{"tags":["performance","assembly","x86"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":70,"score":3,"question_id":13103680,"title":"Can a shift using the CL register result in a partial register stall?","body":"<p>Can a variable shift generate a partial register stall (or register recombining µops) on <code>ecx</code>? If so, on which microarchitecture(s)?</p>\n\n<p>I have tested this on Core2 (65nm), which seems to read only <code>cl</code>.</p>\n\n<pre><code>_shiftbench:\n    push rbx\n    mov edx, -10000000\n    mov ecx, 5\n  _shiftloop:\n    mov bl, 5   ; replace by cl to see possible recombining\n    shl eax, cl\n    add edx, 1\n    jnz _shiftloop\n    pop rbx\n    ret\n</code></pre>\n\n<p>Replacing <code>mov bl, 5</code> by <code>mov cl, 5</code> made no difference, which it would have if there was register recombining going on, as can be demonstrated by replacing <code>shl eax, cl</code> by <code>add eax, ecx</code> (in my tests the version with <code>add</code> experienced a 2.8x slowdown when writing to <code>cl</code> instead of <code>bl</code>).</p>\n\n<hr>\n\n<p>Test results:</p>\n\n<ul>\n<li>Merom: no stall observed</li>\n<li>Penryn: no stall observed</li>\n<li>Nehalem: no stall observed</li>\n</ul>\n"},{"tags":["c++","performance","design-patterns","data-structures","iterator"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":135,"score":0,"question_id":13219821,"title":"How to access and manage block stored data","body":"<p><strong>What I want to do:</strong> I need to store cell data in block-wise form, that is</p>\n\n<p>*cell_member1[cell0] .. cell_member1[cellN]  ...  cell_memberM[cell0] .. cell_memberM[cellN]*</p>\n\n<p>Then I need to access this data efficiently and, if possible, using a nice syntax. It would be great if I could define the data to be stored easily, i.e. by defining an object with members as the data that I want to store and passing it to some \"magic\" that does everything for me.</p>\n\n<p><strong>How I'm currently doing it:</strong> I have a container of the form: </p>\n\n<pre><code>template&lt;class T&gt; struct Container {\n  char* data;\n  Container(const int n) {\n    data = new char[n*T::spaceRequirements()]; //&lt; Data stored \"block-wise\"\n    new(data) typename T::Flags[n]; //&lt; Flags stored \"cell-wise\"\n  }\n  /// Destructor ommited for briefness.\n};\n</code></pre>\n\n<p>in which I store data for some cells of type T. I need some flags per cell and right now I'm using std::bitset to store them which means that I need to store this bitsets in cell-wise form: </p>\n\n<p>*cell_member1[cell0] ... cell_memberM[cell0] ... cell_member1[cellN] .. cell_memberM[cellN]*</p>\n\n<p>I am describing how much data per cell needs to be stored in the following class, which also provides access to the data:</p>\n\n<pre><code>template&lt;int nd&gt; struct CellAccessor {\n  /// Cell flags are stored cell-wise:\n  typedef std::bitset&lt;64&gt; Flags;\n  enum { DELETE = 0, ///&lt; Cell marked for deletion\n         REFINE = 1 ///&lt; Cell marked for refinement\n         //...\n  }; ///&lt; Enum for the flags.\n  static inline Flags&amp; flags(const int cellId) {\n    return *reinterpret_cast&lt;Flags*&gt;(data + sizeof(Flags)*cellId); }\n  template&lt;int pId&gt; static inline Flags::reference flags(const int cellId) {\n    return flags(cellId)[pId]; } //&lt; Cell-wise access to the properties\n\n  /// The rest of the data is stored block-wise:\n  static inline int&amp; order(const int cellId) { ///&lt; One int field.\n    return *reinterpret_cast&lt;int*&gt;\n        (data + maxNoCells*sizeof(Flags) + sizeof(int)*cellId);}\n\n  /// Coordinate vector with nd components:\n  static inline double&amp; coordinates(const int cellId, const int i) {\n    return *reinterpret_cast&lt;double*&gt;\n        (data + maxNoCells*(sizeof(Flags)+sizeof(int))\n         + maxNoCells*i*sizeof(double) + sizeof(double)*cellId); }\n  template&lt;int i&gt; static inline double&amp; coordinates(const int cellId) {\n    return *reinterpret_cast&lt;double*&gt;\n        (data +maxNoCells*(sizeof(Flags)+sizeof(int)+i*sizeof(double))\n         + sizeof(double)*cellId); }\n\n  /// Total amount of memory to allocate per cell: (used by Container)\n  static inline int spaceRequirements() { return\n        sizeof(Flags) // Flags\n        + sizeof(int) // order\n        + nd*sizeof(double) // coordinates\n        ;}\n\n  /// Constructor gets pointer to the beginning of the container \n  /// and the offset for the member variables:\n  CellAccessor(char* d, int n){data = d; maxNoCells = n;}\n private:\n  static char* data;  ///&lt; Pointer to the beginning of the container.\n  static int maxNoCells;  ///&lt; Cell offset for the member variables.\n};\ntemplate&lt;int nd&gt; char* CellAccessor&lt;nd&gt;::data = nullptr;\ntemplate&lt;int nd&gt; int CellAccessor&lt;nd&gt;::maxNoCells = 0;\n</code></pre>\n\n<p>And I use it like this:</p>\n\n<pre><code>int main() {\n  int maxNoCells = 10000;   ///&lt; Maximum number of cells (=cell offset).\n  typedef CellAccessor&lt;2&gt; A;\n  Container&lt; A &gt; cellData(maxNoCells);  ///&lt; Allocate cell data.\n  A cells(cellData.data,maxNoCells);  ///&lt; Provides access to cell data.\n\n  for(int i = 0; i &lt; maxNoCells; ++i){\n    cells.flags&lt;A::DELETE&gt;(i) = i%2==0 ? true : false;\n    cells.flags&lt;A::REFINE&gt;(i) = i%2==0 ? false : true;\n    cells.coordinates(i,0) = i;\n    cells.coordinates&lt;1&gt;(i) = -((double)i);\n    cells.order(i) = 2;\n  }\n}\n</code></pre>\n\n<p><strong>Pros:</strong> </p>\n\n<ul>\n<li><p>The data is in block-wise form, which is what I needed.</p></li>\n<li><p>The syntax is ok.</p></li>\n</ul>\n\n<p><strong>Problems:</strong> </p>\n\n<ul>\n<li><p>My classes are doing too much: providing access to the data for the users, providing how much data needs to be stored for the containers, providing how the data should be moved/copied/swaped for my data structures (which are trees...)... </p></li>\n<li><p>I can't use STL algorithms without iterators. I've implemented iterators by making them store the cell index and reimplementing the CellAccessor class inside them (bad! DRY!). </p></li>\n<li><p>Bitset is still being stored in cell-wise form. I could re-implement bitset for my block-wise data structure...</p></li>\n<li><p>data and maxNoCells are static variables, but I could make them normal member variables if required.</p></li>\n</ul>\n\n<p><strong>Question:</strong> is there any efficient way to store \"objects\" (or what we conceptually understand by objects) in block-wise form and access them as if they were stored in a std container such as vector?</p>\n"},{"tags":["asp.net","performance","azure","webforms","azure-web-roles"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":6,"score":0,"question_id":13297986,"title":"How to determine why my asp.net webforms app on azure takes so long to start up?","body":"<p>I have an ASP.NET (webforms) app hosted on windows azure, on an XS sized instance. All is working well except that when I navigate to the site for the first time after not using it for a while it's <em>really</em> slow - like over 45 seconds!! Once it's up and running the pages are nice and fast; it's just the initial load that takes ages. </p>\n\n<p>How can I diagnose where this time is being spent? I already have logging (to the database) within SessionStart event and two requests in different browsers started 10 seconds apart both get logged in the same second, suggesting any logging or other code I add isn't executed until after all the time is spent. Or possibly the database connection is taking a really long time.</p>\n\n<p>I realise there are a few things that are likely to be affecting this, e.g. </p>\n\n<ul>\n<li><p>It's on an XS instance. They're not big.</p></li>\n<li><p>My code is currently built in Debug mode. That makes for bigger assemblies and pdb files that need to be loaded, so it's slower than it could be. </p></li>\n<li><p>Sites just take a little while to load. That's why people <a href=\"http://stackoverflow.com/questions/2479241/does-windows-azure-support-the-application-warm-up-module-or-something-similar/7866140\">change the idleTimeout</a> and possibly the recycling interval.</p></li>\n<li><p>I'm using Telerik controls which are pretty big, weighing in with 32mb of dlls. (But none used on the first page).</p></li>\n</ul>\n\n<p>But I'd like an approach I can use to identify what is taking all the time, e.g. get some measurements on what's happening and how long it's taking, or simply more things I can do to try to make it start up faster.</p>\n\n<p>On my development machine with local database &amp; web the startup time is maybe 5 seconds. <em>Possibly</em> it's just 10x faster than an XS instance, but if that's the case how can I work out what's taking the time on my local machine so I can try to improve it? </p>\n"},{"tags":["java","performance","collections"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":40,"score":4,"question_id":13297333,"title":"Ideal Java Data Structure for Streaming data","body":"<p>I had a specific use case in mind but was not able to figure out the right data structure to use.</p>\n\n<p>I have one thread which keeps streaming objects into a HashMap. Something similar to market data where you have a high and unknown frequency of ticks.</p>\n\n<p>Another thread which constantly reads this map for updated Price objects and queries by key in no particular order. The queries may be multiple times for the same key in a given cycle. The reads and writes are very frequent but the read thread is only interested in the latest available data that is fully updated and doesn't necessarily block till write is complete.</p>\n\n<p>I wanted your thoughts on an ideal data structure for such use cases. Are there better implementations than ConcurrentHashMap that is available?</p>\n\n<p>Thanks</p>\n"},{"tags":["performance","gzip","google-drive-sdk","google-drive"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":8,"score":1,"question_id":13292724,"title":"Use gzip on Google Drive SDK to boost performance","body":"<p>I'm writing a Google Drive client in Java on the Google App Engine. The communication with Google Drive is working, and now I want to boost the performance. Google documentation specifies the use of gzip;</p>\n\n<p><a href=\"https://developers.google.com/drive/performance\" rel=\"nofollow\">https://developers.google.com/drive/performance</a></p>\n\n<p>How of where do i set these two HTTP headers (Accept-Encoding: gzip and User-Agent: my program (gzip)) in my Drive (com.google.api.services.drive.Drive) service?</p>\n"},{"tags":["jquery","performance","dynamic-loading"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":46,"score":0,"question_id":13290934,"title":"Jquery performance: what is better in most browsers, for loading and caching of js and css files?","body":"<p>Since I swapped from prototype to jquery i'm going through a lot of performance issues I never knew existed before.</p>\n\n<p>but that's not the question. The question is about this function i'm using:\n(note we have a huuge web application)\nI'm using this function:</p>\n\n<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;link href=\"http://ajax.googleapis.com/ajax/libs/jqueryui/1/themes/base/jquery-ui.css\" rel=\"stylesheet\" type=\"text/css\" /&gt;\n&lt;script src=\"http://ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js\"&gt;&lt;/script&gt;\n\n\n&lt;script src=\"http://ajax.googleapis.com/ajax/libs/jqueryui/1/jquery-ui.min.js\"&gt;&lt;/script&gt;\n\n\n\n&lt;meta charset=utf-8 /&gt;\n&lt;title&gt;JS Bin&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;div title=\"jquery ui exists\" style=\"width: 500px; height: 500px; background-color: lightblue\"  &gt;hover me&lt;/div&gt;\n  &lt;script&gt;\n  var MyApp ={\n        loadJsNow: function(libFilename){\n                //the synchronous call ensures the library is in fact loaded before this\n          //(which does not work for crossdomain requests, just for docu)\n                //function returns:     \n\n                $.ajax({\n                        url: libFilename,\n                        dataType: \"script\",                     \n                        type: \"GET\",\n                        async: false,\n                        cache: true,\n                        complete: function(data){\n                           $(\"div\").tooltip();\n                        }\n\n                });\n\n\n            }\n    }\n\n\n       MyApp.loadJsNow(\"http://ajax.googleapis.com/ajax/libs/jqueryui/1/jquery-ui.min.js\");\n\n\n  &lt;/script&gt;\n\n\n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n\n<p>So this function is used to load js files if they are required for a certain element of the web page. Since we have so many pages with sometimes little on them, and sometimes loads on them, this approach seemed to make sense. BUT:\nSince this functions loads on demand, and not like standardly in the header, i'm not sure whether this creates a performance problem by itself.\nIn FF 10 i get 200-600 ms\n<a href=\"http://jsbin.com/oxekod/2/edit\" rel=\"nofollow\">see here</a> </p>\n\n<p>Have a look here at the different approach with the hardcoded values in header:</p>\n\n<p><a href=\"http://jsbin.com/oxekod/3/edit\" rel=\"nofollow\">Hardcoded head js links</a>\ni'm getting ~100-300 ms</p>\n\n<p>drop all support for the on demand loading?\ndo you get similar results?</p>\n\n<p><em><strong>EDIT</em></strong> i want to <a href=\"http://stackoverflow.com/a/13293251/533426\">crossreference this question</a>, because it seems relevant since jquery / firefox seems to not deal the caching of the on demand javascript loading correctly. Sometimes it works, then on the same page it does not work again.</p>\n"},{"tags":["performance","list","haskell","ghc","time-complexity"],"answer_count":0,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":66,"score":3,"question_id":13293587,"title":"Haskell: unexpected time-complexity in the computation involving large lists","body":"<p>I am dealing with the computation which has as an intermediate result a list A=[B], which is a list of K lists of the length L. The time-complexity to compute an element of B is controlled by the parameter M and is theoretically linear in M. Theoretically I would expect the time-complexity for the computation of A to be O(K*L*M). However, this is not the case and I don't understand why? </p>\n\n<p>Here is the simple complete sketch program which exhibits the problem I have explained</p>\n\n<pre><code>import System.Random (randoms, mkStdGen)\nimport Control.Parallel.Strategies (parMap, rdeepseq)\nimport Control.DeepSeq (NFData)\nimport Data.List (transpose)\n\ntype Point = (Double, Double)\n\nfmod :: Double -&gt; Double -&gt; Double\nfmod a b | a &lt; 0     = b - fmod (abs a) b \n         | otherwise = if a &lt; b then a \n                       else let q = a / b in b * (q - fromIntegral (floor q))\n\nstandardMap :: Double -&gt; Point -&gt; Point\nstandardMap k (q, p) = (fmod (q + p) (2 * pi), fmod (p + k * sin(q)) (2 * pi))\n\ntrajectory :: (Point -&gt; Point) -&gt; Point -&gt; [Point] \ntrajectory map initial = initial : (trajectory map $ map initial)\n\njustEvery :: Int -&gt; [a] -&gt; [a]\njustEvery n (x:xs) = x : (justEvery n $ drop (n-1) xs)\njustEvery _ []     = []\n\nsubTrace :: Int -&gt; Int -&gt; [a] -&gt; [a]\nsubTrace n m = take (n + 1) . justEvery m\n\nensemble :: Int -&gt; [Point]\nensemble n = let qs = randoms (mkStdGen 42)\n                 ps = randoms (mkStdGen 21)\n             in take n $ zip qs ps \n\nensembleTrace :: NFData a =&gt; (Point -&gt; [Point]) -&gt; (Point -&gt; a) -&gt; \n                              Int -&gt; Int -&gt; [Point] -&gt; [[a]]\nensembleTrace orbitGen observable n m = \n    parMap rdeepseq ((map observable . subTrace n m) . orbitGen)\n\nmain = let k = 100\n           l = 100\n           m = 100\n           orbitGen = trajectory (standardMap 7)\n           observable (p, q) = p^2 - q^2\n           initials = ensemble k\n           mean xs = (sum xs) / (fromIntegral $ length xs)\n           result =   (map mean) \n                    $ transpose \n                    $ ensembleTrace orbitGen observable l m \n                    $ initials\n       in mapM_ print result\n</code></pre>\n\n<p>I am compiling with </p>\n\n<pre><code>$ ghc -O2 stdmap.hs -threaded\n</code></pre>\n\n<p>and runing with </p>\n\n<pre><code>$ ./stdmap +RTS -N4 &gt; /dev/null\n</code></pre>\n\n<p>on the intel Q6600, Linux 3.6.3-1-ARCH, with GHC 7.6.1 and get the following results\nfor the different sets of the parameters K, L, M (k, l, m in the code of the program)</p>\n\n<pre><code>(K=200,L=200,N=200)   -&gt; real    0m0.774s\n                         user    0m2.856s\n                         sys     0m0.147s\n\n(K=2000,L=200,M=200)  -&gt; real    0m7.409s\n                         user    0m28.102s\n                         sys     0m1.080s\n\n(K=200,L=2000,M=200)  -&gt; real    0m7.326s\n                         user    0m27.932s\n                         sys     0m1.020s\n\n(K=200,L=200,M=2000)  -&gt; real    0m10.581s\n                         user    0m38.564s\n                         sys     0m3.376s\n\n(K=20000,L=200,M=200) -&gt; real    4m22.156s\n                         user    7m30.007s\n                         sys     0m40.321s\n\n(K=200,L=20000,M=200) -&gt; real    1m16.222s\n                         user    4m45.891s\n                         sys     0m15.812s\n\n(K=200,L=200,M=20000) -&gt; real    8m15.060s\n                         user    23m10.909s\n                         sys     9m24.450s\n</code></pre>\n\n<p>I don't quite understand where the problem of such a pure scaling might be. If I understand correctly the lists are lazy and should not be constructed, since they are consumed in the head-tail direction? As could be observed from the measurements there is a correlation between the excessive real-time consumption and the excessive system-time consumption as the excess would be on the system account. But if there is some memory management wasting time, this should still scale linearly in K, L, M. </p>\n\n<p>Help!</p>\n"},{"tags":["css","performance","rendering","micro-optimization"],"answer_count":5,"favorite_count":5,"up_vote_count":16,"down_vote_count":0,"view_count":1933,"score":16,"question_id":536091,"title":"Speed of CSS","body":"<p>This is just a question to help me understand CSS rendering better.</p>\n\n<p>Lets say we have a million lines of this.</p>\n\n<pre><code>&lt;div class=\"first\"&gt;\n    &lt;div class=\"second\"&gt;\n        &lt;span class=\"third\"&gt;Hello World&lt;/span&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n</code></pre>\n\n<p>Which would be the fastest way to change the font of Hello World to red?</p>\n\n<pre><code>.third { color: red; }\ndiv.third { color: red; }\ndiv.second div.third { color: red; }\ndiv.first div.second div.third { color: red; }\n</code></pre>\n\n<p>Also, what if there was at tag in the middle that had a unique id of \"foo\". Which one of the CSS methods above would be the fastest.</p>\n\n<p>I know why these methods are used etc, im just trying to grasp better the rendering technique of the browsers and i have no idea how to make a test that times it.</p>\n\n<p>UPDATE: \nNice answer Gumbo.\nFrom the looks of it then it would be quicker in a regular site to do the full definition of a tag. Since it finds the parents and narrows the search for every parent found.</p>\n\n<p>That could be bad in the sense you'd have a pretty large CSS file though.</p>\n"},{"tags":["asp.net","asp.net-mvc-3","performance","iis-7.5"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":8,"score":2,"question_id":13295313,"title":"File upload and result pooling kills ASP.NET web app","body":"<p>We have created <code>ASP.NET MVC app</code> which accept <code>file upload</code>(up to 80mb) and has <code>result pooling implemented by AsincController</code>. Hosted on Windows 2008 R2 IIS7.5 .NET 4. Server 2 Cores 2.6GHZ, 2GB Ram, fast HDD.</p>\n\n<p>The web site has many users and Performance Monitor show <code>ASP.NET Requests/Sec ~15</code> and <code>Request Current ~270</code></p>\n\n<p>After several minutes ASP.NET starts <code>queuing request</code> and <code>ASP.NET Request Queued</code> counter starts growing and application become extremely slow. I am hunting the problem almost a month, tried to profile code, no performance issues and no memory leaks.  Increased <code>maxWorkerThreads to 400</code> and <code>maxIoThreads to 400</code>. Set <code>maxConcurrentRequestsPerCPU to 5000</code> and <code>MaxConcurrentThreadsPerCPU to 0</code> but that either didn't helps.</p>\n\n<p>One thing which seems helps is increasing app pool Maximum Worker Processes to two or three processes and making app pool web garden. After that Request Current jumps to ~350 and no request queuing. But web garden introduce several new issues which we will not mention here. </p>\n\n<p>Please post any suggestions how we could increase application performance without making our app run in IIS pool web garden?</p>\n"},{"tags":[".net","performance","vb6","collections","dictionary"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":1606,"score":2,"question_id":855761,"title":"What is the known performance difference between a Microsoft.VisualBasic.Collection and a .NET System.Collections.Generic.Dictionary(Of TKey, TValue)?","body":"<p>I'm working on a fairly large project for a trading company in Philadelphia. The company utilizes automated trading algorithms which process streaming quotes and send out quotes for hundreds of products dozens of times per second. Obviously, performance is a significant concern. (Which makes me question why we're using VB.NET, but that's another topic entirely.)</p>\n\n<p>I'm relatively new to the company and am working with another guy on some code that's been around for a while. This code utilizes a Microsoft.VisualBasic.Collection object to store all of the products (objects representing pairs of ETFs or stocks and a large amount of data about each) and does a LOT of searching/retrieving from that Collection.</p>\n\n<p>As I understand it, the Collection class is deprecated and pretty much no one uses it anymore. In our more recent code we've been using .NET collections such as List(Of T) and Dictionary(Of TKey, TValue) and from what I understand it might make sense to replace the old Collection with a Dictionary. However, as the source code is quite substantial, going ahead with this replacement would be a significant undertaking; and so my question is just this:</p>\n\n<p>Has anyone actually measured the performance difference between the old Collection and a .NET Dictionary? Is such a comparison, for whatever reason, inappropriate? It certainly seems that everything we are currently doing with the Collection we could do with a Dictionary; basically I just want to know if it makes sense for us to go through the code and make this transition, or if doing so would essentially be a waste.</p>\n\n<p><strong>EDIT</strong>: Originally in the question I referred to the current Collection we are using as a VB6 Collection. After reading the first two answers I realize it is more accurately a Microsoft.VisualBasic.Collection, which appears to be a class introduced for compatibility between VB6 and VB.NET. I think the question still holds.</p>\n\n<p>Based on the first link provided in Kenneth Cochran's answer, I am led to believe that a Dictionary would indeed be better suited to our purposes than a Collection as it performs better at retrieving items by key and running \"For Each\" loops by several milliseconds for 10,000 runs. At our company, this is a realistic scenario; there are lots of places in the code with statements like the following:</p>\n\n<pre><code>Dim ETF as ETFdetails = ETFcoll(sym)\n</code></pre>\n\n<p>And as I said, these lines execute on hundreds of products, many times per second. With this in mind I am inclined to think we really should go ahead and make the change, then measure any performance difference. I expect that we will see at least a mild but noticeable improvement.</p>\n\n<p>Is there anything obviously wrong with what I've just said? If so, point it out!</p>\n"},{"tags":["performance","math","prolog","logic","clpfd"],"answer_count":4,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":262,"score":3,"question_id":10925285,"title":"Faster implementation of verbal arithmetic in Prolog","body":"<p>I already made a working generalized <a href=\"http://en.wikipedia.org/wiki/Verbal_arithmetic\" rel=\"nofollow\">verbal arithmetic</a> solver in Prolog but it's too slow. It takes 8 minutes just to run the simple expression S E N D + M O R E = M O N E Y. Can someone help me to make it run faster?</p>\n\n<pre><code>/* verbalArithmetic(List,Word1,Word2,Word3) where List is the list of all \npossible letters in the words. The SEND+MORE = MONEY expression would then\nbe represented as\n  verbalArithmetic([S,E,N,D,M,O,R,Y],[S,E,N,D],[M,O,R,E],[M,O,N,E,Y]). */\n\nvalidDigit(X) :- member(X,[0,1,2,3,4,5,6,7,8,9]).\nvalidStart(X) :- member(X,[1,2,3,4,5,6,7,8,9]).\nassign([H|[]]) :- validDigit(H).         \nassign([H|Tail]) :- validDigit(H), assign(Tail), fd_all_different([H|Tail]).\n\nfindTail(List,H,T) :- append(H,[T],List).\n\nconvert([T],T) :- validDigit(T).\nconvert(List,Num) :- findTail(List,H,T), convert(H,HDigit), Num is (HDigit*10+T).\n\nverbalArithmetic(WordList,[H1|Tail1],[H2|Tail2],Word3) :- \n    validStart(H1), validStart(H2), assign(WordList), \n    convert([H1|Tail1],Num1),convert([H2|Tail2],Num2), convert(Word3,Num3), \n    Sum is Num1+Num2, Num3 = Sum.\n</code></pre>\n"},{"tags":["c#","wpf","performance","filter","iteration"],"answer_count":6,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":1342,"score":2,"question_id":2771344,"title":"Is there a way to increase performance on my simple textfilter?","body":"<p>I'm writing a filter that will pick out items. I have a list of Objects. The objects contain a number, name and some other irrelevant items. At the moment, the list contains 200 items. When typing in a <code>textbox</code>, i'm looking if the string matches a part of the number/name of the objects in the list. If so, add them to the <code>listbox</code>. Here's the code for my textbox textchanged event : </p>\n\n<pre><code>private void txtTelnumber_TextChanged(object sender, TextChangedEventArgs e)\n    {\n        lstOverview.Items.Clear();\n        string data = \"\";\n        foreach (ucTelListItem telList in _allUsers)\n        {\n            data = telList.User.H323 + telList.user.E164;\n            if (data.Contains(txtTelnumber.Text))\n                lstOverview.Items.Add(telList);\n        }\n    }\n</code></pre>\n\n<p>I sometimes see a little delay when entering a character, especially when i go from 4 records to 200 records (so when i had a filter and 4 records matched, and i backspace and the whole list appears again).\nMy list is a list of usercontrols, cause I found it takes less time to load the usercontrols from a list, then to have to initialize a new usercontrol each time.</p>\n\n<p>Can I do something about the code, or is it just adding the <code>usercontrol</code> the <code>listbox</code> that causes the small delay (small delay = &lt;1 sec)?</p>\n\n<p>Thanks in advance.</p>\n\n<p><em>Edit</em>\nI've edited post, it's wpf. And putting items in a list and setting the itemssource isn't solving the problem.</p>\n"},{"tags":["c#","performance","sortedlist"],"answer_count":3,"favorite_count":0,"up_vote_count":9,"down_vote_count":0,"view_count":112,"score":9,"question_id":13292945,"title":"C# fast way to check and then insert into a sortedlist","body":"<p>Whenever I want to insert into a <code>SortedList</code>, I check to see if the item exists, then I insert. Is this performing the same search twice? Once to see if the item is there and again to find where to insert the item? Is there a way to optimize this to speed it up or is this just the way to do it, no changes necessary?</p>\n\n<pre><code>if( sortedList.ContainsKey( foo ) == false ){\n    sortedList.Add( foo, 0 );\n}\n</code></pre>\n"},{"tags":["performance","drupal","publishing"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":2,"score":0,"question_id":13292919,"title":"drupal measure author's publication efficiensy drupal 7","body":"<p>I have made a drupal 7 site.</p>\n\n<p>I'm looking for a solution for measuring the authors (blog writers, article writers) efficiensy.\nI would like to see someweher, how many nodes are created by a user (author) per month and how many letters do these nodes contain.</p>\n\n<p>Can you suggest me a modul or something else?</p>\n\n<p>Thanks,\nTamás</p>\n"},{"tags":["python","django","performance","django-templates","many-to-many"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":1558,"score":3,"question_id":1122605,"title":"Django ManyToMany Template rendering and performance issues","body":"<p>I've got a django model that contains a manytomany relationship, of the type,</p>\n\n<pre><code>class MyModel(models.Model):\n  name = ..\n  refby = models.ManyToManyField(MyModel2)\n  ..\n\nclass MyModel2(..):\n  name = ..\n  date = ..\n</code></pre>\n\n<p>I need to render it in my template such that I am able to render all the mymodel2 objects that refer to mymodel. Currently, I do something like the following,</p>\n\n<pre><code>{% for i in mymodel_obj_list %}\n  {{i.name}}\n  {% for m in i.refby.all|dictsortreversed:\"date\"|slice:\"3\" %}\n    {{.. }}\n  {% endfor %}\n  &lt;div&gt; &lt;!--This div toggles hidden/visible, shows next 12--&gt;\n   {% for n in i.refby.all|dictsortreversed:\"date\"|slice:\"3:15\" %}\n     {{.. }}\n   {% endfor %}\n  &lt;/div&gt;\n{% endfor %}\n</code></pre>\n\n<p>As the code suggests, I only want to show the latest 3 mymodel2 objects, sorted in reverse order by date, although the next 12 do get loaded.</p>\n\n<p>Is this a very inefficient method of doing so? (Given that results for the refby.all could be a few 100s, and the total no of results in \"mymodel_obj_list\" is also in 100s - I use a paginator there).</p>\n\n<p>In which case, whats the best method to pre-compute these refby's and render them to the template? Should I do the sorting and computation in the view, and then pass it? I wasn't sure how to do this in order to maintain my pagination.</p>\n\n<p>View code looks something like,</p>\n\n<pre><code>obj_list = Table.objects.filter(..) # Few 100 records\npl = CustomPaginatorClass(obj_list...)\n</code></pre>\n\n<p>And I pass the pl to the page as mymodel_obj_list.</p>\n\n<p>Thanks!</p>\n"},{"tags":["database","performance","postgresql","postgresql-performance"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":100,"score":2,"question_id":13088407,"title":"Postgresql: inner join takes 70 seconds","body":"<p>I have two tables - </p>\n\n<p>Table A : 1MM rows, \nAsOfDate, Id, BId (foreign key to table B)</p>\n\n<p>Table B : 50k rows,\nId, Flag, ValidFrom, ValidTo</p>\n\n<p>Table A contains multiple records per day between 2011/01/01 and 2011/12/31 across 100 BId's.\nTable B contains multiple non overlapping (between validfrom and validto) records for 100 Bids. </p>\n\n<p>The task of the join will be to return the flag that was active for the BId on the given AsOfDate.</p>\n\n<pre><code>select \n    a.AsOfDate, b.Flag \nfrom \n    A a inner Join B b on \n        a.BId = b.BId and b.ValidFrom &lt;= a.AsOfDate and b.ValidTo &gt;= a.AsOfDate\nwhere\n    a.AsOfDate &gt;= 20110101 and a.AsOfDate &lt;= 20111231\n</code></pre>\n\n<p>This query takes ~70 seconds on a very high end server (+3Ghz) with 64Gb of memory.</p>\n\n<p>I have indexes on every combination of field as I'm testing this - to no avail.</p>\n\n<p>Indexes : a.AsOfDate, a.AsOfDate+a.bId, a.bid\nIndexes : b.bid, b.bid+b.validfrom</p>\n\n<p>Also tried the range queries suggested below (62seconds)</p>\n\n<p>This same query on the free version of Sql Server running in a VM takes ~1 second to complete.</p>\n\n<p>any ideas?</p>\n\n<p>Postgres 9.2</p>\n\n<p>Query Plan</p>\n\n<pre><code>QUERY PLAN                                       \n---------------------------------------------------------------------------------------\nAggregate  (cost=8274298.83..8274298.84 rows=1 width=0)\n-&gt;  Hash Join  (cost=1692.25..8137039.36 rows=54903787 width=0)\n    Hash Cond: (a.bid = b.bid)\n     Join Filter: ((b.validfrom &lt;= a.asofdate) AND (b.validto &gt;= a.asofdate))\n     -&gt;  Seq Scan on \"A\" a  (cost=0.00..37727.00 rows=986467 width=12)\n           Filter: ((asofdate &gt; 20110101) AND (asofdate &lt; 20111231))\n     -&gt;  Hash  (cost=821.00..821.00 rows=50100 width=12)\n           -&gt;  Seq Scan on \"B\" b  (cost=0.00..821.00 rows=50100 width=12)\n</code></pre>\n\n<p>see <a href=\"http://explain.depesz.com/s/1c5\" rel=\"nofollow\">http://explain.depesz.com/s/1c5</a> for the analyze output </p>\n\n<p><img src=\"http://i.stack.imgur.com/2CWWU.png\" alt=\"here is the query plan from sqlserver for the same query\"></p>\n"},{"tags":["performance","oracle","jpa","eclipselink","jpql"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":113,"score":0,"question_id":12641891,"title":"Fetch dates (by month or year) stored in TIMESTAMP using JPA","body":"<p>I am facing a problem and I would like you to help me.</p>\n\n<p>It turns out I have one table in my Oracle 11g database where I store failures of one electronic device. The table definition is following:</p>\n\n<pre><code>CREATE TABLE failure\n( failure_id NUMERIC NOT NULL\n, fecha TIMESTAMP NOT NULL\n, module_id NUMERIC NOT NULL\n, code NUMERIC\n, PRIMARY KEY(failure_id)\n);\n</code></pre>\n\n<p><em>Where 'fecha' means 'date'.</em></p>\n\n<p>I need to fetch failures by YEAR or by MONTH for one specific module but I can't. My ORM maps the TIMESTAMP type to java.sql.Date but I don't know how to compare the month in the JPQL sentence. I have tried to use ORACLE functions with native queries but I front with another issue: to cast the results.\nI am using JPA 2.0 with Eclipselink 2.3.2.</p>\n\n<p><strong>My doubts are:</strong></p>\n\n<p>Can I use Oracle functions with this version of Eclipselink library? My experience say no.</p>\n\n<pre><code>Query query = entityManager.createQuery(\"SELECT f FROM Failure f \"\n            + \"WHERE EXTRACT(YEAR FROM f.fecha) = ?1 \"\n            + \"AND f.moduleId.moduleId = ?2\");\n    query.setParameter(1, year);\n    query.setParameter(2, idModule);\n</code></pre>\n\n<p>I get this error: <code>Unexpected token [(]</code></p>\n\n<p>Can I use Eclipselink functions? My experience say no.</p>\n\n<pre><code>Query query = entityManager.createQuery(\"SELECT f FROM Failure f \"\n            + \"WHERE EXTRACT('YEAR', f.fecha) = ?1 \"\n            + \"AND f.moduleId.moduleId = ?2\");\n    query.setParameter(1, year);\n    query.setParameter(2, idModule);\n</code></pre>\n\n<p>Same error.</p>\n\n<p>Do you know a simple way to fetch this data using only one query?\nI know I can fetch one module and then check failures with loops but I think it is not the best performing solution.</p>\n\n<p>Thanks.</p>\n\n<p><strong>My sources:</strong></p>\n\n<ul>\n<li>Eclipselink JPA functions <a href=\"http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Basic_JPA_Development/Querying/Support_for_Native_Database_Functions#Functions\" rel=\"nofollow\">link</a></li>\n<li>Eclipselink Query Enhancements <a href=\"http://wiki.eclipse.org/EclipseLink/Release/2.1.0/JPAQueryEnhancements\" rel=\"nofollow\">link</a></li>\n</ul>\n"},{"tags":["eclipse","performance","eclipse-wtp"],"answer_count":7,"favorite_count":3,"up_vote_count":13,"down_vote_count":0,"view_count":2392,"score":13,"question_id":11639108,"title":"Eclipse Juno + WTP +EGit dead slow","body":"<p>I'm trying to use Eclipse Juno (Version: 4.2.0 Build id: I20120608-1400) with WTP for JavaScript/Node.js development on MacOSX Lion, on my 4GB RAM MacBook Pro. Sometimes it gets dead slow, and unusable. I've tried tweaking both through preferences (disabled all validators) and initialization variables, and also upgraded to 1.7 VM as recommended. Here's the contents of my eclipse.ini file:</p>\n\n<pre><code>-startup\n../../../plugins/org.eclipse.equinox.launcher_1.3.0.v20120522-1813.jar\n--launcher.library\n../../../plugins/org.eclipse.equinox.launcher.cocoa.macosx.x86_64_1.1.200.v20120522-1813\n-showsplash\norg.eclipse.platform\n--launcher.XXMaxPermSize\n256m\n--launcher.defaultAction\nopenFile\n-vm\n/Library/Java/JavaVirtualMachines/1.7.0.jdk/Contents/Home/bin/java\n-vmargs\n-Xms64m\n-Xmx256m\n-Xdock:icon=../Resources/Eclipse.icns\n-XstartOnFirstThread\n-Dorg.eclipse.swt.internal.carbon.smallFonts\n-XX:+UseParallelGC\n-XX:+UseCompressedOops\n-XX:MaxPermSize=256m\n-server\n</code></pre>\n\n<p>Can someone advise what I could do to improve WTP performance?</p>\n"},{"tags":["python","database","django","performance"],"answer_count":5,"favorite_count":3,"up_vote_count":9,"down_vote_count":0,"view_count":3061,"score":9,"question_id":447117,"title":"Django: Increment blog entry view count by one. Is this efficient?","body":"<p>I have the following code in my index view.</p>\n\n<pre><code>latest_entry_list = Entry.objects.filter(is_published=True).order_by('-date_published')[:10]\nfor entry in latest_entry_list:\n    entry.views = entry.views + 1\n    entry.save()\n</code></pre>\n\n<p>If there are ten (the limit) rows returned from the initial query, will the save issue 10 seperate updated calls to the database, or is Django \"smart\" enough to issue just one update call?</p>\n\n<p>Is there a more efficient method to achieve this result?</p>\n"},{"tags":["wcf","performance","windows-authentication"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":5,"score":0,"question_id":13290970,"title":"How to improve WCF Windows Authentication performance for basicHttpBinding?","body":"<p>I'm currently working on an client-server application which is based on .net remoting with tcp as transport channel.\nFor Authentication we are using windows authentication (kerberos).</p>\n\n<p>Now we decided to invest into a more scalable approach, and refactor the service to run on multiple servers (active-active) with a loadbalancer in front of them.\nAs for this scenario Http is much more suitable than TCP, we decided to replace the .net remoting interface with WCF and switch from TCP to HTTP.</p>\n\n<p>The problem though is that my clients are spread over the world, and sometimes with rather low bandwidth. Now I did some protoypes already, which showed me that basicHttp is about 4-5 times slower than TCP. Which is actually too slow for my requirements. 2x I would accept.\nBased on that I did some investigation and measures. What I found is that, if I remove windows authentication from the binding I reach my goal of about 2x slower than tcp. Which means that 1/2 of the total call time is consumed by the authentication.</p>\n\n<p>Now the 1000$-question: Any ideas how to stay with HTTP and still have some windows authentication? Any settings, hooks, ... I could try?</p>\n\n<p>any ideas welcome,\nthx,\nMartin</p>\n"},{"tags":["performance"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":10,"score":0,"question_id":13287695,"title":"How to count the number of arithmetic operations in the following expression?","body":"<p>exp(-(intA-FloatB)^2/(2*(FloatC^2)))\nNote that the exponential is not complex!</p>\n"},{"tags":["java","performance","static"],"answer_count":7,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":79,"score":1,"question_id":13288823,"title":"performance of static vs non static method for an utility class","body":"<p>I have an utility class which has non static methods with no instance variables. So I am thinking of converting all the methods to <code>static</code> methods. I think there will be no memory or performance impacts. But I just wanted to confirm. </p>\n\n<p>Will changing such a method to be a <code>static</code> have any performance impact on the program?</p>\n"},{"tags":["php","performance","design","optimization"],"answer_count":8,"favorite_count":5,"up_vote_count":11,"down_vote_count":0,"view_count":1451,"score":11,"question_id":3470990,"title":"Is micro-optimization worth the time?","body":"<p>I am a PHP developer and I have always thought that micro-optimizations are not worth the time. If you really need that extra performance, you would either write your software so that it's architecturally faster, or you write a C++ extension to handle slow tasks (or better yet, compile the code using HipHop). However, today a work mate told me that there is a big difference in </p>\n\n<pre><code>is_array($array)\n</code></pre>\n\n<p>and</p>\n\n<pre><code>$array === (array) $array\n</code></pre>\n\n<p>and I was like \"eh, that's a pointless comparison really\", but he wouldn't agree with me.. and he is the best developer in our company and is taking charge of a website that does about 50 million SQL queries per day -- for instance. So, I am wondering here is that could he be wrong or micro-optimization really worth the time and when?</p>\n"},{"tags":["php","mysql","performance","indexing"],"answer_count":1,"favorite_count":2,"up_vote_count":3,"down_vote_count":1,"view_count":36,"score":2,"question_id":13289509,"title":"How to improve the query performance using indexing?","body":"<pre><code>SELECT t1 . * FROM table1 t1, table1 t2\n   WHERE t1.history_id &lt; t2.history_id\n     AND (t1.license_id = t2.license_id\n        OR (t1.license_id IS NULL AND t2.license_id IS NULL))\n     AND t1.op_id = t2.op_id\n     AND (t1.service_date = t2.service_date\n         OR (t1.service_date IS NULL AND t2.service_date IS NULL))\n     AND t1.customer_id = t2.customer_id\n     AND t1.serial_id = t2.serial_id.\n</code></pre>\n\n<p>The purpose of the query is to remove duplicated rows based on the above query conditions.The query join table 'table1' to itself.We have created index with group index for</p>\n\n<pre><code>  1.license_id\n  2.service_date\n  3.customer_id\n  4.history_id(primary key)\n  5.op_id.\n</code></pre>\n\n<p>It executes correctly but with the addition of ' <code>OR (t1.service_date IS NULL AND t2.service_date IS NULL)</code>' makes the query execution very slower.The table has more than <code>2lacks</code> of data.\nWe have used mysql EXPLAIN and here is the output</p>\n\n<p><img src=\"http://i.stack.imgur.com/FFNNA.png\" alt=\"enter image description here\"></p>\n\n<p>Please suggest how can I improve the query execution time?</p>\n"},{"tags":["mysql","performance","table","join","big"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":31,"score":0,"question_id":13289072,"title":"mysql join: what is faster?","body":"<p>My tables: big_table</p>\n\n<pre>+-----------------+--------------+------+-----+---------+----------------+\n| Field           | Type         | Null | Key | Default | Extra          |\n+-----------------+--------------+------+-----+---------+----------------+\n| id              | mediumint(7) | NO   | PRI | NULL    | auto_increment |\n| title           | varchar(255) | NO   |     | NULL    |                |\n| category_id     | tinyint(2)   | NO   |     | NULL    |                |\n| sub_category_id | tinyint(2)   | NO   |     | NULL    |                |\n| width           | smallint(5)  | NO   |     | NULL    |                |\n| height          | smallint(5)  | NO   |     | NULL    |                |\n| ratio_width     | smallint(5)  | NO   |     | NULL    |                |\n| ratio_height    | smallint(5)  | NO   |     | NULL    |                |\n| size            | int(8)       | NO   |     | NULL    |                |\n| mime            | tinyint(2)   | NO   |     | NULL    |                |\n| views           | mediumint(7) | NO   | MUL | NULL    |                |\n| time            | int(10)      | NO   |     | NULL    |                |\n| file            | varchar(255) | NO   |     | NULL    |                |\n+-----------------+--------------+------+-----+---------+----------------+\n</pre>\n\n<p>small_table</p>\n\n<pre>\n+--------+--------------+------+-----+---------+-------+\n| Field  | Type         | Null | Key | Default | Extra |\n+--------+--------------+------+-----+---------+-------+\n| id     | mediumint(7) | NO   | PRI | NULL    |       |\n| width  | smallint(5)  | NO   | MUL | NULL    |       |\n| height | smallint(5)  | NO   | MUL | NULL    |       |\n+--------+--------------+------+-----+---------+-------+\n</pre>\n\n<p>so what's faster (example):</p>\n\n<pre><code>   SELECT * FROM `big_table` WHERE `width` =1920 AND `height`=1080;  \n</code></pre>\n\n<p>or use join </p>\n\n<pre><code> select big_table.*\n from small_table_ratio\n left join small_table_ratio small_table_ratio2\n ON (small_table_ratio.id=small_table_ratio2.id\n     and `small_table_ratio`.`height` = '1080')\n left join big_table\n ON (big_table.id=small_table_ratio.id)\n where small_table_ratio.width = '1920'; \n</code></pre>\n\n<p>or join's from the same table?</p>\n\n<pre><code>select big_table.*\nfrom big_table as big_table1\nleft join big_table big_table2\nON (big_table1.id=big_table2.id and  `big_table1`.`height` = '1080')\nleft join big_table \nON (big_table.id=big_table1.id)\nwhere big_table1.width = '1920';\n</code></pre>\n\n<p>or there is some better solution, better select's?\n (on both tables I can use indexes (width and height), but only ID is unique)</p>\n"},{"tags":["database","performance","database-design","normalization","denormalization"],"answer_count":7,"favorite_count":7,"up_vote_count":11,"down_vote_count":0,"view_count":4123,"score":11,"question_id":2349270,"title":"In what way does denormalization improve database performance?","body":"<p>I heard a lot about denormalization which was made to improve performance of certain application. But I've never tried to do anything related.</p>\n\n<p>So, I'm just curious, which places in normalized DB makes performance worse or in other words, what are denormalization principles?</p>\n\n<p>How can I use this technique if I need to improve performance?   </p>\n"},{"tags":["java","performance","excel","apache-poi"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":8,"score":0,"question_id":13288113,"title":"Memory issues during conversion of large volume of XLSX file to CSV with POI","body":"<p>This is a very challenging task to me as i am doing pretty much R&amp;D to get rid of OutOfMemroyError during conversion of XLSX to CSV and my excel file can have three sheets and each sheet with 60000 rows. \nI used XSSF and SAX (Event API) recently since this approach consumes very less memory. However the Event API is triggering events only for things actually stored within the file and this can be cause for me. </p>\n\n<p>Earlier to this Event API approach, i used Workbook class to process XLSX file and eventually i am getting out of memory during this workbook creation provided below.  </p>\n\n<blockquote>\n  <p>Workbook workbook = WorkbookFactory.create(new File(\"myfile.xlsx\"));</p>\n</blockquote>\n\n<p>so, what is the best way to process large volume of XLSX data with apache POI?</p>\n"},{"tags":["performance","unix","networking","connect","telnet"],"answer_count":1,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":53,"score":3,"question_id":13126235,"title":"force connect to work","body":"<p>I run the following line in bash, on a VM running red hat 5:</p>\n\n<pre><code>    for i in {1..100000};\n        do telnet 10.10.10.105 41941;\n    done\n</code></pre>\n\n<p>At some point, telnet connects to the port even though there is no one listening on it. It seems o be connecting to its self?\nThe same issue appears when i start the client side of an application, <strong>without</strong> starting the server - the client successfully connects to the ip:port. The client looks something like this: </p>\n\n<pre><code>    addr.sin_family = AF_INET;\n    addr.sin_port = htons(atoi(port));\n    addr.sin_addr.s_addr = inet_addr(hostname);\n\n    some_while_loop\n    {\n        status = ::connect(sock, (sockaddr *)&amp;addr, sizeof(addr));\n        if (status == -1)\n        {\n            shutdown(sock, 2);\n            close(sock);\n            return false;\n        }\n   }\n</code></pre>\n\n<p>I found this article: <a href=\"http://web.deu.edu.tr/doc/soket/\" rel=\"nofollow\">http://web.deu.edu.tr/doc/soket/</a> which states in 6.2 that the connection will succeed if you to the same machine you're running on. My question is, why is this happening? Is it a hardware issue or is it a fail-safe red-hat kernel is using, or maby it's because of the port i'm using (for 1025 for example, i don't have this issue)... ?</p>\n"},{"tags":["java","performance","web-services","spring","http"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":16,"score":0,"question_id":13286743,"title":"Measure Spring RestTemplate HTTP request time","body":"<p>I want to measure the time of the HTTP GET request of a <code>RestTemplate.getForObject</code> call without the the time needed for parsing the response. So just the time the remote HTTP call needs. I already tried setting a <code>ClientHttpRequestInterceptor</code> but I dont think this is the right way to do it as the time seems to be wrong:</p>\n\n<pre><code>public class PerfRequestSyncInterceptor implements ClientHttpRequestInterceptor {\nprivate Logger log = Logger.getLogger(this.getClass());\n\n@Override\n  public ClientHttpResponse intercept(HttpRequest request, byte[] body,\n        ClientHttpRequestExecution execution) throws IOException {\n\n    long start = System.nanoTime();\n    ClientHttpResponse resp = execution.execute(request, body);\n\n    log.debug(\"remote request time: \"\n            + ((System.nanoTime() - start) * Math.pow(10, -9)));\n    return resp;\n  }\n}\n</code></pre>\n\n<p><br>Call:</p>\n\n<pre><code>RestTemplate rest = new RestTemplate();\nList&lt;ClientHttpRequestInterceptor&gt; interceptors = new ArrayList&lt;ClientHttpRequestInterceptor&gt;();\ninterceptors.add(new PerfRequestSyncInterceptor());\nrest.setInterceptors(interceptors);\n\nResponse inob = rest.getForObject(xmlURL, Response.class);\n</code></pre>\n\n<p>How can I measure the time of a RestTemplate HTTP request?</p>\n"},{"tags":["php","javascript","performance","html5"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":61,"score":0,"question_id":13205729,"title":"Optimum way to insert external html content HTML5 compliant? php vs HTML5 vs javascript methods","body":"<p>I know how simple this probably seems to you gurus, but I have been searching for over an hour to no avail...</p>\n\n<p>Goal:  Use a single footer file and menu file for all my webpages.  Taking into account blocking, speed, etc.  The content of my menu is pure html/css and the content of my footer is pure html/css.  Would the optimal solution change based on the content being injected?  e.g.  If videos, jscript, etc. were involved.</p>\n\n<p>Two part question:\n1)  Which method is optimal?  Some kind of php include, using the  tag, using jscript, etc.\n2)  How precisely is this achieved keeping HTML 5 standards?  i.e.  For the php method to work, does my calling webpage need to be .php and then does that make the HTML5 standard a moot point?  e.g.  If I want to inject footer.php into index.html, does my index file also have to be .php?  Similarly for the  tag, can the external file be an .html file(I don't like the idea of reloading all the header information with .css calls) or should it be .php?  </p>\n\n<p>Within the index.html file I have tried the following:</p>\n\n<pre><code>&lt;object id=\"footerArea\" width=\"100%\" height=\"20%\" \n  type=\"text/html\" data=\"footer.html\"&gt;\n&lt;/object&gt;\n</code></pre>\n\n<p>and</p>\n\n<pre><code>&lt;?php include 'footer.php' ?&gt;\n</code></pre>\n\n<p>Neither of these seem to work for me.</p>\n\n<p>In case you are wondering...  Here is the code for my footer I am trying to inject with sample data to make it shorter and easier to read:</p>\n\n<pre><code>&lt;div class=\"footer box\"&gt;\n&lt;p class=\"f-right t-right\"&gt;\n &lt;a href=\"#\"&gt;www.mysite.com&lt;/a&gt;&lt;br /&gt;\n  Address: Medford, OR&lt;br /&gt;\n  Phone: (541) 555-5555\n&lt;/p&gt;\n\n&lt;p class=\"f-left\"&gt;\n Copyright &amp;copy;&amp;nbsp;2011 &lt;a href=\"#\"&gt;My Name&lt;/a&gt;&lt;br /&gt;\n&lt;/p&gt;\n\n&lt;p class=\"f-left\" style=\"margin-left:20px;\"&gt;\n &lt;a href=\"http://sampleurl.com\" target=\"_blank\"&gt;\n  &lt;img style=\"border:0;width:88px;height:31px\"\n   src=\"http://sampleurl.com\"\n   alt=\"Valid CSS3!\" /&gt;\n &lt;/a&gt;\n&lt;/p&gt;\n\n&lt;p class=\"f-left\" style=\"margin-left:20px;\"&gt;\n &lt;a href=\"http://sampleurl\" target=\"_blank\"&gt;\n &lt;img src=\"http://sample.png\" width=\"228\" height=\"50\" alt=\"sample alt\" title=\"sample title\"&gt;\n &lt;/a&gt;\n&lt;/p&gt;\n&lt;/div&gt;\n</code></pre>\n\n<p>Please excuse my formatting.  I am still new to posting code in forums.  I tried my best :)</p>\n"},{"tags":["asp.net","visual-studio-2010","performance","ram","workstation"],"answer_count":1,"favorite_count":2,"up_vote_count":1,"down_vote_count":0,"view_count":31,"score":1,"question_id":13286312,"title":"What should I do with all this RAM ...?","body":"<p>The powers-that-be have decided to bestow memory upgrades on our developer team. We're all now in control of Mac Pro's with <strong>32GB</strong> of RAM. I would have preferred an SSD instead of half of that RAM and I'm struggling to think of ways to make the most of it all. To date I have installed the x64 version of Windows 7 and also set up a 4GB RAM drive for temp files, browser cache etc. as well as codefiles for the various apps I'm working on.</p>\n\n<p>Despite this, even in the middle of a heavy-duty debug session with a massively multi-project solution I always seem to have what to me as obscene amounts of free memory left and I was wondering if there was anything else I could do to make the most of the available RAM. The only other thing I could think of was to run a virtual Windows server on my workstation for 'proper' (i.e. in a mirror of our production environment) local deployment/testing and so on, but any tools or tricks that could put the 4-6GB to good use in any developer or user-friendly ways would be very welcome.</p>\n\n<p>I work with ASP.Net and SQL Server and use VS2010/12 so any 'tricks' specific for this set-up are especially welcome. I was saddened to see that all that RAM has not made VS2010 any less prone to fits of unresponsiveness.</p>\n\n<p>Thanks for reading.</p>\n"},{"tags":["sql-server","performance","stored-procedures"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":124,"score":1,"question_id":2623425,"title":"SQL Server: how can I know my stored procedure is optimum in performance","body":"<p>I would like to know how to measure/know that one's stored procedure is optimum in performance. That stored procedure can be inserting/deleting/updating or manipulating data in that procedure. Please describe the way/approach how to know performance in SQL Server. </p>\n\n<p>Thanks in advance!</p>\n"},{"tags":["c#","performance",".net-3.5"],"answer_count":6,"favorite_count":0,"up_vote_count":6,"down_vote_count":0,"view_count":125,"score":6,"question_id":13283654,"title":"How to optimize large size for loop","body":"<p>I have a for loop with more than 20k iterations,for each iteration it is taking around two or three seconds and total around 20minutes.  how i can optimize this for loop. I am using .net3.5 so parallel foreach is not possible. so i splited the 200000 nos into small chunks and implemented some threading now i am able reduce the time by 50%. is there any other way to optimize these kind of for loops.</p>\n\n<p>My sample code is given below</p>\n\n<pre><code>    static double sum=0.0;\n    public double AsyncTest()\n    {\n            List&lt;Item&gt; ItemsList = GetItem();//around 20k items\n            int count = 0;\n            bool flag = true;\n            var newItemsList = ItemsList.Take(62).ToList();\n            while (flag)\n            {\n                int j=0;\n                WaitHandle[] waitHandles = new WaitHandle[62];\n                foreach (Item item in newItemsList)\n                {\n                    var delegateInstance = new MyDelegate(MyMethod);\n                    IAsyncResult asyncResult = delegateInstance.BeginInvoke(item.id, new AsyncCallback(MyAsyncResults), null);\n                    waitHandles[j] = asyncResult.AsyncWaitHandle;\n                    j++;\n                }\n                WaitHandle.WaitAll(waitHandles);\n                count = count + 62;\n                newItemsList = ItemsList.Skip(count).Take(62).ToList();  \n            }\n            return sum;\n    }\n\n    public double MyMethod(int id)\n    {\n        //Calculations\n        return sum;\n    }\n\n    static public void MyAsyncResults(IAsyncResult iResult)\n    {\n        AsyncResult asyncResult = (AsyncResult) iResult;\n        MyDelegate del = (MyDelegate) asyncResult.AsyncDelegate;\n        double mySum = del.EndInvoke(iResult);\n        sum = sum + mySum;\n    }\n</code></pre>\n"},{"tags":["asp.net","performance","iis7","cpu-usage"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":43,"score":2,"question_id":13249718,"title":"Server CPU stays on 100% when e-commerce site runs","body":"<p>I have developed an e-commerce website on .NET umbraco. After deploying it i have made a load test with 100 users (<a href=\"http://loadimpact.com/\" rel=\"nofollow\">loadimapct</a>).When ever more than 30 users tried to access the site,it freezes and CPU usage stays at 100% till the test is over. The site is accessing an indexing engine (Fact-Finder) internally which is deployed on a Tomcat Server.\nHere is a snap shot of the condition.</p>\n\n<p><a href=\"http://my.jetscreenshot.com/7391/20121106-xhtd-222kb.jpg\" rel=\"nofollow\">Snapshot</a></p>\n\n<p>I have checked the site with profilers but there is no heavy process running that i can see.</p>\n\n<p>I have made a check with DebugDiag tool (<a href=\"http://www.iis.net/learn/troubleshoot/performance-issues/troubleshooting-high-cpu-in-an-iis-7x-application-pool\" rel=\"nofollow\">Troubleshooting High CPU in an IIS 7.x Application Pool </a>).It indicates high CPU usage but I can't understand which page or resource from my website makes this,So i have attached a copy of report.\n<a href=\"http://www.streamfile.com/myid/nIDcoYgd3tW9\" rel=\"nofollow\">Detail Report Link</a></p>\n\n<p>can any one Help on this ?</p>\n"},{"tags":["javascript","jquery","performance","jquery-selectors"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":119,"score":2,"question_id":3319556,"title":"jQuery selector performance","body":"<p>Why is id selector faster than class selector in jQuery?</p>\n"},{"tags":["ios","performance","uitableview","uitableviewcell","sdk"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":33,"score":0,"question_id":13284430,"title":"True or False. Is it possible to change UITableViewCell height, which was dequeued in GetCell?","body":"<p>There are many talks near the theme.</p>\n\n<p>Imagine that <code>UITableView</code> shows heavy <code>UITableViewCells</code>. Scrolling speed is not fast. So reusing previously created cells could improve it.</p>\n\n<p>Unfortunately, height of each row depends on it's content. For example, dequeued cell #1 with height 100 should update it's height to 150 when it will be used for cell #6 (simple scrolling down).</p>\n\n<p>Is it possible to change <code>UITableViewCell</code> height, which was dequeued in GetCell?</p>\n\n<p>Please note that calling <code>ReloadRow</code> method is equals to creating cell from scratch. It's heavy.</p>\n\n<p>P.S.: I already know optimization techniques like using one <code>ContentView</code> for all and drawing content by hand.</p>\n"},{"tags":["performance","visual-c++","opencv","cpu","mingw32"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":65,"score":0,"question_id":13241691,"title":"Slow CPU + Windows Explorer + Everything when programming in C++ using MinGW compiler and Code::Blocks IDE","body":"<p>When I was working with OpenCV, I added the build/install folder (i.e. where I had my mingw32-make and mingw32-install files created) to PATH variable. However, this made my PC quite slow after I did a few builds on my CodeBlocks IDE. I didn't forget to deallocate memory or anything like that which might cause trouble. Even a simple hello world program takes ages to run after a few builds. I am using i7 with 8GB of RAM and L3 cache which, I believe, is good enough for any development. After I removed the OpenCV directory from my PATH, it was okay, but not permanently. What could the problem be?</p>\n\n<p>I also checked my environment variables and there is no garbage variable in there! I only have MinGW32 compiler i.e. the bin folder in PATH. But it is necessary (I think!).</p>\n\n<p>Surprisingly, I have tried the same with Microsoft Visual C++ 2010 Express and when I try to build the openCV libraries, it really slows the PC down. For example, if I am trying to open the File explorer on Windows, it takes like 15 seconds for a window to pop up. If I try to open a web browser window, it takes about same time. And I have been monitoring my task manager for weird activities, but nothing! I don't know if it has something to do with Windows 7 64-bit OS.</p>\n\n<p>I am not sure if it has something to do with CodeBlocks itself! I have another PC where I use Visual Studio 2010 Professional and it doesn't have the same problem.</p>\n\n<p>I have manage to isolate the problem around the usage of MinGW compiler. The reason is that the problem occurs only when I am using it with an IDE (tried with Eclipse CDT, Dev CPP, and CodeBlocks). I got a stable release of MinGW downloaded from sourceforge.net which shouldn't have any major memory management issue with Windows Platform. Actually, the problem arises if I use it at all (doesn't start immediately, but after I have run my simply program a few times!).</p>\n\n<p>My MinGW compiler is from <a href=\"http://sourceforge.net/projects/mingw/files/Installer/mingw-get-inst/mingw-get-inst-20120426/\" rel=\"nofollow\">http://sourceforge.net/projects/mingw/files/Installer/mingw-get-inst/mingw-get-inst-20120426/</a></p>\n\n<p>I couldn't find a very straightforward answer to this, so I am assuming that this is an unusual problem? If it is a repeated question by any chance which I have missed, please post a link to the comment and I will accept the answer. Ta</p>\n"},{"tags":["java","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":148,"score":-1,"question_id":12683873,"title":"What causes my Java algorithm slower and slower after I start it for the first time?","body":"<p>I made a Java algorithm. It access the MongoDB database. It looks like they don't provide a method like .close() or .dispose(). SO I never use this kind of methods in the codes. Every time I run/debug the program, it becomes slower and slower. Is that any reasons or possibilities, generally or specifically?</p>\n"},{"tags":["performance","azure"],"answer_count":0,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":20,"score":1,"question_id":13284698,"title":"Azure SQL sharding doesn't scale as expected","body":"<p>I have a web application developed for Azure Cloud Services using Azure SQL. I started with one database and using JMeter, I ran a load test from a dedicated virtual machine in the same Azure datacenter. The test plan was set up to test only one page, and only posting to that page (ignoring the initial get request). </p>\n\n<p>My goal was to determine the max number of database transactions the app could handle per second. My first series of tests resulted in an average of 300 req/s using this test plan.</p>\n\n<p>The page in question is accessing the following table:</p>\n\n<pre><code>create table [dbo].[Entries]\n(\n    [Id]                   int                 not null        identity(1,1)\n    /* other columns here, about 10 in total */ \n);\n\nalter table [dbo].[Entries] add constraint [PK_Entries] primary key ([Id]);\nalter table [dbo].[Entries] add constraint [DF_Entries_Created] default(getutcdate()) for [Created];\n\ncreate index [IX_Entries_EmailAddress] on [dbo].[Entries] ([EmailAddress]);\ncreate index [IX_Entries_NACSZ] on [dbo].[Entries] \n(\n     [FirstName]\n    ,[LastName]\n    ,[Address1]\n    ,[City]\n    ,[State]\n    ,[Postal]\n);\n</code></pre>\n\n<p>The page executes only two queries:</p>\n\n<pre><code>select 1 where exists \n(\n    select 1 from Entries\n     where EmailAddress = @EmailAddress\n        or (\n                    FirstName       = @FirstNAme\n                and LastName        = @LastName\n                and Address1        = @Address1\n                and City            = @City\n                and State           = @State\n                and Postal          = @Postal\n           ) \n);\n</code></pre>\n\n<p>and </p>\n\n<pre><code>insert into Entries\n(\n    ...\n)\nvalues\n(\n    ...\n);\n\nselect cast(scope_identity() as int);\n</code></pre>\n\n<p>Performance testing on my machine (quad core, 8GB ram, local SQL 2012 express install) yields up to 800 req/s, so I was rather shocked to see a peak of about 300 req/s on Azure's servers. I chalked this up to resource contention on the database server, and added the necessary code to support sharding. The sharding mechanism uses a consistent hash on one of the key fields to determine which connection string to use (out of 3 possible, currently). The goal here was to split the database load across 3 Azure SQL databases and up the concurrency factor on the app.</p>\n\n<p>I have verified that the sharding mechanism works (approximately equal number of entries end up in each database) and have stripped all non-essential code from the page so that all that is happening is the two queries noted above. I'm using the default isolation level (read commited) on the transaction. The final code looks something like this:</p>\n\n<pre><code>using (var db = ConnectToShard(keyToHash))\nusing (var tx = db.BeginTransaction(IsolationLevel.ReadCommitted))\n{\n    // execute query 1\n    // if result from query 1 is null, \n    //     execute query 2\n\n    tx.Commit();\n}\n</code></pre>\n\n<p>However, even with all this extra work, I can't seem to push the requests/sec above ~500 or so. My ideal target is 1000. I guess I'm missing something with regard to Azure SQL performance, but I'm not sure what. Any thoughts or suggestions for improving requests/sec?</p>\n"},{"tags":["ios","performance","simd","neon"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":20,"score":-1,"question_id":13281995,"title":"Improve C code with Neon iOS","body":"<p>The code belotoning used to scale an image, but the performance is not good. I googled and found some advice to convert the codes to Neon ASM to get better performance. I don't have any experience of that, so I hope someone can help me to convert it. Thanks in advance.</p>\n\n<pre><code>inline void insetw32(char *pb_dst, char *pb_pFore, char *pb_pBack, char *pmix, int w)\n{\n    register char bbm = (char)(pmix[0]&lt;&lt;24&gt;&gt;24);\n    for(int i = 0;i&lt; w;i++)\n    {\n        *pb_dst++ = (((*pb_pFore++ - *pb_pBack++) * bbm)&gt;&gt;8) + *pb_pBack;\n        *pb_dst++ = (((*pb_pFore++ - *pb_pBack++) * bbm)&gt;&gt;8) + *pb_pBack;\n        *pb_dst++ = (((*pb_pFore++ - *pb_pBack++) * bbm)&gt;&gt;8) + *pb_pBack;\n        *pb_dst++ = (((*pb_pFore++ - *pb_pBack++) * bbm)&gt;&gt;8) + *pb_pBack;\n    }\n}\n</code></pre>\n"},{"tags":["c++","performance","theory"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":3,"view_count":65,"score":-2,"question_id":13283831,"title":"Why would a C++ code with a lot of iterations run slower with increasing runtime?","body":"<p>We have a small (~200 lines) C++ code which has loops nested down to five levels. The total number of iterations is in the hundred billions.</p>\n\n<p>For the first few iterations (~10k), the code runs at an acceptable pace, but eventually, it slows down to a crawl.</p>\n\n<p>There is no memory leak (it is a small code which is simple to debug and there is no explicit allocation of memory)</p>\n\n<p>What other factors can cause code to slow down in this manner?</p>\n"},{"tags":["performance","jvm"],"answer_count":0,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":27,"score":4,"question_id":13272543,"title":"Hack the JVM to avoid unnecessary bounds checks and casts","body":"<p>There are some languages that support a sufficiently powerful type system that they can prove at compile time that the code does not address an array outside its bounds. My question is that if we were to compile such a language to the JVM, is there some way we could take advantage of that for performance and remove the array bounds checks that occur on every array access?</p>\n\n<p>1) I know that recent JDK supports some array bound check elimination, but since I know at compile time that certain calls are safe, I could remove a lot more safely.</p>\n\n<p>2) Some might think this doesn't affect performance much but it most certainly does, especially in array/computation heavy applications such as scientific computing.</p>\n\n<p>The same question regarding casting. I know something is a certain type, but Java doesn't because its limited type system. Is there some way to just tell the JVM to \"trust me\" and skip any checks?</p>\n\n<p>I realize there is probably no way to do this as the JVM is generally distributed, could it be reasonable to modify a JVM with this feature? Is this something that has been done?</p>\n\n<p>It's one of the frustrations in compiling a more powerfully typed language to the JVM, it still is hampered by Java's limitations.</p>\n"},{"tags":["performance","mobile","subdomain","maintenance","subdirectory"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":4,"score":1,"question_id":13282988,"title":"Using subdomains or subdirectories for Mobile version of site","body":"<p>I'm setting up a site. And like everybody, I have to plan on separate mobile presentation. I've read several articles about the <a href=\"http://www.timpeter.com/blog/2011/12/14/subdomains-vs-subdirectories-and-seo/#domain-footnote-1\" rel=\"nofollow\">SEO aspect</a> of using a subdirectory, subdomain, or just different content off the same URL. But I have not seen any real technical benefits or costs or performance information here on this site, or on the web.</p>\n\n<p>So my main question is, which way of supporting mobile devices is the best way; Subdirectory, subdomain, or just different content from same URL (not talking mobile apps here). I can't think of a good way to word this technical inquiry that could avoid 'opinions', sorry.</p>\n\n<p>Considerations that I thought of that would affect the answer are:</p>\n\n<p>1/ Which is easier to code for on the backend?</p>\n\n<p>2/ Which is easier to code on the 'View' part?</p>\n\n<p>3/ How is the infrastructure affected, i.e. Session tracking, code maintenance, deployment, code complexity, testing?</p>\n\n<p>4/ Which has better performance? Does the mobile site (supposedly after redirection) still use the browser/device detection code, or just assume that the consumer has come to the location they want? (Is it normal to have a 'full version' link/button to link the session to the full version?)</p>\n"},{"tags":["c++","performance","matrix","sse","simd"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":95,"score":0,"question_id":12965291,"title":"How to improve sse based matrix multiplication","body":"<pre><code>float** matrix::mult(float** matrix1){\n  float** result=new float *[n];\n  int i,j,k;\n  for(i=0;i&lt;n;i++){\n    result[i]=new float [n];\n  }\n  vect v1;\n  vect v2;\n  vect v3;\n  vect total;\n  clock_t start, end;\n  start = clock();\n  float result_ij=0;\n  for(i=0;i&lt;n;i++){   \n    for(j=0;j&lt;n;j++){\n      result_ij=0;\n      total.v=_mm_set1_ps(0);\n      for(k=0;k&lt;n;k=k+4){\n        v1.v=_mm_set_ps(user_matrix[k][j],user_matrix[k+1][j],user_matrix[k+2][j],user_matrix[k+3][j]);\n        v2.v=_mm_set_ps(matrix1[i][k],matrix1[i][k+1],matrix1[i][k+2],matrix1[i][k+3]);\n        v3.v=_mm_mul_ps(v1.v,v2.v);\n        total.v=_mm_add_ps(total.v,v3.v);\n      }\n      result[i][j]=total.a[1]+total.a[0]+total.a[2]+total.a[3];\n    }\n  }\n  end = clock();\n  cout&lt;&lt;(double)(end-start)/CLOCKS_PER_SEC&lt;&lt;endl;\n  return result;\n}\n</code></pre>\n\n<p>This code is about exactly the same speed as the scalar code. I can't see why this would be so slow, it was compiled with g++ and the vect type is a union.</p>\n\n<pre><code>union vect {\n__m128 v;    \nfloat a[4];  \n} ;\n</code></pre>\n\n<p>For the matrix as a multidimensional array, what is the fastest way to load that into the SSE register?</p>\n"},{"tags":["performance","berkeley-db"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":9,"score":0,"question_id":13281809,"title":"insert data to Berkeley DB very poor performance in Big Data","body":"<p>I have already held the 80G Berkeley DB file. I measure the average insert speed is 8ms for one record(32 byte key/100 byte value) without transaction.</p>\n\n<p>Compare to insert to empty database with same interface, the average speed is 3~6 us.</p>\n"},{"tags":["database","performance","csv","migration","redis"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":19,"score":0,"question_id":13280993,"title":"How to move this data to redis for speedy access?","body":"<p>I've purchased a MaxMind.com's geoIP country database. They have a neat little tutorial about accessing it <a href=\"http://dev.maxmind.com/geoip/csv\" rel=\"nofollow\">here</a>.</p>\n\n<p>Here's a snippet of code that would be used with MySQL:</p>\n\n<pre><code>SELECT ip_country\nFROM geoip\nWHERE\nINET_ATON('174.36.207.186') BETWEEN begin_ip_num AND end_ip_num\nLIMIT 1\n</code></pre>\n\n<p>So, there's two columns, begin_ip_num and end_ip_num - determining the country of origin is as easy as finding a value between the two.</p>\n\n<p>Say I want to have this same data in Redis for quick access. What do I do? Will I have to run through the list and generate every possible ip thereby increasing the size of the instance DRAMATICALLY, or is there a clever way to do the same thing BETWEEN does?</p>\n"},{"tags":["performance","cakephp","godaddy","cakephp-2.0"],"answer_count":5,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":1238,"score":4,"question_id":7937168,"title":"Incredibly slow load times using CakePHP on GoDaddy hosting","body":"<p>I have multiple sites on the same hosting account, and they all run really fast.  But now I've started my first CakePHP (2.0 stable) site hosted on this account, and it runs CRAZY-slow.</p>\n\n<p>I'm talking 6.5 to 12 seconds to just load a static/empty 20KB homepage.</p>\n\n<p>The only modifications I did were tweaking the .htaccess files per instructions here: <a href=\"http://book.cakephp.org/2.0/en/installation/advanced-installation.html?highlight=godaddy\">http://book.cakephp.org/2.0/en/installation/advanced-installation.html?highlight=godaddy</a> (htaccess code below) by adding <code>Rewrite Base /</code> (which got the site to work in the first place).</p>\n\n<p>GoDaddy tech guys spent 20 minutes while on the phone with me running lots of tests and assured me the server itself and the database are both running fine/fast (which I tend to believe since my other sites on the same server are coming up really quickly).</p>\n\n<p>/mysite/.htaccess</p>\n\n<pre><code>&lt;IfModule mod_rewrite.c&gt;\nRewriteEngine on\nRewriteBase /\nRewriteRule    ^$ app/webroot/    [L]\nRewriteRule    (.*) app/webroot/$1 [L]\n&lt;/IfModule&gt;\n</code></pre>\n\n<p>/mysite/app/.htaccess</p>\n\n<pre><code>&lt;IfModule mod_rewrite.c&gt;\nRewriteEngine on\nRewriteBase /\nRewriteRule    ^$    webroot/    [L]\nRewriteRule    (.*) webroot/$1    [L]\n&lt;/IfModule&gt;\n</code></pre>\n\n<p>/mysite/app/webroot/.htaccess</p>\n\n<pre><code>&lt;IfModule mod_rewrite.c&gt;\nRewriteEngine On\nRewriteBase /\nRewriteCond %{REQUEST_FILENAME} !-d\nRewriteCond %{REQUEST_FILENAME} !-f\nRewriteRule ^(.*)$ index.php/$1 [QSA,L]\n&lt;/IfModule&gt;\n</code></pre>\n"},{"tags":["iphone","ios","performance","core-graphics","quartz-2d"],"answer_count":2,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":82,"score":6,"question_id":13277031,"title":"Core Graphics Performance on iOS","body":"<h1>Summary</h1>\n\n<p>I'm working on a fairly straightforward 2D tower defense game for iOS.</p>\n\n<p>So far, I've been using Core Graphics exclusively to handle rendering. There are no image files in the app at all (yet). I've been experiencing some significant performance issues doing relatively simple drawing, and I'm looking for ideas as to how I can fix this, short of moving to OpenGL.</p>\n\n<h1>Game Setup</h1>\n\n<p>At a high level, I have a Board class, which is a subclass of <code>UIView</code>, to represent the game board. All other objects in the game (towers, creeps, weapons, explosions, etc) are also subclasses of <code>UIView</code>, and are added as subviews to the Board when they are created.</p>\n\n<p>I keep game state totally separate from view properties within the objects, and each object's state is updated in the main game loop (fired by an <code>NSTimer</code> at 60-240 Hz, depending on the game speed setting). The game is totally playable without ever drawing, updating, or animating the views.</p>\n\n<p>I handle view updates using a <code>CADisplayLink</code> timer at the native refresh rate (60 Hz), which calls <code>setNeedsDisplay</code> on the board objects that need to have their view properties updated based on changes in the game state. All the objects on the board override <code>drawRect:</code> to paint some pretty simple 2D shapes within their frame. So when a weapon, for example, is animated, it will redraw itself based on the weapon's new state.</p>\n\n<h1>Performance Issues</h1>\n\n<p>Testing on an iPhone 5, with about 2 dozen total game objects on the board, the frame rate drops significantly below 60 FPS (the target frame rate), usually into the 10-20 FPS range. With more action on the screen, it goes downhill from here. And on an iPhone 4, things are even worse.</p>\n\n<p>Using Instruments I've determined that only roughly 5% of the CPU time is being spent on actually updating the game state -- the vast majority of it is going towards rendering. Specifically, the <code>CGContextDrawPath</code> function (which from my understanding is where the rasterization of vector paths is done) is taking an enormous amount of CPU time. See the Instruments screenshot at the bottom for more details.</p>\n\n<p>From some research on StackOverflow and other sites, it seems as though Core Graphics just isn't up to the task for what I need. Apparently, stroking vector paths is extremely expensive (especially when drawing things that aren't opaque and have some alpha value &lt; 1.0). I'm almost certain OpenGL would solve my problems, but it's pretty low level and I'm not really excited to have to use it -- it doesn't seem like it should be necessary for what I'm doing here.</p>\n\n<h1>The Question</h1>\n\n<p><strong>Are there any optimizations I should be looking at to try to get a smooth 60 FPS out of Core Graphics?</strong> </p>\n\n<h2>Some Ideas...</h2>\n\n<p>Someone suggested that I consider drawing all my objects onto a single <code>CALayer</code> instead of having each object on its own <code>CALayer</code>, but I'm not convinced that this would help based on what Instruments is showing. </p>\n\n<p>Personally, I have a theory that using <code>CGAffineTransforms</code> to do my animation (i.e. draw the object's shape(s) in <code>drawRect:</code> once, then do transforms to move/rotate/resize its layer in subsequent frames) would solve my problem, since those are based directly on OpenGL. But I don't think it would be any easier to do that than just use OpenGL outright.</p>\n\n<h2>Sample Code</h2>\n\n<p>To give you a feel for the level of drawing I'm doing, here's an example of the <code>drawRect:</code> implementation for one of my weapon objects (a \"beam\" fired from a tower).</p>\n\n<p><em>Note: this beam can be \"retargeted\" and it crosses the entire board, so for simplicity its frame is the same dimensions as the board. However most other objects on the board have their frame set to the smallest circumscribed rectangle possible.</em></p>\n\n<pre><code>- (void)drawRect:(CGRect)rect\n{\n    CGContextRef c = UIGraphicsGetCurrentContext();\n\n    // Draw beam\n    CGContextSetStrokeColorWithColor(c, [UIColor greenColor].CGColor);\n    CGContextSetLineWidth(c, self.width);\n    CGContextMoveToPoint(c, self.origin.x, self.origin.y);\n    CGPoint vector = [TDBoard vectorFromPoint:self.origin toPoint:self.destination];\n    double magnitude = sqrt(pow(self.board.frame.size.width, 2) + pow(self.board.frame.size.height, 2));\n    CGContextAddLineToPoint(c, self.origin.x+magnitude*vector.x, self.origin.y+magnitude*vector.y);\n    CGContextStrokePath(c);\n\n}\n</code></pre>\n\n<h2>Instruments Run</h2>\n\n<p>Here's a look at Instruments after letting the game run for a while:</p>\n\n<p><em><strong>The <code>TDGreenBeam</code> class has the exact <code>drawRect:</code> implementation shown above in the Sample Code section.</em></strong></p>\n\n<p><strong><a href=\"http://cl.ly/image/2b1y3512341d\" rel=\"nofollow\">Full Size Screenshot</a></strong>\n<img src=\"http://i.stack.imgur.com/qSw3j.png\" alt=\"Instruments run of the game, with the heaviest stack trace expanded.\"></p>\n"},{"tags":["javascript","performance","extjs3"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":11,"score":-1,"question_id":13281517,"title":"How to tuning performance with extjs 3.x in large web application?","body":"<p>I want to seek advice about ExtJS 3.x performance tuning. Like layout, code structure, reduce http request, reusability etc...</p>\n\n<p>Thanks a lot.</p>\n"},{"tags":["sql","ruby-on-rails","database","performance","postgresql"],"answer_count":2,"favorite_count":8,"up_vote_count":17,"down_vote_count":0,"view_count":933,"score":17,"question_id":9407442,"title":"Optimise PostgreSQL for fast testing","body":"<p>I am switching to PostgreSQL from SQLite for a typical Rails application.</p>\n\n<p>The problem is that running specs became slow with PG.<br>\nOn SQLite it took ~34 seconds, on PG it's ~76 seconds which is <strong>more than 2x slower</strong>.</p>\n\n<p>So now I want to apply some techniques to <strong>bring the performance of the specs on par with SQLite</strong> with no code modifications (ideally just by setting the connection options, which is probably not possible).</p>\n\n<p>Couple of obvious things from top of my head are:</p>\n\n<ul>\n<li>RAM Disk (good setup with RSpec on OSX would be good to see)</li>\n<li>Unlogged tables (can it be applied on the whole database so I don't have change all the scripts?)</li>\n</ul>\n\n<p>As you may have understood I don't care about reliability and the rest (the DB is just a throwaway thingy here).<br>\nI need to get the most out of the PG and make it <strong>as fast as it can possibly be</strong>.</p>\n\n<p><strong>Best answer</strong> would ideally describe the <em>tricks</em> for doing just that, setup and the drawbacks of those tricks.</p>\n\n<p><strong>UPDATE:</strong> <code>fsync = off</code> + <code>full_page_writes = off</code> only decreased time to ~65 seconds (~-16 secs). Good start, but far from the target of 34.</p>\n\n<p><strong>UPDATE 2:</strong> I <a href=\"https://gist.github.com/1573414\">tried to use RAM disk</a> but the performance gain was within an error margin. So doesn't seem to be worth it.</p>\n\n<p><strong>UPDATE 3:*</strong>\nI found the biggest bottleneck and now my specs run as fast as the SQLite ones.</p>\n\n<p>The issue was the database cleanup that did the <strong>truncation</strong>. Apparently SQLite is way too fast there.</p>\n\n<p>To \"fix\" it I open a <strong>transaction</strong> before each test and roll it back at the end.</p>\n\n<p>Some numbers for ~700 tests.</p>\n\n<ul>\n<li>Truncation: SQLite - 34s, PG - 76s.</li>\n<li>Transaction: SQLite - 17s, PG - 18s.</li>\n</ul>\n\n<p>2x speed increase for SQLite.\n4x speed increase for PG.</p>\n"},{"tags":["sql","sql-server","performance","query","tsql"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":23,"score":0,"question_id":13280548,"title":"Performance issue in retrieving multiple output of the same column with different values from a single table","body":"<p>is there anyway to get the following results from query without joining the same table three times (or) without reading the same \"wordlocation\" table three times (or more if there are more words)? If there are three or more words, it takes about over a minute for the results to be returned.</p>\n\n<p>Currently \"wordlocation\" table has three rows being (\"bookid\",\"wordid\",\"location\") and it currently has 917802 rows.</p>\n\n<p>What I am trying to do is </p>\n\n<ol>\n<li>retrieve the \"bookid\" that contains all the words specified in the query by \"wordid\".</li>\n<li>sum word count of all words (from the query) from each book</li>\n<li>minimum values of each word location, e.g. (min(w0.location), min (w1.location)</li>\n</ol>\n\n<p>I have tried commenting out count(w0.wordid) and min(location) calculations to see whether they are affecting the performance but this is not the case. Joining the same table multiple time was the case.</p>\n\n<p><img src=\"http://i.stack.imgur.com/SYLB7.jpg\" alt=\"enter image description here\"></p>\n\n<p>(this is the same code as the above image)   </p>\n\n<pre><code>select \n    w0.bookid, \n    count(w0.wordid) as wcount, \n    abs(min(w0.location) + min(w1.location) + min(w2.location)) as wordlocation, \n    (abs(min(w0.location) - min(w1.location)) + abs(min(w1.location) - min(w2.location))) as distance \n    from \n    wordlocation as w0 \n    inner join \n    wordlocation as w1 on w0.bookid = w1.bookid \n    join \n    wordlocation as w2 on w1.bookid = w2.bookid \n    where \n    w0.wordid =3 \n    and \n    w1.wordid =52 \n    and \n    w2.wordid =42\n    group by w0.bookid \n    order by wcount desc;\n</code></pre>\n\n<p>This is the result that I am looking for, and which I got from running the above query, but it takes too long if I specify more than 3 words, e.g. (w0 = 3, w1 = 52 , w2 = 42, w3 = 71)</p>\n\n<p><img src=\"http://i.stack.imgur.com/08dD5.jpg\" alt=\"enter image description here\"></p>\n"},{"tags":["javascript","performance","function","function-pointers"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":41,"score":0,"question_id":13280443,"title":"Implications of declaring a function within another function","body":"<p>Given the following two:</p>\n\n<p><strong>Scenario 1</strong></p>\n\n<pre><code>function inner() {\n  // a bunch of code that does stuff\n}\nfunction outer() {\n  inner();\n}\nfor(var i = 0; i &lt; 10000; i++) {\n  outer();\n}\n</code></pre>\n\n<p><strong>Scenario 2</strong></p>\n\n<pre><code>function outer() {\n  function inner() {\n    // a bunch of code that does stuff\n  }\n  inner();\n}\nfor(var i = 0; i &lt; 10000; i++) {\n  outer();\n}\n</code></pre>\n\n<p>Behavior is identical in both cases, no doubt. But what's the difference under the hood? How much extra work, if any, is the interpreter doing in scenario 2? Is the memory affected. Or, say, if the body of <code>inner()</code> gets longer, would that increase the effect on performance?</p>\n\n<p>Please don't bother asking \"why would you want to do that\", because my question is not about a practical issue. Just trying to get a deeper understanding of how JS function are parsed and represented. Thanks!</p>\n"},{"tags":["performance","64bit","firebird","firebird2.5"],"answer_count":5,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":729,"score":3,"question_id":2283497,"title":"Huge page buffer vs. multiple simultaneous processes","body":"<p>One of our customer has a 35 Gb database with average active connections count about 70-80. Some tables in database have more than 10M records per table.</p>\n\n<p>Now they have bought new server: 4 * 6 Core = 24 Cores CPU, 48 Gb RAM, 2 RAID controllers 256 Mb cache, with 8 SAS 15K HDD on each.</p>\n\n<p>64bit OS.</p>\n\n<p>I'm wondering, what would be a fastest configuration:</p>\n\n<p>1) FB 2.5 SuperServer with huge buffer 8192 * 3500000 pages = 29 Gb</p>\n\n<p>or</p>\n\n<p>2) FB 2.5 Classic with small buffer of 1000 pages.</p>\n\n<p>Maybe some one has tested such case before and will save me days of work :)</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["mysql","performance","replication"],"answer_count":2,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":2730,"score":4,"question_id":2832912,"title":"How can \"set timestamp\" be a slow query?","body":"<p>My slow query log is full of entries like the following</p>\n\n<p># Query_time: 1.016361  Lock_time: 0.000000 Rows_sent: 0  Rows_examined: 0\nSET timestamp=1273826821;\nCOMMIT;</p>\n\n<p>I guess the set timestamp command is issued by replication but I don't understand how set timestamp can take over a second. Any ideas?</p>\n"},{"tags":["javascript","performance","for-loop","while-loop","three.js"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":30,"score":0,"question_id":13279912,"title":"three.js why does it use for loops instead of while","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/3629174/what-loop-is-faster-while-or-for\">What loop is faster, while or for</a>  </p>\n</blockquote>\n\n\n\n<p>I see in three.js that there is the common code feature in many languages:</p>\n\n<pre><code>for ( var i = 0, l = something.length; i &lt; l; i++ ) {\n    do some stuff over i\n}\n</code></pre>\n\n<p>but I read that in javascript performance can be better by using:</p>\n\n<pre><code>var i = something.length;\nwhile(i--){\n    do some stuff over i\n}\n</code></pre>\n\n<p>Does this actually improve any performance significantly? is there a reason to prefer one over the other?</p>\n"},{"tags":["performance","cryptography","rsa","public-key-encryption","factorization"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":84,"score":2,"question_id":12637582,"title":"why is integer factorization a non-polynomial time?","body":"<p>I am just a beginner of computer science. I learned something about running time but I can't be sure what I understood is right. So please help me. </p>\n\n<p>So integer factorization is currently not a polynomial time problem but primality test is. Assume the number to be checked is n. If we run a program just to decide whether every number from 1 to sqrt(n) can divide n, and if the answer is yes, then store the number. I think this program is polynomial time, isn't it? </p>\n\n<p>One possible way that I am wrong would be a factorization program should find all primes, instead of the first prime discovered. So maybe this is the reason why. </p>\n\n<p>However, in public key cryptography, finding a prime factor of a large number is essential to attack the cryptography. Since usually a large number (public key) is only the product of two primes, finding one prime means finding the other. This should be polynomial time. So why is it difficult or impossible to attack? </p>\n"},{"tags":["c#","performance","asp.net-4.0","code-readability","boolean-operations"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":38,"score":1,"question_id":13279096,"title":"how to decide on a bool operators usage, performance issues vs readability","body":"<p>Having even more than two options to choose from, leads me to question, which one to choose, if  the result / outcome is same.</p>\n\n<p>in <code>.NET</code> <code>C#</code>  the following conditions are asking same question using different operators, so the question is , what experienced developers use, i tend to assume that ready made operators like <code>Equals</code> would go through more processing actions.</p>\n\n<p>When and why would you choose <code>!</code> over <code>Equals</code>, and both over 'traditional'  <code>==</code> ?</p>\n\n<pre><code>//bool\nif (!Page.IsPostBack) \n{\n    //bool\n    if (NotAuthorized().Equals(false)) \n    {            \n        AppsCtrls.DDLs_Init();\n\n        //bool\n        if (CurrSeSn.Raised(Flag.MainDataSet_IsPopulated) == false)\n        {\n            initALLDataSet(AllDataStColsSelectionMod.doneViaSP);\n        }\n\n        custid = RConv.Str2int(Request.QueryString[\"custid\"]);\n        username = GetTableData.AsString(\"name\", \"tblCustomers\", \"custid\", custid);\n    }\n}\n</code></pre>\n"},{"tags":["java","performance","date","comparison","deprecated"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":31,"score":3,"question_id":13279701,"title":"Most efficient way of checking if Date object and Calendar object are in the same month","body":"<p>I am working on a project that will run many thousands of comparisons between dates to see if they are in the same month, and I am wondering what the most efficient way of doing it would be.  </p>\n\n<p>This isn't exactly what my code looks like, but here's the gist:</p>\n\n<pre><code>List&lt;Date&gt; dates = getABunchOfDates();\nCalendar month = Calendar.getInstance();\nfor(int i = 0; i &lt; numMonths; i++) \n{\n    for(Date date : dates)\n    {\n        if(sameMonth(month, date)\n            .. doSomething\n    }\n    month.add(Calendar.MONTH, -1);\n}\n</code></pre>\n\n<p>Creating a new <code>Calendar</code> object for every date seems like a pretty hefty overhead when this comparison will happen thousands of times, soI kind of want to cheat a bit and use the deprecated method <code>Date.getMonth()</code> and <code>Date.getYear()</code></p>\n\n<pre><code>public static boolean sameMonth(Calendar month, Date date)\n{\n    return month.get(Calendar.YEAR) == date.getYear() &amp;&amp; month.get(Calendar.MONTH) == date.getMonth();\n}\n</code></pre>\n\n<p>I'm pretty close to just using this method, since it seems to be the fastest, but <strong>is there a faster way?</strong>  And <strong>is this a foolish way</strong>, since the <code>Date</code> methods are deprecated?  Note: This project will always run with Java 7</p>\n"},{"tags":["c#","wpf","performance","master-detail"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":2,"view_count":44,"score":-2,"question_id":13279655,"title":"How to make my WPF application as FAST as Outlook","body":"<p>The commons WPF applications take some time for loading medium complex views, once the view is loaded it works fine. For example in a Master - Detail view, if the Detail view is very complex and use different DataTemplates take some seconds (2-3 seconds) for load the view.</p>\n\n<p>When i open the Outlook application, for instance, it renders complex views and it is relative much more fast.</p>\n\n<p>Is there a way for increase the performance of my WPF application? Maybe a way for not loading the template's data every time that change the \"master\" item, and load it only one time in the app time live?</p>\n\n<p>i will appreciate any suggestion. </p>\n"},{"tags":["performance","variables","loops","initialization","declaration"],"answer_count":17,"favorite_count":19,"up_vote_count":69,"down_vote_count":1,"view_count":25338,"score":68,"question_id":407255,"title":"Difference between declaring variables before or in loop?","body":"<p>I have always wondered if, in general, declaring a throw-away variable before a loop, as opposed to repeatedly inside the loop, makes any (performance) difference? \nA (quite pointless example) in Java:</p>\n\n<p>a) declaration before loop:</p>\n\n<pre><code>double intermediateResult;\nfor(int i=0;i&lt;1000;i++){\n    intermediateResult = i;\n    System.out.println(intermediateResult);\n}\n</code></pre>\n\n<p>b) declaration (repeatedly) inside loop:</p>\n\n<pre><code>for(int i=0;i&lt;1000;i++){\n    double intermediateResult = i;\n    System.out.println(intermediateResult);\n}\n</code></pre>\n\n<p>Which one is better, a or b? </p>\n\n<p>I suspect that repeated variable declaration (example b) creates more overhead <em>in theory</em>, but that compilers are smart enough so that it doesn't matter. Example b has the advantage of being more compact and limiting the scope of the variable to where it is used. Still, I tend to code according example a...</p>\n\n<p>Edit: I am especially interested in the Java case.</p>\n"},{"tags":["javascript","arrays","performance","algorithm","multidimensional-array"],"answer_count":5,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":115,"score":7,"question_id":13275839,"title":"Fastest way to reset a multidimensional array?","body":"<p>Say I have a two dimensional array: <code>vectors[x][y]</code>, and the initial array structure looks like this:</p>\n\n<pre><code>vectors = [    \n [0, 0, 0, 0, 0,],\n [0, 0, 0, 0, 0,],\n [0, 0, 0, 0, 0,],\n [0, 0, 0, 0, 0,],\n [0, 0, 0, 0, 0,]\n]\n</code></pre>\n\n<p>After some calculations, the data in the array is randomized. What is the fastest way and most efficient way to return the array to it's initial state?</p>\n\n<p>I know that I could just hardcode the above zeroed array and set vectors equal to it again, but I also know that an algorithm such as:</p>\n\n<pre><code>for (var x = 0; x &lt; vectors.length; x++) {\n    for (var y = 0; y &lt; vectors[x].length; y++) {\n        vectors[x][y] = 0;\n    }\n\n}\n</code></pre>\n\n<p>is O(x * y).</p>\n\n<p>So which is the better way? And is there a better, even faster/more efficient way to solve this?</p>\n\n<p>And for the general case of zeroing a multi-dimensional array of any length, which is the best way? (I'm working in JavaScript if it matters)</p>\n"},{"tags":["ruby-on-rails-3","performance","activerecord"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":10,"score":0,"question_id":13271561,"title":"ActiveRecord: Is it possible to get the number of DB queries executed in addition to total time in the Rails log?","body":"<p>For every request, I get this in the logs:</p>\n\n<pre><code>Completed 200 OK in 854ms (Views: 1.0ms | ActiveRecord: 17.0ms)\n</code></pre>\n\n<p>Is it possible to get it to also include the number of queries?<br>\nSomething like:</p>\n\n<pre><code>Completed 200 OK in 854ms (Views: 1.0ms | ActiveRecord: 17.0ms | Queries: 10)\n</code></pre>\n\n<p>Ideally, I'd like all the \"cached\" ones to show up in that count too. Ie, even if the \"cache\" is saving me from \"N+1\" queries from hitting the DB, I still want to know I have a problem.</p>\n\n<p>I'm fine with monkeypatching / manually editing something, since I really want this just for my dev box. </p>\n\n<p>(If this can be made civilizedly so I can have it in production, that's even better, but if not, I'm fine with just having a manually modified Rails in my own machine)</p>\n\n<p>Thanks!<br>\nDaniel</p>\n"},{"tags":["performance","wordpress","time","blogs"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":6,"score":0,"question_id":13274366,"title":"wordpress blog excerpt prestation","body":"<p>I don't know how this page of my web site, takes much time to load:\nmichelepierri.it/blog \nIn this page there are post excerpts of blog.</p>\n\n<p>Instead other pages like home page, take less time to load.</p>\n\n<p>What can be causing this?\nThanks a lot.</p>\n\n<p>Plugins I use:</p>\n\n<pre><code>Advanced Code Editor\nAll in One SEO Pack\nBetter Related Content\ncbnet Twitter Widget\nCloudFlare\nContact Form 7\nDefault Thumbnail Plus\nDeveloper Formatter\nDisqus Comment System\nFancybox\nFast Secure Contact Form\nFeedBurner FeedSmith Extend\nGoogle Analytics\nGoogle XML Sitemaps\nlorem shortcode\nNextScripts: Social Networks Auto-Poster\nOfficial StatCounter Plugin\nPingler\nReally Simple CAPTCHA\nShareaholic | email, bookmark, share buttons\nSimple Skype Status\nSingle Category Permalink\nSkype Online Status\nSocial Metrics\nSyntaxHighlighter Plus\nTransposh Filtro per Traduzioni\nTrash Manager\nW3 Total Cache\nWP-Cumulus\nWP-o-Matic\nWP Facebook Open Graph protocol\nWP Minify\nWP to Twitter\nYoutube shortcode\n</code></pre>\n"},{"tags":["performance","numpy","scipy","sparse-matrix"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":27,"score":0,"question_id":13272453,"title":"Multiplying Numpy/Scipy Sparse and Dense Matrices Efficiently","body":"<p>I'm working to implement the following equation:</p>\n\n<pre><code>Xu = (Y.T * C * Y) ^ -1\n</code></pre>\n\n<p>Y is a (n x f) matrix and C is (n x n) diagonal one; n is about 300k and f will vary between 100 and 200. As part of an optimization process this equation will be used almost 100 million times so it has to be processed really fast.</p>\n\n<p>Y is initialized by:</p>\n\n<pre><code>from numpy import array, random as rd\nf = 100\nn = 300000\ny = array([rd.random(f) for i in range(n)])\n</code></pre>\n\n<p>Resulting:</p>\n\n<pre><code>&gt;&gt;&gt; y.shape\n&gt;&gt;&gt; (300000L, 100L)\n</code></pre>\n\n<p>C is a very sparse matrix and only a few numbers out of the 300k on the diagonal will be different than 0. Since Numpy's diagonal functions creates dense matrices, I created C as a sparse csr matrix. But when trying to solve the first part of the equation:</p>\n\n<pre><code>r = dot(C, Y)\n</code></pre>\n\n<p>I had to restart my computer as it crashed down trying to make the operation (I researched why but couldn't figure out why this happens).</p>\n\n<p>Then I tried:</p>\n\n<pre><code>r = C * Y\n</code></pre>\n\n<p>and it did work. But it took <strong>53 ms</strong> to do so. </p>\n\n<p>As another option I tried also:</p>\n\n<pre><code>from scipy.linalg import fblas as FB\nr = FB.dgemm(alpha=1., a=C, b=Y)\n</code></pre>\n\n<p>but I discovered dgemm function accepts only dense matrices.</p>\n\n<p>Since C has few elements I also tried to multiply the columns of Y by the scalars in C, like so:</p>\n\n<pre><code>Y.T[0] *= 2\n</code></pre>\n\n<p>but it took <strong>3 ms</strong> per element in C.</p>\n\n<p>I decided then trying to convert Y to csr_matrix and make the same operation:</p>\n\n<pre><code>r = C * Ysparse   \n</code></pre>\n\n<p>and this approach took <strong>1.38 ms</strong>. But this solution is somewhat \"tricky\" since I'm using a sparse matrix to store a dense one, I wonder how efficient this really is. And another problem to this approach is that the inverse operation in the end only accepts dense matrices as input so some time has to be lost transforming the sparse into dense.</p>\n\n<p>Is there some way of multiplying the sparse C and the dense Y without having to turn Y into sparse and improve performance? If somehow C could be represented as diagonal dense without consuming tons of memory maybe this would lead to very efficient performance but I don't know if this is possible.</p>\n\n<p>I appreciate your help!</p>\n"},{"tags":["linux","performance","node.js","iis"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":40,"score":1,"question_id":13271057,"title":"node.js vs IIS 7.5 performance","body":"<h2>The setup:</h2>\n\n<p>2 machines on EC2 of type m3.xlarge.</p>\n\n<p>First one with ubuntu server.</p>\n\n<p>Second one win2008r2.</p>\n\n<p>node.js on ubuntu using the basic example to return a string response to any request.</p>\n\n<p>asp.net httphandler to return the same response.</p>\n\n<p>using <a href=\"https://github.com/newsapps/beeswithmachineguns\" rel=\"nofollow\">https://github.com/newsapps/beeswithmachineguns</a> \nI used 10 machines to execute 200000 with concurrency of 2000 (200 per machine)\nI ran the benchmark and got:</p>\n\n<h2>NodeJS:</h2>\n\n<pre><code> Complete requests:         200000\n\n Requests per second:       5605.170000 [#/sec] (mean)\n\n Time per request:          358.071900 [ms] (mean)\n\n 50% response time:         31.000000 [ms] (mean)\n\n 90% response time:         239.300000 [ms] (mean)\n</code></pre>\n\n<h2>IIS:</h2>\n\n<pre><code> Complete requests:         200000\n\n Requests per second:       9263.810000 [#/sec] (mean)\n\n Time per request:          215.992900 [ms] (mean)\n\n 50% response time:         214.000000 [ms] (mean)\n\n 90% response time:         244.000000 [ms] (mean)\n</code></pre>\n\n<p>The nodeJS code is:</p>\n\n<pre><code>http.createServer(function (request, response) {\n  response.writeHead(200, {'Content-Type': 'text/plain'});\n  response.end('Some response\\n');\n}).listen(80);\n</code></pre>\n\n<p>The httphandler code is:</p>\n\n<pre><code>context.Response.Write(\"Some response\\n\" + Guid.NewGuid().ToString(\"N\"));\n</code></pre>\n\n<p>I thought node js will be much faster, did I do something wrong?</p>\n\n<h2>EDIT:</h2>\n\n<p>after using the cluster module I got 16685 request per second from the node js\nI\"m going to bring up the strongest EC2 instances and check on them</p>\n"},{"tags":["wpf","performance","image","decode","bitmapimage"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":58,"score":2,"question_id":13054909,"title":"BitmapImage decoding speed performance wpf","body":"<p>I have 5 images all the same pixel height and pixel width (2481 * 3508 for that matter). But, one is gif, one jpeg, one png and one bmp. Now I render them into a BitmapSource with (1) two thirds of the original pixel height for DecodePixelHeight and (2) original pixel height for DecodePixelHeight. </p>\n\n<p>First scenario: </p>\n\n<pre><code>bitmapImage.BeginInit();\nbitmapImage.CreateOptions = BitmapCreateOptions.IgnoreColorProfile;\nbitmapImage.CacheOption = BitmapCacheOption.OnLoad;\nbitmapImage.DecodePixelHeight = 2/3 * originalHeight;\nbitmapImage.StreamSource = streamWithTheFile;\nbitmapImage.EndInit();\nbitmapImage.Freeze();\n</code></pre>\n\n<p>BMP and Jpeg are equally slow. Png and Gif need less than half the time. Why?</p>\n\n<p>Second scenario:</p>\n\n<pre><code>bitmapImage.BeginInit();\nbitmapImage.CreateOptions = BitmapCreateOptions.IgnoreColorProfile;\nbitmapImage.CacheOption = BitmapCacheOption.OnLoad;\nbitmapImage.StreamSource = streamWithTheFile;\nbitmapImage.EndInit();\nbitmapImage.Freeze();\n</code></pre>\n\n<p>Png half of the time needed before. Jpeg and BMP one 5th of the time needed before. Gif the same time as before.</p>\n\n<p>According to <a href=\"http://msdn.microsoft.com/en-us/library/system.windows.media.imaging.bitmapimage.decodepixelheight.aspx\" rel=\"nofollow\">documentation</a> I would have assumed that Png and Jpeg performance would somehow be more independent of actual decode size than the other formats.  What could be the reason, that it is not?</p>\n"},{"tags":["windows","performance","drupal","localhost","easyphp"],"answer_count":4,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":127,"score":2,"question_id":13148645,"title":"SLOW Drupal over localhost | windows7 EasyPHP 64x","body":"<p><br>\nI am running Drupal 7.16 on my laptop(Windows7 x64 with 6gb RAM over EasyPHP12.1).<br>\nFor now the drupal running very slow!</p>\n\n<p>I already try to:</p>\n\n<ol>\n<li>Increase php.ini <code>realpath_cache_size</code> to <code>24M</code></li>\n<li>Change my.ini <code>innodb_flush_log_at_trx_commit</code> to <code>0</code></li>\n<li>Change the <code>hosts</code> file to resolve ipv6 bug..</li>\n<li>Try another wamp solution</li>\n<li>It seems that when I run simple query(<code>SELECT uid FROM users</code>) the phpMyAdmin return a quick respond(0.0009s)..</li>\n<li>Another drupal clean installation load also slow...</li>\n</ol>\n\n<p>Thanks,<br>\n~Almog</p>\n\n<p>* <em>I also tried UniformServer and it still slow, and changing the my.ini follwing the posts over here(stackexchange websites) and follwing drupal.org</em><br>\n** <em>It's seems that wordpress load fast, so it seems that the problem is with the drupal only?</em></p>\n"},{"tags":["unit-testing","performance","mstest"],"answer_count":6,"favorite_count":1,"up_vote_count":9,"down_vote_count":0,"view_count":782,"score":9,"question_id":3824762,"title":"How slow is too slow for unit tests?","body":"<p>Michael Feathers, in <em>Working Effectively With Legacy Code</em>, on pages 13-14 mentions:</p>\n\n<blockquote>\n  <p>A unit test that takes 1/10th of a\n  second to run is a slow unit test...\n  If [unit tests] don't run fast, they\n  aren't unit tests.</p>\n</blockquote>\n\n<p>I can understand why 1/10th a second is too slow if one has 30,000 tests, as it would take close to an hour to run. However, does this mean 1/11th of a second is any better? No, not really (as it's only 5 minutes faster). So a hard fast rule probably isn't perfect.</p>\n\n<p>Thus when considering how slow is too slow for a unit tests, perhaps I should rephrase the question. <strong>How long is too long for a developer to wait for the unit test suite to complete?</strong></p>\n\n<p>To give an example of test speeds. Take a look at several MSTest unit test duration timings:</p>\n\n<pre><code>0.2637638 seconds\n0.0589954\n0.0272193\n0.0209824\n0.0199389\n0.0088322\n0.0033815\n0.0028137\n0.0027601\n0.0008775\n0.0008171\n0.0007351\n0.0007147\n0.0005898\n0.0004937\n0.0004624\n0.00045\n0.0004397\n0.0004385\n0.0004376\n0.0003329\n</code></pre>\n\n<p>The average for all 21 of these unit tests comes to 0.019785 seconds. Note the slowest test is due to it using Microsoft Moles to mock/isolate the file system. </p>\n\n<p>So with this example, if my unit test suite grows to 10,000 tests, it <em>could</em> take over 3 minutes to run. </p>\n"},{"tags":["performance","mongomapper"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":10,"score":0,"question_id":13272557,"title":"Mongo Mapper taking too long for query","body":"<p>I'm using Mongo Mapper and it is taking 1.1 second to retrieve only 15 objects.\nThe document has more than 200.000 rows.\nFor my concern mongo should be fast, right? please help me .\nps: the document has indexes as well.</p>\n\n<pre><code>class Mention\ninclude MongoMapper::Document\n\ndef all (options = {})\n      mentions = Mention.paginate({\n         :order    =&gt; \"data_captura desc,\n         :per_page =&gt; 15, \n         :page     =&gt; (options[:pagination_page].to_i || 1),\n      })\n    end\nend\n</code></pre>\n"},{"tags":["performance","entity-framework","generics","dbcontext"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":28,"score":1,"question_id":13270418,"title":"Generics around Entityframework DbContext causes performance degradation?","body":"<p>I wrote a simple import/export application that transforms data from source->destination using EntityFramework and AutoMapper. It basically:</p>\n\n<ol>\n<li><strong>selects</strong> <code>batchSize</code> of records from the source table</li>\n<li>'<strong>maps</strong>' data from source->destination entity</li>\n<li><strong>add</strong> new destination entities to destination table and <strong>saves</strong> context</li>\n</ol>\n\n<p>I move around <em>500k records in under 5 minutes</em>. After I <em>refactored</em> the code <em>using generics</em> the performance <em>drops drastically to 250 records in 5 minutes.</em></p>\n\n<p><strong>Are my delegates that return <code>DbSet&lt;T&gt;</code> properties on the <code>DbContext</code> causing these problems? Or is something else going on?</strong></p>\n\n<h1>Fast non-generic code:</h1>\n\n<pre><code>public class Importer\n{        \n    public void ImportAddress()\n    {\n        const int batchSize = 50;\n        int done = 0;\n        var src = new SourceDbContext();\n\n        var count = src.Addresses.Count();\n\n        while (done &lt; count)\n        {\n            using (var dest = new DestinationDbContext())\n            {\n                var list = src.Addresses.OrderBy(x =&gt; x.AddressId).Skip(done).Take(batchSize).ToList();\n                list.ForEach(x =&gt; dest.Address.Add(Mapper.Map&lt;Addresses, Address&gt;(x)));\n\n                done += batchSize;\n\n                dest.SaveChanges();\n            }\n        }\n\n        src.Dispose();\n    }\n}\n</code></pre>\n\n<h1>(Very) slow generic code:</h1>\n\n<pre><code>public class Importer&lt;TSourceContext, TDestinationContext&gt;\n    where TSourceContext : DbContext\n    where TDestinationContext : DbContext\n{\n    public void Import&lt;TSourceEntity, TSourceOrder, TDestinationEntity&gt;(Func&lt;TSourceContext, DbSet&lt;TSourceEntity&gt;&gt; getSourceSet, Func&lt;TDestinationContext, DbSet&lt;TDestinationEntity&gt;&gt; getDestinationSet, Func&lt;TSourceEntity, TSourceOrder&gt; getOrderBy) \n        where TSourceEntity : class\n        where TDestinationEntity : class\n    {\n        const int batchSize = 50;\n        int done = 0;\n        var ctx = Activator.CreateInstance&lt;TSourceContext&gt;();\n        //Does this getSourceSet delegate cause problems perhaps?\n\n        //Added this\n        var set = getSourceSet(ctx);\n\n        var count = set.Count(); \n\n        while (done &lt; count)\n        {\n            using (var dctx = Activator.CreateInstance&lt;TDestinationContext&gt;())\n            {\n                var list = set.OrderBy(getOrderBy).Skip(done).Take(batchSize).ToList(); \n                //Or is the db-side paging mechanism broken by the getSourceSet delegate?\n                //Added this\n                var destSet = getDestinationSet(dctx);\n                list.ForEach(x =&gt; destSet.Add(Mapper.Map&lt;TSourceEntity, TDestinationEntity&gt;(x)));\n\n                done += batchSize;\n                dctx.SaveChanges();\n            }\n        }\n\n        ctx.Dispose();\n    }\n}\n</code></pre>\n"},{"tags":["windows","performance","driver","wdm"],"answer_count":2,"favorite_count":2,"up_vote_count":6,"down_vote_count":0,"view_count":255,"score":6,"question_id":8627590,"title":"Will moving code into kernel space give more precise timing?","body":"<p>Background information:</p>\n\n<p>I presently have a hardware device that connects to the USB port.  The hardware device is responsible sending out precise periodic messages onto various networks that it, in turn, connects too.  Inside the hardware device I have a couple Microchip dsPICs.  There are two modes of operation.  </p>\n\n<p>One scenario is where send simple \"jobs\" down to the dsPICs that, in turn, can send out the precise messages with .001ms accuracy.  This architecture is not ideal for more complex messaging where we need to send a periodic packet that changes based on events going on within the PC application.  So we have a second mode of operation where our PC application will send the periodic messages and the dsPICs simply convert and transmit in response.  All this, by the way, is transparent to the end user of our software.  Our hardware device is a test tool used in the automotive field.</p>\n\n<p>Currently, we use a USB to serial chip from FTDI and the FTDI Windows drivers to interface the hardware to our PC software.</p>\n\n<p>The problem is that in mode two where we send messages from the PC, the best we are able to achieve is around 1ms on average hardware range.  We are subjected to Windows kernel pre-emption.  I've tried a number of \"tricks\" to improve things such as:</p>\n\n<ol>\n<li>Making sure our reader &amp; writer threads live on seperate CPU affinities when possible.</li>\n<li>Increasing the thread priority of the writer while reducing that of the reader.</li>\n<li>Informing the user to turn off screen saver and other applications when using our software.</li>\n<li>Replacing createthread calls with CreateTimerQueueTimer calls.</li>\n</ol>\n\n<p>All our software is written in C/C++.  I'm very familiar and comfortable with advanced Windows programming; such as IO Completions, Overlapped I/O, lockless thread queues (really a design strategy), sockets, threads, semaphores, etc...</p>\n\n<p>However, I know nothing about Windows driver development.  I've read through a few papers on KMDF vs. UDMF vs. WDM.  </p>\n\n<p>I'm hoping a seasoned Windows kernel mode driver developer will respond here... </p>\n\n<p>The next rev. of our hardware has the option to replace the FTDI chip and use either the dsPIC's USB interface or, possibly, port the open source Linux FTDI stuff to Windows and continue to use the FTDI chip within our custom driver.  I think by going to a kernel mode driver on the PC side, I can establish a kernel driver that can send out periodic messages at more precise intervals without preemption and/or possibly taking advantage of DMA.</p>\n\n<p>We have a competitor in our business who I think does exactly something similar with their tools.  As far as I know, user space applications can not schedule a thread any better than 1ms.  We currently use timeGetTime in a thread.  I've experiemented with timer queues (via CreateTimerQueueTimer) with no real improvement.</p>\n\n<p>Is a WDM the correct approach to achieve more precise timing?</p>\n\n<p>Our competitor some how is achieveing very precise timing from Windows driven signals to their hardware and they do load a kernel driver (.sys) and their device runs over USB2.0 as does ours.</p>\n\n<p>If WDM is the way to go, can I get some advise on what kernel functions I should be studying for setting up the timings?\nThanks for reading</p>\n"},{"tags":["c#","performance"],"answer_count":6,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":1395,"score":4,"question_id":2443164,"title":"Does variable name length matter for performance C#?","body":"<p>I've been wondering if using long descriptive variable names in WinForms C# matters for performance? I'm asking this question since in AutoIt v3 (interpreted language) it was brought up that having variables with short names like <code>aa</code> instead of <code>veryLongVariableName</code> is much much faster (when program is bigger then 5 liner). I'm wondering if it's the same in C#?</p>\n"},{"tags":["sql","performance"],"answer_count":3,"favorite_count":1,"up_vote_count":0,"down_vote_count":2,"view_count":42,"score":-2,"question_id":13270502,"title":"sql - what is the difference between these two queries?","body":"<pre><code>select a.Name from rat. Assessment a where a.AssessmentId = '3'\nselect a.Name from rat. Assessment a where a.AssessmentId = 3\n</code></pre>\n\n<p>whats the difference performance wise ?\nis it slow for the first one ?</p>\n\n<p>How can i know if sql server has casted the the column or value to match the the column type or value type ?</p>\n\n<p>I saw the execution plan. cant understand much.</p>\n"},{"tags":["java","performance","garbage-collection","java1.4"],"answer_count":3,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":3354,"score":3,"question_id":9005632,"title":"UseConcMarkSweepGC vs UseParallelGC","body":"<p>I'm currently having problems with very long garbage collection times. please see the followig. My current setup is that I'm using a -Xms1g and -Xmx3g. my application is using java 1.4.2. I don't have any garbage collection flags set. by the looks of it, 3gb is not enough and I really have a lot of objects to garbage collect.</p>\n\n<p>question:</p>\n\n<p>should I change my garbage collection algorithm?\nwhat should i use? is it better to use <code>-XX:+UseParallelGC or -XX:+UseConcMarkSweepGC</code></p>\n\n<p>or should i use this combination</p>\n\n<pre><code>-XX:+UseParNewGC -XX:+UseConcMarkSweepGC\n</code></pre>\n\n<p>the ones occupying the memory are largely reports data and not cache data. also, the machine has 16gb memory and I plan to increase the heap to 8gb. </p>\n\n<p>What are the difference between the two options as I still find it hard to understand.\nthe machine has multiple processors. I can take hits of up to 5 seconds but 30 to 70 seconds is really hard.</p>\n\n<p>Thanks for the help.</p>\n\n<pre><code>  Line 151493: [14/Jan/2012:11:47:48] WARNING ( 8710): CORE3283: stderr: [GC 1632936K-&gt;1020739K(2050552K), 1.2462436 secs]\n    Line 157710: [14/Jan/2012:11:53:38] WARNING ( 8710): CORE3283: stderr: [GC 1670531K-&gt;1058755K(2050552K), 1.1555375 secs]\n    Line 163840: [14/Jan/2012:12:00:42] WARNING ( 8710): CORE3283: stderr: [GC 1708547K-&gt;1097282K(2050552K), 1.1503118 secs]\n    Line 169811: [14/Jan/2012:12:08:02] WARNING ( 8710): CORE3283: stderr: [GC 1747074K-&gt;1133764K(2050552K), 1.1017273 secs]\n    Line 175879: [14/Jan/2012:12:14:18] WARNING ( 8710): CORE3283: stderr: [GC 1783556K-&gt;1173103K(2050552K), 1.2060946 secs]\n    Line 176606: [14/Jan/2012:12:15:42] WARNING ( 8710): CORE3283: stderr: [Full GC 1265571K-&gt;1124875K(2050552K), 25.0670316 secs]\n    Line 184755: [14/Jan/2012:12:25:53] WARNING ( 8710): CORE3283: stderr: [GC 2007435K-&gt;1176457K(2784880K), 1.2483770 secs]\n    Line 193087: [14/Jan/2012:12:37:09] WARNING ( 8710): CORE3283: stderr: [GC 2059017K-&gt;1224285K(2784880K), 1.4739291 secs]\n    Line 201377: [14/Jan/2012:12:51:08] WARNING ( 8710): CORE3283: stderr: [Full GC 2106845K-&gt;1215242K(2784880K), 30.4016208 secs]\n\n\nxaa:1: [11/Oct/2011:16:00:28] WARNING (17125): CORE3283: stderr: [Full GC 3114936K-&gt;2985477K(3114944K), 53.0468651 secs] --&gt; garbage collection occurring too often as noticed in the time. garbage being collected is quite low and if you would notice is quite close the the heap size. during the 53 seconds, this is equivalent to a pause.\nxaa:2087: [11/Oct/2011:16:01:35] WARNING (17125): CORE3283: stderr: [Full GC 3114943K-&gt;2991338K(3114944K), 58.3776291 secs]\nxaa:3897: [11/Oct/2011:16:02:33] WARNING (17125): CORE3283: stderr: [Full GC 3114940K-&gt;2997077K(3114944K), 55.3197974 secs]\nxaa:5597: [11/Oct/2011:16:03:00] WARNING (17125): CORE3283: stderr: [Full GC[Unloading class sun.reflect.GeneratedConstructorAccessor119]\nxaa:7936: [11/Oct/2011:16:04:36] WARNING (17125): CORE3283: stderr: [Full GC 3114938K-&gt;3004947K(3114944K), 55.5269911 secs]\nxaa:9070: [11/Oct/2011:16:05:53] WARNING (17125): CORE3283: stderr: [Full GC 3114937K-&gt;3012793K(3114944K), 70.6993328 secs]\n</code></pre>\n"},{"tags":["php","xml","json","performance"],"answer_count":7,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":5178,"score":4,"question_id":993282,"title":"PHP: is JSON or XML parser faster?","body":"<p>I'm building classes that interface with the Twitter API, and I'm wondering whether PHP's built-in XML or JSON parser is faster? Twitter will send me the same data in either format, so  PHP performance will determine my choice. I'm using php_apc, so you can disregard parse time and assume I'm running off bytecode.</p>\n\n<p>Thanks!</p>\n\n<p>more: I'm just looking to get associative arrays from the data. I'm not doing tree walking, node iteration or anything too complex. The format will always be the same. (I hope!)</p>\n"},{"tags":["performance","postgresql"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":28,"score":1,"question_id":13266869,"title":"Best config options for PostgreSQL (development machine)","body":"<p>My question is simple: I'm using PostgreSQL 9.1 (will use 9.2 some time later) on my development machine (Linux amd64, quad core, 8GB ram). I've already turned fsync off for speed (because I don't care if my computer crashes, I can rebuild the schema and data at anytime), but I'm a bit confused about tuning the other factory defaults. I want it to be as fast as possible.</p>\n\n<p>Distribution: Linux Mint Debian Edition (latest update pack)</p>\n"},{"tags":["performance","jmeter"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":46,"score":0,"question_id":13199383,"title":"JMeter different results during replay of test","body":"<p>I have a strange problem with JMeter.<br>\nI've made recording of some sort of web application without any problems. Problem appears during playback of test.\nFor some reason I receive different results during playback than during recording.</p>\n\n<p>When I compare Http Request made during recording and playback I don't see a single difference (except for some security token which I'm extracting from earlier requests and passing as parameter).\nTo be more exact during recording I receive a response with a big body (>5kB), and during playback body of response is empty. Response code is 200 (OK).\nThis body contains crucial data from database, so I'm afraid that measurement made by this JMeter script will not reflect actual behavior of application, simply I will not measure what I really need.</p>\n\n<p>Now my questions: </p>\n\n<ol>\n<li>is there some tool or JMeter plug-in which will allow more effectively see contents of HTTP requests and its responses? It would be great If I could compare of requests made during recording and playback. So far I used two listeners: \"View Results Tree\". I've sandwiched between them to compare request from recording and playback.</li>\n<li>is there some known bug in JMeter which could explain the difference? For example something related to recording process?</li>\n</ol>\n\n<p>Here is example of request:</p>\n\n<pre><code>POST http://10.133.27.81:8080/c/portal/render_portlet\n\nPOST data:\np_l_id=69210&amp;p_p_id=blank_WAR_Blank_INSTANCE_iNM3&amp;p_p_action=0&amp;p_p_state=normal&amp;p_p_mode=view&amp;p_p_col_id=column-2&amp;p_p_col_pos=1&amp;p_p_col_count=2\n\n[no cookies]\n\nRequest Headers:\nConnection: keep-alive\nContent-Type: application/x-www-form-urlencoded\nAccept-Language: pl\nAccept: */*\nUser-Agent: Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)\ncsrf_token: 1GXK-0QD7-GFPJ-JLDG-JP2G-J390-BFLG-7LL7\nPragma: no-cache\nMethod: POST /c/portal/render_portlet HTTP/1.1\nX-Requested-With: OWASP CSRFGuard Project\nReferer: http://10.133.27.81:8080/group/bou\nAccept-Encoding: gzip, deflate\nContent-Length: 143\nHost: 10.133.27.81:8080\n</code></pre>\n\n<hr>\n\n<p><strong>Update</strong>: to make sure which headers or parameters are constant I made 4 recordings of same test case during different sessions and compared them, so I'm quite sure that only <code>csrf_token</code> has to be field with value fetched from other request. I've added debug sampler to verify that this value is fetched properly.</p>\n\n<hr>\n\n<p><strong>Update 2</strong>: Problem found.<br>\nThere where two problems:</p>\n\n<ol>\n<li>There is a bug in JMeter when you do a search (Ctrl-F) it searches the whole project except for <code>HTTP Header Menager</code>s and my request contained <code>csrf_token</code> inside of header (I detected that before posting this question). Making a search in xml using text editor was good workaround for that.</li>\n<li>when I try to find source of problems, before I've found problem number one, I've added a new problem by removing a <code>HTTP Cookie Manager</code> (I'm blaming myself and IE for this).</li>\n</ol>\n\n<p>Generally changing Internet Explorer to FireFox with HttpFox add-on help to spot the problem.</p>\n\n<p>Thanks everyone for support.</p>\n\n<p>Marek</p>\n"},{"tags":["performance",".htaccess","optimization","cdn"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":9,"score":-1,"question_id":13269144,"title":"Using a not real CDN for images is still useful?","body":"<p>I have a site with heavy traffic and I've read that moving images on another server could improve performances, best if this second server is on a CDN.</p>\n\n<p>www.mysite.com ---> web site\nimages.mysite.com ---> server for images</p>\n\n<p>if images aren't really on a different server but the images.mysite.com is just a subdirectory on  the same server, same web site, but handled only with htaccess redirects... is this still useful to split requests to server and improve speed for browser?</p>\n"},{"tags":["c++","python","performance","list","boost-python"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":49,"score":2,"question_id":12929196,"title":"How to speed up Boost::Python::Extract when passing a list from python to a C++ vector","body":"<p>I'm new to boost.python and have made a simple function for passing a list from python to a C++ vector:</p>\n\n<pre><code>void SetXValues(boost::python::list xl){\n    int n = len((xl));\n    xvals.resize(n);\n    for(unsigned int i=0; i&lt;n; i++){\n    xvals[i] = boost::python::extract&lt;double&gt;((xl)[i]);\n    }\n}\n</code></pre>\n\n<p>xvals is an C++ STL vector. This function works and I can load the python list into C++ but it seems extremely slow. </p>\n\n<p>A small speed test I did was to write a binning algorithm in C++ and in pure Python. The results show that the C++ method is only 5x faster when the time to pass the data from Python is included but of course the binning algorithm alone is considerably faster (74x).</p>\n\n<p>So is there any way to improve the function above to make it more efficient? </p>\n"},{"tags":["javascript","ajax","performance","google-chrome","heap"],"answer_count":0,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":71,"score":6,"question_id":13240866,"title":"JavaScript Heap Size and Chrome Increasing","body":"<p>I have a single web page application that is all JavaScript.  I noticed the JavaScript heap size goes up on each AJAX call that returns a new view.  Is there something I should be doing to clean up the older views?</p>\n"},{"tags":["performance","r","join","merge","data.table"],"answer_count":3,"favorite_count":22,"up_vote_count":28,"down_vote_count":0,"view_count":3670,"score":28,"question_id":4322219,"title":"What's the fastest way to merge/join data.frames in R?","body":"<p>For example (not sure if most representative example though):</p>\n\n<pre><code>N &lt;- 1e6\nd1 &lt;- data.frame(x=sample(N,N), y1=rnorm(N))\nd2 &lt;- data.frame(x=sample(N,N), y2=rnorm(N))\n</code></pre>\n\n<p>This is what I've got so far:</p>\n\n<pre><code>d &lt;- merge(d1,d2)\n# 7.6 sec\n\nlibrary(plyr)\nd &lt;- join(d1,d2)\n# 2.9 sec\n\nlibrary(data.table)\ndt1 &lt;- data.table(d1, key=\"x\")\ndt2 &lt;- data.table(d2, key=\"x\")\nd &lt;- data.frame( dt1[dt2,list(x,y1,y2=dt2$y2)] )\n# 4.9 sec\n\nlibrary(sqldf)\nsqldf()\nsqldf(\"create index ix1 on d1(x)\")\nsqldf(\"create index ix2 on d2(x)\")\nd &lt;- sqldf(\"select * from d1 inner join d2 on d1.x=d2.x\")\nsqldf()\n# 17.4 sec\n</code></pre>\n"},{"tags":["java","performance","iteration"],"answer_count":8,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":6123,"score":4,"question_id":1006395,"title":"Fastest way to iterate in Java","body":"<p>In Java, is it faster to iterate through an array the old-fashioned way,</p>\n\n<pre><code>for (int i = 0; i &lt; a.length; i++)\n    f(a[i]);\n</code></pre>\n\n<p>Or using the more concise form,</p>\n\n<pre><code>for (Foo foo : a)\n    f(foo);\n</code></pre>\n\n<p>For an ArrayList, is the answer the same?</p>\n\n<p>Of course for the vast bulk of application code, the answer is it makes no discernible difference so the more concise form should be used for readability. However the context I'm looking at is heavy duty technical computation, with operations that must be performed billions of times, so even a tiny speed difference could end up being significant.</p>\n"},{"tags":["c++","c","performance","gcc","sse"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":76,"score":1,"question_id":13267354,"title":"segmentation fault in SSE when -O3 is on","body":"<p>I found a very strange behaviour when using gcc's <strong><em>-O3</em></strong> or <strong><em>-O2</em></strong>  option.</p>\n\n<p>When my program is running under debug mode (<strong><em>-g</em></strong>), it is fine; but it raises a segmentation fault when I turned on <strong><em>-O3</em></strong> or <strong><em>-O2</em></strong>.</p>\n\n<p>The segmentation happens when it is running a function with <strong>SSE2</strong> macro inside; like</p>\n\n<pre><code>_m128i polynomial = _mm_set1_epi8(0x1d)\n</code></pre>\n\n<p>This is only part of the code.</p>\n\n<p>I think I have already eliminate the situation of address alignment on 16 bytes. It's so wired that the -g mode and the <strong><em>-O2</em></strong> or <strong><em>-O3</em></strong> mode behaves differently.</p>\n\n<p>Actually, I am not sure the bug is related to SSE2 or not.</p>\n\n<p>I am using <strong><em>gcc 4.4.3</em></strong>.</p>\n\n<p><strong>Have you encountered the same problem?</strong></p>\n\n<p><strong>Or can you gave me some suggestion on how to deal with it?</strong></p>\n"},{"tags":["android","performance","debugging","sha256"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":14,"score":0,"question_id":13267405,"title":"Spongycastle SHA256 update function is too slow in Debug session","body":"<p>I am trying to verify the signature of a file in our apk package. The file is about 1.5MB and \nI am first reading the file into a byte array and then start the signature verfication with the following code:</p>\n\n<pre class=\"lang-java prettyprint-override\"><code>// verify signature\nSignature sig = Signature.getInstance(\"SHA256withRSA\", new BouncyCastleProvider());\nsig.initVerify(pubKey);\nsig.update(dataBytes);\nreturn sig.verify(sigBytes); \n</code></pre>\n\n<p>When I run my app on the device normally then it takes about 2.5 seconds to pass the signature verification, but in debug session, it takes about 2.5 minutes. I checked where it does take so long to perform, and I saw that update function of SHA256Digest class uses this time to calculate.</p>\n\n<p>Did anyone have a similar problem, or any idea what might cause this performance issue during debug session.</p>\n"},{"tags":["python","performance","timer","pypy"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":29,"score":1,"question_id":13267316,"title":"Python: process average timing, first one/two are much slower","body":"<p>I'm writing some program with sorting algorithms comparsion using Python. I want to measure average sorting time. I have a problem with first measurement.</p>\n\n<p>This:</p>\n\n<pre><code>for i in xrange(self.repeats):\n    # random list generator\n    data_orig = [random.randint(0, self.size - 1) for x in xrange(self.size)]\n\n    sorter = self.class_()\n    data = data_orig[:]\n    debug(\"%s for data size: %d, try #%d\" % (sorter.__class__.__name__, self.size, i+1))\n    t1 = time.clock()\n    sorter.sort(data)\n    t2 = time.clock()\n    debug(\"Took: %0.4fms, shifts: %d, comparisons: %d\" % ((t2-t1)*1000.0, sorter.shifts, sorter.comps))\n</code></pre>\n\n<p><code>class_</code> is a reference to InsertionSort class.\nFor size = 1000 and 5 repeats I get following results:</p>\n\n<pre><code>InsertionSort for data size: 1000, try #1\nTook: 39.5341ms, shifts: 254340, comparisons: 255331\nInsertionSort for data size: 1000, try #2\nTook: 6.0765ms, shifts: 250778, comparisons: 251772\nInsertionSort for data size: 1000, try #3\nTook: 6.9946ms, shifts: 254189, comparisons: 255180\nInsertionSort for data size: 1000, try #4\nTook: 6.7421ms, shifts: 252162, comparisons: 253156\nInsertionSort for data size: 1000, try #5\nTook: 5.9584ms, shifts: 241412, comparisons: 242404\n</code></pre>\n\n<p>For every sorting algorithm and every time I run program first result is bigger than the others. I run it with PyPy (with Python it seems OK, but it's MUCH slower).</p>\n\n<p>I know I can simply ommit first results but this solution doesn't satisfies me :-)</p>\n\n<p>Any ideas?</p>\n"},{"tags":["java","performance","math","sqrt"],"answer_count":3,"favorite_count":1,"up_vote_count":2,"down_vote_count":1,"view_count":99,"score":1,"question_id":13263948,"title":"Fast sqrt in Java at the expense of accuracy","body":"<p>I am looking for a fast square root implementation in Java for double values in the input range of [0, 2*10^12]. For any value in this range, the precision should be upto 5 decimal places. In other words, the result can differ from the <code>Math.sqrt()</code> method after 5 decimal places. However, this method needs to be much faster than <code>Math.sqrt()</code>.</p>\n\n<p>Any ideas? Thanks!</p>\n"},{"tags":["c#",".net","winforms","performance","logging"],"answer_count":7,"favorite_count":4,"up_vote_count":1,"down_vote_count":0,"view_count":6076,"score":1,"question_id":5057567,"title":"How to do logging in c#?","body":"<p>I would like to implement logging in my application but would rather not use any outside frameworks like log4net.</p>\n\n<p>So I would like to do something like Dos's echo to file. What is the most effective way to do it?</p>\n\n<p>Is there a way to log unhandled exceptions logged without using an outside framework?</p>\n"},{"tags":["php","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":37,"score":-1,"question_id":13265093,"title":"Using too many if blocks is good ? what are possible alternatives?","body":"<p>I have been designing a shopping website lately in PHP. Now in Checkout page i have to check many times if the user is logged in. i.e If the user is logged in <em>hide the Signin div</em> and <em>show the Shipping div</em>.</p>\n\n<pre><code>&lt;section class=\"panels &lt;?php if(!isset($_SESSION['username'])) echo\"unactivepanel\"; else echo\"activepanel\";?&gt;\" id=\"Sign In\"&gt;\n        &lt;div class=\"wrapper1\" &gt;\n            &lt;ul&gt;\n            &lt;li&gt;&lt;input type=\"email\" placeholder=\"Enter your email address\" required/&gt;&lt;/li&gt;\n            &lt;li class=\"inf\"&gt;(will only be used for sending you order information.)&lt;/li&gt;\n            &lt;fieldset&gt;&lt;legend class=\"orb\"&gt; OR&lt;/legend&gt;\n            &lt;li&gt;&lt;input type=\"button\" value=\"Sign In to booksmore\" class=\"sb si llb\" /&gt;&lt;/li&gt;&lt;/fieldset&gt;\n            &lt;fieldset&gt;&lt;legend class=\"orb\"&gt; OR&lt;/legend&gt;\n            &lt;li&gt;Sign In using any of following service:&lt;/li&gt;\n            &lt;li&gt;\n                &lt;div class=\"box1 fc\"&gt;Facebook&lt;/div&gt;\n                &lt;div class=\"box1 tw\"&gt;Twitter&lt;/div&gt;\n                &lt;div class=\"box1 gg\"&gt;Google&lt;/div&gt;\n                &lt;div class=\"box1 oi\"&gt;Open Id&lt;/div&gt;\n            &lt;/li&gt;&lt;/fieldset&gt;\n\n            &lt;/ul&gt;\n        &lt;/div&gt;\n    &lt;/section&gt;\n    &lt;section class=\"panels &lt;?php if(!isset($_SESSION['username'])) echo\"unactivepanel\"; else echo\"activepanel\";?&gt;unactivepanel\" id=\"Shipping\"&gt;&lt;/section&gt;\n    &lt;section class=\"panels unactivepanel\" id=\"Confirm\"&gt;&lt;/section&gt;\n</code></pre>\n\n<p>What i have studied is using too many <strong>if</strong> block slows down the executions speed, So am I right using too many <strong>if</strong> blocks ? if I am not coding it right(<em>what i feel right now</em>) then what may be the possible alternatives to <strong>if</strong> blocks ? (I was thinking of using <strong>switch</strong> block.)</p>\n"},{"tags":["java","performance","swing"],"answer_count":2,"favorite_count":0,"up_vote_count":9,"down_vote_count":0,"view_count":2942,"score":9,"question_id":646089,"title":"Java Robot createScreenCapture performance","body":"<p>I need to grab a series of screenshots and concatenate them into a movie. I'm trying to use the java Robot class to capture the screen. </p>\n\n<p>But the createScreenCapture() method takes more than 1 second on my machine. I can't even get 1 fps. Is there a way to speed it up? Or is there any other API?</p>\n\n<p>Edit: It is allocating a buffered image.</p>\n\n<p>BufferedImage image = robot.createScreenCapture(screen);\n//Save the screenshot as a jpg<br />\nFile file = new File(\"images/screen\"+ index + \".jpg\");<br />\nImageIO.write(image, \"jpg\", file);\nindex++;</p>\n\n<p>Writing it to the jpg file takes about 200 ms where as getting BufferedImage takes about 1400ms.</p>\n"},{"tags":["performance","algorithm","statistics","traffic"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":25,"score":0,"question_id":13264027,"title":"Negotiate traffic speed for mobile applications - algorithm?","body":"<p>we have 2 devices - iPad A &amp; iPad B. Both use the same application that is designed to send audio/video data. </p>\n\n<p>Sometimes devices are located far away from each other and traffic goes through internet instead of local network.</p>\n\n<p>In that case, we can't guarantee the same network speed on both ends. One device may send 512Kb/s another one 5 times less.</p>\n\n<p>Question is -</p>\n\n<p>Is it possible to measure traffic between both devices in real time and scale up/down when needed. For example if I know that device A has only 256kb/s incoming connection at this given moment, then device B should scale down from 1mb/s to 256kb/s automatically</p>\n\n<p>Right now the solution we have implemented is to detect the traffic/speed degradation by recognizing lost packets. But it's not perfect.</p>\n\n<p>Maybe you have read something or have an idea in mind?</p>\n\n<p>thanks in Advance,\nDmitry</p>\n"},{"tags":["asp.net","performance","keep-alive","yslow"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":7,"score":0,"question_id":13263834,"title":"How can I enable \"keep-alive\" if I'm with a web farm/cluster as a webhost, and they don't honor it?","body":"<p>I'm with a webhost for my company's website that has 47 pages (asp.net 4/vb).  All speed tests, Google, Yslow, Gtmetrix, etc., want me to <strong>enable keep-alive</strong>.  I have already set http header requests to expire after 30 minutes via IIS7.</p>\n\n<p>I submitted a ticket to my webhost, and they say they don't offer keep-alive.  Does that just mean I'm stuck if I stay with this webhost, or is there something I can do on my own, regardless of my webhost?  Thanks for any guidance!</p>\n"},{"tags":["performance","file","caching","disk"],"answer_count":5,"favorite_count":5,"up_vote_count":21,"down_vote_count":0,"view_count":7130,"score":21,"question_id":478340,"title":"Clear file cache to repeat performance testing","body":"<p>What tools are available to either completely clear, or selectively remove cached information about file and directory contents?</p>\n\n<p>The application that I'm developing is a specialised compression utility, and is expected to do a lot of work reading and writing files that the operating system hasn't touched recently, and whose disk blocks are unlikely to be cached.</p>\n\n<p>I wish to remove the variability I see in IO time when I repeat the task of profiling different strategies for doing the file processing work.</p>\n\n<p>I'm primarily interested in solutions for Windows XP, as that is my main development machine, but I can also test using linux, and so am interested in answers for that environment too.</p>\n\n<p>I tried SysInternals <a href=\"http://technet.microsoft.com/en-us/sysinternals/bb897561.aspx\" rel=\"nofollow\">CacheSet</a>, but clicking \"Clear\" doesn't result in a measurable increase (restoration to timing after a cold-boot) in the time to re-read files I've just read a few times.</p>\n"},{"tags":["c++","performance","visual-c++","dll","random"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":68,"score":0,"question_id":13262498,"title":"Combining rand() and srand() for better performance","body":"<p>So, I am making a DLL that I want to have a function to generate random numbers. I was wondering which of these options is more efficient (performance wise).</p>\n\n<p>This one is just making a function in a DLL that allows me to get a random number.</p>\n\n<pre><code>int getRand(unsigned int seed) {\n    int rNum;  // Random Number.\n\n    srand(seed);\n    rNum = (rand() % // Whatever I need here.\n}\n</code></pre>\n\n<p>Or, would just using <code>srand(time(nullptr))</code> and <code>rand()</code> in the application be better in performance?</p>\n\n<p>Thanks,</p>\n\n<p>Johnny P.</p>\n"},{"tags":["c++","performance","visual-c++","atomic","compare-and-swap"],"answer_count":1,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":79,"score":3,"question_id":13213113,"title":"Should std::atomic<int*>::load be doing a compare-and-swap loop?","body":"<p><strong>Summary</strong>: I had expected that <code>std::atomic&lt;int*&gt;::load</code> with <code>std::memory_order_relaxed</code> would be close to the performance of just loading a pointer directly, at least when the loaded value rarely changes. I saw far worse performance for the atomic load than a normal load on Visual Studio C++ 2012, so I decided to investigate. It turns out that the atomic load is implemented as a <a href=\"http://en.wikipedia.org/wiki/Compare-and-swap\" rel=\"nofollow\">compare-and-swap</a> loop, which I suspect is not the fastest possible implementation.</p>\n\n<p><strong>Question</strong>: Is there some reason that <code>std::atomic&lt;int*&gt;::load</code> needs to do a compare-and-swap loop?</p>\n\n<p><strong>Background</strong>: I believe that MSVC++ 2012 is doing a compare-and-swap loop on atomic load of a pointer based on this test program:</p>\n\n<pre><code>#include &lt;atomic&gt;\n#include &lt;iostream&gt;\n\ntemplate&lt;class T&gt;\n__declspec(noinline) T loadRelaxed(const std::atomic&lt;T&gt;&amp; t) {\n  return t.load(std::memory_order_relaxed);\n}\n\nint main() {\n  int i = 42;\n  char c = 42;\n  std::atomic&lt;int*&gt; ptr(&amp;i);\n  std::atomic&lt;int&gt; integer;\n  std::atomic&lt;char&gt; character;\n  std::cout\n    &lt;&lt; *loadRelaxed(ptr) &lt;&lt; ' '\n    &lt;&lt; loadRelaxed(integer) &lt;&lt; ' '\n    &lt;&lt; loadRelaxed(character) &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>\n\n<p>I'm using a <code>__declspec(noinline)</code> function in order to isolate the assembly instructions related to the atomic load. I made a new MSVC++ 2012 project, added an x64 platform, selected the release configuration, ran the program in the debugger and looked at the disassembly. Turns out that both <code>std::atomic&lt;char&gt;</code> and <code>std::atomic&lt;int&gt;</code> parameters end up giving the same call to <code>loadRelaxed&lt;int&gt;</code> - this must be something the optimizer did. Here is the disassembly of the two loadRelaxed instantiations that get called:</p>\n\n<p><strong><code>loadRelaxed&lt;int * __ptr64&gt;</code></strong></p>\n\n<pre><code>000000013F4B1790  prefetchw   [rcx]  \n000000013F4B1793  mov         rax,qword ptr [rcx]  \n000000013F4B1796  mov         rdx,rax  \n000000013F4B1799  lock cmpxchg qword ptr [rcx],rdx  \n000000013F4B179E  jne         loadRelaxed&lt;int * __ptr64&gt;+6h (013F4B1796h)  \n</code></pre>\n\n<p><strong><code>loadRelaxed&lt;int&gt;</code></strong></p>\n\n<pre><code>000000013F3F1940  prefetchw   [rcx]  \n000000013F3F1943  mov         eax,dword ptr [rcx]  \n000000013F3F1945  mov         edx,eax  \n000000013F3F1947  lock cmpxchg dword ptr [rcx],edx  \n000000013F3F194B  jne         loadRelaxed&lt;int&gt;+5h (013F3F1945h)  \n</code></pre>\n\n<p>The instruction <code>lock cmpxchg</code> is atomic <a href=\"http://en.wikipedia.org/wiki/Compare-and-swap\" rel=\"nofollow\">compare-and-swap</a> and we see here that the code for atomically loading a <code>char</code>, an <code>int</code> or an <code>int*</code> is a compare-and-swap loop. I also built this code for 32-bit x86 and that implementation is still based on <code>lock cmpxchg</code>.</p>\n\n<p><strong>Question</strong>: Is there some reason that <code>std::atomic&lt;int*&gt;::load</code> needs to do a compare-and-swap loop?</p>\n"},{"tags":["c++","performance","boost","c++11","iterator"],"answer_count":1,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":193,"score":5,"question_id":11281298,"title":"Boost any_range performance: std::prev(iterator) versus --iterator","body":"<p>I've recently begun to prefer the free functions <code>std::next</code> and <code>std::prev</code> to explicitly copying and incrementing/decrementing iterators. Now, I am seeing weird behavior in a pretty specific case, and I would appreciate any help demystifying it.</p>\n\n<p>I have an interpolation/extrapolation function operating on a <code>boost::any_range</code> of some <code>X_type</code>. The full definition of the range type is:</p>\n\n<pre><code>boost::any_range &lt;\n    const X_type,\n    boost::random_access_traversal_tag,\n    const X_type,\n    std::ptrdiff_t\n&gt;\n</code></pre>\n\n<p>The <code>any_range</code>, in this particular case, is assigned from an <code>iterator_range</code> holding two pointers to <code>const X_type</code>, which serves as an <code>X_type</code> view of about half of the <code>data()</code> area of a <code>vector&lt;char&gt;</code>.</p>\n\n<p>Compiling my application in MSVC 2010, everything works just fine.\nCompiling the same code in MinGW g++ 4.7.0, it seemed to hang in one particular location, which I've then narrowed down to this (slightly abbreviated):</p>\n\n<pre><code>// Previously ensured conditions:\n// 1) xrange is nonempty;\n// 2) yrange is the same size as xrange.\n\nauto x_equal_or_greater =\n    std::lower_bound(std::begin(xrange),std::end(xrange),xval);\n\nif (x_equal_or_greater == std::end(xrange))\n{\n    return *yit_from_xit(std::prev(x_equal_or_greater),xrange,yrange);\n}\n</code></pre>\n\n<p>Stepping through the code in gdb, I found out it wasn't getting stuck, just taking a very long time to return from the single <code>std::prev</code> call - which in libstdc++ is implemented in terms of <code>std::advance</code> and ultimately the <code>+=</code> operator.</p>\n\n<p>By merely replacing the <code>return</code> line with:</p>\n\n<pre><code>auto xprev=x_equal_or_greater;\n--xprev;\nreturn *yit_from_xit(xprev,xrange,yrange);\n</code></pre>\n\n<p>Performance is great again, and there's virtually no delay.</p>\n\n<p>I am aware of the overhead of using type-erased iterators (those of <code>any_range</code>), but even so, are the two cases above really supposed to carry such different costs? Or am I doing something wrong?</p>\n"},{"tags":["vb.net","performance","vb6"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":51,"score":1,"question_id":13258103,"title":"API call in VB.NET much slower than in VB6","body":"<p>Can somebody explain how it can be that the same API call returns so much quicker with VB6 than with VB.NET?</p>\n\n<p>Here is my VB6 code:</p>\n\n<pre><code>Public Declare Function GetWindowTextLength Lib \"user32\" Alias \"GetWindowTextLengthA\" (ByVal hWnd As Long) As Long\nPublic Declare Function GetWindowText Lib \"user32\" Alias \"GetWindowTextA\" (ByVal hWnd As Long, ByVal lpString As String, ByVal cch As Long) As Long\n\n\nPublic Function GetWindowTextEx(ByVal uHwnd As Long) As String\n\nDim lLen&amp;\nlLen = GetWindowTextLength(uHwnd) + 1\n\nDim sTemp$\nsTemp = Space(lLen)\n\nlLen = GetWindowText(uHwnd, sTemp, lLen)\n\nDim sRes$\nsRes = Left(sTemp, lLen)\n\nGetWindowTextEx = sRes\n\nEnd Function\n</code></pre>\n\n<p>And here is my VB.NET code:</p>\n\n<pre><code>Private Declare Function GetWindowText Lib \"user32\" Alias \"GetWindowTextA\" (ByVal hwnd As Integer, ByVal lpWindowText As String, ByVal cch As Integer) As Integer\n\n    Dim sText As String = Space(Int16.MaxValue)\n    GetWindowText(hwnd, sText, Int16.MaxValue)\n</code></pre>\n\n<p>I ran each version 1000 times. </p>\n\n<p>The VB6 version needed 2.04893359351538 ms.\nThe VB.NET version needed 372.1322491699365 ms.</p>\n\n<p>Both Release and Debug version are about the same.</p>\n\n<p>What is happening here?</p>\n"},{"tags":["database","performance","sqlalchemy","uuid","sharding"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":44,"score":4,"question_id":13167799,"title":"SQLAlchemy, UUIDs, Sharding, and AUTO_INCREMENT primary key... how to get them to work together?","body":"<p>I have a question pertaining to SQLAlchemy, database sharding, and UUIDs for you fine folks. </p>\n\n<p>I'm currently using MySQL in which I have a table of the form:</p>\n\n<pre><code>CREATE TABLE foo (\n    added_id INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    id BINARY(16) NOT NULL,\n    ... other stuff ...\n    UNIQUE KEY(id)\n);\n</code></pre>\n\n<p>A little background on this table. I never care about the 'added_id', I'm only using to ensure that the inserted items are clustered together on disk (since the B-Tree used to index the table in MySQL uses the primary key as the cluster index). The 'id' column contains the binary representation of a UUID -- this is the column I actually care about and all other things reference this ID. Again, I don't want the UUID to be the primary key, since the UUID is random and thus making the B-Tree created to index the table have horrible IO characteristics (at least that is what has been said elsewhere). Also, although UUID1 includes the timestamp to ensure that IDs are generated in \"sequential\" order, the inclusion of the MAC address in the ID makes it something I'd rather avoid. Thus, I'd like to use UUID4s.</p>\n\n<p>Ok, now moving on to the SQLAlchemy part. In SQLAlchemy one can define a model using their ORM for the above table by doing something like:</p>\n\n<pre><code># The SQL Alchemy ORM base class\nBase = declerative_base()\n\n# The model for table 'foo'\nclass Foo(Base):\n    __table__ = 'foo'\n    add_id = Column(Integer, primary_key=True, nullable=False)\n    id = Column(Binary, index=True, unique=True, nullable=False)\n    ...\n</code></pre>\n\n<p>Again, this is basically the same as the SQL above. </p>\n\n<p>And now to the question. Let's say that this database is going to be sharded (horizontally partitioned) into 2 (or more) separate databases. Now, (assuming no deletions) each of these databases will have records with added_id of 1, 2, 3, etc in table foo. Since SQLAlchemy uses a session to manage the objects that are being worked on such that each object is identified only by its primary key, it seems like it would be possible to have the situation where I could end trying to access two Foo objects from the two shards with the same added_id resulting in some conflict in the managed session. </p>\n\n<p>Has anyone run in to this issue? What have you done to solve it? Or, more than likely, am I missing something from the SQLAlchemy documentation that ensures that this cannot happen. However, looking at the sharding example provided with the SQLAlchemy download (examples/sharding/attribute_shard.py) they seem to side-step this issue by designating one of the database shards as an ID generator... creating an implicit bottle neck as all INSERTS  have to go against that single database to get an ID. (They also mention using UUIDs, but apparently that causes the performance issue for the indexes.)</p>\n\n<p>Alternatively, is there a way to set the UUID as the primary key and have the data be clustered on disk using the added_id? If it's not possible in MySQL is it possible in another DB like Postgres?</p>\n\n<p>Thanks in advance for any and all input!</p>\n"},{"tags":["performance","cdn"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":8,"score":0,"question_id":13260464,"title":"Is it possible to implement a Content Delivery Network (CDN) if I'm with a cluster web host?","body":"<p>I'm with a webhost, a web farm or cluster, I guess you could say.  I have a 47 page company website, and all speed tests suggest I use a CDN.</p>\n\n<p>I've googled and SE's this to no end, but still don't understand how to implement a content delivery network.  Are they suggesting I order a subdomain and put all my .css, .js, and image files in that subdomain?  Or are they suggesting that instead of downloading jquery 1.7, I just link to malsup's jquery?  But then what would I do for images and .css?</p>\n\n<p>Just kinda confused here; any help in this regard would be truly appreciated!</p>\n"},{"tags":["c++","optimization","performance","post-increment","pre-increment"],"answer_count":14,"favorite_count":45,"up_vote_count":87,"down_vote_count":1,"view_count":9665,"score":86,"question_id":24901,"title":"Is there a performance difference between i++ and ++i in C++?","body":"<p>We looked at this answer for C in this question:</p>\n\n<p><a href=\"http://stackoverflow.com/questions/24886/is-there-a-performance-difference-between-i-and-i-in-c\" rel=\"nofollow\">http://stackoverflow.com/questions/24886/is-there-a-performance-difference-between-i-and-i-in-c</a></p>\n\n<p>What's the answer for C++?</p>\n"},{"tags":["performance","knockout.js","tuning","knockout-mapping-plugin"],"answer_count":3,"favorite_count":9,"up_vote_count":21,"down_vote_count":0,"view_count":1130,"score":21,"question_id":9927213,"title":"Performance tuning a knockout application - guidelines for improving response times","body":"<p>I have a large, complex page that relies heavily on knockout.js. Performance is starting to become an issue but examining the call stack and trying to find the bottlenecks is a real challenge. </p>\n\n<p>I noticed in another question ( <a href=\"http://stackoverflow.com/questions/9916843/knockout-js-understanding-foreach-and-with\">Knockout.js -- understanding foreach and with</a> ) that the accepted answer has the comment:</p>\n\n<blockquote>\n  <p>...and I suggest not using <code>with</code> where high performance is necessary\n  because of the overhead...</p>\n</blockquote>\n\n<p>Assuming the statement is true, this is really useful stuff to know and I have not found a source for such performance tips.</p>\n\n<p>Therefore, my question is: </p>\n\n<p><strong>Are there general guidelines / top tips that I can apply to help the performance of my application before I get deep into classic performance tuning.</strong></p>\n"}]}
{"total":25592,"page":2,"pagesize":100,"questions":[{"tags":["android","json","performance","parsing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":13259000,"title":"Json parsing strategy in Android","body":"<p>I have a large json file (~3.5 MB), with (~140) complex objects, and the max depth in the object graph is about 4-5. I use Gson to parse it, but it's really slow. I've tried some way to parse it (like mixed parsing or using stream to parse), but I couldn't increase performance.</p>\n\n<p>I checked Memory Analizer, it kill memory (70-80%), if I only parse the base Id of the objects. While parsing there are 400-500k object in memory (mostly string and char).</p>\n\n<p>Would be parsing more efficient if the object graph wouldn't be so deep? Do you have a good idea how could be better? I tried other libs too (like Jackson), but performance wasn't better.</p>\n"},{"tags":["vb.net","performance","vbscript"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":18,"score":1,"question_id":13243111,"title":"HTTPRequest performance VBScript vs. VB.NET","body":"<p>I have an application that uses a VBScript file within operation to make HTTP requests and I would like to preferably move this to a VB.NET solution entirely. </p>\n\n<p>I know there are some differences in the VBScript HTTP requests and the VB.NET ones, but what I really want to know is which one will be faster? After some tests with VB.NET I'm not so sure it will be as fast as my VBScript solution. Does anyone have experience with this and is there even a noticeable change in performance?</p>\n\n<p>Thanks</p>\n"},{"tags":["sql","database","performance","query","filter"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":22,"score":0,"question_id":13257155,"title":"Architecture for a fast screener","body":"<p>I'm building a screener that should be able to search through a table about 50 columns wide and 7000 rows long really fast.</p>\n\n<p>Each row is composed of the following columns.</p>\n\n<p>primary_key, quantity1, quantity2, quantity3...quantity50.</p>\n\n<p>All quantities are essentially floats or integers. Hence a typical screener would look like this.</p>\n\n<pre><code>Get all rows which have quantity1 &gt; x and quantity2 &lt; y and quantity3 &gt;= z.\n</code></pre>\n\n<p>Indexing all columns should lead to really fast search times however some of the columns will be updating in realtime. Indexing everything obviously leads to very low insert/update times.</p>\n\n<p>A portion of the columns are fairly static though. Hence an idea was to segregate the data into two tables, one containing all columns that are static while the other containing data that is dynamic. Any screener would then be applied to both tables based on the actual query. And the results combined in the end.</p>\n\n<p>I am currently planning on using a MySQL engine, most probably INNoDB. However I'm looking to get much faster response times. An implementation of the same problem on a certain site was very snappy. Regardless of the query size, i was getting the results within 500 ms. Wondering what other options are available out there to implement this function.</p>\n\n<p>Any thoughts/hints towards a potential solution would be greatly appreciated.</p>\n"},{"tags":["ios","performance","mvc","key-value-observing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":19,"score":0,"question_id":13256093,"title":"Design assistance for MVC and KVO","body":"<p>I have data that is persisted on the device that needs to sync with my my server, in terms of design am I on the right track?</p>\n\n<p>I have a model with a method to return my local objects, let's call it <code>[model objects]';</code> </p>\n\n<p>Within this method it returns the local objects immediately but I'd also like to check if the server needs to update any of these objects so in the background thread I run a method that uses an <code>async NSUrlConnection</code> that will return the JSON of each object that needs to be updated in which I will save that to core data. In the Table View Controller I would set a KVO to observe when the values changed for the <code>updatedObjects</code> then reload the objects displayed in the <code>objectTableView</code>.</p>\n\n<p><strong>Question: Is this the correct way to handle this? Am I missing something? Can I improve this somehow?</strong></p>\n\n<p>I am also thinking of disabling the UITableViewCells of the objects that are being updated and showing a <code>UIActivityIndicator</code> during the downloading and saving stages but I'm not sure if this will cause any race conditions in the UX.</p>\n\n<p>Hopefully this was explained well enough for you to understand, if you have any questions I will try to reply immediately.</p>\n"},{"tags":["c++","performance","exception","try-catch"],"answer_count":5,"favorite_count":2,"up_vote_count":8,"down_vote_count":0,"view_count":4466,"score":8,"question_id":3730654,"title":"What's better to use, a __try/__except block or a try / catch block?","body":"<p>I'm wondering which is the better way to catch exceptions that I throw: is it a __try / __except block or a try / catch block?  </p>\n\n<p>I'm writing in C++ and the program will only be used on Windows, so portability is not an issue.  </p>\n\n<p>Thanks!</p>\n"},{"tags":["android","performance","opengl-es","gingerbread"],"answer_count":0,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":37,"score":3,"question_id":13256091,"title":"OpenGL bad performance in Android Gingerbread","body":"<p>I'm working in a videogame for Android using OpenGL 1.0.</p>\n\n<p>I followed this tutorial: \n<a href=\"http://www.javacodegeeks.com/2011/06/android-game-development-tutorials.html\" rel=\"nofollow\">http://www.javacodegeeks.com/2011/06/android-game-development-tutorials.html</a></p>\n\n<p>I have created a little demo that works very well and fluid (limited to 30 FPS) in my current mobile: HTC Sensation (Android 4.0.3 Ice Cream Sandwich).</p>\n\n<p>Also, I tested my game in my friend's mobiles with the next results:</p>\n\n<ul>\n<li>Galaxy S2 = perfect! (30FPS)</li>\n<li>Galaxy Note = perfect! (30FPS)</li>\n<li>HTC One S = perfect! (30FPS)</li>\n<li>Sony Xperia Tipo = perfect! (30FPS)</li>\n<li>Samsung Galaxy Ace GT-S5830 = very bad performance >_&lt;' (&lt;10FPS and tested in two mobiles)</li>\n</ul>\n\n<p>If you compare the specs between Xperia Tipo and Samsung Ace you can see that they are almost the same mobile.</p>\n\n<p>XPERIA TIPO:</p>\n\n<ul>\n<li>320 x 480 resolution</li>\n<li>512MB RAM</li>\n<li>Qualcomm MSM7227A 800MHz, GPU Adreno 200</li>\n</ul>\n\n<p>SAMSUNG GALAXY ACE:</p>\n\n<ul>\n<li>320 x 480 resolution</li>\n<li>278MB RAM</li>\n<li>Qualcomm MSM7227 800 MHz, GPU Adreno 200</li>\n</ul>\n\n<p>OK, the RAM is different, but my game is very simple, does not consume more than 200MB! But the performance is totally different. No sense!</p>\n\n<p>However... only one thing is different! Samsung Galaxy Ace uses Android 2.3.X (Gingerbread) and Xperia Tipo uses Android 4.0.3 (ICS).</p>\n\n<p>So, my spearhead is targetting that the problem is with Android 2.3.X - Gingerbread or Galaxy Ace is a trash mobile.</p>\n\n<p>But... I forced my friends to install the AndEngine Examples and test it, with the result that the Nexus ParticleSystem test works very well and fluid.</p>\n\n<p>I'm totally lost! Why this different in performance? What I'm doing wrong?</p>\n\n<p>Some extra info:</p>\n\n<ol>\n<li>I follow all the steps from that tutorial.</li>\n<li>I use GL10 (OpenGL 1.0).</li>\n<li>No shaders.</li>\n<li>No delta time for lost FPS (I do not think this thing is going to solve my problem).</li>\n<li>No native code, just Java.</li>\n<li>SystemClock.uptimemillis() and sleep() to control FPS.</li>\n<li>MediaPlayer to play one MIDI song (no more sounds).</li>\n<li>Textures with a lot of alpha (2D game, is neccesary for sprites).</li>\n<li>minSDKversion=8</li>\n</ol>\n\n<p>If you need more information, please tell me.</p>\n"},{"tags":["performance","haskell","project-euler","micro-optimization"],"answer_count":1,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":140,"score":6,"question_id":13252992,"title":"How do I optimize a loop which can be fully strict","body":"<p>I'm trying to write a brute-force solution to <a href=\"http://projecteuler.net/problem=145\">Project Euler Problem #145</a>, and I cannot get my solution to run in less than about 1 minute 30 secs.</p>\n\n<p><em>(I'm aware there are various short-cuts and even paper-and-pencil solutions; for the purpose of this question I'm not considering those).</em></p>\n\n<p>In the best version I've come up with so far, profiling shows that the majority of the time is spent in <code>foldDigits</code>. This function need not be lazy at all, and to my mind ought to be optimized to a simple loop. As you can see I've attempted to make various bits of the program strict.</p>\n\n<p>So my question is: <strong>without changing the overall algorithm, is there some way to bring the execution time of this program down to the sub-minute mark?</strong></p>\n\n<p>(Or if not, is there a way to see that the code of <code>foldDigits</code> is as optimized as possible?)</p>\n\n<pre><code>-- ghc -O3 -threaded Euler-145.hs &amp;&amp; Euler-145.exe +RTS -N4\n\n{-# LANGUAGE BangPatterns #-}\n\nimport Control.Parallel.Strategies\n\nfoldDigits :: (a -&gt; Int -&gt; a) -&gt; a -&gt; Int -&gt; a\nfoldDigits f !acc !n\n    | n &lt; 10    = i\n    | otherwise = foldDigits f i d\n  where (d, m) = n `quotRem` 10\n        !i     = f acc m\n\nreverseNumber :: Int -&gt; Int\nreverseNumber !n\n    = foldDigits accumulate 0 n\n  where accumulate !v !d = v * 10 + d\n\nallDigitsOdd :: Int -&gt; Bool\nallDigitsOdd n\n    = foldDigits andOdd True n\n  where andOdd !a d = a &amp;&amp; isOdd d\n        isOdd !x    = x `rem` 2 /= 0\n\nisReversible :: Int -&gt; Bool\nisReversible n\n    = notDivisibleByTen n &amp;&amp; allDigitsOdd (n + rn)\n  where rn                   = reverseNumber n\n        notDivisibleByTen !x = x `rem` 10 /= 0\n\ncountRange acc start end\n    | start &gt; end = acc\n    | otherwise   = countRange (acc + v) (start + 1) end\n  where v = if isReversible start then 1 else 0\n\nmain\n    = print $ sum $ parMap rseq cr ranges\n  where max       = 1000000000\n        qmax      = max `div` 4\n        ranges    = [(1, qmax), (qmax, qmax * 2), (qmax * 2, qmax * 3), (qmax * 3, max)]\n        cr (s, e) = countRange 0 s e\n</code></pre>\n"},{"tags":["android","performance","touch","velocity","acceleration"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":320,"score":0,"question_id":10155907,"title":"speed or acceleration of motion event android","body":"<p>Is it possible to get the speed or velocity or acceleration of touch event in android with the existing api? I have gone through MotionEvent class and none of the fields in that class seem to retrieve information that i need. Any help would be greatly appreciated</p>\n"},{"tags":["database","performance","algorithm","caching","graph"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":69,"score":3,"question_id":11216410,"title":"Caching Strategy for a Deep, Traversable Egocentric Graph","body":"<p><strong>Firstly, let me explain what I'm building:</strong></p>\n\n<ul>\n<li><p>I have a D3.js Force Layout graph which is rooted at the center, and has a bunch of nodes spread around it. The center node is an Entity of some sort, the nodes around it are other Entities which are somehow related to the root. The edges are the actual relations (i.e. how the two are related).</p></li>\n<li><p>The outer nodes can be clicked to center the target Entity and load its relations</p></li>\n<li><p>This graph is \"Egocentric\" in the sense that every time a node is clicked, it becomes the center, and only relations <em>directly</em> involved with itself are displayed.</p></li>\n</ul>\n\n<p><strong>My Setup, in case any of it matters:</strong></p>\n\n<ul>\n<li><p>I'm serving an API through Node.js, which translates requests into queries to a CouchDB server with huge data sets.</p></li>\n<li><p>D3.js is used for layout, and aside from jQuery and Bootstrap, I'm not using any other client-side libraries. If any would help with this caching task, I'm open to suggestions :)</p></li>\n</ul>\n\n<p><strong>My Ideas:</strong></p>\n\n<ul>\n<li><p>I could easily grab a few levels of the graph each time (recurse through the process of listing and expanding children a few times) but since clicking on any given node loads completely unrelated data, it is not guaranteed to yield a high percentage of the similar data as was loaded for the root. This seems like a complete waste, and actually a step in the <em>opposite</em> direction -- I'd end up doing <em>more</em> processing this way!</p></li>\n<li><p>I can easily maintain a hash table of Entities that have already been retrieved, and check the list before requesting data for that entity from the server. I'll probably end up doing this regardless of the cache strategy I implement, since it's a really simple way of reducing queries.</p></li>\n</ul>\n\n<p><strong>Now, how do you suggest I cache this data?</strong></p>\n\n<p>Is there any super-effective strategy you can think of for doing this kind of caching? Both server-and-client-side options are greatly welcomed. A <em>ton</em> of data is involved in this process, and any reduction of querying/processing puts me miles ahead of the game.</p>\n\n<p>Thanks!</p>\n"},{"tags":["java","performance","arraylist","hashtable"],"answer_count":6,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":659,"score":3,"question_id":8512356,"title":"Java Iterated HashTable vs ArrayList speed","body":"<p>I am writing a simple 3D SW rendering engine. I have a default <code>ArrayList&lt;Object3D&gt;</code> containing the whole scene. Now, I want to be able to add, remove and select objects by name, like 3D editors do (because its MUCH more simple than mouse select, but still looking good in homework :) ).</p>\n\n<p>So, the first thing I thought is to have <code>Hashtable</code> for name and index to scene <code>ArrayList</code>. But, then I thought I could just simply save the scene using <code>Hashtable</code> directly, and go through it to render using iterator. </p>\n\n<p>So I want to ask, in a 3D engine, what is speed-preferable? Because I will for-loop the scene many times per second, compared to selecting object. Is <code>ArrayList</code> any faster than <code>iterated Hashtable</code>? Thanks.</p>\n"},{"tags":["c#","performance","encryption","aes","bouncycastle"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":35,"score":2,"question_id":13248587,"title":"How can I improve cyphering speed of the Bouncy Castle API?","body":"<p>I'm developing a software in C#, until now I was using the cryptographic library included in .NET (especially <code>AesCryptoServiceProvider</code>), but now I've special needs and I need to move to the Bouncy Castle API's. I've done some tests and I found out that these API are slower in comparison of what it's included in the .NET framework. Any idea on how to improve their performance?\nThis is the cipher that I'm using:\n<code>IBufferedCipher cipher = new CtsBlockCipher(new CbcBlockCipher(new AesFastEngine()));</code></p>\n\n<p>To be more specific, I need to move to the Bouncy Castle API's because I need both input and output file with the same length and the .NET <code>RijndaelManaged</code> (the only class that can assure that kind of behavior) is way slower than <code>AesCryptoServiceProvider</code>.</p>\n\n<p>To encrypt a file of 1.7MB with <code>AesCryptoServiceProvider</code> it takes about 40ms in my machine and about 170ms with Bouncy Castle. It doesn't look like much, but I need to use this software in a server with hundreds of requests per minutes...</p>\n\n<p>Thank you very much!!</p>\n"},{"tags":["android","performance","listview"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":29,"score":0,"question_id":13250307,"title":"Android: avoid unwanted calls from getView() in an adapter","body":"<p>I searched for hours but didn't find a suitable solution for me. \nWhat I want to do: in my ListFragment I use the onListItemClick(...) method to handle the click events. Here, I change the background of the row item. But unfortunately every time the onListItemClick(...) is called, also the getView() from the adapter is called and updates all 8 visible row items. That takes to much time: 0.5 seconds. Because the row layout is pretty complex (2 Images, 8 TextViews). \nSo I want to update only the row which is clicked. I want to use <a href=\"http://stackoverflow.com/questions/2123083/android-listview-refresh-single-row/9987714#9987714\">this solution</a> but that has no effect, when the other 7 row items are updated anyways. \nI already followed <a href=\"http://lucasr.org/2012/04/05/performance-tips-for-androids-listview/\" rel=\"nofollow\">these advices</a> to speed the list up, but it's still to slow. </p>\n\n<p>Any help, ideas and thoughts are appreciated. :)\nThank you!</p>\n\n<p>[EDIT]</p>\n\n<p>Thanks to CommensWare for giving me some new ideas. What I did now was to check what traceview says. And the result is, that the delay is devided in two parts. The first 300ms of the delay takes the \"FastXMLSerialzier.escapeAndAppendString()\" with over 22.000 calls. That seems a lot! In the second half, many, maybe all, onMeasure()-methods of the views and layouts are called. </p>\n\n<p>What I tried:\nI filled every textview with static dummy values in the adapter and excluded the part with loading the images. It changes nothing, traceview shows the same picture. \nIn the second try, I looked at the LinearLayout of my list item and replaced every \"wrap_content\" I found with \"match_parent\" - nothing. Still the same. </p>\n\n<p>I am still open for your thoughts and hints. :)</p>\n"},{"tags":["sql","sql-server","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":59,"score":2,"question_id":13253522,"title":"Fast SQL Server search on 40M text records","body":"<p>I have a SQL Server 2005 database with a table containing 40 million records. Each record contains a column that stores a comma separated list of keywords. Each keyword is a combination of letters and numbers. The keywords are up to 7 characters long and on average there are 15 keywords per record. The keywords are not unique across rows.</p>\n\n<p>I want to search on full or part of the keyword.</p>\n\n<p>I've created Full text index which shows 328,245,708 unique key count. The search efficiency is fine for queries of 4 or more characters (around <strong>100ms</strong> on the test machine), but too slow for queries that have 3 or less characters (up to <strong>3s</strong> on the test machine).</p>\n\n<p>I've been trying both <code>CONTAINSTABLE</code> and <code>CONTAINS</code> queries of a sort <code>'[query]*'</code> with similar result.</p>\n\n<p>I believe the performance of the short queries is slower, because short words repeat across different records more frequently.</p>\n\n<p>Sorting the results is not crucial, and I've been trying to return <code>TOP X</code> results sorted on Rank from <code>CONTAINSTABLE</code>. This doesn't provide the desired performance.</p>\n\n<p>How can I make this search faster for short queries?</p>\n"},{"tags":["python","performance","path","directory"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":36,"score":0,"question_id":13252980,"title":"Common pathname manipulations to save in a specific directory a file","body":"<p>I am writing a function where I read <code>inFile</code> in order to split it into two files (outFile1,outFile2).</p>\n\n<p>What I want is if <code>outFile1</code> and/or <code>outFile2</code> are specified without pathname directory (ex: <code>outFile1=\"result1.txt\"</code> and <code>outFile2=\"result2.txt\"</code>) both files are saved in the same directory as <code>inFile</code> (ex: inFile=\"C:\\mydata\\myfile.txt\"). If the  pathname directory for the output files is present, I wish to save the results in that directory.  </p>\n\n<p>when I don't report the the <code>outFile</code> pathname directory, the files are saved in the same directory as my python script.</p>\n\n<pre><code>def LAS2LASDivide(inFile,outFile1,outFile2,Parse,NumVal):\n    inFile_path, inFile_name_ext = os.path.split(os.path.abspath(inFile))\n    outFile1_path, outFile1_name_ext = os.path.split(os.path.abspath(outFile1))\n    outFile2_path, outFile2_name_ext = os.path.split(os.path.abspath(outFile2))\n    outFile1_name = os.path.splitext(outFile1_name_ext)[0]\n    outFile2_name = os.path.splitext(outFile2_name_ext)[0]\n</code></pre>\n\n<p>example</p>\n\n<pre><code>inFile=\"C:\\\\mydoc\\\\Area_18.las\"\noutFile1=\"Area_18_overlap.las\"\noutFile2=\"Area_18_clean.las\"\n\ninFile_path, inFile_name_ext = os.path.split(os.path.abspath(inFile))\ninFile_path, inFile_name_ext\n('C:\\\\mydoc', 'Area_18.las')\n\noutFile1_path, outFile1_name_ext = os.path.split(os.path.abspath(outFile1))\noutFile1_path, outFile1_name_ext \n('C:\\\\Program Files\\\\PyScripter', 'Area_18_overlap.las')\n</code></pre>\n\n<p>this is all my code (tested) modify with the suggestion of mgilson</p>\n\n<pre><code>import os\nfrom os import path\nfrom liblas import file as lasfile\n\n\ndef LAS2LASDivide(inFile,outFile1,outFile2,Parse,NumVal):\n    inFile_path, inFile_name_ext = os.path.split(os.path.abspath(inFile))\n    outFile1_path, outFile1_name_ext = os.path.split(os.path.abspath(outFile1))\n    outFile2_path, outFile2_name_ext = os.path.split(os.path.abspath(outFile2))\n    outFile1_name = os.path.splitext(outFile1_name_ext)[0]\n    outFile2_name = os.path.splitext(outFile2_name_ext)[0]\n    if outFile1_name != outFile2_name:\n        # function pesudo_switch\n        def pseudo_switch(x):\n            return {\n                \"i\": p.intensity,\n                \"r\": p.return_number,\n                \"n\": p.number_of_returns,\n                \"s\": p.scan_direction,\n                \"e\": p.flightline_edge,\n                \"c\": p.classification,\n                \"a\": p.scan_angle,\n            }[x]\n        h = lasfile.File(inFile,None,'r').header\n        # change the software id to libLAS\n        h.software_id = \"alessandro.montaghi@gmail.com\"\n        if not os.path.split(outFile1)[0]:\n            file_out1 = lasfile.File(os.path.abspath(\"{0}\\\\{1}.las\".format(inFile_path,outFile1_name)),mode='w',header= h)\n        else:\n            file_out1 = lasfile.File(os.path.abspath(\"{0}\\\\{1}.las\".format(outFile1_path,outFile1_name)),mode='w',header= h)\n        if not os.path.split(outFile2)[0]:\n            file_out2 = lasfile.File(os.path.abspath(\"{0}\\\\{1}.las\".format(inFile_path,outFile2_name)),mode='w',header= h)\n        else:\n            file_out2 = lasfile.File(os.path.abspath(\"{0}\\\\{1}.las\".format(outFile2_path,outFile2_name)),mode='w',header= h)\n        for p in lasfile.File(inFile,None,'r'):\n            if pseudo_switch(Parse) == int(NumVal):\n                file_out1.write(p)\n            elif pseudo_switch(Parse) != int(NumVal):\n                file_out2.write(p)\n        file_out1.close()\n        file_out2.close()\n    else:\n        print \"outFile1 and outFile2 cannot have the same name\"\n</code></pre>\n"},{"tags":["c#","performance","linq"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":60,"score":1,"question_id":13253561,"title":"Improving performance of LINQ query with IEqualityComparer","body":"<p>I'm using <code>IEqualityComparer</code> to match \"near duplicates\" in a database using LINQ to Entities.</p>\n\n<p>With a record set of around 40,000, this query is taking around 15 seconds to complete and I wondered if there were any structural changes that could be made to the code below.</p>\n\n<p><strong>My public method</strong></p>\n\n<pre><code>public List&lt;LeadGridViewModel&gt; AllHighlightingDuplicates(int company)\n        {\n\n            var results = AllLeads(company)\n                  .GroupBy(c =&gt; c, new CompanyNameIgnoringSpaces())\n                  .Select(g =&gt; new LeadGridViewModel\n                  {\n                      LeadId = g.First().LeadId,\n                      Qty = g.Count(),\n                      CompanyName = g.Key.CompanyName\n                  }).OrderByDescending(x =&gt; x.Qty).ToList();\n\n            return results;\n\n        }\n</code></pre>\n\n<p><strong>Private method to grab the leads</strong></p>\n\n<pre><code>private char[] delimiters = new[] { ' ', '-', '*', '&amp;', '!' };\nprivate IEnumerable&lt;LeadGridViewModel&gt; AllLeads(int company)\n        {\n            var items = (from t1 in db.Leads\n                          where\n                              t1.Company_ID == company\n                          select new LeadGridViewModel\n                          {\n                              LeadId = t1.Lead_ID,\n                              CompanyName = t1.Company_Name,\n                          }).ToList();\n\n\n            foreach (var x in items)\n                x.CompanyNameStripped = string.Join(\"\", (x.CompanyName ?? String.Empty).Split(delimiters));\n\n            return items;\n        }\n</code></pre>\n\n<p><strong>My IEqualityComparer</strong></p>\n\n<pre><code> public class CompanyNameIgnoringSpaces : IEqualityComparer&lt;LeadGridViewModel&gt;\n    {\n        public bool Equals(LeadGridViewModel x, LeadGridViewModel y)\n        {\n            var delimiters = new[] {' ', '-', '*', '&amp;', '!'};\n            return delimiters.Aggregate(x.CompanyName ?? String.Empty, (c1, c2) =&gt; c1.Replace(c2, '\\0')) \n                == delimiters.Aggregate(y.CompanyName ?? String.Empty, (c1, c2) =&gt; c1.Replace(c2, '\\0'));\n        }\n\n        public int GetHashCode(LeadGridViewModel obj)\n        {\n            var delimiters = new[] {' ', '-', '*', '&amp;', '!'};\n            return delimiters.Aggregate(obj.CompanyName ?? String.Empty, (c1, c2) =&gt; c1.Replace(c2, '\\0')).GetHashCode();\n        }\n    }\n</code></pre>\n"},{"tags":["eclipse","performance","slowness"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":32,"score":-1,"question_id":13253363,"title":"Eclipse slowing down computer?","body":"<p>I feel like when I use Eclipse (especially for big projects) and even after I close it, the performance of my computer slows down permanently. It happened to my old laptop and I see it's happening with my new one. Is that possible and what can I do about it?</p>\n\n<p>(btw im using a Dell Inspiron Intel core i3, CPU 1.40 Ghz,RAM 6GB)</p>\n"},{"tags":["performance","drools"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":23,"score":0,"question_id":13251664,"title":"Drools decision tables performance","body":"<p>I'm using Drools decision tables. I have an .XSL file with condition columns, an action column, and several rows. Everything is working fine, but I'm worried about performance if the decision table start growing.\nDoes anybody know if there is some kind of limitations in decision tables? or something to keep in mind about the number of rows/columns that could affect the performance?\nThe application needs to be always available, and decision tables are taking care of a critical part of my application, and thats why I'm worried.\nThanks.</p>\n"},{"tags":["c#",".net","sql","performance","ado.net"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":13252966,"title":"Fastest Get data from remote server","body":"<p>I'm creating a windows application in which I need to get data using ado.net/(Or any other way using C# if any ). From one table. The database table apparently has around <code>100000</code> records and it takes forever to download.</p>\n\n<p>Is there any faster way where I could get data into faster? </p>\n\n<p>I tried the <code>DataReader</code> but still isn't fast enough. </p>\n"},{"tags":["database","performance","plsql","cursor"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":17,"score":0,"question_id":13251545,"title":"PLSQL block takes very long time to declare a cursor","body":"<p>I'm executing a PL/SQL blocks that updates some rows from an explicit cursor. The code to declare the cursor is the following:    </p>\n\n<pre><code>cursor DUP_SUBJECTS is  \n  select * \n    from ODS_SUBJECT_D  \n   WHERE SUBJECT_COD = ANY (SELECT SUBJECT_COD  \n                              FROM ODS_SUBJECT_D  \n                             WHERE END_DATE = TO_DATE ('31-12-9999','DD-MM-YYYY')   \n                             GROUP BY SUBJECT_COD, ROW_TYPE_DE  \n                            HAVING COUNT(*) &gt; 1)  \n   ORDER BY SUBJECT_COD, START_DATE; \n</code></pre>\n\n<p>The first statement in the body is a <code>DBMS_OUTPUT.PUT_LINE</code> in order to notify when it starts to executing the block. The query in the above script returns 20000 rows out of 2900000 rows in the table. It seems to take very long time in the declaration block of the script (after 30 minutes it does not print the message yet).<br>\nAny suggestion to optimize the performances of the script?  </p>\n\n<p>Thanks,<br>\nAntonio</p>\n"},{"tags":["python","performance","csv","data","statistics"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":7,"view_count":54,"score":-6,"question_id":13251754,"title":"python generate unique counter from csv file","body":"<p>I have a huge csv (tab) file that is about 1 GB of student data for overall grades.<br/><br/>\n(section) Example:</p>\n\n<pre><code>Mandy 1Q 2011 82 English\nCarey 4Q 2010 70 Math\nFreeman 2Q 2012 52 Music\nDarrick 4Q 2010 82 Science\nOsvaldo 4Q 2012 59 Science\nOsvaldo 4Q 2012 79 Science\nJudy 1Q 2012 89 Science\nEmilia 2Q 2011 31 Music\nEmilia 2Q 2011 31 Music\nCordia 1Q 2011 79 Science\nCelestine 1Q 2012 62 English\nCelestine 1Q 2012 62 English\nOpal 4Q 2012 93 Math\nKarlene 1Q 2011 87 English\nKellie 2Q 2012 44 Science\n</code></pre>\n\n<p>I'm looking for:<br/>\nStudent Average, Max, Min (whole, Qtr, Yr) (by class type)<br/>\nAverage per class type (Whole, Qtr, Yr)<br/></p>\n\n<p>Thanks for any help</p>\n"},{"tags":["sql","performance","query","postgresql","postgresql-performance"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":83,"score":3,"question_id":13234812,"title":"Improving query speed: simple SELECT in big postgres table","body":"<p>I'm having trouble regarding speed in a SELECT query on a Postgres database.</p>\n\n<p>I have a table with two integer columns as key: (int1,int2)\nThis table has around 70 million rows.</p>\n\n<p>I need to make two kind of simple SELECT queries in this environment:</p>\n\n<pre><code>SELECT * FROM table WHERE int1=X;\nSELECT * FROM table WHERE int2=X;\n</code></pre>\n\n<p>These two selects returns around 10.000 rows each out of these 70 million. For this to work as fast as possible I thought on using two HASH indexes, one for each column. Unfortunately the results are not that good:</p>\n\n<pre><code>                                                               QUERY PLAN                                                               \n----------------------------------------------------------------------------------------------------------------------------------------\n Bitmap Heap Scan on lec_sim  (cost=232.21..25054.38 rows=6565 width=36) (actual time=14.759..23339.545 rows=7871 loops=1)\n   Recheck Cond: (lec2_id = 11782)\n   -&gt;  Bitmap Index Scan on lec_sim_lec2_hash_ind  (cost=0.00..230.56 rows=6565 width=0) (actual time=13.495..13.495 rows=7871 loops=1)\n         Index Cond: (lec2_id = 11782)\n Total runtime: 23342.534 ms\n(5 rows)\n</code></pre>\n\n<p>This is an EXPLAIN ANALYZE example of one of these queries. It is taking around 23 seconds. My expectations are to get this information in less than a second.</p>\n\n<p>These are some parameters of the postgres db config:</p>\n\n<pre><code>work_mem = 128MB\nshared_buffers = 2GB\nmaintenance_work_mem = 512MB\nfsync = off\nsynchronous_commit = off\neffective_cache_size = 4GB\n</code></pre>\n\n<p>Any help, comment or thought would be really appreciated.</p>\n\n<p>Thank you in advance.</p>\n"},{"tags":["python","performance","numpy","cython"],"answer_count":1,"favorite_count":0,"up_vote_count":8,"down_vote_count":0,"view_count":84,"score":8,"question_id":13241724,"title":"Assigning values to array slices is slow","body":"<p>I'm trying to optimize a Python algorithm by implementing it in Cython. My question is regarding a certain performance bottleneck that exists in the following code:</p>\n\n<pre><code>@cython.boundscheck(False) # turn off bounds-checking for entire function\ndef anglesToRGB( np.ndarray[double, ndim=2] y, np.ndarray[double, ndim=2] x ):\n\ncdef double angle\ncdef double Hp\ncdef double C\ncdef double X\ncdef np.ndarray[double, ndim=3] res = np.zeros([y.shape[0], y.shape[1], 3], dtype=np.float64)\n\nfor i in xrange(y.shape[0]):\n    for j in xrange(y.shape[1]):\n        angle = atan2( y[i,j], x[i,j] )*180.0/PI+180\n\n        C = sqrt(pow(y[i,j],2)+pow(x[i,j],2))/360.0 #Chroma\n        Hp = angle/60.0\n        X = C*(1-fabs( Hp%2-1))\n\n        C *= 255\n        X *= 255\n\n        if (0. &lt;= Hp &lt; 1.):\n            res[i,j,:] = [C,X,0]\n        elif (1. &lt;= Hp &lt; 2.):\n            res[i,j,:] = [X,C,0]\n        elif (2. &lt;= Hp &lt; 3.):\n            res[i,j,:] = [0,C,X]\n        elif (3. &lt;= Hp &lt; 4.):\n            res[i,j,:] = [0,X,C]\n        elif (4. &lt;= Hp &lt; 5.):\n            res[i,j,:] = [X,C,C]\n        else:\n            res[i,j,:] = [C,0,X]\n\nreturn res\n</code></pre>\n\n<p>I've identified the major bottleneck to be when i assign a list of values to a slice of the res array, \nlike with</p>\n\n<pre><code>res[i,j,:] = [C,X,0]\n</code></pre>\n\n<p>However, if i change the assignment to</p>\n\n<pre><code>res[i,j,0] = C\nres[i,j,1] = X\nres[i,j,2] = 0\n</code></pre>\n\n<p>Then the code runs orders of magnitude faster.\nTo me this is strange because surely the Cython compiler should be smart enough to do this for me? Or do i need to provide it with some hints first?\nI should note that changing the slicing to 0:3 instead of : and making the list of values a numpy array doesn't improve the performance.</p>\n\n<p>What i'd like to know is why this operation is killing performance so badly and if there's any way to solve it without having to sacrifice the convenient list and slice notation.</p>\n\n<p>Best regards</p>\n"},{"tags":["python","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":2,"view_count":77,"score":0,"question_id":13112527,"title":"Python performance issues","body":"<p>I built a simple gravity simulation shown here:</p>\n\n<p><img src=\"http://i.stack.imgur.com/8YzWt.jpg\" alt=\"enter image description here\"></p>\n\n<p>At first, I used a variable timestep, by calculating how much it takes for one update, and applying that to the next one - standard stuff.</p>\n\n<p>However, this method reduces determinism, and results can vary from simulation to simulation - something I definitely do not want.</p>\n\n<p>I figured that I should use a fixed time step (10 seconds), but then I noticed that the simulation would speed up and down in regular intervals (of around 1 second).</p>\n\n<p>Why is that happening? Does it have to do anything with Python itself?</p>\n\n<p><br /></p>\n\n<h1>The code</h1>\n\n<p>The main loop:</p>\n\n<pre><code>while run:\n    uni.update(10)\n\n    for event in pygame.event.get():\n        if event.type == QUIT:\n            run = False\n</code></pre>\n\n<p>The update method:</p>\n\n<pre><code>def update (self, dt):\n    self.time += dt\n\n    for b1, b2 in combinations(self.bodies.values(), 2):\n        fg = self.Fg(b1, b2)\n\n        if b1.position.x &gt; b2.position.x:\n            b1.force.x -= fg.x\n            b2.force.x += fg.x\n        else:\n            b1.force.x += fg.x\n            b2.force.x -= fg.x\n\n\n        if b1.position.y &gt; b2.position.y:\n            b1.force.y -= fg.y\n            b2.force.y += fg.y\n        else:\n            b1.force.y += fg.y\n            b2.force.y -= fg.y\n\n\n    for b in self.bodies.itervalues():\n        ax = b.force.x/b.m\n        ay = b.force.y/b.m\n\n        b.position.x += b.velocity.x*dt\n        b.position.y += b.velocity.y*dt\n\n        nvx = ax*dt\n        nvy = ay*dt\n\n        b.position.x += 0.5*nvx*dt\n        b.position.y += 0.5*nvy*dt\n\n        b.velocity.x += nvx\n        b.velocity.y += nvy\n\n        b.force.x = 0\n        b.force.y = 0\n</code></pre>\n"},{"tags":["performance","search","microsoft","full-text-search","fast-esp"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":321,"score":0,"question_id":4979894,"title":"Need information for FAST Search Server 2010 for Internet Sites","body":"<p>This is NOT the FAST Search for SharePoint product. I am looking for information regarding the FAST product that can be installed without SharePoint.</p>\n\n<p>I've Googled and searched but can't find any more information other than the following blog - <a href=\"http://consultingblogs.emc.com/manjunathasubbarya/archive/2010/12/05/fsis-fast-search-for-internet-sites.aspx\" rel=\"nofollow\">http://consultingblogs.emc.com/manjunathasubbarya/archive/2010/12/05/fsis-fast-search-for-internet-sites.aspx</a></p>\n\n<p>I am curious if there are SMEs supporting installation and configuration. If there is any information on Microsoft's website? If anyone knows if FAST is supported outside of SharePoint.</p>\n"},{"tags":["performance","sql-server-2008-r2","certification"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":17,"score":-1,"question_id":13251086,"title":"sql server 2008r2 multi column clustered index vs covering index","body":"<p>suppose you have this table, note Bookid is nonclustered. I want to choose C, but D seems also makes sense (in this particular scenario). </p>\n\n<p>The column descriptions: Column, data type, primary key, indexed, index type\n<img src=\"http://i.stack.imgur.com/1eA50.jpg\" alt=\"enter image description here\"></p>\n"},{"tags":["performance","algorithm","language-agnostic","unix","pi"],"answer_count":22,"favorite_count":41,"up_vote_count":112,"down_vote_count":6,"view_count":13456,"score":106,"question_id":19,"title":"Fastest way to get value of pi","body":"<p>Solutions welcome in any language. :-) I'm looking for the fastest way to obtain the value of pi, as a personal challenge. More specifically I'm using ways that don't involve using <code>#define</code>d constants like <code>M_PI</code>, or hard-coding the number in.</p>\n\n<p>The program below tests the various ways I know of. The inline assembly version is, in theory, the fastest option, though clearly not portable; I've included it as a baseline to compare the other versions against. In my tests, with built-ins, the <code>4 * atan(1)</code> version is fastest on GCC 4.2, because it auto-folds the <code>atan(1)</code> into a constant. With <code>-fno-builtin</code> specified, the <code>atan2(0, -1)</code> version is fastest.</p>\n\n<p>Here's the main testing program (<code>pitimes.c</code>):</p>\n\n<pre><code>#include &lt;math.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;time.h&gt;\n\n#define ITERS 10000000\n#define TESTWITH(x) {                                                       \\\n    diff = 0.0;                                                             \\\n    time1 = clock();                                                        \\\n    for (i = 0; i &lt; ITERS; ++i)                                             \\\n        diff += (x) - M_PI;                                                 \\\n    time2 = clock();                                                        \\\n    printf(\"%s\\t=&gt; %e, time =&gt; %f\\n\", #x, diff, diffclock(time2, time1));   \\\n}\n\nstatic inline double\ndiffclock(clock_t time1, clock_t time0)\n{\n    return (double) (time1 - time0) / CLOCKS_PER_SEC;\n}\n\nint\nmain()\n{\n    int i;\n    clock_t time1, time2;\n    double diff;\n\n    /* Warmup. The atan2 case catches GCC's atan folding (which would\n     * optimise the ``4 * atan(1) - M_PI'' to a no-op), if -fno-builtin\n     * is not used. */\n    TESTWITH(4 * atan(1))\n    TESTWITH(4 * atan2(1, 1))\n\n#if defined(__GNUC__) &amp;&amp; (defined(__i386__) || defined(__amd64__))\n    extern double fldpi();\n    TESTWITH(fldpi())\n#endif\n\n    /* Actual tests start here. */\n    TESTWITH(atan2(0, -1))\n    TESTWITH(acos(-1))\n    TESTWITH(2 * asin(1))\n    TESTWITH(4 * atan2(1, 1))\n    TESTWITH(4 * atan(1))\n\n    return 0;\n}\n</code></pre>\n\n<p>And the inline assembly stuff (<code>fldpi.c</code>), noting that it will only work for x86 and x64 systems:</p>\n\n<pre><code>double\nfldpi()\n{\n    double pi;\n    asm(\"fldpi\" : \"=t\" (pi));\n    return pi;\n}\n</code></pre>\n\n<p>And a build script that builds all the configurations I'm testing (<code>build.sh</code>):</p>\n\n<pre><code>#!/bin/sh\ngcc -O3 -Wall -c           -m32 -o fldpi-32.o fldpi.c\ngcc -O3 -Wall -c           -m64 -o fldpi-64.o fldpi.c\n\ngcc -O3 -Wall -ffast-math  -m32 -o pitimes1-32 pitimes.c fldpi-32.o\ngcc -O3 -Wall              -m32 -o pitimes2-32 pitimes.c fldpi-32.o -lm\ngcc -O3 -Wall -fno-builtin -m32 -o pitimes3-32 pitimes.c fldpi-32.o -lm\ngcc -O3 -Wall -ffast-math  -m64 -o pitimes1-64 pitimes.c fldpi-64.o -lm\ngcc -O3 -Wall              -m64 -o pitimes2-64 pitimes.c fldpi-64.o -lm\ngcc -O3 -Wall -fno-builtin -m64 -o pitimes3-64 pitimes.c fldpi-64.o -lm\n</code></pre>\n\n<p>Apart from testing between various compiler flags (I've compared 32-bit against 64-bit too, because the optimisations are different), I've also tried switching the order of the tests around. The <code>atan2(0, -1)</code> version still comes out top every time, though.</p>\n\n<p>I'm keen to hear what results you have, as well as improvements to the testing process. :-)</p>\n"},{"tags":[".net","performance","optimization","logging"],"answer_count":5,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":255,"score":1,"question_id":2684961,"title":"Minimize performance impact of event logging?","body":"<p>I have a static logging function that uses StringBuilder to concatenate a bunch of query parameters before sending the string to a log.  This process can get moderately long, as we may have ~10 parameters (.Append calls) and end up being ~200 chars long.</p>\n\n<p>I want to minimize the performance impact of the logging function. (This logging function may be called multiple times per web request, and we measure the processing time for each web request)</p>\n\n<p>How/Should/Can I build a \"pool\" of StringBuilders to improve performance?</p>\n\n<p>I can also do all this logging asynchronously, right?  How should I do that?</p>\n"},{"tags":["asp.net-mvc","performance","entity-framework","iis","entity-framework-5"],"answer_count":0,"favorite_count":2,"up_vote_count":5,"down_vote_count":0,"view_count":78,"score":5,"question_id":13250679,"title":"How to \"warm-up\" Entity Framework? When does it get \"cold\"?","body":"<p>No, the answer to my second question is not the winter.</p>\n\n<p><strong>Preface:</strong></p>\n\n<p>I've been doing a lot of research on Entity Framework recently and something that keeps bothering me is its performance when the queries are not warmed-up, so called cold queries. </p>\n\n<p>I went through the <a href=\"http://msdn.microsoft.com/en-us/data/hh949853.aspx\">performance considerations</a> article for Entity Framework 5.0. The authors introduced the concept of <strong>Warm</strong> and <strong>Cold</strong> queries and how they differ, which I also noticed myself without knowing of their existence. Here it's probably worth to mention I only have six month of experience behind my back.</p>\n\n<p>Now I know what topics I can research into additionally if I want to understand the framework better in terms of performance. Unfortunately most of the information on the Internet is outdated or bloated with subjectivity, hence my inability to find any additional information on the <em>Warm</em> vs <em>Cold</em> queries topic.</p>\n\n<p>Basically what I've noticed so far is that whenever I have to recompile or the recycling hits in, my initial queries are getting very slow. Any subsequent data read is fast (<em>subjective</em>), as expected.</p>\n\n<p>We'll be migrating to Windows Server 2012, IIS8 and SQL Server 2012 and as a Junior I actually won myself the opportunity to test them before the rest. I'm very happy they introduced a warming-up module that will get my application ready for that first request. However, I'm not sure how to proceed with warming up my Entity Framework.</p>\n\n<p>What I already know is worth doing:</p>\n\n<ul>\n<li>Generate my Views in advance as suggested.</li>\n<li>Eventually move my models into a separate assembly.</li>\n</ul>\n\n<p>What I consider doing, by going with common sense, <strong>probably wrong approach</strong>:</p>\n\n<ul>\n<li>Doing dummy data reads at Application Start in order to warm things\nup, generate and validate the models.</li>\n</ul>\n\n<p><strong>Questions:</strong> </p>\n\n<ul>\n<li>What would be the best approach to have high availability on my Entity Framework at anytime?</li>\n<li>In what cases does the Entity Framework gets \"cold\" again? (Recompilation, Recycling, IIS Restart etc.)</li>\n</ul>\n"},{"tags":["performance","drools"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":1269,"score":2,"question_id":7743892,"title":"How to improve drools performance?","body":"<p>Is there a way to log / monitor the time taken for rule in a Drools rule set?</p>\n\n<p>Is there a way to make sure that one rule is not executed more than once(It seems to be happening in my case)</p>\n\n<p>What are the general guidelines on improving Drools performance?</p>\n\n<p>Currently I am using a one single DRL file with 100 odd rules. </p>\n\n<p>Any additiional information you need will be provided.</p>\n\n<p>Thanks,\nAbdul</p>\n"},{"tags":["javascript","performance","google-chrome-devtools"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":27,"score":0,"question_id":13247386,"title":"Page resources takes long to load","body":"<p>I am building a little web-based inventory application but I keep having trouble that the page <a href=\"http://tinyurl.com/csxc7e4\" rel=\"nofollow\">link</a> takes far too long to load.</p>\n\n<p>There are some resources that takes always five seconds to load, in the developer tools is see for example that that the file query.placeholderfix.js is five seconds waiting and then loads right away.</p>\n\n<p>If I remove that file there will be a other file that is just waiting five seconds and then load almost directly. </p>\n\n<p>The server which host this website have more than enough resources available (it is my own private server and is doing mostly nothing) so I don’t see where the problem my lay. </p>\n\n<p><strong>Update</strong></p>\n\n<p>I have changed some value's in the Apache configuration and it is now working perfectly. </p>\n\n<pre><code>    KeepAliveTimeout 2\n\n    &lt;IfModule mpm_prefork_module&gt;\n         StartServers      5\n         MinSpareServers   5\n         MaxSpareServers   10\n         MaxClients        25\n         MaxRequestsPerChild 800\n    &lt;/IfModule&gt;\n</code></pre>\n"},{"tags":["javascript","performance","html5","html5-canvas","webrtc"],"answer_count":0,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":35,"score":0,"question_id":13212382,"title":"WebRTC performance - very high cpu load","body":"<p>working on a motion detector <a href=\"https://github.com/alonisser/WebcamSwiper\" rel=\"nofollow\">js library</a> built with WebRTC + canvas. \nwhen I run the app I immediatly get very high cpu usage.\nI optimized the loops etc, but the basic problem seems to be accessing the camera eg WebRTC.</p>\n\n<p><strong>Is there a way to make WebRTC behave better?</strong> another configuration? something I'm missing?\nor What am I doing wrong?</p>\n\n<p>or maybe some js memory leak I'm handling wrong?</p>\n\n<p>you can check another demo <a href=\"http://swiper.4p-tech.co.il\" rel=\"nofollow\">here</a> with the same lib (simpler and mine)</p>\n\n<p>and a different one using WebRTC and with same problem <a href=\"http://enotionz.github.com/jscii/\" rel=\"nofollow\">here</a></p>\n"},{"tags":["sql","sql-server","performance","nested-sets","hierarchyid"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":115,"score":4,"question_id":12568201,"title":"SQL Server Nested set vs Hierarchyid performance","body":"<p>I have a hierarchical data. The most common queries will be \"get parent branch for node\" and \"get subtree of node\". Updates and inserts are not likely to occur often. I am choosing between nested sets and hierarchyid. As far as I am concerned, search on nested set should be pretty fast on indexed columns, however, I have no clue about internal implementation of hierarchyid. What should I use in order to achieve highest performance possible?</p>\n"},{"tags":["asp.net","performance","yslow"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":11,"score":0,"question_id":13244410,"title":"Will including these javascript files in the header eliminate Etag errors, and other SEO errors from YSlow?","body":"<p>I have been working on performance lately, and recently ran a YSlow test on my company's asp.net website.  I have already set up an IIS7 rule to set expiration dates to 30 days.  After researching, I think Yahoo suggests you add tags like this to your head section:</p>\n\n<pre><code>&lt;script src=\"http://yui.yahooapis.com/2.9.0/build/yahoo/yahoo-min.js\" &gt;&lt;/script&gt;\n&lt;script src=\"http://yui.yahooapis.com/2.9.0/build/get/get-min.js\" &gt;&lt;/script&gt;\n&lt;script src=\"http://yui.yahooapis.com/2.9.0/build/yahoo-dom-event/yahoo-dom-event.js\"&gt; \n&lt;/script&gt;\n&lt;script src=\"http://yui.yahooapis.com/2.9.0/build/imageloader/imageloader-min.js\"&gt;\n&lt;/script&gt;\n&lt;script src=\"http://yui.yahooapis.com/2.9.0/build/cookie/cookie-min.js\"&gt;&lt;/script&gt;\n</code></pre>\n\n<p>If you see this link, <a href=\"http://www.google.com/search?hl=en&amp;as_q=&amp;as_epq=build/yahoo/yahoo-min.js&amp;as_oq=&amp;as_eq=&amp;as_nlo=&amp;as_nhi=&amp;lr=&amp;cr=&amp;as_qdr=all&amp;as_sitesearch=&amp;as_occt=any&amp;safe=images&amp;tbs=&amp;as_filetype=&amp;as_rights=\" rel=\"nofollow\">Yahoo's recommendations on how to improve SEO</a>, it seems as if yahoo suggests putting these .js files in your header.</p>\n\n<p>Are these javascript files from yahoo something you should include in your head section?  Or are they just unnecessary external .js files that will slow your site down?  I just don't understand yahoo's logic behind this.  Any guidance to clear this up would be greatly appreciated!</p>\n"},{"tags":["performance","distributed","hadoop","shared-nothing"],"answer_count":7,"favorite_count":4,"up_vote_count":9,"down_vote_count":0,"view_count":2873,"score":9,"question_id":17721,"title":"Experience with Hadoop?","body":"<p>Have any of you tried Hadoop? Can it be used without the distributed filesystem that goes with it, in a Share-nothing architecture? Would that make sense?</p>\n\n<p>I'm also interested into any performance results you have...</p>\n"},{"tags":["c#",".net","performance","linq"],"answer_count":5,"favorite_count":1,"up_vote_count":7,"down_vote_count":0,"view_count":231,"score":7,"question_id":8595258,"title":"Why the order of LINQ to objects methods counts","body":"<p>I read <a href=\"http://stackoverflow.com/questions/7499384/does-the-order-of-linq-functions-matter\">this</a> question's answers that explain the order of the LINQ to objects methods makes a difference. My question is why?</p>\n\n<p>If I write a LINQ to SQL query, it doesn't matter the order of the LINQ methods-<code>projections</code> for example:</p>\n\n<pre><code>session.Query&lt;Person&gt;().OrderBy(x =&gt; x.Id)\n                       .Where(x =&gt; x.Name == \"gdoron\")\n                       .ToList();\n</code></pre>\n\n<p>The expression tree will be transformed to a rational SQL like this:</p>\n\n<pre><code>  SELECT   * \n  FROM     Persons\n  WHERE    Name = 'gdoron'\n  ORDER BY Id; \n</code></pre>\n\n<p>When I Run the query, SQL query will built according to the expression tree no matter how weird the order of the methods.<br>\nWhy it doesn't work the same with <code>LINQ to objects</code>?<br>\nwhen I enumerate an <em>IQueryable</em> all the projections can be placed in a rational order(e.g. Order By after Where) just like the Data Base optimizer does.</p>\n"},{"tags":["c++","performance"],"answer_count":3,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":129,"score":4,"question_id":13243274,"title":"Why is accumulate faster than a simple for cycle?","body":"<p>I was testing algorithms and run into this weird behavior, when <code>std::accumulate</code> is faster than a simple <code>for</code> cycle.</p>\n\n<p>Looking at the generated assembler I'm not much wiser :-) It seems that the <code>for</code> cycle is optimized into MMX instructions, while accumulate expands into a loop.</p>\n\n<p>This is the code. The behavior manifests with <code>-O3</code> optimization level, gcc 4.7.1</p>\n\n<pre><code>#include &lt;vector&gt;                                                                                                                                                                                                                                                              \n#include &lt;chrono&gt;                                                                                                                                                                                                                                                              \n#include &lt;iostream&gt;                                                                                                                                                                                                                                                            \n#include &lt;random&gt;                                                                                                                                                                                                                                                              \n#include &lt;algorithm&gt;                                                                                                                                                                                                                                                           \nusing namespace std;                                                                                                                                                                                                                                                           \n\nint main()                                                                                                                                                                                                                                                                     \n{                                                                                                                                                                                                                                                                              \n    const size_t vsize = 100*1000*1000;                                                                                                                                                                                                                                        \n\n    vector&lt;int&gt; x;\n    x.reserve(vsize);\n\n    mt19937 rng;\n    rng.seed(chrono::system_clock::to_time_t(chrono::system_clock::now()));\n\n    uniform_int_distribution&lt;uint32_t&gt; dist(0,10);\n\n    for (size_t i = 0; i &lt; vsize; i++)\n    {\n        x.push_back(dist(rng));\n    }\n\n    long long tmp = 0;\n    for (size_t i = 0; i &lt; vsize; i++)\n    {\n        tmp += x[i];\n    }\n\n    cout &lt;&lt; \"dry run \" &lt;&lt; tmp &lt;&lt; endl;\n\n    auto start = chrono::high_resolution_clock::now();\n    long long suma = accumulate(x.begin(),x.end(),0);\n    auto end = chrono::high_resolution_clock::now();\n\n    cout &lt;&lt; \"Accumulate runtime \" &lt;&lt; chrono::duration_cast&lt;chrono::nanoseconds&gt;(end-start).count() &lt;&lt; \" - \" &lt;&lt; suma &lt;&lt; endl;\n\n    start = chrono::high_resolution_clock::now();\n    suma = 0;\n    for (size_t i = 0; i &lt; vsize; i++)\n    {\n        suma += x[i];\n    }\n    end = chrono::high_resolution_clock::now();\n\n    cout &lt;&lt; \"Manual sum runtime \" &lt;&lt; chrono::duration_cast&lt;chrono::nanoseconds&gt;(end-start).count() &lt;&lt; \" - \" &lt;&lt; suma &lt;&lt;  endl;\n\n    return 0;\n}\n</code></pre>\n"},{"tags":["performance","language-agnostic","compiler"],"answer_count":30,"favorite_count":22,"up_vote_count":41,"down_vote_count":13,"view_count":4335,"score":28,"question_id":405770,"title":"Why are compilers so stupid?","body":"<p>I always wonder why compilers can't figure out simple things that are obvious to the human eye. They do lots of simple optimizations, but never something even a little bit complex. For example, this code takes about 6 seconds on my computer to print the value zero (using java 1.6):</p>\n\n<pre><code>int x = 0;\nfor (int i = 0; i &lt; 100 * 1000 * 1000 * 1000; ++i) {\n    x += x + x + x + x + x;\n}\nSystem.out.println(x);\n</code></pre>\n\n<p>It is totally obvious that x is never changed so no matter how often you add 0 to itself it stays zero. So the compiler could in theory replace this with System.out.println(0).</p>\n\n<p>Or even better, this takes 23 seconds:</p>\n\n<pre><code>public int slow() {\n    String s = \"x\";\n    for (int i = 0; i &lt; 100000; ++i) {\n        s += \"x\";\n    }\n    return 10;\n}\n</code></pre>\n\n<p>First the compiler could notice that I am actually creating a string s of 100000 \"x\" so it could automatically use s StringBuilder instead, or even better directly replace it with the resulting string as it is always the same. Second, It does not recognize that I do not actually use the string at all, so the whole loop could be discarded!</p>\n\n<p>Why, after so much manpower is going into fast compilers, are they still so relatively dumb?</p>\n\n<p><strong>EDIT</strong>: Of course these are stupid examples that should never be used anywhere. But whenever I have to rewrite a beautiful and very readable code into something unreadable so that the compiler is happy and produces fast code, I wonder why compilers or some other automated tool can't do this work for me.</p>\n"},{"tags":["ruby-on-rails","ruby-on-rails-3","performance","time-series"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":13,"score":0,"question_id":13243137,"title":"Efficiently plot database activity as a time series","body":"<p>I am working on a system where 200,000+ records have been created in the past year and I need to plot their creation on a time series with various added filters. At this present, this requires performing lots of <code>count</code> queries (30 for each month plotted). How should these dates be stored for maximum speed?</p>\n\n<p>One idea: store the most commonly-visualized data in a number of serialized fields containing <code>count</code>s for each day over the past month. Update each day with <code>cron</code> and serve up as necessary. (Where should these be stored - some new database table or a separate file accessible by Heroku cron?)</p>\n"},{"tags":["mysql","performance","table","sql-tuning"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":38,"score":0,"question_id":13241558,"title":"Slow mysql query on large table with GROUP BY, AVG, etc","body":"<p>I have a query on a large table(over 2 million rows) which takes ~10 seconds to complete. Is there any way to optimize it?  The query is as below:</p>\n\n<pre><code>SELECT\n    DATE_FORMAT(date0, '%Y-%m' ) AS Yr_Mo, \n    DATE_FORMAT(date0, '%p' ) AS AM_PM,\n    province AS Province,\n    SUM( IF( top_ads + left_ads =0, 1, 0 ) ) AS pagesWithRightAdsOnly, \n    AVG( top_ads ) AS top_ads, \n    AVG( left_ads ) AS left_ads, \n    AVG( right_ads ) AS right_ads\nFROM ad_counts\nGROUP BY Yr_Mo, AM_PM, Province\n</code></pre>\n\n<p>The table 'ad_counts':</p>\n\n<pre><code>date0 (timestamp)\nprovince(varchar)\nkeyword_id\nnumber_ads(int)\ntop_ads (int)\nleft_ads (int)\nright_ads (int)\n</code></pre>\n\n<p>Index on date0, but date0 is not unique.</p>\n\n<p>Any thoughts?</p>\n\n<p>Edit:\nEXPLAIN:</p>\n\n<pre>\nid  select_type table   type    possible_keys   key key_len ref rows    Extra\n1   SIMPLE  baidu_pro_ad_counts ALL NULL    NULL    NULL    NULL    2160752 Using temporary; Using filesort\n</pre>\n"},{"tags":["performance","magento","cart"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":24,"score":1,"question_id":13241954,"title":"magento - slow add to cart - high number of queries","body":"<p>we are running an Enterprise instance and see some strange behaviour on add to cart where a transaction trace shows inventory &amp; status being checked multiple times - even for a small'ish basket (e.g. 20 items).  We run new-relic and can see hundreds of calls in the trace and while they respond quickly they add up given they are called so many times.  Attached shows a sample trace for a simple cart add.  We are running nginx, varnish, apc as core config.  Any ideas/help greatly appreciated.  Anyone seen similar before?</p>\n\n<p>load time: 14,871   </p>\n\n<p>98.82% Mage_Core_Controller_Varien_Action::dispatch</p>\n\n<p>6.13%</p>\n\n<p>156 fast method calls        2.050s 1.0 0.01%\ncatalogrule_product_price - SELECT   2.050s 1.0\ncatalog_product_entity_group_price - SELECT 2.054  s 0.0\ncatalog_product_flat_1 - SELECT      2.059  s\ncataloginventory_stock_status - SELECT       2.061  s\ncatalog_product_flat_1 - SELECT      2.073  s\ncataloginventory_stock_status - SELECT       2.082  s\ncatalog_product_flat_1 - SELECT      2.087  s\ncataloginventory_stock_status - SELECT       2.097  s\ncataloginventory_stock_status - SELECT       2.104  s\nand so on for hundreds of calls..</p>\n"},{"tags":["ios","performance","maps","mkmapview"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":40,"score":1,"question_id":13239838,"title":"iOS6 Maps performance issue","body":"<p>On iPhone4s/iPad2, apps using a fullsize MKMapView exhibit a <strong>much worse scrolling performance</strong> than in the stock Maps app. Especially on fast scroll gestures it lags very bad. </p>\n\n<p>It seems that loading the vectors/tiles needs much time. When I test it with satellite maps it runs smoothly (so issue just comes up in vector based map).</p>\n\n<p><strong>I verified this issue:</strong></p>\n\n<ul>\n<li>in own apps and Instagram (Map feature)</li>\n<li>on iPhone4s and iPad2</li>\n</ul>\n\n<p>On my iPad there was iOS 5 still running. I checked in iOS5 my App's map + Instagram's map and both were smoothly. After that I updated it to 6.0.1 and both maps of the apps were lagging.</p>\n\n<p><strong>How to reproduce?</strong></p>\n\n<ul>\n<li>create a view based iPhone Application in Xcode </li>\n<li>add a mapview to the generated xib (fullscreen)</li>\n<li>launch it on your device an scroll very fast</li>\n</ul>\n\n<p><strong>How I did work around it:</strong></p>\n\n<p>I set the map to satellite type and overlay it with OSM vectors (http://wiki.openstreetmap.org/wiki/OSM_in_MapKit). That runs smoothly but needs double traffic (for satellite pictures and osm vectors)</p>\n\n<p><strong>I don't like this workaround, so does anybody noticed this issue?</strong></p>\n\n<p>Update: I filled a bug with the Apple Bug reporter - ID:<strong>12638328</strong> feel free to participate</p>\n"},{"tags":["performance","algorithm","data-structures","stack"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":67,"score":2,"question_id":13223574,"title":"Retrieving the Min element in a stack in O(1) Time","body":"<p>The reason I'm asking this question is because I cannot see why the way I think cannot be applied to this particular quesiton</p>\n\n<p><i>\"How would you design a stack which, \nin addition to push and pop, also has a function min which returns the minimum element? Push, pop and min should all operate in O(1) time</i>\"</p>\n\n<p><b>My basic solution: </b>Wouldn't it be possible if we had a variable in <b>stack</b> class, that whenever we were pushing an item to stack we would check if it is <b>smaller</b> than our <b>min</b> variable. If it is assign the value to the min, if not ignore.</p>\n\n<p>You would still get the O(1) as the min fucntion would be;</p>\n\n<pre><code>int getMinimum(){\n  return min;\n}\n</code></pre>\n\n<p>Why this solution is never mentioned, or what is the fault with the way I think?</p>\n"},{"tags":["mysql","crash","load","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":228,"score":1,"question_id":859072,"title":"Mysql Connections topping out within seconds","body":"<p>i have aweird problem that has only just started happening.</p>\n\n<p>i have a small cluster (one web and one db) setup and i host a rather popular group of4 -5 sites that allow users to dynamicly create their own mobile chat communitys automaticly. each site gets its own mysql db createdand populated automaticly.</p>\n\n<p>this is all fine, </p>\n\n<p>but in the last 24hours weird things have begun happening,\npreviously i had the sql max_connections set to 500 and this was perfectly adaqute for the demand but now even when i set the connection to 4000+ they are all maxxed out within 5-10 minutes, and mysql processlist shows thousands of unauthenticated user connections sitting at login status,</p>\n\n<p>i have gone through the sites and all their mysql configs are fine so i cant see what the issue is. </p>\n\n<p>server specs below</p>\n\n<p>db server:</p>\n\n<ul>\n<li>dual amd opteron 246</li>\n<li>8GB ram</li>\n<li>120gb hd(64gb free)</li>\n<li>33gb swap (rarly used but their for emergencys)</li>\n<li>centos 5 64bit.</li>\n<li>direct 100mbit lan to web serv</li>\n</ul>\n\n<p>only mysql,ssh and webmin running, no other apps installed</p>\n\n<p>web server:</p>\n\n<ul>\n<li>amd athlon 64 3800+</li>\n<li>plesk 9.2.1 </li>\n<li>4gb rram</li>\n<li>2x120gb hds</li>\n</ul>\n\n<p>apache status onthe web server only shows 120ish http connections butthe sql keeps climbing </p>\n"},{"tags":["javascript","regex","performance"],"answer_count":2,"favorite_count":1,"up_vote_count":8,"down_vote_count":0,"view_count":300,"score":8,"question_id":9750338,"title":"Dynamic vs Inline RegExp performance in JavaScript","body":"<p>I stumbled upon that performance test, saying that RegExps in JavaScript are not necessarily slow: <a href=\"http://jsperf.com/regexp-indexof-perf\">http://jsperf.com/regexp-indexof-perf</a></p>\n\n<p>There's one thing i didn't get though: two cases involve something that i believed to be exactly the same:</p>\n\n<pre><code>RegExp('(?:^| )foo(?: |$)').test(node.className);\n</code></pre>\n\n<p>And</p>\n\n<pre><code>/(?:^| )foo(?: |$)/.test(node.className);\n</code></pre>\n\n<p>In my mind, those two lines were <strong>exactly</strong> the same, the second one being some kind of shorthand to create a RegExp object. Still, it's <strong>twice faster</strong> than the first.</p>\n\n<p>Those cases are called \"dynamic regexp\" and \"inline regexp\".</p>\n\n<p>Could someone help me understand the difference (and the performance gap) between these two?</p>\n"},{"tags":["performance","matlab","signal-processing","time-series","vectorization"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":49,"score":1,"question_id":13238926,"title":"how to find difference between two vectors as a function of time-lag in MATLAB","body":"<p>First time posting a question here! Will happily take any advice<code>||</code>criticism I can get.</p>\n\n<p>We have two vectors: <code>v1</code> and <code>v2</code>. Assume <code>length(v1)</code> >> <code>length(v2)</code>. I move a window of size <code>length(v2)</code> along the vector <code>v1</code>. At each lag index, from the windowed portion of <code>v1</code> I subtract <code>v2</code>. I then sum the terms of the resulting vector and return this sum for every lag index along the length of vector <code>v1</code>. For simplicity, let's ignore the edge cases.</p>\n\n<p>I've done this with a <code>for</code> loop, but the length of my vectors is on the order of 10^9 [and larger], and even though the calculation is simple, it appears to take a long time to just iterate through the whole thing.</p>\n\n<p>Any ideas? I suspect there is a function that does something like this, but I have had no luck finding it.</p>\n"},{"tags":["performance","matlab"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":31,"score":3,"question_id":13237377,"title":"Why mutating cell array in a class property is so slow?","body":"<p>For example, I have the following class:</p>\n\n<pre><code>classdef testclass &lt; handle\n    properties\n            buckets\n    end\n    methods\n        function tc = testclass(sz)\n            tc.buckets = cell(1, sz);\n        end\n        function put(tc,k)\n            tc.buckets{k}{1} = 1;\n        end\n    end\nend\n</code></pre>\n\n<p>And the following example loop, where I compare performance with a plain cell array:</p>\n\n<pre><code>tm = @java.lang.System.currentTimeMillis;\n\nfor N=[100 200 400 1000 2000 4000 8000]\n    tc = testclass(N);\n    Tstart = tm();\n    for k=1:N\n        tc.put(k);\n    end\n    Tend = tm();\n    fprintf(1, 'filling hash class (size %d): %d ms\\n', N, Tend - Tstart);\n\n    arr = cell(1,N);\n    Tstart = tm();\n    for k=1:N\n        arr{k}{1} = 1;\n    end\n    Tend = tm();\n    fprintf(1, 'filling cell array (size %d): %d ms\\n', N, Tend - Tstart);\nend\n</code></pre>\n\n<p>The output is:</p>\n\n<pre><code>filling hash class (size 100): 8 ms\nfilling cell array (size 100): 0 ms\nfilling hash class (size 200): 9 ms\nfilling cell array (size 200): 0 ms\nfilling hash class (size 400): 24 ms\nfilling cell array (size 400): 1 ms\nfilling hash class (size 1000): 108 ms\nfilling cell array (size 1000): 2 ms\nfilling hash class (size 2000): 370 ms\nfilling cell array (size 2000): 5 ms\nfilling hash class (size 4000): 1396 ms\nfilling cell array (size 4000): 10 ms\nfilling hash class (size 8000): 5961 ms\nfilling cell array (size 8000): 21 ms\n</code></pre>\n\n<p>As you can see, plain cell array exhibits \"linear\" performance (which is to be expected), but array wrapped in a class gives horrible quadratic performance.</p>\n\n<p>I tested this on Matlab 2008a and Matlab 2010a.</p>\n\n<p>What causes this? And how can I work around it?</p>\n"},{"tags":["performance","windows-phone-7","asynchronous","outofmemoryexception","loadimage"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":19,"score":0,"question_id":13237048,"title":"LowProfileImageLoader get OutOfMemoryException Windows Phone","body":"<p>I am using LowProfileImageLoader  to load images async without block UI.</p>\n\n<p>You can get it from link: <a href=\"http://blogs.msdn.com/b/delay/archive/2011/03/03/quot-your-feedback-is-important-to-us-please-stay-on-the-line-quot-improving-windows-phone-7-application-performance-is-even-easier-with-these-lowprofileimageloader-and-deferredloadlistbox-updates.aspx\" rel=\"nofollow\">LowProfileImageLoader </a></p>\n\n<p>Everything is OK if DataSource of List has many different links.</p>\n\n<p><strong>But now, I want to test with DataSource with all same URL so In file TwitterService.cs, I edit followed:</strong></p>\n\n<pre><code>private static void HandleGetFollowersResponse(IAsyncResult result)\n        {\n            var state = (GetFollowersState)result.AsyncState;\n//#if DEBUG\n            try\n            {\n//#endif\n                //using (var response = state.Request.EndGetResponse(result))\n                //{\n                //    using (var stream = response.GetResponseStream())\n                //    {\n                //        var document = XDocument.Load(stream);\n                //        Deployment.Current.Dispatcher.BeginInvoke(() =&gt;\n                //        {\n                //            foreach (var user in document.Element(\"users_list\").Element(\"users\").Elements(\"user\"))\n                //            {\n                //                state.Collection.Add(new TwitterUser(user.Element(\"screen_name\").Value, new Uri(user.Element(\"profile_image_url\").Value)));\n                //            }\n                //        });\n                //        var nextCursor = document.Element(\"users_list\").Element(\"next_cursor\").Value;\n                //        if (\"0\" == nextCursor)\n                //        {\n                //            // Load completed\n                //            if (null != state.OnFollowersLoaded)\n                //            {\n                //                Deployment.Current.Dispatcher.BeginInvoke(() =&gt; state.OnFollowersLoaded());\n                //            }\n                //        }\n                //        else\n                //        {\n                //            // Load the next set\n                //            MakeGetFollowersRequest(state.ScreenName, nextCursor, state.Collection, state.OnFollowersLoaded);\n                //        }\n                //    }\n                //}\n\n                throw new WebException();\n//#if DEBUG\n            }\n            catch (WebException)\n            {\n                 //No network access; create some fake users for debugging purposes\n                for (var i = 0; i &lt; 200; i++)\n                {\n                    state.Collection.Add(new TwitterUser(\"Fake User \" + i, new Uri(\"http://4.bp.blogspot.com/-O-6vxSiyvbk/UClib6CiR0I/AAAAAAAAQaI/5Fr1dI-kQBI/s1600/Flowers+beauty+desktop+wallpapers.+(1).jpg\")));\n                }\n                if (null != state.OnFollowersLoaded)\n                {\n                    Deployment.Current.Dispatcher.BeginInvoke(() =&gt; state.OnFollowersLoaded());\n                }\n            }\n//#endif\n        }\n</code></pre>\n\n<p><strong>It will create dataSource of List with 200 item that has same URL to test.</strong></p>\n\n<p><em><strong>In LowProfileImageLoader.cs, I log the currentUsedMemory of app:</em></strong></p>\n\n<pre><code>private static void WorkerThreadProc(object unused)\n    {\n            // Process pending completions\n                    if (0 &lt; pendingCompletions.Count)\n                    {\n                        // Get the Dispatcher and process everything that needs to happen on the UI thread in one batch\n                        Deployment.Current.Dispatcher.BeginInvoke(() =&gt;\n                        {\n\n                            while (0 &lt; pendingCompletions.Count)\n                            {\n\n                                Logger.printUsedMemory();\n                                }\n                            }\n                        }\n\n    }\n</code></pre>\n\n<p>Logger.printUsedMemory() is helper function to help me log current used memory of my app.\nNothing edit anymore in code.</p>\n\n<p>But when run my app, the result is strange:\n<strong>I get OutOfMemoryException.</strong></p>\n\n<p><em><strong>Text log in Output windows (used memory in bytes) bellow:</em></strong></p>\n\n<pre><code> Used memory: 21966848\n Used memory: 25051136\n Used memory: 28442624\n Used memory: 32673792\n Used memory: 35512320\n Used memory: 39079936\n Used memory: 43364352\n Used memory: 46571520\n Used memory: 49815552\n Used memory: 53497856\n Used memory: 52514816\n Used memory: 55902208\n Used memory: 60452864\n Used memory: 62001152\n Used memory: 65503232 ~ 65.5mb\n Used memory: 69005312 ~ 69mb\n</code></pre>\n\n<p>Text log show that <strong>after loading image memory increase ~ 3mb</strong> although <strong>image size (from URL) only 120kb.</strong></p>\n\n<p>Why does the OutofMemoryException throws ?\nWhy garbage collection not call and my memory increased steady ?</p>\n\n<p>Any help is very apprepriate.\nThank in advance.</p>\n"},{"tags":["visual-studio-2010","performance","internet"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":70,"score":0,"question_id":13183772,"title":"VS2010 slowed to crawl when Hurricane Sandy killed internet connection","body":"<p>I'm in the greater New York area, and hurricane sandy has left us with power, but has killed our Internet connection. (I'm typing this on my ipad at a local McDonalds)</p>\n\n<p>This has dramatically slowed VS2010 when starting a debug session within the IDE. There now seems to be a long pause...ten seconds for each dll...for every dll referenced in the project.  Now it takes several minutes every time we run our project.  </p>\n\n<p>The project itself has absolutely no Internet functionality...seems to be strictly related to VS 2010. </p>\n\n<p>Disabling MS Symbol Server had no effect, and I could not find anything else in VS settings that would need the Internet.</p>\n\n<p><strong>Edit</strong></p>\n\n<p>We now have internet again, and startup times are back to around 20 seconds.</p>\n\n<p>Previously, I also tried disabling Symbol Server again, and this time it did have a major effect - reduced startup time from 4 minutes to 1 minute.  Don't know why I did not see the same result the first time I tried.</p>\n\n<p>Some testing with Win7 Resource Monitor indicates that VS is trying to talk to 224.0.0.251 and 252...Multicast DNS address and Link-local Multicast Name Resolution address respectively). </p>\n\n<p>Also 239.255.255.250 (Simple Service Discovery Protocol).  Got the definitions by looking up the IP addresses on Wikipedia, but that does not really help me.</p>\n\n<p>Any suggestions on how to track this down?</p>\n"},{"tags":["c#","asp.net","asp.net-mvc","asp.net-mvc-3","performance"],"answer_count":0,"favorite_count":1,"up_vote_count":1,"down_vote_count":1,"view_count":56,"score":0,"question_id":13238988,"title":"MVC3 App High Memory Usage","body":"<p>I have an MVC App that when deployed to the test server uses quite a bit of memory.  The Worker Process for the app pool that is running under is right around 1GB of memory usage. What is the best way to debug something like this?  I am not currently explicitly calling Garbage Collection because I have been told it is not necessary in MVC3.   </p>\n"},{"tags":["performance","cuda"],"answer_count":0,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":80,"score":6,"question_id":13236408,"title":"break overhead vs control flag","body":"<p>I was using a naive prime generated function. This code takes about, 5.25 seconds to generate 10k prime numbers (device_primes[0] holds the number primes already found, the remaining position the prime numbers found).</p>\n\n<pre><code>_global__ void getPrimes(int *device_primes,int n)\n{ \n    int c = 0;\n    int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n    int num = thread_id+2;\n\n    if (thread_id == 0) device_primes[0] = 1;\n    __syncthreads();\n\n    while(device_primes[0] &lt; n)\n    {\n        for (c = 2; c &lt;= num - 1; c++)\n        { \n            if (num % c == 0) //not prime\n            {\n                break;\n            }\n        }\n        if (c == num) //prime\n        {\n            int pos = atomicAdd(&amp;device_primes[0],1);\n            device_primes[pos] = num;\n        }\n        num += blockDim.x * gridDim.x; // Next number for this thread       \n    }\n}\n</code></pre>\n\n<p>I was just starting to optimize the code, and i made the follow modification, instead of :</p>\n\n<pre><code>for (c = 2; c &lt;= num - 1; c++)\n{ \n    if (num % c == 0) //not prime\n         break;\n}\n if (c == num) {...}\n</code></pre>\n\n<p>i have now :</p>\n\n<pre><code>   int prime = 1;\n\n   ...\n   for (c = 2; c &lt;= num - 1 &amp;&amp; prime; c++)\n    { \n        if (num % c == 0) prime = 0; // not prime\n    }\n     if (prime) {...} // if prime\n</code></pre>\n\n<p>Now i can generate 10k in 0.707s. I was just wondering why such speed up with a this simple modification, is break that bad?</p>\n"},{"tags":["iphone","ios","performance","phonegap","titanium"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":560,"score":0,"question_id":9980079,"title":"Why does PhoneGap seem faster than Titanium?","body":"<p>I'm trying to measure the execution perfomance of a few cross-platform solutions, among which are: Titanium and PhoneGap.</p>\n\n<p>So here's an example of the Titanium version of my performance tester, it's very simple, but I'm just trying to get a feeling of how fast my code gets executed:</p>\n\n<pre><code>var looplength;\nvar start1;\nvar start2;\nvar end1;\nvar end2;\nvar duration1;\nvar duration2;\nvar diff;\nvar diffpiter;\nvar power;\nvar info;\n\nfor (power = 0; power &lt; 24; power++) {\n  looplength = Math.pow(2, power);\n\n  start1 = new Date().getTime();\n  for (iterator = 0; iterator &lt; looplength; iterator++) {a=iterator;b=iterator;}\n  end1 = new Date().getTime();\n\n  start2 = new Date().getTime();\n  for (iterator = 0; iterator &lt; looplength; iterator++) {a=iterator;}\n  end2 = new Date().getTime();\n\n  duration1 = end1 - start1;\n  duration2 = end2 - start2;\n  diff      = duration1 - duration2;\n  diffpiter = diff / looplength;\n\n  info={title:'2^' + power + ' ' + diffpiter};\n  tableView.appendRow(Ti.UI.createTableViewRow(info),{animated:true});\n}\n</code></pre>\n\n<p>The PhoneGap version is the same except for the last two lines which get replaced </p>\n\n<pre><code>document.write('2^' + power + ' ' + diffpiter + '&lt;br /&gt;');\n</code></pre>\n\n<p>Both are executed on an iPhone 4S. I've run the test numerous times, to eliminate errors.</p>\n\n<p>How in the name of all that is holy can the Titanium version measure <code>~0.0009</code> milliseconds per iteration while the PhoneGap version measures <code>~0.0002</code> milliseconds per iteration?</p>\n\n<p>Titanium is supposed to compile my javascript code so I expect it to be faster. In this case however it's at least <strong>4 times</strong> slower! I'm not an expert on performance testing, but the test I designed should be at least remotely accurate...</p>\n\n<p>Thank you for any tips you can give me.</p>\n"},{"tags":["asp.net","performance","architecture","thread-safety","application-pool"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":186,"score":-1,"question_id":9558333,"title":"Confusion about Worker Process threading","body":"<p>Try open the following page in <strong>two different tabs</strong> in your browser.  hit the refresh button to avoid getting the page from browser cache :</p>\n\n<pre><code>    protected void Page_Load(object sender, EventArgs e)\n    {\n        Thread.Sleep(10000);\n        Response.Write(DateTime.Now.ToString());\n    }\n</code></pre>\n\n<p>As you can see, it seems that there is a thread created for every request, and they did not wait for each other there is no lock acquired.  </p>\n\n<p>Now Create the following pages, and GlobalCustomClass</p>\n\n<p><strong>GlobalCustomClass</strong></p>\n\n<pre><code>public class GlobalCustomClass\n    {\n        public static string GlobalVariable\n        {\n            get;\n            set;\n        }\n    }\n</code></pre>\n\n<p><strong>Default.aspx</strong></p>\n\n<pre><code>protected void Page_Load(object sender, EventArgs e)\n        {\n            GlobalCustomClass.GlobalVariable = \"Default page\";\n            Thread.Sleep(10000);\n            Response.Write(DateTime.Now.ToString());\n            Response.Write(\"&lt;br /&gt;\");\n            Response.Write(GlobalCustomClass.GlobalVariable);\n        }\n</code></pre>\n\n<p><strong>Page2.aspx</strong></p>\n\n<pre><code> protected void Page_Load(object sender, EventArgs e)\n        {\n            Response.Write(DateTime.Now.ToString());\n            Response.Write(\"&lt;br /&gt;\");\n            GlobalCustomClass.GlobalVariable = \"Page2\";\n            Response.Write(GlobalCustomClass.GlobalVariable);\n\n        }\n</code></pre>\n\n<p>Refresh Default page, and before 10 secs elapses, refresh Page2....Default.aspx is rendering \"Page2\".  Yes it is not thread safe.  </p>\n\n<p>Now try this, open the following page in two browser tabs: </p>\n\n<pre><code>    protected void Page_Load(object sender, EventArgs e)\n    {\n         Session[\"x\"] = \"ABC\";\n         Thread.Sleep(10000);\n         Response.Write(DateTime.Now.ToString());\n    }\n</code></pre>\n\n<p>Now it seems that first thread lock on the session object, and the other have to wait for the whole 10 secs!!</p>\n\n<p>Now try First case but put Global.ascx in the project...and say what, first thread locks on something!!!!!</p>\n\n<p>In real application, it is common to access global state variables that need a thread safety like Sessions.  </p>\n\n<p>If you have a Backend application that contains a report that need 30 secs to render.  and if 60 users opens that report, then user number 61 will wait for half hour till page load into his browser!!  That is a Typical thread starvation!!  and I will be probably fired :(</p>\n\n<p><strong>1)</strong> Every user is waiting till another finish his request. that should make any heavy page a problem.  because it can make the whole web application in-responsive.  <strong>Agree?</strong></p>\n\n<p><strong>2) How to fix that situation?</strong> </p>\n\n<p><strong>3) Under which topic I should research about that? Do you know a good book?</strong></p>\n\n<p>Thanks</p>\n"},{"tags":["performance","ipad","ios6","mkmapview"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":97,"score":1,"question_id":13040044,"title":"MKMapView iOS6 - buggy and slow on scroll and zoom","body":"<p>with the iOS5 MKMapView (google maps) i could zoom and scroll fine on any device.\nNow with iOS6 MKMapView (apple maps) when I start scrolling around the app ends up hanging pretty much every time. \nThis worked fine under iOS5</p>\n\n<p>issues\n- new memory footprint of apple maps is 15x compared to google maps\n- MKMapView becomes unresponsive on my ipad2 in full screen if you scroll around enough ( 20 seconds)\n- annotations start jumping around as well as it begins to become unresponsive</p>\n\n<p>Has anyone got a nice working example on ipad2 with a full screen MKMapView that lets you scroll and zoom without crashing?</p>\n\n<p>Thanks</p>\n"},{"tags":["asp.net","profiling","performance","query-optimization","production-environment"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":675,"score":0,"question_id":3572318,"title":"possible ways of profiling an asp.net website in a production server?","body":"<p>I have an asp.net website up and running in my production server. I want to get the possible ways of profiling an asp.net website in a production server because my application is really slow? As i say slow i don't mean the delivery of static content but the database operations and my c# code? So any suggestion?</p>\n"},{"tags":["c#","performance","performance-testing","evaluation","evaluator"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":7,"view_count":55,"score":-6,"question_id":13237232,"title":"Performance Evaluator in C#","body":"<p>I need to develop an evaluation tool using C# that will run on the system for hours and then will show the overal performance of the system. </p>\n\n<p>The system is supposed to run a service and we want to evaluate how this service is affecting the performance of the system. Will be great if I could use the performance counters that are available in \"Windows Performance Monitor\"... I'm not sure if there is any API available for developers to use them.</p>\n\n<p>I was just looking for suggestions...</p>\n\n<p>Thanks</p>\n"},{"tags":["javascript","html","css","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":35,"score":1,"question_id":13236427,"title":"Auditing front end performance on web application","body":"<p>I am currently trying to performance tune the UI of a company web application. The application is only ever going to be accessed by staff, so the speed of the connection between the server and client will always be considerably more than if it was on the internet. </p>\n\n<p>I have been using performance auditing tools such as Y Slow! and Google Chrome's profiling tool to try and highlight areas that are worth targeting for investigation. However, these tools are written with the internet in mind. For example, the current suggestions from a Google Chrome audit of the application suggests is as follows:</p>\n\n<p><strong>Network Utilization</strong></p>\n\n<ul>\n<li>Combine external CSS (Red warning)</li>\n<li>Combine external JavaScript (Red warning)</li>\n<li>Enable gzip compression (Red warning)</li>\n<li>Leverage browser caching (Red warning)</li>\n<li>Leverage proxy caching (Amber warning)</li>\n<li>Minimise cookie size (Amber warning)</li>\n<li>Parallelize downloads across hostnames (Amber warning)</li>\n<li>Serve static content from a cookieless domain (Amber warning)</li>\n</ul>\n\n<p><strong>Web Page Performance</strong></p>\n\n<ul>\n<li>Remove unused CSS rules (Amber warning)</li>\n<li>Use normal CSS property names instead of vendor-prefixed ones (Amber warning)</li>\n</ul>\n\n<p>Are any of these bits of advice totally redundant given the connection speed and usage pattern? The users will be using the application frequently throughout the day, so it doesn't matter if the initial hit is large (when they first visit the page and build their cache) so long as a minimal amount of work is done on future page views.</p>\n\n<p>For example, is it worth the effort of combining all of our CSS and JavaScript files? It may speed up the initial page view, but how much of a difference will it really make on subsequent page views throughout the working day? </p>\n\n<p>I've tried searching for this but all I keep coming up with is the standard internet facing performance advice. Any advice on what to focus my performance tweaking efforts on in this scenario, or other auditing tool recommendations, would be much appreciated.</p>\n"},{"tags":["android","performance","webview"],"answer_count":3,"favorite_count":9,"up_vote_count":5,"down_vote_count":0,"view_count":4805,"score":5,"question_id":7422427,"title":"Android webview slow","body":"<p>My android webviews are slow. This is on everything from phones to 3.0+ tablets with more than adequate specs</p>\n\n<p>I know that webviews are supposed to be \"limited\" but I see web apps done with phone gap that must be using all sorts of CSS3 and JQuery sorcery, they run just fine and speedy</p>\n\n<p>so I'm missing something, is there some kind of <code>myWebview.SPEEDHACK(1)</code> that I can use to speed things up? Thanks</p>\n\n<p>also, sometimes the contents of my webview just simply don't load, instead of slowly loading, it just wont load. The asset I am testing with is stored locally, no errors.</p>\n"},{"tags":["wcf","performance","windows-8","winrt"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":61,"score":0,"question_id":13231754,"title":"Why is my ARM chip/Surface processing WCF calls a lot slower than my i7 laptop, and is there anything I can do to speed it up?","body":"<p>I am diagnostic testing my winRT store application, and am noticing considerable performance differences between my Surface RT device and my i7 laptop.</p>\n\n<p>Now - i know there is a big difference in expected performance between an ARM CPU and an i7 - but when my average WCF web call on my i7 takes ~0.2s, and my surface device takes ~1.2s I am forced to start looking at optimization and improvements. If the performance difference between the two was only a few hundred milliseconds then I wouldn't mind so much, but the surface device does feel a little bit clunky - and the only bottleneck seems to be the services!</p>\n\n<p>Does anyone have an explanation, or even some performance improvements tips? I should mention that I am running the services across basicBinding with binary serialization. </p>\n"},{"tags":["performance","algorithm","rating"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":71,"score":-1,"question_id":13227946,"title":"Rating Algorithm","body":"<p>I'm trying to develop a rating system for an application I'm working on. Basically app allows you to rate an <b>object</b> from 1 to 5(represented by stars). But I of course know that keeping a rating count and adding the rating the number itself is <b>not</b> feasible.</p>\n\n<p>So the first thing that came up in my mind was <b>dividing the received rating by the total ratings given.</b> Like if the object has received the rating 2 from a user and if the number of times that object has been rated is 100 maybe adding the 2/100. However I believe this method is not good enough since 1)A naive approach 2) In order for me to get the number of times that object has been rated I have to do a look up on db which might end up having time complexity O(n)</p>\n\n<p>So I was wondering what alternative and possibly <b>better</b> ways to approach this problem?</p>\n"},{"tags":["mysql","performance","view","subquery","table-relationships"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":13113411,"title":"Dissolve Nested Subqueries in View","body":"<p>First, I = MySQL.stupid, so please bear with me.</p>\n\n<p>I don't even come close to having the vocabulary to search this out, so apologies if this has already been covered.</p>\n\n<p>I've seen on many pages (older) that nested subqueries can degrade performance.  I'd like to avoid that.  Can anyone show me how to \"dissolve\" them properly into my view if possible?</p>\n\n<p>Please help.</p>\n\n<p>Many thanks in advance!</p>\n\n<pre><code>    CREATE VIEW database1.table1_output \nAS \n  SELECT table1.id                                  AS id, \n     table1.table1column1                       AS table1column1, \n     (SELECT COALESCE(Sum(table2column1), 0) \n      FROM   table2 \n      WHERE  table2column2 = 1 \n             AND table1_id = table1.id) - COALESCE(Sum( \n                                          Abs(table3.table3column1)), 0) \n                                                AS sum1, \n     (SELECT COALESCE(Count(table3column2), 0) \n      FROM   table3 \n      WHERE  ( table3column2 = table4.table4column1 \n                OR table4.table4column1 IS NULL ) \n             AND table1_id = table1.id) - (SELECT \n     COALESCE(Count(table3column2 \n              ), 0) \n                                           FROM   table3 \n                                           WHERE  table3column2 = \n                                                  -1 * table4.table4column1 \n                                                  AND table1_id = table1.id) \n     - COALESCE(Count(table3.table3column3), 0) AS sum2 \n  FROM   table4, \n     table3, \n     table1 \n  WHERE  table3.table1_id = table1.id \n     AND table4.id = table3.article_id \n  GROUP  BY table1.id \n</code></pre>\n\n<p><strong>EDIT</strong></p>\n\n<p>I got rid of the low-hanging fruit, and now my problem is isolated to those three selects.  I'll probably change the title to reflect that; however, I have no idea what the problem is, so I can't even get started on terminology.</p>\n"},{"tags":["sql","performance","stored-procedures"],"answer_count":2,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":39,"score":3,"question_id":13202227,"title":"SQL stored proc runs extremely slow when filtering by high row numbers","body":"<p>This query is generated from a very long dynamic sequel stored procedure -- the procedure returns the requested number of records starting at a given index to be displayed in a Telerik Radgrid, effectively handling paging.  A simplified version of the stored proc's output:</p>\n\n<pre><code>SELECT r.* FROM (\n       SELECT ROW_NUMBER() OVER(ORDER BY InventoryId DESC) as row,\n       v.* FROM vInventorySearch v\n       ) as R WHERE [ROW] BETWEEN 1 AND 10\n</code></pre>\n\n<p>When the \"BETWEEN\" clause is between 1 and 10, it runs in a fraction of a second, but if it's between something like 10000 and 1010 it takes almost a full minute to execute.</p>\n\n<p>I feel like I may be missing something fundamental here, but it seems to me that it shouldn't matter which 10 records I'm retrieving, it should take the same amount of time.</p>\n\n<p>Thanks for any input, I'm looking forward to being embarrassed!</p>\n\n<hr>\n\n<p>Solution, courtesy Martin Smith (below) :</p>\n\n<pre><code>SELECT r.*, inv.* FROM \n(\n    SELECT ROW_NUMBER() OVER(ORDER BY InventoryId DESC) as row, v.InventoryID\n    FROM vInventorySearch v\n    WHERE 1=1 \n) as R \ninner join vInventory inv on r.InventoryID = inv.InventoryID\nWHERE [ROW] BETWEEN 10001 AND 10010\n</code></pre>\n\n<p>Thanks for your help!</p>\n"},{"tags":["java","eclipse","performance","windows-7","scala-ide"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":36,"score":1,"question_id":13050300,"title":"When Package Explorer is pinned open Eclipse takes a long time to redraw itself when restored","body":"<p>I'm out of ideas so I'm posing the question to StackOverflow.  I recently started a scala project.  I setup Eclipse 3.7.2 on Windows 7 and downloaded the latest stable release of the <a href=\"http://scala-ide.org/\" rel=\"nofollow\">Scala IDE</a> 2.0.2.  The problem I'm experiencing is that when the package explorer pane is pinned open in my workspace and I switch context away from Eclipse and then back to Eclipse, it takes a very long time for Eclipse to redraw itself.  It can take anywhere from 2 to 10 seconds to fully redraw.  I know it's related to the package explorer because I don't experience the issue at all when it's not open.  I'm not sure it's specifically related to Scala IDE, but all I've been using Eclipse for recently is with Scala projects and so far I've experienced this problem with all of them.</p>\n\n<p>So far I've tried optimizing my JVM settings as described in the <a href=\"http://stackoverflow.com/questions/142357/what-are-the-best-jvm-settings-for-eclipse\">following post</a>.  I've also tried reverting to a Java 6 JVM.</p>\n\n<p>I'm open for new suggestions or advice on how to troubleshoot this problem further.</p>\n\n<p>EDIT:</p>\n\n<p>I've since discovered there are many views similar to package explorer in the way that they let you navigate a project via it's directory or package structure.  There's the package explorer, project explorer, and navigation view.  Now that I'm using the navigation view I no longer experience any performance problems mentioned in my original post.  However, the question still stands: why does using package explorer have such a detrimental effect on Eclipse performance.</p>\n\n<p>EDIT 2: Scratch that, even while using other \"project navigation\" type views I still have the problem.</p>\n"},{"tags":["mysql","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":13228301,"title":"MySQL query performance slow down suddenly","body":"<p>I have a MySQL (InnoDB) database that contains tables with rows count between 1 000 000 and 50 000 000. \nAt night there is aggregating job which counts some information and writes them to reporting tables. </p>\n\n<p>Fist job execution is very fast. Every query executes between 100ms and 1s. \nAfter that almost every single query is very slow. </p>\n\n<p>The example query is: </p>\n\n<pre><code>SELECT count(*) FROM tableA \n  JOIN tableB ON tableA.id = tableB.tableA_id\n</code></pre>\n\n<p>execution plan for that query shows that for both tables indexes will be used. </p>\n\n<p>Important thing is that CPU, I/O, memory usage is very low. \nMySQL server version: 5.5.28 with default setup (just installed on windows 7 developer computer). </p>\n"},{"tags":[".net","wpf","performance",".net-3.5","measureoverride"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":686,"score":0,"question_id":6959012,"title":"WPF MeasureOverride called multiple times after multiple PropertyChanged notifications","body":"<p>In my application, I have a graph (Custom Wpf Panel) with Wpf controls as elements. The elements are custom controls derived from Wpf Control. DataTemplates associate the controls to their view models.\nThe view models collection, \"GraphElements\" is binded to the itemsControl as shown below.</p>\n\n<pre><code>&lt;ItemsControl x:Name=\"PART_GraphItemsControl\" Grid.Column=\"1\"\n              VerticalAlignment=\"Stretch\" \n              HorizontalAlignment=\"Left\"\n              ItemsSource=\"{Binding Path=GraphElements}\"\n              VirtualizingStackPanel.IsVirtualizing=\"True\"\n              VirtualizingStackPanel.VirtualizationMode=\"Recycling\"&gt;\n    &lt;ItemsControl.ItemsPanel&gt;\n        &lt;ItemsPanelTemplate&gt;\n            &lt;local:GraphDesignerPanel HorizontalAlignment=\"Stretch\"\n                                      VerticalAlignment=\"Top\" /&gt;\n        &lt;/ItemsPanelTemplate&gt;\n    &lt;/ItemsControl.ItemsPanel&gt;\n&lt;/ItemsControl&gt;\n</code></pre>\n\n<p>The elements in the graph can vary from 2 to 500. Under certain mode of the application, the elements display their value. To display the value, the ViewModel for the element fires the <code>INotifyPropertyChanged.PropertyChanged(\"VariableValue\")</code></p>\n\n<p><strong>Issue:</strong>\nWhen i have 100+ elements in the graph, each elements view model fires INotifyPropertyChanged.PropertyChanged to have the elements value displayed. This calls MeasureOverride 100+ times leading to memory and peformance issues.</p>\n\n<p>How can I reduce the number of MeasureOverride calls?</p>\n\n<h2>XAML for Value display for the graph element:</h2>\n\n<pre><code> &lt;TextBlock  \n    Text=\"{Binding Path=VariableValue, StringFormat={}({0})}\" Width=\"60\"\n    FontSize=\"11\" Name=\"txtBlk\"&gt;\n&lt;/TextBlock&gt;\n</code></pre>\n\n<p>The TextBlock above is collapsed if VariableValue is null</p>\n\n<pre><code>&lt;DataTrigger Binding=\"{Binding Path=VariableValue}\" Value=\"{x:Null}\"&gt;\n    &lt;Setter TargetName=\"txtBlk\"  Property=\"Visibility\" Value=\"Collapsed\"/&gt;\n&lt;/DataTrigger&gt;\n</code></pre>\n\n<p><strong>UPDATE:</strong> The issue is reproducible in the sample at the link given below. Download, build, debug. Once App is opened, set a breakpoint in Window.xaml.cs MeasureOverride. Come back to the App and hit the \"Click Me\" button. The breakpoint is hit 11 times !.</p>\n\n<p><a href=\"http://sivainfotech.co.uk/measureoverrideissue.zip\" rel=\"nofollow\">http://sivainfotech.co.uk/measureoverrideissue.zip</a></p>\n\n<p>Any ideas greatly appreciated.</p>\n"},{"tags":["performance","caching","joomla","session","joomla1.5"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":3162,"score":1,"question_id":2897765,"title":"Disabling user sessions for guest","body":"<p>Is it possible to disable session handling in Joomla 1.5 for guests?</p>\n\n<p>I do not use user system in the front end, i assumed that it's not needed to run queries like below:</p>\n\n<p>Site will use APC or Memcache as caching system under heavy load, so it's important for me.</p>\n\n<pre><code>DELETE FROM jos_session WHERE ( time &lt; '1274709357' )\n\nSELECT *  FROM jos_session WHERE session_id = '70c247cde8dcc4dad1ce111991772475'\n\nUPDATE `jos_session` SET `time`='1274710257',`userid`='0',`usertype`='',`username`='',`gid`='0',`guest`='1',`client_id`='0',`data`='__default|a:8:{s:15:\\\"session.counter\\\";i:5;s:19:\\\"session.timer.start\\\";i:1274709740;s:18:\\\"session.timer.last\\\";i:1274709749;s:17:\\\"session.timer.now\\\";i:1274709754;s:22:\\\"session.client.browser\\\";s:88:\\\"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.2.3) Gecko/20100401 Firefox/3.6.3\\\";s:8:\\\"registry\\\";O:9:\\\"JRegistry\\\":3:{s:17:\\\"_defaultNameSpace\\\";s:7:\\\"session\\\";s:9:\\\"_registry\\\";a:1:{s:7:\\\"session\\\";a:1:{s:4:\\\"data\\\";O:8:\\\"stdClass\\\":0:{}}}s:7:\\\"_errors\\\";a:0:{}}s:4:\\\"user\\\";O:5:\\\"JUser\\\":19:{s:2:\\\"id\\\";i:0;s:4:\\\"name\\\";N;s:8:\\\"username\\\";N;s:5:\\\"email\\\";N;s:8:\\\"password\\\";N;s:14:\\\"password_clear\\\";s:0:\\\"\\\";s:8:\\\"usertype\\\";N;s:5:\\\"block\\\";N;s:9:\\\"sendEmail\\\";i:0;s:3:\\\"gid\\\";i:0;s:12:\\\"registerDate\\\";N;s:13:\\\"lastvisitDate\\\";N;s:10:\\\"activation\\\";N;s:6:\\\"params\\\";N;s:3:\\\"aid\\\";i:0;s:5:\\\"guest\\\";i:1;s:7:\\\"_params\\\";O:10:\\\"JParameter\\\":7:{s:4:\\\"_raw\\\";s:0:\\\"\\\";s:4:\\\"_xml\\\";N;s:9:\\\"_elements\\\";a:0:{}s:12:\\\"_elementPath\\\";a:1:{i:0;s:74:\\\"C:\\\\xampp\\\\htdocs\\\\sites\\\\iv.mynet.com\\\\libraries\\\\joomla\\\\html\\\\parameter\\\\element\\\";}s:17:\\\"_defaultNameSpace\\\";s:8:\\\"_default\\\";s:9:\\\"_registry\\\";a:1:{s:8:\\\"_default\\\";a:1:{s:4:\\\"data\\\";O:8:\\\"stdClass\\\":0:{}}}s:7:\\\"_errors\\\";a:0:{}}s:9:\\\"_errorMsg\\\";N;s:7:\\\"_errors\\\";a:0:{}}s:13:\\\"session.token\\\";s:32:\\\"a2b19c7baf223ad5fd2d5503e18ed323\\\";}'\n\n  WHERE session_id='70c247cde8dcc4dad1ce111991772475'\n</code></pre>\n"},{"tags":["performance","delphi","report","fastreport"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":54,"score":0,"question_id":13231372,"title":"Can I use two types of barcode on Fast Report?","body":"<p>I'm want develop a system to print barcode using fastreport, but, I want print Code128B for numbers &lt; 13 and EAN13 for numbers = 13.</p>\n\n<p>How I can use two (2) types of barcode in my report? I using a databand.</p>\n\n<p>Thanks!</p>\n\n<ul>\n<li>I using Delphi and Fast Report 4.9</li>\n</ul>\n"},{"tags":["javascript","html","css","performance","improvements"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":43,"score":1,"question_id":13230332,"title":"Good Tools to Improve JavaScript, CSS and HTML performance","body":"<p>I have an app I've developed and put through PhoneGap Build, and am currently trying to find good tools to improve the performance of the app. It uses HTML, JavaScript &amp; CSS. I'm having trouble finding tools to compress the files and was hoping for suggestions to websites that have them. </p>\n\n<p>Thanks a lot in advance xxx</p>\n"},{"tags":["java","performance","jvm","daemon","startup"],"answer_count":5,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":1032,"score":3,"question_id":4056280,"title":"Anyway to Boost java JVM Startup Speed?","body":"<p>Well ,  it is said that Java is 10x faster than python in terms of performance, thats what i see from benchmarks too. But what really brings down java is its startup time of JVM which is quite stupid.</p>\n\n<p>This is a test i made:</p>\n\n<pre><code>$time xlsx2csv.py Types\\ of\\ ESI\\ v2.doc-emb-Package-9\n...\n&lt;output skipped&gt;\nreal    0m0.085s\nuser    0m0.072s\nsys     0m0.013s\n\n\n$time java  -jar -client /usr/local/bin/tika-app-0.7.jar -m Types\\ of\\ ESI\\ v2.doc-emb-Package-9\n\nreal    0m2.055s\nuser    0m2.433s\nsys     0m0.078s\n</code></pre>\n\n<p>Same file , a 12 KB ms XLSX embedded file inside Docx and Python is 25x faster !! WTH!!</p>\n\n<p>its takes 2.055 sec for Java , which is a joke,.</p>\n\n<p>I know it is all due to startup time, but what i need is i need to call it via a script to parse some documents which i do not want to re-invent the wheel in python.</p>\n\n<p>But as to parse 10k+ files , it is just not practical..</p>\n\n<p>Anyway to speed it up (I already tried -client option and it only speed up by so little(20%) ).</p>\n\n<p>My another idea? Run it as a long-running daemon , communicate using UDP or Linux-ICP sockets locally?</p>\n"},{"tags":["c#","performance","email","imap","imaplib"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":27,"score":1,"question_id":13229465,"title":"High performance IMAP library for .NET","body":"<p>I'm looking for a commercial library to use in my C# app for retrieving messages for IMAP servers. The library would be deployed on a server that handles multiple retrievals from different inboxes. As such, I'm looking for an async library that downloads and parses messages quickly and manages and disposed of its memory efficiently to handle inboxes of potentially hundreds of MB.\nI've tried a few commercial libraries (and ones suggested in other posts), but none of them provide all the required functionality </p>\n\n<p>Would like to know if anybody has successfully deployed a library in such an environment.</p>\n"},{"tags":["javascript","jquery","css","performance","frontend"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":51,"score":0,"question_id":13219367,"title":"The right way to remove a class with jQuery","body":"<p>I have a simple question that interests me a lot:\nIf I want to remove a CSS class with jQuery, what's the right way?\n1. removing after checking for the existence of the class?</p>\n\n<pre><code>if($(div).hasClass('css-class')) {\n  $(div).removeClass('css-class');\n}\n</code></pre>\n\n<p>2. just removing it?</p>\n\n<pre><code>$(div).removeClass('css-class');\n</code></pre>\n\n<p>3.any other suggestions?</p>\n"},{"tags":["wpf","performance","bing"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":175,"score":1,"question_id":8457613,"title":"WPF Application with Bing Maps Control painfully slow on startup","body":"<p>I currently work on a WPF application that has mapping support. I use the Bing Maps WPF Control (from here: <a href=\"http://www.microsoft.com/download/en/details.aspx?id=27165\" rel=\"nofollow\">http://www.microsoft.com/download/en/details.aspx?id=27165</a>) to help with the mapping but now there is a pretty big problem:</p>\n\n<p>The application takes quite some time to start up because the Bing Maps Control retrieves all the initial data to display the map. </p>\n\n<p>The mapping part of my application is only rarely needed and so it would be pretty bad to have slow start up for a feature that is not even used every time so I initially set the control's visibility to \"Collapsed\" in the hope that then no requests would be made, but that doesn't help.</p>\n\n<p>Is there a way to explicitly initialize the Bing Maps control when I want to use it and not at application start up?</p>\n"},{"tags":["java","performance","messaging"],"answer_count":7,"favorite_count":5,"up_vote_count":7,"down_vote_count":0,"view_count":3331,"score":7,"question_id":1481853,"title":"Technique or utility to minimize Java \"warm-up\" time?","body":"<p>I am supporting a Java messaging application that requires low latency (&lt; 300 microseconds processing each message). However, our profiling shows that the Sun Java Virtual Machine runs slowly at first, and speeds up after the first 5,000 messages or so. The first 5,000 messages have latency of 1-4 milliseconds. After about the first 5,000, subsequent messages have ~250 microseconds latency, with occasional outliers.</p>\n\n<p>It's generally understood that this is typical behavior for a Java application. However, from a business standpoint it's not acceptable to tell the customer that they have to wait for the JVM to \"warm-up\" before they see the performance they demand.  The application needs to be \"warmed-up\" before the first customer message is processed</p>\n\n<p>The JVM is Sun 1.6.0 update 4.</p>\n\n<p>Ideas for overcoming this issue:</p>\n\n<ol>\n<li>JVM settings, such as -XX:CompileThreshold= </li>\n<li>Add a component to \"warm-up\" the application on startup, for example by sending \"fake messages\" through the application. </li>\n<li>Statically load application and JDK classes upon application startup, so that classes aren't loaded from JARs while processing customer messages. </li>\n<li>Some utility or Java agent that accomplishes either or both of the above two ideas, so that I don't have to re-invent the wheel.</li>\n</ol>\n\n<p>NOTE: Obviously for this solution I'm looking at all factors, including chip arch, disk type and configuration and OS settings. However, for this question I want to focus on what can be done to optimize the Java application and minimize \"warm up\" time.</p>\n"},{"tags":["matlab","performance","anonymous-function"],"answer_count":3,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":1712,"score":3,"question_id":1673193,"title":"Slow performance using anonymous functions in MATLAB... have others noticed this?","body":"<p>In order to refactor my MATLAB code, I thought I'd pass around functions as arguments (what MATLAB calls anonymous functions), inspired by functional programming.</p>\n\n<p>However, it seems performance is hit quite severely. In the examples below, I compare different approaches. (The code snippet is wrapped in a function in order to be able to use subfunctions)</p>\n\n<p>The result I get is 0 seconds for direct, almost 0 seconds using a subfunction, and 5 seconds using anonymous functions. I'm running MATLAB 7.7 (R2007b) on OS X 10.6, on a C2D 1.8 GHz. </p>\n\n<p>Can anyone run the code and see what they get? I'm especially interested in performance on Windows.</p>\n\n<pre><code>function [] = speedtest()\n\n\nclear all; close all;\n\nfunction y = foo(x)\n    y = zeros(1,length(x));\n    for j=1:N\n        y(j) = x(j)^2;\n    end\nend\n\nx = linspace(-100,100,100000);\nN = length(x);\n\n\n%% direct\nt = cputime;\n\ny = zeros(1,N);\nfor i=1:N\n    y(i) = x(i)^2;\nend\n\nr1 = cputime - t;\n\n%% using subfunction\nt = cputime;\ny = foo(x);\nr2 = cputime - t;\n\n%% using anon function\nfn = @(x) x^2;\n\nt = cputime;\n\ny = zeros(1,N);\nfor i=1:N\n    y(i) = fn(x(i));\nend\n\nr3 = cputime-t;\n\n[r1 r2 r3]\n\nend\n</code></pre>\n"},{"tags":["sql","performance","postgresql"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":112,"score":3,"question_id":12848711,"title":"CTE scan is far far slower than it should be","body":"<p>The following query is being run on about 4 million rows. The first two CTE statements execute in about an hour. The final one however is on track to last more than 15 years.</p>\n\n<pre><code>WITH parsed AS (\n   SELECT name, array(...) description FROM import\n), counts AS (\n   SELECT unnest(description) token, count(*) FROM parsed GROUP BY 1\n) \nINSERT INTO table (name, description) \nSELECT name, ARRAY(\n    SELECT ROW(token, count)::a \n    FROM (\n        SELECT token, (\n            SELECT count \n            FROM counts \n            WHERE a.token=counts.token\n            ) \n        FROM UNNEST(description) a(token)\n        ) _\n    )::a[] description \nFROM parsed;\n\n                                                                  QUERY PLAN                                                                   \n-----------------------------------------------------------------------------------------------------------------------------------------------\n Insert on table  (cost=55100824.40..162597717038.41 rows=3611956 width=96)\n   CTE parsed\n     -&gt;  Seq Scan on import  (cost=0.00..51425557.67 rows=3611956 width=787)\n           Filter: ((name IS NOT NULL) AND (description IS NOT NULL))\n           SubPlan 1\n             -&gt;  HashAggregate  (cost=11.59..12.60 rows=101 width=55)\n                   -&gt;  Append  (cost=0.00..11.34 rows=101 width=55)\n                         -&gt;  Result  (cost=0.00..0.01 rows=1 width=0)\n                         -&gt;  Index Scan using import_aliases_mid_idx on import_aliases  (cost=0.00..10.32 rows=100 width=56)\n                               Index Cond: (mid = \"substring\"(import.mid, 5))\n           SubPlan 2\n             -&gt;  HashAggregate  (cost=0.78..1.30 rows=100 width=0)\n                   -&gt;  Result  (cost=0.00..0.53 rows=100 width=0)\n   CTE counts\n     -&gt;  HashAggregate  (cost=3675165.23..3675266.73 rows=20000 width=32)\n           -&gt;  CTE Scan on parsed  (cost=0.00..1869187.23 rows=361195600 width=32)\n   -&gt;  CTE Scan on parsed  (cost=0.00..162542616214.01 rows=3611956 width=96)\n         SubPlan 6\n           -&gt;  Function Scan on unnest a  (cost=0.00..45001.25 rows=100 width=32)\n                 SubPlan 5\n                   -&gt;  CTE Scan on counts  (cost=0.00..450.00 rows=100 width=8)\n                         Filter: (a.token = token)\n</code></pre>\n\n<p>There are about 4 million rows in both <code>parsed</code> and <code>counts</code>. The query's currently running, and the final statement's inserting a row roughly every 2 minutes. It's barely touching disk, but eating CPU like crazy, and I'm confused.</p>\n\n<p>What's wrong with the query?</p>\n\n<p>The final statement's supposed to look up each element of <code>description</code> in <code>counts</code>, transforming something like this <code>[a,b,c]</code> to something like this <code>[(a,9),(b,4),(c,0)]</code> and inserting it.</p>\n\n<p><br/>\n<strong>Edit</strong></p>\n\n<p>With parsed and counts as tables, and <code>token</code> in counts indexed, this is the plan:</p>\n\n<pre><code>explain INSERT INTO table (name, mid, description) SELECT name, mid, ARRAY(SELECT ROW(token, count)::a FROM (SELECT token, (SELECT count FROM counts WHERE a.token=counts.token) FROM UNNEST(description) a(token)) _)::a[] description FROM parsed;\n                                              QUERY PLAN                                              \n------------------------------------------------------------------------------------------------------\n Insert on table  (cost=0.00..5761751808.75 rows=4002061 width=721)\n   -&gt;  Seq Scan on parsed  (cost=0.00..5761751808.75 rows=4002061 width=721)\n         SubPlan 2\n           -&gt;  Function Scan on unnest a  (cost=0.00..1439.59 rows=100 width=32)\n                 SubPlan 1\n                   -&gt;  Index Scan using counts_token_idx on counts  (cost=0.00..14.39 rows=1 width=4)\n                         Index Cond: (a.token = token)\n</code></pre>\n\n<p>Which is much more reasonable. The arrays have an average of 57 elements, so I guess it was just the sheer number of lookups against the presumable fairly inefficient CTE table that was killing performance. It's now going at 300 rows per second, which I'm happy with.</p>\n"},{"tags":["css","performance","stylesheet","sprite","css-sprites"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":16,"score":0,"question_id":13228039,"title":"Sprite only partially shown","body":"<p>I attempted to make a sprite image for the bottom of my asp.net home page, to reduce the number of http requests.  I looked at all the coding, and can't find any errors.  Here is the image:</p>\n\n<p><img src=\"http://i.stack.imgur.com/T0oEN.jpg\" alt=\"5 social media buttons\"></p>\n\n<p>But for some reason, my google+ image doesn't show up; it only shows 4 of the 5 social media buttons.  Here is the jsfiddle, which doesn't show the google+ button also:</p>\n\n<p><a href=\"http://jsfiddle.net/jasonpaulweber/R8Eu8/\" rel=\"nofollow\">JSFiddle - social media buttons</a></p>\n\n<p>Thank you for anybody who can offer any guidance in this regard!</p>\n\n<p>SE wants me to include code in here, although it's on the Fiddle.  But anyway, here is what I have in my stylesheet:</p>\n\n<pre><code>.homepage{\nwidth:23px;\nheight:23px;\nbackground-image: url(http://www.ussvision.com/images/sprite-homepage.jpg); \nbackground-repeat:no-repeat;\n}\n.fbook{\nbackground-position: 0px 0px;\n}\n.twit{\nbackground-position: -25px 0px;\n}\n.yt{\nbackground-position: -50px 0px;\n}\n.goog{\nbackground-position: -103px 0px;\n}\n.linked{\nbackground-position: -150px 0px;\n}\n</code></pre>\n"},{"tags":["arrays","performance","scala"],"answer_count":1,"favorite_count":3,"up_vote_count":4,"down_vote_count":0,"view_count":87,"score":4,"question_id":13226192,"title":"Fast packed arrays of structs in Scala","body":"<p>I'm investigating what it would take to turn an existing mixed Python/C++ numerical codebase into mixed Scala/C++ (ideally mostly Scala in the long run).  I expect the biggest issue to be packed arrays of structs.  For example, in C++ we have types like</p>\n\n<pre><code>Array&lt;Vector&lt;double,3&gt;&gt; # analogous to double [][3]\nArray&lt;Frame&lt;Vector&lt;double,3&gt;&gt;&gt; # a bunch of translation,quaternion pairs\n</code></pre>\n\n<p>These can be converted back and forth between Python and C++ without copying thanks to Numpy.</p>\n\n<p>On the JVM, since unboxed arrays can only have a handful of types, the only way I can imagine proceeding is to create (1) a boxed Scala type for each struct, such as <code>Vector&lt;double,3&gt;</code> and (2) a typed thin wrapper around <code>Array[Double]</code> that knows what struct it's supposed to be and creates/consumes boxed singletons as necessary.</p>\n\n<p>Are there any existing libraries that do such a thing, or that implement any alternatives for packed arrays of structs?  Does anyone have experience regarding what the performance characters would be likely be, and whether existing compilers and JVM's would be able to optimize the boxes away in at least the nonpolymorphic, sealed case?</p>\n\n<p>Note that packing and nice typing are not optional: Without packing I'll blow memory very quickly, and if all I have is Array[Double] C++'s type system (unfortunately) wins.</p>\n"},{"tags":["c#","performance","optimization","copying"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":56,"score":0,"question_id":13227856,"title":"Will copying occur if the methods of an array element, which is a value type object, are called?","body":"<p>To save the memory, I defined some record-like structs and stored their instances in arrays, as the following code shows:</p>\n\n<pre><code>struct Foo\n{\n    readonly char _bar;\n    readonly int _baz;\n    // other probable fields ...\n    public char Bar{get{return _bar;}}\n    public int Baz{get{return _baz;}}\n\n    public Foo(char bar, int baz){_bar = bar; _baz = baz;}\n}\n\nstatic void Main ()\n{\n    Foo[] arr = new Foo[1000000];\n    int baz;\n    for(var i = 1000000; i-- &gt; 0;){ arr[i] = new Foo('x',42); }\n    for(var i = 1000000; i-- &gt; 0;)\n    { \n        baz = arr[i].Baz; //Will the Foo obj resides at arr[i] be copied?\n    }\n}\n</code></pre>\n\n<p>I know the copying won't happen if the stuff above was implemented in C/C++, yet I'm not sure about C#, what if I replace the <code>Foo[]</code> with an <code>List&lt;Foo&gt;</code>? C# doesn't have a mechanism to return a reference, so theoretically the indexer would return either a pointer(4 bytes) for reference type or the whole copy for value type, if the return value optimization was not involved. So, could anyone tell me, whether this sort of ridiculous copying is guaranteed to be avoided by .NET/MONO jit ?</p>\n"},{"tags":["performance","pex"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":35,"score":0,"question_id":12884982,"title":"Theory : starting Pex Exploration and number of PUT generated","body":"<p>I read many papers about pex , I'm focus on the explorations strategies . I understand that the Fitnex strategy is default used but I don't understand how Pex create the first parametrized unit test .</p>\n\n<p>Where I can find this information ?</p>\n\n<p>By sperimental tests I think :\n- If parameter is \"int\" start with 0\n- If parameter is \"bool\" start with false\n- If parameter is an object start with Null</p>\n\n<p>Is it correct ?</p>\n\n<p>Now about Fitnex strategy and solver constraints Z3 I think that it is able to solve only one constrain by iteration , sorry for my little English now I do an example :</p>\n\n<p>Suppose to have the following method :</p>\n\n<pre><code>public void branchOverTests(bool a, bool b)\n{\n1   if (a)\n    {\n2       Console.WriteLine(\"B1\");\n    }\n    else\n    {\n3       Console.WriteLine(\"B2\");\n    }\n4   if (b)\n    {\n5       Console.WriteLine(\"B3\");\n    }\n    else\n    {\n6       Console.WriteLine(\"B4\");\n    }\n}\n</code></pre>\n\n<p>Numbers are lines identify, Pex generate 3 tests :</p>\n\n<pre><code>--- Test 1\nbranchOverTests(a=false,b=false)\nPath: 1F 3T 4F 6T\nreturn target != (ClassMethod)null;\nreturn target != (ClassMethod)null &amp;&amp; a == false;\nreturn target != (ClassMethod)null &amp;&amp; a == false;\nreturn target != (ClassMethod)null &amp;&amp; a == false &amp;&amp; b == false;\n\n--- Test 2\nbranchOverTests(a=false,b=true)\nPath: 1F 3T 4T 5T\nreturn target != (ClassMethod)null;\nreturn target != (ClassMethod)null &amp;&amp; a == false;\nreturn target != (ClassMethod)null &amp;&amp; a == false;\nreturn target != (ClassMethod)null &amp;&amp; b != false &amp;&amp; a == false;\n\nNote: From Test 1 Flipped last branch: \nreturn target != (ClassMethod)null &amp;&amp; a == false &amp;&amp; b == false; \n-&gt; return target != (ClassMethod)null &amp;&amp; b != false &amp;&amp; a == false; \n=&gt; b = true\n\n--- Test 3\nbranchOverTests(a=true,b=false)\nPath: 1T 2T 4F 6T\nreturn target != (ClassMethod)null;\nreturn target != (ClassMethod)null &amp;&amp; a != false;\nreturn target != (ClassMethod)null &amp;&amp; a != false;\nreturn target != (ClassMethod)null &amp;&amp; a != false &amp;&amp; b == false;\n\nNote: From Test 2 Resolve second condition of last branch:\nreturn return target != (ClassMethod)null &amp;&amp; b != false &amp;&amp; a == false; \n-&gt; return target != (ClassMethod)null &amp;&amp; a != false &amp;&amp; b == false; \n=&gt; a = true\n=&gt; return target != (ClassMethod)null &amp;&amp; a != false;\n=&gt; return target != (ClassMethod)null &amp;&amp; a != false;\n</code></pre>\n\n<p>But the most efficient set of parametrized tests is :</p>\n\n<pre><code>branchOverTests(a=false,b=false)\nbranchOverTests(a=true,b=true)\n</code></pre>\n\n<p>After Test 1 :</p>\n\n<pre><code>return target != (ClassMethod)null &amp;&amp; a == false;\n-&gt; return target != (ClassMethod)null &amp;&amp; a != false;\n=&gt; a = true\n=&gt; return target != (ClassMethod)null &amp;&amp; a != false &amp;&amp; b != false;\n=&gt; b = true\n</code></pre>\n\n<p>Ideal Test 2 :</p>\n\n<pre><code>branchOverTests(a=true,b=true)\nPath: 1T 2T 4T 5T\nreturn target != (ClassMethod)null;\nreturn target != (ClassMethod)null &amp;&amp; a != false;\nreturn target != (ClassMethod)null &amp;&amp; a != false;\nreturn target != (ClassMethod)null &amp;&amp; a != false &amp;&amp; b != false;\n</code></pre>\n\n<p>Is That I think correct ?</p>\n\n<p>Thanks best regards.</p>\n"},{"tags":["performance","assembly","bytecode","opcode","multiplatform"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":181,"score":1,"question_id":5685601,"title":"Unified assembly language","body":"<p>I wonder if there exists some kind of universal and easy-to-code opcode (or assembly) language which provides basic set of instructions available in most of today's CPUs (not some fancy CISC, register-only computer, just common one). With possibility to \"compile\", micro-optimize and \"interpret\" on any mentioned CPUs? </p>\n\n<p>I'm thinking about something like MARS MIPS simulator (rather simple and easy to read code), with possibility to make real programs. No libraries necessary (but nice thing if that possible), just to make things (libraries or UNIX-like tools) faster in uniform way.</p>\n\n<p>Sorry if that's silly question, I'm new to assembler. I just don't find NASM or UNIX assembly language neither extremely cross-platform nor easy to read and code.</p>\n"},{"tags":["performance","internet-explorer-8","knockout.js","leak"],"answer_count":0,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":59,"score":5,"question_id":13227501,"title":"KnockoutJS IE8 performance issues and memory leaks","body":"<p>We build complex enterprise application using KnockoutJS and faced poor performance especially in IE8. Application might take more than 1GB of RAM and never frees memory. During investigation we discovered that KnockoutJS keeps references to the DOM nodes and never deletes it. This can be reproduced using IESieve against any public KnockoutJS example, just watch DOM utilization while playing with example which adds and deletes DOM nodes.</p>\n\n<p>Have anyone faced this problem and has any ideas of how to work this out?</p>\n"},{"tags":["c++","performance"],"answer_count":1,"favorite_count":2,"up_vote_count":2,"down_vote_count":4,"view_count":76,"score":-2,"question_id":13226761,"title":"Fast i/o function implementation","body":"<p>I was searching net about fast i/o in c++ for various contest and i found one piece of fast input function . But i am just a beginner in c++ and couldn't implement it to a simple programme to input using that function . So if someone can give an example code like to input a variable using that function , it would really be a help to me . Here's is the function i found :-</p>\n\n<pre><code>inline void fastRead(int *a)\n{\n register char c=0;\n while (c&lt;33) c=getchar_unlocked();\n *a=0;\n while (c&gt;33)\n {\n     *a=*a*10+c-'0';\n     c=getchar_unlocked();\n }\n}\n</code></pre>\n"},{"tags":["iphone","objective-c","ios","performance","cocoa"],"answer_count":1,"favorite_count":2,"up_vote_count":5,"down_vote_count":0,"view_count":109,"score":5,"question_id":12998733,"title":"Benchmarking performance using Instruments","body":"<p>I am looking for some advice on how to use Instruments' Time Profiler to enhance a specific operation. I have a paging scroll view that loads its content on demand. As a new page is scrolled to, another is loaded two pages to the right. This happens when the current page is scrolled 50% of screen, and on slower devices the loading is enough of a bottle neck that it interrupts the smoothness of the scroll. The scroll feels like it pauses very briefly at the 50% pint and then jumps back into action.</p>\n\n<p>I should note that there is no network component to my application, so the bottle neck is not while data is fetched - it is all in the loading of the new view.</p>\n\n<p>As I work on improving this, I need to benchmark the transition so I can evaluate the effect of my enhancements. Having watched the WWDC sessions, I understand the basics of the time profiler but I'm not confident I am looking at the right thing.</p>\n\n<p>I am running the instrument and then executing the scroll. I see the expected spike in CPU activity. I am then selecting the spike and looking at the symbol names. As you can see below, when I hide system libraries and show Obj-C I am dealing a brief 90% spike with 81.0 ms of running time.</p>\n\n<p><img src=\"http://i.stack.imgur.com/MSdQT.png\" alt=\"enter image description here\"></p>\n\n<p>My confusion comes when I drill down to my code. I find that one of the largest contributors (23 ms) is a small routine that just uses <code>sortedArrayUsingDescriptors</code>. Not much I can do about that one. Other times the root cause will me using CGRectInset - a few more milliseconds. I guess I expected to see more related to image use or something like that.</p>\n\n<p>I guess I don't know if I'm benchmarking the right things. Am I really meant to try and reduce my 6ms operations down to 5ms and expect to see a difference? Am I looking at the right tools to diagnose my problem?</p>\n\n<p>Any tips or instructions on what to measure and where to focus my efforts would be really valued.</p>\n"},{"tags":["sql-server","performance","stored-procedures","database-connection",".net-4.5"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":46,"score":0,"question_id":13224186,"title":"1 stored procedure returning 2 recordsets or 2 database calls","body":"<p>I am writing an ASP.NET 4.5 application that communicates with a SQL Server database. My repository layer communicates with the SQL Server via stored procedures.</p>\n\n<p>I want to query 2 related tables. The first query returns 1 row. The second query returns multiple rows related to the first query. Its not possible to return both in the one query. </p>\n\n<p>At the moment both queries are in the one stored procedure. I user multiple ado.net readers to parse the record set in to an object. </p>\n\n<p>Is the following more performant? </p>\n\n<p>Have two separate stored procedures. Each returning the above record sets in 2 separate calls. I then use the await keyword to indicate that the second call can asynchronously wait on the first i.e. using the new features in .net 4.5. This will keep the database connection open for a shorter period of time but means 2 database connections as oppose to 1.</p>\n"},{"tags":["css","performance","scroll","responsive-design"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":14,"score":0,"question_id":13216926,"title":"Portfolio page on skeleton theme scrolling slow and or jerking in Chrome","body":"<p>If someone can, please view this page:</p>\n\n<p><a href=\"http://thebc.co/our-work\" rel=\"nofollow\">http://thebc.co/our-work</a></p>\n\n<p>Everything is loading fast but when you scroll in Chrome, it isn't smooth at all.</p>\n\n<p>This only happens on the page above. I do not think the images are a problem.  They are super optimized and again, all of the pages load very fast.</p>\n\n<p>The website is built on the skeleton responsive framework.</p>\n\n<p>Thanks for your time.</p>\n"},{"tags":["c","performance","low-level","assembly"],"answer_count":28,"favorite_count":18,"up_vote_count":41,"down_vote_count":0,"view_count":6227,"score":41,"question_id":791533,"title":"Why do you program in assembly?","body":"<p>I have a question for all the hardcore low level hackers out there.  I ran across this sentence in a blog.  I don't really think the source matters (it's Haack if you really care) because it seems to be a common statement.</p>\n\n<blockquote>\n  <p>For example, many modern 3-D Games have their high performance core engine written in C++ and Assembly.</p>\n</blockquote>\n\n<p>As far as the assembly goes - is the code written in assembly because you don't want a compiler emitting extra instructions or using excessive bytes, or are you using better algorithms that you can't express in C (or can't express without the compiler mussing them up)?</p>\n\n<p>I completely get that it's important to understand the low-level stuff.  I just want to understand the <em>why</em> program in assembly after you do understand it.</p>\n\n<p><strong>Edit</strong>:  Wow!  Thank you all for the fantastic responses!  This was way better than I had hoped for!</p>\n"},{"tags":["c#","visual-studio-2010","performance","video","xna"],"answer_count":0,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":32,"score":3,"question_id":13224448,"title":"How to control HD video play position and speed in XNA?","body":"<p>I am developing a game for Windows in <strong>C# using Visual Studio 2010</strong> and <strong>XNA 4.0</strong>. I would like to be able to set and change the play position of an HD video and also play the video in reverse, depending on user input. </p>\n\n<p>I am having trouble finding where to start. XNA's videoPlayer class does not provide these type of functions. I've read that XNA DirectShow is now out of date  and slow when using HD video. </p>\n\n<p>I don't quite understand how I would be able to use or implement tools such as ffmpeg with my project. It seems some people have had similar questions and posted solutions but without much detail. These are below.</p>\n\n<ul>\n<li><p>interop out to talk to the core DX functionality.</p></li>\n<li><p>write a managed c++ wrapper to interop ffmpeg.</p></li>\n<li><p>write an mpeg decoder.</p></li>\n</ul>\n\n<p>I am not sure what would be best and where to begin.\nThanks!</p>\n"},{"tags":["mysql","windows","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":526,"score":2,"question_id":7735546,"title":"Any MySQLTuner equivalent/alternative for Windows?","body":"<p>I know auto tuning has its limits, but it would be great to quickly determine reasonable starting points for my Windows MySQL InnoDB configuration (<code>innodb_additional_mem_pool_size, innodb_buffer_pool_size, innodb_log_file_size, innodb_log_buffer_size</code>), given the RAM that I want to allocate to MySQL (512 Mo from a total 4 GB installed).\nI found <a href=\"http://stackoverflow.com/questions/4667185/mysqltuner-pl-on-a-windows-2008-server-r2\">this previous similar question</a> but it had no answer.</p>\n\n<p>Your help is much appreciated!</p>\n"},{"tags":["performance","algorithm","language-agnostic","data-structures","fibonacci-heap"],"answer_count":2,"favorite_count":21,"up_vote_count":64,"down_vote_count":0,"view_count":12055,"score":64,"question_id":504823,"title":"Has anyone actually implemented a Fibonacci-Heap efficiently?","body":"<p>Has anyone of you ever implemented a <a href=\"http://en.wikipedia.org/wiki/Fibonacci_heap\">Fibonacci-Heap</a>? I did so a few years back, but it was several orders of magnitude slower than using array-based BinHeaps.</p>\n\n<p>Back then, I thought of it as a valuable lesson in how research is not always as good as it claims to be. However, a lot of research papers claim the running times of their algorithms based on using a Fibonacci-Heap. </p>\n\n<p>Did you ever manage to produce an efficient implementation? Or did you work with data-sets so large that the Fibonacci-Heap was more efficient? If so, some details would be appreciated.</p>\n"},{"tags":["performance","opengl"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":48,"score":0,"question_id":13223416,"title":"Is drawing outside the viewport in OpenGL expensive?","body":"<p>I have several thousand quads to draw, some of which might fall entirely outside the viewport. I could write code which will detect which quads fall wholly outside viewport and ask OpenGL to draw only those which will be at least partially visible. Alternatively, I could simply have OpenGL draw all of the quads, regardless of whether they intersect with the viewport.</p>\n\n<p>I don't have enough experience with OpenGL to know if one of these is obviously better (or if OpenGL offers some quick viewport intersection test I can use). Are draws outside the viewport close to being no-ops, or are they expensive enough that I should I try to avoid them?</p>\n"},{"tags":["performance","optimization","set","computer-science","combinations"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":28,"score":0,"question_id":13203216,"title":"Optimizing Sets and Combinations","body":"<p>I have a bunch of sets, A1, A2, A3, ... AN. Each set contains N elements (lets say 1000 max) of values from 0 to 2000.\nEx.</p>\n\n<pre><code>A1{1,2,3,4}\nA2{2,4,3,5}\nA3(1,2,5,6)\n</code></pre>\n\n<p>Now for a size k, e.g. 3, the combinations of A1 would be C1(1,2,3), C2(1,2,4), C3(2,3,4) For A1 to AN I need to figure out if all combinations in A1 also exists somewhere in the combinations of A2 to AN.</p>\n\n<p>i.e. Combination C3 of A1 would match a combination in A2, and maybe AN. Now I need the results of C1 and C2 of A1 aswell against A2 to AN.\nThe simple and inefficient method would be to generate all combinations of k size for a all sets A1 to AN. Then for C1 of A where/if it exists in A2 to AN. After that C2, then C3. Then move on to the next set and repeat.</p>\n\n<p>How could I improve this method as it requires frequent and expensive computation once a new set is added?</p>\n\n<p>One other solution I've found would run in O(N^2 + N) without optimizing which basically involves taking intersections of A1 and A2...AN, computing the combinations of those intersections, then seeing how many times each of those combinations occur in the generated result.</p>\n"},{"tags":["performance","oracle","testing","awr"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":20,"score":0,"question_id":13221144,"title":"Oracle10g AWR How to measure improvement after splitting tablespaces into multiple disk volumes?","body":"<p>I have a recently released 10 000 user analytical application that is suffering from performance issues because of excessive amount of transactions. Apart from re-writing many of the SQLs in the application for better performance we are also on a top down approach where we are adding more volumes and tweeks to memory etc. An Oracle specialist redesigned our server from single tablespace with 2 volumes to 2 tablespaces with 4 volumes where the indexes will have their own volume and tablespace.   Each volume is a separate disk array so they do not compete for IO. </p>\n\n<p>We have executed this change in development environment and we are about to move into acceptance but before paying for the change the business would like to see measurable gains. </p>\n\n<p><strong>Where in the AWR report can I make comparisons to measure such data on the before and after the change is executed?</strong> </p>\n"},{"tags":[".net","wcf","performance","serialization",".net-4.5"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":25,"score":1,"question_id":13223062,"title":"direct project reference or communication via wcf","body":"<p>I am currently architecting my .net 4.5 applications. At present I have web layer and a wcf services layer. They communicate to each other over soap.</p>\n\n<p>Both layers are co located on the one server.</p>\n\n<p>I am considering moving from communicating over soap to direct project references. So that it moves from one application from two. Main motivation behind this is to improve performance of not having to undergo serializing and deserializing of communication between web and services.</p>\n\n<p>What I lose from this change, is scalability of moving my services layer to a separate tier in the future, ie a different server.</p>\n\n<p>I am looking for feedback on this, is there a major performance hit from the serialization. Thoughts on the above? Anything that I should consider that I have omitted.</p>\n"},{"tags":["mysql","primary-key","sql-order-by","performance","limit"],"answer_count":3,"favorite_count":11,"up_vote_count":16,"down_vote_count":0,"view_count":3108,"score":16,"question_id":4481388,"title":"Why does MYSQL higher LIMIT offset slow the query down?","body":"<p>Scenario in short: A table with more than 16 million records [2GB in size]. The higher LIMIT offset with SELECT, the slower the query becomes, when using     ORDER BY *primary_key*</p>\n\n<p>So  </p>\n\n<pre><code>SELECT * FROM large ORDER BY `id`  LIMIT 0, 30 \n</code></pre>\n\n<p>takes far less than  </p>\n\n<pre><code>SELECT * FROM large ORDER BY `id` LIMIT 10000, 30 \n</code></pre>\n\n<p>That only orders 30 records and same eitherway. So it's not the overhead from ORDER BY.<br>\nNow when fetching the latest 30 rows it takes around 180 seconds. How can I optimize that simple query?</p>\n"},{"tags":["java","performance","biginteger"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":116,"score":2,"question_id":10933470,"title":"Improving performance of addition with very big numbers","body":"<p>I wrote this program to calculate very big numbers without using any BigInteger method. I finished it and it's working properly. I used StringBuilder and lots of parseInt call to get it done. Is there a more efficient way to do this?</p>\n\n<p><em>By the way, this is just worksheet, ignore bad programming style, after finishing my job, I will reorganize that.</em></p>\n\n<pre><code>private String add (String x, String y)\n{\n    String g = \"\";\n    StringBuilder str = new StringBuilder();\n    int sum;\n    double atHand = 0;\n    int dif = (int)(Math.abs(x.length()-y.length())); \n\n    if(x.length() &gt;= y.length())   //adding zero for equalise number of digits.\n    {\n        for(int i = 0; i&lt;dif; i++)\n            g += \"0\";\n        y = g+y;    \n    }\n    else \n    {\n        for(int i = 0; i&lt;dif; i++)\n            g += \"0\";\n        x = g + x;\n    }\n\n    for (int i = y.length()-1; i &gt;=0 ; i--)\n    {\n        sum = Integer.parseInt(x.substring(i, i+1)) +Integer.parseInt(y.substring(i,i+1)) + (int)atHand; \n\n        if(sum&lt;10) \n        {\n            str.insert(0, Integer.toString(sum)); \n            atHand = 0;  \n        }else\n        {\n            if(i==0)\n                str.insert(0, Integer.toString(sum)); \n            else\n            {\n                atHand = sum *0.1;\n                sum = sum %10;  \n                str.insert(0, Integer.toString(sum));\n            }\n        }\n    }\n    return str.toString();\n}\n</code></pre>\n"},{"tags":["c#","asp.net","html","performance"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":969,"score":2,"question_id":7080773,"title":"HtmlGenericControl(\"a\") vs. HtmlAnchor","body":"<p>I was looking into why one of my applications was running quite slowly.  The application generates and displays what is kind of like a Gantt Chart; days along the top, people down the side, and  tasks stretching horizontally across the table to show how long they were worked on for.  Inside each task (table cell) there is an <code>&lt;a href=\"...</code> which brings up some more info about the task.</p>\n\n<p>As this is all built up dynamically from the codebehind, I've used <code>HTMLTableRows/Cells</code> to create the rows and cells, then used the <code>Controls</code> properties to add <code>HTMLAnchors</code>.  Whenever I'm setting attributes I've used <code>HTMLAnchor.HRef</code>, <code>HTMLTableCell.ColSpan</code>, etc.</p>\n\n<p>I noticed that if I use <code>HTMLGenericControl</code> and then just use its <code>Attributes</code> collection, e.g. </p>\n\n<pre><code>HTMLGenericControl a = new HTMLGenericControl(\"a\");\na.Attributes[\"href\"] = task.getLink();\n</code></pre>\n\n<p>it runs significantly quicker than what I would have thought is the preferred way of doing the same thing: </p>\n\n<pre><code>HtmlAnchor a = new HTMLAnchor;\na.HRef = task.getLink();\n</code></pre>\n\n<p>Does anyone have any explanation for where this apparent extra 'overhead' comes from?</p>\n\n<p><strong>EDIT</strong> </p>\n\n<p>In case anyone is confused by my explanation, I posted another question for the same project, which has a <a href=\"http://stackoverflow.com/questions/6278562\">screenshot</a>.</p>\n"},{"tags":["performance","gcc","compilation","ccache"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":193,"score":0,"question_id":8553928,"title":"disadvantages of ccache","body":"<p>I am using ccache for experiments, but I am not quite sure that I should use this. Can anyone explain the situation when ccache can result in wrong behavior. Or should we always use ccache ? Anyone who got that ccache is producing wrong object files or changes in header files are not being considered ?<br></p>\n"},{"tags":["iphone","performance","cocoa","uiview","uiviewcontroller"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":33,"score":0,"question_id":13221005,"title":"Forcing a controller's view into memory","body":"<p>To address some performance issues, I started recycling some view controllers. However, the performance benefit of re-using a recycled controller's view is only present if that view has been drawn. If, for, example, I want to pre-populate the recycle queue with a controller but never place its view on screen, I get no such benefit.</p>\n\n<p>How can I force the controller's view to be 'pre-rendered' and added to my queue such that when it is recycled I receive the performance benefit I am seeing from my other recycled controllers? I know that a controllers's view is created when first needed, but even adding the view and immediately removing it (before the parent view is displayed) doesn't seem to do it.</p>\n"},{"tags":["java","multithreading","performance","concurrency","threadpool"],"answer_count":1,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":129,"score":3,"question_id":12649024,"title":"ExecutorService suitable for a huge amount of short-lived tasks","body":"<p>Is there an ExecutorService that is suitable for a huge amount of very short-lived tasks? I envision something that internally tries busy waiting before switching over to synchronized waiting. Keeping the order of the tasks is not important, but it should be possible to enforce memory consistency (all tasks <em>happen-before</em> the main thread regains control).</p>\n\n<p>The test posted below consists of 100'000 tasks that each generate 100 <code>double</code>s in a row. It accepts the size of the thread pool as command-line parameter and always tests the serial version vs. the parallel one. (If no command-line arg is given, only the serial version is tested.) The parallel version uses a thread pool of fixed size, allocation of the tasks is not even part of the time measurement. Still, the parallel version is <em>never</em> faster than the serial version, I've tried up to 80 threads (on a machine with 40 hyperthreaded cores). Why?</p>\n\n<pre><code>import java.util.ArrayList;\nimport java.util.Random;\nimport java.util.concurrent.Callable;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\n\npublic class ExecutorPerfTest {\n    public static final int TASKS = 100000;\n    public static final int SUBTASKS = 100;\n\n    static final ThreadLocal&lt;Random&gt; R = new ThreadLocal&lt;Random&gt;() {\n        @Override\n        protected synchronized Random initialValue() {\n            return new Random();\n        }\n    };\n\n    public class SeqTest implements Runnable {\n        @Override\n        public void run() {\n            Random r = R.get();\n            for (int i = 0; i &lt; TASKS; i++)\n                for (int j = 0; j &lt; SUBTASKS; j++)\n                    r.nextDouble();\n        }\n    }\n\n    public class ExecutorTest implements Runnable {\n        private final class RandomGenerating implements Callable&lt;Double&gt; {\n            @Override\n            public Double call() {\n                double d = 0;\n                Random r = R.get();\n                for (int j = 0; j &lt; SUBTASKS; j++)\n                    d = r.nextDouble();\n                return d;\n            }\n        }\n\n        private final ExecutorService threadPool;\n        private ArrayList&lt;Callable&lt;Double&gt;&gt; tasks = new ArrayList&lt;Callable&lt;Double&gt;&gt;(TASKS);\n\n        public ExecutorTest(int nThreads) {\n            threadPool = Executors.newFixedThreadPool(nThreads);\n            for (int i = 0; i &lt; TASKS; i++)\n                tasks.add(new RandomGenerating());\n        }\n\n        public void run() {\n            try {\n                threadPool.invokeAll(tasks);\n            } catch (InterruptedException e) {\n                e.printStackTrace();\n            } finally {\n                threadPool.shutdown();\n            }\n        }\n    }\n\n    public static void main(String[] args) {\n        ExecutorPerfTest executorPerfTest = new ExecutorPerfTest();\n        if (args.length &gt; 0)\n            executorPerfTest.start(new String[]{});\n        executorPerfTest.start(args);\n    }\n\n    private void start(String[] args) {\n        final Runnable r;\n        if (args.length == 0) {\n            r = new SeqTest();\n        }\n        else {\n            final int nThreads = Integer.parseInt(args[0]);\n            r = new ExecutorTest(nThreads);\n        }\n        System.out.printf(\"Starting\\n\");\n        long t = System.nanoTime();\n        r.run();\n        long dt = System.nanoTime() - t;\n        System.out.printf(\"Time: %.6fms\\n\", 1e-6 * dt);\n    }\n}\n</code></pre>\n"},{"tags":["c++","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":110,"score":-1,"question_id":13220205,"title":"What is faster? two ints or __int64?","body":"<p>What is faster: (Performance)</p>\n\n<pre><code>__int64 x,y;\nx=y;\n</code></pre>\n\n<p>or</p>\n\n<pre><code>int x,y,a,b;\nx=a;\ny=b;\n</code></pre>\n\n<p>?</p>\n\n<p>Or they are equal?</p>\n"},{"tags":["performance","matlab","matrix-multiplication"],"answer_count":10,"favorite_count":9,"up_vote_count":41,"down_vote_count":0,"view_count":6034,"score":41,"question_id":6058139,"title":"Why is MATLAB so fast in matrix multiplication?","body":"<p>I am making some benchmarks with CUDA, C++, C#, and Java, and using MATLAB for verification and matrix generation.  But when I multiply with MATLAB, 2048x2048 and even bigger matrices are almost instantly multiplied.</p>\n\n\n\n<pre class=\"lang-none prettyprint-override\"><code>             1024x1024   2048x2048   4096x4096\n             ---------   ---------   ---------\nCUDA C (ms)      43.11      391.05     3407.99\nC++ (ms)       6137.10    64369.29   551390.93\nC# (ms)       10509.00   300684.00  2527250.00\nJava (ms)      9149.90    92562.28   838357.94\nMATLAB (ms)      75.01      423.10     3133.90\n</code></pre>\n\n<p>Only CUDA is competitive, but I thought that at least C++ will be somewhat close and not 60x slower.</p>\n\n<p>So my question is - How is MATLAB doing it that fast? </p>\n\n<p>C++ Code:</p>\n\n<pre><code>float temp = 0;\ntimer.start();\nfor(int j = 0; j &lt; rozmer; j++)\n{\n    for (int k = 0; k &lt; rozmer; k++)\n    {\n        temp = 0;\n        for (int m = 0; m &lt; rozmer; m++)\n        {\n            temp = temp + matice1[j][m] * matice2[m][k];\n        }\n        matice3[j][k] = temp;\n    }\n}\ntimer.stop();\n</code></pre>\n\n<p>Edit:\nI also dont know what to think about the C# results. The algorithm is just the same as C++ and Java, but there's a giant jump 2048 from 1024?</p>\n\n<p>Edit2:\nUpdated MATLAB and 4096x4096 results</p>\n"}]}
{"total":25592,"page":3,"pagesize":100,"questions":[{"tags":["performance","sybase","ssas"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":133,"score":1,"question_id":12643452,"title":"SSAS 2008 R2 with Sybase OLEDB (ASEOLEDB.1)","body":"<p>I am designing a high volume data warehouse using Adaptive Server Enterprise 15.7. The business is already using SSAS 2008 R2 for much of their data analysis and wishes to continue using it on top of the aforementioed data warehouse.</p>\n\n<p>I was wondering if anyone out there in the community had done anything like this before and could share some advice. A few estimations about this data warehouse follow:</p>\n\n<ul>\n<li>Dimensions are slowly changing (every 3 days or so)</li>\n<li>Facts will get ~15 million new records daily (12 measures)</li>\n</ul>\n\n<p>I'm most concerned with the performance of the processing rather than the querying.</p>\n\n<p>Cheers,</p>\n\n<p>RA</p>\n"},{"tags":["java","performance","sorting"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":38,"score":0,"question_id":13219491,"title":"Sorting different type of objects (performance)","body":"<p>Let's say I have a class called Unit (with a position variable x) and extend the class to UnitA, UnitB, UnitC, etc..</p>\n\n<p>this is something I came up with:</p>\n\n<pre><code>Unit[] ordered = new Unit[a_num+b_num+c_num];\nordered = Arrays.copyOf(a_units, a_num); //and add b_units, c_units, etc\nArrays.sort(ordered); //sort using compareTo method\n</code></pre>\n\n<ul>\n<li>For best performance and neat programming, what is the best way to sort these values from left to right (the x variable)?</li>\n<li>When the array is sorted, in order to access unit specific variables, how do I find out what type of object each entry is?</li>\n</ul>\n"},{"tags":["performance","assembly","intel","rdtsc"],"answer_count":3,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":589,"score":4,"question_id":6432669,"title":"Variance in RDTSC overhead","body":"<p>I'm constructing a micro-benchmark to measure performance changes as I experiment with the use of SIMD instruction intrinsics in some primitive image processing operations. However, writing useful micro-benchmarks is difficult, so I'd like to first understand (and if possible eliminate) as many sources of variation and error as possible.</p>\n\n<p>One factor that I have to account for is the overhead of the measurement code itself. I'm measuring with RDTSC, and I'm using the following code to find the measurement overhead:</p>\n\n<pre><code>extern inline unsigned long long __attribute__((always_inline)) rdtsc64() {\n    unsigned int hi, lo;\n        __asm__ __volatile__(\n            \"xorl %%eax, %%eax\\n\\t\"\n            \"cpuid\\n\\t\"\n            \"rdtsc\"\n        : \"=a\"(lo), \"=d\"(hi)\n        : /* no inputs */\n        : \"rbx\", \"rcx\");\n    return ((unsigned long long)hi &lt;&lt; 32ull) | (unsigned long long)lo;\n}\n\nunsigned int find_rdtsc_overhead() {\n    const int trials = 1000000;\n\n    std::vector&lt;unsigned long long&gt; times;\n    times.resize(trials, 0.0);\n\n    for (int i = 0; i &lt; trials; ++i) {\n        unsigned long long t_begin = rdtsc64();\n        unsigned long long t_end = rdtsc64();\n        times[i] = (t_end - t_begin);\n    }\n\n    // print frequencies of cycle counts\n}\n</code></pre>\n\n<p>When running this code, I get output like this:</p>\n\n<pre><code>Frequency of occurrence (for 1000000 trials):\n234 cycles (counted 28 times)\n243 cycles (counted 875703 times)\n252 cycles (counted 124194 times)\n261 cycles (counted 37 times)\n270 cycles (counted 2 times)\n693 cycles (counted 1 times)\n1611 cycles (counted 1 times)\n1665 cycles (counted 1 times)\n... (a bunch of larger times each only seen once)\n</code></pre>\n\n<p>My questions are these:</p>\n\n<ol>\n<li><strong>What are the possible causes of the bi-modal distribution of cycle counts generated by the code above?</strong></li>\n<li><strong>Why does the fastest time (234 cycles) only occur a handful of times&mdash;what highly unusual circumstance could <em>reduce</em> the count?</strong></li>\n</ol>\n\n<hr>\n\n<p><strong>Further Information</strong></p>\n\n<p>Platform:</p>\n\n<ul>\n<li>Linux 2.6.32 (Ubuntu 10.04)</li>\n<li>g++ 4.4.3</li>\n<li>Core 2 Duo (E6600); this has constant rate TSC.</li>\n</ul>\n\n<p>SpeedStep has been turned off (processor is set to performance mode and is running at 2.4GHz); if running in 'ondemand' mode, I get two peaks at 243 and 252 cycles, and two (presumably corresponding) peaks at 360 and 369 cycles.</p>\n\n<p>I'm using <code>sched_setaffinity</code> to lock the process to one core. If I run the test on each core in turn (i.e., lock to core 0 and run, then lock to core 1 and run), I get similar results for the two cores, except that the fastest time of 234 cycles tends to occur slightly fewer times on core 1 than on core 0.</p>\n\n<p>Compile command is:</p>\n\n<pre><code>g++ -Wall -mssse3 -mtune=core2 -O3 -o test.bin test.cpp\n</code></pre>\n\n<p>The code that GCC generates for the core loop is:</p>\n\n<pre><code>.L105:\n#APP\n# 27 \"test.cpp\" 1\n    xorl %eax, %eax\n    cpuid\n    rdtsc\n# 0 \"\" 2\n#NO_APP\n    movl    %edx, %ebp\n    movl    %eax, %edi\n#APP\n# 27 \"test.cpp\" 1\n    xorl %eax, %eax\n    cpuid\n    rdtsc\n# 0 \"\" 2\n#NO_APP\n    salq    $32, %rdx\n    salq    $32, %rbp\n    mov %eax, %eax\n    mov %edi, %edi\n    orq %rax, %rdx\n    orq %rdi, %rbp\n    subq    %rbp, %rdx\n    movq    %rdx, (%r8,%rsi)\n    addq    $8, %rsi\n    cmpq    $8000000, %rsi\n    jne .L105\n</code></pre>\n"},{"tags":["visual-studio-2010","performance","workflow"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":16,"score":0,"question_id":13217313,"title":"Visual C++ Workflow: Edit > (Slow) Compile > Run?","body":"<p>Whats your workflow when developing VC++ applications in Visual Studio? Coming from a mostly dynamic interpreted language (and mostly Java in school), I am increasingly frustrated by speed of VC++ development workflow ... I am doing <code>edit &gt; save/compile &gt; run/test</code>, a typical workflow? But the compile step is long ... considering I'm new to C++ and thus try alot of things (many times) then compile (~10-20s). The compile times is significant, how can I speed things up? </p>\n\n<p>I am on </p>\n\n<ul>\n<li>i3 2100</li>\n<li>have an SSD but I'm not sure its a good idea to have VS on it, will it write alot when compiling thus wearing down the SSD? Its a Crucial M4. </li>\n<li>8GB DDR3 </li>\n</ul>\n\n<p>I think specs are reasonable? I heard a better processor helps more than an SSD? </p>\n"},{"tags":["php","performance","concurrency"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":44,"score":2,"question_id":13215757,"title":"A Busy Php script making other scripts slower- how?","body":"<p>On my localserver, XAMPP environment, I'm running a test PHP script that takes 20 seconds to run, but uses only 2MB of memory and 10% CPU. </p>\n\n<p>When I open a new window and run the same script at the same time as the first, it takes over 30 seconds for both scripts to finish. </p>\n\n<p>--The script is a simple for loop that writes to mysql DB, InnoDb, 200 times.</p>\n\n<p>Shouldn't the script take the same amount of time, but use more system resources?</p>\n\n<p>As in, scale linearly.</p>\n\n<p>Why is this?</p>\n\n<pre><code>  //the code in all its glory-- Post extends a CRUD class\n  // These are the values to be saved:\n   $values = array(\n  'id' =&gt; '',\n  'content' =&gt; 'This is the VALUE'\n                 );\n  //And the action. I know-Saving Mysql in a loop is a no-no-- \n    //for demonstration only  \n   for($i=0; $i&lt;250; $i++){\n\n   $object = new Post($values); //instantiate the Post Class with values\n   $object-&gt;create($values);   //save the values to the Db. The end\n                           }\n</code></pre>\n\n<p>20 seconds.</p>\n"},{"tags":["c#","performance"],"answer_count":3,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":72,"score":3,"question_id":13215065,"title":"Find equal or nearest smaller value from an array","body":"<p>Let's suppose I have this array <em>(it is actually 255 long, values up to int.MaxValue)</em>:</p>\n\n<pre><code>int[] lows = {0,9,0,0,5,0,0,8,4,1,3,0,0,0,0};\n</code></pre>\n\n<p>From this array I would like to get index of a value <strong>equal</strong> or <strong>smaller</strong> to my number.</p>\n\n<pre><code>number = 7 -&gt; index = 4\nnumber = 2 -&gt; index = 9\nnumber = 8 -&gt; index = 7\nnumber = 9 -&gt; index = 1\n</code></pre>\n\n<p>What would be the fastest way of finding it? </p>\n\n<p>So far I've used linear search, but that turned out to be too inefficient for my need, because even though this array is only 255 long, values will be searched for a few million times. </p>\n\n<p>I would need something equal to <em>TreeSet.floor(E)</em> used in java. I wanted to use <em>Dictionary</em>, but i don't know if it can find first smaller or equal value like I need.</p>\n"},{"tags":["java","performance","random"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":35,"score":1,"question_id":13213395,"title":"Adjusting XORShift generator to return a number within a maximum","body":"<p>I need to generate random integers within a maximum. Since <strong>performance is critical</strong>, I decided to use a XORShift generator instead of Java's Random class.</p>\n\n<pre><code>long seed = System.nanoTime();\nseed ^= (seed &lt;&lt; 21);\nseed ^= (seed &gt;&gt;&gt; 35);\nseed ^= (seed &lt;&lt; 4);\n</code></pre>\n\n<p>This implementation <sub><a href=\"http://www.javamex.com/tutorials/random_numbers/xorshift.shtml\" rel=\"nofollow\">(source)</a></sub> gives me a long integer, but what I really want is an integer between 0 and a maximum.  </p>\n\n<pre><code>public int random(int max){ /*...*/}\n</code></pre>\n\n<p>What it is the most efficient way to implement this method?</p>\n"},{"tags":["performance","parallel-processing","processing-efficiency"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":60,"score":1,"question_id":13207743,"title":"Efficiency & speedup of parallel vs. serial","body":"<p>Currently, I am reading over a study a guide that my professor handed out in class. The study guide is not an assignment, just something to know what to expect on an exam. I've completed all but 1 problem and was hoping someone could help me out.</p>\n\n<p>Here is the question:\nSuppose Tserial = n and Tparallel = n/p + log2(p), where times are in miliseconds and p is the \nnumber of processes. If we increase p by a factor of k, find a formula for how much we’ll need to increase n in order to maintain constant efficiency. How much should we increase n by if we double the number of processes from 8 to 16? Is the parallel program scalable?</p>\n\n<p>Any help in understanding this would be greatly appreciated.</p>\n"},{"tags":["performance","convolution"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":32,"score":2,"question_id":13208451,"title":"What is the best 2D convolution freely-available implementation?","body":"<h1>What is the best 2D convolution freely-available implementation? </h1>\n\n<p>Some context :</p>\n\n<ul>\n<li> <b>language agnostic</b>: whatever language you like C/C++/Python/Matlab ... you name it, I need the fastest 2D convolution implementation;</li>\n<li> <b>theoretically principled</b>: For example, <b><i>FFT</i> is NOT always the fastest</b> way because it highly depends on the sizes of the 2 inputs, thus I need an implementation that is smart enough to switch between different modes accordingly;</li>\n<li> <b>format agnostic between single and double input entries</b> BUT for 64bits computers;</li>\n<li> [optional] rather <b>UNIX-based OS</b> solution <i>e.g.</i> Linux and MacOSX;</li>\n<li> [optional] rather <b>NOT GPU-based</b> solution unfortunately because it is difficult to install it/deploy it/assume that anybody has it already ready to go both for compilation and execution especially if you are not <i>root</i>/<i>admin</i>;</li>\n<li> [optional] rather <b>multi-threaded</b> solution but with an easy way for the user <b>to set the number of allowed threads/cores</b> let's say at execution time for <i>e.g.</i> large scale cluster intensive computations.</li>\n</ul>\n\n<p>Although I spent a lot of time crawling around the www, I only found:</p>\n\n<p><i>git clone <a href=\"https://github.com/rbgirshick/voc-dpm.git\" rel=\"nofollow\">https://github.com/rbgirshick/voc-dpm.git</a></i></p>\n\n<p>in the gdetect folder there is <b>fconvsse.cc file that is much faster that anything I found anywhere else</b>. </p>\n\n<p>Is it the best possible?</p>\n\n<p>I think it could be great for the Research Community to get a nice code for this old Computer Science problem with even a popularized www link that I do not know in spite of my efforts.</p>\n\n<p>In other words:</p>\n\n<h1>Do you know a fast convolution implementation that you could give me the link?</h1>\n\n<p>All I am asking is one or a few links.</p>\n\n<p>Best regards,</p>\n\n<p><i>A new grateful Stack<b>Overflow</b> user</i></p>\n"},{"tags":["c++","performance","profiling","code-snippets"],"answer_count":6,"favorite_count":12,"up_vote_count":8,"down_vote_count":0,"view_count":2276,"score":8,"question_id":61278,"title":"Quick and dirty way to profile your code","body":"<p>What method do you use when you want to get performance data about specific code paths?</p>\n"},{"tags":["sql","sql-server","performance","sql-server-2008","query"],"answer_count":4,"favorite_count":1,"up_vote_count":7,"down_vote_count":0,"view_count":35270,"score":7,"question_id":4193705,"title":"SQL Server SELECT LAST N Rows","body":"<p>This is a known question but the best solution I've found is something like:</p>\n\n<pre><code>SELECT TOP N *\nFROM MyTable\nORDER BY Id DESC\n</code></pre>\n\n<p>I've a table with lots of rows. It is not a posibility to use that query because it takes lot of time. So how can I do to select last N rows without using ORDER BY?</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>Sorry duplicated question of <a href=\"http://stackoverflow.com/questions/311054/how-do-i-select-last-5-rows-in-a-table-without-sorting\">this one</a></p>\n"},{"tags":["java","performance","instantiation","bigdecimal"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":70,"score":2,"question_id":13209833,"title":"Fastest way to instantiate BigDecimal from string","body":"<p>Consider an application that</p>\n\n<ol>\n<li>Reads several thousands of <code>String</code> values from a text file.</li>\n<li>Selects (via a regular expression match) those values that represent a number (from simple integers to very large values written in scientific notation with mantissa).</li>\n<li>For each <code>String</code> value representing a number, instantiates a <code>BigDecimal</code> object (at a total rate of thousands of Bigdecimal objects per second).</li>\n<li>Uses each instantiated <code>BigDecimal</code> object for further processing.</li>\n</ol>\n\n<p>Given the above scenario, obviously the instantiation of each <code>BigDecimal</code> object has an impact on performance.</p>\n\n<p>One way to instantiate those <code>BigDecimal</code> objects from a non-null String <code>str</code>, is:</p>\n\n<pre><code>BigDecimal number = new BigDecimal(str.toCharArray(), 0, str.length()));\n</code></pre>\n\n<p>which is exactly what the <a href=\"http://docs.oracle.com/javase/6/docs/api/java/math/BigDecimal.html#BigDecimal%28java.lang.String%29\" rel=\"nofollow\">String constructor of BigDecimal</a> expands to in the <code>Oracle</code> implementation of the <code>JDK</code>.</p>\n\n<p>Is there a faster way of instantiating <code>BigDecimal</code> objects from such strings, or via an alternative approach?</p>\n"},{"tags":["java","performance","getter-setter"],"answer_count":5,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":80,"score":1,"question_id":13209311,"title":"Should we use collection, get and sets?","body":"<p>I was reading a java book, where it said that when accessing/modifying  to variables in different class it should be use the get/set methods to manipulate them.</p>\n\n<p>My question is, overtime and in big projects using gets/sets will not jeopardize the application performance?</p>\n\n<p>Similar question, typically should we preferable use arrays in detriment of more abstract data type (such likened list of instance), since array are normally more cache friendly.</p>\n"},{"tags":["c#",".net","performance","sockets"],"answer_count":11,"favorite_count":20,"up_vote_count":17,"down_vote_count":0,"view_count":15595,"score":17,"question_id":319732,"title":"Tips / techniques for high-performance C# server sockets","body":"<p>I have a .NET 2.0 server that seems to be running into scaling problems, probably due to poor design of the socket-handling code, and I am looking for guidance on how I might redesign it to improve performance.</p>\n\n<p><strong>Usage scenario:</strong> 50 - 150 clients, high rate (up to 100s / second) of small messages (10s of bytes each) to / from each client. Client connections are long-lived - typically hours. (The server is part of a trading system. The client messages are aggregated into groups to send to an exchange over a smaller number of 'outbound' socket connections, and acknowledgment messages are sent back to the clients as each group is processed by the exchange.) OS is Windows Server 2003, hardware is 2 x 4-core X5355. </p>\n\n<p><strong>Current client socket design:</strong> A <code>TcpListener</code> spawns a thread to read each client socket as clients connect. The threads block on <code>Socket.Receive</code>, parsing incoming messages and inserting them into a set of queues for processing by the core server logic. Acknowledgment messages are sent back out over the client sockets using async <code>Socket.BeginSend</code> calls from the threads that talk to the exchange side.</p>\n\n<p><strong>Observed problems:</strong> As the client count has grown (now 60-70), we have started to see intermittent delays of up to 100s of milliseconds while sending and receiving data to/from the clients. (We log timestamps for each acknowledgment message, and we can see occasional long gaps in the timestamp sequence for bunches of acks from the same group that normally go out in a few ms total.) </p>\n\n<p>Overall system CPU usage is low (&lt; 10%), there is plenty of free RAM, and the core logic and the outbound (exchange-facing) side are performing fine, so the problem seems to be isolated to the client-facing socket code. There is ample network bandwidth between the server and clients (gigabit LAN), and we have ruled out network or hardware-layer problems.</p>\n\n<p>Any suggestions or pointers to useful resources would be greatly appreciated. If anyone has any diagnostic or debugging tips for figuring out exactly what is going wrong, those would be great as well.</p>\n\n<p><em>Note: I have the MSDN Magazine article <a href=\"http://msdn2.microsoft.com/en-us/magazine/cc300760.aspx\" rel=\"nofollow\">Winsock: Get Closer to the Wire with High-Performance Sockets in .NET</a>, and I have glanced at the Kodart \"XF.Server\" component - it looks sketchy at best.</em></p>\n"},{"tags":["c#","performance","events","xna","keyboard"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":30,"score":0,"question_id":13208320,"title":"Keyboard button \"event\" efficiency in XNA","body":"<p>I have been trying to figure it out for myself, but all tutorials online and everything I could find, does not really explain my question, so I hope someone here can help me.</p>\n\n<p>I so far have only worked with C# mainly using WPF and if I want to raise an event whey a key is pressed on the keyboard, I simply use the <code>KeyDown</code> event. There I can easily identify the pressed key by <code>e.Key</code>.</p>\n\n<p>Now in XNA everything I have seen is using <code>KeyboardState state = Keyboard.GetState();</code> to get the state of the keyboard and constantly check in the <code>Update()</code>-method if e.g. <code>state.IsKeyDown(Keys.Left);</code> returns true of false.</p>\n\n<p>And my question is: Is that not really inefficient? If for example my game uses 15 keys for input, I would get the keyboard state and check every single of those 15 keys and that 30 times a second. Is there a reason, why it seems to be so common to use this approach in XNA?\nThe only explanation I could think of, is to make sure everything remains in the <code>Update</code>-method so it will definietly be executed, such that no delayed events cause problems in the game.</p>\n"},{"tags":["css","performance","selectors"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":49,"score":0,"question_id":13207683,"title":"Do browsers read css selectors from right to left?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/5797014/why-do-browsers-match-css-selectors-from-right-to-left\">Why do browsers match CSS selectors from right to left?</a>  </p>\n</blockquote>\n\n\n\n<p>I've heard browsers read css selectors from right to left. So if we write like this;</p>\n\n<pre><code>#block1 a{text-decoration:none;} \n</code></pre>\n\n<p>browsers find first, all a tag then find block1 and performance drops a bit. Is it true?</p>\n"},{"tags":["java","performance","activemq"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":22,"score":1,"question_id":13207660,"title":"Activemq messaging rate","body":"<p>I will be using activemq for a project and wanted to be able to stress test the application. In which way can I send x number of messages per second so that I can account for a normal period of messaging and see how it handles an abnormal rate of messaging also?</p>\n\n<p>Thanks</p>\n"},{"tags":["mysql","performance","wordpress"],"answer_count":5,"favorite_count":6,"up_vote_count":7,"down_vote_count":0,"view_count":5540,"score":7,"question_id":3753504,"title":"Mysqltuner suggestions and changes to my.cnf","body":"<p>Had this question on Serverfault for a few days with no luck.</p>\n\n<p>I've run mysqltuner.pl on a VPS and have a bunch of questions as to the suggestions on variables to change. I'm sure these are general questions with complex answers.</p>\n\n<p>I'm not knowledgable enough to write queries and test them against the server, but am just trying to get a bit more performance out of the server that runs five WordPress sites with >200,000 page views/month.</p>\n\n<p>I've optimized the database via phpmyadmin (and do that regularly), but the tuner still says there are fragmented tables. And because this is WordPress, I can't change queries in core code.</p>\n\n<p>But how much should I increase the variables like query_cache_size and  innodb_buffer_pool_size? What about the other innodb variables?</p>\n\n<p>Some of the variables suggested don't exist in my.cnf, like table_cache, and are flagged in tuner report, etc. Can I add them to my.cnf?</p>\n\n<blockquote>\n  <p><em>(And why is this block duplicated in my.cnf? Can I delete the duplicate?)</em></p>\n\n<pre><code>set-variable = innodb_buffer_pool_size=2M\nset-variable = innodb_additional_mem_pool_size=500K\nset-variable = innodb_log_buffer_size=500K\nset-variable = innodb_thread_concurrency=2\n</code></pre>\n</blockquote>\n\n<p>Below is the my.cnf and the output of mysqltuner:</p>\n\n<p><strong>Contents of my.cnf:</strong></p>\n\n<pre><code>query-cache-type = 1\nquery-cache-size = 8M\n\nset-variable=local-infile=0\ndatadir=/var/lib/mysql\nsocket=/var/lib/mysql/mysql.sock\nuser=mysql\n\nold_passwords=1\n\nskip-bdb\n\nset-variable = innodb_buffer_pool_size=2M\nset-variable = innodb_additional_mem_pool_size=500K\nset-variable = innodb_log_buffer_size=500K\nset-variable = innodb_thread_concurrency=2\n\n[mysqld_safe]\nlog-error=/var/log/mysqld.log\npid-file=/var/run/mysqld/mysqld.pid\nskip-bdb\n\nset-variable = innodb_buffer_pool_size=2M\nset-variable = innodb_additional_mem_pool_size=500K\nset-variable = innodb_log_buffer_size=500K\nset-variable = innodb_thread_concurrency=2\n</code></pre>\n\n<p><strong>Output of mysqltuner:</strong></p>\n\n<pre><code>------- General Statistics --------------------------------------------------\n[--] Skipped version check for MySQLTuner script\n[OK] Currently running supported MySQL version 5.0.45\n[!!] Switch to 64-bit OS - MySQL cannot currently use all of your RAM\n\n-------- Storage Engine Statistics -------------------------------------------\n[--] Status: -Archive -BDB -Federated +InnoDB -ISAM -NDBCluster \n[--] Data in MyISAM tables: 133M (Tables: 637)\n[--] Data in InnoDB tables: 10M (Tables: 344)\n[--] Data in MEMORY tables: 126K (Tables: 2)\n[!!] Total fragmented tables: 69\n\n-------- Security Recommendations  -------------------------------------------\n[OK] All database users have passwords assigned\n\n-------- Performance Metrics -------------------------------------------------\n[--] Up for: 1d 6h 24m 13s (2M q [22.135 qps], 116K conn, TX: 4B, RX: 530M)\n[--] Reads / Writes: 97% / 3%\n[--] Total buffers: 35.0M global + 2.7M per thread (100 max threads)\n[OK] Maximum possible memory usage: 303.7M (8% of installed RAM)\n[OK] Slow queries: 0% (4/2M)\n[OK] Highest usage of available connections: 53% (53/100)\n[OK] Key buffer size / total MyISAM indexes: 8.0M/46.1M\n[OK] Key buffer hit rate: 99.6% (749M cached / 2M reads)\n[OK] Query cache efficiency: 32.2% (685K cached / 2M selects)\n[!!] Query cache prunes per day: 948863\n[OK] Sorts requiring temporary tables: 0% (0 temp sorts / 660K sorts)\n[!!] Temporary tables created on disk: 46% (400K on disk / 869K total)\n[!!] Thread cache is disabled\n[!!] Table cache hit rate: 0% (64 open / 24K opened)\n[OK] Open file limit used: 10% (109/1K)\n[OK] Table locks acquired immediately: 99% (2M immediate / 2M locks)\n[!!] InnoDB data size / buffer pool: 10.6M/2.0M\n\n-------- Recommendations -----------------------------------------------------\nGeneral recommendations:\n    Run OPTIMIZE TABLE to defragment tables for better performance\n    Enable the slow query log to troubleshoot bad queries\n    When making adjustments, make tmp_table_size/max_heap_table_size equal\n    Reduce your SELECT DISTINCT queries without LIMIT clauses\n    Set thread_cache_size to 4 as a starting value\n    Increase table_cache gradually to avoid file descriptor limits\nVariables to adjust:\n    query_cache_size (&gt; 8M)\n    tmp_table_size (&gt; 32M)\n    max_heap_table_size (&gt; 16M)\n    thread_cache_size (start at 4)\n    table_cache (&gt; 64)\n    innodb_buffer_pool_size (&gt;= 10M)\n</code></pre>\n"},{"tags":["performance","oracle","business-intelligence","partitioning","tuning"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":861,"score":0,"question_id":3193715,"title":"Optimize SELECT from Partitioned Fact Table in Oracle 10","body":"<p>I have a fact table containing 8 Million rows with 1 Million rows increase per month. The table already contains indexes on it. The table is used by IBM Cognos environment to generate reports. Currently I am looking for way to optimize the table SELECT statements. </p>\n\n<p>As first try, I partitioned the table (each partition has equal distribution of rows) and the query is suitable for the partitions, but for some reason, I am getting equal or even worse performance, which is weird. Only one partition is affected per query. Can someone explain how to optimize this ?</p>\n\n<p>Second idea I came to is to implement the fact table as Index organized table, but it will have to have all the columns as primary key. Is this alright and will there be performance gain ?</p>\n\n<p>Third idea is to implement the fact table in a way that will contain all the columns that are joined from the star schema. Will there be performance gain ?</p>\n\n<p>EDIT: Here is the execution plan:\n<a href=\"http://i50.tinypic.com/11qtzr6.jpg\" rel=\"nofollow\">http://i50.tinypic.com/11qtzr6.jpg</a></p>\n\n<p>I have managed to reduce the access time to fact table FT_COSTS by 3 times (cost was 42000, now is 14900) AFTER I created indexes containing the partitioning criteria, but before that I was getting worse results than in unpartitioned table. I used this link to solve my partitioning problem <a href=\"http://stackoverflow.com/questions/2535908/range-partition-skip-check\">http://stackoverflow.com/questions/2535908/range-partition-skip-check</a></p>\n\n<p>From what I see now, the main bottleneck is the GROUP BY which raises the cost from 34000 to 85 000 , which is more than doubling . Does anyone have idea about a workaround on this ?</p>\n"},{"tags":["performance","python-3.x","double-underscore"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":36,"score":1,"question_id":13195510,"title":"the cost of rewrite __setattr__ () was too high","body":"<p>I want to save the time and mark the object as modified, so I wrote a class and override its <code>__setattr__</code> function. </p>\n\n<pre><code>import time\n\nclass CacheObject(object):\n    __slots__ = ('modified', 'lastAccess')\n    def __init__(self):\n        object.__setattr__(self,'modified',False)\n        object.__setattr__(self,'lastAccess',time.time())\n\n    def setModified(self):\n        object.__setattr__(self,'modified',True)\n        object.__setattr__(self,'lastAccess',time.time())\n\n    def resetTime(self):\n        object.__setattr__(self,'lastAccess',time.time())\n\n    def __setattr__(self,name,value):\n        if (not hasattr(self,name)) or object.__getattribute__(self,name)!=value: \n            object.__setattr__(self,name,value)\n            self.setModified()\n\nclass example(CacheObject):\n    __slots__ = ('abc',)\n    def __init__(self,i):\n        self.abc = i\n        super(example,self).__init__()\n\nt = time.time()\nf = example(0)\nfor i in range(100000):\n    f.abc = i\n\nprint(time.time()-t)\n</code></pre>\n\n<p>I measured the process time, and it took 2 seconds. When I commented out overridden function, the process time was 0.1 second, I know the overridden function would be slower but almost 20 times the gap is too much. I think I must get something wrong.</p>\n\n<p>take the suggestion from cfi</p>\n\n<p>1.elimate the if condition</p>\n\n<pre><code>    def __setattr__(self,name,value):\n#        if (not hasattr(self,name)) or object.__getattribute__(self,name)!=value: \n            object.__setattr__(self,name,value)\n            self.setModified()\n</code></pre>\n\n<p>the running time down to 1.9, a little improve but mark the object modified if it's not changed would cost more in other process, so not an option.</p>\n\n<p>2.change self.func to classname.func(self)</p>\n\n<pre><code>def __setattr__(self,name,value):\n    if (not hasattr(self,name)) or object.__getattribute__(self,name)!=value: \n        object.__setattr__(self,name,value)\n        CacheObject.setModified(self)\n</code></pre>\n\n<p>running time is 2.0 .so nothing really changed</p>\n\n<p>3)extract setmodified function </p>\n\n<pre><code>def __setattr__(self,name,value):\n    if (not hasattr(self,name)) or object.__getattribute__(self,name)!=value: \n        object.__setattr__(self,name,value)\n        object.__setattr__(self,'modified',True)\n        object.__setattr__(self,'lastAccess',time.time())\n</code></pre>\n\n<p>running time down to 1.2!!That's great ,it do save almost 50% time,though the cost is  still  high.</p>\n"},{"tags":["wpf","performance","datagrid"],"answer_count":5,"favorite_count":15,"up_vote_count":21,"down_vote_count":0,"view_count":14915,"score":21,"question_id":697701,"title":"WPF Datagrid Performance","body":"<p>I am working with the WPF Toolkit data grid and it is scrolling extremely slow at the moment.  The grid has 84 columns and 805 rows. (Including 3 fixed columns and the header is fixed.)  Scrolling both horizontally and vertically is extremely slow.  Virtualization is turned on and I have enabled column virtualization and row virtualization explicitly in the xaml.  Is there anything to watch out for that can really effect performance, such as binding methods, or what xaml is in each celltemplate?</p>\n\n<p>One thing to note is I am dynamically adding the columns on creation of the datagrid.  Could that be effecting anything? (I also dynamically create the celltemplate at the same time so that my bindings are set right.)</p>\n\n<p>Below is the code from the template for most of the cells that get generated.  Basically for the columns I need to dynamically add (which is most of them), I loop through my list and add the columns using the AddColumn method, plus I dynamically build the template so that the binding statements properly index the right item in the collection for that column.  The template isn't too complex, just two TextBlocks, but I do bind four different properties on each.  It seems like I was able to squeeze out a little bit more performance by changes the bindings to OneWay:</p>\n\n<pre><code> private void AddColumn(string s, int index)\n    {\n        DataGridTemplateColumn column = new DataGridTemplateColumn();\n        column.Header = s;\n        //Set template for inner cell's two rectangles\n        column.CellTemplate = CreateFactViewModelTemplate(index);\n        //Set Style for header, ie rotate 90 degrees\n        column.HeaderStyle = (Style)dgMatrix.Resources[\"HeaderRotateStyle\"];\n        column.Width = DataGridLength.Auto;\n        dgMatrix.Columns.Add(column);\n    }\n\n\n    //this method builds the template for each column in order to properly bind the rectangles to their color\n    private static DataTemplate CreateFactViewModelTemplate(int index)\n    {\n        string xamlTemplateFormat =\n            @\"&lt;DataTemplate xmlns=\"\"http://schemas.microsoft.com/winfx/2006/xaml/presentation\"\"\n            xmlns:x=\"\"http://schemas.microsoft.com/winfx/2006/xaml\"\"&gt;\n            &lt;Grid&gt;\n            &lt;Grid.ColumnDefinitions&gt;\n                &lt;ColumnDefinition /&gt;\n                &lt;ColumnDefinition /&gt;\n            &lt;/Grid.ColumnDefinitions&gt;\n            &lt;TextBlock Grid.Column=\"\"0\"\" MinHeight=\"\"10\"\" MinWidth=\"\"10\"\" HorizontalAlignment=\"\"Stretch\"\" Padding=\"\"3 1 3 1\"\" TextAlignment=\"\"Center\"\" Foreground=\"\"{Binding Path=FactViewModels[~Index~].LeftForeColor,Mode=OneWay}\"\" Background=\"\"{Binding Path=FactViewModels[~Index~].LeftColor,Mode=OneWay}\"\" Text=\"\"{Binding Path=FactViewModels[~Index~].LeftScore,Mode=OneWay}\"\" /&gt;\n            &lt;TextBlock Grid.Column=\"\"1\"\" MinHeight=\"\"10\"\" MinWidth=\"\"10\"\" HorizontalAlignment=\"\"Stretch\"\" Padding=\"\"3 1 3 1\"\" TextAlignment=\"\"Center\"\" Foreground=\"\"{Binding Path=FactViewModels[~Index~].RightForeColor,Mode=OneWay}\"\" Background=\"\"{Binding Path=FactViewModels[~Index~].RightColor,Mode=OneWay}\"\" Text=\"\"{Binding Path=FactViewModels[~Index~].RightScore,Mode=OneWay}\"\" /&gt;\n            &lt;/Grid&gt;\n            &lt;/DataTemplate&gt;\";\n\n\n\n\n        string xamlTemplate = xamlTemplateFormat.Replace(\"~Index~\", index.ToString());\n\n        return (DataTemplate)XamlReader.Parse(xamlTemplate);\n    }\n</code></pre>\n"},{"tags":["c#",".net","wpf","performance"],"answer_count":3,"favorite_count":5,"up_vote_count":10,"down_vote_count":0,"view_count":3692,"score":10,"question_id":3022921,"title":"WPF DataGrid performance concerns","body":"<p>I am testing WPF DataGrid in hopes of replacing some winforms controls, and so far have been very pleased with the development process. Performance seems to be my biggest concern right now. My development workstation has just about the best cpu on the market running windows 7, with 6 gigs of DDR3 memory. The windows control i am replacing is considerably more responsive which is worrisome.</p>\n\n<p>My test is a basic implementation of DataGrid bound to ObservableCollection which gets updated once per second. It also includes Details area which is expandable to reveal more info about each row. Details area is just a stackpanel with a ItemsControl wrapping TextBlock (which repeats 6 times)</p>\n\n<p>My complaint is that if i try to scroll this collection, it is often jerky with lag, and if i try to expand each row as they come in, about 15% of clicks do not trigger buttons click event (DataGridTmplateColumn > CellTemplate > DataTemplate > Button)\nAlso scrolling is more jittery if some rows detail is expanded (with scroll bar that resizes it self as it goes up/down)</p>\n\n<p>what are some things to look for / optimize / avoid ? </p>\n\n<p><strong>update</strong></p>\n\n<p>here are some points i found helpful so far: </p>\n\n<ul>\n<li><p>rely as little as possible on dynamic layout. as each component contains many subcomponents and in dynamic layout world, all of them have to call Measure and Layout methods which can be cpu intensive. so instead of column width Auto (or no width specified), use fixed widths</p></li>\n<li><p>install <a href=\"http://msdn.microsoft.com/en-us/library/aa969767.aspx#installing_the_wpf_performance_suite\" rel=\"nofollow\">WPF Performance Suite</a> and get in touch with how your app is rendered. trully awesome app</p></li>\n<li><p>as Andrew pointed out ListView is a great alternative, for when you don't need advanced DataGrid features, such as updating the data back, or possibly Details View (which i am still hoping to reproduce)</p></li>\n<li><p>also <a href=\"http://stackoverflow.com/questions/2460557/itemscontrol-itemssource-mvvm-performance\">SuspendableObservableCollection</a> is ideal for when you're adding multiple items in very short period of time (i.e. 100 items in 0.01 second etc) </p></li>\n<li><p>after lots of testing, i found that BindingList is much faster than ObservableCollection . I posted performance profiler snapshots <a href=\"http://stackoverflow.com/questions/3305383/wpf-whats-the-most-efficient-fast-way-of-adding-items-to-a-listview\">here</a> of the same load handled by BindingList vs Observable collection, and the former takes less than half cpu time. (keep in mind that this is not just collection performance, but when paired with a ListView)</p></li>\n</ul>\n\n<p>my search still continues as something appears to be leaking memory in my app and slows it down to a halt after couple hours.</p>\n"},{"tags":["javascript","asp.net","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":45,"score":0,"question_id":13205965,"title":"Is putting the javascript before the closing body tag okay on an asp.net website?","body":"<p>I pretty much stated what I have to ask.  But is taking all of your external .js files and putting them <strong>before</strong> the closing body tag on your master pages okay on an asp.net website?</p>\n\n<p>I'm just going off of what yslow and google speed have been showing.  I can't combine these javascripts, so I'm trying to load them \"<strong>after page load</strong>\", but doing so makes them useless; <strong>some of my jquery things don't work</strong>.</p>\n\n<p>I moved my .js files above the opening body tag, and they work.  What am I doing wrong?  And what could I do to load my .js files <strong>after page load</strong>?  Thanks for any advice anybody can offer!</p>\n"},{"tags":["c++","performance","visual-c++","c++11"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":126,"score":0,"question_id":13205885,"title":"Performance wise, is it faster to use 'nullptr' or just '0'?","body":"<p>For example:</p>\n\n<pre><code>object* pObject = nullptr;\n</code></pre>\n\n<p>OR:</p>\n\n<pre><code>object* pObject = 0;\n</code></pre>\n\n<p>Again, which one is better performance wise?</p>\n"},{"tags":["asp.net-mvc","performance","nhibernate","guid"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":26,"score":1,"question_id":13202558,"title":"nhibernate guid performance","body":"<p>What would be the overhead of using GUID's instead of an integer identity in nHibernate for table primary keys? </p>\n\n<p>The main reason for use would be the obfuscation of table ids on user facing data.</p>\n"},{"tags":["android","performance","listview","image-loading"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":241,"score":0,"question_id":12099383,"title":"ListView VERY slow when images are loaded (using Universal Image Loader)","body":"<p>My listView runs very smooth with just text - but as soon as I try to load in thumbnails (even from cache), it runs SOO choppy.</p>\n\n<p>I'm using the <a href=\"https://github.com/nostra13/Android-Universal-Image-Loader\" rel=\"nofollow\">Universal Image Loader script</a></p>\n\n<p>The code in my ArticleEntryAdapter within <code>public View getView(...)</code> method:</p>\n\n<pre><code>/**\n     * PHOTOS\n     */\n    ImageLoaderConfiguration config = new ImageLoaderConfiguration.Builder(this.mContext)\n        .enableLogging()\n        .memoryCacheSize(41943040)\n        .discCacheSize(104857600)\n        .threadPoolSize(10)\n        .build();\n\n    DisplayImageOptions imgDisplayOptions = new DisplayImageOptions.Builder()\n        //.showStubImage(R.drawable.stub_image)\n        .cacheInMemory() \n        .cacheOnDisc() \n        //.imageScaleType(ImageScaleType.EXACT) \n        .build();\n\n    ImageLoader imageLoader = ImageLoader.getInstance();\n    imageLoader.init(config);\n\n    //loads image (or hides image area)\n    imageLoader.displayImage(\"\", viewHolder.thumbView); //clears previous one\n    if(article.photopath != null &amp;&amp; article.photopath.length() != 0)\n    {\n        imageLoader.displayImage(\n            \"http://img.mysite.com/processes/resize_android.php?image=\" + article.photopath + \"&amp;size=150&amp;quality=80\",\n            viewHolder.thumbView,\n            imgDisplayOptions\n            );\n        viewHolder.thumbView.setVisibility(View.VISIBLE);\n    }\n    else\n    {\n        viewHolder.thumbView.setVisibility(View.GONE); //hide image\n        viewHolder.thumbView.invalidate(); //should call after changing to GONE\n    }\n</code></pre>\n\n<p>Logcat shows that it's loading the images from cache (I think):</p>\n\n<pre><code>ImageLoader    Load image from memory cache [http://img.mysite.com/processes/...\n</code></pre>\n\n<p>I'm testing it on my Samsung Galaxy Nexus and running Android 4.0.4 (though my minSdkVersion=\"8\")</p>\n"},{"tags":["performance","query","google-app-engine","memcached","gae-datastore"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":22,"score":-1,"question_id":13205472,"title":"Most Efficient Way for Users to Search for all of a Database Model","body":"<p>I have two database models in my webapp and one has around 10,000 entries and the other has close to double that. I have it set up right now so that all of these entries are kept in memcache.  I also have it set up so that the user can search by certain constraints and I just filter the cache by those constraints with some if statements.  Part of this is so that I don't have to pay for the search quota with the Google Search API.</p>\n\n<p>I wanted to see if using memcache here is the best option or if I should simply do a DB query which will quickly run up my read quota or if there is any other good option.  One of my problems here is that as my database gets larger the searches are getting slower.</p>\n\n<p>Here is some example code where I am searching by location and then filtering by a sport category:</p>\n\n<pre><code>        #this is the cache I am using\n        groups = group_cache()\n        search_location = self.request.get(\"search_location\")\n        search_postsport = self.request.get(\"search_postsport\")\n        GPSlocation = None\n        error=None\n        groups_to_render = None\n\n        if search_location:\n            g = geocoders.Google()\n            searched_location = None\n            #to catch an error where there is no corresponding location\n            try:\n                #to catch an error where there are multiple returned locations\n                try:\n                    place, (lat, lng) = g.geocode(search_location)\n                    place, searched_location = g.geocode(search_location)\n                except ValueError:\n                    geocodespot = g.geocode(search_location, exactly_one=False)\n                    place, (lat, lng) = geocodespot[0]\n                    place, searched_location = geocodespot[0]\n                GPSlocation = \"(\"+str(lat)+\", \"+str(lng)+\")\"\n\n                groups = group_cache()\n                return_groups = []                   \n                for group in groups:\n                    distance.distance = distance.GreatCircleDistance\n                    location_distance = distance.distance(searched_location, point.Point(group.lat, group.lng)).miles\n\n                    if location_distance &lt; 300:\n                        return_groups.append(group)\n                groups = return_groups\n                search=True\n                groups_to_render = sorted(groups, key=attrgetter('recordCount'), reverse=True)\n                groups_to_render = groups_to_render[:10]\n</code></pre>\n"},{"tags":["python","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":3,"down_vote_count":1,"view_count":31,"score":2,"question_id":13205026,"title":"Determining Pythagorean Triples using c value","body":"<p>Hello stackoverflow community, I have a quick question to ask of you. Below is my written code to determine if based on the hypotenuse, if a pythagorean triple is possible, and what are the lengths. I was wondering if there a more efficient way of doing so. Thanks! </p>\n\n<pre><code>#Victor C\n#Determines if a right triangle with user inputted hypotenuse is capable of being a Pythagorean Triple\n\nimport math\ntriplecheck = True\nwhile True:\n    hypotenuse = int(input(\"Please enter the hypotentuse:\")) #Smallest hypotenuse which results in a pythagorean triple is 5.\n    if hypotenuse &gt; 4:\n        break\nc = csqr = hypotenuse**2 #sets two variables to be equivalent to c^^2. c is to be modified to shorten factoring, csqr as a reference to set a value\nif c % 2 != 0:   #even, odd check of value c, to create an integer when dividing by half.\n    c = (c+1)//2\nelse:\n    c = c//2\nfor b in range(1,c+1):  #let b^^2 = b || b is equal to each iteration of factors of c^^2, a is set to be remainder of c^^2 minus length b.\n    a = csqr-b\n    if (math.sqrt(a))%1 == 0 and (math.sqrt(b))%1 == 0: #if squareroots of a and b are both equal to 0, they are integers, therefore fulfilling conditions\n       tripleprompt = \"You have a Pythagorean Triple, with the lengths of \"+str(int(math.sqrt(b)))+\", \"+str(int(math.sqrt(a)))+\" and \"+str(hypotenuse)\n       print(tripleprompt)\n       triplecheck = False\nif triplecheck == True:\n    print(\"Sorry, your hypotenuse does not make a Pythagorean Triple.\")\n</code></pre>\n"},{"tags":["php","json","performance","api","slow-load"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":342,"score":0,"question_id":12130091,"title":"I am using json api for twitch and own3d tv and my page is veeery slow","body":"<p>As i said in the title i am using through php the json api for twitch tv and own3d tv to get the information of the stream i want.</p>\n\n<p>Ths problem is that the page is not loading fast, as a matter of fact some times the php server stops because of 30 or more secs of function.\nThe error : Fatal error: Maximum execution time of 30 seconds exceeded in</p>\n\n<p>I am using an online indicator : </p>\n\n<pre><code> function status($stream_id, $type){\nif($type == 't'){\n\n    $chan = \"http://api.justin.tv/api/stream/list.json?channel=\" . $stream_id;\n    $json = file_get_contents($chan);\n    $exist = strpos($json, $stream_id);\n    if($exist) {\n        return true;\n    }else{\n        return false;   \n    }\n\n\n}else if($type == 'o'){\n\n    $url = 'http://api.own3d.tv/liveCheck.php?live_id=' . $stream_id;\n    $xml = simplexml_load_file($url);\n\n    $isLive=$xml-&gt;liveEvent-&gt;isLive;\n\n    if ($isLive == \"true\") {\n        return true;\n    }else{\n        return false;   \n    }\n\n}\n}\n</code></pre>\n\n<p>and i am using a function that get some info from the stream :</p>\n\n<pre><code>function api_stream_data($stream_id, $type){\n$stream_id = sanitize($stream_id);\n$type = sanitize($type);\n\nif($type == 't'){\n\n    $streamData = json_decode(file_get_contents(\"http://api.justin.tv/api/stream/list.json?channel=$stream_id\"),true);\n\n    $data = array(\n        'image'=&gt;$streamData[0]['channel']['image_url_medium'],\n        'title'=&gt;$streamData[0]['title'],\n        'limage'=&gt;$streamData[0]['channel']['screen_cap_url_huge'],\n        'game'=&gt;$streamData[0]['meta_game']\n    );\n}else if($type == 'o'){\n    $streamData = json_decode(file_get_contents(\"http://api.own3d.tv/rest/live/list.json?liveid=$stream_id\"),true);\n    $data = array(\n        'image'=&gt;$streamData[0]['thumbnail_small'],\n        'title'=&gt;$streamData[0]['live_name'],\n        'limage'=&gt;$streamData[0]['thumbnail_large'],\n        'game'=&gt;$streamData[0]['game_name']\n    );\n}\n\nreturn $data;\n\n}\n</code></pre>\n\n<p>All functions works perfectly but the problem is the time they get to excecute....</p>\n\n<p>Is there any possible way to do that faster??\nI have seen some other site examples that are loading very fast like <a href=\"http://www.solomid.net\" rel=\"nofollow\">www.solomid.net</a> and <a href=\"http://www.clgaming.net\" rel=\"nofollow\">www.clgaming.net</a> .</p>\n\n<p>Thanks in advance for any help!</p>\n"},{"tags":["performance","ios5","ios6","abaddressbook","slowness"],"answer_count":0,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":110,"score":0,"question_id":12754683,"title":"ABAddressBook poor performance on since iOS6","body":"<p>i have a App that imports round about 12000 numbers in 3 contacts and 1 group. Since iOS6 this process needs 5 times longer than before with iOS 5.1. In the simulator it isn't that slow than on a device.</p>\n\n<p>I tried some profiling and i found out that the main time is used by ABAddressBookSave with 28% of execusion time. I also got the feeling that everything with Addressbook is slower than befor. On iOS 5 its about 3% of the total execusion time. Memory and CPU are ok.</p>\n\n<p>Has anyone similar problems, did anyone found out why this happens or a solution to fix it?</p>\n\n<p><strong>Here is a stackTrace why it is so slow in iOS6</strong> </p>\n\n<p>Running Time    Self        Symbol Name\n3212.0ms   41.6%    0,0       ABAddressBookSave</p>\n\n<p>3212.0ms   41.6%    0,0        ABCSave</p>\n\n<p>3212.0ms   41.6%    0,0         ABCAddressBookSaveWithConflictPolicy</p>\n\n<p>3198.0ms   41.4%    0,0          CPRecordStoreSaveWithPreAndPostCallbacksAndTransactionType</p>\n\n<p>3134.0ms   40.6%    0,0           CFDictionaryApplyFunction</p>\n\n<p>3134.0ms   40.6%    0,0            CFBasicHashApply</p>\n\n<p>3134.0ms   40.6%    0,0             __CFDictionaryApplyFunction_block_invoke_0</p>\n\n<p>3134.0ms   40.6%    0,0              CPRecordStoreUpdateRecord</p>\n\n<p>2971.0ms   38.5%    0,0               _didSave</p>\n\n<p>2971.0ms   38.5%    0,0                ABCDContextUpdateSearchIndexForPersonAndProperties</p>\n\n<p>2773.0ms   35.9%    0,0                 CPSqliteStatementPerform</p>\n\n<p>2773.0ms   35.9%    0,0                  sqlite3_step</p>\n\n<p>2773.0ms   35.9%    0,0                   sqlite3VdbeExec</p>\n\n<p>2772.0ms   35.9%    0,0                    fts3UpdateMethod</p>\n\n<p>2765.0ms   35.8%    0,0                     fts3PendingTermsAdd</p>\n\n<p>2734.0ms   35.4%    0,0                      ABCFFTSTokenizerOpen</p>\n\n<p>2734.0ms   35.4%    0,0                       ABTokenListPopulateFromString</p>\n\n<p>2631.0ms   34.1%    1,0                        CFStringGetBytes</p>\n\n<p>2630.0ms   34.1%    2624,0                          __CFStringEncodeByteStream</p>\n\n<p>6.0ms    0.0%   0,0                          CFStringEncodingIsValidEncoding</p>\n\n<p><strong>And here the same method in iOS 5</strong></p>\n\n<p>Running Time    Self        Symbol Name</p>\n\n<p>245.0ms   12.9% 0,0     ABAddressBookSave</p>\n\n<p>245.0ms   12.9% 0,0      ABCSave</p>\n\n<p>245.0ms   12.9% 0,0       ABCAddressBookSaveWithConflictPolicy</p>\n\n<p>234.0ms   12.3% 0,0        CPRecordStoreSaveWithPreAndPostCallbacksAndTransactionType</p>\n\n<p>167.0ms    8.8% 0,0         CFDictionaryApplyFunction</p>\n\n<p>167.0ms    8.8% 0,0          CFBasicHashApply</p>\n\n<p>167.0ms    8.8% 0,0           __CFDictionaryApplyFunction_block_invoke_0</p>\n\n<p>167.0ms    8.8% 0,0            CPRecordStoreUpdateRecord</p>\n\n<p>162.0ms    8.5% 0,0             CFDictionaryApplyFunction</p>\n\n<p>162.0ms    8.5% 0,0              CFBasicHashApply</p>\n\n<p>162.0ms    8.5% 0,0               __CFDictionaryApplyFunction_block_invoke_0</p>\n\n<p>162.0ms    8.5% 0,0                CPRecordStoreSaveProperty</p>\n\n<p>158.0ms    8.3% 0,0                 ABCMultiValueSave</p>\n\n<p>158.0ms    8.3% 1,0                  ABCDBContextSaveMultiValue</p>\n\n<p>143.0ms    7.5% 0,0                   CPSqliteConnectionAddRecord</p>\n\n<p>143.0ms    7.5% 1,0                    CPSqliteConnectionAddRecordWithRowid</p>\n\n<p>85.0ms    4.5%  0,0                     CPSqliteStatementPerform</p>\n\n<p>16.0ms    0.8%  2,0                     CFRelease</p>\n"},{"tags":["performance","caching","computer-science"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":20,"score":0,"question_id":13204285,"title":"How to determine cache misses in this example of code","body":"<p>I wanted help on determining misses in the cache, and how/what is the process of determining it? <br>Thank you so much for your time and help!</p>\n\n<p><img src=\"http://i.stack.imgur.com/tXDN1.png\" alt=\"enter image description here\"></p>\n\n<p>DM: Direct Mapped<br>\nC:Cache Size<br>\nB:Block Size (Bytes)<br>\nS:Number of Sets<br>\nE:Number of Lines per Set</p>\n"},{"tags":["performance","oracle","exadata"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":22,"score":0,"question_id":13202412,"title":"Can I run exadata with OPTIMIZER_FEATURES_ENABLE set to 9 or 10?","body":"<p>Does Exadata support OPTIMIZER_FEATURES_ENABLE with pre-11 values?</p>\n\n<p>Or does the unique nature of Exadata prevent that from making sense?</p>\n"},{"tags":["mysql","performance","joomla","webserver"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":44,"score":1,"question_id":13137792,"title":"Joomla 300 concurrent users","body":"<p>I've an issue on a website made in Joomla 2.5 with Ja teline IV template\nthat has 300 concurrent user, \nit is a soccer magazine, so the article are updated often, \nalso minute by minute during the match. </p>\n\n<p>I've a server of 16gb ram and quad core processor, but the website freeze when 300 users are accessing to the website.</p>\n\n<p>I've done all the frontend optimization, but, I the last optimization could be enable caching. \nMy issues are: \n - caching enabled also for logged in users \n - caching timing, if I have that type of article, I can enable cache\n   expiring to 1 minute? It is also a good option? Could optimize the performance.</p>\n\n<p>Can you suggest me what to do? Other possible optimization?</p>\n"},{"tags":["python","performance","profiling","profile","cprofile"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":26,"score":0,"question_id":13199657,"title":"Python Profiling - Rollup function calls that are outside my code","body":"<p>I am trying to profile our django unittests (if the tests are faster, we'll run 'em more often). I've ran it through python's built in cProfile profiler, producing a pstats file.</p>\n\n<p>However the signal to noise ration is bad. There are too many functions listed. Lots and lots of django internal functions are called when I make one database query. This makes it hard to see what's going on.</p>\n\n<p>Is there anyway I can \"roll up\" all function calls that are outside a certain directory?</p>\n\n<p>e.g. if I call a python function outside my directory, and it then calls 5 other functions (all outside my directory), then it should roll all those up, so it looks like there was only one function call, and it should show the cumulative time for the whole thing.</p>\n\n<p>This, obviously, is bad if you want to profile (say) Django, but I don't want to do that.</p>\n\n<p>I looked at the pstats.Stats object, but can't see an obvious way to modify this data.</p>\n"},{"tags":["c++","performance","gcc"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":213,"score":4,"question_id":13200858,"title":"C++: Why does this speed my code up?","body":"<p>I have the following function </p>\n\n<pre><code>double single_channel_add(int patch_top_left_row, int patch_top_left_col, \n        int image_hash_key, \n        Mat* preloaded_images,\n        int* random_values){\n\n    int first_pixel_row = patch_top_left_row + random_values[0];\n    int first_pixel_col = patch_top_left_col + random_values[1];\n    int second_pixel_row = patch_top_left_row + random_values[2];\n    int second_pixel_col = patch_top_left_col + random_values[3];\n\n    int channel = random_values[4];\n\n    Vec3b* first_pixel_bgr = preloaded_images[image_hash_key].ptr&lt;Vec3b&gt;(first_pixel_row, first_pixel_col);\n    Vec3b* second_pixel_bgr = preloaded_images[image_hash_key].ptr&lt;Vec3b&gt;(second_pixel_row, second_pixel_col);\n\n    return (*first_pixel_bgr)[channel] + (*second_pixel_bgr)[channel];\n}\n</code></pre>\n\n<p>Which is called about one and a half million times with different values for <code>patch_top_left_row</code> and <code>patch_top_left_col</code>. This takes about 2 seconds to run, now when I change the calculation of first_pixel_row etc to not use the arguments but hard coded numbers instead (shown below), the thing runs sub second and I don't know why. Is the compiler doing something smart here ( I am using gcc cross compiler)? </p>\n\n<pre><code>double single_channel_add(int patch_top_left_row, int patch_top_left_col, \n        int image_hash_key, \n        Mat* preloaded_images,\n        int* random_values){\n\n        int first_pixel_row = 5 + random_values[0];\n        int first_pixel_col = 6 + random_values[1];\n        int second_pixel_row = 8 + random_values[2];\n        int second_pixel_col = 10 + random_values[3];\n            int channel = random_values[4];\n\n    Vec3b* first_pixel_bgr = preloaded_images[image_hash_key].ptr&lt;Vec3b&gt;(first_pixel_row, first_pixel_col);\n    Vec3b* second_pixel_bgr = preloaded_images[image_hash_key].ptr&lt;Vec3b&gt;(second_pixel_row, second_pixel_col);\n\n    return (*first_pixel_bgr)[channel] + (*second_pixel_bgr)[channel];\n}\n</code></pre>\n\n<p>EDIT:</p>\n\n<p>I have pasted the assembly from the two versions of the function\nusing arguments: <a href=\"http://pastebin.com/tpCi8c0F\" rel=\"nofollow\">http://pastebin.com/tpCi8c0F</a>\nusing constants: <a href=\"http://pastebin.com/bV0d7QH7\" rel=\"nofollow\">http://pastebin.com/bV0d7QH7</a></p>\n\n<p>EDIT:</p>\n\n<p>After compiling with -O3 I get the following clock ticks and speeds:</p>\n\n<p>using arguments: 1990000 ticks and 1.99seconds \nusing constants: 330000 ticks and 0.33seconds </p>\n\n<p>EDIT: \nusing argumenst with -03 compilation: <a href=\"http://pastebin.com/fW2HCnHc\" rel=\"nofollow\">http://pastebin.com/fW2HCnHc</a>\nusing constant with -03 compilation: <a href=\"http://pastebin.com/FHs68Agi\" rel=\"nofollow\">http://pastebin.com/FHs68Agi</a></p>\n"},{"tags":["performance","task-parallel-library","sql-azure","async-await","federation"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":44,"score":1,"question_id":13163850,"title":"query many sql azure atomic units as fast as possible","body":"<p>I have a SQL Federation with 97 members, i.e. physical shards. Each member has 1-16 virtual shards, i.e. atomic units. This data tier powers a search lookup web service (on an Azure web role web server), which requires all atomic units to respond before it knows enough to answer the user response. </p>\n\n<p>Given the search parameters, the web server is able to determine the atomic unit IDs that it needs to query, but not their associated federation members (I am using USE Federation for this translation). The goal is to have the web server query all atomic units (wherever they are) as quickly as possible. </p>\n\n<p>Currently, the best solution I have for this works as follows:</p>\n\n<ol>\n<li>Generate list of needed atomic units.</li>\n<li>Generate USE Federation and SQL statement for each atomic unit. Currently, the best performance I have found specifies FILTERING = OFF in the USE Federation statement, while manually specifying the predicate for the atomic unit in the SQL statement (rather than relying on FILTERING = ON to add these predicates for me).</li>\n<li>For each atomic unit, open a SqlConnection to the Federation Root, execute the USE Federation statement, and then the SQL query, both asynchronously. I use the TPL Dataflow library and async/await to wait for all of the atomic unit queries to conclude, after which I apply (optional, depending on what web request it was) business logic to the results and send back the response.</li>\n</ol>\n\n<p>Each atomic unit in these queries will return between 100-600 records, never more than 2000. The design objective is to query at most 200 atomic units at once => so 400,000 records is the maximum amount that the web server will ever need to apply business logic to, although that logic is never more than \"get one object of each distinct numerical ID in the result,\" for which I have implemented IEqualityComparer.</p>\n\n<p>This approach does not seem to scale <em>that</em> well. Even when querying 30-40 atomic units, there is a marked increase in response time, even though I can test and see that individual atomic unit responses each take less than 1 second.</p>\n\n<p>I think likely places for my issues are:</p>\n\n<ol>\n<li>Using a separate SqlConnection for each atomic unit query => am I leveraging connection pooling effectively?</li>\n<li>Business logic and object serialization => should I be approaching the distinct operation across atomic units differently? What if I want to do sum operations on other fields as well (not the default use-case, but a common one).</li>\n</ol>\n\n<p>If anyone has a favorite way of handling this, I would love to hear about it. Thanks.</p>\n\n<p>Current solution:</p>\n\n<p><code></p>\n\n<pre><code>        #region Identify atomic units of search query from search parameters (static methods, no i/o, very fast)\n        List&lt;string&gt; AtomicUnits = AtomicUnitsOfSearch(data);\n        #endregion                       \n\n        #region Atomic unit-targeted subqueries setup\n        var atomicUnitQueries = AtomicUnits.Select(au =&gt; \n            {                    \n                return GetItemsFromAtomicUnit(\n                    CreateConstraintSQLQuery(data), \n                    au,\n                    verbosity);\n            });            \n        #endregion\n\n        #region Execute async query across relevant shards; Distinct result item summary query business logic\n        if (Verbosity.basic.Equals(verbosity)) // return one item per conceptid\n        {\n            return (await TaskEx.WhenAll(atomicUnitQueries)).SelectMany(a =&gt; a)                    \n                .Distinct(new SimpleItemComparer()).ToList(); // Distinct() the fan-out results based on ID\n        }\n        else // assume _count verbosity =&gt; pick one item per Id, sum Categories and Item counts across atomic units (do not double-count for synonyms)\n        { \n            // create the ending schema before executing the results lookup\n            List&lt;Item&gt; atomicResults = (await TaskEx.WhenAll(atomicUnitQueries)).SelectMany(a =&gt; a).ToList();\n            List&lt;Item&gt; distinctResults = new List&lt;Item&gt;();\n            foreach (IGrouping&lt;int, Item&gt; g in atomicResults.GroupBy(a =&gt; a.Id)) // gets the lists of equivalent items across all atomic units queried\n            {\n                Item f = g.First();\n                distinctResults.Add(new Item(f.Id, f.Name, g.Sum(a =&gt; a.CategoryCount), g.Sum(a =&gt; a.ItemCount)));\n            }\n            return distinctResults;                \n        }                        \n        #endregion\n</code></pre>\n\n<p></code></p>\n"},{"tags":["php","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":58,"score":1,"question_id":13195426,"title":"who's online with timestamp with high performance","body":"<p>i just created a user login system with php session and now users can register and login to site and do another things...</p>\n\n<p>now i want to create online.php which will fetch all online users.i almost search everything in google and stackoverflow for this with no success.</p>\n\n<p>ok now i want to describe the system which i want to create with high performance..\nwhen a user logged in we just update table <code>user</code>.<code>lastlogin</code> which is a timestamp and then in online.php we SELECT * every user where time interval is &lt; 5 minutes.\nfor this purpose i can update this timestamp <code>lastlogin</code> field in database when user load each page,and this cost many mysql query to do the job... then in each page load i have to update </p>\n\n<blockquote>\n  <p>UPDATE <code>user</code> set <code>last</code>=now()</p>\n</blockquote>\n\n<p>that will cost me many mysql query.now i am looking for some another way like using sessions or something that i found in <a href=\"http://stackoverflow.com/questions/6385880/how-to-handle-user-online-status-when-he-she-close-the-browser?answertab=votes#tab-top\">this link</a></p>\n\n<blockquote>\n  <p>\"The normal solution is to store a timestamp in the table which you update every time the user does something. Users with a recent timestamp (say, five minutes) are shown as logged in, everybody else are logged out.</p>\n  \n  <p>It doesn't even have to be updated on every page load. Keep a timestamp in the session with the last update time as well, and only update the table when the database flag are about to expire.\"</p>\n</blockquote>\n\n<p>but unfortunately the answer wasnt quite helpful and i need an example or more describe on this.</p>\n"},{"tags":["database","performance","full-text-search","sql-server-2012","sql-server-performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":30,"score":0,"question_id":13200498,"title":"What is the fastest approach to populate a SQL Server database with large amount of data","body":"<p><strong>Dilemma:</strong></p>\n\n<p>I am about to perform population of data on SQL Server 2012 Dev Edition. Data is based on production data. Amount is around 4TB. </p>\n\n<p><strong>Purpose:</strong></p>\n\n<p>To test performance on full text search and on regular index as well. Target number should be around 300 million items around 500K each. </p>\n\n<p><strong>Question:</strong> </p>\n\n<p>What should I do before to speed up the process or consequences that I should worry about?</p>\n\n<p>Ex. </p>\n\n<ol>\n<li>Switching off statistics? </li>\n<li>Should I do a bulk insert of 1k items per transaction instead of single transaction? </li>\n<li>Simple recovery model? </li>\n<li>Log truncation? </li>\n</ol>\n\n<p><strong>Important:</strong></p>\n\n<p>I will use sample of 2k of production items to create every random item that will be inserted into database. I will use near unique samples generated in c#. It will be one table:</p>\n\n<pre><code>table \n(\n    long[id], \n    nvarchar(50)[index], \n    nvarchar(50)[index], \n    int[index], \n    float, \n    nvarchar(50)[index], \n    text[full text search index]\n)\n</code></pre>\n"},{"tags":["xcode","performance","search","ide","apple"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":11,"score":0,"question_id":13200602,"title":"Sluggish XCode searches in project navigator and through code","body":"<p>This is not a coding question but I'm not sure which stack exchange this really belongs in.</p>\n\n<p>I'm experiencing very slow search speeds on the project navigator and code Search fields. Whenever I type into either fields, my key presses show about 1.5 characters per second or slower. I can tell that my input is being buffered and slowly being fed into the field. </p>\n\n<p>From what I can see, it's not even doing instant searching while I'm typing - at least not while I'm typing into the project navigator search bar. I know this because it won't return results until I hit enter. When I search through my code, it's similar but at least there I know there might be some slow downs due to instant search. However, this still seems extremely sluggish compared to other IDEs that I've used on Ubuntu and Windows. </p>\n\n<p>I'm spec'd with a Macbook from 2010 (2.4 GHz Intel Core 2 Duo w/ 4 GB 1067 MHz DDR3 RAM). The main drive is a 120 GB SSD with 22 GB free on it (I know I shouldn't go this full on an SSD, but I doubt that's why the search fields in xcode are super sluggish.</p>\n\n<p>Thanks all!</p>\n"},{"tags":["performance","c#-4.0"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":18,"score":0,"question_id":13200572,"title":"Restrict Specific Control - Not to Post to Server","body":"<p>I am working in C#.Net. I am having a Treeview in my page. In the page load, i am generating a XML Format and binding the XML to the treeview component. In the samepage, i am having a button, \"SHOW TREEVIEW\". Whenever i click the button i should show the tree view in a Panel using model popup extender. Everything is working fine..But the performance of the page goes down. Whenever any post back occurs, the whole form is posted to the  server, at that time all the XML Data (which i generated for treeview) are going to the server. Because of this the sending time, waiting time goes high.</p>\n\n<p>Whether we can do anything, so that particular treeview control can be stopped of post to server.?</p>\n\n<p>Code to bind XML Data to Treeview...</p>\n\n<pre><code>XmlDocument document = new XmlDocument();\n        document.LoadXml(sb.ToString());\n        ASTreeViewXMLDescriptor descripter1 = new ASTreeViewXMLDescriptor();\n        this.astvMyTreeProd.DataSourceDescriptor = descripter1;\n        this.astvMyTreeProd.DataSource = document;\n        this.astvMyTreeProd.DataBind();\n</code></pre>\n\n<p>Model Popup Extender Code part..</p>\n\n<pre><code>&lt;asp:UpdatePanel ID=\"UpdatePanel4\" runat=\"server\"&gt;\n                    &lt;ContentTemplate&gt;\n                        &lt;ajaxToolKit:ModalPopupExtender ID=\"mp1\" runat=\"server\" PopupControlID=\"Panel1\" TargetControlID=\"btnLoc\"\n                            CancelControlID=\"imgClose\"&gt;\n                        &lt;/ajaxToolKit:ModalPopupExtender&gt;\n\n                        &lt;asp:Button ID=\"btnLoc\" ToolTip=\"Location\" runat=\"server\" Text=\"Location\" CausesValidation=\"false\"\n                            BackColor=\"LightGray\" ForeColor=\"Black\" OnClientClick=\"return FindSelectedItems();\"\n                            Enabled=\"false\" /&gt;\n                        &lt;asp:Panel ID=\"Panel1\" runat=\"server\" Style=\"display: none; width: 600px; border: 2px solid;\"\n                            BackColor=\"#f0f0f0\"&gt;\n                            &lt;div style=\"position: fixed; width: 44.4%; border: 1px solid; text-align: center;\"&gt;\n                                &lt;br /&gt;\n                                &lt;asp:Label ID=\"Label4\" runat=\"server\" Text=\"LOCATION HIERARCHY\" Font-Bold=\"true\"\n                                    Font-Size=\"Small\" Style=\"text-align: center; vertical-align: middle\"&gt;&lt;/asp:Label&gt;\n                                &lt;asp:ImageButton ID=\"imgClose\" runat=\"server\" ImageUrl=\"~/img/close_icon.png\" align=\"right\"&gt;\n                                &lt;/asp:ImageButton&gt;\n                                &lt;br /&gt;\n                            &lt;/div&gt;\n                            &lt;div style=\"margin-top: 33px; height: 450px; overflow: auto;\"&gt;\n                                &lt;table&gt;\n                                    &lt;tr valign=\"top\"&gt;\n                                        &lt;td width=\"650\"&gt;\n                                            &lt;ct:ASTreeView ID=\"astvMyTree\"/&gt;\n                                        &lt;/td&gt;\n                                                                              &lt;/tr&gt;\n                                &lt;/table&gt;\n                            &lt;/div&gt;\n                        &lt;/asp:Panel&gt;\n                    &lt;/ContentTemplate&gt;\n                &lt;/asp:UpdatePanel&gt;\n</code></pre>\n"},{"tags":["performance","scala","collections","autoboxing","specialized-annotation"],"answer_count":2,"favorite_count":2,"up_vote_count":15,"down_vote_count":0,"view_count":1270,"score":15,"question_id":5477675,"title":"Why are so few things @specialized in Scala's standard library?","body":"<p>I've searched for the use of <code>@specialized</code> in the source code of the standard library of Scala 2.8.1. It looks like only a handful of traits and classes use this annotation: <code>Function0</code>, <code>Function1</code>, <code>Function2</code>, <code>Tuple1</code>, <code>Tuple2</code>, <code>Product1</code>, <code>Product2</code>, <code>AbstractFunction0</code>, <code>AbstractFunction1</code>, <code>AbstractFunction2</code>.</p>\n\n<p>None of the collection classes are <code>@specialized</code>. Why not? Would this generate too many classes?</p>\n\n<p>This means that using collection classes with primitive types is very inefficient, because there will be a lot of unnecessary boxing and unboxing going on.</p>\n\n<p>What's the most efficient way to have an immutable list or sequence (with <code>IndexedSeq</code> characteristics) of <code>Int</code>s, avoiding boxing and unboxing?</p>\n"},{"tags":["c","performance","bash","shell"],"answer_count":3,"favorite_count":1,"up_vote_count":7,"down_vote_count":1,"view_count":164,"score":6,"question_id":13088807,"title":"Shell script vs C performance","body":"<p>I was wondering how bad would be the impact in the performance of a program migrated to shell script from C. </p>\n\n<p>I have intensive I/O operations.</p>\n\n<p>For example, in C, I have a loop reading from a filesystem file and writing into another one. I'm taking parts of each line without any consistent relation. I'm doing this using pointers. A really simple program.</p>\n\n<p>In the Shell script, to move through a line, I'm using <code>${var:(char):(num_bytes)}</code>. After I finish processing each line I just concatenate it to another file.</p>\n\n<pre><code>\"$out\" &gt;&gt; \"$filename\"\n</code></pre>\n\n<p>The program does something like:</p>\n\n<pre><code>while read line; do\n    out=\"$out${line:10:16}.${line:45:2}\"\n    out=\"$out${line:106:61}\"\n    out=\"$out${line:189:3}\"\n    out=\"$out${line:215:15}\"\n    ...\n    echo \"$out\" &gt;&gt; \"outFileName\"\n\ndone &lt; \"$fileName\"\n</code></pre>\n\n<p>The problem is, C takes like half a minute to process a 400MB file and the shell script takes 15 minutes.</p>\n\n<p>I don't know if I'm doing something wrong or not using the right operator in the shell script.</p>\n\n<p>Edit: I cannot  use awk since there is not a pattern to process the line</p>\n\n<p>I tried commenting the \"echo $out\" >> \"$outFileName\" but it doesn't gets much better. I think the problem is the ${line:106:61} operation. Any suggestions? </p>\n\n<p>Thanks for your help.</p>\n"},{"tags":["linux","performance","popup","javafx","intel-atom"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":18,"score":0,"question_id":13193035,"title":"JavaFX Popup performance on Linux – Intel Atom","body":"<p><strong>Background/Context:</strong></p>\n\n<p>I am developing touch screen based kiosk application.\nHW: Intel Atom E640 CPU @1.0GHz, SDRAM size 1GB\nOS: Linux Mint 10 (Julia)</p>\n\n<p>I am using virtual/on-screen keyboard which is based on Popup class (the keyboard is added to Popup content) similar to this:</p>\n\n<p><a href=\"http://code.google.com/p/fx-onscreen-keyboard/\" rel=\"nofollow\">on-screen-keyboard</a></p>\n\n<p>however, once the keyboard is shown the app gets very slow, each click on a button or even move over button takes very long.</p>\n\n<p>Once the popup is made visible I got this in terminal:</p>\n\n<p><em>Can't create transparent stage, because your screen doesn't support alpha channel. You need to enable XComposite extension.</em></p>\n\n<p>However, in my xorg.conf I have:</p>\n\n<pre><code>Section \"Extensions\"    \n    Option  \"Composite\" \"enable\"    \nEndSection\n</code></pre>\n\n<p>It is not only my app with this problem I got exactly same message when running Ensamble.jar. The extra-info popup in the search box is problematic. </p>\n\n<p><strong>My questions:</strong></p>\n\n<p>How do I enable XComposite extension or manage to get popup perform relatively smoothly?</p>\n\n<p>In case I need to find another solution for virtual keyb. instead of popup, what that should be? I need possibility to click on keyboard´s buttons and still preserve focus on TextField or HTML input element in WebView.  If you were to tackle this problem, what would be your approach to that?</p>\n"},{"tags":["sql","mysql","performance","exists"],"answer_count":9,"favorite_count":12,"up_vote_count":36,"down_vote_count":0,"view_count":22833,"score":36,"question_id":1676551,"title":"Best way to test if a row exists in a MySQL table","body":"<p>I'm trying to find out if a row exists in a table. Using MySQL, is it better to do a query like this:</p>\n\n<pre><code>SELECT COUNT(*) AS total FROM table1 WHERE ...\n</code></pre>\n\n<p>and check to see if the total is non-zero or is it better to do a query like this:</p>\n\n<pre><code>SELECT * FROM table1 WHERE ... LIMIT 1\n</code></pre>\n\n<p>and check to see if any rows were returned?</p>\n\n<p>In both queries, the WHERE clause uses an index.</p>\n"},{"tags":["c++","performance","optimization","language-agnostic","branch-prediction"],"answer_count":8,"favorite_count":1600,"up_vote_count":3046,"down_vote_count":7,"view_count":156258,"score":3039,"question_id":11227809,"title":"Why is processing a sorted array faster than an unsorted array?","body":"<p>Here is a piece of C++ code that shows some very peculiar performance. For some strange reason, sorting the data miraculously speeds up the code by almost 6x:</p>\n\n<pre class=\"lang-cpp prettyprint-override\"><code>#include &lt;algorithm&gt;\n#include &lt;ctime&gt;\n#include &lt;iostream&gt;\n\nint main()\n{\n    // Generate data\n    const unsigned arraySize = 32768;\n    int data[arraySize];\n\n    for (unsigned c = 0; c &lt; arraySize; ++c)\n        data[c] = std::rand() % 256;\n\n    // !!! With this, the next loop runs faster\n    std::sort(data, data + arraySize);\n\n    // Test\n    clock_t start = clock();\n    long long sum = 0;\n\n    for (unsigned i = 0; i &lt; 100000; ++i)\n    {\n        // Primary loop\n        for (unsigned c = 0; c &lt; arraySize; ++c)\n        {\n            if (data[c] &gt;= 128)\n                sum += data[c];\n        }\n    }\n\n    double elapsedTime = static_cast&lt;double&gt;(clock() - start) / CLOCKS_PER_SEC;\n\n    std::cout &lt;&lt; elapsedTime &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"sum = \" &lt;&lt; sum &lt;&lt; std::endl;\n}\n</code></pre>\n\n<ul>\n<li>Without <code>std::sort(data, data + arraySize);</code>, the code runs in <strong>11.54</strong> seconds.</li>\n<li>With the sorted data, the code runs in <strong>1.93</strong> seconds.</li>\n</ul>\n\n<hr>\n\n<p>Initially I thought this might be just a language or compiler anomaly. So I tried it in Java:</p>\n\n<pre class=\"lang-java prettyprint-override\"><code>import java.util.Arrays;\nimport java.util.Random;\n\npublic class Main\n{\n    public static void main(String[] args)\n    {\n        // Generate data\n        int arraySize = 32768;\n        int data[] = new int[arraySize];\n\n        Random rnd = new Random(0);\n        for (int c = 0; c &lt; arraySize; ++c)\n            data[c] = rnd.nextInt() % 256;\n\n        // !!! With this, the next loop runs faster\n        Arrays.sort(data);\n\n        // Test\n        long start = System.nanoTime();\n        long sum = 0;\n\n        for (int i = 0; i &lt; 100000; ++i)\n        {\n            // Primary loop\n            for (int c = 0; c &lt; arraySize; ++c)\n            {\n                if (data[c] &gt;= 128)\n                    sum += data[c];\n            }\n        }\n\n        System.out.println((System.nanoTime() - start) / 1000000000.0);\n        System.out.println(\"sum = \" + sum);\n    }\n}\n</code></pre>\n\n<p>with a similar but less extreme result.</p>\n\n<hr>\n\n<p>My first thought was that sorting brings the data into cache, but my next thought was how silly that is because the array was just generated.</p>\n\n<p>What is going on? Why is a sorted array faster than an unsorted array? The code is summing up some independent terms, the order should not matter.</p>\n"},{"tags":["mysql","performance","innodb"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":13196407,"title":"Mysql innoDB write operations are extremely slow","body":"<p>I'm having serious performance problems with MySQL and the InnoDB engine. Even the simplest table makes writing operations (creating the table, inserting, updating and deleting) horribly slow, as you can see in the following snippet.</p>\n\n<pre><code>mysql&gt; CREATE TABLE `test` (`id` int(11) not null auto_increment,\n   -&gt; PRIMARY KEY(`id`)) ENGINE=InnoDB;\nQuery OK, 0 rows affected (4.61 sec)\n\nmysql&gt; insert into test values ();\nQuery OK, 1 row affected (1.92 sec)\n\nmysql&gt; insert into test values ();\nQuery OK, 1 row affected (0.88 sec)\n\nmysql&gt; insert into test values ();\nQuery OK, 1 row affected (1.10 sec)\n\nmysql&gt; insert into test values ();\nQuery OK, 1 row affected (6.27 sec)\n\nmysql&gt; select * from test;\n+----+\n| id |\n+----+\n|  1 |\n|  2 |\n|  3 |\n|  4 |\n+----+\n4 rows in set (0.00 sec)\n\nmysql&gt; delete from test where id = 2;\nQuery OK, 1 row affected (0.28 sec)\n\nmysql&gt; delete from test where id = 3;\nQuery OK, 1 row affected (6.37 sec)\n</code></pre>\n\n<p>I have been looking at htop and the long waiting times are not because of abnormal CPU load. It's almost zero, and memory usage is also normal. If I create the same table using the MyISAM engine, then it works normally. My my.cnf file contains this (if I remember right I haven't changed anything from the default Debian configuration):</p>\n\n<pre><code>[client]\nport        = 3306\nsocket      = /var/run/mysqld/mysqld.sock\n[mysqld_safe]\nsocket      = /var/run/mysqld/mysqld.sock\nnice        = 0\n\n[mysqld]\nuser        = mysql\npid-file    = /var/run/mysqld/mysqld.pid\nsocket      = /var/run/mysqld/mysqld.sock\nport        = 3306\nbasedir     = /usr\ndatadir     = /var/lib/mysql\ntmpdir      = /tmp\nlanguage    = /usr/share/mysql/english\nskip-external-locking\nbind-address        = 127.0.0.1\nkey_buffer      = 40M\nmax_allowed_packet  = 16M\nthread_stack        = 128K\nthread_cache_size       = 8\nmyisam-recover         = BACKUP\nmax_connections        = 100\ntable_cache            = 64\nthread_concurrency     = 10\nquery_cache_limit   = 1M\nquery_cache_size        = 40M\nlog_slow_queries    = /var/log/mysql/mysql-slow.log\nlong_query_time = 2\nlog-queries-not-using-indexes\nexpire_logs_days    = 10\nmax_binlog_size         = 100M\n\n[mysqldump]\nquick\nquote-names\nmax_allowed_packet  = 16M\n\n[isamchk]\nkey_buffer      = 16M\n!includedir /etc/mysql/conf.d/\n</code></pre>\n\n<p>I have also tried to restart the server, but it doesn't solve anything.</p>\n\n<p>The slow queries log doesn't give any extra information.</p>\n"},{"tags":["java","performance","simulation","statemachine"],"answer_count":3,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":95,"score":1,"question_id":13177347,"title":"Efficient state machine pattern in java","body":"<p>I am writing a java simulation application which has a lot of entities to simulate. Each of these entities has a certain state at any time in the system. A possible and natural approach to model such an entity would be using the <a href=\"http://en.wikipedia.org/wiki/State_pattern\" rel=\"nofollow\">state (or state machine)</a> pattern. The problem is that it creates a lot of objects during the runtime if there are a lot of state switches, what might cause bad system performance. What design alternatives do I have? I want performance to be the main criteria after maintainability.</p>\n\n<p>Thanks</p>\n"},{"tags":["asp.net","css","performance","sprite"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":60,"score":2,"question_id":13186033,"title":"Should I turn these images into a sprite, and if so, how would I do so? (picture included)","body":"<p>I have an asp.net (vb) website with a clients page. On that page, there are hyperlinked logos to some of our larger clients.  Each logo is identical in size (w 207 h 119).  There are 3 columns and 5 rows of images, so 15 logos in total.</p>\n\n<p>Currently, the images are coded like this -- but I'm thinking I need to remove the asp images and just use regular images:</p>\n\n<pre><code>&lt;td&gt;\n&lt;asp:HyperLink ID=\"HyperLink15\" runat=\"server\" ImageUrl=\"~/images/clgm.jpg\" \nNavigateUrl=\"http://www.gm.com\" Target=\"_blank\"    \nitemprop=\"url\"&gt;HyperLink&lt;/asp:HyperLink&gt;\n&lt;/td&gt;\n&lt;td&gt;\n&lt;asp:HyperLink ID=\"HyperLink16\" runat=\"server\" ImageUrl=\"~/images/clford.jpg\" \nNavigateUrl=\"http://www.ford.com\" Target=\"_blank\" \nitemprop=\"url\"&gt;HyperLink&lt;/asp:HyperLink&gt;\n&lt;/td&gt; \n</code></pre>\n\n<p>And so on, for all 15 clients.  Would it be better for speed and performance (by reducing http requests if I sprited these images into one image?  Assuming that's the case, should I <strong>change these asp:hyperlinks and asp:images to just regular html links and images</strong>?  And then how would I sprite them?  I'm not too good with css, so I'd truly appreciate any help anybody can offer in this regard.</p>\n\n<p>If needed, below is what they look like -- again, 3 columns and 5 rows (<strong>that's where I get even more confused</strong>):</p>\n\n<p><img src=\"http://i.stack.imgur.com/m93cd.jpg\" alt=\"enter image description here\"></p>\n"},{"tags":["hibernate","orm","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":536,"score":2,"question_id":4468150,"title":"Hibernate/EclipseLink? Which is best with Weblogic,Oracle DB?","body":"<p>In my company, we are using Oracle DB and Weblogic application server. So in the process to upgrade or switch to new ORM, we shortlisted two options - Hibernate and EclipseLink.</p>\n\n<p>I gathered following summary regarding both ORMs -</p>\n\n<p>Hibernate:</p>\n\n<ol>\n<li>When you need to train people, like we are going to do next week – most of the companies have Hibernate experts.</li>\n<li>When you hire new developers, most of them come with specific Hibernate experience.</li>\n<li>When you need to consult with experts, both in the internet or consultants, you have LOTS of options. Endless forums and communities all regarding Hibernate.</li>\n<li>Hibernate is an open source which has a huge community. This means that it will be improved all the time and will push the ORM market forward.</li>\n<li>Hibernate is an open source which means you have the code to handle, and in case needed, fit it to your needs.</li>\n<li>There are lots of plugins to Hibernate, such as validations tool, audit tools, etc. These becomes standard as well and dismiss you from impl. things yourself.</li>\n<li>One most important thing with ORM tool, is to configure it according to your application’s needs. Usually the default setting doesn’t fit to your needs.\nFor that sake, when the market has a huge experience with the tool’s configuration, and lots of experts (see point 1 and 3) – most of chances you will find similar cases and\nlots of knowledge about how to configure the tool and thus – your application.</li>\n</ol>\n\n<p>EclipseLink:</p>\n\n<ol>\n<li>Fully supported by Oracle. Hibernate no. In case of pb, it could be cumbersome to prove that it is a pure Weblogic one. Concretely, we will have to prove it (waste of time and complexity).</li>\n<li>Eclipse link is developed by Oracle and the preferred ORM in the Weblogic /Oracle DB world.</li>\n<li>Even if at a certain time EclipseLink was a bit late compared to Hibernate (feature), EclipseLink evolved very fast and we can consider now that they close the gap.</li>\n<li>No additional fee as soon as you have Weblogic license. You will need to pays additional fee if you want some professional support on Hibernate.</li>\n<li>We are currently relying on Hibernate for our legacy offer and are facing pb in second level cache (JGroups). Today, we are riding off this part!. Consequences are limitation in clustering approach (perf)</li>\n<li>On EclipseLink side we do succeed to manage first and second level cache in a clustering approach.</li>\n<li>Indeed Hibernate is open source, so you can imagine handling it. In reality, the code is so complex that it is nearly impossible to modify it. Moreover as it is LGPL, you need to feedback all the modified sources to the community systematically.</li>\n<li>All tests performed by Oracle concerning Weblogic are using EclipseLink. Moreover, Oracle says that some specific optimizations are done to manage Oracle DB.</li>\n<li>Hibernate comes from JBoss community.</li>\n</ol>\n\n<p>Right now we are preferring Hibernate but there are concerns/reasons like EclipseLink developed by Oracle and preferred ORM in Webogic/ Oracle DB world (compatibility of ORM with DB and App. server), support comparison with both ORM, which are preventing to finalize the decision.</p>\n\n<p>Please help me with you views and opinions and share you experience with us as which one is better and why so that we can make a perfect decision.</p>\n\n<p>If you want you can also reply to me @ yogesh.golande@gmail.com.</p>\n\n<p>Thanks.</p>\n"},{"tags":["performance","query","postgresql","index"],"answer_count":4,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":56,"score":3,"question_id":13183735,"title":"Query performance in PostgreSQL using 'similar to'","body":"<p>I need to retrieve certain rows from a table depending on certain values in a specific column, named <strong>columnX</strong> in the example:</p>\n\n<pre><code>select *\nfrom tableName \nwhere columnX similar to ('%A%|%B%|%C%|%1%|%2%|%3%')\n</code></pre>\n\n<p>So if <strong>columnX</strong> contains at least one of the values specified (A, B, C, 1, 2, 3), I will keep the row.  </p>\n\n<p>I can't find a better approach than using <strong>similar to</strong>.  The problem is that the query takes too long for a table with more than a million rows.</p>\n\n<p>I've tried indexing it:</p>\n\n<pre><code>create index tableName_columnX_idx on tableName (columnX) \nwhere columnX similar to ('%A%|%B%|%C%|%1%|%2%|%3%')\n</code></pre>\n\n<p>However, if the condition is variable (the values could be other than A, B, C, 1, 2, 3), I would need a different index for each condition.</p>\n\n<p>Is there any better solution for this problem?</p>\n\n<p>EDIT: Thanks everybody for the feedback.  Looks like I've achieved to this point maybe because of a design mistake (topic I've posted in a <a href=\"http://stackoverflow.com/questions/13194906/data-structure-design-for-database-replication-support\">separated question</a>).</p>\n"},{"tags":["java","python","performance","slowness"],"answer_count":1,"favorite_count":2,"up_vote_count":2,"down_vote_count":2,"view_count":63,"score":0,"question_id":13193871,"title":"using Python for a solution cannot be handled in JAVA because of slow execution","body":"<p>I have 3 millions line of data each has 30 features - it is hard to include all in memory for my computer and slow to process it with learning algorithm - . I want to write a little code that makes random sampling but in JAVA and with my PC configurations it does not work or takes so much times to execute. I know that writing in C or C++ gives better solution but I am also curious about the availability of python for such case. Is it reasonable to use Python in such a case that Java is not working efficiently because of slowness and memory restriction - please do not say to increase heap size or such-?</p>\n"},{"tags":["c#","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":8,"view_count":79,"score":-8,"question_id":13193024,"title":"using @ to locate file via string","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/5179389/at-sign-in-file-path-string\">@(at) sign in file path/string</a>  </p>\n</blockquote>\n\n\n\n<p>in C# for example this function you can use <code>@\"stringpath\"</code> instead of <code>\"stringpath\"</code></p>\n\n<p>Why should I add an @ there in front? I get the same results without using @??</p>\n\n<p>example:</p>\n\n<pre><code>UploadFileMethod(@\"C:\\test.txt\", @\"http://site.com/bla/file.txt\");\n\npublic static bool UploadFileToDocumentLibrary(string sourceFilePath, string targetDocumentLibraryPath)\n\n{\n//stuff here\n}\n</code></pre>\n"},{"tags":["javascript","performance","html5","google-chrome","web-worker"],"answer_count":0,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":34,"score":0,"question_id":13170828,"title":"Unresponsive Google Chrome while running HTML5 Web workers","body":"<p>I am using the following code to run webworker to fune prime numbers in a web page in the latest Google Chrome</p>\n\n<p><a href=\"https://dl.dropbox.com/u/655237/project/prime.html\" rel=\"nofollow\">https://dl.dropbox.com/u/655237/project/prime.html</a></p>\n\n<p>However after clicking the start Worker button,  the stop button becomes unresponsive for some time. After 30 second or so it becomes active when it is mouse hovered. This does not happen in Firefox.</p>\n\n<p>Is there any flaw in the code ?</p>\n\n<p><strong>prime.html</strong> </p>\n\n<pre><code>        &lt;section&gt;\n            Last Prime Found:-  &lt;p id=\"number\"&gt;NA&lt;/p&gt;\n            &lt;button id=\"prime\"&gt;Find &amp;nbsp; Prime&lt;/button&gt;\n            &lt;button id=\"primew\"&gt;Start &amp;nbsp;Worker&lt;/button&gt;\n            &lt;button id=\"primes\"&gt;Stop &amp;nbsp;Worker&lt;/button&gt;\n        &lt;/section&gt;\n        &lt;script&gt;\n            var worker;\n            document.querySelector('#prime').addEventListener('click', function () {\n                findPrime();\n            }, false);\n            document.querySelector('#primew').addEventListener('click', function () {\n                findPrimeW();\n            }, false);\n            document.querySelector('#primes').addEventListener('click', function () {\n                stopWorker();\n            }, false);\n\n            function findPrime(){\n                var n = 1;\n                search: while (true) {\n                    n += 1;\n                    for (var i = 2; i &lt;= Math.sqrt(n); i += 1)\n                        if (n % i == 0)\n                            continue search;\n                    // found a prime!\n                    document.querySelector(\"#number\").textContent=n;\n                }\n            }\n            function findPrimeW(){\n                worker = new Worker('js/worker1.js');\n                worker.onmessage = function (event) {\n                    document.querySelector(\"#number\").textContent = event.data;\n                };\n            }\n\n            function stopWorker()\n            { \n                worker.terminate();\n            }\n        &lt;/script&gt;\n</code></pre>\n\n<p><strong>worker1.js</strong></p>\n\n<pre><code>    var n = 1;\nsearch: while (true) {\n  n += 1;\n  for (var i = 2; i &lt;= Math.sqrt(n); i += 1)\n    if (n % i == 0)\n     continue search;\n  // found a prime!\n  postMessage(n);\n}\n</code></pre>\n\n<p>Another example is this\n<a href=\"https://dl.dropbox.com/u/655237/events/gdg-html5/index.html#/webworkers\" rel=\"nofollow\">https://dl.dropbox.com/u/655237/events/gdg-html5/index.html#/webworkers</a></p>\n\n<p>You can not change the slide for a some time after clicking \"Start Worker\" button. Ideally it should not happen as the heavy weight computation is delegated to a separate web worker.</p>\n"},{"tags":["c#",".net","performance","dynamic","compiler"],"answer_count":2,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":106,"score":4,"question_id":13193799,"title":"Performance cost of using dynamic typing in .NET","body":"<p>What is the performance cost of using <code>dynamic</code> vs <code>object</code> in .NET? Say for example I have a method which accepts a parameter of any type. E.G.</p>\n\n<pre><code>public void Foo(object obj)\n{\n}\n</code></pre>\n\n<p>or</p>\n\n<pre><code>public void Foo(dynamic obj)\n{\n}\n</code></pre>\n\n<p>ILSpy tells me that when using dynamic code, the compiler must insert a code block to handle dynamism. Therefore I want to know if using dynamic in place of object is advised and to what level this usage comes at the cost of performance?</p>\n"},{"tags":["python","search","list","performance"],"answer_count":4,"favorite_count":2,"up_vote_count":8,"down_vote_count":0,"view_count":2187,"score":8,"question_id":2701173,"title":"Most efficient way for a lookup/search in a huge list (python)","body":"<p>-- I just parsed a big file and I created a list containing 42.000 strings/words. I want to query [against this list] to check if a given word/string belongs to it. So my question is:</p>\n\n<p>What is the most efficient way for such a  lookup?</p>\n\n<p>A first approach is to sort the list (<code>list.sort()</code>) and then just use </p>\n\n<pre><code>&gt;&gt; if word in list: print 'word'\n</code></pre>\n\n<p>which is really trivial and I am sure there is a better way to do it. My goal is to apply a fast lookup that finds whether a given string is in this list or not.  If you have any ideas of another data structure, they are welcome. Yet, I want to avoid for now more sophisticated data-structures like Tries etc. I am interested in hearing ideas (or tricks) about fast lookups or any other python library methods that might do the search faster than the simple <code>in</code>. </p>\n\n<p>And also i want to know the index of the search item</p>\n"},{"tags":["c++","visual-studio-2010","performance","opengl","windows-8"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":141,"score":3,"question_id":13182563,"title":"OpenGL game runs fine in Win7, drops to 5fps in Windows 8?","body":"<p>I have just recently installed Windows 8, and I tried to compile and build a simple c++ game project in VS 2010, but when I did, it was running at 5 fps. On windows 7, it runs at a solid 60 fps. Nothing has been changed in the code, but there is just horrible slow down.</p>\n\n<p>I have updated my video drivers, but there is still horrible lag. I thought the problem was to do with compatibility issues with windows 8 and OpenGL, but I can't find anything to confirm this. I was wondering if anyone else has had this problem, and if you have solved it.</p>\n"},{"tags":["c++","c","performance","algorithm","io"],"answer_count":0,"favorite_count":0,"up_vote_count":3,"down_vote_count":9,"view_count":175,"score":-6,"question_id":13192537,"title":"fastest method to print output in c/c++","body":"<p>My program generates a lot of integers(about 300000) to be output on the screen. Each integer needs to be printed on a separate line. Traditional output methods using printf for each line or sprintf to a buffer and then printf the buffer take huge time. I want to print my result within 3 seconds.</p>\n\n<p>Please suggest some faster ways</p>\n"},{"tags":["c#",".net","performance","linq"],"answer_count":6,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":186,"score":2,"question_id":13166443,"title":"How to delete every 2nd character in a string?","body":"<p>How to delete every 2nd character in a string?</p>\n\n<p>For example:</p>\n\n<pre><code>3030313535333635  -&gt; 00155365\n3030303336313435  -&gt; 00036145\n3032323437353530  -&gt; 02247550\n</code></pre>\n\n<p>The strings are always 16-characters long and the result is always 8 characters long - and the character that is being removed is always a '3' - Don't ask why however - I did not dream up this crazy source data.</p>\n"},{"tags":["javascript","jquery","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":77,"score":1,"question_id":13192045,"title":"How to optimize javascript/jquery code to speed up it's performance?","body":"<p>On one of my web projects, I use a lot of javascript/jqeury code, which is pretty slow on browsers (Windows 7 x64), especially on IE. \nI use 3 Ajax requests at the same time only on Home page. \nOn Search page, I also use ajax requests, which are fired on scroll event, on any 'search tag' (simple anchor tag) click event and etc. which in general is making data loading very slow. </p>\n\n<p>I use jquery plugins such as, Anythingslider, jquery coockies plugin, Raty( rating plugin ), Tipsuy, jQuery coreUISelect, jScrollPane, mouse wheel and etc. All those 3rd party plugins i have minified and combinied in jquery.plugins.js file, which is almosw 80KB. </p>\n\n<p>I select a lot of DOM elements with jQuery. For example I use the folowing code:</p>\n\n<pre><code>$(\"#element\")\n</code></pre>\n\n<p>Instead of:</p>\n\n<pre><code>document.getElementById('element');\n</code></pre>\n\n<p>I also have one big CSS file, which is more than 5 000 lines, because I have combined all 3rd party jquery plugins's css files into one file, for caching and less HTTP requests.</p>\n\n<p>1) Well, I wonder, what can I do to optimize my code for better performance and speeding up web page load? </p>\n\n<p>2) What Kind of tools can I use to debug and my JS code? I forgot to mention that, when I refresh page in Google Chrome or Firefox with firebug or Chrome native developer tools opened, the page in that case loads also very slow. Sometimes the Firefox is even crushed. </p>\n\n<p>3) Will selecting of DOM elements with raw js give me a better and faster way to parse the document? Or should I leave, the jQuery selecting? Talk about is about 50 elements. </p>\n\n<p>4) Should I seperate and after that minify external plugins, such as Anythingslider? Or is it better when I have 'all in one' js file?</p>\n\n<p>5) Is it better to aslo seperate jquery plugins's css code from main style.css? Because even hovering on element and affecting the :hover state from css file, is pretty slow. </p>\n\n<p>Well guys, I'm really counting on you.\nI've been googling all night to find answers on my questions and really hope to find it here. \nThanks. </p>\n"},{"tags":["ruby-on-rails","performance","data-structures","volatile"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":45,"score":0,"question_id":12403706,"title":"Rails: A Volatile Data Structure To Store Temporary Data","body":"<p>I'm programming the backend of a mobile application and I've come across this problem, wondering whether I can use a rails tool or should I implement a new technology to my current system.</p>\n\n<p>We have our user that is able to make a request, demanding to chat anyone who is around. However our system (the backend) has to collect this data and choose one of users who agree to chat randomly. But for that I want to keep all the ones that agree to chat in a list and pick one element randomly. But I would like to implement this in a volatile way so that when someone random selected all the other candidates will be gone.</p>\n\n<p>Of course, those candidates could be easily stored in a table and later on could be deleted but I believe that there is a structure that I can use on demand and dump whenever I want. So what kind of data structure I should use to provide this efficiency?</p>\n"},{"tags":["c#","performance","design-patterns","design"],"answer_count":3,"favorite_count":2,"up_vote_count":8,"down_vote_count":0,"view_count":249,"score":8,"question_id":12000635,"title":"High Performance Development","body":"<p><strong>Background</strong></p>\n\n<p>We have been working very hard to try come up with solutions for a \"High Performance\" application. The application is basically a high throughput in-memory manager, with a sync back to disk. The \"reads\" and \"writes\" are tremendously high, around 3000 transactions a second. We try and do as much as possible in memory, but eventually the data gets stale and needs to be flushed to disk, and this is where a huge \"bottleneck\" ensues. The app is multi-threaded, with about 50 threads. There is no IPC (inter-process comms)</p>\n\n<p><strong>Attempts</strong></p>\n\n<p>We initially wrote this in Java, and it worked quite well, up until a certain load, the bottleneck was hit and it just couldn't keep up.\nThen we tried it in C#, and the same bottle-neck was reached.\nWe tried this with unmanaged code (C#), and though on initial tests was blindingly fast using MMF (Memory-map files), in production, reading was slow (are using Views).\nWe did try CouchBase, but we stumbled into problems surround high network utilization. This might be poor configuration on our part!</p>\n\n<p><strong>Extra Info:</strong> In our Java attempt (non-MMF), our thread with the Queue of information that needs to get flushed to disk builds to the extent of being unable to keep up \"writing\" to disk.\nIn our C# Memory-Map File Approach, the problems is that READS are very slow, and the WRITES working perfectly. For some reason, the Views are slow!</p>\n\n<p><strong>Question</strong></p>\n\n<p>So the question is, situations where you intend of transferring massive amounts of data; can someone please assist with a possible approach or architectural design that might be able to assist? I know this seems a bit broad, but I think the specific nature of high performance, high throughput should narrow down the answers.</p>\n\n<p>Can anyone vouch for using Couchbase, MongoDB or Cassandra at such a level? Other ideas or\nsolutions would be appreciated.</p>\n"},{"tags":["performance","postgresql","unique-constraint"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":34,"score":2,"question_id":13190682,"title":"How does unique constraint affect write performance in Postgres DB","body":"<p>Does the <code>UNIQUE</code> constraint specified on a column or group of columns affect the write performance of Postgres DB in any way? How does it internally function? </p>\n\n<p>I mean, does it perform unique checking at the time of insertion of a new record? If yes, how does it do that, does it do a linear search for a duplicate value already existing in the DB? In that case, it is deemed to affect the performance i.e. more the number of unique constraints worse would be the write/insert performance? Is it true? </p>\n"},{"tags":["java","performance","data-structures","queue"],"answer_count":4,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":2623,"score":2,"question_id":827831,"title":"What are the advantages of Blocking Queue in Java?","body":"<p>I am working on a project that uses a queue that keeps information about the messages that to be sent to remote hosts. In that case one thread is responsible to put the information into the queue and another thread is responsible for get the pop the information from the queue and send. In that case the 2nd thread needs to check the queue for the information periodically. </p>\n\n<p>But later I found that it is a \"Reinvention of Wheel\" :) I could use a blocking queue for this purpose.</p>\n\n<p>Now my question is What are the other advantages of using a blocking queue for above application? (Ex : Performance, Modifiable of the code, Any special tricks etc )</p>\n"},{"tags":["c#","multithreading","performance","parallel-processing","threadpool"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":938,"score":1,"question_id":349513,"title":"Using Parallel.For to test SQL queries and comparison with the ThreadPool","body":"<p>I'm looking for a way to easily load test and benchmark some of our SQL (using ADO.NET, nothing fancy using LINQ or PLINQ) that has to be performant when running under high parallel load.</p>\n\n<p>I've thought of using the new parallel extensions CTP and specifically <code>Parallel.For</code> / <code>Parallel.ForEach</code> to simply run the SQL over 10k iterations or so - but I've not been able to find any data on what these have been optimized for.</p>\n\n<p>Essentially I'm worried that because database access is inherently I/O bound, it won't create sufficient load. Does anyone know if Parallel. For is  intelligent enough to use > x threads (where x = # of CPUs) if the tasks it is executing are not totally CPU bound? I.e. does it behave in a similar manner to the managed thread pool?</p>\n\n<p>Would be rather cool if it was so!</p>\n\n<p><b>EDIT: As @CVertex has kindly referred to below, you can set the number of threads independently. Does anyone know if the parallel libraries by default are intelligent enough to keep adding threads if a job is I/O bound?</b></p>\n"},{"tags":["performance","hibernate","join","associations","one-to-many"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":28,"score":0,"question_id":13190307,"title":"Hibernate: LazyLoading - Search by Object in One-To-Many Collection","body":"<p>I got an Entity 1 -> SubEntity n One-To-Many-relation.</p>\n\n<p>I just would like to check, if this is the <em>most efficient</em> way <strong>to search for an Entity, including a specified SubEntity in its \"One-To-Many-Collection\"</strong>. It works, but do I have to join fetch the SubEntities or is there a <strong>more lightweight</strong> solution if I don't need all SubEntities to be loaded? (FetchMode = Lazy)</p>\n\n<pre><code>public Entity getEntityBySubEntity(SubEntity subEntity) { \n     List&lt;Entity&gt; result = (List&lt;Entity&gt;)getHibernateTemplate.findByNamedParam(\n         \"From Entity as e left join fetch e.subEntities as sub where sub.id = :id\",\"id\",subEntity.getId()); \n     if (!result.isEmpty()) { \n          return result.get(0);\n     } else {\n          throw new NoResultException();\n     }\n }\n</code></pre>\n\n<p>(by the way, there should always be just one result...)     </p>\n\n<p>thx in adv,</p>\n\n<p>cav              </p>\n"},{"tags":["sql","sql-server","performance","sql-server-2008"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":94,"score":0,"question_id":11724333,"title":"SQL Server - joining 4 fast queries gives me one slow query","body":"<p>I have 4 views in my MS Sql Server Database which are all quite fast (less than 2 seconds) and return all less than 50 rows.</p>\n\n<p>BUT when I create a query where I join those 4 views (left outer joins) I get a query which takes almost one minute to finish.</p>\n\n<p>I think the query optimizer is doing a bad job here, is there any way to speed this up. I am tempted to copy each of the 4 views into a table and join them together but this seems like too much of a workaround to me.</p>\n\n<p>(Sidenote: I can't set any indexes on any tables because the views come from a different database and I am not allowed to change anything there, so this is not an option)</p>\n\n<p><strong>EDIT:</strong> I am sorry, but I don't think posting the sql queries will help. They are quite complex and use around 50 different tables. I cannot post an execution plan either because I don't have enought access rights to generate an execution plan on some of the databases.</p>\n\n<p>I guess my best solution right now is to generate temporary tables to store the results of each query.</p>\n"},{"tags":["javascript","asp.net","performance","onload"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":23,"score":0,"question_id":13190552,"title":"How do I combine javascripts and load them after page load on an asp.net website?","body":"<p>I just read this <a href=\"http://stackoverflow.com/questions/3291681/combining-javascript-files-as-recommended-by-yslow-optimal-size\">SE Post</a> where it explains the question asked is similar to what I'm asking.  Yslow recommends combining all of your external javascripts, if possible. I've actually taken internal javascripts and made them into an external .js file and put it in my /scripts folder, per some recommendation I found on some website.  But this leads me to my 2 (similar) questions; I didn't think separate posts were needed.</p>\n\n<p><strong>(1)</strong> - How would I combine javascript codes?  Just leave a blank line and then copy and paste the other javascript after the other, and so on, until it's all in one file?</p>\n\n<p><strong>(2)</strong> - I have an asp.net website with 5 master pages.  4 of the 5 have 6 javascript files (all minified, some on my own using some compression tool that worked).  But I read on a SE post and online that you should load the script(s) <strong>after page load</strong>.  How do you do this on an asp.net website?  All my .js files are in the \n\n<p>@bluesmoon gave a nice, thorough answer, but I wasn't able to comprehend it:</p>\n\n<pre><code>To load a script after page load, do something like this:\n\n// This function should be attached to your onload handler\n// it assumes a variable named script_url exists.  You could easily\n// extend it to use an array of scripts or figure it out some other\n// way (see note late)\nfunction lazy_load() {\n    setTimeout(function() {\n            var s = document.createElement(\"script\");\n            s.src=script_url;\n            document.body.appendChild(s);\n        }, 50);\n}\n\nThis is called from onload, and sets a timeout for 50ms later at which point it will \nadd a new script node to the document's body. The script will start downloading after \nthat. Now since javascript is single threaded, the timeout will only fire after onload \nhas completed even if onload takes more than 50ms to complete.\n</code></pre>\n\n<p>I don't really understand what he's saying or how to go about implementing it.  So that's all ... I just need help in <strong>combining javascripts</strong> and loading them (or it) <strong>after page load</strong>; I don't see anything saying \"onload\" in my master pages or .aspx pages.  Thank you for any help or guidance anybody can offer me!</p>\n"},{"tags":["c","performance","sse","vectorization","avx"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":76,"score":2,"question_id":13183258,"title":"Union with __m256 and array of two __m128","body":"<p>Can I have a union like this</p>\n\n<pre><code>  union eight_floats_t\n  {\n    __m256 a;\n    __m128 b[2];\n  };\n  eight_floats_t eight_floats;\n</code></pre>\n\n<p>to have an instant access to the two 128 bit parts of a 256 bit register?</p>\n\n<p>Edit: I was asking to understand the performance impact of this approach.</p>\n"},{"tags":["mysql","performance","flask","load-testing"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":51,"score":1,"question_id":13181114,"title":"Web request performance really bad under stress","body":"<p>I wrote a web application using python and Flask framework, and set it up on Apache with mod_wsgi. \nToday I use JMeter to perform some load testing on this application. \nFor one web URL:</p>\n\n<ul>\n<li><p>when I set only 1 thread to send request, the response time is 200ms</p></li>\n<li><p>when I set 20 concurrent threads to send requests, the response time increases to more than 4000ms(4s). THIS IS UNACCEPTABLE!</p></li>\n</ul>\n\n<p>I am trying to find the problem, so I recorded the time in before_request and teardown_request methods of flask. And it turns out the time taken to process the request is just over 10ms.</p>\n\n<p>In this URL handler, the app just performs some SQL queries (about 10) in Mysql database, nothing special.</p>\n\n<p>To test if the problem is with web server or framework configuration, I wrote another method Hello in the same flask application, which just returns a string. It performs perfectly under load, the response time is 13ms with 20-thread concurrency.</p>\n\n<p>And when doing the load test, I execute 'top' on my server, there are about 10 apache threads, but the CPU is mostly idle.</p>\n\n<p>I am at my wit's end now. Even if the request are performed serially, the performance should not drop so drastically... My guess is that there is some queuing somewhere that I am unaware of, and there must be overhead besides handling the request.</p>\n\n<p>If you have experience in tuning performance of web applications, please help!</p>\n\n<p><em>EDIT</em></p>\n\n<p>About apache configuration, I used MPM worker mode, the configuration:</p>\n\n<pre><code>&lt;IfModule mpm_worker_module&gt;\n    StartServers          4\n    MinSpareThreads      25\n    MaxSpareThreads      75\n    ThreadLimit          64\n    ThreadsPerChild      50\n    MaxClients          200\n    MaxRequestsPerChild   0\n&lt;/IfModule&gt;\n</code></pre>\n\n<p>As for mod_wsgi, I tried turning WSGIDaemonProcess on and off (by commenting the following line out), the performance looks the same.</p>\n\n<pre><code># WSGIDaemonProcess tqt processes=3 threads=15 display-name=TQTSERVER\n</code></pre>\n"},{"tags":["performance","r","data.frame"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":91,"score":0,"question_id":13122882,"title":"a faster implementation of merge.data.frame() in R","body":"<p>Let's say <code>a</code> and <code>b</code> are two data frames. The goal is to write a function\n<code>f(a,b)</code> that produces a merged data frame, in the same way as merge\n<code>merge(a,b,all=TRUE)</code> would do, that is filling missing variables in <code>a</code> or <code>b</code> with NAs. (The problem is <code>merge()</code> appears to be very slow.)</p>\n\n<p>This can be done as follows (pseudo-code):</p>\n\n<pre><code>for each variable `var` found in either `a` or `b`, do:\n    unlist(list(a.srcvar, b.srcvar), recursive=FALSE, use.names=FALSE)\n\nwhere:\nx.srcvar is x$var if x$var exists, or else\n            rep(NA, nrow(x)) if y$var !is.factor, or else\n            as.factor(rep(NA, nrow(x)))\n</code></pre>\n\n<p>and then wrap everything in a data frame.</p>\n\n<p>Here's a \"naive\" implementation:</p>\n\n<pre><code>merge.datasets1 &lt;- function(a, b) {\n  a.fill &lt;- rep(NA, nrow(a))\n  b.fill &lt;- rep(NA, nrow(b))\n  a.fill.factor &lt;- as.factor(a.fill)\n  b.fill.factor &lt;- as.factor(b.fill)\n  out &lt;- list()\n  for (v in union(names(a), names(b))) {\n    if (!v %in% names(a)) {\n      b.srcvar &lt;- b[[v]]\n      if (is.factor(b.srcvar))\n        a.srcvar &lt;- a.fill.factor\n      else\n        a.srcvar &lt;- a.fill\n    } else {\n      a.srcvar &lt;- a[[v]]\n      if (v %in% names(b))\n        b.srcvar &lt;- b[[v]]\n      else if (is.factor(a.srcvar))\n        b.srcvar &lt;- b.fill.factor\n      else\n        b.srcvar &lt;- b.fill\n    }\n    out[[v]] &lt;- unlist(list(a.srcvar, b.srcvar),\n                       recursive=FALSE, use.names=FALSE)\n  }\n  data.frame(out)\n}\n</code></pre>\n\n<p>Here's a different implementation that uses \"vectorized\" functions:</p>\n\n<pre><code>merge.datasets2 &lt;- function(a, b) {\n  srcvar &lt;- within(list(var=union(names(a), names(b))), {\n    a.exists &lt;- var %in% names(a)\n    b.exists &lt;- var %in% names(b)\n    a.isfactor &lt;- unlist(lapply(var, function(v) is.factor(a[[v]])))\n    b.isfactor &lt;- unlist(lapply(var, function(v) is.factor(b[[v]])))\n    a &lt;- ifelse(a.exists, var, ifelse(b.isfactor, 'fill.factor', 'fill'))\n    b &lt;- ifelse(b.exists, var, ifelse(a.isfactor, 'fill.factor', 'fill'))\n  })\n  a &lt;- within(a, {\n    fill &lt;- NA\n    fill.factor &lt;- factor(fill)\n  })\n  b &lt;- within(b, {\n    fill &lt;- NA\n    fill.factor &lt;- factor(fill)\n  })\n  out &lt;- mapply(function(x,y) unlist(list(a[[x]], b[[y]]),\n                                     recursive=FALSE, use.names=FALSE),\n                srcvar$a, srcvar$b, SIMPLIFY=FALSE, USE.NAMES=FALSE)\n  out &lt;- data.frame(out)\n  names(out) &lt;- srcvar$var\n  out\n}\n</code></pre>\n\n<p>Now we can test:</p>\n\n<pre><code>sample.datasets &lt;- lapply(1:50, function(i) iris[,sample(names(iris), 4)])\n\nsystem.time(invisible(Reduce(merge.datasets1, sample.datasets)))\n&gt;&gt;   user  system elapsed \n&gt;&gt;  0.192   0.000   0.190 \nsystem.time(invisible(Reduce(merge.datasets2, sample.datasets)))\n&gt;&gt;   user  system elapsed \n&gt;&gt;  2.292   0.000   2.293 \n</code></pre>\n\n<p>So, the naive version is orders of magnitude faster than the other. How can\nthis be? I always thought that <code>for</code> loops are slow, and that one should\nrather use <code>lapply</code> and friends and steer clear of loops in R. I would welcome any idea on how to improve my function in terms of speed.</p>\n"},{"tags":["performance","solr","lucene"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":1,"view_count":271,"score":1,"question_id":9162290,"title":"Lucene performance impact of returning large result sets","body":"<p>Does anyone know the performance impact of letting Lucene (or Solr) return <em>very</em> long result sets instead of just the usual \"top 10\". \nWe would like to return all results (which can be around 100.000 documents) from a user search  and then post-process the returned document ids before returning the actual result.</p>\n\n<p>Our current index contains about 10-20 million documents.</p>\n"},{"tags":["sql","performance","indexing","db2"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":54,"score":0,"question_id":13148822,"title":"How should I improve my query performance?","body":"<p>I have created a table with a couple of columns.</p>\n\n<p>MyTable </p>\n\n<ul>\n<li>Column1 -> RecordId (int)</li>\n<li>Column2 -> Title (varchar)</li>\n<li>Column3 -> Shortname (varchar)</li>\n<li>Column4 ->  (varchar) </li>\n<li><p>Column5 -> varchar</p>\n\n<p>...</p>\n\n<p>...</p></li>\n<li>Column10 -> varchar</li>\n</ul>\n\n<p>I am not joining this table with any other tables. I have 4 rows of data in the table.</p>\n\n<p>When I am querying for title, shortname for a particular RecordId, the query seems to be too slow. It nearly takes 7 seconds to load the page. </p>\n\n<p>I haven't used any indexing on my tables. Can you please suggest how do I improve my table/query performance? </p>\n\n<p>I am making db connection from my jsp in order to display the query results in the listbox. </p>\n\n<p>My code looks something like this- \nconn= DataObjectConnectionPool.getInstance().getConnection();\nprepStmt = conn.prepStmt(\"select title,shortname from MyTable where RecordId=1\");</p>\n"},{"tags":["php","database","performance","messaging","dbms"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":38,"score":1,"question_id":13187476,"title":"PHP-MYSQL messaging system performance problems","body":"<p>I want to create a messaging module for the social network web-site project (PHP &amp; MySQL) I've been working on. What substructures do well-known sites use for modules like this?</p>\n\n<p>I think, generating a new record on MySQL for each of new messages will slow down the system and cause problems after a while. What would you suggest me for saving the datas? SQLLite, file system or a high-performance dbms ? (exp: postgresql)</p>\n\n<p>Thanks</p>\n"},{"tags":["performance","memcached","redis"],"answer_count":6,"favorite_count":62,"up_vote_count":84,"down_vote_count":0,"view_count":30206,"score":84,"question_id":2873249,"title":"Is memcached a dinosaur in comparison to Redis?","body":"<p>I have worked quite a bit with memcached the last weeks and just found out about Redis. When I read this part of their readme, I suddenly got a warm, cozy feeling in my stomach:</p>\n\n<blockquote>\n  <p>Redis can be used as a memcached on steroids because is as fast as\n  memcached but with a number of\n  features more.\n  Like memcached, Redis also supports setting timeouts to keys so\n  that this key will be automatically\n  removed when a given amount of time\n  passes.</p>\n</blockquote>\n\n<p>This sounds amazing. I'd also found this page with benchmarks: <a href=\"http://www.ruturaj.net/redis-memcached-tokyo-tyrant-mysql-comparison\" rel=\"nofollow\">http://www.ruturaj.net/redis-memcached-tokyo-tyrant-mysql-comparison</a></p>\n\n<p>So, honestly - Is memcache really that old dinousaur that is a bad choice from a performance perspective when compared to this newcomer called Redis?</p>\n\n<p>I haven't heard lot about Redis previously, thereby the approach for my question!</p>\n"},{"tags":["mysql","performance","activerecord","ruby-on-rails-3.2","mysql2"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":62,"score":2,"question_id":12349318,"title":"rails activerecord, mysql and mysql2 performance degradation?","body":"<p>I have only recently upgraded from Rails 2.3.5 to Rails 3.2.7 and noticed some performance degradation in some of my queries. I am aware of Rails 3 ActiveRecord being slower in some cases than Rails 2.3.5, but the benchmarks I have surprised me and I just want to make sure I am not missing anything.</p>\n\n<p>I ran the following query, which is very popular in my application, as a benchmark</p>\n\n<pre><code>SELECT SQL_NO_CACHE table_name.* FROM table_name WHERE ((string_col = 'value') AND (int_col1 BETWEEN 5 AND 30)) ORDER BY int_col2 DESC LIMIT 1000\n</code></pre>\n\n<p>I checked:</p>\n\n<ul>\n<li>rails 3.2.7 vs. rails 2.3.5</li>\n<li>rails 3.2.7 with mysql adapter vs mysql2</li>\n<li>ActiveRecord.find_by_sql vs ActiveRecord.connection.select_all</li>\n</ul>\n\n<h1>Results</h1>\n\n<h3>Rails 3.2.7</h3>\n\n<p>rails 3.2.7, mysql adapter, \"select_all\": avg. 0.0148 seconds<br/>\nrails 3.2.7, mysql adapter, \"find_by_sql\" avg. 0.0555 seconds</p>\n\n<p>rails 3.2.7, mysql2 adapter, \"select_all\": avg. 0.045 seconds<br/>\nrails 3.2.7, mysql2 adapter, \"find_by_sql\" avg. 0.088 seconds</p>\n\n<h3>Rails 2.3.5</h3>\n\n<p>rails 2.3.5, mysql adapter \"select_all\": avg. 0.013 seconds<br/>\nrails 2.3.5, mysql adapter \"find_by_sql\": avg. 0.0177 seconds</p>\n\n<p>Although my original code is using ActiveRecord query api, I used hardcoded sql for the benchmark and also verified that calling mysql directly from the bash command line is stable and the above numbers result from rails/mysql adapter and not the db.</p>\n\n<h1>Question</h1>\n\n<p>Are these differences reasonable?<br/>\nThe diff between \"find_by_sql\" and \"select_all\" is much bigger in Rails 3.2.7 than in Rails 2.3.5.<br/>\nAnd why is mysql2 slower than mysql?<br/></p>\n"},{"tags":["php","performance"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":32,"score":2,"question_id":13187273,"title":"Minimal memory usage per call?","body":"<p>I just noticed - by calling memory_get_peak_usage() on an 'empty' php file, using php-fastcgi and NGINX, the result is ~120KB of memory</p>\n\n<pre><code>&lt;?php\n\n    print_r(memory_get_peak_usage());\n\n?&gt;\n</code></pre>\n\n<p>Does PHP really need that 'much' memory for every call, or does that only happen for the first call (initializing something I guess) and then every consecutive call needs less Memory?</p>\n\n<p>I'm asking, because I'm kind of surprised that an empty file already uses up 140KB - guessing that a couple of classes, functions and arrays will push that number up quite fast.</p>\n\n<p>And yeah, I know that this probably counts as premature optimization, but I'm really curious about knowing where those 120KB are coming from, and if there's a way to minify that cost per call.</p>\n"},{"tags":["mysql","performance","mongodb"],"answer_count":3,"favorite_count":3,"up_vote_count":5,"down_vote_count":0,"view_count":3717,"score":5,"question_id":9702643,"title":"Mysql vs Mongodb 1000 reads","body":"<p>I have been very excited about MongoDb and have been testing it lately. I had a table called posts in MySql with about 20million records indexed only on a field called 'id'.</p>\n\n<p>I wanted to compare speed with MongoDb and I ran a test which would get and print 15 records randomly from our huge databases. I ran the query about 1,000 times each for mysql and mongodb and I am surpised that I do not notice a lot of difference in speed. Maybe Monogodb is 1.1 times faster. Thats very disappointing. Is there something I am doing wrong? I know that my tests are not perfect but is MySQL on par with MongoDb when it comes to read intensive chores. </p>\n\n<p>Note:</p>\n\n<ul>\n<li>I have dual core  + ( 2 threads ) i7 cpu and 4GB ram</li>\n<li>I have 20  partitions on mysql each of 1 million records</li>\n</ul>\n\n<p><strong>Sample Code Used For Testing MongoDB</strong></p>\n\n<pre><code>&lt;?php\nfunction microtime_float()\n{\n    list($usec, $sec) = explode(\" \", microtime());\n    return ((float)$usec + (float)$sec);\n}\n$time_taken = 0;\n$tries = 100;\n// connect\n$time_start = microtime_float();\n\nfor($i=1;$i&lt;=$tries;$i++)\n{\n    $m = new Mongo();\n    $db = $m-&gt;swalif;\n    $cursor = $db-&gt;posts-&gt;find(array('id' =&gt; array('$in' =&gt; get_15_random_numbers())));\n    foreach ($cursor as $obj)\n    {\n        //echo $obj[\"thread_title\"] . \"&lt;br&gt;&lt;Br&gt;\";\n    }\n}\n\n$time_end = microtime_float();\n$time_taken = $time_taken + ($time_end - $time_start);\necho $time_taken;\n\nfunction get_15_random_numbers()\n{\n    $numbers = array();\n    for($i=1;$i&lt;=15;$i++)\n    {\n        $numbers[] = mt_rand(1, 20000000) ;\n\n    }\n    return $numbers;\n}\n\n?&gt;\n</code></pre>\n\n<p><strong>Sample Code For Testing MYSQL</strong></p>\n\n<pre><code>&lt;?php\nfunction microtime_float()\n{\n    list($usec, $sec) = explode(\" \", microtime());\n    return ((float)$usec + (float)$sec);\n}\n$BASE_PATH = \"../src/\";\ninclude_once($BASE_PATH  . \"classes/forumdb.php\");\n\n$time_taken = 0;\n$tries = 100;\n$time_start = microtime_float();\nfor($i=1;$i&lt;=$tries;$i++)\n{\n    $db = new AQLDatabase();\n    $sql = \"select * from posts_really_big where id in (\".implode(',',get_15_random_numbers()).\")\";\n    $result = $db-&gt;executeSQL($sql);\n    while ($row = mysql_fetch_array($result) )\n    {\n        //echo $row[\"thread_title\"] . \"&lt;br&gt;&lt;Br&gt;\";\n    }\n}\n$time_end = microtime_float();\n$time_taken = $time_taken + ($time_end - $time_start);\necho $time_taken;\n\nfunction get_15_random_numbers()\n{\n    $numbers = array();\n    for($i=1;$i&lt;=15;$i++)\n    {\n        $numbers[] = mt_rand(1, 20000000);\n\n    }\n    return $numbers;\n}\n?&gt;\n</code></pre>\n\n<p>Thanking you\nImran</p>\n"},{"tags":["performance","iis","powershell","monitoring"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":21,"score":0,"question_id":13184697,"title":"IIS 7 request duration monitoring","body":"<p>i am curious if there is a way of monitoring the request duration time on an iis server. Personally I have came up with a solution but it's really resource intensive and that is why i'm asking the question, just to gather more opinions. </p>\n\n<p>My plan is to extract the duration time of each request and send it to graphite so as to have a real time overview of the performance of the webserver. The idea i've came up with is to use poweshell with its webadministration module. And if you run  <code>get-item IIS:\\AppPools\\DefaultAppPool | Get-WebRequest</code> for example you get all the requests on that app pool with a lot of info including the time info.</p>\n\n<p>The thing is that i should have a script which runs every 100 ms to get all requests and that is kinda wasteful. Is there a way to tell iis to put the request duration time(in miliseconds) in the logs? Because then it would be much easier to get the information I need.</p>\n"},{"tags":["php","arrays","performance","warnings"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":13184704,"title":"Calling isset() vs. returning undefined index","body":"<p>I have code that is used very extensively which fetches an array from another method, and sometimes returns the first element of that array. Given that <code>null</code> is an acceptable return value for the function, is it worth the performance overhead of calling <code>isset()</code> on the array index (or checking the array length, etc), or is it better to just return the non-existant index (warnings aside). What are the advantages of calling <code>isset()</code> aside from preventing the warning.</p>\n\n<p>The example below is simplified, the real function doesn't <em>just</em> get the first element of the array.</p>\n\n<p>Return index which may not exist:</p>\n\n<pre><code>function get_array_element(){\n    $array = get_array();       // function that returns array\n    return $array[0];           // return index 0 which may not exist\n}\n</code></pre>\n\n<p>Versus checking if index is set:</p>\n\n<pre><code>function get_array_element(){\n    $array = get_array();       // function that returns array\n    return (isset($array[0]))?  // check if index 0 isset() else return null\n        $array[0] : \n        null; \n}\n</code></pre>\n"},{"tags":["java","android","performance","hibernate"],"answer_count":3,"favorite_count":11,"up_vote_count":19,"down_vote_count":0,"view_count":8857,"score":19,"question_id":4257374,"title":"Is Hibernate an overkill for an Android application?","body":"<p>I'm looking for a good ORM for my android application and at first glance it seems like for a mobile device I would prefer to use something simpler maybe. The thing is I'm just assuming here with no real evidence, so I thought I would ask the community's opinion (maybe there's is someone that has been through the experience). It is a fairly large(for mobile) application and will be run on a dedicated tablet.</p>\n\n<p>What does everyone else think ? Is Hibernate too much for an android application ? Will there be performance problems ?</p>\n\n<p>What would you use instead if you think it is too much ?</p>\n\n<p>I am aware there are other questions asking for alternatives, but I decided to ask since most of those questions simply assumed it was an overkill and asked for other options and I started wondering \"Is it really and overkill ? Why ?\" Due to my lack of experience I simply think it it, but can't really provide an answer if I'm asked to explain why. Is it performance ? Is it too much configuration (Which I don't mind) ? </p>\n\n<p>Thanks!</p>\n"},{"tags":["php","performance","benchmarking","zend-framework2"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":2,"view_count":52,"score":-2,"question_id":13182088,"title":"possible ways to increase performance in ZF2 applications?","body":"<p>i read a lot benchmark weblog posts about ZF2 that says zf2 has a low performance for response time because of huge functions load and class loads ...</p>\n\n<p>so i need a list of possible ways to increase zf2 performance</p>\n\n<p>thanks</p>\n"},{"tags":["java","multithreading","performance"],"answer_count":6,"favorite_count":0,"up_vote_count":3,"down_vote_count":1,"view_count":1381,"score":2,"question_id":5538847,"title":"Multi threaded game - updating, rendering, and how to split them","body":"<p>So, I'm working on a game engine, and I've made pretty good progress. However, my engine is single-threaded, and the advantages of splitting updating and rendering into separate threads sounds like a very good idea.</p>\n\n<p>How should I do this? Single threaded game engines are (conceptually) very easy to make, you have a loop where you update -> render -> sleep -> repeat. However, I can't think of a good way to break updating and rendering apart, especially if I change their update rates (say I go through the update loop 25x a second, and have 60fps for rendering) - what if I begin updating halfway through a render loop, or vice versa?</p>\n"},{"tags":["c#","multithreading","performance","c#-4.0","parallel-processing"],"answer_count":2,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":84,"score":4,"question_id":13176529,"title":"Parallel GZip Decompression of Log Files - Tweaking MaxDegreeOfParallelism for the Highest Throughput","body":"<p>We have up to 30 GB of GZipped log files per day. Each file holds 100.000 lines and is between 6 and 8 MB when compressed. The simplified code in which the parsing logic has been stripped out, utilises the Parallel.ForEach loop. </p>\n\n<p>The maximum number of lines processed peaks at MaxDegreeOfParallelism of 8 on the two-NUMA node, 32 logical CPU box (Intel Xeon E7-2820 @ 2 GHz):</p>\n\n<pre class=\"lang-cs prettyprint-override\"><code>using System;\n\nusing System.Collections.Concurrent;\n\nusing System.Linq;\nusing System.IO;\nusing System.IO.Compression;\n\nusing System.Threading.Tasks;\n\nnamespace ParallelLineCount\n{\n    public class ScriptMain\n    {\n        static void Main(String[] args)\n        {\n            int    maxMaxDOP      = (args.Length &gt; 0) ? Convert.ToInt16(args[0]) : 2;\n            string fileLocation   = (args.Length &gt; 1) ? args[1] : \"C:\\\\Temp\\\\SomeFiles\" ;\n            string filePattern    = (args.Length &gt; 1) ? args[2] : \"*2012-10-30.*.gz\";\n            string fileNamePrefix = (args.Length &gt; 1) ? args[3] : \"LineCounts\";\n\n            Console.WriteLine(\"Start:                 {0}\", DateTime.UtcNow.ToString(\"yyyy-MM-ddTHH:mm:ss.fffffffZ\"));\n            Console.WriteLine(\"Processing file(s):    {0}\", filePattern);\n            Console.WriteLine(\"Max MaxDOP to be used: {0}\", maxMaxDOP.ToString());\n            Console.WriteLine(\"\");\n\n            Console.WriteLine(\"MaxDOP,FilesProcessed,ProcessingTime[ms],BytesProcessed,LinesRead,SomeBookLines,LinesPer[ms],BytesPer[ms]\");\n\n            for (int maxDOP = 1; maxDOP &lt;= maxMaxDOP; maxDOP++)\n            {\n\n                // Construct ConcurrentStacks for resulting strings and counters\n                ConcurrentStack&lt;Int64&gt; TotalLines = new ConcurrentStack&lt;Int64&gt;();\n                ConcurrentStack&lt;Int64&gt; TotalSomeBookLines = new ConcurrentStack&lt;Int64&gt;();\n                ConcurrentStack&lt;Int64&gt; TotalLength = new ConcurrentStack&lt;Int64&gt;();\n                ConcurrentStack&lt;int&gt;   TotalFiles = new ConcurrentStack&lt;int&gt;();\n\n                DateTime FullStartTime = DateTime.Now;\n\n                string[] files = System.IO.Directory.GetFiles(fileLocation, filePattern);\n\n                var options = new ParallelOptions() { MaxDegreeOfParallelism = maxDOP };\n\n                //  Method signature: Parallel.ForEach(IEnumerable&lt;TSource&gt; source, Action&lt;TSource&gt; body)\n                Parallel.ForEach(files, options, currentFile =&gt;\n                    {\n                        string filename = System.IO.Path.GetFileName(currentFile);\n                        DateTime fileStartTime = DateTime.Now;\n\n                        using (FileStream inFile = File.Open(fileLocation + \"\\\\\" + filename, FileMode.Open))\n                        {\n                            Int64 lines = 0, someBookLines = 0, length = 0;\n                            String line = \"\";\n\n                            using (var reader = new StreamReader(new GZipStream(inFile, CompressionMode.Decompress)))\n                            {\n                                while (!reader.EndOfStream)\n                                {\n                                    line = reader.ReadLine();\n                                    lines++; // total lines\n                                    length += line.Length;  // total line length\n\n                                    if (line.Contains(\"book\")) someBookLines++; // some special lines that need to be parsed later\n                                }\n\n                                TotalLines.Push(lines); TotalSomeBookLines.Push(someBookLines); TotalLength.Push(length);\n                                TotalFiles.Push(1); // silly way to count processed files :)\n                            }\n                        }\n                    }\n                );\n\n                TimeSpan runningTime = DateTime.Now - FullStartTime;\n\n                // Console.WriteLine(\"MaxDOP,FilesProcessed,ProcessingTime[ms],BytesProcessed,LinesRead,SomeBookLines,LinesPer[ms],BytesPer[ms]\");\n                Console.WriteLine(\"{0},{1},{2},{3},{4},{5},{6},{7}\",\n                    maxDOP.ToString(),\n                    TotalFiles.Sum().ToString(),\n                    Convert.ToInt32(runningTime.TotalMilliseconds).ToString(),\n                    TotalLength.Sum().ToString(),\n                    TotalLines.Sum(),\n                    TotalSomeBookLines.Sum().ToString(),\n                    Convert.ToInt64(TotalLines.Sum() / runningTime.TotalMilliseconds).ToString(),\n                    Convert.ToInt64(TotalLength.Sum() / runningTime.TotalMilliseconds).ToString());\n\n            }\n            Console.WriteLine();\n            Console.WriteLine(\"Finish:                \" + DateTime.UtcNow.ToString(\"yyyy-MM-ddTHH:mm:ss.fffffffZ\"));\n        }\n    }\n}\n</code></pre>\n\n<p>Here's a summary of the results, with a clear peak at MaxDegreeOfParallelism = 8:</p>\n\n<p><img src=\"http://i.stack.imgur.com/VvFz1.png\" alt=\"enter image description here\"></p>\n\n<p>The CPU load (shown aggregated here, most of the load was on a single NUMA node, even when DOP was in 20 to 30 range):</p>\n\n<p><img src=\"http://i.stack.imgur.com/hZoUW.png\" alt=\"enter image description here\"></p>\n\n<p>The only way I've found to make CPU load cross 95% mark was to split the files across 4 different folders and execute the same command 4 times, each one targeting a subset of all files.</p>\n\n<p>Can someone find a bottleneck?</p>\n"},{"tags":["windows","performance","winapi","delay","clock"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":27,"score":1,"question_id":13183350,"title":"So is Sleep() and this the same?","body":"<p>So, are these two the same?\nUsing delay CPU usage is crazy in task manager.. is this the same case as the System Idle Process?</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;time.h&gt;\n//#include &lt;windows.h&gt;\n\nint delay(long int time)\n{\n    clock_t beginning = clock();\n    while(clock() - beginning &lt; time) {}\n    return 0;\n}\n\nint main()\n{\n    clock_t beginning = clock();\n\n    begin:\n\n    std::cout &lt;&lt; \"delay this by 1000ms\\n\";\n\n    //Sleep(1000);\n    delay(1000);\n\n    goto begin; //i know, i know\n\n    return 0;\n}\n</code></pre>\n"},{"tags":["php","performance","numbers","generate"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":30,"score":1,"question_id":13183025,"title":"Fastest way to generate and check numbers in PHP?","body":"<p>I need some help with generating numbers in PHP then checking for example if that number is below a hundred give 0.1 points. This is for a coins system that will be called when certain actions in a flash game are used.</p>\n\n<p>I want to do this in the fastest time possible and here is the code i'm using to test it before i update it to the main server for the game.</p>\n\n<pre><code>&lt;?php\n$coins = 0;\n$rand = rand(0, 1000);\n\nif($rand &lt; 50){\n$coins = $coins + 0.01;\n}\n\nif($rand &lt; 100 &amp;&amp; $rand &gt; 50){\n$coins = $coins + 0.2;\n}\n\nif($rand &gt; 100 &amp;&amp; $rand &lt; 200){\n$coins = $coins + 0.2;\n}\n\nif($rand &gt; 500){\n$coins = $coins + 0.5;\n}\n\necho \"Generated $coins coins \\n\";\n?&gt;\n</code></pre>\n\n<p>Sorry if anyone does not understand my quest in a few words i'm asking how could i make this faster and does anyone have any tips for it?</p>\n"},{"tags":["sql","sql-server","performance","application","architecture"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":54,"score":0,"question_id":13173984,"title":"Is having a copy of SQL data in an application a good idea to save SQL SELECTS?","body":"<p>I am working on a multithreading .NET 4 application which acquires data continuously and writes them into a SQL database (MySQL or SQL Server - not yet sure).</p>\n\n<p>Everytime when a <code>INSERT</code> is executed, at leat one prior <code>SELECT</code> is necessary in order so synchronize with the databaes. This means the applications gets a block which contains new and old data and then has to check which data sets are new and which are already in the database.</p>\n\n<p>This means a lot of <code>SELECTS</code> which result everytime in more or less the same data.</p>\n\n<p>Would it be a good idea to have a copy of the last x entries per table within the application?\nThis way the synchronization could be done on the copy instead of the database.</p>\n\n<p>Pro: </p>\n\n<ul>\n<li>Faster</li>\n</ul>\n\n<p>Contra:</p>\n\n<ul>\n<li>Uses a lot of memory</li>\n<li>Risk of becomming unsynchronized with the database</li>\n</ul>\n\n<p>What do you think? What is the best practice for such a use case?\nAny other pros and cons?</p>\n"},{"tags":["sql","mysql","performance","types"],"answer_count":8,"favorite_count":2,"up_vote_count":19,"down_vote_count":0,"view_count":5486,"score":19,"question_id":1962310,"title":"Importance of varchar length in MySQL table","body":"<p>I have a MySQL table where rows are inserted dynamically.  Because I can not be certain of the length of strings and do not want them cut off, I make them varchar(200) which is generally much bigger than I need.  Is there a big performance hit in giving a varchar field much more length than necessary?</p>\n"},{"tags":["mysql","performance","caching"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":33,"score":1,"question_id":13181954,"title":"query cache mysql","body":"<p>I would like to use query cache on MySQL, but my doubt is if I update or insert data on already cached query, will the cache reflect it?</p>\n\n<p>Will it return only the cached query or the updated query?</p>\n"},{"tags":["c++","performance","visual-c++","opengl","drawing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":3,"view_count":49,"score":-3,"question_id":13176514,"title":"slow redrawing how to draw with better performance","body":"<p>I am trying to draw a recursive tree with OpenGL but when I change the position of the camera, redrawing is very slow. I try to use <code>glGenLists</code> but the drawing is still very slow.</p>\n\n<pre><code>void glTree(void)\n{\n    tree = glGenLists (1);\n    if (tree != 0) \n    {   \n        glNewList(tree, GL_COMPILE);\n        drawTree(5);\n        glEndList();\n    }\n}\n\n\nvoid Dibuja (void)\n{\n    glClear( GL_COLOR_BUFFER_BIT );\n    glMatrixMode(GL_MODELVIEW);\n    glLoadIdentity();\n    glRotated (beta, 1.0,0.0,0.0);\n    glRotated (-alfa, 0.0,1.0,0.0);\n    glCallList(treeLeaf);\n    glFlush();\n}\n\n\n\nvoid drawTree( int level){\n\n   GLUquadric* quadricobj=gluNewQuadric();\n\n\n   //dibuja tronco\n\n    glPushMatrix();\n    if(level&gt;1){\n        glColor3b(92,64,23);\n\n    }\n    else{\n        glColor3f(0,1,0);\n    }\n    glRotated(-90,1,0,0);\n    gluCylinder(quadricobj, 0.4, 0.2, 5.0, 8, 4);\n    glPopMatrix();\n\n//dibuja ramas\nif (level&lt;=0) return;\n\n\n\n    glPushMatrix();\n    glTranslated(0,1,0);\n    glRotated(40,0,0,1);\n   glScaled(0.5,0.5,0.5);\n    drawTree(level - 1);\n    glPopMatrix();\n\n    glPushMatrix();\n    glTranslated(0,2,0);\n    glRotated(30,1,0,0);\n    glScaled(0.5,0.5,0.5);\n    drawTree(level  - 1);\n    glPopMatrix();\n\n    glPushMatrix();\n    glTranslated(0,3,0);\n    glRotated(-25,0,0,1);\n    glScaled(0.5,0.5,0.5);\n    drawTree( level - 1);\n    glPopMatrix();\n\n    glPushMatrix();\n    glTranslated(0,4,0);\n    glRotated(-20,1,0,0);\n    glScaled(0.5,0.5,0.5);\n    drawTree( level - 1);\n    glPopMatrix();\n\n    glPushMatrix();\n    glTranslated(0,5,0);\n    glScaled(0.5,0.5,0.5);\n    drawTree(level - 1);\n    glPopMatrix();\n\n\n\n }\n\nvoid openwindow(int numeroArgumentos, char ** listaArgumentos)\n\n    {\n        glutInit(&amp;numeroArgumentos, listaArgumentos);\n        glutInitDisplayMode (GLUT_SINGLE | GLUT_RGB);\n        glutInitWindowSize (VentanaAncho, VentanaAlto);\n        glutInitWindowPosition (VentanaX, VentanaY);\n        glutCreateWindow (listaArgumentos[0]);\n        glutDisplayFunc (Dibuja);\n        glutReshapeFunc (TamanyoVentana);\n        glClearColor (0.0f, 0.0f, 0.0f, 0.0f); \n        glClear (GL_COLOR_BUFFER_BIT); \n        glColor3f (1.0f, 1.0f, 1.0f); \n    }\nint main(int numArgumentos, char ** listaArgumentos)\n{   \n\n    openwindow(numArgumentos, listaArgumentos);\n        startFuntionCallback ();\n    startDisplayLists ();\n    glutMainLoop();\n\n    return (0);\n}\n</code></pre>\n"},{"tags":["php","mysql","search","performance"],"answer_count":3,"favorite_count":6,"up_vote_count":3,"down_vote_count":0,"view_count":1112,"score":3,"question_id":2954022,"title":"MySQL/PHP Search Efficiency","body":"<p>I'm trying to create a small search for my site. I've tried using full-text index search, but I could never get it to work. Here is what I've come up with:</p>\n\n<pre><code>if(isset($_GET['search'])) {\n\n$search = str_replace('-', ' ', $_GET['search']);\n$result = array();\n\n$titles = mysql_query(\"SELECT title FROM Entries WHERE title LIKE '%$search%'\");\nwhile($row = mysql_fetch_assoc($titles)) {\n    $result[] = $row['title'];\n}\n\n$tags = mysql_query(\"SELECT title FROM Entries WHERE tags LIKE '%$search%'\");\nwhile($row = mysql_fetch_assoc($tags)) {\n    $result[] = $row['title'];\n}\n\n$text = mysql_query(\"SELECT title FROM Entries WHERE entry LIKE '%$search%'\");\nwhile($row = mysql_fetch_assoc($text)) {\n    $result[] = $row['title'];\n}\n\n$result = array_unique($result);\n}\n</code></pre>\n\n<p>So basically, it searches through all the titles, body-text, and tags of all the entries in the DB. This works decently well, but I'm just wondering how efficient would it be? This would only be for a small blog, too. Either way I'm just wondering if this could be made any more efficient.</p>\n"},{"tags":["c++","performance","qt","foreach","for-loop"],"answer_count":10,"favorite_count":1,"up_vote_count":8,"down_vote_count":0,"view_count":15238,"score":8,"question_id":771008,"title":"'for' loop vs 'foreach' in C++","body":"<p>Which is better (or faster), a C++ <code>for</code> loop or the <code>foreach</code> operator provided by Qt? For example, the following condition</p>\n\n<pre><code>QList&lt;QString&gt; listofstrings;\n</code></pre>\n\n<p>Which is better?</p>\n\n<pre><code>foreach(QString str, listofstrings)\n{\n    //code\n}\n</code></pre>\n\n<p>or</p>\n\n<pre><code>int count = listofstrings.count();\nQString str = QString();\nfor(int i=0;i&lt;count;i++)\n{\n    str = listofstrings.at(i);\n    //Code\n}\n</code></pre>\n"},{"tags":["php","performance","architecture"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":3,"view_count":34,"score":-1,"question_id":13180000,"title":"whole process at a single php file","body":"<p>I'm doing the whole process at a single php file. For example; crud operation, writing forms,post/get data. Is that wrong? Or that causing performance loss?</p>\n"},{"tags":["arrays","programming-languages","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":835,"score":1,"question_id":5029575,"title":"Best programming language for very large arrays and very large numbers?","body":"<p>What would be the best programming language for very large arrays and very large numbers?</p>\n\n<ul>\n<li>With arrays over 30,000 indexes</li>\n<li>And numbers over 100 digits</li>\n</ul>\n\n<p>Also it needs to be efficient, or easy to make efficient. </p>\n\n<p>Thanks. </p>\n"},{"tags":["c#","string","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":66,"score":0,"question_id":13179554,"title":"Is it faster to access char in string via [] operator or faster to access char in char[] via [] operator?","body":"<p>I have a simple question. I'm working with a gigantic <code>string</code> in C# and I'm repeatedly going through it and accessing individual characters via the <code>[]</code> operator. Is it faster to turn the <code>string</code> into a <code>char[]</code> and use the <code>[]</code> operator on the <code>char</code> array or is my approach faster? Or are they both the same? I want my program to be bleeding edge fast.</p>\n"},{"tags":["c++","performance","time","timestamp","epoch"],"answer_count":4,"favorite_count":3,"up_vote_count":2,"down_vote_count":0,"view_count":91,"score":2,"question_id":13175573,"title":"why is microsecond timestamp is repetetive using (a private) gettimeoftheday() i.e. epoch","body":"<p>I am printing microseconds continuously using gettimeofday(). As given in program output you can see that the time is not updated microsecond interval rather its repetitive for certain samples then increments not in microseconds but in milliseconds. </p>\n\n<pre><code>while(1)\n{\n  gettimeofday(&amp;capture_time, NULL);\n  printf(\".%ld\\n\", capture_time.tv_usec);\n}\n</code></pre>\n\n<p>Program output:</p>\n\n<pre><code>.414719\n.414719\n.414719\n.414719\n.430344\n.430344\n.430344\n.430344\n\n e.t.c\n</code></pre>\n\n<p>I want the output to increment sequentially like,</p>\n\n<pre><code>.414719\n.414720\n.414721\n.414722\n.414723\n</code></pre>\n\n<p>or </p>\n\n<pre><code>.414723, .414723+x, .414723+2x, .414723 +3x + ...+ .414723+nx\n</code></pre>\n\n<p>It seems that microseconds are not refreshed when I acquire it from capture_time.tv_usec.</p>\n\n<p>=================================\n//Full Program</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;windows.h&gt;\n#include &lt;conio.h&gt;\n#include &lt;time.h&gt;\n#include &lt;stdio.h&gt;\n\n#if defined(_MSC_VER) || defined(_MSC_EXTENSIONS)\n  #define DELTA_EPOCH_IN_MICROSECS  11644473600000000Ui64\n#else\n  #define DELTA_EPOCH_IN_MICROSECS  11644473600000000ULL\n#endif\n\nstruct timezone \n{\n  int  tz_minuteswest; /* minutes W of Greenwich */\n  int  tz_dsttime;     /* type of dst correction */\n};\n\ntimeval capture_time;  // structure\n\nint gettimeofday(struct timeval *tv, struct timezone *tz)\n{\n  FILETIME ft;\n  unsigned __int64 tmpres = 0;\n  static int tzflag;\n\n  if (NULL != tv)\n  {\n    GetSystemTimeAsFileTime(&amp;ft);\n\n    tmpres |= ft.dwHighDateTime;\n    tmpres &lt;&lt;= 32;\n    tmpres |= ft.dwLowDateTime;\n\n    /*converting file time to unix epoch*/\n    tmpres -= DELTA_EPOCH_IN_MICROSECS; \n    tmpres /= 10;  /*convert into microseconds*/\n    tv-&gt;tv_sec = (long)(tmpres / 1000000UL);\n    tv-&gt;tv_usec = (long)(tmpres % 1000000UL);\n  }\n\n  if (NULL != tz)\n  {\n    if (!tzflag)\n    {\n      _tzset();\n      tzflag++;\n    }\n\n    tz-&gt;tz_minuteswest = _timezone / 60;\n    tz-&gt;tz_dsttime = _daylight;\n  }\n\n  return 0;\n}\n\nint main()\n{\n   while(1)\n  {     \n    gettimeofday(&amp;capture_time, NULL);     \n    printf(\".%ld\\n\", capture_time.tv_usec);// JUST PRINTING MICROSECONDS    \n   }    \n}\n</code></pre>\n"},{"tags":["performance","physics","gameloop"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":17,"score":0,"question_id":13178649,"title":"Efficient Game Loop - Singly-Threaded / Fixed Time Step With Remainder","body":"<p>(For this question, assume I already have functionality that lets me yield to the OS in such a way that I'm negligibly-likely to encounter hitching.)</p>\n\n<p>My criteria for a simple game loop are this:</p>\n\n<ul>\n<li>Singly-threaded (hopefully multiply-threaded in the future, but let's set that aside)</li>\n<li>For physics calculations, time steps do not need to be fixed but they need to be within a range</li>\n<li>CPU power should be preserved by taking maximal time steps when possible</li>\n<li>The renderer using interpolation between physics states is undesirable for my situation</li>\n</ul>\n\n<p>This is the loop I've come up with, in pseudocode, and I'd like to see if anyone can see any glaring flaws in my plan.</p>\n\n<pre><code>max_time_step = (largest time step that lets physics run comfortably)\nmin_time_step = (smallest time step that lets physics run comfortably)\nfixed_time_step = max_time_step - min_time_step\n\ncontinual_loop\n{\n    process_input\n\n    frame_time_remaining = get_time_elapsed_since_last_render_frame\n\n    if frame_time_remaining &lt; min_time_step\n        yield\n        restart_loop\n\n    while frame_time_remaining &gt; max_time_step\n        step ( fixed_time_step )\n        frame_time_remaining -= fixed_time_step\n\n    step ( frame_time_remaining )\n}\n</code></pre>\n\n<p>What I hope to accomplish with this are:</p>\n\n<ul>\n<li>Processing input before checking minimum frame time hopefully will allow us to skip the yield step any time there is input (since checking input takes time)</li>\n<li>The minimum number of physics updates are performed per render frame while preserving the physics calculations' integrity</li>\n<li>Rendering always uses exact determined positions and never interpolates, while still maintaining maximum framerate</li>\n<li>If the game is blocked for multiple seconds, minimum slowdown results from large amounts of physics calculation to catch up</li>\n<li>If the game is running too fast (i.e., without vsync/fps_limit on), will not often stutter and hitch while waiting for sufficient time to pass</li>\n<li>Is maximally-synchronized with user input, instead of trying to use downtime to predict next physics step without input</li>\n</ul>\n"},{"tags":["java","performance","optimization","virtual-machine","final"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":85,"score":4,"question_id":13173046,"title":"static final field, static field and performances","body":"<p>Even thought it's not its main purpose, I've always thought that the <code>final</code> keyword (in some situations and VM implementations) could help the JIT. <br/>\nIt might be an urban legend but I've never imagined that setting a field <code>final</code> could negatively affect the performances. \n<br/><br/>Until I ran into some code like that:</p>\n\n<pre class=\"lang-java prettyprint-override\"><code>   private static final int THRESHOLD = 10_000_000;\n   private static int [] myArray = new int [THRESHOLD];\n\n   public static void main(String... args) {\n      final long begin = System.currentTimeMillis();\n\n      //Playing with myArray\n      int index1,index2;\n      for(index1 = THRESHOLD - 1; index1 &gt; 1; index1--)\n          myArray[index1] = 42;             //Array initial data\n      for(index1 = THRESHOLD - 1; index1 &gt; 1; index1--) {\n                                            //Filling the array\n          for(index2 = index1 &lt;&lt; 1; index2 &lt; THRESHOLD; index2 += index1)\n              myArray[index2] += 32;\n      }\n\n      long result = 0;\n      for(index1 = THRESHOLD - 1; index1 &gt; 1; index1-=100)\n          result += myArray[index1];\n\n      //Stop playing, let's see how long it took\n      System.out.println(result);\n      System.out.println((System.currentTimeMillis())-begin+\"ms\");\n   }\n</code></pre>\n\n<p><br/></p>\n\n<p>Let's have a look at:\n<code>private static int [] myArray = new int [THRESHOLD];</code><br/>\nUnder W7 64-bit and on a basis of 10 successive runs, I get the following results:<br/></p>\n\n<ol>\n<li><p><code>THRESHOLD = 10^7</code>, 1.7.0u09 client VM (Oracle):<br/></p>\n\n<ul>\n<li>runs in ~2133ms when <code>myArray</code> is not final.</li>\n<li>runs in ~2287ms when <code>myArray</code> is final.</li>\n<li>The -server VM produces similar figures i.e 2131ms and 2284ms.\n<br/><br/></li>\n</ul></li>\n<li><p><code>THRESHOLD = 3x10^7</code>, 1.7.0u09 client VM (Oracle):<br/></p>\n\n<ul>\n<li>runs in ~7647ms when <code>myArray</code> is not final.</li>\n<li>runs in ~8190ms when <code>myArray</code> is final.</li>\n<li>The -server VM produces ~7653ms and ~8150ms.\n<br/><br/></li>\n</ul></li>\n<li><p><code>THRESHOLD = 3x10^7</code>, 1.7.0u01 client VM (Oracle):<br/></p>\n\n<ul>\n<li>runs in ~8166ms  when <code>myArray</code> is not final.</li>\n<li>runs in ~9694ms when <code>myArray</code> is final. That's more than 15% difference !</li>\n<li>The -server VM produces a neglectable difference in favour of the non-final version, about 1%.\n<br/><br/></li>\n</ul></li>\n</ol>\n\n<p><em>Remark: I used the bytecode produced by JDK 1.7.0u09's javac for all my tests. The bytecode produced is exactly the same for both versions except for <code>myArray</code> declaration, that was expected.</em><br/><br/>\n<b>So why is the version with a <code>static final myArray</code> slower than the one with <code>static myArray</code> ?</b><br/></p>\n\n<hr>\n\n<p><strong>EDIT (using Aubin's version of my snippet):</strong> </p>\n\n<p>It appears that the differences between the version with <code>final</code> keyword and the one without only lies in the first iteration. Somehow, the version with <code>final</code> is always slower than its counterpart without on the first iteration, then next iterations have similar timings.</p>\n\n<p>For example, with <code>THRESHOLD = 10^8</code> and running with 1.7.0u09 client the first computation takes approx 35s while the second 'only' takes 30s.</p>\n\n<p>Obviously the VM performed an optimization, was that the JIT in action and why didn't it kick earlier (for example by compiling the second level of the nested loop, this part was the hotspot) ?<br></p>\n\n<p><strong>Note that my remarks are still valid with 1.7.0u01 client VM. With that very version</strong> (and maybe earlier releases), <strong>the code with <code>final myArray</code> runs slower than the one without this keyword: 2671ms vs 2331ms on a basis of 200 iterations.</strong></p>\n"},{"tags":["mysql","performance","sphinx"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":13170509,"title":"Sphinx get millions of results","body":"<p>I use <strong>MySQL</strong> + <strong>Sphinx</strong> to store many millions rows of data. We have website to view all information from our database. </p>\n\n<p>For example, movie titles (<strong>100,000,000</strong> rows). I need to view ALL of them on our website, 100 titles per page. Also, i need view them sorted by actors popularity. </p>\n\n<p>For the first 10 pages all works great. But after that i reached <a href=\"http://sphinxsearch.com/docs/1.10/conf-max-matches.html\" rel=\"nofollow\">max_matches</a> limit. Increasing this limit will force sphinx to use more CPU/RAM.</p>\n\n<p>Also, i can't even set max_matches to <strong>20,000,000</strong>.</p>\n\n<blockquote>\n  <p>WARNING: max_matches=20000000 out of bounds; using default 1000</p>\n</blockquote>\n\n<p>I can use MySQL to perform queries like this:</p>\n\n<pre><code>SELECT * FROM titles WHERE tid &gt;= $start AND tid &lt;= $end\n</code></pre>\n\n<p>to use <strong>tid</strong> index. But i can't sort it by <strong>tid</strong>. I need to sort my titles by info from other tables.</p>\n\n<p>What is the best way to get access to many millions of rows, sorted and do it quick. Please help.</p>\n\n<p><strong>UPDATE</strong>: from sphinx source: <strong>/src/searchd.cpp</strong></p>\n\n<pre><code>if ( iMax&lt;0 || iMax&gt;10000000 )\n{\n    sphWarning ( \"max_matches=%d out of bounds; using default 1000\", iMax );\n} else\n{\n    g_iMaxMatches = iMax;\n}\n</code></pre>\n\n<p>Is <strong>10000000</strong> is a limit ? How can i get offset more than that ?</p>\n"},{"tags":["c#","winforms","performance","image"],"answer_count":4,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":89,"score":4,"question_id":13176192,"title":"What image format can c# load fastest?","body":"<p>this may seem like an odd question, and I don't know the formats of images so I'll just go ahead and ask...</p>\n\n<p>I'm making a minesweeper game (relevant to different things too) which utilizes a grid of buttons, then I'm adding a sprite to the buttons using <code>backgroundImage</code>. If the grid is 9x9 it's fine. 15x15 it slows down and 30x30 you can visibly see each button being loaded. </p>\n\n<p>That raises me to my question: Which image format would load fastest? Obviously, file size takes a part in the loading speed, however, I am enquiring as to if, say, a jpeg - with the same filesize as a gif - will load faster. or a bmp, png, etc.</p>\n\n<p>I'm asking this as to see if I can get my grid to load faster using a different format.</p>\n\n<p>Thanks!</p>\n"},{"tags":["php","mysql","performance","benchmarking"],"answer_count":5,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":79,"score":4,"question_id":13176469,"title":"PHP vs MySQL Performance ( if , functions ) in query","body":"<p>I just see this <a href=\"http://www.onextrapixel.com/2010/06/23/mysql-has-functions-part-5-php-vs-mysql-performance/\" rel=\"nofollow\">artice</a></p>\n\n<p>i need to know what's is best berformance in this cases</p>\n\n<p>if statment in query</p>\n\n<p><code>SELECT *,if( status = 1 , \"active\" ,\"unactive\") as status_val FROM comments</code></p>\n\n<p><strong>VS</strong></p>\n\n<pre><code>&lt;?php\n    $x = mysql_query(\"SELECT * FROM comments\");\n\n     while( $res = mysql_fetch_assoc( $x ) ){\n       if( $x['status'] == 1 ){\n             $status_val = 'active';\n        }else{\n             $status_val = 'unactive';\n        }\n     }\n  ?&gt;\n</code></pre>\n\n<p>Cut 10 from string</p>\n\n<p><code>SELECT * , SUBSTR(comment, 0, 10) as min_comment FROM comments</code></p>\n\n<p><strong>VS</strong></p>\n\n<pre><code>&lt;?php\n    $x = mysql_query(\"SELECT * FROM comments\");\n\n     while( $res = mysql_fetch_assoc( $x ) ){\n       $min_comment = substr( $x['comment'],0,10 ) ;\n     }\n  ?&gt;\n</code></pre>\n\n<p>etc ?????   <strong>and</strong> When i use MYSQL functions or PHP functions ?</p>\n"}]}
{"total":25592,"page":4,"pagesize":100,"questions":[{"tags":["ruby-on-rails-3","performance","redis","ohm"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":12730716,"title":"redis ohm slow performance","body":"<p>We're scratching our heads the last couple of days trying to understand why some things seem a bit too slow with redis/ohm on our rails project.</p>\n\n<p>The gist of it is that some requests to ohm/redis take around 100-200 ms, which seem rather high considering our high-expectations of blazing-fast performance.</p>\n\n<p>The command we're running looks like:</p>\n\n<p><code>Stats::TermStats.find(term_slug: 'term_slug', user_id: 123).to_a</code></p>\n\n<p>It seems to us that we're not doing anything extremely complex or crazy. Our profiling (using <a href=\"https://github.com/SamSaffron/MiniProfiler/tree/master/Ruby\" rel=\"nofollow\">miniprofiler</a>) of this single command so far revealed the following:</p>\n\n<ul>\n<li>Some of those calls complete within about 2-4 ms (is this ok/good/bad?)</li>\n<li>Some however take 100-200 ms (this definitely feels bad)</li>\n<li>Using <code>slowlog get</code> on the redis-client does not show anything particularly slow on redis. Most of the redis command complete within less than 20 <strong>microseconds</strong> (0.02 ms)</li>\n<li>Using the rails console and running a quick benchmark using the exact same slug/id in a loop, the same behaviour is observed, i.e. a few of those (same) requests seem to take considerably longer than most others</li>\n</ul>\n\n<p>Our redis config is pretty much out-of-the-box with basically no tweaking. During testing the server is not doing much else.</p>\n\n<p>Any suggestions how to improve performance / test what slows down things so much between redis and ohm/rails ??</p>\n"},{"tags":["flash","actionscript-3","performance","math","negative-number"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":177,"score":0,"question_id":6301965,"title":"Fastest way to make Negative numbers","body":"<p>Is there any Fastest way for this line?</p>\n\n<pre><code>ballAngelRadianVector = -ballAngelRadianVector;\n</code></pre>\n\n<p>and also this:</p>\n\n<pre><code>ballDegree = fee - ballDegree ;\n</code></pre>\n"},{"tags":["sql","sql-server","recursion","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":1469,"score":1,"question_id":1224623,"title":"WITH Common Table Expression vs. CREATE VIEW performance","body":"<p>I have several queries that use a WITH clause, or Common Table Expression, with a UNION ALL statement to recur through a table with a tree like structure in SQL server as described <a href=\"http://www.webinade.com/web-development/creating-recursive-sql-calls-for-tables-with-parent-child-relationships\" rel=\"nofollow\">here</a>.  Would I see a difference in performance if I were to CREATE that same VIEW instead of including it with the WITH clause and having it generated every time I run the query?  Would it generally be considered good practice to actually CREATE the view since it is used in several queries?</p>\n"},{"tags":["performance","sharepoint","search","claims-based-identity"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":13,"score":0,"question_id":13175402,"title":"Custom Claims Provider - The search request was unable to connect to the Search Service","body":"<p>I have a custom claims provider in a farm SharePoint 2010 and FAST search service.\nCustom claims provider is deployed as farm feature.</p>\n\n<p>If the farm feature claims provider is not activate the search works fine.\nIf I active the feature then I receive a random error in search results page:\n\"The search request was unable to connect to the Search Service\".\nWhat I have to check?</p>\n"},{"tags":["asp.net",".net","wcf","performance","appharbor"],"answer_count":5,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":395,"score":3,"question_id":9348598,"title":"How to improve performance on a .net wcf service","body":"<p>I'm porting a cpu-heavy .net 4.0 windows application to a .net 4.0 wcf service. Basically I just imported the .net classes to the wcf service.</p>\n\n<p>All is working well except for performance at the wcf service - a task that takes 6267947 ticks (2539ms) uses 815349861 ticks (13045ms) on the aspx.net wcf service running locally on the same develop machine.</p>\n\n<p>I allready have uploaded the service + a test client to appharbor where the performance is as bad as on my local machine - the link to my test app is: <a href=\"http://www.wsolver.com\" rel=\"nofollow\">http://www.wsolver.com/</a>. Any ideas on how I can improve performance?</p>\n"},{"tags":["performance","monitoring","esb","talend"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":10,"score":0,"question_id":13175163,"title":"Talend ESB Performance Monitoring","body":"<p>I've created a service job in Talend Open Studio for ESB 5.1.2 and I want to see the performance of specific sections of my job to discern which part is taking the longest to process.<br/><br/>\nI have used tChronometerStart and tChronometerStop to log timings for specific sections but I'd be interested to know if there are any overall monitoring features of Talend ESB that I'm not yet making use of. I am ultimately aiming to tune all of the steps but can imagine that the use of timers on each step will eventually get a bit Heisenberg.</p>\n\n<p>Thanks,</p>\n\n<p>mids</p>\n"},{"tags":["wcf","performance"],"answer_count":3,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":3094,"score":0,"question_id":264366,"title":"Performance Tuning WCF Service","body":"<p>What is the single most important performance tuning area for a WCF Web service?  </p>\n\n<ul>\n<li>ASP.net Thread settings?</li>\n<li>WCF throttling?</li>\n</ul>\n"},{"tags":["java","arrays","performance","recursion","splitting"],"answer_count":2,"favorite_count":1,"up_vote_count":8,"down_vote_count":0,"view_count":228,"score":8,"question_id":13156894,"title":"Performance of coin splitting algorithm","body":"<p>My question is about a CodeFu practice problem (2012 round 2 problem 3). It basically comes down to splitting an array of integers in two (almost) equal halves and returning the smallest possible difference between the two. I have included the problem description below. As noted in the comments this can be described as a <a href=\"http://stackoverflow.com/questions/12781159/balanced-partition\">balanced partition problem</a>, which is a problem in the realm of <a href=\"http://en.wikipedia.org/wiki/Dynamic_programming\" rel=\"nofollow\">dynamic programming</a>.</p>\n\n<p>Now similar problems have been discussed a lot, but I was unable find an efficient solution for this particular one. The problem is of course that the number of possible combinations to traverse soon grows too large for a brute force search (at least when using recursion). I have a recursive solution that works fine for all but the largest problem sets. I tried to add some optimizations that stop the recursion early, but the performance is still too slow to solve some arrays of the maximum length (30) within the 5 second maximum allowed by CodeFu. Any suggestions for how to improve or rewrite the code would be very welcome. I would also love to know if it might help to make the iterative version.</p>\n\n<p><strong>Update:</strong> on <a href=\"http://people.csail.mit.edu/bdean/6.046/dp/\" rel=\"nofollow\">this fine site</a> there is a theoretical discussion of the balanced partition problem, which gives a good idea of how to go about and solve this in a dynamic way. That is really what I am after, but I do not know how to put the theory into practice exactly. The movie mentions that the elements in the two subcollections can be found \"using the old trick of back pointers\", but I don't see how.</p>\n\n<h2>Problem</h2>\n\n<blockquote>\n  <p>You and your friend have a number of coins with various amounts. You\n  need to split the coins in two groups so that the difference between\n  those groups in minimal.</p>\n  \n  <p>E.g.   Coins of sizes 1,1,1,3,5,10,18 can be split as:  1,1,1,3,5 and\n  10,18  1,1,1,3,5,10 and 18  or  1,1,3,5,10 and 1,18 The third\n  combination is favorable as in that case the difference between the\n  groups is only 1.    Constraints:   coins will have between 2 and 30\n  elements inclusive   each element of coins will be between 1 and\n  100000 inclusive</p>\n  \n  <p>Return value:   Minimal difference possible when coins are split into\n  two groups</p>\n</blockquote>\n\n<p>NOTE: the CodeFu rules state that the execution time on CodeFu's server may be no more than 5 seconds.</p>\n\n<h2>Main Code</h2>\n\n<pre><code>Arrays.sort(coins);\n\nlower = Arrays.copyOfRange(coins, 0,coins.length-1);\n//(after sorting) put the largest element in upper\nupper = Arrays.copyOfRange(coins, coins.length-1,coins.length);            \n\nsmallestDifference = Math.abs(arraySum(upper) - arraySum(lower));\nreturn findSmallestDifference(lower, upper, arraySum(lower), arraySum(upper), smallestDifference);\n</code></pre>\n\n<h2>Recursion Code</h2>\n\n<pre><code>private int findSmallestDifference (int[] lower, int[] upper, int lowerSum, int upperSum, int smallestDifference) {\n    int[] newUpper = null, newLower = null;\n    int currentDifference = Math.abs(upperSum-lowerSum);\n    if (currentDifference &lt; smallestDifference) {\n        smallestDifference = currentDifference;\n    } \n    if (lowerSum &lt; upperSum || lower.length &lt; upper.length || lower[0] &gt; currentDifference \n            || lower[lower.length-1] &gt; currentDifference \n            || lower[lower.length-1] &lt; upper[0]/lower.length) {\n        return smallestDifference;\n    }\n    for (int i = lower.length-1; i &gt;= 0 &amp;&amp; smallestDifference &gt; 0; i--) {           \n       newUpper = addElement(upper, lower[i]);\n       newLower = removeElementAt(lower, i);\n       smallestDifference = findSmallestDifference(newLower, newUpper, \n               lowerSum - lower[i], upperSum + lower [i], smallestDifference);\n    }\n    return smallestDifference;\n}\n</code></pre>\n\n<h2>Data Set</h2>\n\n<p>Here is an example of a set that takes too long to solve.</p>\n\n<blockquote>\n  <p>{100000,60000,60000,60000,60000,60000,60000,60000,60000,\n              60000,60000,60000,60000,60000,60000,60000,60000,60000,\n              60000,60000,60000,60000,60000,60000,60000,60000,60000,\n              60000,60000,60000}</p>\n</blockquote>\n\n<p>If you would like the entire source code, I have put it on <a href=\"http://ideone.com/weHDoP\" rel=\"nofollow\">Ideone</a>.</p>\n"},{"tags":["wpf","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":98,"score":2,"question_id":11575142,"title":"WPF Application takes too much CPU for some users","body":"<p>I have a WPF application written in VS2010 Express for .Net 4. Some testers report that the application takes alot of CPU (~80%-100%) whenever the application window is visible and 0% when not visible. This is not the case on my computer, nor on the other computers i have tested. What I see is that it takes a bunch of CPU for the first few seconds after opening the window, but then goes down to a few percent.</p>\n\n<p>My suspicion is that this is an issue with the rendering of the WPF window. But i can not understand why it only occurs for some users. </p>\n\n<p>I understand that this is not much to go on, but if anyone have an idea where to start looking for the cause of this performance issue, it would be much appreciated.  </p>\n\n<p>Thank you!</p>\n"},{"tags":["c#","performance","bitmap","contrast"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":2027,"score":2,"question_id":8990926,"title":"Faster contrast algorithm for a bitmap","body":"<p>I have a tool with trackbar slider controls used to adjust an image's brightness, contrast, gamma, etc.</p>\n\n<p>I am trying to get real-time updates to my image while the user drags the slider.  The brightness and gamma algorithms are an acceptable speed (around 170ms).  But the contrast algorithm is about 380ms.</p>\n\n<p>Basically my form is a tool window with sliders.  Each time the image is updated, it sends an event to the parent which redraws the new image.  The tool window keeps the original unmodified image locked in memory so I always have access to the bytes of it.  So basically I do this each time the ValueChanged event for a slider (such as the Contrast slider) is changed.</p>\n\n<ul>\n<li>LockBits of the working (destination) bitmap as Format24bppRgb (original bitmap is in Format32bppPArgb)</li>\n<li>Marshal.Copy the bits to a byte[] array</li>\n<li>Check which operation I'm doing (which slider was chosen)</li>\n<li>Use the following code for Contrast:</li>\n</ul>\n\n<p>Code:</p>\n\n<pre><code>double newValue = 0;\ndouble c = (100.0 + contrast) / 100.0;\n\nc *= c;\n\nfor (int i = 0; i &lt; sourcePixels.Length; i++)\n{\n    newValue = sourcePixels[i];\n\n    newValue /= 255.0;\n    newValue -= 0.5;\n    newValue *= c;\n    newValue += 0.5;\n    newValue *= 255;\n\n    if (newValue &lt; 0)\n        newValue = 0;\n    if (newValue &gt; 255)\n        newValue = 255;\n\n    destPixels[i] = (byte)newValue;\n}\n</code></pre>\n\n<p>I read once about using integer instead of floating point values to increase the speed of contrast, but I couldn't find that article again.</p>\n\n<p>I tried using unsafe code (pointers) but actually noticed a speed decrease.  I assume it was because the code was using nested for loops to iterate x and y instead of a single loop.</p>\n"},{"tags":["asp.net","performance","hang","debugdiag"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":13173253,"title":"slow/hanging Asp.net web application - full dump shows several threads waiting for lock","body":"<p>I have a web application and sometimes it is hanging / performing very slow.  I have taken a full dump using DebugDiag, and tried to analyse it using the Crash/Hang analysis. </p>\n\n<p>The summary gave me that 7.86% of my threads (10) are blocked and waiting for <code>Monitor.Wait</code>.</p>\n\n<p>However, when I check the Call Stack / Stack Trace with the thread, the below is outputted:</p>\n\n<pre><code>.NET Call Stack\n\n\n\nFunction \nSystem.Threading.Monitor.ObjWait(Boolean, Int32, System.Object) \nSystem.Threading.Monitor.Wait(System.Object, Int32, Boolean) \nQuartz.Simpl.SimpleThreadPool+WorkerThread.Run() \nSystem.Threading.ThreadHelper.ThreadStart_Context(System.Object) \nSystem.Threading.ExecutionContext.runTryCode(System.Object) \nSystem.Runtime.CompilerServices.RuntimeHelpers.ExecuteCodeWithGuaranteedCleanup(TryCode, CleanupCode, System.Object) \nSystem.Threading.ExecutionContext.RunInternal(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object) \nSystem.Threading.ExecutionContext.Run(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object, Boolean) \nSystem.Threading.ExecutionContext.Run(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object) \nSystem.Threading.ThreadHelper.ThreadStart() \n\n\nFull Call Stack\n\n\n\nFunction   Source \nntdll!NtWaitForMultipleObjects+15    \nKERNELBASE!WaitForMultipleObjectsEx+100    \nkernel32!WaitForMultipleObjectsExImplementation+e0    \nclr!WaitForMultipleObjectsEx_SO_TOLERANT+56    \nclr!Thread::DoAppropriateAptStateWait+4d    \nclr!Thread::DoAppropriateWaitWorker+17d    \nclr!Thread::DoAppropriateWait+60    \nclr!CLREvent::WaitEx+106    \nclr!CLREvent::Wait+19    \nclr!Thread::Wait+1d    \nclr!Thread::Block+1a    \nclr!SyncBlock::Wait+169    \nclr!ObjHeader::Wait+2c    \nclr!ObjectNative::WaitTimeout+147    \nSystem.Threading.Monitor.Wait(System.Object, Int32, Boolean)    \nSystem.Threading.ThreadHelper.ThreadStart_Context(System.Object)    \nSystem.Threading.ExecutionContext.runTryCode(System.Object)    \nclr!CallDescrWorker+33    \nclr!CallDescrWorkerWithHandler+8e    \nclr!MethodDesc::CallDescr+194    \nclr!MethodDesc::CallTargetWorker+21    \nclr!MethodDescCallSite::Call+1c    \nclr!ExecuteCodeWithGuaranteedCleanupHelper+bb    \nclr!ReflectionInvocation::ExecuteCodeWithGuaranteedCleanup+138    \nSystem.Threading.ExecutionContext.RunInternal(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object)    \nSystem.Threading.ExecutionContext.Run(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object)    \nSystem.Threading.ThreadHelper.ThreadStart()    \nclr!CallDescrWorker+33    \nclr!CallDescrWorkerWithHandler+8e    \nclr!MethodDesc::CallDescr+194    \nclr!MethodDesc::CallTargetWorker+21    \nclr!ThreadNative::KickOffThread_Worker+1e1    \nclr!Thread::DoExtraWorkForFinalizer+114    \nclr!Thread::ShouldChangeAbortToUnload+101    \nclr!Thread::ShouldChangeAbortToUnload+399    \nclr!Thread::RaiseCrossContextException+3f8    \nclr!Thread::DoADCallBack+358    \nclr!Thread::DoExtraWorkForFinalizer+fa    \nclr!Thread::ShouldChangeAbortToUnload+101    \nclr!Thread::ShouldChangeAbortToUnload+399    \nclr!Thread::ShouldChangeAbortToUnload+43a    \nclr!ManagedThreadBase::KickOff+15    \nclr!ThreadNative::KickOffThread+23e    \nclr!Thread::intermediateThreadProc+4b    \nkernel32!BaseThreadInitThunk+e    \nntdll!__RtlUserThreadStart+70    \nntdll!_RtlUserThreadStart+1b \n</code></pre>\n\n<p>It doesn't actually show me which lock they are waiting to obtain - Any idea on how to get this information?</p>\n"},{"tags":["javascript","performance","algorithm","optimization"],"answer_count":23,"favorite_count":43,"up_vote_count":215,"down_vote_count":16,"view_count":9551,"score":199,"question_id":13136724,"title":"Why is i-- faster than i++ in loops?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/1340589/javascript-are-loops-really-faster-in-reverse\">JavaScript - Are loops really faster in reverse…?</a>  </p>\n</blockquote>\n\n\n\n<p>I don't know if this question is valid in other languages or not, but I'm asking this specifically for <code>JavaScript</code>.</p>\n\n<p>I see in some articles and questions that the fastest loop in JavaScript is something like:</p>\n\n<pre><code>for(var i = array.length; i--; )\n</code></pre>\n\n<p>Also in Sublime Text 2, when you try to write a loop, it suggests:</p>\n\n<pre><code>for (var i = Things.length - 1; i &gt;= 0; i--) {\n    Things[i]\n};\n</code></pre>\n\n<p>I want to know, why is <code>i--</code> faster than <code>i++</code> in loops?</p>\n"},{"tags":["javascript","performance","variables","ecmascript-harmony","ecmascript-4"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":110,"score":1,"question_id":12379191,"title":"Javascript optional type hinting","body":"<p>When a programming language is statically typed, the compiler can be more precise about memory allocation and thus be generally more performant (with all other things equal).</p>\n\n<p>I believe ES4 introduced optional type hinting (from what I understand, Adobe had a huge part in contributing to its spec due to actionscript).  Does javascript officially support type hinting as a result?  Will ES6 support optional type hinting for native variables?</p>\n\n<p>If Javascript does support type hinting, are there any benchmarks that show how it pays off in terms of performance?  I have not seen an open source project use this yet.</p>\n"},{"tags":["css","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":104,"score":2,"question_id":9962081,"title":"Which should I use? (performance)","body":"<p>I want to know a simple thing:</p>\n\n<p>when setting up a style that is inherited by all its children, is it recommended to be most specific?</p>\n\n<blockquote>\n  <p><strong>Structure:</strong> html > body > parent_content > wrapper > p</p>\n</blockquote>\n\n<p>I want to apply a style to <code>p</code> but respecting these:</p>\n\n<ul>\n<li>I don't care having <code>parent_content</code> or <code>wrapper</code> having the style</li>\n<li>I do care changing the <code>html</code> or <code>body</code> style (or all <code>p</code>)</li>\n</ul>\n\n<p>So what should I use?</p>\n\n<pre><code>#parent_content{\n    color:#555;\n}\n\n#parent_content p{\n    color:#555;\n}\n\n#wrapper{\n    color:#555;\n}\n\n#wrapper p{\n    color:#555;\n}\n\n/*...etc...*/\n</code></pre>\n\n<p>Also, some links to tutorials about this would be great</p>\n"},{"tags":["asp.net","performance","cookies","yslow"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":24,"score":2,"question_id":13169140,"title":"How do I make my asp.net website \"more cookie free\"?","body":"<p>I ran my website through Yahoo's <a href=\"http://developer.yahoo.com/yslow/\" rel=\"nofollow\">YSlow</a> on my asp.net (vb) website that has 47 pages. There were a few problems, but one of them said I get a \"<strong>Grade F on Use cookie-free domains</strong>\".</p>\n\n<p>Specifically, it says:</p>\n\n<p><em><strong>When the browser requests a static image and sends cookies with the request, the server ignores the cookies. These cookies are unnecessary network traffic. To workaround this problem, make sure that static components are requested with cookie-free requests by creating a subdomain and hosting them there.</em></strong></p>\n\n<p>I really don't know what they're trying to tell me.  They say 43 components on my homepage aren't cookie-free, including:  site.css, print.css, homeslider.js, and then 38 or 39 .jpg or .png images aren't cookie-free.</p>\n\n<p>Does anybody know how I can improve this and improve my site's performance?  Thank you for any suggestions you can offer!</p>\n"},{"tags":["perl","shell","performance","benchmarking"],"answer_count":6,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":1423,"score":5,"question_id":2703938,"title":"Is it possible to write a shell script which is faster than the equivalent script in Perl?","body":"<p>I wrote multiple scripts in Perl and shell and I have compared the real execution time. In all the cases, the Perl script was more than 10 times faster than the shell script.</p>\n\n<p>So I wondered if it possible to write a shell script which is faster than the same script in Perl? And why is Perl faster than shell although I use the <a href=\"http://perldoc.perl.org/functions/system.html\" rel=\"nofollow\">system</a> function in Perl script?</p>\n"},{"tags":["sql","performance","sql-server-2005","sql-server-2012"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":4,"view_count":830,"score":-4,"question_id":10926841,"title":"Sql Server 2012 slower than 2005?","body":"<p>We have deployed SQL Server 2012 Enterprise and we have performance issues:</p>\n\n<ul>\n<li>same data (backuped from our SQL Server 2005 Enterprise and restored on 2012)</li>\n<li>test script of 3200 sql SELECT statements</li>\n</ul>\n\n<p>We do tests using Management Studio:</p>\n\n<ol>\n<li>results as plain text</li>\n<li>results in a file</li>\n</ol>\n\n<p>On same computer:</p>\n\n<ol>\n<li>2005 : 15 sec, 2012 : 2 min</li>\n<li>2005 : 14 sec, 2012 : 30 sec</li>\n</ol>\n\n<p>Even with a more powerful computer, 2012 is still slower than 2005.</p>\n\n<p>What can be wrong? The way we installed SQL Server 2012 and default parameters? The way we restored the backup? What can we do?</p>\n"},{"tags":["database","performance","updates"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":12850180,"title":"MySQL: Direct Update or Select before a conditional Update?","body":"<p>Just curious to know- if we need to do a conditional Update in a <strong>large</strong> table, then which is the best approach-</p>\n\n<p>Directly doing an Update or check for existing entry before Update.</p>\n\n<pre><code>function doDirectUpdate()\n{\n   // UPDATE table WHERE condn\n}\n</code></pre>\n\n<p>OR</p>\n\n<pre><code>function doCheckAndUpdate()\n{\n   // SELECT COUNT(id) AS exist FROM table WHERE condn\n   if(id exists)\n   {\n      // UPDATE table WHERE condn\n   }\n   else\n   {\n      echo 'No matching entry';\n   }\n}\n</code></pre>\n"},{"tags":["c++","performance","for-loop","c++11","move-semantics"],"answer_count":3,"favorite_count":0,"up_vote_count":10,"down_vote_count":1,"view_count":199,"score":9,"question_id":13130708,"title":"What is the advantage of using universal references in range-based for loops?","body":"<p><code>const auto&amp;</code> would suffice if I want to perform read-only operations. However, I have bumped into</p>\n\n<pre><code>for (auto&amp;&amp; e : v)  // v is non-const\n</code></pre>\n\n<p>a couple of times recently. This makes me wonder:</p>\n\n<p>Is it possible that in some obscure corner cases there is some performance benefit in using universal references, compared to <code>auto&amp;</code> or <code>const auto&amp;</code>?</p>\n\n<p>(<code>shared_ptr</code> is a suspect for obscure corner cases)</p>\n\n<hr>\n\n<p><strong>Update</strong>\nTwo examples that I found in my favorites:</p>\n\n<p><a href=\"http://stackoverflow.com/a/13057846/341970\">Any disadvantage of using const reference when iterating over basic types?</a><br/>\n<a href=\"http://stackoverflow.com/a/13087074/341970\">Can I easily iterate over the values of a map using a range-based for loop?</a></p>\n\n<p>Please concentrate on the question: <strong>why would I want to use auto&amp;&amp; in range-based for loops?</strong></p>\n"},{"tags":["performance","azure","monitoring","windows-azure-storage","performance-counters"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":28,"score":0,"question_id":13154908,"title":"How do I monitor performance of Azure Storage operations in my code?","body":"<p>I have a web role that uses Azure Storage while serving requests. Now I see that requests are served damn slow - like dozens times slower that usual. I've looked over the performance counters saved by the role - they look usual to me. I've checked connectivity from my network to the role - it looks okay.</p>\n\n<p>I suspect the problem might be with Azure Storage operations invoked from inside my service being rather slow for some periods of time and that would slow serving requests down.</p>\n\n<p>How do I continuously gather data that would help me estimate performance of these requests? Is there anything like a performance counter that I could enable or anything similar?</p>\n"},{"tags":["performance","localhost","ruby-1.9.3","slow-load"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":13014345,"title":"rails app extremely slow","body":"<p>I know there are lots of similar questions but I wasn't able to find a solution from any of those questions. So here we go..\nI am using Ubuntu 12.04 on a sony vaio. I am running Rails 3.2.8 with Ruby1.9.3-p194. I am using thin server instead of webbrick. I have mongoid, elasticseaerch and redis running as well.</p>\n\n<p>the page load is taking extremely long (over 10+ mins) on localhost. I know the app is not slow because it works on my colleagues' macbooks just fine. I am not sure what is causing it to have a very high loading time. </p>\n\n<p>any tips is appreciated. i am really sad/unhappy as it is slowing me down considerably at work. i asked my colleagues but none of them seem to know the answer.</p>\n\n<p>the app runs fine on company's production machine. it's only on my laptop that seems to be the problem. keep in mind that I have a very fast laptop (i5, 6gb RAM)</p>\n"},{"tags":["c","performance","optimization","time-complexity","codechef"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":2,"view_count":87,"score":-2,"question_id":13028801,"title":"Optimizing C code -> Codechef solution time exceeded","body":"<p>What are tips and tricks so that my code can be executed in given time limit. May be some logical changes or anything.</p>\n\n<p>When I submit my solution in codechef it shows time limit exceeded but I think it is fast enough simple code. I am amateur in programming any help and suggestions would be helpful. Guide me so that it could be executed within the time limit i.e. 3 seconds. Thank you !!</p>\n"},{"tags":["performance","optimization","opencv"],"answer_count":2,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":47,"score":4,"question_id":13128696,"title":"Which is the most efficient way to do alpha mask in opencv?","body":"<p>I know OpenCV only supports binary masks.<br>\nBut I need to do an overlay where I have a grayscale mask that specifies transparency of the overlay. </p>\n\n<p>Eg. if a pixel in the mask is 50% white it should mean a <code>cv::addWeighted</code> operation for that pixel with alpha=beta=0.5, gamma = 0.0.</p>\n\n<p>Now, if there is no opencv library function, what algorithm would you suggest as the most efficient?</p>\n"},{"tags":["objective-c","performance","cocoa","exception","error-handling"],"answer_count":2,"favorite_count":0,"up_vote_count":8,"down_vote_count":0,"view_count":162,"score":8,"question_id":3683439,"title":"What is the cost of using exceptions in Objective-C?","body":"<p>I mean in the current implementation of clang or the gcc version.</p>\n\n<p>C++ and Java guys always tell me that exceptions do not cost any performance unless they are thrown. Is the same true for Objective-C? </p>\n"},{"tags":["c#",".net","performance","task-parallel-library"],"answer_count":5,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":85,"score":2,"question_id":13142099,"title":"Making the most of the .NET Task Parallel Library","body":"<p><strong>Question 1.</strong></p>\n\n<p><em>Is using Parallel.For and Parallel.ForEach better suited to working with tasks that are ordered or unordered?</em></p>\n\n<p>My reason for asking is that I recently updated a serial loop where a StringBuilder was being used to generate a SQL statement based on various parameters. The result was that the SQL was a bit jumbled up (to the point it contained syntax errors) in comparison to when using a standard foreach loop, therefore my gut feeling is that TPL is not suited to performing tasks where the data must appear in a particular order.</p>\n\n<p><strong>Question 2.</strong></p>\n\n<p><em>Does the TPL automatically make use of multicore architectures of must I provision anything prior to execution?</em></p>\n\n<p>My reason for asking this relates back to an eariler question I asked relating to performance profiling of TPL operations. An answer to the question enlightened me to the fact that TPL is not always more efficient than a standard serial loop as the application may not have access to multiple cores, and therefore the overhead of creating additional threads and loops would create a performance decrease in comparison to a standard serial loop.</p>\n"},{"tags":["performance","graphics","time-complexity","raytracing"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":40,"score":2,"question_id":13138841,"title":"Performance of Ray Tracing","body":"<p>I wonder whether we can formulate the performance of ray-tracing. In my primitive ray-tracer, performance depends on this formula mostly: width x height x number of sampler x (number of objects + number of lights)</p>\n\n<p>So for example, in Pixar or any other big companies, do they follow such formula for performance evaluation. Isn't performance depends on triangle count of the objects? For example if I want to calculate roughly the maximum render time of the frame of 1000x1000 with average 500 objects that consists 5.000.000 triangles, would it be possible?</p>\n"},{"tags":["performance","postgresql"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":106,"score":3,"question_id":13014277,"title":"Why is this count query so slow?","body":"<p>Hi I'm hosted on Heroku running postgresql 9.1.6 on a their Ika plan (7,5gb ram). I have a table called cars. I need to do the following:</p>\n\n<pre><code>SELECT COUNT(*) FROM \"cars\" WHERE \"cars\".\"reference_id\" = 'toyota_hilux'\n</code></pre>\n\n<p>Now this takes an awful lot of time (64 sec!!!)</p>\n\n<pre><code>Aggregate  (cost=2849.52..2849.52 rows=1 width=0) (actual time=63388.390..63388.391 rows=1 loops=1)\n  -&gt;  Bitmap Heap Scan on cars  (cost=24.76..2848.78 rows=1464 width=0) (actual time=1169.581..63387.361 rows=739 loops=1)\n        Recheck Cond: ((reference_id)::text = 'toyota_hilux'::text)\n        -&gt;  Bitmap Index Scan on index_cars_on_reference_id  (cost=0.00..24.69 rows=1464 width=0) (actual time=547.530..547.530 rows=832 loops=1)\n              Index Cond: ((reference_id)::text = 'toyota_hilux'::text)\nTotal runtime: 64112.412 ms\n</code></pre>\n\n<p>A little background:</p>\n\n<p>The table holds around 3.2m rows, and the column that I'm trying to count on, has the following setup:</p>\n\n<pre><code>reference_id character varying(50);\n</code></pre>\n\n<p>and index:</p>\n\n<pre><code>CREATE INDEX index_cars_on_reference_id\n  ON cars\n  USING btree\n  (reference_id COLLATE pg_catalog.\"default\" );\n</code></pre>\n\n<p>What am I doing wrong? I expect that this performance is not what I should expect - or should I?</p>\n"},{"tags":["c","programming-languages","loops","performance"],"answer_count":15,"favorite_count":14,"up_vote_count":63,"down_vote_count":1,"view_count":4287,"score":62,"question_id":2823043,"title":"Is it faster to count down than it is to count up?","body":"<p>Our computer science teacher once said that for some reason it is more efficient to count down than to count up.\nFor example if you need to use a FOR loop and the loop index is not used somewhere (like printing a line of N * to the screen)\nI mean that code like this:</p>\n\n<pre><code>for (i = N; i &gt;= 0; i--)  \n  putchar('*');  \n</code></pre>\n\n<p>is better than:</p>\n\n<pre><code>for (i = 0; i &lt; N; i++)  \n  putchar('*');  \n</code></pre>\n\n<p>Is it really true? And if so, does anyone know why?</p>\n"},{"tags":["asp.net","vb.net","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":45,"score":1,"question_id":13162475,"title":"Why am I unable to specify height and width for images on my asp.net website? (html output included)","body":"<p>Whenever I run a speed test on Google or <a href=\"http://gtmetrix.com\" rel=\"nofollow\">GMetrix</a>, they always give me an F because they say my image sizes aren't specified, so more http requests are required.  But on my asp.net (vb) website, the links they specify are linkbuttons and look like this <strong>using CssClass</strong>:</p>\n\n<pre><code>&lt;asp:Image ID=\"Image9\" runat=\"server\" ImageUrl=\"~/images/flagpt.png\" \nCssClass=\"flagbutton\" tooltip=\"Veja este site em portuguÃªs\" \ntitle=\"Veja este site em portuguÃªs\"/&gt;\n</code></pre>\n\n<p>This results in this <strong>html output</strong> (using CssClass) but still doesn't have img dimensions:</p>\n\n<pre><code>&lt;img id=\"Image9\" title=\"Veja este site em portuguÃªs\" \n class=\"flagbutton\" title=\"Veja este site em portuguÃªs\" \n src=\"images/flagpt.png\" /&gt;\n</code></pre>\n\n<p>But if I just put <strong>height=\"15\" width=\"26\"</strong> like this, it still <strong>doesn't</strong> work:</p>\n\n<pre><code>  &lt;asp:Image ID=\"Image7\" runat=\"server\" ImageUrl=\"~/images/flagde.png\" \n  height=\"15\" width=\"26\" tooltip=\"View this website in Deutsch\" \n  title=\"View this website in Deutsch\"/&gt;\n</code></pre>\n\n<p>This results in this <strong>html output</strong>, but speed tests don't detect img dimensions:</p>\n\n<pre><code>&lt;img id=\"Image7\" title=\"View this website in Deutsch\" \n   title=\"View this website in Deutsch\" \n   src=\"images/flagde.png\" style=\"height:15px;width:26px;\" /&gt;\n</code></pre>\n\n<p>The speed tests still suggest I'm not specifying my dimensions.  Any suggestions to help me with this would be greatly appreciated!</p>\n"},{"tags":["eclipse","performance","optimization","eclipse-juno"],"answer_count":5,"favorite_count":4,"up_vote_count":15,"down_vote_count":0,"view_count":7943,"score":15,"question_id":11446825,"title":"Very slow Eclipse 4.2, how to make it more responsive?","body":"<p>I'm using Eclipse PDT on a rather large PHP project and the IDE is almost unusable. It takes nearly 30 seconds to open a file, and other actions, like selecting a folder in the file explorer, editing some text, etc. are equally slow.</p>\n\n<p>I followed various instructions to speed it up but nothing seems to work. This is my current <code>eclipse.ini</code> file. Any idea how I can improve it?</p>\n\n<pre><code>-startup\nplugins/org.eclipse.equinox.launcher_1.3.0.v20120522-1813.jar\n--launcher.library\nplugins/org.eclipse.equinox.launcher.win32.win32.x86_1.1.200.v20120522-1813\n-showsplash\norg.eclipse.platform\n--launcher.XXMaxPermSize\n256m\n--launcher.defaultAction\nopenFile\n-vmargs\n-server\n-Dosgi.requiredJavaVersion=1.7\n-Xmn128m\n-Xms1024m\n-Xmx1024m\n-Xss2m\n-XX:PermSize=128m\n-XX:MaxPermSize=128m\n-XX:+UseParallelGC\n</code></pre>\n\n<p>System: Eclipse 4.2.0, Windows 7, 4 GB RAM</p>\n"},{"tags":["javascript","performance","design","extjs"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":76,"score":2,"question_id":13162570,"title":"ExtJS 4.1 - Store.add() (followed by sync) vs Model.save()","body":"<p><strong>FIRST:</strong> I realize this question has been asked here: <a href=\"http://stackoverflow.com/questions/10115001/in-extjs-is-it-better-to-call-model-save-or-store-sync\">in ExtJS, is it better to call Model.save() or Store.Sync()?</a> - however I wish to examine this further, specifically regarding minimizing XHR's and unnecessary overhead on both the client and server. I do not feel either of these points were addressed in the linked question.</p>\n\n<p>I have a somewhat large application designed for enterprise resource management, consisting of many models, views and controllers. I handle all responses from my server by establishing a listener to Ext.Ajax <code>requestComplete</code> and <code>requestException</code> events. I took this approach rather than writing duplicate event handlers on every model's proxy <code>afterRequest</code> event. This enables me to have all of my back-end (using the Zend Framework) controllers responding with three parameters: <code>success</code>, <code>message</code> and <code>data</code>.</p>\n\n<p>After a successful request (i.e., HTTP 200), the method run for <code>requestComplete</code> will inspect the JSON response for the aforementioned parameters. If <code>success</code> is <code>false</code>, it is expected that there will be an error message contained in <code>message</code>, which is then displayed to the user (e.g. 'There was a problem saving that product. Invalid product name'). If success is true, action is taken depending on the type of request, i.e., Create, Read, Update or Destroy. After a successful <code>create</code>, the new record is added to the appropriate data store, after delete the record is destroyed, and so forth.</p>\n\n<p>I chose to take this approach rather than adding records to a store and calling the store's <code>sync</code> method in order to minimize XHR's and otherwise round trips. My current means of saving/updating data is to send the request to the backend and react to the result on the Ext front end. I do this by populating a model with data and calling model.save() for create/update requests, or model.destroy() to remove the data.</p>\n\n<p>I found that when adding/updating/removing records from a store, then calling store.sync(), I would have to react to server's response in a way that felt awkward. Take for example, deleting a record:</p>\n\n<ol>\n<li>First, remove the record from the store via <code>store.remove()</code></li>\n<li>Invoke <code>store.sync()</code> as I have store's <code>autoSync</code> set to <code>false</code>.</li>\n<li>This fires the AJAX destroy request from the store's model proxy.</li>\n<li>Here's where it gets weird.... if there is an error on the server while dropping the row from the database, the response will return <code>success: false</code>, however the record will have already been removed from the ExtJS Data Store.</li>\n<li>At this point, I can either call <code>store.sync()</code>, <code>store.load()</code> (both requiring a round trip) or get the record from the request and add it back to the store followed by a <code>commitChanges()</code> to avoid calling an additional sync/load and thus avoiding an unnecessary round trip.</li>\n</ol>\n\n<p>The same goes for adding records, if the server fails somewhere while adding data to the database, the record is still in the ExtJS store and must be removed manually to avoid a round trip with <code>store.sync()</code> or <code>store.load()</code>.</p>\n\n<p>In order to avoid this whole issue, as I previously explained, I instantiate one of my model objects (e.g. a Product model), populate it with data, and call <code>myModel.save()</code>. This, in turn, invokes the proxy's <code>create</code> or <code>update</code> depending on the ID of the model, and fires the appropriate AJAX request. In the event that the back-end fails, the front-end store is still unchanged. On successful requests (read: <code>success: true</code>, not HTTP 200), I manually add the record to the store and invoke <code>store.commitChanges(true)</code>, effectively syncing the store with the database without an additional round trip and avoiding unnecessary overhead. For all requests, the server will respond with the new/modified data as well as a success parameter, and conditionally a message to display on the client.</p>\n\n<p>Am I missing something here, or is this approach a good way to minimize XHR's and server/client overhead? I am happy to provide example code should that be requested, however I feel that this is a rather general concept with fundamental code. </p>\n"},{"tags":["c#","arrays","performance","x86","64bit"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":4,"view_count":50,"score":-4,"question_id":13162987,"title":"Most efficient array size in bytes in C# for class references for x86 and x64","body":"<p>I guess what I'm asking is, at what size in bytes does the first non-linear slowdown in accessing such an array come? Or, what's a good size for a small, very fast array?</p>\n\n<p>Note: Single-dimension array.</p>\n\n<p>Thanks.</p>\n\n<p>The object pools I've tested, themselves. Note that they're in order of speed:</p>\n\n<pre><code>using System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\n\nnamespace ObjectPool\n{\n    public sealed class FastPool&lt;T&gt;\n    {\n        private List&lt;Ref&gt; available;\n        private List&lt;Ref&gt; unavailable;\n        private List&lt;int&gt; availableUnavailables;\n        private Func&lt;T&gt; createFunc;\n        private int counter = 0;\n\n\n        public FastPool( int capacity, Func&lt;T&gt; createFunc )\n        {\n            available = new List&lt;Ref&gt;( 0 );\n            unavailable = new List&lt;Ref&gt;( 0 );\n            availableUnavailables = new List&lt;int&gt;( 0 );\n            this.createFunc = createFunc;\n            Increase( capacity );\n        }\n\n\n        public Ref Get( )\n        {\n            if ( counter == -1 )\n                Increase( available.Capacity );\n            if ( counter &lt; 0 )\n                throw new IndexOutOfRangeException( );\n            if ( counter &gt;= available.Count )\n                throw new NotImplementedException( );\n\n            var r = available[ counter ];\n            int i = availableUnavailables[ counter ];\n            --counter;\n            unavailable[ i ] = r;\n            r.index = i;\n            return r;\n        }\n\n\n        private void Increase( int addInstances )\n        {\n            int prevCapacity = available.Capacity;\n            available.Capacity += addInstances;\n            unavailable.Capacity += addInstances;\n            availableUnavailables.Capacity += addInstances;\n            for ( int t = 0 ; t &lt; addInstances ; ++t )\n            {\n                available.Add( new Ref( this, createFunc( ), -1 ) );\n                unavailable.Add( null );\n                availableUnavailables.Add( t );\n            }\n            counter = available.Count - 1;\n        }\n\n\n        public class Ref\n        {\n            public T Value\n            {\n                get\n                {\n                    if ( index == -1 )\n                        throw new NullReferenceException( );\n                    return value;\n                }\n            }\n\n\n            private FastPool&lt;T&gt; pool;\n            private T value;\n            internal int index;\n\n\n            internal Ref( FastPool&lt;T&gt; pool, T value, int index )\n            {\n                this.pool = pool;\n                this.value = value;\n                this.index = index;\n            }\n\n\n            public void Release( )\n            {\n                if ( index == -1 )\n                    return;\n                ++pool.counter;\n                pool.available[ pool.counter ] = this;\n                pool.unavailable[ index ] = null;\n                pool.availableUnavailables[ pool.counter ] = this.index;\n                index = -1;\n            }\n        }\n    }\n\n\n    public sealed class FastFixedPool&lt;T&gt;\n    {\n        private Ref[] available;\n        private Ref[] unavailable;\n        private int[] availableUnavailables;\n        private Func&lt;T&gt; createFunc;\n        private int counter;\n\n\n        public FastFixedPool( int capacity, Func&lt;T&gt; createFunc )\n        {\n            available = new Ref[ capacity ];\n            unavailable = new Ref[ capacity ];\n            availableUnavailables = new int[ capacity ];\n            this.createFunc = createFunc;\n\n            this.counter = available.Length - 1;\n            for ( int t = 0 ; t &lt; capacity ; ++t )\n            {\n                available[ t ] = new Ref( this, createFunc( ), -1 );\n                unavailable[ t ] = null;\n                availableUnavailables[ t ] =  t;\n            }\n        }\n\n\n        public Ref Get( )\n        {\n            if ( counter &lt; 0 )\n                throw new IndexOutOfRangeException( );\n            if ( counter &gt;= available.Length )\n                throw new NotImplementedException( );\n\n            var r = available[ counter ];\n            int i = availableUnavailables[ counter ];\n            --counter;\n            unavailable[ i ] = r;\n            r.index = i;\n            return r;\n        }\n\n\n        public class Ref\n        {\n            public T Value\n            {\n                get\n                {\n                    if ( index == -1 )\n                        throw new NullReferenceException( );\n                    return value;\n                }\n            }\n\n\n            private FastFixedPool&lt;T&gt; pool;\n            private T value;\n            internal int index;\n\n\n            internal Ref( FastFixedPool&lt;T&gt; pool, T value, int index )\n            {\n                this.pool = pool;\n                this.value = value;\n                this.index = index;\n            }\n\n\n            public void Release( )\n            {\n                if ( index == -1 )\n                    return;\n                ++pool.counter;\n                pool.available[ pool.counter ] = this;\n                pool.unavailable[ index ] = null;\n                pool.availableUnavailables[ pool.counter ] = this.index;\n                index = -1;\n            }\n        }\n    }\n\n\n    public sealed class FasterFixedPool&lt;T&gt;\n    {\n        private T[] values;\n        private Ref[] available;\n        private Func&lt;T&gt; createFunc;\n        private int counter;\n\n\n        public FasterFixedPool( int capacity, Func&lt;T&gt; createFunc )\n        {\n            values = new T[ capacity ];\n            available = new Ref[ capacity ];\n            this.createFunc = createFunc;\n\n            this.counter = available.Length - 1;\n            for ( int t = 0 ; t &lt; capacity ; ++t )\n            {\n                values[ t ] = createFunc( );\n                available[ t ] = new Ref( this, -1 );\n            }\n        }\n\n\n        public Ref Get( )\n        {\n            var r = available[ counter ];\n            r.live = true;\n            --counter;\n            return r;\n        }\n\n\n        public class Ref\n        {\n            public T Value\n            {\n                get\n                {\n                    if ( !live )\n                        throw new NullReferenceException( );\n                    return pool.values[ index ];\n                }\n            }\n\n\n            private FasterFixedPool&lt;T&gt; pool;\n            private int index;\n            internal bool live;\n\n\n            internal Ref( FasterFixedPool&lt;T&gt; pool, int index )\n            {\n                this.pool = pool;\n                this.index = index;\n                this.live = false;\n            }\n\n\n            public void Release( )\n            {\n                if ( !live )\n                    return;\n                ++pool.counter;\n                pool.available[ pool.counter ] = this;\n                live = false;\n            }\n        }\n    }\n\n\n    public sealed class EvenFasterFixedPool&lt;T&gt;\n    {\n        private Ref[] available;\n        private Func&lt;T&gt; createFunc;\n        private int counter;\n\n\n        public EvenFasterFixedPool( int capacity, Func&lt;T&gt; createFunc )\n        {\n            available = new Ref[ capacity ];\n            this.createFunc = createFunc;\n\n            this.counter = available.Length - 1;\n            for ( int t = 0 ; t &lt; capacity ; ++t )\n            {\n                available[ t ] = new Ref( this, createFunc( ), -1 );\n            }\n        }\n\n\n        public Ref Get( )\n        {\n            var r = available[ counter ];\n            r.live = true;\n            --counter;\n            return r;\n        }\n\n\n        public class Ref\n        {\n            public T Value\n            {\n                get\n                {\n                    if ( !live )\n                        throw new NullReferenceException( );\n                    return value;\n                }\n            }\n\n\n            private EvenFasterFixedPool&lt;T&gt; pool;\n            private T value;\n            internal bool live;\n\n\n            internal Ref( EvenFasterFixedPool&lt;T&gt; pool, T value, int index )\n            {\n                this.pool = pool;\n                this.value = value;\n                this.live = false;\n            }\n\n\n            public void Release( )\n            {\n                if ( !live )\n                    return;\n                ++pool.counter;\n                pool.available[ pool.counter ] = this;\n                live = false;\n            }\n        }\n    }\n\n\n    public interface ICreate&lt;T&gt;\n    {\n        T Create( );\n    }\n}\n</code></pre>\n\n<p>The test code itself:</p>\n\n<pre><code>using System;\nusing System.Collections.Generic;\nusing System.Diagnostics;\nusing System.Linq;\nusing System.Runtime.InteropServices;\nusing System.Text;\nusing ObjectPool;\n\n\nnamespace ObjectPool_Testing\n{\n    class Program\n    {\n        static void Main( string[ ] args )\n        {\n            int count = 4096;\n\n\n            // DefaultNewTesting( count );\n            // FastPoolTesting( count );\n            // FastFixedPoolTesting( count );\n            // FasterFixedPoolTesting( count );\n            EvenFasterFixedPoolTesting( count );\n\n\n            Console.ReadKey( true );\n        }\n\n\n        static void DefaultNewTesting( int count )\n        {\n            Console.WriteLine( \"Default new testing\" );\n\n            int t = 0;\n            var timer = new Stopwatch( );\n            timer.Restart( );\n            List&lt;Class16&gt; list = new List&lt;Class16&gt;( );\n            for ( ; t &lt; count ; ++t )\n            {\n                list.Add( new Class16( ) );\n            }\n            list.Clear( );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            timer.Restart( );\n            for ( int t2 = 0 ; t2 &lt; 10 ; ++t2 )\n            {\n                for ( t = 0 ; t &lt; count ; ++t )\n                {\n                    list.Add( new Class16() );\n                }\n                list.Clear( );\n            }\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            Console.WriteLine( );\n        }\n\n\n        static void FastPoolTesting( int count )\n        {\n            Console.WriteLine( \"FastPool&lt;T&gt; testing\" );\n\n            int t = 0;\n            var timer = new Stopwatch( );\n            timer.Restart( );\n            var pool =\n                new FastPool&lt;Class16&gt;(\n                    count,\n                    delegate()\n                    {\n                        return new Class16();\n                    } );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            t = 0;\n            timer.Restart( );\n            var list = new List&lt;FastPool&lt;Class16&gt;.Ref&gt;( );\n            for ( ; t &lt; count ; ++t )\n            {\n                list.Add( pool.Get( ) );\n            }\n            for ( t = 0 ; t &lt; list.Count ; ++t )\n                list[ t ].Release( );\n            list.Clear( );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            timer.Restart( );\n            for ( int t2 = 0 ; t2 &lt; 10 ; ++t2 )\n            {\n                for ( t = 0 ; t &lt; count ; ++t )\n                {\n                    list.Add( pool.Get( ) );\n                }\n                for ( t = 0 ; t &lt; list.Count ; ++t )\n                    list[ t ].Release( );\n                list.Clear( );\n            }\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            Console.WriteLine( );\n        }\n\n\n        static void FastFixedPoolTesting( int count )\n        {\n            Console.WriteLine( \"FastFixedPool&lt;T&gt; testing\" );\n\n            int t = 0;\n            var timer = new Stopwatch( );\n            timer.Restart( );\n            var pool =\n                new FastFixedPool&lt;Class16&gt;(\n                    count,\n                    delegate( )\n                    {\n                        return new Class16( );\n                    } );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            t = 0;\n            timer.Restart( );\n            var list = new List&lt;FastFixedPool&lt;Class16&gt;.Ref&gt;( );\n            for ( ; t &lt; count ; ++t )\n            {\n                list.Add( pool.Get( ) );\n            }\n            for ( t = 0 ; t &lt; list.Count ; ++t )\n                list[ t ].Release( );\n            list.Clear( );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            timer.Restart( );\n            for ( int t2 = 0 ; t2 &lt; 10 ; ++t2 )\n            {\n                for ( t = 0 ; t &lt; count ; ++t )\n                {\n                    list.Add( pool.Get( ) );\n                }\n                for ( t = 0 ; t &lt; list.Count ; ++t )\n                    list[ t ].Release( );\n                list.Clear( );\n            }\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            Console.WriteLine( );\n        }\n\n\n        static void FasterFixedPoolTesting( int count )\n        {\n            Console.WriteLine( \"FasterFixedPool&lt;T&gt; testing\" );\n\n            int t = 0;\n            var timer = new Stopwatch( );\n            timer.Restart( );\n            var pool =\n                new FasterFixedPool&lt;Class16&gt;(\n                    count,\n                    delegate( )\n                    {\n                        return new Class16( );\n                    } );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            t = 0;\n            timer.Restart( );\n            var list =\n                new List&lt;FasterFixedPool&lt;Class16&gt;.Ref&gt;(\n                    count );\n            for ( ; t &lt; count ; ++t )\n            {\n                list.Add( pool.Get( ) );\n            }\n            for ( t = 0 ; t &lt; list.Count ; ++t )\n                list[ t ].Release( );\n            list.Clear( );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            timer.Restart( );\n            for ( int t2 = 0 ; t2 &lt; 10 ; ++t2 )\n            {\n                for ( t = 0 ; t &lt; count ; ++t )\n                {\n                    list.Add( pool.Get( ) );\n                }\n                for ( t = 0 ; t &lt; list.Count ; ++t )\n                    list[ t ].Release( );\n                list.Clear( );\n            }\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            Console.WriteLine( );\n        }\n\n\n        static void EvenFasterFixedPoolTesting( int count )\n        {\n            Console.WriteLine( \"FasterFixedPool&lt;T&gt; testing\" );\n\n            int t = 0;\n            var timer = new Stopwatch( );\n            timer.Restart( );\n            var pool =\n                new EvenFasterFixedPool&lt;Class16&gt;(\n                    count,\n                    delegate( )\n                    {\n                        return new Class16( );\n                    } );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            t = 0;\n            timer.Restart( );\n            var list =\n                new List&lt;EvenFasterFixedPool&lt;Class16&gt;.Ref&gt;(\n                    count );\n            for ( ; t &lt; count ; ++t )\n            {\n                list.Add( pool.Get( ) );\n            }\n            for ( t = 0 ; t &lt; list.Count ; ++t )\n                list[ t ].Release( );\n            list.Clear( );\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            timer.Restart( );\n            for ( int t2 = 0 ; t2 &lt; 100000; ++t2 )\n            {\n                for ( t = 0 ; t &lt; count ; ++t )\n                {\n                    list.Add( pool.Get( ) );\n                }\n                for ( t = 0 ; t &lt; list.Count ; ++t )\n                    list[ t ].Release( );\n                list.Clear( );\n            }\n            timer.Stop( );\n            Console.WriteLine(\n                Handy.Iterations(\n                    count * 100000,\n                    timer.Elapsed.TotalSeconds,\n                    \"second\" ) );\n\n            Console.WriteLine( );\n        }\n    }\n\n\n    static class Handy\n    {\n        public static string Iterations(\n            int iterations,\n            double time,\n            string timeDesignation )\n        {\n            return\n                iterations\n                + \" iterations in \"\n                + time\n                + ' '\n                + timeDesignation\n                + \"s at \"\n                + ( iterations / time )\n                + \" per \"\n                + timeDesignation;\n        }\n    }\n\n\n    [StructLayout( LayoutKind.Auto, Size = 16 )]\n    class Class16\n    {\n    }\n\n\n    sealed class Class16Factory : ICreate&lt;Class16&gt;\n    {\n        public Class16 Create( )\n        {\n            return new Class16( );\n        }\n    }\n}\n</code></pre>\n\n<p>There. It's even formatted. You have nothing to complain about.</p>\n"},{"tags":["windows","performance","iis","queue","processor"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":23,"score":0,"question_id":13162631,"title":"Processor Queue Length high with IIS Server","body":"<p>I have a 2 core server (2 x 2.66 GHZ INTEL Xeon) w/ 8 GM Ram and 50 GB 15,000 RPM HDD space.\nThis is a VM running w/ VM ware and is running Microsoft Server 2008R2. We have had some performance issues recently. We have up to 350 users w/ multiple devices/sessions per user and sometimes 800 sessions at once. We are pushing out data (about 100KB per file) on an average of every 3 minutes. It is a reporting application that updates the data for everyone so they all have the data in real-ish time.</p>\n\n<p>We are hosting a .net application written in C# though IIS server 7.5. We are running 2 worker processes. We do no have memory utilization above 5 GB (out of 8) and disk I/O seems fine.</p>\n\n<p>The processor queue length has been generally above 50 and has averaged as high as 120. Microsoft says it should be between 2 and 6.</p>\n\n<p>technet.microsoft.com/en-us/library/bb742410.aspx</p>\n\n<p>I am happy to answer any questions you may have, and this is my first time on here so be nice if I didn't explain something correctly!</p>\n\n<p>I am guessing that I need more CPU cores, please let me know your thoughts.</p>\n\n<p>THANKS GUYS!</p>\n"},{"tags":["xcode","json","performance","webserver","html-parsing"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":8,"score":0,"question_id":13161805,"title":"XCode: JSON or parse HTML for performance if reading folders from web server","body":"<p>I am trying to access some folders in a web server and have become hopelessly confused so apologies if this seems vague.</p>\n\n<p>I'm testing on a local host (MAMP), with the folders located at:</p>\n\n<pre><code>http://localhost:8888/Files/\n</code></pre>\n\n<p>I have an obj-C routine which returns HTML similar to the following:</p>\n\n<pre><code>&lt;html&gt;\n &lt;head&gt;\n  &lt;title&gt;Index of /Files&lt;/title&gt;\n &lt;/head&gt;\n &lt;body&gt;\n&lt;h1&gt;Index of /Files&lt;/h1&gt;\n&lt;ul&gt;&lt;li&gt;&lt;a href=\"/\"&gt; Parent Directory&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"Folder 1/\"&gt; Folder 1/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"Folder 2/\"&gt; Folder 2/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"Folder 3/\"&gt; Folder 3/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"Folder 4/\"&gt; Folder 4/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"Folder 5/\"&gt; Folder 5/&lt;/a&gt;&lt;/li&gt;&lt;/li&gt;\n&lt;/ul&gt;\n</code></pre>\n\n<p></p>\n\n<p>Each folder contains either sub-folders, files (.pdf, .doc, etc) or a combination of both.</p>\n\n<p>What I'm trying to do is have the information returned so that I can process the files for display in an iPad app.</p>\n\n<p>Reading similar questions on here, I understand this can be done using an HTML parser but that this can increase bandwidth demands and there seems to be a majority opinion that JSON should be used instead. (I realise I may be misinterpreting that.)</p>\n\n<p>In this question:\n<a href=\"http://stackoverflow.com/questions/6396901/parse-html-iphone\">Parse HTML iPhone</a>\none answer suggests leaving a static JSON file on the web server.</p>\n\n<p>My question is, would I be better trying convert to the file structure to JSON (I've no experience of JSON at all) or carry on down the HTML-parsing route?\nAny pointers appreciated.</p>\n"},{"tags":["html","performance","cookies","yslow"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":27,"score":1,"question_id":13154782,"title":"Why does yslow identifiy my images, scripts and css as cookies?","body":"<p><img src=\"http://i.stack.imgur.com/yg5kx.png\" alt=\"Yslow\"></p>\n\n<p>I am using yslow version 3.1.4</p>\n"},{"tags":["c#","performance","sql-server-2008-r2","daab","data-access-app-block"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":24,"score":2,"question_id":13161009,"title":"Why is a call to DatabaseInstance.ExecuteSprocAccessor<T>(...) taking so long for such a simple query?","body":"<p>Very odd slowdown when calling the Data Access Application block.</p>\n\n<p>The SP (\"QuestionsToBeAnswered\") it's calling returns 58 rows with three columns (two GUIDs and an integer: 21AF77DA-2E76-47DB-AB54-0E5C85CD9AD8, 21AF77DA-2E76-47DB-AB54-0E5C85CD9AF0, 2) in less than 1 second when executed directly on the server. My SQL experience is pretty good, and I'm convinced the problem doesn't exist on the SQL server.</p>\n\n<p>However, when it's called through DAAB, it's taking a very long time to return the collection of objects. ExecuteSprocAccessor(...) normally returns an IEnumerable, and the SP isn't executed until the collection is enumerated or otherwise consumed, so this problem doesn't show up until consumption occurs.</p>\n\n<pre><code>DatabaseInstance.ExecuteSprocAccessor&lt;T&gt;(storedProcedure, rowMapper, args);\n</code></pre>\n\n<p>Given that the same code has no problem returning >200 rows of considerably more complex information, I am baffled as to why this code is taking so long (55 seconds!) to execute.</p>\n\n<p>Any ideas would be welcomed...</p>\n"},{"tags":["android","performance","jelly-bean"],"answer_count":0,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":21,"score":3,"question_id":13160352,"title":"requestFocus for TextView on Jelly Bean slow","body":"<p>I am developing an application that has 4 text fields for entering data into and I have come across a performance issue when moving focus from one to the other. When a field has a character entered into it I use the addTextChangedListener to monitor the text and move the focus to the next text field. This was working fine on versions of android before 4.1.1 but since testing on 4.1.1 there is a noticeable lag when you press a key before the focus appears in the next field. This means if the user types rapidly, key presses can be lost.</p>\n\n<p>I have a simple app using the following code</p>\n\n<pre><code>public void onCreate(Bundle savedInstanceState) {\n    super.onCreate(savedInstanceState);\n    setContentView(R.layout.activity_main);\n\n\n    one = (EditText)findViewById(R.id.editText1);\n    two = (EditText)findViewById(R.id.editText2);\n\n    one.addTextChangedListener(new TextWatcher() {\n\n\n        @Override\n        public void afterTextChanged(Editable s) {\n            two.requestFocus();\n\n        }\n\n        @Override\n        public void beforeTextChanged(CharSequence s, int start, int count,\n                int after) {\n            // TODO Auto-generated method stub\n\n        }\n\n        @Override\n        public void onTextChanged(CharSequence s, int start, int before,\n                int count) {\n            // TODO Auto-generated method stub\n\n        }\n    });\n}\n</code></pre>\n\n<p>that highlights the issue. When run on a 4.0.4 based device everything is fine, but on 4.1.1 it takes a while to move the focus.</p>\n\n<p>I have tested this on 2 different Samsung Galaxy s3's one with 4.0.4 and one with 4.1.1.</p>\n\n<p>Has anyone else seen this?</p>\n\n<p>Many thanks</p>\n\n<p>Paul</p>\n"},{"tags":["performance","active-directory","windows-authentication"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":20,"score":0,"question_id":13160019,"title":"Active Directory - application performance c#","body":"<p>I have application with windows authentication mode.</p>\n\n<p>On deploy apps i am synchronizing all users from active directory (security group) to my database.</p>\n\n<p>Well across my apps i need current logged user guid. Example : </p>\n\n<pre><code>var pc = new PrincipalContext(ContextType.Domain, AppSettings.DomainName);\n// hitting to ad\nvar user = UserPrincipal.FindByIdentity(pc, HttpContext.Current.User.Identity.Name);\nif (user != null &amp;&amp; user.Guid.HasValue)\n    return user.Guid.Value;\n</code></pre>\n\n<p>Well my question is about ad performance. Is really bad, when i need current logged user Guid call this query to ad?</p>\n\n<p>Thanks in advance</p>\n"},{"tags":["performance","entity-framework","entity-framework-4","linq-to-entities"],"answer_count":2,"favorite_count":3,"up_vote_count":6,"down_vote_count":0,"view_count":1140,"score":6,"question_id":5458762,"title":"Entity Framework queries speed","body":"<p>Recently I started to learning Entity Framework.</p>\n\n<p>First question made in my mind is: </p>\n\n<p>When we want to use LINQ to fetching data in EF, every query like this:</p>\n\n<pre><code>var a = from p in contacts select p.name ;\n</code></pre>\n\n<p>will be converts to SQL commands like this :</p>\n\n<pre><code>select name from contacts\n</code></pre>\n\n<ol>\n<li>does this converting repeat every time that we are querying?</li>\n<li>I heard that stored procedures are cached in database, does this event happens in LINQ queries in Entity Framework ?</li>\n</ol>\n\n<p>And at last is my question clear?</p>\n"},{"tags":["java","performance","optimization"],"answer_count":5,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":881,"score":1,"question_id":9745389,"title":"Is the ternary operator faster than an \"if\" condition","body":"<p>I am prone to \"<em>if-conditional syndrome</em>\" which means I tend to use if conditions all the time. I rarely ever use the ternary operator. For instance:\n</p>\n\n<pre class=\"lang-java prettyprint-override\"><code>//I like to do this:\nint a;\nif (i == 0)\n{\n    a = 10;\n}\nelse\n{\n    a = 5;\n}\n\n//When I could do this:\nint a = (i == 0) ? 10:5;\n</code></pre>\n\n<p>Does it matter which I use? Which is faster? Are there any notable performance differences? Is it a better practice to use the shortest code whenever possible?</p>\n\n<p>I use Java programming language.</p>\n"},{"tags":["java","performance","bufferedreader"],"answer_count":3,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":118,"score":3,"question_id":13155700,"title":"Fastest Way To Read and Write Large Files Line By Line in Java","body":"<p>I have been searching a lot for the fastest way to read and write again a large files (0.5 - 1 GB) in java with limited memory (about 64MB). Each line in the file represents a record, so I need to get them line by line. The file is a normal text file.</p>\n\n<p>I tried BufferedReader and BufferedWriter but it doesn't seem to be the best option. It takes about 35 seconds to read and write a file of size 0.5 GB, only read write with no processing. I think the bottleneck here is writing as reading alone takes about 10 seconds.</p>\n\n<p>I tried to read array of bytes, but then searching for lines in each array that was read takes more time.</p>\n\n<p>Any suggestions please?\nThanks</p>\n"},{"tags":["sql","performance","select","ssis"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":59,"score":0,"question_id":13153932,"title":"Does it matter where I SELECT fewer columns in SSIS in order to increase performance?","body":"<p>Say in a data flow task, I have an OLE DB source. I would like to increase the performance of the SSIS. Does it matter where I SELECT less columns?</p>\n\n<ol>\n<li><p>Create a view in database that SELECT less columns, use that as the source.</p></li>\n<li><p>Type SQL SELECT inside the source to select less columns.</p></li>\n<li><p>Choose the table then untick the columns inside the source.</p></li>\n</ol>\n\n<p>Thank you</p>\n"},{"tags":["performance","oracle","query-optimization"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":57,"score":0,"question_id":13153395,"title":"Same query, slow on Oracle 9, fast on Oracle 10","body":"<p>We have a table called <code>t_reading</code>, with the following schema:</p>\n\n<pre><code>MEAS_ASS_ID     NUMBER(12,0)\nREAD_DATE       DATE\nREAD_TIME       VARCHAR2(5 BYTE)\nNUMERIC_VAL     NUMBER\nCHANGE_REASON   VARCHAR2(240 BYTE)\nOLD_IND         NUMBER(1,0)\n</code></pre>\n\n<p>This table is indexed as follows:</p>\n\n<pre><code>CREATE INDEX RED_X4 ON T_READING\n(\n  \"OLD_IND\",\n  \"READ_DATE\" DESC,\n  \"MEAS_ASS_ID\",\n  \"READ_TIME\"\n)\n</code></pre>\n\n<p>This exact table (with the same data) exists on two servers, the only difference is the Oracle version installed on each one.</p>\n\n<p>The query in question is:</p>\n\n<pre><code>SELECT * FROM t_reading WHERE OLD_IND = 0 AND MEAS_ASS_ID IN (5022, 5003) AND read_date BETWEEN to_date('30/10/2012', 'dd/mm/yyyy') AND to_date('31/10/2012', 'dd/mm/yyyy');\n</code></pre>\n\n<p>This query executes in less than a second on Oracle 10, and around a minute in Oracle 9.</p>\n\n<p>Are we missing something?</p>\n\n<p>EDIT:</p>\n\n<p>Execution plan for Oracle 9:\n<img src=\"http://i.stack.imgur.com/u359A.jpg\" alt=\"enter image description here\"></p>\n\n<p>Execution plan for Oracle 10:\n<img src=\"http://i.stack.imgur.com/uryvp.jpg\" alt=\"enter image description here\"></p>\n"},{"tags":["javascript","jquery","ajax","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":63,"score":4,"question_id":13144144,"title":"jQuery request a list of url while limit the max number of concurrent request","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/4797566/queue-ajax-calls\">Queue AJAX calls</a>  </p>\n</blockquote>\n\n\n\n<p>I have a list of id:</p>\n\n<p><code>var ids = [3738, 75995, 927, ... ]; // length is about 2000</code></p>\n\n<p>I'd like to request the url <code>http://xx/ + id</code> with <code>$.getJSON</code>, like:</p>\n\n<pre><code>ids.forEach(function(id, index){\n    $.getJSON('http://xx/' + id, function(data){\n        // store the data in another array.\n    });\n});\n</code></pre>\n\n<p>However, this will make too much requests in one time, making the browser blocking for a while, so my question is, how could I limit the number of concurrent ajax request in jQuery? for example, I send 10 request and when each of them got the response I send another request.</p>\n"},{"tags":["c","performance","sse"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":49,"score":1,"question_id":13157773,"title":"SSE unsigned/signed subtraction of 16 bit register","body":"<p>I have a __m128i register (Vector A) with 16 bit values with the content:</p>\n\n<pre><code>{100,26,26,26,26,26,26,100} // A Vector\n</code></pre>\n\n<p>Now I subtract the vector </p>\n\n<pre><code>{82,82,82,82,82,82,82,82}\n</code></pre>\n\n<p>With the instruction </p>\n\n<pre><code>_mm_sub_epi16(a_vec,_mm_set1_epi16(82)) \n</code></pre>\n\n<p>The expected result should be the following vector </p>\n\n<pre><code>{18,-56,-56,-56,-56,-56,-56,18}\n</code></pre>\n\n<p>But I get </p>\n\n<pre><code>{18,65480,65480,65480,65480,65480,65480,18}\n</code></pre>\n\n<p>How can I solve that the vector is treated as signed? </p>\n\n<p>The A Vector was created by this instruction: </p>\n\n<pre><code>__m128i a_vec = _mm_srli_epi16(_mm_unpacklo_epi8(score_vec_8bit, score_vec_8bit), 8)\n</code></pre>\n"},{"tags":["performance","dynamic-languages"],"answer_count":11,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":1418,"score":4,"question_id":2198684,"title":"Are dynamic languages slower than static languages?","body":"<p>Are dynamic languages slower than static languages because, for example, the run-time has to check the type consistently?</p>\n"},{"tags":["performance","nsarray","nsdictionary","nspredicate"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":539,"score":3,"question_id":7408047,"title":"NSDictionary VS NSArray+NSPredicate: which is faster/recommented","body":"<p>What is faster getting to an object from a collection?</p>\n\n<p>a. Searching in an NSDictionary with <code>[dictionary objectForKey:key];</code>\nor</p>\n\n<p>b. Searching in an NSArray with <code>[NSPredicate predicateWithFormat:@\"someKey like %@\",someKeyValue];</code></p>\n\n<p>In both cases  I create the collections.</p>\n\n<p>Regards!</p>\n"},{"tags":["c++","performance","list","size"],"answer_count":2,"favorite_count":1,"up_vote_count":12,"down_vote_count":0,"view_count":277,"score":12,"question_id":13157164,"title":"Why isn't std::list.size() constant-time?","body":"<p>This code ran for 0.012 seconds:</p>\n\n<pre><code> std::list&lt;int&gt; list;\n list.resize(100);\n int size;\n for(int i = 0 ; i &lt; 10000; i++)\n     size = list.size();\n</code></pre>\n\n<p>This one for 9.378 seconds:</p>\n\n<pre><code> std::list&lt;int&gt; list;\n list.resize(100000);\n int size;\n for(int i = 0 ; i &lt; 10000; i++)\n     size = list.size();\n</code></pre>\n\n<p>In my opinion it would be possible to implement std::list in such way, that size would be stored in a private variable but according to this it is computed again each time I call size. Can anyone explain why?</p>\n"},{"tags":["sql","performance","linq"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":75,"score":2,"question_id":13155738,"title":"SQL vs LINQ performance","body":"<p>We currently have a self-made entity framework that relies on a DB-indipendent ORM.</p>\n\n<p>I have to build a software that batch-loads metadata in the DB for about 150 excel template (with info on cell position, cell type, formatting and more).</p>\n\n<p>I can operate</p>\n\n<ul>\n<li><p>via SQL batch (faster but less interactive)</p></li>\n<li><p>via building objects in memory, processing them with LINQ queries for various integrity checks, and then committing modifications to the DB</p></li>\n</ul>\n\n<p>I know that SQL is absolutely faster, but I would know... <strong>how much is it faster?</strong></p>\n\n<p>In detail, how much is a SQL query faster then a LINQ query <em>(assuming that all needed data has been already loaded in memory by ORM)</em> ?</p>\n"},{"tags":["java","performance","swing","garbage-collection","awt"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":29,"score":0,"question_id":13154879,"title":"While importing a full package how to prevent the unwanted classes loaded? What are the important places GC has to be performed when using awt swing?","body":"<p>I'm doing programs via notepad (windows 7)importing the whole package makes the code simple, but the program looks quite weighing when i used verbose command. While compiling and running my java class, a lot of unwanted class files to the code has been loaded, is there any functionality in java so that it would prevent the unwanted loading of class files?</p>\n\n<p>I knew there are IDE's but right now I'm feeling comfortable with notepad. So someone suggest if there is any functionality that avoids unwanted class loading to the program that I compile when a package is completely imported?</p>\n\n<p>and When should be GC performed(specific areas whether when adding components/setting action events)?</p>\n"},{"tags":["ruby-on-rails","performance","heroku","newrelic-rpm"],"answer_count":2,"favorite_count":2,"up_vote_count":5,"down_vote_count":0,"view_count":124,"score":5,"question_id":12181133,"title":"Strange TTFB (time to first byte) issue","body":"<p>We're in the process of improving performance of the our rails app hosted at Heroku (rails 3.2.8 and ruby 1.9.3). During this we've come across one alarming problem for which the source seems to be extremely difficult to track. Let me quickly explain how we experience the problem and how we've tried to isolate it.</p>\n\n<p>--</p>\n\n<p>Since around June we've experienced weird lag behavior in Time to First Byte all over the site. The problems is obvious from using the site (sometimes the application doesn't respond for 10-20 seconds), and it's also present in waterfall analysis via webpagetest.org.\nWe're based in Denmark but get this result from any host.</p>\n\n<p>To confirm the problem we've performed a benchmark test where we send 300 identical requests to a simple page and measured the response time.\nIf we send 300 requests to the front page the median response time is below 1 second, which is fairly good. What scares us is that 60 requests takes more that double that time and 40 of those takes more than 4 seconds. Some requests take as much as 16 seconds.</p>\n\n<p>None of these slow requests show up in New Relic, which we use for performance monitoring. No request queuing shows up and the results are the same no matter how high we scale our web processes.\nStill, we couldn't reject that the problem was caused by application code, so we tried another experiment where we responded to the request via rack middleware.</p>\n\n<p>By placing this middleware (TestMiddleware) at the beginning of the rack stack, we returned a request before it even hit the application, ensuring that none of the following middleware or the rails app could cause the delay.</p>\n\n<pre><code>Middleware setup:\n$ heroku run rake middleware\nuse Rack::Cache\nuse ActionDispatch::Static\nuse TestMiddleware\nuse Rack::Rewrite\nuse Rack::Lock\nuse Rack::Runtime\nuse Rack::MethodOverride\nuse ActionDispatch::RequestId\nuse Rails::Rack::Logger\nuse ActionDispatch::ShowExceptions\nuse ActionDispatch::DebugExceptions\nuse ActionDispatch::RemoteIp\nuse Rack::Sendfile\nuse ActionDispatch::Callbacks\nuse ActiveRecord::ConnectionAdapters::ConnectionManagement\nuse ActiveRecord::QueryCache\nuse ActionDispatch::Cookies\nuse ActionDispatch::Session::DalliStore\nuse ActionDispatch::Flash\nuse ActionDispatch::ParamsParser\nuse ActionDispatch::Head\nuse Rack::ConditionalGet\nuse Rack::ETag\nuse ActionDispatch::BestStandardsSupport\nuse NewRelic::Rack::BrowserMonitoring\nuse Rack::RailsExceptional\nuse OmniAuth::Builder\nrun AU::Application.routes\n</code></pre>\n\n<p>We then ran the same script to document response time and got pretty much the same result. The median response time was around 130ms (obviously faster because it doesn't hit the app. But still 60 requests took more than 400ms and 25 requests took more than 1 second. Again, with some requests as slow as 16 seconds.</p>\n\n<p>One explanation could be related to slow hops on the network or DNS setup, but the results of traceroute looks perfectly OK.</p>\n\n<p>This result was confirmed from running the response script on another rails 3.2 and ruby 1.9.3 application hosted on Heroku - no weird behavior at all.</p>\n\n<p>The DNS setup follows Heroku's recommendations.</p>\n\n<p>--</p>\n\n<p>We're confused to say the least. Could there be something fishy with Heroku's routing network?\nWhy the heck are we seeing this weird behavior? How do we get rid of it? And why can't we see it in New Relic?</p>\n"},{"tags":["sql","database","performance","sqlite","sqlite3"],"answer_count":6,"favorite_count":2,"up_vote_count":8,"down_vote_count":0,"view_count":1220,"score":8,"question_id":8988915,"title":"SQLite: COUNT slow on big tables","body":"<p>I'm having a performance problem in SQLite with a SELECT COUNT(*) on a large tables.</p>\n\n<p>As I didn't yet receive a usable answer and I did some further testing, I edited my question to incorporate my new findings.</p>\n\n<p>I have 2 tables:</p>\n\n<pre><code>CREATE TABLE Table1 (\nKey INTEGER NOT NULL,\n... several other fields ...,\nStatus CHAR(1) NOT NULL,\nSelection VARCHAR NULL,\nCONSTRAINT PK_Table1 PRIMARY KEY (Key ASC))\n\nCREATE Table2 (\nKey INTEGER NOT NULL,\nKey2 INTEGER NOT NULL,\n... a few other fields ...,\nCONSTRAINT PK_Table2 PRIMARY KEY (Key ASC, Key2 ASC))\n</code></pre>\n\n<p>Table1 has around 8 million records and Table2 has around 51 million records, and the databasefile is over 5GB.</p>\n\n<p>Table1 has 2 more indexes:</p>\n\n<pre><code>CREATE INDEX IDX_Table1_Status ON Table1 (Status ASC, Key ASC)\nCREATE INDEX IDX_Table1_Selection ON Table1 (Selection ASC, Key ASC)\n</code></pre>\n\n<p>\"Status\" is required field, but has only 6 distinct values, \"Selection\" is not required and has only around 1.5 million values different from null and only around 600k distinct values.</p>\n\n<p>I did some tests on both tables, you can see the timings below, and I added the \"explain query plan\" for each request (QP). I placed the database file on an USB-memorystick so i could remove it after each test and get reliable results without interference of the disk cache. Some requests are faster on USB (I suppose due to lack of seektime), but some are slower (table scans).</p>\n\n<pre><code>SELECT COUNT(*) FROM Table1\n    Time: 105 sec\n    QP: SCAN TABLE Table1 USING COVERING INDEX IDX_Table1_Selection(~1000000 rows)\nSELECT COUNT(Key) FROM Table1\n    Time: 153 sec\n    QP: SCAN TABLE Table1 (~1000000 rows)\nSELECT * FROM Table1 WHERE Key = 5123456\n    Time: 5 ms\n    QP: SEARCH TABLE Table1 USING INTEGER PRIMARY KEY (rowid=?) (~1 rows)\nSELECT * FROM Table1 WHERE Status = 73 AND Key &gt; 5123456 LIMIT 1\n    Time: 16 sec\n    QP: SEARCH TABLE Table1 USING INDEX IDX_Table1_Status (Status=?) (~3 rows)\nSELECT * FROM Table1 WHERE Selection = 'SomeValue' AND Key &gt; 5123456 LIMIT 1\n    Time: 9 ms\n    QP: SEARCH TABLE Table1 USING INDEX IDX_Table1_Selection (Selection=?) (~3 rows)\n</code></pre>\n\n<p>As you can see the counts are very slow, but normal selects are fast (except for the 2nd one, which took 16 seconds).</p>\n\n<p>The same goes for Table2:</p>\n\n<pre><code>SELECT COUNT(*) FROM Table2\n    Time: 528 sec\n    QP: SCAN TABLE Table2 USING COVERING INDEX sqlite_autoindex_Table2_1(~1000000 rows)\nSELECT COUNT(Key) FROM Table2\n    Time: 249 sec\n    QP: SCAN TABLE Table2 (~1000000 rows)\nSELECT * FROM Table2 WHERE Key = 5123456 AND Key2 = 0\n    Time: 7 ms\n    QP: SEARCH TABLE Table2 USING INDEX sqlite_autoindex_Table2_1 (Key=? AND Key2=?) (~1 rows)\n</code></pre>\n\n<p>Why is SQLite not using the automatically created index on the primary key on Table1 ?\nAnd why, when he uses the auto-index on Table2, it still takes a lot of time ?</p>\n\n<p>I created the same tables with the same content and indexes on SQL Server 2008 R2 and there the counts are nearly instantaneous.</p>\n\n<p>One of the comments below suggested executing ANALYZE on the database. I did and it took 11 minutes to complete.\nAfter that, I ran some of the tests again:</p>\n\n<pre><code>SELECT COUNT(*) FROM Table1\n    Time: 104 sec\n    QP: SCAN TABLE Table1 USING COVERING INDEX IDX_Table1_Selection(~7848023 rows)\nSELECT COUNT(Key) FROM Table1\n    Time: 151 sec\n    QP: SCAN TABLE Table1 (~7848023 rows)\nSELECT * FROM Table1 WHERE Status = 73 AND Key &gt; 5123456 LIMIT 1\n    Time: 5 ms\n    QP: SEARCH TABLE Table1 USING INTEGER PRIMARY KEY (rowid&gt;?) (~196200 rows)\nSELECT COUNT(*) FROM Table2\n    Time: 529 sec\n    QP: SCAN TABLE Table2 USING COVERING INDEX sqlite_autoindex_Table2_1(~51152542 rows)\nSELECT COUNT(Key) FROM Table2\n    Time: 249 sec\n    QP: SCAN TABLE Table2 (~51152542 rows)\n</code></pre>\n\n<p>As you can see, the queries took the same time (except the query plan is now showing the real number of rows), only the slower select is now also fast.</p>\n\n<p>Next, I create dan extra index on the Key field of Table1, which should correspond to the auto-index. I did this on the original database, without the ANALYZE data. It took over 23 minutes to create this index (remember, this is on an USB-stick).</p>\n\n<pre><code>CREATE INDEX IDX_Table1_Key ON Table1 (Key ASC)\n</code></pre>\n\n<p>Then I ran the tests again:</p>\n\n<pre><code>SELECT COUNT(*) FROM Table1\n    Time: 4 sec\n    QP: SCAN TABLE Table1 USING COVERING INDEX IDX_Table1_Key(~1000000 rows)\nSELECT COUNT(Key) FROM Table1\n    Time: 167 sec\n    QP: SCAN TABLE Table2 (~1000000 rows)\nSELECT * FROM Table1 WHERE Status = 73 AND Key &gt; 5123456 LIMIT 1\n    Time: 17 sec\n    QP: SEARCH TABLE Table1 USING INDEX IDX_Table1_Status (Status=?) (~3 rows)\n</code></pre>\n\n<p>As you can see, the index helped with the count(*), but not with the count(Key).</p>\n\n<p>Finaly, I created the table using a column constraint instead of a table constraint:</p>\n\n<pre><code>CREATE TABLE Table1 (\nKey INTEGER PRIMARY KEY ASC NOT NULL,\n... several other fields ...,\nStatus CHAR(1) NOT NULL,\nSelection VARCHAR NULL)\n</code></pre>\n\n<p>Then I ran the tests again:</p>\n\n<pre><code>SELECT COUNT(*) FROM Table1\n    Time: 6 sec\n    QP: SCAN TABLE Table1 USING COVERING INDEX IDX_Table1_Selection(~1000000 rows)\nSELECT COUNT(Key) FROM Table1\n    Time: 28 sec\n    QP: SCAN TABLE Table1 (~1000000 rows)\nSELECT * FROM Table1 WHERE Status = 73 AND Key &gt; 5123456 LIMIT 1\n    Time: 10 sec\n    QP: SEARCH TABLE Table1 USING INDEX IDX_Table1_Status (Status=?) (~3 rows)\n</code></pre>\n\n<p>Although the query plans are the same, the times are a lot better. Why is this ?</p>\n\n<p>The problem is that ALTER TABLE does not permit to convert an existing table and I have a lot of existing databases which i can not convert to this form. Besides, using a column contraint instead of table constraint won't work for Table2.</p>\n\n<p>Has anyone any idea what I am doing wrong and how to solve this problem ?</p>\n\n<p>I used System.Data.SQLite version 1.0.74.0 to create the tables and to run the tests I used SQLiteSpy 1.9.1.</p>\n\n<p>Thanks,</p>\n\n<p>Marc</p>\n"},{"tags":["c#","performance","debugging","release"],"answer_count":10,"favorite_count":18,"up_vote_count":58,"down_vote_count":0,"view_count":12780,"score":58,"question_id":2446027,"title":"C# debug vs release performance","body":"<p>I've encountered the following paragraph:</p>\n\n<p>“Debug vs Release setting in the IDE when you compile your code in Visual Studio makes almost no difference to performance… the generated code is almost the same. The C# compiler doesn’t really do any optimisation. The C# compiler just spits out IL… and at the runtime it’s the JITer that does all the optimisation. The JITer does have a Debug/Release mode and that makes a huge difference to performance. But that doesn’t key off whether you run the Debug or Release configuration of your project, that keys off whether a debugger is attached.”</p>\n\n<p>The source is <a href=\"http://andrewmyhre.wordpress.com/2008/05/13/c-debugrelease-and-performance/\">here</a> and the podcast is <a href=\"http://go2.wordpress.com/?id=725X1342&amp;site=andrewmyhre.wordpress.com&amp;url=http%3A%2F%2Fwww.microsoft.com%2Fdownloads%2Fdetails.aspx%3FFamilyId%3DB11AD912-4158-44CC-A771-A5E044F7E3BB%26displaylang%3Den\">here</a>.</p>\n\n<p>Can someone direct me to a Microsoft article that can actually prove this?</p>\n\n<p>Googling \"C# debug vs release performance\" mostly returns results saying \"Debug has a lot of performance hit\", \"release is optimized\", and \"don't deploy debug to production.\"</p>\n"},{"tags":["performance","hibernate","jpa","openjpa","timesten"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":26,"score":0,"question_id":13152822,"title":"performance issue in find() method after migration to Hibernate 4.0 from OpenJPA 1.2","body":"<p>I migrate from OpenJPA 1.2 to Hiberante 4.0<br>\nI'm using TimesTen DB  </p>\n\n<p>I'm doing a native query to get id's of object's that I need , and then perform find on each on of them.\nIn <code>OpenJPA</code> instead of find I used <code>findCache()</code> method and if it return null I use the <code>find()</code> method , In hibernate I used only the <code>find()</code> method.</p>\n\n<p>I performed this operation on the same DB.</p>\n\n<p>after running couple of test I saw that the performance of OpenJPA is far better.</p>\n\n<p>I printed the statistics of hibernate session ( after querying and finding the same object's) and saw that the <code>hit\\miss</code> count to the first level cache is always 0.\nwhile the OpenJPA is clearly reaching it's cache by fetching object's with the <code>findCache</code> method.</p>\n\n<p>How can I improve the performance of find in Hibernate ?\nI suspect it referred to the difference in the first level cache implementation of this tools.</p>\n\n<p>another fact: I use the same EntityManager for the application run time ( I need to minimize the cost of creating of an EntityManager - my app is soft real time )</p>\n\n<p>thanks.</p>\n"},{"tags":["sql","sql-server","performance","tsql"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":76,"score":0,"question_id":13140520,"title":"Simple SQL Server query across databases runs extremely slow R2","body":"<p>I have two databases on a local machine, connected to <code>localhost</code>.  They both have roughly two million rows a piece.  I was doing the following very simple join and it took over a minute to complete.  </p>\n\n<pre><code>select distinct x.patid\n    from [i 3 sci study].dbo.clm_extract as x\n    left join [i 3 study].dbo.claims as y on y.patid=x.patid\n    where y.patid is null\n</code></pre>\n\n<p>When I looked at the execution plan I saw that the join showplan operator had this to say\n<img src=\"http://i.stack.imgur.com/eQAu6.jpg\" alt=\"enter image description here\"></p>\n\n<p>Why is the actual number of rows so exorbitantly high compared to the actual number of rows in both tables?</p>\n"},{"tags":["asp.net","performance","google","pagespeed"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":18,"score":0,"question_id":13152672,"title":"How do I improve my asp.net site when the Google speed test says \"expiration not specified\"?","body":"<p>I have taken different speed tests, including using <a href=\"https://developers.google.com/speed/pagespeed/insights\" rel=\"nofollow\">Google's PageSpeed Insights</a>.  For every image on every page, it warned me that I should specify heights and widths on every image, but more importantly, it said \"<strong>expiration not specified</strong>\" after every image.  It provided a link to suggestions on how to fix this, like use <strong>HTTP cache headers</strong>, but I didn't understand this at all.  I have 47 pages, many of them have 10+ pictures on a page.  </p>\n\n<p>All the images are optimized, but since I'm on an asp.net website with 5 master pages, what can I do?  I've seen solutions for php pages.  I don't even know what \"expiration not specified\" means.  Is there something I should just put in the head of my master pages, or do I need to do some type of coding on every image in my website?  Any guidance in this regard would be greatly appreciated!</p>\n"},{"tags":["sql-server","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":423,"score":0,"question_id":2042484,"title":"Ways to improve sql server query optimizer results","body":"<p>The question is quite simple: <b>What can I do to make sure that the SQL Server query optimizer has all the information it needs to choose the \"best\" query plan?</b></p>\n\n<p>The background of this question is that recently we've run into more and more cases where SQL Server chooses a bad query plan, i.e., cases, where adding query hints, join hints or explicitly using temporary tables instead of \"one big SQL\" drastically improved performance. I'm actually quite surprised that the query optimizer delivers such a lot of bad results, so I'm wondering whether we did something wrong. No indexes are missing (according to query analyzer and common sense), and statistics are updated frequently by a maintainance task.</p>\n\n<p>Let me emphasize that I am not talking about missing indexes here! I'm talking about the situation where there is a \"good\" and a \"bad\" query plan (given the current state of the DB), and SQL Server chooses a \"bad\" plan although the indexes present would allow it to use a \"good\" plan. I'm wondering whether there is some possibility to <em>improve</em> the results of the query optimizer without having to optimize all queries manually (with query hints or USE PLAN).</p>\n"},{"tags":["java","performance","compiler","gwt"],"answer_count":11,"favorite_count":45,"up_vote_count":90,"down_vote_count":0,"view_count":38286,"score":90,"question_id":1011863,"title":"How do I speed up the gwt compiler?","body":"<p>We're starting to make heavier use of GWT in our projects, and the performance of the GWT compiler is becoming increasingly annoying. </p>\n\n<p>We're going to start altering our working practices to mitigate the problem, including a greater emphasis on the hosted-mode browser, which defers the need to run the GWT compiler until a later time, but that brings its own risks, particularly that of not catching issues with real browsers until much later than we'd like.</p>\n\n<p>Ideally, we'd like to make the GWT compiler itself quicker - a minute to compile a fairly small application is taking the piss. However, we are using the compile if a fairly naive fashion, so I'm hoping we can make some quick and easy gains.</p>\n\n<p>We're currently invoking com.google.gwt.dev.Compiler as a java application from ant Ant target, with 256m max heap and lots of stack space. The compiler is launched by Ant using fork=true and the latest Java 6 JRE, to try and take advantage of Java6's improved performance. We pass our main controller class to the compiler along with the application classpath, and off it goes.</p>\n\n<p>What else can we do to get some extra speed? Can we give it more information so it spends less time doing discovery of what to do?</p>\n\n<p>I know we can tell it to only compile for one browser, but we need to do multi-browser testing, so that's not really practical.</p>\n\n<p>All suggestions welcome at this point.</p>\n"},{"tags":["performance","zoom","sample","graphicsmagick"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":7,"score":0,"question_id":13150758,"title":"GraphicsMagick zoom and sample function performance difference","body":"<p>We use GM to do image compressing, and observe a significant performance difference between zoom and sample functions, which we suppose to do the same thing.</p>\n\n<p>When using zoom function for a 68k image, a process could consume all the cpu time for all cores(12 core, 2.4G Hz machine), the throughput is 65/seconds, response time is 469 ms on average, load by top command is around 11, cpu usage is close to 100% </p>\n\n<p>When using sample function in the same environment, 24 process work together providing throughput close to 1000/seconds, response time is 37 ms on average, load by top command is around 3, cpu usage fluctuates between 50% and 80%</p>\n\n<p>the official document for these two functions is very simple as below:</p>\n\n<p>sample\nResize image by using pixel sampling algorithm:</p>\n\n<p>void            sample ( const Geometry &amp;geometry_ )</p>\n\n<p>zoom\nZoom (resize) image to specified size:</p>\n\n<p>void            zoom ( const Geometry &amp;geometry_ )</p>\n\n<p>the effects after the image processing are similar, but the difference is huge.</p>\n\n<ol>\n<li><p>Could anyone explain the different circumstances of using these two functions, since we might choose sample over zoom because of the performance issue</p></li>\n<li><p>Further, could anyone tell me why zoom is so cpu-time-consuming.</p></li>\n</ol>\n"},{"tags":["c#","c++","performance","for-loop"],"answer_count":5,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":88,"score":1,"question_id":13150001,"title":"What's the benefit of declaring for the loop index variable outside the loop?","body":"<p>I see this in a lot of game engine code. Is this supposed to be faster than if you were to declare it in the for loop body? Also this is followed by many other for loops, each of them using the same variable.</p>\n\n<pre><code>int i;\nfor(i=0; i&lt;count; ++i)\n{\n}\n</code></pre>\n\n<p>vs</p>\n\n<pre><code>for(int i=0; i&lt;count; ++i)\n{\n}\n</code></pre>\n\n<p>Btw I never do this myself, just curious about the idea behind it, since apart from performance I don't know why anyone would do this.</p>\n"},{"tags":["performance","r","matlab"],"answer_count":1,"favorite_count":0,"up_vote_count":4,"down_vote_count":2,"view_count":118,"score":2,"question_id":13142273,"title":"Discrepancy between R and Matlab speed","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/7142767/why-are-loops-slow-in-r\">Why are loops slow in R?</a>  </p>\n</blockquote>\n\n\n\n<p>Consider the following task. A dataset has 40 variables for 20,000 \"users\". Each user has between 1 and 150 observations. All users are stacked in a matrix called data. The first column is the id of the user and identifies the user. All id are stored in a 20,000 X 1 matrix called userid.</p>\n\n<p>Consider the following R code</p>\n\n<pre><code>useridl = length(userid)\nitime=proc.time()[3]    \nfor (i in 1:useridl) {\ntemp =data[data[,1]==userid[i],]\n   }\n etime=proc.time()[3]\n etime-itime\n</code></pre>\n\n<p>This code just goes through the 20,000 users, creating the temp matrix every time. With the subset of observations belonging to userid[i]. It takes about 6 minutes in a MacPro.</p>\n\n<p>In MatLab, the same task </p>\n\n<pre><code>tic\nfor i=1:useridl\ntemp=data(data(:,1)==userid(i),:);\nend\ntoc\n</code></pre>\n\n<p>takes 1 minute. </p>\n\n<p>Why is R so much slower? This is standard task, I am using matrices in both cases. Any ideas?</p>\n"},{"tags":["performance","algorithm","math","prime-factoring"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":91,"score":2,"question_id":13145636,"title":"number of divisors of a number which are not smaller than another number","body":"<p>Is there any efficient way to find the number of divisors of a number (say n) which are not smaller than another number (say m).\nn can be up to 10^12.\ni thought about sieve algorithm &amp; then find the number of divisors.\nmy method check all the numbers from m to square root of n.\nBut i think there is another way(efficient) to do that .</p>\n"},{"tags":["java","performance","optimization"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":29,"score":1,"question_id":13148544,"title":"Do Java JVMs perform any specialization or partial evaluation type optimizations?","body":"<p>Do any of the Java JVMs implement specialization or partial evaluation type optimizations? </p>\n\n<p>Does the final keyword get used for these optimizations? </p>\n"},{"tags":["django","performance","internet-explorer","loading"],"answer_count":4,"favorite_count":0,"up_vote_count":8,"down_vote_count":0,"view_count":556,"score":8,"question_id":6046625,"title":"Django: IE doesn't load locahost or loads very SLOWLY","body":"<p>I'm just starting to learn Django, building a project on my computer, running Windows 7 64-bit, Python 2.7, Django 1.3.</p>\n\n<p>Basically whatever I write, it loads in Chrome and Firefox instantly. But for IE (version 9), it just stalls there, and does nothing. I can load up \"http://127.0.0.1:8000\" on IE and leave the computer on for hours and it doesn't load. Sometimes, when I refresh a couple of times or restart IE it'll work. If I change something in the code, again, Chrome and Firefox reflects changes instantly, whereas IE doesn't - if it loads the page at all. </p>\n\n<p>What is going on? I'm losing my mind here.... </p>\n"},{"tags":["sql","performance","azure","sql-azure"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":68,"score":0,"question_id":13148473,"title":"SQL Azure - very slow compared to localhost database","body":"<p>I decided I wanted to try out Microsoft SQL Azure, as many people have talked very highly about it. It should be fast, flexible, cheap and many other things.</p>\n\n<p>I got it up and running, migrated my data to Azure and hooked up the connectionstring. I tried to run some queries on the database, and was shocked about how slow even simple queries were. A \"SELECT *\" from a table with 700 rows took 7 seconds. My page also seems extremely slow, compared to when I used a localhost managent studio or a database on a shared hosting. </p>\n\n<p>Now, when I setup my server, I couldn't pick a physical position. However, I live in Denmark, and I can see the server is the \"South Central US\". This might be the issue.</p>\n\n<p>I don't use any stored procedures (so I guess no parameter sniffing).. I can also see my indexes is transfered succesfully.</p>\n\n<p>Any ideas on what to do? Any performance things I am missing? </p>\n"},{"tags":["c","performance","optimization","x86","avx"],"answer_count":0,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":40,"score":2,"question_id":13148420,"title":"When should prefetch be used on modern machines?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/7327994/prefetching-examples\">Prefetching Examples?</a>  </p>\n</blockquote>\n\n\n\n<p>In many cases prefetch instructions seem to slow performance on modern machines, because there are typically a few different hardware prefetch units. </p>\n\n<p>Are there any particular cases where it always helps to use prefetch instructions? </p>\n"},{"tags":["performance","postgresql","index"],"answer_count":0,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":72,"score":4,"question_id":13130535,"title":"PostgreSQL Index optimization with dates","body":"<p>I have a large table of objects (15M rows +), for which I want to query for outdated field. PostgreSQL (9.0.8)</p>\n\n<p>I want to divide the query by millions, for scalability &amp; concurrency purposes, and I want to fetch all data with the updated_at field with a date of a few days ago.</p>\n\n<p>I have tried many indexes, and queries, on a million ids, and I can't seem to get performance under 100seconds. (with heroku's Ronin hardware).</p>\n\n<p>I am looking for suggestions I haven't tried to make this as efficient as possible.</p>\n\n<p>TRY #1</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE (date(updated_at)) &lt; (date(now())-7) AND id &gt;= 5000001 AND id &lt; 6000001;\n INDEX USED : (date(updated_at),id)\n 268578.934 ms\n</code></pre>\n\n<p>TRY #2</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE ((date(now()) - (date(updated_at)) &gt; 7)) AND id &gt;= 5000001 AND id &lt; 6000001;\n INDEX USED : primary key\n 335555.144 ms\n</code></pre>\n\n<p>TRY #3</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE (date(updated_at)) &lt; (date(now())-7) AND id/1000000 = 5;\n INDEX USED : (date(updated_at),(id/1000000))\n 243427.042 ms\n</code></pre>\n\n<p>TRY #4</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE (date(updated_at)) &lt; (date(now())-7) AND id/1000000 = 5 AND updated_at IS NOT NULL;\n INDEX USED : (date(updated_at),(id/1000000)) WHERE updated_at IS NOT NULL \n 706714.812 ms\n</code></pre>\n\n<p>TRY #5 (for a single month of outdated data)</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE (EXTRACT(MONTH from date(updated_at)) = 8) AND id/1000000 = 5 ;\n INDEX USED : (EXTRACT(MONTH from date(updated_at)),(id/1000000))\n 107241.472 ms\n</code></pre>\n\n<p>TRY #6</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE (date(updated_at)) &lt; (date(now())-7) AND id/1000000 = 5 ;\n INDEX USED : ( (id/1000000 ) ASC ,updated_at DESC NULLS LAST)\n 106842.395 ms\n</code></pre>\n\n<p>TRY #7</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE id/1000000 = 5 and (date(updated_at)) &lt; (date(now())-7)  ;\n INDEX USED : ( (id/1000000 ) ASC ,date(updated_at) DESC NULLS LAST);\n 100732.049 ms\n Second try: 87280.728 ms \n     http://explain.depesz.com/s/DQP\n</code></pre>\n\n<p>TRY #8</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE (date(updated_at)) &lt; (date(now())-7) AND id/1000000 = 5 AND updated_at IS NOT NULL;\n INDEX USED :  ( (id/1000000 ) ASC ,date(updated_at) ASC NULLS LAST);\n 129133.022 ms\n</code></pre>\n\n<p>TRY #9 (with partial index, as per Erwin's suggestion)</p>\n\n<pre><code> EXPLAIN ANALYZE SELECT count(*) FROM objects WHERE id BETWEEN 5000000 AND 5999999 AND (date(updated_at)) &lt; '2012-10-23'::date;\n INDEX USED : (date(updated_at) DESC NULLS LAST) WHERE id BETWEEN 5000000 AND 6000000 AND date(updated_at) &lt; '2012-10-23'::date ;\n 73861.047 ms\n http://explain.depesz.com/s/p9A\n</code></pre>\n\n<p>============================\nUPDATE (db settings)</p>\n\n<pre><code> select name,min_val, max_val, boot_val from pg_settings;\n\n              name               |  min_val  |   max_val    |     boot_val      \n---------------------------------+-----------+--------------+-------------------\n allow_system_table_mods         |           |              | off\n application_name                |           |              | \n archive_command                 |           |              | \n archive_mode                    |           |              | off\n  archive_timeout                 | 0         | 2147483647   | 0\n array_nulls                     |           |              | on\n authentication_timeout          | 1         | 600          | 60\n autovacuum                      |           |              | on\n autovacuum_analyze_scale_factor | 0         | 100          | 0.1\n autovacuum_analyze_threshold    | 0         | 2147483647   | 50\n autovacuum_freeze_max_age       | 100000000 | 2000000000   | 200000000\n autovacuum_max_workers          | 1         | 536870911    | 3\n autovacuum_naptime              | 1         | 2147483      | 60\n autovacuum_vacuum_cost_delay    | -1        | 100          | 20\n autovacuum_vacuum_cost_limit    | -1        | 10000        | -1\n autovacuum_vacuum_scale_factor  | 0         | 100          | 0.2\n autovacuum_vacuum_threshold     | 0         | 2147483647   | 50\n backslash_quote                 |           |              | safe_encoding\n bgwriter_delay                  | 10        | 10000        | 200\n bgwriter_lru_maxpages           | 0         | 1000         | 100\n bgwriter_lru_multiplier         | 0         | 10           | 2\n block_size                      | 8192      | 8192         | 8192\n bonjour                         |           |              | off\n bonjour_name                    |           |              | \n bytea_output                    |           |              | hex\n check_function_bodies           |           |              | on\n checkpoint_completion_target    | 0         | 1            | 0.5\n checkpoint_segments             | 1         | 2147483647   | 3\n checkpoint_timeout              | 30        | 3600         | 300\n checkpoint_warning              | 0         | 2147483647   | 30\n client_encoding                 |           |              | SQL_ASCII\n client_min_messages             |           |              | notice\n commit_delay                    | 0         | 100000       | 0\n commit_siblings                 | 1         | 1000         | 5\n constraint_exclusion            |           |              | partition\n cpu_index_tuple_cost            | 0         | 1.79769e+308 | 0.005\n cpu_operator_cost               | 0         | 1.79769e+308 | 0.0025\n cpu_tuple_cost                  | 0         | 1.79769e+308 | 0.01\n cursor_tuple_fraction           | 0         | 1            | 0.1\n custom_variable_classes         |           |              | \n DateStyle                       |           |              | ISO, MDY\n db_user_namespace               |           |              | off\n deadlock_timeout                | 1         | 2147483      | 1000\n debug_assertions                |           |              | off\n debug_pretty_print              |           |              | on\n debug_print_parse               |           |              | off\n debug_print_plan                |           |              | off\n debug_print_rewritten           |           |              | off\n default_statistics_target       | 1         | 10000        | 100\n default_tablespace              |           |              | \n default_text_search_config      |           |              | pg_catalog.simple\n default_transaction_isolation   |           |              | read committed\n default_transaction_read_only   |           |              | off\n default_with_oids               |           |              | off\n effective_cache_size            | 1         | 2147483647   | 16384\n effective_io_concurrency        | 0         | 1000         | 1\n enable_bitmapscan               |           |              | on\n enable_hashagg                  |           |              | on\n enable_hashjoin                 |           |              | on\n enable_indexscan                |           |              | on\n enable_material                 |           |              | on\n enable_mergejoin                |           |              | on\n enable_nestloop                 |           |              | on\n enable_seqscan                  |           |              | on\n enable_sort                     |           |              | on\n enable_tidscan                  |           |              | on\n escape_string_warning           |           |              | on\n extra_float_digits              | -15       | 3            | 0\n from_collapse_limit             | 1         | 2147483647   | 8\n fsync                           |           |              | on\n full_page_writes                |           |              | on\n geqo                            |           |              | on\n geqo_effort                     | 1         | 10           | 5\n  geqo_generations                | 0         | 2147483647   | 0\n  geqo_pool_size                  | 0         | 2147483647   | 0\n  geqo_seed                       | 0         | 1            | 0\n  geqo_selection_bias             | 1.5       | 2            | 2\n  geqo_threshold                  | 2         | 2147483647   | 12\n  gin_fuzzy_search_limit          | 0         | 2147483647   | 0\n  hot_standby                     |           |              | off\n  ignore_system_indexes           |           |              | off\n  integer_datetimes               |           |              | on\n  IntervalStyle                   |           |              | postgres\n  join_collapse_limit             | 1         | 2147483647   | 8\n  krb_caseins_users               |           |              | off\n  krb_srvname                     |           |              | postgres\n  lc_collate                      |           |              | C\n  lc_ctype                        |           |              | C\n  lc_messages                     |           |              |\n  lc_monetary                     |           |              | C\n  lc_numeric                      |           |              | C\n  lc_time                         |           |              | C\n  listen_addresses                |           |              | localhost\n  lo_compat_privileges            |           |              | off\n  local_preload_libraries         |           |              |\n  log_autovacuum_min_duration     | -1        | 2147483      | -1\n  log_checkpoints                 |           |              | off\n  log_connections                 |           |              | off\n  log_destination                 |           |              | stderr\n  log_disconnections              |           |              | off\n  log_duration                    |           |              | off\n  log_error_verbosity             |           |              | default\n  log_executor_stats              |           |              | off\n  log_hostname                    |           |              | off\n  log_line_prefix                 |           |              |\n  log_lock_waits                  |           |              | off\n  log_min_duration_statement      | -1        | 2147483      | -1\n  log_min_error_statement         |           |              | error\n  log_min_messages                |           |              | warning\n  log_parser_stats                |           |              | off\n  log_planner_stats               |           |              | off\n  log_rotation_age                | 0         | 35791394     | 1440\n  log_rotation_size               | 0         | 2097151      | 10240\n  log_statement                   |           |              | none\n  log_statement_stats             |           |              | off\n  log_temp_files                  | -1        | 2147483647   | -1\n  log_timezone                    |           |              | UNKNOWN\n  log_truncate_on_rotation        |           |              | off\n  logging_collector               |           |              | off\n  maintenance_work_mem            | 1024      | 2097151      | 16384\n  max_connections                 | 1         | 536870911    | 100\n  max_files_per_process           | 25        | 2147483647   | 1000\n  max_function_args               | 100       | 100          | 100\n  max_identifier_length           | 63        | 63           | 63\n  max_index_keys                  | 32        | 32           | 32\n  max_locks_per_transaction       | 10        | 2147483647   | 64\n  max_prepared_transactions       | 0         | 536870911    | 0\n  max_stack_depth                 | 100       | 2097151      | 100\n  max_standby_archive_delay       | -1        | 2147483      | 30000\n  max_standby_streaming_delay     | -1        | 2147483      | 30000\n  max_wal_senders                 | 0         | 536870911    | 0\n  password_encryption             |           |              | on\n  port                            | 1         | 65535        | 5432\n  post_auth_delay                 | 0         | 2147483647   | 0\n  pre_auth_delay                  | 0         | 60           | 0\n  random_page_cost                | 0         | 1.79769e+308 | 4\n  search_path                     |           |              | \"$user\",public\n  segment_size                    | 131072    | 131072       | 131072\n  seq_page_cost                   | 0         | 1.79769e+308 | 1\n  server_encoding                 |           |              | SQL_ASCII\n  server_version                  |           |              | 9.0.8\n  server_version_num              | 90008     | 90008        | 90008\n  session_replication_role        |           |              | origin\n  shared_buffers                  | 16        | 1073741823   | 1024\n  silent_mode                     |           |              | off\n  sql_inheritance                 |           |              | on\n  ssl                             |           |              | off\n  ssl_renegotiation_limit         | 0         | 2097151      | 524288\n  standard_conforming_strings     |           |              | off\n  statement_timeout               | 0         | 2147483647   | 0\n  superuser_reserved_connections  | 0         | 536870911    | 3\n  synchronize_seqscans            |           |              | on\n  synchronous_commit              |           |              | on\n  syslog_facility                 |           |              | local0\n  syslog_ident                    |           |              | postgres\n  tcp_keepalives_count            | 0         | 2147483647   | 0\n  tcp_keepalives_idle             | 0         | 2147483647   | 0\n  tcp_keepalives_interval         | 0         | 2147483647   | 0\n  temp_buffers                    | 100       | 1073741823   | 1024\n  temp_tablespaces                |           |              |\n  TimeZone                        |           |              | UNKNOWN\n  timezone_abbreviations          |           |              | UNKNOWN\n  trace_notify                    |           |              | off\n  trace_recovery_messages         |           |              | log\n  trace_sort                      |           |              | off\n  track_activities                |           |              | on\n  track_activity_query_size       | 100       | 102400       | 1024\n  track_counts                    |           |              | on\n  track_functions                 |           |              | none\n  transaction_isolation           |           |              |\n  transaction_read_only           |           |              | off\n  transform_null_equals           |           |              | off\n  unix_socket_group               |           |              |\n  unix_socket_permissions         | 0         | 511          | 511\n  update_process_title            |           |              | on\n  vacuum_cost_delay               | 0         | 100          | 0\n  vacuum_cost_limit               | 1         | 10000        | 200\n  vacuum_cost_page_dirty          | 0         | 10000        | 20\n  vacuum_cost_page_hit            | 0         | 10000        | 1\n  vacuum_cost_page_miss           | 0         | 10000        | 10\n  vacuum_defer_cleanup_age        | 0         | 1000000      | 0\n  vacuum_freeze_min_age           | 0         | 1000000000   | 50000000\n  vacuum_freeze_table_age         | 0         | 2000000000   | 150000000\n  wal_block_size                  | 8192      | 8192         | 8192\n  wal_buffers                     | 4         | 2147483647   | 8\n  wal_keep_segments               | 0         | 2147483647   | 0\n  wal_level                       |           |              | minimal\n  wal_segment_size                | 2048      | 2048         | 2048\n  wal_sender_delay                | 1         | 10000        | 200\n  wal_sync_method                 |           |              | fdatasync\n  wal_writer_delay                | 1         | 10000        | 200\n  work_mem                        | 64        | 2097151      | 1024\n  xmlbinary                       |           |              | base64\n  xmloption                       |           |              | content\n  zero_damaged_pages              |           |              | off\n  (195 rows) \n</code></pre>\n"},{"tags":["performance","azure"],"answer_count":11,"favorite_count":16,"up_vote_count":21,"down_vote_count":1,"view_count":7967,"score":20,"question_id":2711868,"title":"azure performance","body":"<p>I've moved my app from a dedicated server to azure (and sql azure), and have noticed substantial performance degradation.  </p>\n\n<p>obviously not having the database and web server on the same piece of hardware is much of it, but I'm curious what other people have found in migrating to azure, and if there is anything any of you would suggest I do to improve it.  Right now I'm considering moving back to my dedicated server...</p>\n\n<p>So in summary, are there any rules of thumb for this, existing research (wasn't able to find much) or other pieces of advice on improving the performance of the app?  has anyone else found the same to be true, and improved their site's performance in some way?  it's built in C# on asp.net mvc 2.</p>\n\n<p>Thanks!</p>\n"},{"tags":["javascript","performance","caching"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":13146657,"title":"Does the result of a function run several times is cached?","body":"<p>In JavaScript, I've an object with an array, and a method wich gets a slice of that array and a concatenation with another array.</p>\n\n<p>If that method is run several times in the same function to return always the same value, does the performance will be faster after of the first run (due to the result will be cached in CPU cache)?</p>\n"},{"tags":["performance","cassandra","scaling"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":41,"score":0,"question_id":13142819,"title":"Cassandra scaling cheat-sheet","body":"<p>Of course you can only know the performance of your system with your load with your use-cases by ... actually implementing it!  That aside, before embarking on a prototype, I'm searching for some very rough estimates of how Cassandra performs.</p>\n\n<p>For various configurations of nodes and data-centres, and for various read and write consistency levels, what the chances of reading a stale value?  What kind of key reads and writes per second would you expect to sustain, and what kind of latency would each read and write have?</p>\n"},{"tags":["android","performance","cpu","benchmarking","cpu-speed"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":24,"score":1,"question_id":13147168,"title":"Whetstone CPU performance algorithm","body":"<p>I'm going to develop an application for android that uses the whetstone algorithm to measure CPU performance. I've chosen the Whetstone algorithm because my research tells me it's an appropriate way of measure performance of less powerful cpu's.</p>\n\n<p>I've got the source code and pseudo code(from the sweet 60's) for the whetstone algorithm, and so far so good.\nBut the whole whetstone algorithm seem's a bit secret, and it's hard to find useful information about it. So my questions are:</p>\n\n<ul>\n<li>Why are the Whetstone algorithm suitable to measure performance of less powerful cpu's?</li>\n<li>In brief, can anyone tell me what exactly makes an cpu performance algorithm being a whetstone algorithm?</li>\n<li>Can anyone explain in short, the pseudo code of the Whetstone algorithm?</li>\n</ul>\n\n<p>Answer on any of this questions is really appreciated?</p>\n"},{"tags":["android","performance","testing","android-emulator","emulator"],"answer_count":11,"favorite_count":20,"up_vote_count":58,"down_vote_count":0,"view_count":29470,"score":58,"question_id":2662650,"title":"Making the Android emulator run faster","body":"<p>The Android emulator is a bit sluggish. For some devices, like the Motorola Droid and the Nexus One, the app runs faster in the actual device than the emulator. This is a problem when testing games and visual effects.</p>\n\n<p>How do you make the emulator run as fast as possible? I've been toying with its parameters but haven't found a configuration that shows a noticeable improvement yet.</p>\n"},{"tags":["javascript","performance","dom","browser"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":20,"score":1,"question_id":13146746,"title":"What's the fastest way to set DOMElement bounds?","body":"<p>Currently, to place an absolute element in DOM, I use :</p>\n\n<pre><code>this.myObject.style.left = aValue1 + 'px' ;\nthis.myObject.style.top = aValue2 + 'px' ;\nthis.myObject.style.width = aValue3 + 'px' ;\nthis.myObject.style.height = aValue4 + 'px' ;\n</code></pre>\n\n<p>Is there a better (quick for browsers) way to do this ?</p>\n\n<p>Also, maybe do I remove the element from DOM and re-append after ?</p>\n\n<p>Thanks for your anwsers.</p>\n"},{"tags":[".net","performance","collections","list","hash"],"answer_count":11,"favorite_count":7,"up_vote_count":36,"down_vote_count":0,"view_count":20402,"score":36,"question_id":150750,"title":"HashSet vs. List performance","body":"<p>It's clear that a search performance of the generic <code>HashSet&lt;T&gt;</code> class is higher than of the generic <code>List&lt;T&gt;</code> class. Just compare the hash-based key with the linear approach in the <code>List&lt;T&gt;</code> class.</p>\n\n<p>However calculating a hash key may itself take some CPU cycles, so for a small amount of items the linear search can be a real alternative to the <code>HashSet&lt;T&gt;</code>.</p>\n\n<p>My question: where is the break-even?</p>\n\n<p>To simplify the scenario (and to be fair) let's assume that the <code>List&lt;T&gt;</code> class uses the element's <code>Equals()</code> method to identify an item.</p>\n"},{"tags":["php","performance","image-processing","imagemagick"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":26,"score":0,"question_id":13143968,"title":"Imagick pixel iterator - slow as fck","body":"<p>I'm trying to implement <a href=\"http://www.php.net/manual/en/function.imagefilter.php#109809\" rel=\"nofollow\">this code snippet</a> (a \"vignette\" effect) with Imagick, but the processing is incredibly slow:</p>\n\n<pre><code>set_time_limit(90);\n$iterator = $imagick-&gt;getPixelIterator();\n$width = $imagick-&gt;getImageWidth();\n$height = $imagick-&gt;getImageHeight();\n\nforeach($iterator as $y =&gt; $pixels){\n  foreach($pixels as $x =&gt; $pixel){\n\n    $l = 1 - 0.7 * (1 - pow((sin(M_PI / $width * $x) * sin(M_PI / $height * $y)), 0.4));       \n\n    extract($pixel-&gt;getColor());   \n\n    $pixel-&gt;setColor(sprintf('rgb(%d,%d,%d)', $r * $l, $g * $l, $b * $l));\n  }\n\n  $iterator-&gt;syncIterator();\n}\n</code></pre>\n\n<p>Original:</p>\n\n<p><img src=\"http://i.stack.imgur.com/TSDEE.jpg\" alt=\"enter image description here\"></p>\n\n<p>Result:</p>\n\n<p><img src=\"http://i.stack.imgur.com/63E6e.jpg\" alt=\"enter image description here\"></p>\n\n<p>For a 1600x1200 image, it takes like 35 seconds for image to be processed. Is there a better way to do this?</p>\n"},{"tags":["performance","opengl","window","resolution","viewport"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":36,"score":0,"question_id":13139080,"title":"OpenGL Viewport Resolution and Performance","body":"<p>I want to allow the user to tweak windowed-mode performance by lowering the resolution without altering the size of the actual window which must remain fixed. If I alter it with glViewport, will it actually process fewer fragments, or is that purely a visual transformation? Assume I have early depth-testing on in my shader, if that matters at all.</p>\n"},{"tags":["sql","performance","sqlite","optimization"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":53,"score":3,"question_id":13143725,"title":"SQLite: speedup SQL statement with COUNT and GROUP BY","body":"<p>I am working with a table with a \"state\" column, which typically holds only 2 or 3 different values. Sometimes, when this table holds several million rows, following SQL statement becomes slow (I assume a full table scan is done):</p>\n\n<pre><code>SELECT state, count(*) FROM mytable GROUP BY state\n</code></pre>\n\n<p>I expect to get something like this:</p>\n\n<pre><code>disabled |  500000\nenabled  | 2000000\n</code></pre>\n\n<p>(basically I want to know how many items are \"enabled\" and how many items are \"disabled\" - actually that's a number instead of a text in my real application)</p>\n\n<p>I guess adding an index for my state column is pretty useless, since only very few different values can be found there. What other options do I have?</p>\n\n<p>There is also a \"timestamp\" column (with an index). Ideally the solution should also work well if I add:</p>\n\n<pre><code>WHERE timestamp BETWEEN x AND y\n</code></pre>\n\n<p>Right now I'm using an SQLite3 database, but it looks like other database engines are not too different, so solutions for other DB engines might be interesting as well.</p>\n\n<p>Thank you!</p>\n"},{"tags":["sql","performance","oracle","query-optimization"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":47,"score":0,"question_id":13141970,"title":"Performance of query with OR condition between join tables","body":"<p>There are two tables, tparent and tchild, which are created using script #1.\nIt's need to query all the information from the two tables where uname is equal to 'sky'.\nThe following query statement #2 is fast when there are only a few of records.\nBut it will become very slowly when inserting a huge number of records using the following script #3.\nI think it's caused by the OR condition between tables and the index will be unusefull for querying. \nSo I got a fast solution that changing the statement to three sub ones and union result, like #4.\nI want to konw is there a better solution? what's it? </p>\n\n<p>Thanks!</p>\n\n<p><strong># 1</strong></p>\n\n<pre><code>drop table tparent;\ndrop table tchild;\ncreate table tparent(typeid int,sno number,uname varchar2(50) );\ncreate table tchild(typeid int,sno number,seqno int,uname varchar2(50));\ncreate unique index uidx_tparent_typeidsno on tparent(typeid,sno);\ncreate unique index uidx_tchild_typeidsnoseqno on tchild(typeid,sno,seqno);\ncreate index idx_tparent_name on tparent(uname);\ncreate index idx_tchild_name on tchild(uname);\n\ninsert into tparent values (1,10,'lily');\ninsert into tparent values (1,11,'eric');\ninsert into tparent values (2,10,'tom');\ninsert into tparent values (2,11,'eric');\ninsert into tparent values (3,10,'sky');\n\ninsert into tchild values (1,10,1,'sam');\ninsert into tchild values (1,10,2,'sky');\ninsert into tchild values (1,11,1,'eric');\ninsert into tchild values (1,11,2,'john');\ninsert into tchild values (2,10,1,'sky');\ninsert into tchild values (2,11,1,'eric');\ninsert into tchild values (3,10,1,'tony');\n</code></pre>\n\n<p><strong># 2</strong></p>\n\n<pre><code>select p.typeid,p.sno,p.uname,c1.uname as uname1,c2.uname as uname2 from tparent p\n  left join tchild c1 on c1.typeid=p.typeid and c1.sno = p.sno and c1.seqno=1\n  left join tchild c2 on c2.typeid=p.typeid and c2.sno = p.sno and c2.seqno=2\nwhere (p.uname='sky' or c1.uname='sky' or c2.uname='sky');\n</code></pre>\n\n<p><strong># 3</strong></p>\n\n<pre><code>BEGIN\n    FOR x IN 1..10\n    LOOP\n        BEGIN\n            FOR y IN 10000..100000\n            LOOP\n                BEGIN\n                    insert into tparent values (x,y,'name'|| y);\n                    insert into tchild values (x,y,1,'name'|| y);\n                    insert into tchild values (x,y,2,'name'|| y);\n                END;\n            END LOOP;\n            COMMIT;\n        END;\n    END LOOP;\nEND;\n</code></pre>\n\n<p><strong>#4</strong></p>\n\n<pre><code>select typeid,sno,max(uname),max(uname1),max(uname2) from (\n\nselect p.typeid,p.sno,p.uname,c1.uname as uname1,c2.uname as uname2 from tparent p\n  left join tchild c1 on c1.typeid=p.typeid and c1.sno = p.sno and c1.seqno=1\n  left join tchild c2 on c2.typeid=p.typeid and c2.sno = p.sno and c2.seqno=2\nwhere (p.uname='sky' )\nunion \nselect p.typeid,p.sno,p.uname,c1.uname as uname1,c2.uname as uname2 from tparent p\n  left join tchild c1 on c1.typeid=p.typeid and c1.sno = p.sno and c1.seqno=1\n  left join tchild c2 on c2.typeid=p.typeid and c2.sno = p.sno and c2.seqno=2\nwhere ( c1.uname='sky' )\nunion\n\nselect p.typeid,p.sno,p.uname,c1.uname as uname1,c2.uname as uname2 from tparent p\n  left join tchild c1 on c1.typeid=p.typeid and c1.sno = p.sno and c1.seqno=1\n  left join tchild c2 on c2.typeid=p.typeid and c2.sno = p.sno and c2.seqno=2\nwhere ( c2.uname='sky')\n) tb group by typeid,sno\norder by typeid,sno\n;\n</code></pre>\n"},{"tags":["c","performance","pthreads","coroutine"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":42,"score":1,"question_id":13144044,"title":"Performance characteristics of pthreads vs ucontext","body":"<p>I'm trying to port a library that uses ucontext over to a platform which supports pthreads but not ucontext.  The code is pretty well written so it should be relatively easy to replace all the calls to the ucontext API with a call to pthread routines.  However, does this introduce a significant amount of additional overhead?  Or is this a satisfactory replacement.  I'm not sure how ucontext maps to operating system threads, and the purpose of this facility is to make coroutine spawning fairly cheap and easy.</p>\n\n<p>So, question is:  Does replacing ucontext calls with pthread calls significantly change the performance characteristics of a library?</p>\n"},{"tags":["c#",".net","winforms","performance","splash-screen"],"answer_count":4,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":1252,"score":4,"question_id":5743458,"title":"Show a splash screen at once","body":"<p>We are dealing with slow start for WinForm applications (it is a large application and has many control assemblies). Control assemblies are DevComponents. Ngen was applied to prevent jit compilation, but the loading time just decreased a little. </p>\n\n<p>The app has a splash screen, but it appears only in 12 seconds after the app has started. Is there any approach to show the splash screen at once?</p>\n\n<p>Our current suggestion is to create  a lightweight app with the splash screen, run the main app in a separate process, and close the lightweight app when initialization of the main app is done.</p>\n"},{"tags":["java","c","performance","caching","data-structures"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":88,"score":0,"question_id":13142896,"title":"Array of Structs are always faster than Structs of arrays?","body":"<p>I was wondering if the data layout Structs of Arrays (SoA) is always faster than Array of Structs (AoS) or Array of Pointers (AoP) for problems with an input that only fits in RAM in C/JAVA.</p>\n\n<p>Some days ago i was improving the performance of an Molecular dynamic algorithm (in C), summarizing in this algorithm it is calculated the force interaction between particles based on their force and position.</p>\n\n<p>Original the particles were represented by a struct containing 9 different doubles, 3 for particles forces (Fx,Fy,Fz) , 3 for positions and 3 for velocity, and the algorithm had a array containing pointers to all particles (AoP). I decided to change the layout from AoP to SoA to improve the cache use.</p>\n\n<p>So now i have a Struct with 9 array where each array stores Forces, velocity and positions (x,y,z) of each particle, each particle is accessed by it own array index.</p>\n\n<p>I had a gain in performance (for an input that only fits in RAM) of about 1.9x, so i was wondering if typically changing from AoP or AoS to SoA it will always performance better, and if not in each type algorithms where this did not occurs  </p>\n"},{"tags":["mysql","performance","configuration","batch-insert"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":23,"score":0,"question_id":13139224,"title":"Tracing slow batch insert queries in mysql","body":"<p>I have a program that at some point performs thousands of inserts into a MySQL table. Those inserts are surrounded by a transaction. When I run the code on a dev machine using production data (copied prod. db), it takes a couple of minutes to complete. When I run it in production, it runs for over 30 minutes. </p>\n\n<p>Both dev and prod servers run MySQL 5.1 (with a minor version diff, 5.1.64 vs 5.1.41 in prod). The server is a powerful machine with 12 cores, 16GB RAM, fast disks, etc (compared to my puny dev computer). The only difference is that the production machine is also a MySQL replication master. However, the specific schema that I write to isn't replicated.</p>\n\n<p>I'm leaning towards placing the problem on <code>my.cnf</code> configuration values, but any other ideas will help. I've also noticed that although the specific schema isn't replicated, it also isn't ignored in the binlog (<code>binlog-ignore-db = &lt;db-name&gt;</code> in the <code>[mysqld]</code> section), so this is also something I'd like to look into. </p>\n\n<p>What are other red flags I should pay attention to in configuration values to improve the speed of thousands-of-inserts scale transactions? Where else should I be looking to improve batch insert performance? thanks.</p>\n\n<h3>EDIT - the code that does this (very simplified)</h3>\n\n<p>in ruby, using <code>mysql2</code> adapter:</p>\n\n<pre><code>inserts = []\n\n# a loop that generates INSERT statements\n\n...\ninserts &lt;&lt; insert_stmt\n...\n\n# end loop\n\nbegin\n  connection.query(\"BEGIN;\")\n  inserts.each { |q| connection.query(q) }\n  connection.query(\"COMMIT;\")\nrescue\n  connection.query(\"ROLLBACK;\")\nend\n</code></pre>\n"},{"tags":["wpf","performance","drawingbrush"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":916,"score":1,"question_id":1337846,"title":"DrawingBrush Performance","body":"<p>Are there any differences when it comes to performance between the following three border objects?</p>\n\n<pre><code>&lt;Border Grid.Column=\"0\" Grid.ColumnSpan=\"2\" Opacity=\"1\"&gt;\n  &lt;Border.Background&gt;\n    &lt;DrawingBrush&gt;\n      &lt;DrawingBrush.Drawing&gt;\n        &lt;DrawingGroup&gt;\n          &lt;GeometryDrawing Brush=\"Red\"&gt;\n            &lt;GeometryDrawing.Geometry&gt;\n              &lt;GeometryGroup&gt;\n                &lt;RectangleGeometry Rect=\"0,0 100,1000\" /&gt;\n                &lt;LineGeometry StartPoint=\"0,0\" EndPoint=\"100,1000\"/&gt;\n                &lt;LineGeometry StartPoint=\"100,0\" EndPoint=\"0,1000\"/&gt;\n              &lt;/GeometryGroup&gt;\n            &lt;/GeometryDrawing.Geometry&gt;\n            &lt;GeometryDrawing.Pen&gt;\n              &lt;Pen Thickness=\"20\" Brush=\"Black\"/&gt;\n            &lt;/GeometryDrawing.Pen&gt;\n          &lt;/GeometryDrawing&gt;\n        &lt;/DrawingGroup&gt;\n      &lt;/DrawingBrush.Drawing&gt;\n    &lt;/DrawingBrush&gt;\n  &lt;/Border.Background&gt;\n&lt;/Border&gt;\n\n&lt;Border Grid.Column=\"0\" Grid.ColumnSpan=\"2\" Opacity=\"1\"&gt;\n  &lt;Border.Background&gt;\n    &lt;DrawingBrush&gt;\n      &lt;DrawingBrush.Drawing&gt;\n        &lt;DrawingGroup&gt;\n          &lt;GeometryDrawing Brush=\"Red\"&gt;\n            &lt;GeometryDrawing.Geometry&gt;\n              &lt;RectangleGeometry Rect=\"0,0 100,1000\" /&gt;\n            &lt;/GeometryDrawing.Geometry&gt;\n            &lt;GeometryDrawing.Pen&gt;\n              &lt;Pen Thickness=\"20\" Brush=\"Black\"/&gt;\n            &lt;/GeometryDrawing.Pen&gt;\n          &lt;/GeometryDrawing&gt;\n          &lt;GeometryDrawing&gt;\n            &lt;GeometryDrawing.Geometry&gt;\n              &lt;LineGeometry StartPoint=\"0,0\" EndPoint=\"100,1000\"/&gt;\n            &lt;/GeometryDrawing.Geometry&gt;\n            &lt;GeometryDrawing.Pen&gt;\n              &lt;Pen Thickness=\"20\" Brush=\"Black\"/&gt;\n            &lt;/GeometryDrawing.Pen&gt;\n          &lt;/GeometryDrawing&gt;\n          &lt;GeometryDrawing&gt;\n            &lt;GeometryDrawing.Geometry&gt;\n              &lt;LineGeometry StartPoint=\"100,0\" EndPoint=\"0,1000\"/&gt;\n            &lt;/GeometryDrawing.Geometry&gt;\n            &lt;GeometryDrawing.Pen&gt;\n              &lt;Pen Thickness=\"20\" Brush=\"Black\"/&gt;\n            &lt;/GeometryDrawing.Pen&gt;\n          &lt;/GeometryDrawing&gt;\n        &lt;/DrawingGroup&gt;\n      &lt;/DrawingBrush.Drawing&gt;\n    &lt;/DrawingBrush&gt;\n  &lt;/Border.Background&gt;\n&lt;/Border&gt;\n\n&lt;Border Grid.Column=\"3\" Grid.ColumnSpan=\"2\" Opacity=\"1\"&gt;\n  &lt;Image Stretch=\"Uniform\"&gt;\n    &lt;Image.Source&gt;\n      &lt;DrawingImage&gt;\n        &lt;DrawingImage.Drawing&gt;\n          &lt;DrawingGroup&gt;\n            &lt;GeometryDrawing Brush=\"Red\"&gt;\n              &lt;GeometryDrawing.Geometry&gt;\n                &lt;GeometryGroup&gt;\n                  &lt;RectangleGeometry Rect=\"0,0 100,1000\" /&gt;\n                  &lt;LineGeometry StartPoint=\"0,0\" EndPoint=\"100,1000\"/&gt;\n                  &lt;LineGeometry StartPoint=\"100,0\" EndPoint=\"0,1000\"/&gt;\n                &lt;/GeometryGroup&gt;\n              &lt;/GeometryDrawing.Geometry&gt;\n              &lt;GeometryDrawing.Pen&gt;\n                &lt;Pen Thickness=\"20\" Brush=\"Black\"/&gt;\n              &lt;/GeometryDrawing.Pen&gt;\n            &lt;/GeometryDrawing&gt;\n          &lt;/DrawingGroup&gt;\n        &lt;/DrawingImage.Drawing&gt;\n      &lt;/DrawingImage&gt;\n    &lt;/Image.Source&gt;\n  &lt;/Image&gt;\n&lt;/Border&gt;\n</code></pre>\n"},{"tags":["php","performance","image","analytics"],"answer_count":3,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":511,"score":3,"question_id":4665960,"title":"Most efficient way to display a 1x1 GIF (tracking pixel, web beacon)","body":"<p>I'm building a basic analytics service, based in theory off of how Google Analytics works, but instead of requesting an actual image, I'm routing the image request to a script that accepts the data and then outputs an image. Since browsers will be requesting this image on every load, every millisecond counts.</p>\n\n<p>I'm looking for the most efficient way for a file to output a gif file from a PHP script. So far, I've established 3 main methods. </p>\n\n<p><strong>Is there a more efficient way for me output a 1x1 GIF file from within a PHP script? If not, which of these is the most efficient and scalable?</strong> </p>\n\n<h2>Three Identified Methods</h2>\n\n<p>PHP image building libraries</p>\n\n<pre><code>$im = imagecreatetruecolor(1, 1);\nimagefilledrectangle($im, 0, 0, 0, 0, 0xFb6b6F);\nheader('Content-Type: image/gif');\nimagegif($im);\nimagedestroy($im);\n</code></pre>\n\n<p><code>file_get_contents</code> the image off of the server and output it</p>\n\n<pre><code>$im = file_get_contents('raw.gif'); \nheader('Content-Type: image/gif'); \necho $im; \n</code></pre>\n\n<p><code>base64_decode</code> the image</p>\n\n<pre><code>header('Content-Type: image/gif');\necho base64_decode(\"R0lGODdhAQABAIAAAPxqbAAAACwAAAAAAQABAAACAkQBADs=\");\n</code></pre>\n\n<p>(My gut was that base64 would be fastest, but I have no idea how resource intensive that function is; and that file_get_contents would likely scale less well, since it adds another file-system action.)</p>\n\n<p>For reference, the GIF I'm using is here: <a href=\"http://i.stack.imgur.com/LQ1CR.gif\" rel=\"nofollow\">http://i.stack.imgur.com/LQ1CR.gif</a></p>\n\n<h2>EDIT</h2>\n\n<p>So, the reason I'm serving this image is that my analytics library builds a query string and attaches it to this image request. Rather than parse logs, I'm routing the request to a PHP script which processes the data and responds with an image,so that the end user's browser doesn't hang or throw an error. My question is, how do I best serve that image within the confines of a script?</p>\n"},{"tags":["php","arrays","performance","maintainability"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":48,"score":1,"question_id":12929837,"title":"What is more performant, multiple Arrays or one Array containing multiple arrays?","body":"<p>What will be more performant and ressource friendlier?</p>\n\n<p>To use</p>\n\n<pre><code>$array1=Array();\n$array2=Array();\n$array3=Array();\n</code></pre>\n\n<p>or:</p>\n\n<pre><code>$arr=Array();\n$arr[] = Array();\n$arr[] = Array();\n$arr[] = Array();\n</code></pre>\n\n<p>And what is better to handle in the code when maintenance is required?</p>\n\n<p>I have to handle about 2800 different arrays so the performance is very important.</p>\n"},{"tags":["python","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":4,"down_vote_count":1,"view_count":74,"score":3,"question_id":13140619,"title":"Python performance of conditional evaluation","body":"<p>I was trying to find out if there's any penalty for negating a boolean when evaluation a conditional statement (python 2.6.6). I first tried this simple test (no <code>else</code> branch) </p>\n\n<pre><code>&gt;&gt;&gt; import timeit\n&gt;&gt;&gt; timeit.timeit(\"if not True: pass\", number=100000)\n0.011913061141967773\n&gt;&gt;&gt; timeit.timeit(\"if True: pass\", number=100000)\n0.018882036209106445\n</code></pre>\n\n<p>So I though the results where skewed because the pass statement might be translated to a noop which is at least <em>some</em> operation.</p>\n\n<p>I did a second try and got these results:</p>\n\n<pre><code>&gt;&gt;&gt; timeit.timeit(\"a=False\\nif not a: pass\\nelse: pass\", number=100000)\n0.02387714385986328\n&gt;&gt;&gt; timeit.timeit(\"a=False\\nif a: pass\\nelse: pass\", number=100000)\n0.015386819839477539\n&gt;&gt;&gt; timeit.timeit(\"a=True\\nif a: pass\\nelse: pass\", number=100000)\n0.02389812469482422\n&gt;&gt;&gt; timeit.timeit(\"a=True\\nif not a: pass\\nelse: pass\", number=100000)\n0.015424966812133789\n</code></pre>\n\n<p>I didn't expect to see any large penalty but from this results it looks like evaluating the <code>else</code> branch is cheaper than the implicit <code>then</code> branch. And the difference is huge!</p>\n\n<p>So The third attempt return these results:</p>\n\n<pre><code>&gt;&gt;&gt; timeit.timeit(\"if True: a=1\\nelse: a=1\", number=100000)\n0.022008895874023438\n&gt;&gt;&gt; timeit.timeit(\"if not True: a=1\\nelse: a=1\", number=100000)\n0.022121906280517578\n</code></pre>\n\n<p>And finally I got the expected results. Though out of curiosity I tried a last time:</p>\n\n<pre><code>&gt;&gt;&gt; timeit.timeit(\"if False: a=1\\nelse: a=1\", number=100000)\n0.02385997772216797\n&gt;&gt;&gt; timeit.timeit(\"if not False: a=1\\nelse: a=1\", number=100000)\n0.02244400978088379\n</code></pre>\n\n<p>And that's it...\nI have no idea why the negated condition leading to the <code>then</code> branch is faster.</p>\n\n<p>What might be happening?</p>\n\n<p>All those results are reproducible on my computer, it doesn't matter how many times I run them I get pretty much the same results.</p>\n\n<p>I think the first tests where skewed because the compiler might have removed the <code>else: pass</code> part altogether. Is that possible?</p>\n\n<p>Might all this results be related to the branch predictor in the CPU?</p>\n\n<p>Any other possible culprits?</p>\n"},{"tags":["javascript","asp.net","performance","checksum"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":26,"score":1,"question_id":13140298,"title":"Performance Javascript MD5 Checksum generation","body":"<p>I am trying to create a .net web site that allows users to upload files along with some form data.  To verify file integrity of file uploaded, a client side javascript checksum would be passed up to compare against a server generated checksum. My question is the javascript checksum generation performance if say 1-5 of the files uploading are 1 gb. The uploader is based on this project, <a href=\"http://www.codeproject.com/Articles/460142/ASP-NET-Multiple-File-Upload-With-Drag-Drop-and-Pr\" rel=\"nofollow\">http://www.codeproject.com/Articles/460142/ASP-NET-Multiple-File-Upload-With-Drag-Drop-and-Pr</a> </p>\n"},{"tags":["javascript","jquery","html","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":59,"score":0,"question_id":13139003,"title":"What would be a more programmatically effective way of doing this","body":"<p>I have built a simple accordian type side menu and looking at it, it's pretty heavy for what it does. What methods can I learn to reduce the amount of code and time to execute if any?</p>\n\n<p>I am mainly asking this as a learning point.</p>\n\n<pre><code>$('#one').css(\"height\", \"22\");\n$('#dtwo').css(\"height\", \"22\"); \n$('#three').css(\"height\", \"22\");   \n    $('#t1').click(function() {\n      if ($('#one').hasClass(\"extended\")) {\n        $('#one').stop(true, true).animate({height: '22px'},500);\n        $('#one').removeClass(\"extended\");\n        $('#a1').stop(true, true).animate({opacity: '1'},500);\n      } else {\n        $('#one').animate({height: '120' + 'px'},500);\n        $('#one').addClass(\"extended\");\n        $('#a1').animate({opacity: '0'},300);\n      }\n});\n\n$('#t2').click(function() {\n      if ($('#dtwo').hasClass(\"extended\")) {\n        $('#dtwo').stop(true, true).animate({height: '22px'},500);\n        $('#dtwo').removeClass(\"extended\");\n        $('#a2').stop(true, true).animate({opacity: '1'},500);\n      } else {\n        var height = 0;\n        $(this).closest(\"div\").children().each(function(){\n           height += $(this).outerHeight(true);\n        });\n        $('#dtwo').animate({height: height + 5 + 'px'},500);\n        $('#dtwo').addClass(\"extended\");\n        $('#a2').animate({opacity: '0'},300);\n      }\n});\n\n $('#t3').click(function() {\n      if ($('#three').hasClass(\"extended\")) {\n        $('#three').stop(true, true).animate({height: '22px'},500);\n        $('#three').removeClass(\"extended\");\n        $('#a3').stop(true, true).animate({opacity: '1'},500);\n      } else {\n        $('#three').animate({height: '270px'},500);\n        $('#three').addClass(\"extended\");\n        $('#a3').animate({opacity: '0'},300);\n      }\n});\n\n $('#a1').click(function() {\n      if ($('#one').hasClass(\"extended\")) {\n        $('#one').stop(true, true).animate({height: '22px'},500);\n        $('#one').removeClass(\"extended\");\n        $('#a1').stop(true, true).animate({opacity: '1'},500);\n      } else {\n        $('#one').animate({height: '120px'},500);\n        $('#one').addClass(\"extended\");\n        $('#a1').animate({opacity: '0'},300);\n      }\n});\n\n$('#a2').click(function() {\n      if ($('#dtwo').hasClass(\"extended\")) {\n        $('#dtwo').stop(true, true).animate({height: '22px'},500);\n        $('#dtwo').removeClass(\"extended\");\n        $('#a2').stop(true, true).animate({opacity: '1'},500);\n      } else {\n        $('#dtwo').animate({height: '120px'},500);\n        $('#dtwo').addClass(\"extended\");\n        $('#a2').animate({opacity: '0'},300);\n      }\n});\n\n $('#a3').click(function() {\n      if ($('#three').hasClass(\"extended\")) {\n        $('#three').stop(true, true).animate({height: '22px'},500);\n        $('#three').removeClass(\"extended\");\n        $('#a3').stop(true, true).animate({opacity: '1'},500);\n      } else {\n        $('#three').animate({height: '270px'},500);\n        $('#three').addClass(\"extended\");\n        $('#a3').animate({opacity: '0'},300);\n      }\n});\n</code></pre>\n"},{"tags":["python","performance","indexing","deduplication"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":230,"score":1,"question_id":9495623,"title":"Best way or algorithm to near duplicate check against huge list of files?","body":"<p>I am using python to neardupe huge list of file (over 20000 ) files. Totaling about 300 MB</p>\n\n<p>Current way is to do near-dupe checking using difflib's SequenceMatcher and getting result using QuickRatio .</p>\n\n<p>With 4 worker process it takes 25 hours to get the job done , which is quite slow.</p>\n\n<p>I also tried Livenstheine which gives C base near-dupe checking but its even slower and less accurate than difflib.</p>\n\n<p>The checking need to be done in this manner : \nThere are 20000 files in a folder. each file need to compare against 20000 files in the folder on every iterations. so there will be 20000 * 20000 iterations.</p>\n\n<p>What I think of is to index all the files and comparing indexes but i am new to indexing and i am not sure it would work. If that the way what is the best indexing options?</p>\n\n<p>Thanks.</p>\n\n<p>Below is the code :</p>\n\n<pre><code>import os,sys,chardet, csv,operator,time,subprocess\nfrom difflib import SequenceMatcher\nimport threading\n#from threading import Timer\nimport multiprocessing\nfrom multiprocessing import Pool\n\nOrgFile = \"\"\nmark = int(sys.argv[2])\n\ndef init_logger():\n    print \"Starting %s\" % multiprocessing.current_process().name\n\n#----Get_Near_DupeStatus--------#\ndef Get_Near_DupeStatus(score):\n    if score &gt; 30 and score &lt;= 50:\n        return \"Low Inclusive\"\n    elif score &gt; 50 and score &lt;= 75:\n        return \"Inclusive\"\n    elif score &gt; 75 and score &lt;= 85:\n        return \"Most Inclusive\"\n    elif score &gt; 85 and score &lt;= 99:\n        return \"Near-Dupe\"\n    elif score == 100:\n        return \"Unique\"\n    else: return \"No Inclusive\"\n\n#----Write_To_CSV --- ALL-------#\ndef Write_To_CSV_All(List):\n    writer = csv.writer(open('./TableList.csv','wb'),delimiter=';', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n    writer.writerow(['Path/FileName(Source);'+'ID;'+'NearDupeID;'+'Similarity Score;'+'Near_DupeStatus;'+'NearDupeProcess(Y/N);'+'Encoding'])\n    for i,li in enumerate(sorted(List, key=operator.itemgetter(\"NearDupeID\"))):\n        writer.writerow([li['Path/FileName(Source)']+\";\"+'ID00'+str(i+1)+\";\"+str(li['NearDupeID'])+\";\"+str(li['Similarity Score'])+\";\"+li['Near_DupeStatus']+\";\"+li['NearDupeProcess(Y/N)']+\";\"+li['Encoding']])\n\n#Get Finish File List\ndef Finish_Files(List,count,id):\n    finish_files = []\n    for i,li in enumerate(sorted(List, key=operator.itemgetter(\"Similarity Score\"), reverse=True)):\n        if i &lt; count:\n            li['NearDupeID'] = id\n            finish_files.append(li)\n        if count == 0:\n            li['NearDupeID'] = id\n#            if li['Similarity Score'] &gt; 50:\n            finish_files.append(li)\n    return finish_files\n\n#----Search Files in Dir--------#\ndef GetFileListFrom_Dir(dir):\n    FileList = []\n    for root,dirs,filenames in os.walk(dir):\n        for filename in filenames:\n            realpath = os.path.join(root, filename)\n            FileList.append(realpath)\n    return FileList\n\n#----Matcher--------#\ndef Matcher(realpath):\n    junk = [\"\\t\",\"\\n\",\"\\r\"]\n    score = 0\n    dict = {}\n    MatchFile = \"\"\n    dupe_Process = 'N'\n    if os.path.isfile(realpath):\n        MatchFile =  open(realpath).read()\n        matcher = SequenceMatcher(lambda x: x in junk,OrgFile, MatchFile)\n        score = int(matcher.ratio()*100)\n        if score &gt;= mark:\n            encoding = chardet.detect(MatchFile)['encoding']\n            if encoding == None: encoding = 'None'\n            if score &gt; 85: dupe_Process = 'Y'\n            dict = {'Path/FileName(Source)':realpath,'Similarity Score':score,'Near_DupeStatus':Get_Near_DupeStatus(score),'NearDupeProcess(Y/N)':dupe_Process,'Encoding':encoding}\n            return dict\n\n#-------------Pooling--------------------#\ndef MatcherPooling(FileList,orgFile,process):\n    global OrgFile\n    OrgFile = open(orgFile).read()\n    pool_obj = Pool(processes=process)\n    #pool_obj = Pool(processes=process,initializer=init_logger)\n    dict = {}\n    DictList = []\n    dict = pool_obj.map(Matcher,FileList)\n    DictList.append(dict)\n    pool_obj.close()\n    pool_obj.join()\n    return DictList\n\ndef Progress():\n    p = \"/-\\\\|\"\n#    global t\n    for s in p:\n        time.sleep(0.1)\n        sys.stdout.write(\"%c\" % s)\n        sys.stdout.flush()\n        sys.stdout.write('\\b')\n    t2 = threading.Timer(0.1,Progress).start()\n#    t.start()\n\n\n#----Main--------#\ndef Main():\n    Mainls = []\n    dictList = []\n    finish_List = []\n    BLINK = '\\033[05m'\n    NOBLINK = '\\033[25m'\n    dir = sys.argv[1]\n    process = int(sys.argv[3])\n    Top_rec = int(sys.argv[4])\n    Mainls = GetFileListFrom_Dir(dir)\n    bar = \"*\"\n    # setup toolbar\n    sys.stdout.write(\"%s\" % BLINK+\"Processing....\"+ NOBLINK + \"With \"+ str(process) + \" Multi Process...\")#+\" \\n\")\n    if Top_rec != 0:\n        charwidth = len(Mainls)/Top_rec\n    elif Top_rec == 0: charwidth = len(Mainls)\n    t = threading.Timer(0.1,Progress)\n    t.start()\n#    sys.stdout.write(\"[%s]\" % (\"-\" * charwidth))\n#    sys.stdout.flush()\n#    sys.stdout.write(\"\\b\" * (charwidth+1)) # return to start of line, after '['\n\n    #----------------------------------------------------------#\n    for id,orgFile in enumerate(sorted(Mainls)):\n        for dl in MatcherPooling(sorted(Mainls),orgFile,process):\n            for dict in dl:\n                if dict != None:\n                    dictList.append(dict)\n\n            #Append Finish Files List For CSV ALL(Write Once)\n            fl = Finish_Files(dictList,Top_rec,id+1)\n            if Top_rec != 0:\n                for del_List in fl:\n                    Mainls.remove(del_List['Path/FileName(Source)'])\n                    Mainls.sort()\n\n            finish_List.extend(fl)\n            dictList = []\n\n        sys.stdout.write(\"%s\" % bar)\n        sys.stdout.flush()\n\n        #Exit Loop\n        if len(Mainls) == 0:\n            break\n    #----------------------------------------------------------#\n    Write_To_CSV_All(finish_List)\n    #print os.system('clear')\n    sys.stdout.write(\"%s\" % \" \")\n    print \"Finished!\"\n    t.cancel()\n    print os._exit(99)\n\nif __name__ == '__main__':\n    Main()\n</code></pre>\n"},{"tags":["performance","matlab","vectorization"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":53,"score":1,"question_id":13136929,"title":"Can vectorization be done? will it do any good?","body":"<p>This function here is eating a lot of time in my run. But what is see is the most of the time goes in the inbuilt function <code>polyarea</code>. Can this code be vectorized for performance boost? </p>\n\n<p>Profiler Report - </p>\n\n<pre><code>  time   calls\n                  1 function [S S_area] = Polygons_intersection_Compute_area(S)\n                  2 % Guillaume JACQUENOT\n                  3 % guillaume at jacquenot at gmail dot com\n                  4 % 2007_10_08\n                  5 % 2009_06_16\n                  6 % Compute area of each polygon of in S.\n                  7 % Results are stored as a field in S\n                  8 \n  0.50   51945    9 S_area = struct('A', {}); \n  0.20   51945   10 for i=1:numel(S) \n  0.28  103890   11     S(i).area = 0; \n  1.34  103890   12     S_area(i).A = zeros(1,numel(S(i).P)); \n  0.69  103890   13     for j=1:numel(S(i).P) \n  9.24  103890   14         S_area(i).A(j) = polyarea(S(i).P(j).x,S(i).P(j).y); \n  0.28  103890   15         S(i).area      = S(i).area + (1-2*S(i).P(j).hole) * S_area(i).A(j);         \n  0.01  103890   16     end \n  0.08  103890   17 end \n</code></pre>\n"},{"tags":["c","linux","performance","data-structures","g-wan"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":211,"score":4,"question_id":12451398,"title":"Techniques to improve transaction rate","body":"<p>Lighttpd, nginx and others use a range of techniques to provide maximum application performance such as AIO, sendfile, MMIO, caching and epoll and lock free data structures.  </p>\n\n<p>My collegue and I have written a little application server which uses many of these techniques and can also server static files.  So we tested it with apache bench and compared ours with lighttpd and nginx and have at least matched the performance for static content for files from 100 bytes to 1K.</p>\n\n<p>However, when we compare the transaction rate over the same static files to that of G-WAN, G-WAN is miles ahead.</p>\n\n<p>I know this question may be a little subjective but what techniques apart from the obvious ones I've mentioned might Pierre Gauthier be using in GWAN that would enable him to achieve such astounding performance?</p>\n"},{"tags":["performance","cuda","real-time"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":92,"score":1,"question_id":13131183,"title":"Is CUDA suitable for real-time applications?","body":"<p>In continuation of my <a href=\"http://stackoverflow.com/questions/13130967/why-cuda-memory-copy-speed-behaves-like-this-some-constant-driver-overhead\">previous question</a>. Is CUDA suitable for real-time quick applications?\nThe task is: I need my application to make a lot of calculations in 0.1-0.3 ms. CUDA kernels cope with these calculations in a very good time suitable for my project, but with all the overheads I get (memory copy) the time is not acceptable.</p>\n\n<p>Is CUDA just not usable for this kind of applications or there are some hacks to avoid sutuations described in my previous question?</p>\n\n<p><a href=\"http://real-time.ccur.com/Real-Time_CUDA.aspx\" rel=\"nofollow\">These guys</a> provide so called \"GPU Workbench\" with the modified gpu driver built on their own linux verson. They say that their system performs much faster then typical GPU configuraions. Anyone knows about them?</p>\n"},{"tags":["android","performance","optimization","internationalization"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":44,"score":0,"question_id":13135249,"title":"Best practice for mapping variables with translations","body":"<p>I have a Table with lots of rows which should be translated. Most of the values are integers so the values don't need to be translated.</p>\n\n<p>However my data comes from a JSON structure where the data are stored normal in key value pairs. E.g.:</p>\n\n<pre><code>{\n  \"age\":24,\n  \"hair-color\":\"black\",\n  \"weight\":42,\n  \"height\":123,\n  // ...\n}\n</code></pre>\n\n<p>So far I have a strings.xml which looks like this:</p>\n\n<pre><code>&lt;resources&gt;\n  &lt;string name=\"meta_age\"&gt;Alter&lt;/string&gt;\n  &lt;string name=\"meta_hair_color\"&gt;Haarfarbe&lt;/string&gt;\n  &lt;string name=\"meta_weight\"&gt;Gewicht&lt;/string&gt;\n  &lt;string name=\"meta_height\"&gt;Körpergröße&lt;/string&gt;\n  &lt;!-- ... --&gt;\n&lt;/resources&gt;\n</code></pre>\n\n<p>This data are visualisated in a list view with a custom ArrayAdapter. This works fine however I'm not sure what is the best way for mapping the key value pairs with its translations.</p>\n\n<p>I have now this code here:</p>\n\n<pre><code>public static final int[] fields = new int[] {R.string.meta_age, R.string.meta_hair_color, R.string.meta_weight, R.string.meta_height, /* ... */;\n</code></pre>\n\n<p>For the building the UI the strings need to be mapped to the <em>keys</em> so far this is done with this messy code:</p>\n\n<pre><code>List&lt;Integer&gt; fieldIndex = new ArrayList&lt;Integer&gt;(fields.length);\nfor(int i : fields) {\n    fieldIndex.add(i);\n}\ntranslation = new HashMap&lt;String, Integer&gt;();\ntranslation.put(\"age\", fieldIndex.indexOf(R.string.meta_age));\ntranslation.put(\"hair-color\", fieldIndex.indexOf(R.string.meta_hair_color));\ntranslation.put(\"weight\", fieldIndex.indexOf(R.string.meta_weight));\ntranslation.put(\"height\", fieldIndex.indexOf(R.string.meta_height));\n</code></pre>\n\n<p>The next step is that I now know the target index for the json field and put them together with the fields list of string ids. But I think that this code is not very performant. How can I write that better?</p>\n"},{"tags":["css","performance","css3","css-transitions","transition"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":448,"score":0,"question_id":9516047,"title":"Does the CSS3 transition slow down a website?","body":"<p>I have currently added the <strong>CSS3 transition</strong> to my website.\nI'm not shure if it is possible, that it slows down my website, but everything seems flickering and there's this \"jerky behaviour\" on the transitions and flash videos.</p>\n\n<p>I'm using Mozilla Firefox 10.0.02.</p>\n\n<p>I added following to my CSS stylesheet:  </p>\n\n<pre><code>*:link, *:visited, *:hover, *:active, *:focus {\n    -webkit-transition: color .25s linear, background-color .25s linear, border-color .25s linear;\n    -o-transition: color .25s linear, background-color .25s linear, border-color .25s linear;\n    -moz-transition: color .25s linear, background-color .25s linear, border-color .25s linear;\n    transition: color .25s linear, background-color .25s linear, border-color .25s linear;\n}\n</code></pre>\n\n<p>Can you tell me if it's rather my browser being slow or if it is the CSS I added and if, then what's the evidence?</p>\n\n<p>Thank you!</p>\n"},{"tags":["c++","multithreading","performance","c++11","atomic"],"answer_count":3,"favorite_count":3,"up_vote_count":7,"down_vote_count":1,"view_count":304,"score":6,"question_id":13135834,"title":"std::atomic<bool> is VERY slow","body":"<p>I've been using volatile bool for years for thread execution control and it worked fine</p>\n\n<pre><code>// in my class declaration\nvolatile bool stop_;\n\n-----------------\n\n// In the thread function\nwhile (!stop_)\n{\n     do_things();\n}\n</code></pre>\n\n<p>Now, since c++11 added support for atimic operations, I decied to try that instead</p>\n\n<pre><code>// in my class declaration\nstd::atomic&lt;bool&gt; stop_;\n\n-----------------\n\n// In the thread function\nwhile (!stop_)\n{\n     do_things();\n}\n</code></pre>\n\n<p>But it's several orders of magnitude slower than the <code>volatile bool</code>!</p>\n\n<p>Simple test case I've written takes about 1 second to complete with <code>volatile bool</code> approach. With <code>std::atomic&lt;bool&gt;</code> however I've been waiting for about 10 minutes and gave up!</p>\n\n<p>I tried to use <code>memory_order_relaxed</code> flag with <code>load</code> and <code>store</code> to the same effect. </p>\n\n<p>My platform:\nWindows 7 64 bit\nMinGW gcc 4.6.x</p>\n\n<p>What I'm doing wrong?</p>\n\n<p><strong>UPD</strong></p>\n\n<p>Yes, I know that volatile does not make a variable thread safe. My question is not about volatile, it's about why atomic is redicolously slow.</p>\n\n<p><strong>UPD2</strong>\n@all, thank you for your comments - I will try all the suggested when I get to my machine tonight.</p>\n"},{"tags":["c++","performance","optimization","x64","double-precision"],"answer_count":3,"favorite_count":0,"up_vote_count":8,"down_vote_count":0,"view_count":574,"score":8,"question_id":9283717,"title":"Why c++ program compiled for x64 platform is slower than compiled for x86?","body":"<p>I've wrote program, and compiled it for x64 and x86 platform in Visual Studio 2010 on Intel Core i5-2500. x64 version take about 19 seconds for execution and x86 take about 17 seconds. What can be the reason of such behavior?</p>\n\n<pre><code>#include \"timer.h\"\n\n#include &lt;vector&gt;\n#include &lt;iostream&gt;\n#include &lt;algorithm&gt;\n#include &lt;string&gt;\n#include &lt;sstream&gt;\n\n/********************DECLARATIONS************************************************/\nclass Vector\n{\npublic:\n    Vector():x(0),y(0),z(0){}\n\n    Vector(double x, double y, double z)\n        : x(x)\n        , y(y)\n        , z(z)\n    {\n    }\n\n    double x;\n    double y;\n    double z;\n};\n\n\ndouble Dot(const Vector&amp; a, const Vector&amp; b)\n{\n    return a.x * b.x + a.y * b.y + a.z * b.z;\n}\n\n\nclass Vector2\n{\npublic:\n    typedef double value_type;\n\n    Vector2():x(0),y(0){}\n\n    Vector2(double x, double y)\n        : x(x)\n        , y(y)\n    {\n    }\n\n    double x;\n    double y;\n};\n\n/******************************TESTS***************************************************/\n\nvoid Test(const std::vector&lt;Vector&gt;&amp; m, std::vector&lt;Vector2&gt;&amp; m2)\n{\n    Vector axisX(0.3f, 0.001f, 0.25f);\n    Vector axisY(0.043f, 0.021f, 0.45f);\n\n    std::vector&lt;Vector2&gt;::iterator i2 = m2.begin();\n\n    std::for_each(m.begin(), m.end(),\n        [&amp;](const Vector&amp; v)\n    {\n        Vector2 r(0,0);\n        r.x = Dot(axisX, v);\n        r.y = Dot(axisY, v);\n\n        (*i2) = r;\n        ++i2;\n    });\n}\n\n\nint main()\n{\n    cpptask::Timer timer;\n\n    int len2 = 300;\n    size_t len = 5000000;\n    std::vector&lt;Vector&gt; m;\n    m.reserve(len);\n    for (size_t i = 0; i &lt; len; ++i)\n    {\n        m.push_back(Vector(i * 0.2345, i * 2.67, i * 0.98));\n    }\n\n    /***********************************************************************************/\n    {\n        std::vector&lt;Vector2&gt; m2(m.size());\n        double time = 0;\n        for (int i = 0; i &lt; len2; ++i)\n        {\n            timer.Start();\n            Test(m, m2);\n            time += timer.End();\n        }\n        std::cout &lt;&lt; \"Dot product double - \" &lt;&lt; time / len2 &lt;&lt; std::endl;\n    }\n    /***********************************************************************************/\n\n\n    return 0;\n}\n</code></pre>\n"},{"tags":["php","performance","apache","centos","lamp"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":36,"score":0,"question_id":13137702,"title":"how to find out what php scripts are slow with apache?","body":"<p>what are my methods of finding out what scripts on my server (LAMP (centos))?</p>\n\n<p>I've found mod_log_slow but it was last updated 2009. Is it worth trying?</p>\n\n<p>thanks</p>\n"},{"tags":["mysql","sql","performance","database-performance"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":27,"score":0,"question_id":13136278,"title":"Optimize sql request for statistics","body":"<p>I would like to perform some request in mysql that i know will be really slow:</p>\n\n<p>I have 3 tables:</p>\n\n<p>Users:</p>\n\n<pre><code>id, username, email\n</code></pre>\n\n<p>Question:</p>\n\n<pre><code>id, date, question\n</code></pre>\n\n<p>Answer</p>\n\n<pre><code>id_question, id_user, response, score\n</code></pre>\n\n<p>And i would like to do some statistics like the top X users with the best score (sum of all the scores) for all time or for a given amount of time (last month for example). Or it could be users between the 100th and the 110th range</p>\n\n<p>I will have thousands of users and hundred of questions so the requests could be very long since I'll need to order by sum of scores, limit to a given range and sometimes only select some questions depending on the date, ...</p>\n\n<p>I would like to know if there are some methods to optimize the requests!</p>\n"},{"tags":["performance","jqgrid","freeze","distortion","rownum"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":61,"score":1,"question_id":12927114,"title":"jqGrid freeze rowheader (rownum)","body":"<p>I am trying to freeze rownum column of grid. Because I have replaced text of rownum with meaningful row description.<br>\nI took help from jqgrid freezing columns demo. But it is not working out for rownum column.\nMy grid loads local array data and using version jQgrid 4.3.2\nAftert his I tried another approach to freeze first column with property </p>\n\n<pre><code>'frozen: true' and then I call function  $(grid_selector).jqGrid('setFrozenColumns');\n</code></pre>\n\n<p>and I get below issues:</p>\n\n<ul>\n<li>The grid loads becomes exceptionally slow (tested in IE, Firefox)</li>\n<li>After much wait when page loads, the row header alignment is distorted. All rowheaders get shifted one step up and overlaps Caption div.</li>\n</ul>\n\n<p>Row1Header goes into Caption place - Row2Header goes into Row1 and so on..</p>\n\n<p>My ultimate requirement is to freeze rowheaders. Any help is much appreciated.  </p>\n\n<p>Thanks</p>\n"},{"tags":["c#","performance","compiler"],"answer_count":4,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":74,"score":1,"question_id":13135759,"title":"What is the overhead for a method call in a loop?","body":"<p>I’ve been working on a C# maze generator for a while which can generate mazes of like 128000x128000 pixels. All memory usage is optimized already so I’m currently looking at speeding the generation up.</p>\n\n<p>A problem (well more off an interest point) I found was the following (just some example code to illustrate the problem):</p>\n\n<p>This code runs in about 1.4 seconds on my machine when pixelChanged is null:</p>\n\n<pre><code>public void Go()\n{\n    for (int i = 0; i &lt; bitjes.Length; i++)\n    {\n        BitArray curArray = bitjes[i];\n        for (int y = 0; y &lt; curArray.Length; y++)\n        {\n            curArray[y] = !curArray[y];\n            GoDrawPixel(i, y, false);\n        }\n    }\n}\n\npublic void GoDrawPixel(int i, int y, Boolean enabled)\n{\n    if (pixelChanged != null)\n    {\n        pixelChanged.Invoke(new PixelChangedEventArgs(i, y, enabled));\n    }\n}\n</code></pre>\n\n<p>Where the following code runs actually 0.4 seconds faster</p>\n\n<pre><code>public void Go()\n{\n    for (int i = 0; i &lt; bitjes.Length; i++)\n    {\n        BitArray curArray = bitjes[i];\n        for (int y = 0; y &lt; curArray.Length; y++)\n        {\n            curArray[y] = !curArray[y];\n            if (pixelChanged != null)\n            {\n                pixelChanged.Invoke(new PixelChangedEventArgs(i, y, false));\n            }\n        }\n    }\n}\n</code></pre>\n\n<p>It seems that when just calling an “empty” method takes about 20% of the cpu this algorithm uses. Isn’t this strange? I’ve tried to compile the solution in debug and release mode but haven’t found any noticeable differences.</p>\n\n<p>What this means is that every method call I have in this loop will slow my code down by about 0.4 seconds. Since the maze generator code currently consist of a lot of seperate method calls that excecute different actions this starts to get a substantial ammount.</p>\n\n<p>I've also checked google and other posts on Stack Overflow but haven't really found a solution yet.</p>\n\n<p>Is it possible to automatically optimize code like this? (Maybe with things like project Roslyn???) or should I place everything together in one big method?</p>\n\n<p>Edit:\nI'm also interested in maybe some analysis on the JIT/CLR code differences in these 2 cases. (So where this problem actually comes from)</p>\n\n<p>Edit2:\n<strong>All code was compiled in release mode</strong></p>\n"}]}
{"total":25592,"page":5,"pagesize":100,"questions":[{"tags":["performance","ember.js"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":70,"score":4,"question_id":13125146,"title":"Ember.JS: Observing @each, but just iterating over new/changed items","body":"<p>I'm currently observing some Ember arrays like so:</p>\n\n<pre><code> observe_array: function() {\n     this.get(\"my_array\").forEach(function(e,i) {\n         // do something\n     });\n }.observes(\"my_array.@each\");\n</code></pre>\n\n<p>Most times, if my_array is updated, multiple elements are added at once.\nHowever, the observer fires one-by-one as each element is added, which becomes extremely inefficient. Is there anyway to do this more efficiently? Essentially, I need to be able to have a mutated array based on \"my_array\"</p>\n\n<p>For reference, realistic sizes of my_array will be between 600-1200 elements. The \"do something\" block involves some operations that take a little more time - creating Date objects from strings and converting each element to json representation.</p>\n\n<p>Instead of doing an observer I also tried a property with the cacheable() method/flag, but that didn't seam to speed things up very much....</p>\n"},{"tags":["c#","winforms","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":52,"score":1,"question_id":13136157,"title":"Application performance when using user controls C#","body":"<p>I'm working on somewhat large application that is divided into groups by functionality. Since every functionality is mostly independent (they all use the same database, but there is no direct interaction between different functionalities), I'm using the user defined controls and treating them as individual \"applications\". The way the application works is this:</p>\n\n<ul>\n<li>The \"root\" application only contains the main menu and 2 panels. The main menu is used to select the group of functionalities.</li>\n<li>After the group is selected (on application startup the first group is automatically selected), the functionalities (represented by buttons) from that group are displayed in the first panel.</li>\n<li><p>The user selects the functionality (by clicking on the appropriate button) he/she wants and the user control that contains the \"form\" is displayed in the second panel.\nThe code that displays all of the user controls looks like this:</p>\n\n<pre><code>    panel2.Controls.Clear();\n    UserControl1 uc1 = new UserControl1();\n    uc1.Location = new Point(0, 0);\n    panel2.Controls.Add(uc1);\n    label6.Text = \"User control 1\";\n</code></pre></li>\n</ul>\n\n<p>So, when the user selects one of the functionalities, the application clears existing controls, and displays the selected one.\nThe application works fine (the part I implemented so far), so this is my question - how does this approach manage computer resources, mainly the memory. Specifically, if the user uses one functionality, and then switches to another one, will the .NET's services release the memory used by the previous functionality (I think garbage collector is in charge of that) and will the SQL connections, that I use to communicate with the database, be closed?\nAlso, are there some other issues that I should be aware of?\nAs I said, the functionalities work properly, but I'm still very far from full testing of the application as a whole (I only test every functionality individually when I create it, and only on the computer I create it on, so I can't consider it as a proper testing). Because of that, I am worried that the application's performances might deteriorate if the application is constantly used over a longer period of time.\nI'm using VS 2010 (C#) and SQL Server 2005 to create this application.\nIf you have any suggestions, please write them. With this question, I'm trying to prevent major reconstructions of the application once it comes to the phase of testing and implementing due to bad resource management.\nThanks.</p>\n"},{"tags":["php","sql-server","performance","apache","debugging"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":31,"score":2,"question_id":13135087,"title":"Apache Server and SQL Server 2008 Express Performance. Isolate, method or suggestions","body":"<p>I was wondering if some tips or guidance can be provided for the following issue. </p>\n\n<p>Environment type: Small office, approx 20 users.\nNetwork: All server, workstation belong to a single subnet\nInfrastructure: Virtualised webserver and virtualised database server.</p>\n\n<p>Currently I have a web server (Win7SP1 x64) running Apache 2.2.22 (Win32) and PHP 5.3.10. I have built some pages that connects to our database (SQL2k8 Express) and calls procedures, the procedures are basically just basic select statements with joins and conditions. Once the result set are returned, PHP loop through the result set, row by row and display the rows as it flow through. </p>\n\n<p>What happens is that from time to time, the result page can take a extremely long to load,where no results set are displayed but the web browser loading bar/icon flashes. Some times it will just give a blank screen at the end of the load, which typically means the query connection has timed-out or some times it will return the result set. For example, under normal behaviour, a page will take approx 1-3s to load, while at odd times, it may take up to 20-30s.</p>\n\n<p>I have noticed that if I run queries in SQLSVR Mgmt Studio, queries that may take ~5s to return a result set may take approx ~15s to load on my web server. </p>\n\n<p>I know poor PHP coding can cause slow downs but the odd performance behaviour has me bit baffled, as Im sure my coding isnt that bad.</p>\n\n<p>So if people could offer some tips in how I can go about diagnosting or isolating the problem, or advise areas that I could look at, it will be great. I know there are Apache logs, SQL SVR logs and performance indicators, but Im still fairly new to this area and am really lost in the approach /methodology I should take. </p>\n\n<p>Example, any specific mods I should enable in Apache etc. or the type of Apache server I should use, connection sockets in web server etc?</p>\n\n<p>Example of my PHP query/procedure calls</p>\n\n<pre><code>$queryStatement = \"exec myProc @para1='asd'\";\n$prepare = $dbConnect-&gt;prepare($queryStatement);\n$result = $prepare-&gt;execute();\nWHILE($result = $prepare-&gt;fetch(PDO::FETCH_ASSOC)){\nset_time_limit(180);\necho $result\n}\n</code></pre>\n\n<p>Cheers everyone :)</p>\n"},{"tags":[".net","performance","matlab"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":163,"score":4,"question_id":9912184,"title":".NET performance from Matlab","body":"<p>I have a large simulation suite written in Matlab, but due to concerns about better interfacing with other internal projects (as well execution speed) I'm thinking about moving some functionality to .NET and calling such objects from within Matlab. What is the overhead associated with calling .NET objects from <em>within</em> Matlab?</p>\n\n<p>Here's a good <a href=\"http://stackoverflow.com/questions/1693429/is-matlab-oop-slow-or-am-i-doing-something-wrong/1745686#1745686\">discussion on Matlab OO</a> that doesn't talk about .NET</p>\n\n<p><strong>Edit</strong>: Brief study</p>\n\n<p>I ran a quick test on my own from within Matlab of simple access and assignment operations within different objects including formal Matlab objects (R2011b), Java and .NET calling each 1,000,000 times. The method calls refer to internal looping, the property/field calls refer to accessing the public field from Matlab and looping in Matlab. The last results puzzle me as the overhead for .NET is much higher than Java but the actual run-time is about half. What is going on?</p>\n\n<pre>\n    Access(s)  Assign(s)  Type of object/call\n    --- MATLAB ---\n    0.003361   0.004268   'myObj.field'\n    0.003403   0.004263   'myStruct.field'\n    0.003376   0.003392   'myVar'   \n    0.152629   0.303579   'myHandleObj.field'\n    25.79159   -          'TestConstant.const'\n    0.003384   -          'myTestConstant.const' (instance)\n    0.006794   0.008689   'TestObj.methods'\n    0.157509   0.303357   'TestHandleObj.methods'\n\n    --- NON-MATLAB ---\n    10.70006   16.42527   'JavaObj fields'\n    0.005063   0.005441   'JavaObj methods'\n    43.49988   43.96159   'NetObj fields'\n    0.002194   0.002306   'NetObj methods'\n</pre>\n"},{"tags":["asp.net","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":20,"score":1,"question_id":13135452,"title":"Performance issues with TransferRequestHandler and BeginRequest","body":"<p>I have started to use New Relic to monitor the performance of <a href=\"http://alternativeto.net\" rel=\"nofollow\">http://alternativeto.net</a> that is a fairly large website.</p>\n\n<p>What I've noticed is that a significant time is spent in a method they report as \"TransferRequestHandler\" and when i dive into it i see that it's really the \"BeginRequest()\" method that is taking time.</p>\n\n<p>It looks like this in New Relic.</p>\n\n<p><img src=\"http://i.stack.imgur.com/RdY32.png\" alt=\"http://content.screencast.com/users/stuckish/folders/Jing/media/22c8137e-21b1-4b36-8185-15989e173f57/2012-10-30_0941.png\"></p>\n\n<p>The closest thing I've come to find anything that could be the problem is this thread here on Stack Overflow <a href=\"http://stackoverflow.com/questions/3629709/i-just-discovered-why-all-asp-net-websites-are-slow-and-i-am-trying-to-work-out\">I just discovered why all ASP.Net websites are slow, and I am trying to work out what to do about it</a> but i've actually tried to replace the Session Module but that didn't help.</p>\n\n<p>The site is a hybrid between ASP.NET MVC and Webforms.</p>\n\n<p>I've realized that this is a long shot and you don't have much to \"go on\" but if someone can put me in the right direction and most importantly be able to reproduce the behavior locally or something like that i would be extremely grateful :)</p>\n"},{"tags":["c++","performance","random"],"answer_count":4,"favorite_count":0,"up_vote_count":10,"down_vote_count":0,"view_count":180,"score":10,"question_id":13045214,"title":"Fast adding random variables in C++","body":"<p><strong>Short version</strong>: how to most efficiently represent and add two random variables given by lists of their realizations? </p>\n\n<p><strong>Mildly longer version:</strong>\nfor a workproject, I need to add several random variables each of which is given by a list of values. For example, the realizations of rand. var. A are {1,2,3} and the realizations of B are {5,6,7}. Hence, what I need is the distribution of A+B, i.e. {1+5,1+6,1+7,2+5,2+6,2+7,3+5,3+6,3+7}. And I need to do this kind of adding several times (let's denote this number of additions as COUNT, where COUNT might reach 720) for different random variables (C, D, ...). </p>\n\n<p><strong>The problem:</strong> if I use this stupid algorithm of summing each realization of A with each realization of B, the complexity is exponential in COUNT. Hence, for the case where each r.v. is given by three values, the amount of calculations for COUNT=720 is 3^720 ~ 3.36xe^343 which will last till the end of our days to calculate:) Not to mention that in real life, the lenght of each r.v. is gonna be 5000+.</p>\n\n<p><strong>Solutions:</strong>\n1/ The first solution is to use the fact that I am OK with rounding, i.e. having integer values of realizations. Like this, I can represent each r.v. as a vector and for at the index corresponding to a realization I have a value of 1 (when the r.v. has this realization once). So for a r.v. A and a vector of realizations indexed from 0 to 10, the vector representing A would be [0,1,1,1,0,0,0...] and the representation for B would be [0,0,0,0,0,1,1,1,0,0,10]. Now I create A+B by going through these vectors and do the same thing as above (sum each realization of A with each realization of B and codify it into the same vector structure, quadratic complexity in vector length). The upside of this approach is that the complexity is bound. The problem of this approach is that in real applications, the realizations of A will be in the interval [-50000,50000] with a granularity of 1. Hence, after adding two random variables, the span of A+B gets to -100K, 100K.. and after 720 additions, the span of SUM(A, B, ...) gets to [-36M, 36M] and even quadratic complexity (compared to exponential complexity) on arrays this large will take forever. </p>\n\n<p>2/ To have shorter arrays, one could possibly use a hashmap, which would most likely reduce the number of operations (array accesses) involved in A+B as the assumption is that some non-trivial portion of the theoreical span [-50K, 50K] will never be a realization. However, with continuing summing of more and more random variables, the number of realizations increases exponentially while the span increases only linearly, hence the density of numbers in the span increases over time. And this would kill the hashmap's benefits.</p>\n\n<p>So the question is: how can I do this problem efficiently? The solution is needed for calculating a VaR in electricity trading where all distributions are given empirically and are like no ordinary distributions, hence formulas are of no use, we can only simulate.</p>\n\n<hr>\n\n<p>Using math was considered as the first option as half of our dept. are mathematicians. However, the distributions that we're going to add are badly behaved and the COUNT=720 is an extreme. More likely, we are going to use COUNT=24 for a daily VaR. Taking into account the bad behaviour of distributions to add, for COUNT=24 the central limit theorem would not hold too closely (the distro of SUM(A1, A2, ..., A24) would not be close to normal). As we're calculating possible risks, we'd like to get a number as precise as possible.</p>\n\n<p>The intended use is this: you have hourly casflows from some operation. The distribution of cashflows for one hour is the r.v. A. For the next hour, it's r.v. B, etc. And your question is: what is the largest loss in 99 percent of cases? So you model the cashflows for each of those 24 hours and add these cashflows as random variables so as to get a distribution of the total casfhlow over the whole day. Then you take the 0.01 quantile.</p>\n"},{"tags":["performance","gwt","comparison","smartgwt"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":65,"score":0,"question_id":13133093,"title":"GWT vs SmartGWT","body":"<p>Can you please let me know which of the option( <code>GWT or SmartGWT</code>) is better, considering the below scenario.</p>\n\n<ol>\n<li>Will be used for designing screens and client-side validations.</li>\n<li>Performance ( page loading , grid loading) should be good.</li>\n<li>Need to communicate with server through soap (web services.)</li>\n</ol>\n"},{"tags":["php","mysql","performance","optimization"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":52,"score":1,"question_id":13134628,"title":"Best settings for mysql full text search","body":"<p>I'm making forum search engine that will crawl forums and help user to find related topics and answers.\nI will use php/mysql for front end.</p>\n\n<p>On each search query it will use mysql full text search across big amount of data (I don't have idea how many records will be there).</p>\n\n<p>Now I have concern about mysql optimization and settings and also about choosing right hardware.\nI don't know much about disk types, etc..</p>\n\n<p>I will start with hetzner server (http://www.hetzner.de/en/hosting/produkte_rootserver/ex5).</p>\n\n<p>Can anybody give any tips about this?</p>\n"},{"tags":["android","performance","google-maps","application","android-mapview"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":55,"score":0,"question_id":13133011,"title":"Enhance Performance of mapview with 200+ markers","body":"<p>I am having 3 types of overlays on the map. \n1) there is one kind of overlays which is having simple markers. And on click of overlay i show pop up.\n2) other kind of overlay contains canvas drawing. and onTouch event of these overlays we have to show third kind of overlay which contains bitmap drawing on the map. And click event of third kind of overlay is also there.</p>\n\n<p>so, due to heavy  calculations and lots of canvas drawing, performance of mapview is degraded.</p>\n\n<p>I have gone through Overlaymanager libreary from <a href=\"https://github.com/staroud-android/OverlayManager\" rel=\"nofollow\">https://github.com/staroud-android/OverlayManager</a>. but its not helping me.</p>\n\n<p>Is there any way which can fulfill my requirements with high performance? </p>\n\n<p>I also need suggestion that Polaris map library can help me for better performance? <a href=\"https://github.com/cyrilmottier/Polaris\" rel=\"nofollow\">https://github.com/cyrilmottier/Polaris</a></p>\n"},{"tags":["performance","mongodb","count"],"answer_count":3,"favorite_count":2,"up_vote_count":7,"down_vote_count":0,"view_count":2072,"score":7,"question_id":7658228,"title":"MongoDB 'count()' is very slow. How do we refine/work around with it?","body":"<p>I am currently using MongoDB with millions of data records. I discovered one thing that's pretty annoying.</p>\n\n<p>When I use 'count()' function with a small number of queried data collection, it's very fast. However, when the queried data collection contains thousand or even millions of data records, the entire system becomes very slow.</p>\n\n<p>I made sure that I have indexed the required fields.</p>\n\n<p>Has anybody encountered an identical thing? How do you do to improve that?</p>\n"},{"tags":["sql-server","performance","ado.net","sql-server-2012"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":50,"score":2,"question_id":13132903,"title":"SQL Server performance - schema vs multiple databases","body":"<p>This is purely from performance standpoint and only for SQL Server. I am using SQL Server 2012. I am migrating from a different database server (Ctree). The databases are from less than 100mbs to about 2-3GBs, five in total. There are a lot of tables - over 400 tables in all. </p>\n\n<p>Would it be better in terms of performance only to use a single database and multiple schema or multiple databases as is? The existing logic is that there are multiple databases and there are 7 different applications (C# - ADO.NET) that use these. </p>\n"},{"tags":["performance","html5","web-applications","jquery-mobile","inline"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":43,"score":1,"question_id":12341821,"title":"Considerations about optimizing singlepage mobile web apps by inlining all js and css","body":"<p>I am creating a mobile singlepage web app using jquery mobile. The webapp includes a number of javascript files and a number of css files. I have written a deploy script that concatenates and minifies js and css files, and now I am wondering whether I should inline the concatenated js and css directly in the HTML file - please note that I am talking about a singlepage app here (I know that this would be a bad idea in a traditional web 1.0 app with dynamically generated HTML). I am also using appcache/manifest file to cache the singlepage app so that subsequent access to the web app will be served from the cache, so it is the initial load time that is my primary concern. </p>\n\n<p>When I inline everything (jquery, jquery mobile etc.), my 7kb HTML file increases to 350kb (100kb zipped) but now everything can be loaded in a single request.  </p>\n\n<p>But am I missing some other benefits such as parallel downloading of js files - and would it therefore be better to not inline the css and js, but instead just concatenate all js and css to a single js file and a single css file and then fetch each of them in separate requests?</p>\n\n<p>Are there any limits regarding file size that I should be aware of? Maybe caching in network routers works better with smaller file sizes or whatever?    </p>\n\n<p>So my question boils down to whether it is a good idea to inline <em>everything</em> when making singlepage mobile web apps?</p>\n"},{"tags":["objective-c","performance","super"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":26,"score":1,"question_id":13132720,"title":"Is it inefficient to only override a method and only call super?","body":"<p>Do I take a performance hit by leaving code like the following in my app?</p>\n\n<pre><code>- (void)viewDidUnload \n{\n  [super viewDidUnload];\n  // Release any retained subviews of the main view.\n  // e.g. self.myOutlet = nil;\n}\n</code></pre>\n\n<p>I think the answer is yes, because it results in an unnecessary method call. But I wanted to make sure.</p>\n"},{"tags":["c++","performance","benchmarking"],"answer_count":4,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":249,"score":6,"question_id":13128430,"title":"Why is `std::copy` 5x (!) slower than `memcpy` in my test program?","body":"<p>This is a follow-up to <a href=\"http://stackoverflow.com/questions/13117211/why-is-memcpy-slower-than-a-reinterpret-cast-when-parsing-binary-data\">this question</a> where I posted this program:</p>\n\n<pre><code>#include &lt;algorithm&gt;\n#include &lt;cstdlib&gt;\n#include &lt;cstdio&gt;\n#include &lt;cstring&gt;\n#include &lt;ctime&gt;\n#include &lt;iomanip&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;chrono&gt;\n\nclass Stopwatch\n{\npublic:\n    typedef std::chrono::high_resolution_clock Clock;\n\n    //! Constructor starts the stopwatch\n    Stopwatch() : mStart(Clock::now())\n    {\n    }\n\n    //! Returns elapsed number of seconds in decimal form.\n    double elapsed()\n    {\n        return 1.0 * (Clock::now() - mStart).count() / Clock::period::den;\n    }\n\n    Clock::time_point mStart;\n};\n\nstruct test_cast\n{\n    int operator()(const char * data) const\n    {\n        return *((int*)data);\n    }\n};\n\nstruct test_memcpy\n{\n    int operator()(const char * data) const\n    {\n        int result;\n        memcpy(&amp;result, data, sizeof(result));\n        return result;\n    }\n};\n\nstruct test_memmove\n{\n    int operator()(const char * data) const\n    {\n        int result;\n        memmove(&amp;result, data, sizeof(result));\n        return result;\n    }\n};\n\nstruct test_std_copy\n{\n    int operator()(const char * data) const\n    {\n        int result;\n        std::copy(data, data + sizeof(int), reinterpret_cast&lt;char *&gt;(&amp;result));\n        return result;\n    }\n};\n\nenum\n{\n    iterations = 2000,\n    container_size = 2000\n};\n\n//! Returns a list of integers in binary form.\nstd::vector&lt;char&gt; get_binary_data()\n{\n    std::vector&lt;char&gt; bytes(sizeof(int) * container_size);\n    for (std::vector&lt;int&gt;::size_type i = 0; i != bytes.size(); i += sizeof(int))\n    {\n        memcpy(&amp;bytes[i], &amp;i, sizeof(i));\n    }\n    return bytes;\n}\n\ntemplate&lt;typename Function&gt;\nunsigned benchmark(const Function &amp; function, unsigned &amp; counter)\n{\n    std::vector&lt;char&gt; binary_data = get_binary_data();\n    Stopwatch sw;\n    for (unsigned iter = 0; iter != iterations; ++iter)\n    {\n        for (unsigned i = 0; i != binary_data.size(); i += 4)\n        {\n            const char * c = reinterpret_cast&lt;const char*&gt;(&amp;binary_data[i]);\n            counter += function(c);\n        }\n    }\n    return unsigned(0.5 + 1000.0 * sw.elapsed());\n}\n\nint main()\n{\n    srand(time(0));\n    unsigned counter = 0;\n\n    std::cout &lt;&lt; \"cast:      \" &lt;&lt; benchmark(test_cast(),     counter) &lt;&lt; \" ms\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"memcpy:    \" &lt;&lt; benchmark(test_memcpy(),   counter) &lt;&lt; \" ms\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"memmove:   \" &lt;&lt; benchmark(test_memmove(),  counter) &lt;&lt; \" ms\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"std::copy: \" &lt;&lt; benchmark(test_std_copy(), counter) &lt;&lt; \" ms\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"(counter:  \" &lt;&lt; counter &lt;&lt; \")\" &lt;&lt; std::endl &lt;&lt; std::endl;\n\n}\n</code></pre>\n\n<p>I noticed that for some reason <code>std::copy</code> performs much worse than memcpy. The output looks like this on my Mac using gcc 4.7.</p>\n\n<pre><code>g++ -o test -std=c++0x -O0 -Wall -Werror -Wextra -pedantic-errors main.cpp\ncast:      41 ms\nmemcpy:    46 ms\nmemmove:   53 ms\nstd::copy: 211 ms\n(counter:  3838457856)\n\ng++ -o test -std=c++0x -O1 -Wall -Werror -Wextra -pedantic-errors main.cpp\ncast:      8 ms\nmemcpy:    7 ms\nmemmove:   8 ms\nstd::copy: 19 ms\n(counter:  3838457856)\n\ng++ -o test -std=c++0x -O2 -Wall -Werror -Wextra -pedantic-errors main.cpp\ncast:      3 ms\nmemcpy:    2 ms\nmemmove:   3 ms\nstd::copy: 27 ms\n(counter:  3838457856)\n\ng++ -o test -std=c++0x -O3 -Wall -Werror -Wextra -pedantic-errors main.cpp\ncast:      2 ms\nmemcpy:    2 ms\nmemmove:   3 ms\nstd::copy: 16 ms\n(counter:  3838457856)\n</code></pre>\n\n<p>As you can see, even with <code>-O3</code>it is up to 5 times (!) slower than memcpy.</p>\n\n<p>The results are similar on Linux.</p>\n\n<p>Does anyone know why?</p>\n"},{"tags":["iphone","performance","pdf","catiledlayer"],"answer_count":4,"favorite_count":7,"up_vote_count":6,"down_vote_count":0,"view_count":5289,"score":6,"question_id":3442253,"title":"Why is this CATiledLayer/PDF code slow?","body":"<p>Here is the code:</p>\n\n<p><a href=\"https://dl.dropbox.com/u/147189/PDFScroller.zip\" rel=\"nofollow\">https://dl.dropbox.com/u/147189/PDFScroller.zip</a></p>\n\n<p>I took the WWDC 2010 PhotoScroller sample code that implements nested UIScrollViews for zooming, inside a UIScrollView for paging, and swapped out what I thought would be minimal amount of code required for displaying a multi-page PDF instead of images.</p>\n\n<p>It works. But it's slow on my iPhone4, about three seconds to paint the first page, and even slower on my iPod Touch. I can watch it painting the individual tiles. This same PDF already opens up more quickly, with no visible tile drawing, in an alternate CATiledLayer implementation I have which simply uses a single CATiledLayer/UIScrollView and touch events to change pages. I'd like to use this PhotoScroller technique, it's very nice.</p>\n\n<p>I watched it with CPU Sampler in Instruments, and it doesn't seem to be the PDF rendering code, it looks like the time is taken up in threading and messaging. I'd appreciate it if someone could help point out what this sample is doing to incur the overhead.</p>\n\n<p>Thanks,</p>\n\n<p>Jim</p>\n\n<hr>\n\n<p>Update 1: I had originally used the <code>TilingView</code> class technique from the sample code of defining</p>\n\n<pre><code>+ (Class) layerClass {\n  return [CATiledLayer class];\n}\n</code></pre>\n\n<p>And then drawing in <code>- (void)drawRect:(CGRect)rect</code> but switched to the explicit <code>CATiledLayer</code> subclass as a first attempt at seeing whether it would make a difference, but it did not, and so I left the code as-is for posting here.  There is also a missing <code>[tiledLayer release];</code> leak in TilingView.</p>\n"},{"tags":["iphone","ios","xcode","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":13130429,"title":"iOS app slow when segue between ViewControllers","body":"<p>i'm working on a iOS app and, when I try it on the device, it is too slow when changing the View. In the simulator it works perfectly.</p>\n\n<p>That's my viewDidLoad code. Y don't know if some function is too heavy to implement.</p>\n\n<pre><code>CGRect screenBounds = [[UIScreen mainScreen] bounds];\nif (screenBounds.size.height == 568) {\n    // code for 4-inch screen\n    self.backImage.image = [UIImage imageNamed:@\"settings-568h@2x.png\"];\n} else {\n    // code for 3.5-inch screen\n    self.backImage.image = [UIImage imageNamed:@\"settings@2x.png\"];\n}\n\narray = [[NSMutableArray alloc] initWithObjects:@\"Object1\",@\"Object2\",@\"Object3\", nil];\n\nNSString *loadedString = [array objectAtIndex:0]; //Defalut\nself.checkedIndexPath = [NSIndexPath indexPathForRow:0 inSection:0];\n\nNSString *savePath = [self pathOfSavingFile:@\"fileSetting\"];\nif([[NSFileManager defaultManager] fileExistsAtPath:savePath])\n{\n    NSString *tempSTR = [[NSString alloc] initWithContentsOfFile:savePath encoding:5 error:nil];\n    loadedString = tempSTR;\n}\n\nif([[array objectAtIndex:0] isEqualToString:loadedString]) {\n    self.checkedIndexPath = [NSIndexPath indexPathForRow:0 inSection:0];\n} else if ([[array objectAtIndex:1] isEqualToString:loadedString]) {\n    self.checkedIndexPath = [NSIndexPath indexPathForRow:1 inSection:0];\n} else if ([[array objectAtIndex:2] isEqualToString:loadedString]) {\n    self.checkedIndexPath = [NSIndexPath indexPathForRow:2 inSection:0];\n}\n\nUIApplication *pNote = [UIApplication sharedApplication];\n[[NSNotificationCenter defaultCenter] addObserver:self selector:@selector(applicationDidEnterBackground:) name:UIApplicationDidEnterBackgroundNotification object:myApp];\n\nself.titleLabel.text = NSLocalizedString(@\"settingsLoc\", nil);\n[self.backButton     setTitle:NSLocalizedString(@\"backLoc\", nil)     forState:UIControlStateNormal];\n\n[super viewDidLoad];\n</code></pre>\n\n<p>Thank you vey much.</p>\n"},{"tags":["performance","depth-first-search"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":12,"score":1,"question_id":13130378,"title":"Could this dfs implementation be made more faster","body":"<p>This is my first implementation on Graph Theory , and i think my implemented dfs code is very slow . How could i make it more fast ? </p>\n\n<p>Here's the code (it is actually solution to this problem <a href=\"http://www.iarcs.org.in/inoi/contests/dec2005/Advanced-2.php\" rel=\"nofollow\">http://www.iarcs.org.in/inoi/contests/dec2005/Advanced-2.php</a> ):- <a href=\"http://pastebin.com/FSizr10z\" rel=\"nofollow\">http://pastebin.com/FSizr10z</a></p>\n\n<p><strong>Can anybody please point out edit required in my dfs to make it fast , the dfs i use takes O(n^2) which is obviously too slow .</strong></p>\n"},{"tags":["performance","nhibernate"],"answer_count":4,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":1201,"score":4,"question_id":2151198,"title":"NHibernate select grows exponentially slow","body":"<p>My problem is that NHibernate gets exponentially slow when fetching records from the database. I had a request to basically pull all the data from a very large database to be used in a report. </p>\n\n<p>I figured, well since I can't get all the records in one shot because the recordset is so large, i thought try breaking it up. Basically I'm iterating through ranges of an index, ie. records id x to y, then y+1 to z, and so forth.</p>\n\n<p>Each result set is about 10megs. The first 20 or so pulls takes less than a minute each, then on the next pull, it takes 10minutes, then 30minutes, and 1hr. I stopped the program there, didn't want to wait till the next pull will come. I ran the program again starting from the index where I left off, again, the first 20 or so pulls are really quick, then for some odd reason there is a major slowdown.</p>\n\n<p>Any help would be greatly appreciated.</p>\n"},{"tags":["performance","matlab","for-loop","parallel-processing"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":46,"score":1,"question_id":13103229,"title":"parallelize a large loop","body":"<p>I have a large loop where I am trying to calculate certain attributes for each pixel in a DEM (4800x6000). I am calling a function demPHV in which I've vectorized all calculations that outputs a structure with 26 fields .  I have 4 cores but also have access to a multi-core cluster. I would like to speed up the weeks it would take to run this.</p>\n\n<p>Z is the dem for this example. R is the spatialref object (a vector for example's sake). latlim and lonlim are vectors of lat and long of the western US coastline (made up pairs in the example).\nfor example:</p>\n\n<pre><code>Z=rand(48,60);\nR=makerefmat(120,40,.5,.5)\nlatlim=[40:60]';\nlonlim=[136:(143-136)/(length(latlim)-1):143]';\n</code></pre>\n\n<p>Then my original loop:</p>\n\n<pre><code>for col=11:size(Z,2)-11\n    for row=11:size(Z,1)-11\n        dpv=demPHV(Z,R,row,col,latlim,lonlim)\n\n    fn=fieldnames(dpv);\n    for k=1:length(fieldnames(dpv))\n        DEM_PHV.(fn{k}).{row,col}=dpv.(fn{k});\n    end\nend\n</code></pre>\n\n<p>Loops for parallelizing:</p>\n\n<p>option 1:</p>\n\n<pre><code>[rows, cols] = meshgrid(12:(size(Z,1)-12), 12:(size(Z,2)-12));\ninds = sub2ind(size(Z), rows, cols);\ninds = inds(:)';\nparfor i=inds(1):inds(end)\n       dpv=demPHV(Z,R,i,latlim,lonlim)\nend\n</code></pre>\n\n<p>This includes <code>[r,c]=ind2sub(size(Z),i)</code> in the function to use in the function demPHV.</p>\n\n<p>option 2:</p>\n\n<pre><code>parfor col=11:size(Z,2)-11\n    for row=11:size(Z,1)-11\n         dpv=demPHV(Z,R,row,col,latlim,lonlim)\n    end\nend\n</code></pre>\n\n<p>parfor requires consecutive integers hence some of these changes.  I have to exclude the bordering 11 rows and columns because my function uses surrounding pixels to calculate some of the attributes.</p>\n\n<p>So, my questions:</p>\n\n<ol>\n<li>Would you expect either of these two options to be faster than the other?</li>\n<li><p>parfor does not allow me to include the second part of my original loop:</p>\n\n<p><code>fn=fieldnames(dpv);</code></p>\n\n<p><code>for k=1:length(fieldnames(dpv))</code></p>\n\n<p><code>DEM_PHV.(fn{k}).{row,col}=dpv.(fn{k});</code></p>\n\n<p><code>end</code></p></li>\n</ol>\n\n<p>during which I assign the output structure to another variable.  The ultimate goal is to have the variable DEM_PHV have fields for every attribute I need, and every field to be a matrix size(Z) where every cell is the corresponding value for that attribute. I've tried to have my function output the values in the correct cell of the matrix, but then I get a matrix size(Z) with <code>[]</code> everywhere except for the value at location <code>row,col</code>.  This seems like a horribly inefficient use of memory...  any better suggestions? I hope I covered everything.\nThanks for looking!</p>\n"},{"tags":["javascript","jquery","json","performance","jquery-autocomplete"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":49,"score":0,"question_id":13128628,"title":"Performance issue with JSON and auto Complete","body":"<p>I am trying to write an app that will work online and offline using technologies such as application cache and local storage. I am using jQuery mobile and an jqm autocomplete solution which can be found here <a href=\"http://andymatthews.net/code/autocomplete/\" rel=\"nofollow\">http://andymatthews.net/code/autocomplete/</a></p>\n\n<p>The idea is that if the app is on-line then I will be calling a remotely accessible function via ajax which will set data. The ajax calls that are made bring back data for events, countries and hospitals. Once the data as been returned I am setting the data in localStorage.</p>\n\n<p>If the data already exists in localStorage then I can continue while being offline as I don't have to rely on the ajax calls.</p>\n\n<p>I am having performance issues with this code when running on iPad/mobile devices. Where the hospital data is concerned. There is a large amount of data being returned for hospitals, please see sample of JSON for hospital data below. It can be a little slow on desktop but not nearly as bad. </p>\n\n<p>Can anyone see any improvements to the code I have below to increase the performance.</p>\n\n<p><strong>JSON hospital data example</strong></p>\n\n<pre><code>{ \"hospitals\" : { \n  \"1\" : { \"country\" : \"AD\",\n      \"label\" : \"CAIXA ANDORRANA SEGURETAT SOCIAL\"\n    },\n  \"10\" : { \"country\" : \"AE\",\n      \"label\" : \"Advance Intl Pharmaceuticals\"\n    },\n  \"100\" : { \"country\" : \"AT\",\n      \"label\" : \"BIO-KLIMA\"\n    },\n  \"1000\" : { \"country\" : \"BE\",\n      \"label\" : \"Seulin Nicolas SPRL\"\n    },\n  \"10000\" : { \"country\" : \"ES\",\n      \"label\" : \"Dental 3\"\n    },\n  \"10001\" : { \"country\" : \"ES\",\n      \"label\" : \"PROTESIS DENTAL MACHUCA PULIDO\"\n    },\n  \"10002\" : { \"country\" : \"ES\",\n      \"label\" : \"JUST IMPLANTS, S.L.\"\n    },\n  \"10003\" : { \"country\" : \"ES\",\n      \"label\" : \"CTRO DENTAL AGRIC ONUBENSE DR.DAMIA\"\n    },\n  \"10004\" : { \"country\" : \"ES\",\n      \"label\" : \"HTAL. VIRGEN DE ALTAGRACIA\"\n    },\n  \"10005\" : { \"country\" : \"ES\",\n      \"label\" : \"HOSPITAL INFANTA CRISTINA\"\n    }....\n\n\n/*global document,localStorage,alert,navigator: false, console: false, $: false */\n\n$(document).ready(function () {\n//ECMAScript 5 - It catches some common coding bloopers, throwing exceptions. http://ejohn.org/blog/ecmascript-5-strict-mode-json-and-more/\n//It prevents, or throws errors, when relatively \"unsafe\" actions are taken (such as gaining access to the global object).\n//It prevents, or throws errors, when relatively \"unsafe\" actions are taken (such as gaining access to the global object).\n\"use strict\";\n\n//initialise online/offline workflow variables\nvar continueWorkingOnline, continueWorkingOffline, availableEvents, availableHospitals, availableCountries, availableCities;\ncontinueWorkingOnline = navigator.onLine;\n\nvar getLocalItems = function () {\n\n    var localItems = [];    \n    availableEvents = localStorage.getItem('eventData');\n    availableHospitals = localStorage.getItem('hospitalData');\n    availableCountries = localStorage.getItem('countryData');\n\n    if (availableEvents) {\n        //only create the array if availableEvents exists\n        localItems[0] = [];\n        localItems[0].push(availableEvents);\n    }\n    if (availableCountries) {\n        //only create the array if availableCountries exists\n        localItems[1] = [];\n        localItems[1].push(availableCountries);\n    }\n    if (availableHospitals) {\n        //only create the array if availableHospitals exists\n        localItems[2] = [];\n        localItems[2].push(availableHospitals);\n    }\n    if (availableCities) {\n        //only create the array if availableHospitals exists\n        localItems[3] = [];\n        localItems[3].push(availableCities);\n    }\n    return localItems;              \n};\n\n\n//Check to see if there are 3 local items. Events, Countries, Cities. If true we know we can still run page off line\ncontinueWorkingOffline = getLocalItems().length === 3  ? true: false; \n\n//Does what is says on the tin\nvar populateEventsDropDown = function (data) {      \n    var eventsDropDown = $('#eventSelection');\n\n    var item = data.events;\n    $.each(item, function (i) {\n        eventsDropDown.append($('&lt;option&gt;&lt;/option&gt;').val(item[i].id).html(item[i].name));\n    });\n};\n\n//Called once getData's success call back is fired\nvar setFormData = function setData(data, storageName) {\n    //localStorage.setItem(storageName, data);\n    localStorage.setItem(storageName, data);\n};\n\n//This function is only called if continueWorkingOnline === true\nvar getRemoteFormData = function getData(ajaxURL, storageName) {\n    $.ajax({\n        url: ajaxURL,\n        type: \"POST\",\n        data: '',\n        success: function (data) {\n            setFormData(data, storageName);             \n        }           \n    });\n};\n\n//Function for autoComplete on Hospital data\nvar autoCompleteDataHospitals = function (sourceData) {\n\n    var domID = '#hospitalSearchField';\n    var country = $('#hiddenCountryID').val();\n\n    var items = $.map(sourceData, function (obj) { \n        if (obj.country === country) {\n            return obj;\n        } \n    });     \n\n    $(domID).autocomplete({\n        target: $('#hospitalSuggestions'),\n        source: items,\n        callback: function (e) {\n            var $a = $(e.currentTarget);\n            $(domID).val($a.data('autocomplete').label);                                        \n            $(domID).autocomplete('clear');\n        }\n    });\n};  \n\n//Function for autoComplete on Country data\nvar autoCompleteDataCountries = function (sourceData) {\n\n    var domID = '#countrySearchField';\n    var domHiddenID = '#hiddenCountryID';\n\n    var items = $.map(sourceData, function (obj) { \n        return obj; \n    }); \n\n    $(domID).autocomplete({\n        target: $('#countrySuggestions'),\n        source: items,\n        callback: function (e) {\n\n            var $a = $(e.currentTarget);\n            $(domID).val($a.data('autocomplete').label);                                        \n            $(domID).autocomplete('clear');\n            $(domHiddenID).val($a.data('autocomplete').value);\n\n            //enable field to enter Hospital\n            $('#hospitalSearchField').textinput('enable');\n\n            //Call to autoComplete function for Hospitals\n            autoCompleteDataHospitals(availableHospitals.hospitals);\n        }\n    });     \n};\n\nif (continueWorkingOnline === false &amp;&amp; continueWorkingOffline === false) {\n    alert(\"For best results this form should be initiated online. You can still use this but auto complete features will be disabled\");\n}\n\nif (continueWorkingOnline === true &amp;&amp; continueWorkingOffline === false) {               \n    getRemoteFormData('templates/cfc/Events.cfc?method=getEventsArray', 'eventData');       \n    getRemoteFormData('templates/cfc/Countries.cfc?method=getCountriesArray', 'countryData');\n    getRemoteFormData('templates/cfc/Hospitals.cfc?method=getHospitalsArray', 'hospitalData');\n\n    $(document).ajaxStop(function () {\n        //set the variables once localStorage has been set\n\n        availableEvents = JSON.parse(localStorage.getItem(\"eventData\"));\n        availableHospitals = JSON.parse(localStorage.getItem('hospitalData'));\n        availableCountries = JSON.parse(localStorage.getItem('countryData'));\n\n        //Inserts data into the events drop down\n        populateEventsDropDown(availableEvents);\n\n        autoCompleteDataCountries(availableCountries.countries);            \n    });\n}\n\nif (continueWorkingOnline === true &amp;&amp; continueWorkingOffline === true) {                \n    //get the localStorage which we know exists because of continueWorkingOffline is true\n\n    availableEvents = JSON.parse(localStorage.getItem('eventData'));\n    availableHospitals = JSON.parse(localStorage.getItem('hospitalData'));\n    availableCountries = JSON.parse(localStorage.getItem('countryData'));\n\n    //Inserts data into the events drop down\n    populateEventsDropDown(availableEvents);\n\n    autoCompleteDataCountries(availableCountries.countries);        \n}       \n});\n</code></pre>\n"},{"tags":["mysql","performance","web","cpu"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":38,"score":-1,"question_id":13128407,"title":"Mysql CPU usage over 800%","body":"<p>I've been having a lot of trouble recently with my webserver. The loading speed of my webpage is extremely slow and when I check htop I notice the mysql process to use around 800-1000% of CPU. I have tried everything the mysqltuner suggests but I have yet to see an improvement.</p>\n\n<p>Here are the mysqltuner results:</p>\n\n<blockquote>\n  <p>-------- General Statistics --------------------------------------------------</p>\n  \n  <p>[--] Skipped version check for MySQLTuner script</p>\n  \n  <p>[OK] Currently running supported MySQL version 5.1.54-log</p>\n  \n  <p>[OK] Operating on 64-bit architecture</p>\n  \n  <p>-------- Storage Engine Statistics -------------------------------------------</p>\n  \n  <p>[--] Status: +Archive -BDB -Federated +InnoDB -ISAM -NDBCluster</p>\n  \n  <p>[--] Data in MyISAM tables: 58M (Tables: 243)</p>\n  \n  <p>[--] Data in InnoDB tables: 34M (Tables: 303)</p>\n  \n  <p>[!!] Total fragmented tables: 304</p>\n  \n  <p>-------- Performance Metrics -------------------------------------------------</p>\n  \n  <p>[--] Up for: 2d 0h 10m 24s (3M q [20.584 qps], 594K conn, TX: 2B, RX:\n  373M)</p>\n  \n  <p>[--] Reads / Writes: 78% / 22%</p>\n  \n  <p>[--] Total buffers: 652.0M global + 14.6M per thread (250 max threads)</p>\n  \n  <p>[OK] Maximum possible memory usage: 4.2G (82% of installed RAM)</p>\n  \n  <p>[OK] Slow queries: 1% (50K/3M)</p>\n  \n  <p>[OK] Highest usage of available connections: 46% (115/250)</p>\n  \n  <p>[OK] Key buffer size / total MyISAM indexes: 256.0M/18.7M</p>\n  \n  <p>[OK] Key buffer hit rate: 100.0% (3B cached / 46K reads)</p>\n  \n  <p>[OK] Query cache efficiency: 49.9% (785K cached / 1M selects)</p>\n  \n  <p>[!!] Query cache prunes per day: 24539</p>\n  \n  <p>[OK] Sorts requiring temporary tables: 0% (0 temp sorts / 91K sorts)</p>\n  \n  <p>[!!] Joins performed without indexes: 199247</p>\n  \n  <p>[!!] Temporary tables created on disk: 29% (159K on disk / 547K total)</p>\n  \n  <p>[OK] Thread cache hit rate: 99% (118 created / 594K connections)</p>\n  \n  <p>[OK] Table cache hit rate: 66% (1K open / 1K opened)</p>\n  \n  <p>[OK] Open file limit used: 23% (1K/4K)</p>\n  \n  <p>[OK] Table locks acquired immediately: 99% (1M immediate / 1M locks)</p>\n  \n  <p>[OK] InnoDB data size / buffer pool: 34.4M/128.0M</p>\n  \n  <p>-------- Recommendations -----------------------------------------------------</p>\n  \n  <p>General recommendations:</p>\n\n<pre><code>Run OPTIMIZE TABLE to defragment tables for better performance\n\nAdjust your join queries to always utilize indexes\n\nWhen making adjustments, make tmp_table_size/max_heap_table_size equal\n\nReduce your SELECT DISTINCT queries without LIMIT clauses\n</code></pre>\n  \n  <p>Variables to adjust:</p>\n\n<pre><code>query_cache_size (&gt; 80M)\n\njoin_buffer_size (&gt; 12.0M, or always use indexes with joins)\n\ntmp_table_size (&gt; 180M)\n\nmax_heap_table_size (&gt; 180M)\n</code></pre>\n</blockquote>\n\n<p>What could I do to improve the webpage loading speed and reduce the mysql stress on the CPU?</p>\n"},{"tags":["mysql","database","performance","query"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":24,"score":1,"question_id":13127703,"title":"need a sql query to find out the Database wait time ratio for MySQL","body":"<p>Oracle supports sql queries to <code>SYS.V_$SYSMETRIC</code> to find out Database Wait Time Ratio (% of wait or bottlenecks experienced in time spent on user-level calls)  <a href=\"http://www.oracle.com/technetwork/articles/schumacher-analysis-099313.html\" rel=\"nofollow\">http://www.oracle.com/technetwork/articles/schumacher-analysis-099313.html</a>. What is an equivalent query to find similar stats for MySQL database? </p>\n\n<p>I have looked at <code>information_schema.PROFILING</code> and <code>information_schema.PROCESSLIST</code> tables to calculate a similar measure but unable to comprehend what values to use / address.</p>\n\n<p>Any suggestions?</p>\n\n<p>I need it for both Windows and Linux Platforms with remote MySql server. I think a light-weight DB query would help as opposed to gathering CPU Utilization for the machine hosting MySQL server. </p>\n\n<p>Thanks.</p>\n"},{"tags":["project-management","process","performance","kanban"],"answer_count":5,"favorite_count":3,"up_vote_count":6,"down_vote_count":0,"view_count":1348,"score":6,"question_id":1917264,"title":"Kanban as a Software Development Process in Practice","body":"<p>Has anyone used the <a href=\"http://www.kanbandistilled.com/\" rel=\"nofollow\">kanban method</a> for software development management?</p>\n\n<p>I am evaluating kanban as a technique and would be curious to hear from anyone who has actually applied it in practice as to how effective it is. I've seen questions like: <a href=\"http://stackoverflow.com/questions/924121/is-anyone-using-kanban\">is-anyone-using-kanban</a>, <a href=\"http://stackoverflow.com/questions/1156667/kanban-vs-scrum\">kanban-vs-scrum</a>, and <a href=\"http://stackoverflow.com/questions/1367491/apply-kanban-in-an-agile-team\">apply-kanban-in-an-agile-team</a> but they don't address my concerns.</p>\n\n<p>What I'm interested in specifically is:</p>\n\n<ol>\n<li>Does it actually offer the advantages is claims in terms of dynamically identifying bottlenecks?</li>\n<li>Is it easy to execute in practice, or does it have logistical challenges that you need to manage?</li>\n<li>Does it scale well to project teams with many parallel streams of work and many developers?</li>\n<li>How does it compare to critical path analysis (as implemented in MS Project), how is it different?</li>\n<li>What other benefits can be gained from applying kanban?</li>\n</ol>\n\n<p>Thanks.</p>\n"},{"tags":["django","performance","http","caching","http-etag"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":104,"score":1,"question_id":2752046,"title":"Any reason not to use USE_ETAGS with CommonMiddleware in Django?","body":"<p>The only reason I can think of is that calculating <code>ETag</code>'s might be expensive. If pages change very quickly, the browser's cache is likely to be invalidated by the <code>ETag</code>. In that case, calculating the <code>ETag</code> would be a waste of time. On the other hand, a giving a <code>304</code> response when possible minimizes the amount of time spent in transmission. What are some good guidelines for when <code>ETag</code>'s are likely to be a net winner when implemented with Django's <code>CommonMiddleware</code>?</p>\n"},{"tags":["mysql","performance","mysqli"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":39,"score":1,"question_id":13126795,"title":"What is the fastest way to insert thousand records in mysql?","body":"<p>I want to insert many thousand records and I need this to be fast. I read a lot about this issue so I decided to drop my old approach (mysql_connect) and use prepare statements and mysqli. So In order to test the speed of this I write the following.</p>\n\n<pre><code>function load_data2db($sms_id){\n$mysqli = new mysqli('localhost', 'root', 'cr3at1ve', 'tmp-clicknsend');\n/* check connection */\nif (mysqli_connect_errno()) {\n    printf(\"Connect failed: %s\\n\", mysqli_connect_error());\n    exit();\n}\n\n\n// Create statement object\n$stmt = $mysqli-&gt;stmt_init();\nif ($stmt = $mysqli-&gt;prepare(\"INSERT INTO isec_test (sms_id, status,msgid,prmsgid,mob,sms_history_id) values (?, ?, ?, ?, ?, ?)\")) {\n\n\n        /* Bind our params */\n        $stmt-&gt;bind_param('ssssss',$sms_id,$status,$msgid,$prmsgid,$mob,$sms_history_id);\n\n    for($i=0;$i&lt;100000;$i++)\n    {\n        /* Set our params */\n        $sms_id = \"110\";\n        $status = \"OK\";\n        $msgid  = \"msgid\";\n        $prmsgid = \"100-0\";\n        $mob = \"306974386068\";\n        $sms_history_id = 102;\n\n        /* Execute the prepared Statement */\n        $stmt-&gt;execute();\n    }\n\n\n        /* Close the statement */\n        $stmt-&gt;close();\n}\nelse {\n        /* Error */\n        printf(\"Prepared Statement Error: %s\\n\", $mysqli-&gt;error);\n}\n}\n\nload_data2db(10);\n</code></pre>\n\n<p>Then I did the same with the old way ( looping and inserting one by one)</p>\n\n<pre><code>include(\"mysql_connect.php\");\n for($i=0;$i&lt;100000;$i++)\n    {\n    $query = \"INSERT INTO isec_test(sms_id,status,msgid) values ('1','OK','123-123')\";\n    $query = @mysql_query($query);\n    }\nmysql_close($dbc);\n</code></pre>\n\n<p>I made a lot of tests and always the simple mysql way was 1 sec faster. So I am puzzled. What can I do? Use LOAD DATA?</p>\n"},{"tags":["mysql","performance","node.js","database-connection"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":23,"score":0,"question_id":13126824,"title":"nodejs-mysql-native vs node-mysql - Special situation","body":"<p>I'm working on a NodeJS server that will interact a lot with a MySQL server. This will be an API server that handles authenticated responses from clients. The special situation here is that the back end will have multiple databases, which means that every client request would require a new database connection. Has someone work on an architecture similar? what module would you suggest?</p>\n\n<p>My concern is that the database selection for each query would slow down the application considerably.</p>\n"},{"tags":["performance","gwt","zk"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":74,"score":1,"question_id":13064470,"title":"zk vs gwt zk too many requests","body":"<p>Hi I am trying to compare the performance of zk and gwt.</p>\n\n<p>In my comparision I cant write any javascript by myself if the framework itself converts some code into js its ok(like gwt) but I cant write js by myself</p>\n\n<p>On writing code in the above way for almost anything done on browser a request is sent to the server in ZK.</p>\n\n<p>Hence eventually if you compare the no of request sent by zk to server is too high as compared to gwt.</p>\n\n<p>i would like to ask the following.</p>\n\n<ol>\n<li>whose performance is better zk or gwt while ignoring the above.</li>\n<li>if we dont ignore the above then is my conclusion that gwts performance is better than  zk right ?</li>\n</ol>\n\n<p>I know that there might be other parameters when comparing performance... but if the difference between requests are so high i cant really see zk beating gwt which some people have said on some forums</p>\n\n<p>pls help\nthanks</p>\n"},{"tags":["performance","compiler","haskell","ghc"],"answer_count":4,"favorite_count":53,"up_vote_count":64,"down_vote_count":0,"view_count":2979,"score":64,"question_id":6121146,"title":"Reading GHC Core","body":"<p>Core is GHC's intermediate language. Reading Core can help you better understand the performance of your program. Someone asked me for documentation or tutorials on reading Core, but I couldn't find much.</p>\n\n<p>What documentation is available for reading GHC Core?</p>\n\n<p>Here's what I've found so far:</p>\n\n<ul>\n<li><a href=\"http://donsbot.wordpress.com/2008/05/06/write-haskell-as-fast-as-c-exploiting-strictness-laziness-and-recursion/\" rel=\"nofollow\">Write Haskell as fast as C: exploiting strictness, laziness and recursion</a></li>\n<li><a href=\"http://donsbot.wordpress.com/2008/06/04/haskell-as-fast-as-c-working-at-a-high-altitude-for-low-level-performance/\" rel=\"nofollow\">Haskell as fast as C: working at a high altitude for low level performance</a></li>\n<li><a href=\"http://book.realworldhaskell.org/read/profiling-and-optimization.html\" rel=\"nofollow\">RWH: Chapter 25. Profiling and optimization</a></li>\n<li><a href=\"http://blog.johantibell.com/2010/09/slides-from-my-high-performance-haskell.html\" rel=\"nofollow\">High-Performance Haskell talk at CUFP</a> (slide 65-80)</li>\n</ul>\n"},{"tags":["javascript","performance","compiler","v8"],"answer_count":1,"favorite_count":5,"up_vote_count":15,"down_vote_count":0,"view_count":233,"score":15,"question_id":12497995,"title":"JavaScript object code caching: which of these assertions are wrong?","body":"<p>Because I have been around engineers for so many years, I know that if I don't provide context, I'm just going to get a hundred answers of the form \"What are you trying to accomplish?\" I am going to give the background which motivates my question. But <strong>don't confuse the background context for the question I am asking</strong>, which is specifically related to the JavaScript semantics that made object code uncacheable between padge requests. I am not going to give marks for advice on how to make my webapp faster. It's completely tangential to my question, which will probably only be answerable by someone who has worked on a JavaScript compiler or at least the compiler for a dynamic language.</p>\n\n<p>Background:</p>\n\n<p>I am trying to improve the performance of a web application. Among its many resources, it contains one enormous JavaScript file with 40k lines and 1.3million characters pre-minification. Post-minification it's still large, and it still adds about 100ms to the window.onload event when synchronously loaded, even when the source is cached client-side. (I have conclusively ruled out the possibility that the resource is not being cached by watching the request logs and observing that it is not being requested.)</p>\n\n<p>After confirming that it's still slow after being cached, I started doing some research on JavaScript caching in the major browsers, and have learned that none of them cache object code.</p>\n\n<hr>\n\n<p>My question is in the form of some hypothetical assertions based on this research. Please object to these assertions if they are wrong.</p>\n\n<ol>\n<li><p>JavaScript object code is not cached in any modern browser.</p>\n\n<p>\"Object code\" can mean anything from a byte code representing a simple linearized parse tree all the way to <a href=\"https://developers.google.com/v8/design#mach_code\">native machine code</a>.</p></li>\n<li><p>JavaScript object code in a web browser is difficult to cache.</p>\n\n<p>In other words, even if you're including a cached JS source file in an external  tag, there is a linear cost to the inclusion of that script on a page, even if the script contains only function definitions, because all of that source needs to be compiled into an object code.</p></li>\n<li><p>JavaScript object code is difficult to cache because JS source must evaluated in order to be compiled.</p>\n\n<p>Statements have the ability to affect the compilation of downstream statements in a dynamic way that is difficult to statically analyze.</p>\n\n<p>3a. (3) is true mostly because of eval().</p></li>\n<li><p>Evaluation can have side effects on the DOM.</p></li>\n<li><p><em>Therefore</em>, JavaScript source needs to be compiled on every page request.</p></li>\n</ol>\n\n<p>Bonus question: do any modern browsers cache a parse tree for cached JS source files? If not, why not?</p>\n\n<p>Edit: If all of these assertions are correct, then I will give the answer to anyone who can <strong>expound on why they are correct,</strong> for example, by providing a sample of JS code that couldn't be cached as object code and then explaining why not.</p>\n\n<p>I appreciate the suggestions on how to proceed from here to make my app faster, and I mostly agree with them. But the knowledge gap that I'm trying to fill is related to JS object code caching.</p>\n"},{"tags":["sql","sql-server","performance","sql-server-2008","ssis"],"answer_count":3,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":497,"score":4,"question_id":9290291,"title":"SQL Script in VM taking long time for execution","body":"<p>I have the Execute SQL Script package that contains the script to insert about 150K records.</p>\n\n<p>Problem in here is when I execute the package in the Virtual machine its taking 25 min's approx and the same package in physical machine its taking 2 min's</p>\n\n<p>Question 1? Why its taking that much time to load the same data in VM.\nQuestion 2? How to solve this performance issue.</p>\n\n<p>Physical machine configuration has 4GB Ram and 250GB HD + Windows server 2008 R2 + SQL server 2008 R2 Standard Edition.\nVirtual machine has the same Configuration</p>\n\n<p><strong>Update: The Problem is with the SQL Server in VM.</strong></p>\n\n<p><strong>Question 1? Why its taking that much time to Run the same script in VM.</strong></p>\n\n<p><strong>Question 2? How to solve this performance issue.</strong></p>\n\n<p>Both the batabases schema in Physical Machine and VM are identical. Other databases are also same. There was no indexing applied for that tables in both machines. Datatypes are same. harddisk as I said has the same configuration.</p>\n\n<p>No RAID is done on both the machines.</p>\n\n<blockquote>\n  <p>Physical machine has the 2.67GHz RAM Quad Core and in the virtual machine has the\n  2.00GHz RAM Quad Core</p>\n</blockquote>\n\n<p>Version of SQL PM:</p>\n\n<p>Microsoft SQL Server 2008 R2 (RTM) - 10.50.1600.1 (X64) Apr  2 2010 15:48:46    Copyright (c) Microsoft Corporation Standard Edition (64-bit) on Windows NT 6.1  (Build 7601: Service Pack 1)</p>\n\n<p>Version of SQL PM:</p>\n\n<p>Microsoft SQL Server 2008 R2 (RTM) - 10.50.1600.1 (X64) Apr  2 2010 15:48:46    Copyright (c) Microsoft Corporation Standard Edition (64-bit) on Windows NT 6.1  (Build 7601: Service Pack 1) (Hypervisor)</p>\n\n<p>I executed the script Execution plan for both are the same as there is no difference in plan.</p>\n\n<p>Vendor is HP ML350 Machine.</p>\n\n<p>There are almost 20 VM's on the same physical server out of which 7 servers are active.</p>\n"},{"tags":["c++","performance"],"answer_count":3,"favorite_count":1,"up_vote_count":3,"down_vote_count":1,"view_count":161,"score":2,"question_id":13117211,"title":"Why is memcpy slower than a reinterpret_cast when parsing binary data?","body":"<h3>TLDR: I forgot to enable compiler optimizations. With the optimizations enabled the performance is (nearly) identical.</h3>\n\n<p><br/></p>\n\n<h3>Original post</h3>\n\n<p>When reading integer from binary data I noticed that memcpy is slower than a casting solution.</p>\n\n<p>Version 1: reinterpret_cast, smelly due to potential alignment problems, but also faster (?)</p>\n\n<pre><code>int get_int_v1(const char * data) { return *reinterpret_cast&lt;const int*&gt;(data); }\n</code></pre>\n\n<p>Version 2: memcpy, correct and a little slower:</p>\n\n<pre><code>int get_int_v2(const char * data) { int result; memcpy(&amp;result, data, sizeof(result)); return result; }\n</code></pre>\n\n<p>I have <a href=\"http://ideone.com/6LEmA3\" rel=\"nofollow\">a benchmark on Ideone</a>.</p>\n\n<p>For future reference, the code is:</p>\n\n<pre><code>#include &lt;cstdlib&gt;\n#include &lt;cstdio&gt;\n#include &lt;cstring&gt;\n#include &lt;ctime&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;sys/time.h&gt;\n\ndouble get_current_time()\n{\n    timeval tv;\n    gettimeofday(&amp;tv, NULL);\n    return double (tv.tv_sec) + 0.000001 * tv.tv_usec;\n}\n\nint get_int_v1(const char * data) { return *reinterpret_cast&lt;const int*&gt;(data); }\nint get_int_v2(const char * data) { int result; memcpy(&amp;result, data, sizeof(result)); return result; }\n\nconst unsigned iterations = 200 * 1000 * 1000;\n\ndouble test_v1(const char * c, unsigned &amp; prevent_optimization)\n{\n    double start = get_current_time();\n    for (unsigned i = 0; i != iterations; ++i)\n    {\n        prevent_optimization += get_int_v1(c);\n    }\n    return get_current_time() - start;\n}\n\ndouble test_v2(const char * c, unsigned &amp; prevent_optimization)\n{\n    double start = get_current_time();\n    for (unsigned i = 0; i != iterations; ++i)\n    {\n        prevent_optimization += get_int_v2(c);\n    }\n    return get_current_time() - start;\n}\n\nint main()\n{\n    srand(time(0));\n\n    // Initialize data\n    std::vector&lt;int&gt; numbers(1000);\n    for (std::vector&lt;int&gt;::size_type i = 0; i != numbers.size(); ++i)\n    {\n        numbers[i] = i;\n    }\n\n    // Repeat benchmark 4 times.\n    for (unsigned i = 0; i != 4; ++i)\n    {\n        unsigned p = 0;\n        std::vector&lt;int&gt;::size_type index = rand() % numbers.size();\n        const char * c = reinterpret_cast&lt;const char *&gt;(&amp;numbers[index]);    \n        std::cout &lt;&lt; \"v1: \" &lt;&lt; test_v1(c, p) &lt;&lt; std::endl;\n        std::cout &lt;&lt; \"v2: \" &lt;&lt; test_v2(c, p) &lt;&lt; std::endl &lt;&lt; std::endl;\n    }\n}\n</code></pre>\n\n<p>And the results are:</p>\n\n<pre><code>v1: 0.176457\nv2: 0.557588\n\nv1: 0.17654\nv2: 0.220581\n\nv1: 0.176826\nv2: 0.22012\n\nv1: 0.176131\nv2: 0.220633\n</code></pre>\n\n<p>My questions are: </p>\n\n<ul>\n<li>Is my benchmark correct?</li>\n<li>If yes, then why is v2 (with memcpy) slower? Since both version return a copy of the data I think there should be no difference in performance.</li>\n<li>How can I implement a solution that is correct and fast?</li>\n</ul>\n\n<p><br/></p>\n\n<h3>Update</h3>\n\n<p>I was being silly and forgot to consider that Ideone doesn't perform compiler optimizations. I also tweaked the code a little and came up with the following:</p>\n\n<pre><code>#include &lt;algorithm&gt;\n#include &lt;cstdlib&gt;\n#include &lt;cstdio&gt;\n#include &lt;cstring&gt;\n#include &lt;ctime&gt;\n#include &lt;iomanip&gt; \n#include &lt;iostream&gt; \n#include &lt;vector&gt;\n#include &lt;sys/time.h&gt;\n\ndouble get_current_time()\n{\n    timeval tv;\n    gettimeofday(&amp;tv, NULL);\n    return double (tv.tv_sec) + 0.000001 * tv.tv_usec;\n}\n\nstruct test_cast\n{\n    int operator()(const char * data) const \n    {\n        return *((int*)data);\n    }\n};\n\nstruct test_memcpy\n{\n    int operator()(const char * data) const \n    {\n        int result;\n        memcpy(&amp;result, data, sizeof(result));\n        return result;\n    }\n};\n\nstruct test_std_copy\n{\n    int operator()(const char * data) const \n    {\n        int result;\n        std::copy(data, data + sizeof(int), reinterpret_cast&lt;char *&gt;(&amp;result));\n        return result;\n    }\n};\n\nenum\n{\n    iterations = 2000,\n    container_size = 2000\n};\n\nstd::vector&lt;int&gt; get_random_numbers()\n{\n    std::vector&lt;int&gt; numbers(container_size);\n    for (std::vector&lt;int&gt;::size_type i = 0; i != numbers.size(); ++i)\n    {\n        numbers[i] = rand();\n    }\n    return numbers;\n}\n\nstd::vector&lt;int&gt; get_random_indices()\n{\n    std::vector&lt;int&gt; numbers(container_size);\n    for (std::vector&lt;int&gt;::size_type i = 0; i != numbers.size(); ++i)\n    {\n        numbers[i] = i;\n    }\n    std::random_shuffle(numbers.begin(), numbers.end());\n    return numbers;\n}\n\ntemplate&lt;typename Function&gt;\nunsigned benchmark(const Function &amp; f, unsigned &amp; counter)\n{\n    std::vector&lt;int&gt; container = get_random_numbers();\n    std::vector&lt;int&gt; indices = get_random_indices();\n    double start = get_current_time();\n    for (unsigned iter = 0; iter != iterations; ++iter)\n    {\n        for (unsigned i = 0; i != container.size(); ++i)\n        {\n            counter += f(reinterpret_cast&lt;const char*&gt;(&amp;container[indices[i]]));\n        }\n    }\n    return unsigned(0.5 + 1000.0 * (get_current_time() - start));\n}\n\nint main()\n{\n    srand(time(0));\n    unsigned counter = 0;\n\n    std::cout &lt;&lt; \"cast:      \" &lt;&lt; benchmark(test_cast(),     counter) &lt;&lt; \" ms\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"memcpy:    \" &lt;&lt; benchmark(test_memcpy(),   counter) &lt;&lt; \" ms\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"std::copy: \" &lt;&lt; benchmark(test_std_copy(), counter) &lt;&lt; \" ms\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"(counter:  \" &lt;&lt; counter &lt;&lt; \")\" &lt;&lt; std::endl &lt;&lt; std::endl;\n\n}\n</code></pre>\n\n<p>The results are now nearly equal (except for <code>std::copy</code> which is slower for some reason):</p>\n\n<pre><code>g++ -o test -O0 -Wall -Werror -Wextra -pedantic-errors main.cpp\ncast:      56 ms\nmemcpy:    60 ms\nstd::copy: 290 ms\n(counter:  2854155632)\n\ng++ -o test -O1 -Wall -Werror -Wextra -pedantic-errors main.cpp\ncast:      9 ms\nmemcpy:    14 ms\nstd::copy: 20 ms\n(counter:  3524665968)\n\ng++ -o test -O2 -Wall -Werror -Wextra -pedantic-errors main.cpp\ncast:      4 ms\nmemcpy:    5 ms\nstd::copy: 20 ms\n(counter:  2590914608)\n\ng++ -o test -O3 -Wall -Werror -Wextra -pedantic-errors main.cpp\ncast:      4 ms\nmemcpy:    5 ms\nstd::copy: 18 ms\n(counter:  2590914608)\n</code></pre>\n"},{"tags":["iphone","performance","html5"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":13125013,"title":"Iphone4 \"add to home screen\" instant startup","body":"<p>How do you make a web app start up instantly - is it possible to make as fast as a native app that is already loaded?</p>\n\n<p>I have developed a HTML5 web app that runs nicely on Iphone 4 with a splashscreen. But now I'm looking into performance:</p>\n\n<p>I use a manifest file to ensure that all files are loaded from the local storage. I have checked both in chrome and mobile safari, that the files are stored correctly locally. Now performance it quite different depending on how I access my web app:</p>\n\n<p>~4s When I load the web app in browser (not from a home screen icon).\n~6s When I load from an \"add to homescreen\" icon</p>\n\n<p>When I load in chrome browser it takes 234ms to load and render the whole page. I seems like the lack of speed is due to the rendering being pretty slow.</p>\n\n<p>Any performance suggestions are very welcome. </p>\n"},{"tags":["java","performance","character-encoding"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":89,"score":3,"question_id":13120585,"title":"Decoding characters in Java: why is it faster with a reader than using buffers?","body":"<p>I am trying several ways to decode the bytes of a file into characters.</p>\n\n<p>Using <em>java.io.Reader</em> and <em>Channels.newReader(...)</em></p>\n\n<pre><code>public static void decodeWithReader() throws Exception {\n    FileInputStream fis = new FileInputStream(FILE);\n    FileChannel channel = fis.getChannel();\n    CharsetDecoder decoder = Charset.defaultCharset().newDecoder();\n    Reader reader = Channels.newReader(channel, decoder, -1);\n\n    final char[] buffer = new char[4096];\n    for(;;) {\n        if(-1 == reader.read(buffer)) {\n            break;\n        }\n    }\n\n    fis.close();\n}\n</code></pre>\n\n<p>Using buffers and a decoder <em>manually</em>:</p>\n\n<pre><code>public static void readWithBuffers() throws Exception {\n    FileInputStream fis = new FileInputStream(FILE);\n    FileChannel channel = fis.getChannel();\n    CharsetDecoder decoder = Charset.defaultCharset().newDecoder();\n\n    final long fileLength = channel.size();\n    long position = 0;\n    final int bufferSize = 1024 * 1024;   // 1MB\n\n    CharBuffer cbuf = CharBuffer.allocate(4096);\n\n    while(position &lt; fileLength) {\n        MappedByteBuffer bbuf = channel.map(MapMode.READ_ONLY, position, Math.min(bufferSize, fileLength - position));\n        for(;;) {\n            CoderResult res = decoder.decode(bbuf, cbuf, false);\n\n            if(CoderResult.OVERFLOW == res) {\n                cbuf.clear();\n            } else if (CoderResult.UNDERFLOW == res) {\n                break;\n            }\n        }\n        position += bbuf.position();\n    }\n\n    fis.close();\n}\n</code></pre>\n\n<p>For a 200MB text file, the first approach consistently takes 300ms to complete. The second approach consistently takes 700ms. Do you have any idea why the reader approach is so much faster?</p>\n\n<p>Can it run even faster with another implementation?</p>\n\n<p>The benchmark is performed on Windows 7, and JDK7_07.</p>\n"},{"tags":["performance","timer"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":14,"score":1,"question_id":13122308,"title":"System.Diagnostics.Stopwatch timer off by factor of 3","body":"<p>I'm trying to measure elapsed time using <code>Stopwatch</code> but consistently get times which are lower ~3.1 times than actual elapsed time. I can't put my finger on why this happens, code is dead simple. I've tried the code on 3 machines and getting same result (although with nearly same hardware and software).</p>\n\n<p>i5-2500k, i5-2500, Windows 7 64 bit, Windows 8 RTM 64 bit</p>\n\n<pre><code>Stopwatch sw = new Stopwatch();\nsw.Restart();\nDateTime start = DateTime.Now;\n\nfor (int i = 0; i &lt; 5; i++)\n{\n    Thread.Sleep(1000);\n}\n\nDateTime end = DateTime.Now;\nsw.Stop();\n\nConsole.WriteLine(\"Time passed: \" + StringFormatter.ToShortString(sw));\nConsole.WriteLine(\"Time passed: \" + StringFormatter.ToShortString(end - start));\n</code></pre>\n\n<p>result is consistently ~3.1 times off.</p>\n\n<pre><code>Time passed: 00:00:01.6347\nTime passed: 00:00:05.0600\n</code></pre>\n\n<p>for 10 sec delays:</p>\n\n<pre><code>Time passed: 00:00:03.2358\nTime passed: 00:00:10.0100\n</code></pre>\n"},{"tags":["c#","performance","linq-to-sql"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":44,"score":0,"question_id":13122152,"title":"Linq-toSql mass update speedup possibilities","body":"<p>I would be grateful if community could suggest some ways to speed-up query like</p>\n\n<pre><code>(from item in _dbContext.Items\n where item.SomeField &gt; someConstant\n &amp;&amp; item.ParentID==otherConstant\n select item).ToList().ForEach(item=&gt;item.FieldToChange = item.FieldToChange+3);\ncontext.SubmitChanges();\n</code></pre>\n\n<p>As I saw from profiler this equals to CountOfSelectedItems separate updates. With 200+ ms ping it makes me cry.</p>\n\n<p>Stored procedures and plain sql queries are restricted by our architects. Looking for transactions-like solution. </p>\n"},{"tags":["performance","http","iis-7.5","windows-server-2008"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":119,"score":0,"question_id":12980115,"title":"Please help resolve bottle neck in wait times for Http Responses?","body":"<p>As far as a performance issue, the server is performing fine.  With the exception of the http response wait times.  This will become more of an issue as we grow our line of online services.  All things being equal, I’m confused how this new server is it not loading pages as quickly as an older server running multiple websites, logging, etc… </p>\n\n<p>Here is a screen shot from <a href=\"http://www.gtmetrix.com\" rel=\"nofollow\">http://www.gtmetrix.com</a> the online testing tool I’ve been using.  These results are consistent regardless of time of day,  The numbers here don’t make sense.  The new site page is 75% smaller, yet its total time to live is only 26ms faster.  In the below image the left side is NEW SERVER, the right side is OLD SERVER\nThe left portion of the timeline is the Handshaking portion.  So, you can see, the new server, is about the same speed.  The purple middle section, that represents wait time.  It’s about 4 times the delay in milliseconds as OLD SERVER.  The Grayish section on the right represents the actual time to download the file.  You will also notice that the new server is significantly faster at downloading the response, this is most likely due to the 75% decrease in the response size.</p>\n\n<p><img src=\"http://i.stack.imgur.com/UKW8m.gif\" alt=\"GtMetrix Resuts\"> You can see the complete results for the new server here. <a href=\"http://gtmetrix.com/reports/204.193.113.47/Kl614UCf\" rel=\"nofollow\">http://gtmetrix.com/reports/204.193.113.47/Kl614UCf</a></p>\n\n<p>Here’s a table of the differences that I’m aware of, let me know if you see one that could be the culprit.  I forgot to add this to the table, but the old server, is in production, right now serving requests, when www.gtmetrix is hitting it.  In contrast, to my New server, which is just me connecting and generating requests.</p>\n\n<p><img src=\"http://i.stack.imgur.com/Kazw0.gif\" alt=\"Server Config Comparison\">      </p>\n\n<p>My current hypothesis, is that the slowness is caused some combination of the server being virtualized, incorrect IIS settings, or the difference between 32bit and 64bit OSes</p>\n"},{"tags":["performance","ios5","uiview","uiimageview"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":28,"score":0,"question_id":13085462,"title":"UIImageView delay when adding to another UIView","body":"<p>This is my first post here - but I've been a reader for a long time. Thank you so much for this site! :-)</p>\n\n<p>I am currently working on a port of my XNA-based 2D engine from WP7 to iOS (5). I would prefer not to use OpenGL directly, because I prefer to invest my time more in gameplay than in technique. So I would be very happy to get an answer not involving OpenGL (directly).</p>\n\n<p>The problem:\nWhen I add an UIImageView to another UIView, there is a short delay before the UIImageView gets drawn. I assume, this is due to the caching the UIView-class performs before converting everything internally to OpenGL then drawing.</p>\n\n<p>What I want:\nTell the UIView (superview) to perform all neccessary calculations for all subviews and <em>then</em> draw them <em>all</em> at once.\nCurrently the behaviour I observe ist: Calculate uiimageview_1, draw uiimageview_1, calculate uiimageview_n, draw uiimageview_n, ...</p>\n\n<p>Dummycode of what I want:</p>\n\n<pre><code>// put code here to tell superview to pause drawing\nfor (int i = 0; i &lt; 400; i ++)\n{\n    add UIImageView[i] to superview;\n}\n// put code here to tell superview to draw now\n</code></pre>\n\n<p>Possible workaround (but coming from C# &amp; Windows, I have no idea how to implement it efficiently in Objective-C on iOS) - I am afraid that this code is inefficient because large blocks of RAM had to be transferred (per frame!) on retina displays at native resolution:</p>\n\n<pre><code>for (int i = 0; i &lt; 400; i ++)\n{\n    add UIImageView[i] to superview;\n}\n// put code here to get a bitmap in ram from superview\n// return bitmap and draw it in a view for the scenery/canvas\n</code></pre>\n\n<p>Any help on how to approach this \"popping\"-problem would be highly appreciated.</p>\n\n<p>Thanks</p>\n"},{"tags":["performance","matlab","vectorization"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":223,"score":3,"question_id":9008259,"title":"Speeding up MATLAB code for FDR estimation","body":"<p>I have 2 input variables: </p>\n\n<ul>\n<li>a vector of p-values (<strong>p</strong>) with <em>N</em> elements (unsorted)</li>\n<li>and <em>N</em> x <em>M</em> matrix with p-values obtained by random permutations (<strong>pr</strong>) with <em>M</em> iterations. <em>N</em> is quite large, 10K to 100K or more. <em>M</em> let's say 100.</li>\n</ul>\n\n<p>I'm estimating the False Discovery Rate (FDR) for each element of <code>p</code> representing how many p-values from random permutations will pass if the current p-value (from <code>p</code>) will be the threshold.</p>\n\n<p>I wrote the function with ARRAYFUN, but it takes lot of time for large N (2 <em>min</em> for <em>N</em>=20K), comparable to for-loop.</p>\n\n<pre><code>function pfdr = fdr_from_random_permutations(p, pr)\n%# ... skipping arguments checks\npfdr = arrayfun( @(x) mean(sum(pr&lt;=x))./sum(p&lt;=x), p);\n</code></pre>\n\n<p>Any ideas how to make it faster?</p>\n\n<p>Comments about statistical issues here are also welcome.</p>\n\n<p>The test data can be generated as <code>p = rand(N,1); pr = rand(N,M);</code>.</p>\n"},{"tags":["performance","functional-programming","ocaml","fold"],"answer_count":3,"favorite_count":1,"up_vote_count":7,"down_vote_count":0,"view_count":228,"score":7,"question_id":4851542,"title":"Optimisations with folds","body":"<p>I am just curious if there are any (first order polymorphic only) optimisations with folds.</p>\n\n<p>For maps, there's deforestation: <code>map g (map f ls) =&gt; map (g . f) ls</code>, and <code>rev (map f ls) =&gt; rev_map f ls</code> (faster in Ocaml).</p>\n\n<p>But fold is so powerful, it seems to defy any optimisation.</p>\n"},{"tags":["eclipse","performance","workspace"],"answer_count":4,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":2528,"score":5,"question_id":4386119,"title":"Eclipse getting too slow - workspace recreation helped","body":"<p>My Eclipse was getting slower and slower over time. Tips I found on the Internet did not help.\nWhat I did is completely deleted my workspace, created new one and reimported all my projects into the new workspace and this really made the difference.</p>\n\n<p>So my question is whether it's possible to perform this workspace clean-up without deleting and recreating workspace...\nMaybe there is some cache in workspace which is getting big? Any ideas?</p>\n\n<p>Thank you!</p>\n"},{"tags":["performance","apache","http-status-code-404","apache-config"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":597,"score":0,"question_id":4411827,"title":"Will broken css/image link 404 errors affect server performance under load?","body":"<p>I have a few images and a css file on my site that don't exist on the server, so each time a visitor comes Apache throws 3 404 errors in it's log.  The items are hidden on the page, so it does not affect the display of the page.  Our site performs fine in testing and production environment. We recently recieved a 2 day traffic spike of 30,000-40,000 visitors per day and apache slowed to a halt, waiting 22-25 seconds before returning anything to the browser.  </p>\n\n<p>Would the 404 errors thrown on the page change the server performance?  </p>\n\n<p>Is a 404 error more resource-intensive then a normal request?</p>\n\n<p>Any info re: the way Apache handles 404 errors would be appreciated.</p>\n\n<p>Thanks,\nSj</p>\n"},{"tags":["sql-server-2005","performance","full-text"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":285,"score":0,"question_id":1875126,"title":"Any idea why contains(...) querys so slow in SQL Server 2005","body":"<p>I've got a simple select query which executes in under 1 second normally, but when I add in a contains(column, 'text') into the where clause, suddenly it's running for 20 seconds up to a minute. The table it's selecting from has around 208k rows.</p>\n\n<p>Any ideas what would cause this query to run so slow with just the addition of the contains clause?</p>\n"},{"tags":["javascript","performance","setinterval"],"answer_count":5,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":61,"score":0,"question_id":13095972,"title":"Is it possible to have \"setInterval\" set too fast?","body":"<p>I have a setinterval function in my javascript that I want to be as fast as possible, i.e. check the status of an event every 1ms. Is it possible that this would be asking too much from a user's browser? It seems to work just fine, but I am wondering if its a bad practice.</p>\n"},{"tags":["arrays","performance","datatable"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":15,"score":1,"question_id":13115924,"title":"Return type string array or datatable?","body":"<p>I have a utility method from which I need to return set of strings which could binded to a list control in the UI. I am in doubt whether to go with array of strings which I think is the lighter than a datatable Or add them into datatable so that I can directly bind the datatable to list control? Which one is bettter in performance?</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["performance","networking","vim"],"answer_count":6,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":2217,"score":5,"question_id":2103968,"title":"GVim runs very slowly when editing files on a windows share","body":"<p>On my computer at work, any time I open a file located on a network share, GVim becomes completely unusable. Scrolling through the document can take 15 seconds to go one \"page\". Even using the movement keys to go from one word to another can take 2 to 3 seconds. This seems to be a new behavior, but for the life of me I can't remember changing anything that would cause it. I'm under the impression that Vim doesn't actually access a file except on open and on save. Is that right?</p>\n"},{"tags":["performance","architecture","nginx","netty"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":51,"score":0,"question_id":13113587,"title":"Push (i.e. Netty) vs. Pull (i.e. Nginx) for a live \"ping\" server?","body":"<p>Imagine an application which receives something like news updates every minute.</p>\n\n<p>Would it be more efficient to build the server-side with something like Netty- which would maintain the connections and push the data once a minute, or something like nginx/php which would drop/open the connection each time a pull-request is made?</p>\n\n<p>Each request would require a database lookup custom tailored to that user (i.e. no caching) and some basic processing (i.e. encryption/decryption)</p>\n\n<p>?</p>\n"},{"tags":["performance","testing","loadrunner"],"answer_count":3,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":947,"score":2,"question_id":5567634,"title":"Load Runner and Browser Client Performance","body":"<p>I'd like to confirm a few things:</p>\n\n<p>1) Standard load runner scripts do NOT record the time the browser would spend rendering HTML and processing JS?</p>\n\n<p>2) A GUI VUser would be needed to accomplish #1.  Are there other ways?</p>\n\n<p>3) The scripting for GUI Vuser is different than that of a standard vuser?</p>\n\n<p>4) Is there any full proof way of determining if LoadRunner is capturing client time (as some analyzing the results not the executor/designer of the test)?</p>\n\n<p>Thanks.</p>\n"},{"tags":["c#","performance","mono","scalability","server-side"],"answer_count":6,"favorite_count":4,"up_vote_count":15,"down_vote_count":0,"view_count":350,"score":15,"question_id":10660990,"title":"C# server scalability issue on linux","body":"<p>I've a C# server developed on both Visual Studio 2010 and Mono Develop 2.8. NET Framework 4.0</p>\n\n<p>It looks like this server behaves much better (in terms of scalability) on Windows than on Linux.\nI tested the server scalability on native Windows(12 physical cores), and 8 and 12 cores Windows and Ubuntu Virtual Machines using Apache's ab tool.</p>\n\n<p>The windows response time is pretty much flat. It starts picking up when the concurrency level approaches/overcomes the number of cores.</p>\n\n<p>For some reason the linux response times are much worse. They grow pretty much linearly starting from level 5 of concurrency. Also 8 and 12 cores Linux VM behave similarly. </p>\n\n<p><strong>So my question is: why does it perform worse on linux? (and How can I fix that?).</strong></p>\n\n<p>Please take a look at the graph attached, it shows the averaged time to fulfill 75% of the requests as a function of the requests concurrency(the range bar are set at 50% and 100%).\n<img src=\"http://i.stack.imgur.com/mVPMh.png\" alt=\"time to fulfill 75% of the request as a function of the request concurrency\"></p>\n\n<p>I have a feeling that this might be due to mono's Garbage Collector. I tried playing around with the GC settings but I had no success. <strong>Any suggestion?</strong></p>\n\n<p>Some additional background information: the server is based on an HTTP listener that quickly parses the requests and queues them on a thread pool. The thread pool takes care of replying to those requests with some intensive math (computing an answer in ~10secs).</p>\n"},{"tags":["c#","performance","sockets","crash","wireshark"],"answer_count":6,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":4040,"score":2,"question_id":679643,"title":"TCP Socket Server Builds Up CLOSE_WAITs Occasionally Over Time Until Inoperable","body":"<p>Hopefully someone can help us as we're reaching as far as investigation can go!</p>\n\n<p>We've got a simple asynchronous socket server written in C# that accepts connections from an ASP.NET web application, is sent a message, performs some processing (usually against a DB but other systems too) and then sends a response back to the client. The client is in charge of closing the connection.</p>\n\n<p>We've been having issues where if the system is under heavy load over a long period of time (days usually), CLOSE_WAIT sockets build up on the server box (netstat -a) to an extent that the process will not accept any further connections. At that point we have to bounce the process and off it runs again.</p>\n\n<p>We've tried running some load tests of our ASP.NET application to attempt to replicate the problem (because inferring some issue from the code wasn't possible). We think we've managed this and ended up with a WireShark <a href=\"http://drop.io/close%5Fwait3793\" rel=\"nofollow\">packet trace</a> of the issue manifesting itself as a SocketException in the socket server's logs:</p>\n\n<blockquote>\n  <p>System.Net.Sockets.SocketException: An existing connection was forcibly closed by the remote host\n  at System.Net.Sockets.Socket.BeginSend(Byte[] buffer, Int32 offset, Int32 size, SocketFlags socketFlags, AsyncCallback callback, Object state)</p>\n</blockquote>\n\n<p>I've tried to reproduce the issue from the packet trace as a single threaded process directly talking to the socket server (using the same code the ASP.NET app does) and am unable. </p>\n\n<p>Has anybody got any suggestions of next things to try, check for or obvious things we may be doing wrong?</p>\n"},{"tags":["css","performance","html5"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":3,"view_count":75,"score":1,"question_id":13114260,"title":"More CSS, less HTML?","body":"<p>Which is better performance wise?</p>\n\n<pre><code>&lt;a class=\"btn loginbtn\" href=\"#\"&gt;Login&lt;/a&gt;​\n\n.btn {\n    background: #555 \n}\n.loginbtn {\n    padding: 10px\n}​\n</code></pre>\n\n<p><strong>or</strong></p>\n\n<pre><code>&lt;a class=\"loginbtn\" href=\"#\"&gt;Login&lt;/a&gt;​\n\n.btn,.loginbtn {\n    background: #555  \n}\n.loginbtn {\n    padding: 10px     \n}​\n</code></pre>\n\n<p>Since my CSS will be cached I was thinking the second one would be better.</p>\n\n<p>Help me out please.</p>\n"},{"tags":["asp.net","database","performance","blob","httphandler"],"answer_count":2,"favorite_count":3,"up_vote_count":5,"down_vote_count":0,"view_count":70,"score":5,"question_id":13041310,"title":"perfomance of .ashx handlers for retrieving a lot of binary images","body":"<p>I used .ashx handler for getting images from database.I want to retrieve a lot of images (>1000) in this way:</p>\n\n<pre><code>    &lt;img src='GetImage.ashx?id= &lt;%# Eval(\"id\") %&gt;'/&gt;\n</code></pre>\n\n<p>(why I do this you can understand if read my previous question: <a href=\"http://stackoverflow.com/questions/13002578/bind-database-image-to-itemtemplate-in-ascx/\">bind database image to ItemTemplate in .ascx</a> ).I am afraid that multipiles database querys (first query to get all id's,all others for getting image one by one) will take a lot of time,is it? What are possible solutions?</p>\n"},{"tags":["django","performance","apache","memory","nginx"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":66,"score":0,"question_id":13110028,"title":"Django Performance / Memory usage","body":"<p>I am running an alpha version of my app on a EC2 Small instance (1.7 GB RAM) with postgres and apache (wsgi-mod not as daemon but directly) on it.</p>\n\n<p>Performance is alright, but it could be better. I am also worried about memory usage if too many test users would join.</p>\n\n<p>Is it wise to switch from Apache to nginx server? Has any Django developer done that and is happier with the results? Any other tips on the way are also welcome.</p>\n\n<p>Thanks</p>\n"},{"tags":["java","performance","reflection","map","annotations"],"answer_count":2,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":352,"score":2,"question_id":8098648,"title":"Performance of calling Method/Field.getAnnotation(Class) several times vs. Pre-caching this data in a Map","body":"<p>I'd like to know if there are any comparison/studies about the performance of repeatidly calling (in Java) the <code>Method.getAnnotation(Class)</code> and <code>Field.getAnnotation(Class)</code> methods, versus storing (at program start up time) a precomputed Map with this metadata information of the classes and querying it repeatidly later. Which one would provide the best runtime performance?</p>\n\n<p>And this performance would be the same under Java 5, 6 and 7?</p>\n"},{"tags":["performance","vb6"],"answer_count":5,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":321,"score":7,"question_id":8175336,"title":"How to decrease VB6 project startup time / Pinpointing what's taking so long","body":"<p>There are two vb6 applications that I work with.  One of them starts up very quickly whereas the other one takes quite a long time.  I thought I would do a little analysis to find out why the one takes so long.</p>\n\n<p>So I hit F8 to start at the beginning and I realize that a significant portion of that startup time is actually between the time I hit F8 and the time it highlights the very first line of code.</p>\n\n<p>Which of the following is most likely causing this?</p>\n\n<ul>\n<li>Number of dependencies</li>\n<li>Having too many projects in the group project instead of referencing them as dlls</li>\n<li>Number of forms</li>\n<li>Number of objects in the startup form</li>\n<li>Number of objects on all forms</li>\n<li>What else?</li>\n</ul>\n\n<p>And as a bonus, I would love any ideas on how to more specifically pinpoint the problem if it could be in multiple areas.</p>\n\n<p>Thanks! </p>\n\n<p>Edit: It seems I may have not been clear enough on exactly 'where' the slowdown is occurring.  So to make it clear I created the following procedure:</p>\n\n<pre><code>Sub Main()\nEnd Sub\n</code></pre>\n\n<p>That's it, and it's in a module that contains absolutely nothing besides these two lines.  No forms are getting loaded, and while there are other modules with \"Dim o as New SomeObject\", I know those objects aren't getting instantiated because I know that visual basic doesn't create objects declared this way until you actually use them for the first time.</p>\n\n<p>I believe I have now optimized the startup code as much as is technically possible.  Yet it still takes the same amount of time to startup.</p>\n\n<p>Edit 2: I just realized that the compiled application actually starts up reasonably fast.  It's just starting it in the ide that takes so long.  However, I care a lot more about the speed for me than I do the customer cause they just start it once and leave it running all day whereas I start it a couple dozen times a day.</p>\n"},{"tags":["java","performance","scala","memory"],"answer_count":8,"favorite_count":13,"up_vote_count":42,"down_vote_count":0,"view_count":15785,"score":42,"question_id":5901452,"title":"scala vs java, performance and memory?","body":"<p>I am keen to look into Scala, and have one basic question I cant seem to find an answer to:\nin general, is there a difference in performance and usage of memory between Scala and Java?</p>\n"},{"tags":["entity-framework","import","loops","performance","savechanges"],"answer_count":3,"favorite_count":15,"up_vote_count":12,"down_vote_count":0,"view_count":5096,"score":12,"question_id":1930982,"title":"When should I call SaveChanges() when creating 1000's of Entity Framework objects? (like during an import)","body":"<p>I am running an import that will have 1000's of records on each run.  Just looking for some confirmation on my assumptions:</p>\n\n<p>Which of these makes the most sense:</p>\n\n<ol>\n<li>Run <code>SaveChanges()</code> every <code>AddToClassName()</code> call.</li>\n<li>Run <code>SaveChanges()</code> every <em>n</em> number of <code>AddToClassName()</code> calls.</li>\n<li>Run <code>SaveChanges()</code> after <em>all</em> of the <code>AddToClassName()</code> calls.</li>\n</ol>\n\n<p>The first option is probably slow right?  Since it will need to analyze the EF objects in memory, generate SQL, etc.</p>\n\n<p>I assume that the second option is the best of both worlds, since we can wrap a try catch around that <code>SaveChanges()</code> call, and only lose <em>n</em> number of records at a time, if one of them fails.  Maybe store each batch in an List&lt;>.  If the <code>SaveChanges()</code> call succeeds, get rid of the list.  If it fails, log the items.</p>\n\n<p>The last option would probably end up being very slow as well, since every single EF object would have to be in memory until <code>SaveChanges()</code> is called.  And if the save failed nothing would be committed, right?</p>\n"},{"tags":["php","mysql","string","performance","query"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":34,"score":0,"question_id":13106195,"title":"How can I find word matches from a string in a simple mysql/php words in string wordsolver app?","body":"<p>I have 27 tables in my database.  One word table (a scrabble word list), and 26 association tables.</p>\n\n<pre><code>Table  Fields\n================\nword   [id,word]\na      [word_id]\nb      [word_id]\n...\nz      [word_id]\n</code></pre>\n\n<p>I'm trying to figure out matching words given a string.</p>\n\n<p>For example, if the given string is <code>pant</code>, I want to know: <code>pant, apt, pat, tap, ant, tan, nap, pan, at, ta, pa, an, na</code>.</p>\n\n<p>My current strategy is to explode each letter in the string and find the associated words that match all the letters.</p>\n\n<p>For example:</p>\n\n<pre><code>SELECT word.word\nFROM word, p, a, n, t\nWHERE\n    word.id = p.word_id OR\n    word.id = a.word_id OR\n    word.id = n.word_id OR\n    word.id = t.word_id\n</code></pre>\n\n<p>But this ends up printing all words that have a p,a,n or t in them.</p>\n\n<p>And if I switch all the operators to <code>AND</code>, I'm stuck with only one match: <code>pant</code>.</p>\n\n<p>Can you help me solve this riddle?</p>\n\n<p>I'm also concerned with how to handle duplicate letters in the string.  For example, <code>PPANT</code> should find a match for <code>app</code>, when plain <code>PANT</code> should not.</p>\n\n<p>Am I on the right track with the association tables or is there a better way?</p>\n\n<p>I'm trying to handle this fairly efficiently in php/mysql.  I'm aware there are others who have solved this riddle before in C, perl, java and the like.</p>\n"},{"tags":["javascript","performance","garbage-collection"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":73,"score":1,"question_id":13110791,"title":"Which way is better when we're focus on garbage collecting?","body":"<p>Simple question, what is better for Garbage Collector:</p>\n\n<ul>\n<li>adding object to array and assigning null to array cell</li>\n</ul>\n\n<p><code>this.requests[i] = new NiceClass();</code></p>\n\n<p><code>this.requests[i] = null;</code></p>\n\n<ul>\n<li>creating simple variable and assigning null to this variable</li>\n</ul>\n\n<p><code>var niceClass = new NiceClass();</code></p>\n\n<p><code>niceClass = null;</code></p>\n\n<p>I'm asking because I want to avoid using array, because during creating a lot of object - despite breaking reference to object so garbage collector is able to grab unnecessary objects - array is growing, so I'm curious, is it necessary here or it will be even better to use just variables?</p>\n"},{"tags":["c#","asp.net",".net","performance","profiling"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":477,"score":4,"question_id":5667978,"title":"Code Profiling ASP.NET MVC2 applications","body":"<p>The thread here seems close:\n<a href=\"http://stackoverflow.com/questions/378617/profiling-asp-net-websites-with-eqatec-profiler\">Profiling ASP.NET websites with EQATEC Profiler</a></p>\n\n<p>However, in the free version of Equatec I downloaded today there is no checkboxes for ASP.NET, and ordinary web. I have pointed the App path to my bin directory in my project folder as well as started up the localhost hosting for my application via Visual Studio.</p>\n\n<p>I am open to other <strong>free</strong> tools as well. I am just looking for someway to profile the code as to optimize some reflection we are using.</p>\n\n<p>I am using the professional edition so unfortunantly do not have access to MS Code Profiling.</p>\n\n<p>I am looking to do performance profiling at this point.</p>\n\n<p>Is the free version of Equatec capable of doing ASP.NET applications?</p>\n\n<p>Is there a free profiler (I realize this has been asked before, and little seems to have surfaced but paid apps, but might as well ask)?</p>\n\n<p>Is MVC a special thing to look for in a profiler?</p>\n"},{"tags":["c#",".net","performance","download","webclient"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":1345,"score":2,"question_id":4932541,"title":"C# WebClient acting slow the first time","body":"<p>I am using a WebClient to download a string from a website (which just contains plain text, nothing else), so I use the DownloadString method:</p>\n\n<pre><code>WebClient wc = new WebClient();\nstring str = wc.DownloadString(\"http://blah\");\n</code></pre>\n\n<p>It works fine, but the problem is that the first time it downloads the string it takes a long time, like 5 seconds. After that it works fast. Why does this happen and how can overcome this problem?</p>\n"},{"tags":["performance","vim","highlighting"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":37,"score":1,"question_id":13111330,"title":"Performance on highlighting words/lines in Vim","body":"<p>I have a set of small functions in VimL highlight a line (or a word) depending on certain conditions.</p>\n\n<p>You should consider the workings of the functions to act similar as the spelling (<code>:set spell</code>), underlining when the conditions are met.</p>\n\n<p>I have found, however, that when the number of highlighted lines exceed about 75, there is a significant lag when moving. Either from side to side or up or down.</p>\n\n<p>I had some convenient <code>AutoCommands</code> that I was enabling by default, (for example, to echo why the line is highlighted) but even with all of them disabled, as soon as I call the function that highlights everything, I can tell there is a huge lag.</p>\n\n<p>This is what I am using to highlight a word:</p>\n\n<pre><code>call matchadd('MyCheck', '^\\%'. line . 'l\\_.\\{-}\\zs\\k\\+\\k\\@!\\%&gt;' . column . 'c')\n</code></pre>\n\n<p>And this is what I use to highlight the whole line</p>\n\n<pre><code>call matchadd('MyCheck', '\\%' . line . 'l\\n\\@!')\n</code></pre>\n\n<p>The 75 number I use as a reference for determining a lag is just a reference, it is a bit of a sweet spot for me, but just to demonstrate that most anything beyond gets increasingly worse.</p>\n\n<p>I also use the <code>SpellBad</code> highlighting for <code>MyCheck</code>, but seriously doubt that this causes any problems.</p>\n\n<p>Is there something I could do differently to avoid the lag? Is <code>matchadd</code> the best option?</p>\n\n<p>EDIT:\nJust to make sure it is not any of my functions or code doing something weird, I opened a 500 line file and did this:</p>\n\n<pre><code>highlight link MyCheck SpellBad                                                      \nfor line in range(line('$'))\n    call matchadd('MyCheck', '\\%' . line . 'l\\n\\@!')\nendfor\n</code></pre>\n\n<p>Which basically highlights every single line on the file. Everything clearly got impossibly slow.</p>\n\n<p>EDIT 1: \nIt seems that unsetting <code>cursorline</code> has a drastic (positive) effect in performance. I did <code>:set nocursorline</code> and now my movements (regardless of highlighting) are snappy as before</p>\n"},{"tags":["c#","algorithm","performance","cartesian-product"],"answer_count":7,"favorite_count":3,"up_vote_count":10,"down_vote_count":1,"view_count":3379,"score":9,"question_id":1741364,"title":"Efficient Cartesian Product algorithm","body":"<p>Can somebody please demonstrate for me a more efficient Cartesian product algorithm than the one I am using currently (assuming there is one).  I've looked around SO and googled a bit but can't see anything obvious so I could be missing something.</p>\n\n<pre><code>foreach (int i in is) {\n   foreach (int j in js) {\n      //Pair i and j\n   }\n}\n</code></pre>\n\n<p>This is a highly simplified version of what I do in my code.  The two integers are lookup keys which are used to retrieve one/more objects and all the objects from the two lookups are paired together into new objects.</p>\n\n<p>This small block of code in a much larger more complex system becomes a major performance bottleneck as the dataset it's operating over scales.  Some of this could likely be mitigated by improving the data structures used to store the objects and the lookups involved but the main issue I feel is still the computation of the Cartesian product itself.</p>\n\n<p><strong>Edit</strong> </p>\n\n<p>So some more background on my specific usage of the algorithm to see if there may be any tricks that I can use in response to Marc's comment.  The overall system is a SPARQL query engine which processes SPARQL queries over sets of Graph data, SPARQL is a pattern based language so each query consists of a series of patterns which are matched against the Graph(s).  In the case where two subsequent patterns have no common variables (they are disjoint) it is necessary to compute the Cartesian product of the solutions produced by the two patterns to get the set of possible solutions for the overall query.  There may be any number of patterns and I may have to compute Cartesian products multiple times which can lead to a fairly exponential expansion in possible solutions if the query is composed of a series of disjoint patterns.</p>\n\n<p>Somehow from the existing answers I doubt whether there any tricks that could apply</p>\n\n<p><strong>Update</strong></p>\n\n<p>So I thought I would post an update on what I implemented in order to minimise the need to do Cartesian products and thus optimise the query engine generally.  Note that it's not always possible to completely eliminate the need for products but it's nearly always possible to optimise so the size of the two sets being joined is much smaller.</p>\n\n<p>Since each BGP (Basic Graph Pattern) which is a set of Triple Patterns is executed as a block (in essence) the engine is free to reorder patterns within a BGP for optimal performance.  For example consider the following BGP:</p>\n\n<pre><code>?a :someProperty ?b .\n?c :anotherProperty ?d .\n?b a :Class .\n</code></pre>\n\n<p>Executed as is the query requires a cartesian product since the results of the first pattern are disjoint from the second pattern so the results of the first two patterns is a cartesian product of their individual results.  This result will contain far more results than we actually need since the third pattern restricts the possible results of the first pattern but we don't apply this restriction till afterwards.  But if we reorder like so:</p>\n\n<pre><code>?b a :Class .\n?a :someProperty ?b .\n?c :anotherProperty ?d .\n</code></pre>\n\n<p>We'll still need a cartesian product to get the final results since the 2nd and 3rd patterns are still disjoint but by reordering we restrict the size of the results of the 2nd pattern meaning the size of our cartesian product will be much smaller.</p>\n\n<p>There are some various other optimisations we make but I'm not going to post them here as it starts to get into fairly detailed discussion of SPARQL engine internals.  If anyone is interested in further details just leave a comment or send me a tweet @RobVesse</p>\n"},{"tags":["c#","performance","try-catch","direct3d","slimdx"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":22,"score":1,"question_id":13111037,"title":"SlimDX Lost Device: Try Catch performance?","body":"<p>I'm using SlimDX and Direct3D9 classes to create a C# game engine.</p>\n\n<p>I started with it one year ago and I remember that I successfully and safely managed to catch lost devices and reset them (on Windows 7). Now (on Windows 8) I started working on it again, but it looks like I'm not catching lost devices in any case anymore, especially in Ctrl+Alt+Del cases.</p>\n\n<p>This is the previously working code: \nThe \"IsDeviceLost\" method erroneously returns false at Ctrl+Alt+Del, so the device is about to present, but it crashes to a Direct3D9Exception: \"D3DERR_DEVICELOST\":</p>\n\n<pre><code>    public void Update(float time)\n    {\n        if (!IsDeviceLost(true))\n        {\n            _device.Present();\n        }\n    }\n\n    private bool IsDeviceLost(bool resetIfNeeded)\n    {\n        bool deviceLost = false;\n\n        // Check if DeviceLost is detected\n        Result result = _device.TestCooperativeLevel();\n        if (result == ResultCode.DeviceLost)\n        {\n            Log.Write(LogType.Warning, \"Direct3D9Manager: Device lost and cannot be reset yet.\");\n            // Device has been lost and cannot be reset yet\n            deviceLost = true;\n        }\n        else if (result == ResultCode.DeviceNotReset)\n        {\n            Log.Write(LogType.Information, \"Direct3D9Manager: ResultCode.DeviceNotReset\");\n            // Device is available again but has not yet been reset\n            if (resetIfNeeded)\n            {\n                // Reset device and check if it can work again\n                _device.Reset(_presentParams);\n                deviceLost = IsDeviceLost(false);\n                if (deviceLost)\n                {\n                    // Reset failed, device still lost\n                }\n                else\n                {\n                    // Reset successful, device restored\n                    // TODO: Reload textures and render states which are lost after a reset\n                }\n            }\n            else\n            {\n                deviceLost = true;\n            }\n        }\n\n        return deviceLost;\n    }\n</code></pre>\n\n<p>So I researched the web about this problem and found several code, which puts the Update code into the try part of a try-catch block, but I'm not sure if that is the right way to fix this.</p>\n\n<ul>\n<li>Isn't try-catch slow for an Update method in a game engine which gets called every frame?</li>\n<li>Aren't there better ways to catch a lost device, which are not using try-catch?</li>\n</ul>\n"},{"tags":["performance","delphi","serialization"],"answer_count":0,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":111,"score":1,"question_id":13110821,"title":"Is there a benchmark for Delphi serialization libraries?","body":"<p>For high performance Delphi / Free Pascal applications which need to communicate over IPC / the network I am interested in performance tests of serialization libraries for Delphi.</p>\n\n<p>As this is not for cross-language operation, binary serialization is an option too, it is not limited to JSON or XML. I am also not limited to serialization of TPersistent or TRemotable descendants, or usage of classic vs extended RTTI.</p>\n\n<p>I have not yet seen a benchmark which allows to run performance statistics for available libraries. Have you seen anything in this direction?</p>\n"},{"tags":["ruby-on-rails","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":22,"score":0,"question_id":13110448,"title":"Ignoring bots when tracking events in Rails app","body":"<p>I am tracking events through Mixpanel in my Rails app. For example, my controller looks something like</p>\n\n<pre><code>class HomeController &lt; ApplicationController\n  def index\n    track_event \"Visitor: View Landing Page\"\n  end\nend\n</code></pre>\n\n<p>The problem is that the app gets hit by a number of bots, most notably Pingdom (performance tracking service we use). Is there a clean way to ignore tracking when it is a bot that hits my app?</p>\n\n<p><em>Note: I am interested in tracking unique visitors, so I assign a cookie to each visitor with a unique id. Bots obviously don't store cookies.</em></p>\n"},{"tags":["performance","logging","openjpa","runtime.exec"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":12,"score":-1,"question_id":13110347,"title":"openjpa mappingtool log","body":"<p>I use openjpa 1.2.0. </p>\n\n<p>Now, there is a strange thing that happens :\nwhen I run the application the simple way ( from eclipse or double click on exe ), it starts fine (about 2 minutes), but when I run it from another program ( by using runtime.exec(...) ) it takes about 20 minutes to start.</p>\n\n<p>After some test and profiling I found that it spends most of the time in MappingTool.run() writing logs. I couldn't find the log file or the specific logs that are written there. Since it runs in a separate process, I can't debug it the regular way.</p>\n\n<p>Can somebody please tell me:\nWhere to find the log file? or\nHow to debug this application? or\nHow to reduce the start time?</p>\n\n<p>Thanks in advance</p>\n"},{"tags":["xcode","performance","ipad"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":21,"score":-1,"question_id":13110122,"title":"Xcode - Speeding up iPad App","body":"<p>i have a question about speeding my iPad app. The main part of my app is the home viewcontroller which loads two another viewcontrollers in it. One of them is a calendar which i made, and the other a note view. </p>\n\n<p>When i change the month of the calendar, the calendar viewcontroller as it should, ads buttons, depends of how many months did you go back or forward. I know that the problem is in that, i tried to reduce the buttons image size to 2kb, didn't work. Are there any other solutions ?  </p>\n\n<p>I saw the problem at the main screen, when i press some button to show a modal view controller. </p>\n\n<p>Any idea how can i speed up my app ? </p>\n"},{"tags":["jquery","performance","jquery-ui","drag-and-drop"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":2692,"score":4,"question_id":1133413,"title":"jQuery drag drop slower for more DIV items","body":"<p>I have got a hierarchichal  tags (with parent child relationship) in my page and it will account to 500 - 4500 (can even grow). When i bound the draggable and droppable for all i saw very bad performance in IE7 and IE6. The custom helper wont move smoothly and was very very slow. Based on some other post i have made the droppable been bound/unbound on mouseover and mouseout events (dynamically). Its better now. </p>\n\n<p>But still i dont see the custom helper move very smoothly there is a gap between the mouse cursor and the helper when they move and gets very bad when i access the site from remote.</p>\n\n<p>Please help me to address this performance issue. Am totally stuck here.. :(</p>\n"},{"tags":["c#","asp.net-mvc","performance","entity-framework"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":54,"score":1,"question_id":13103852,"title":"I want to improve performance of my Edmx by using AutoDetectChanges","body":"<p>I have a reasonably large edmx generated from a database and I have been working on performance recently to improve my application I have read a number of articles in a variety of places some here some not</p>\n\n<p>this one on disabling auto detect of changes <a href=\"http://msdn.microsoft.com/en-us/data/jj556205.aspx\" rel=\"nofollow\">http://msdn.microsoft.com/en-us/data/jj556205.aspx</a> </p>\n\n<p>this one on improving performance on delete <a href=\"http://stackoverflow.com/questions/10103310/dbcontext-is-very-slow-when-adding-and-deleting\">DbContext is very slow when adding and deleting</a></p>\n\n<p>this one (which I think is pretty good) <a href=\"http://www.codeproject.com/Articles/38922/Performance-and-the-Entity-Framework\" rel=\"nofollow\">http://www.codeproject.com/Articles/38922/Performance-and-the-Entity-Framework</a></p>\n\n<p>I am already using myentities.tablename.MergeOption = MergeOption.NoTracking, i am using compiledqueries, I am pregenerated my View using EdmGen, I have reduced the data I am fetching etc..  and, of course,  I have gained performance in leaps and bounds so that a page that was loading in 54 seconds is now taking 16.1 seconds - however I have to get it to 3 seconds So I am still looking for the next improvement </p>\n\n<p>so the research is all well and great and as a result  I have upgraded to the latest EntityFramework, I have regenerated my .edmx from db etc... and tried a variety of things but I simply cannot find a myEntities.Configuration.AutoDetectChangesEnabled in order to set it to false. Now I must be missing a simple easy trick - how do I get my edmx to have this option.</p>\n\n<p>I am in this environment.Net 4.0.3, visual studio 2010, latest version of EntityFramework, MVC 4.0... All I need is somebody to say \"aha\" you need to go and do this....</p>\n\n<p>Currently if I delete 1000 records from one of my larger tables (134million rows) it takes nearly 10 minutes to savechanges. So from what I have read AutoDetectChangesEnabled is what I need to alter but it doesnt exist in my classes? where is it what must I do to get it?</p>\n\n<p>Any help appreciated I am trying to solve this one quickly</p>\n\n<p>Regards Julian</p>\n"},{"tags":["mysql","performance","amazon-ec2","amazon-web-services","amazon-ebs"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":2531,"score":1,"question_id":5122889,"title":"Mysql and High CPU IO Wait","body":"<p>I've the following problem. I'm running a MySQL server 5.1.37 on Ubuntu 9.10 x86 on Amazon. For data store I use EBS volume formatted for ext3.</p>\n\n<p>From time to time the following problem occurs. MySQL start processing about queries 10~20 queries and processing of these takes more than 300sec (These SQL are using filesort). During that time no other transaction could be executed.</p>\n\n<p>I've checked CPU Wait and here what is shows:</p>\n\n<pre>\n----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--\nusr sys idl wai hiq siq| read  writ| recv  send|  in   out | int   csw \n24   9  66   0   0   0|   0    76k| 258k  989k|   0     0 |5970  3014 \n 23   1  75   0   0   1|4096B   28k| 229k 1536k|   0     0 |3249  2308 \n 19   6  74   0   0   0|4096B  316k| 209k  609k|   0     0 |4943  2542 \n 19  17  62   0   0   2|4096B   36k| 230k  718k|   0     0 |5482  2520 \n 21  19  57   2   0   2|  16k  800k| 271k  860k|   0     0 |6549  2923 \n 23  27  44   5   0   1| 480k   40k| 288k  979k|   0     0 |4140  2682 \n 12   0  86   1   0   0| 256k   48k| 237k  771k|   0     0 |3404  2627 \n 22   1  75   0   0   1|8192B   60k| 285k  908k|   0     0 |4009  2786 \n 54  21  19   3   0   2|4096B 3384k| 287k 1556k|   0     0 |3962  2284 \n 49  24  24   1   0   2|4096B  928k| 285k 2795k|   0     0 |3257  2005 \n 61  19  17   2   0   2|8192B   36k| 215k  577k|   0     0 |3246  1922 \n 40  49   8   0   0   3|   0    40k| 312k  905k|   0     0 |3282  1732 \n 56  23  20   1   0   1|4096B  188k| 247k  897k|   0     0 |3102  2238 \n 39  19  27  16   0   0|4096B   77M| 265k  819k|   0     0 |5147  3075 \n 35  35  12  16   0   1|4096B   56M| 259k 1052k|   0     0 |4656  2739 \n 36  27   8  28   0   1|4096B   59M| 259k 1139k|   0     0 |5549  2821 \n 27  13  36  23   0   1|4096B   64M| 251k 1218k|   0     0 |4207  2540\nusr sys idl wai hiq siq| read  writ| recv  send|  in   out | int   csw \n 26   4  13  57   0   1|4096B   66M| 275k  681k|   0     0 |5205  3291 \n 22   6  27  43   0   1|4096B   52M| 237k  684k|   0     0 |4906  2602 \n 14   3  24  58   0   0|4096B   46M| 278k 1058k|   0     0 |6448  3687 \n 19   3  34  43   0   2|  32k   51M| 233k  685k|   0     0 |5006  2652 \n 27   3   9  61   0   1|4096B   51M| 294k  800k|   0     0 |4428  2384 \n 17   3  30  50   0   1|4096B   42M| 243k  699k|   0     0 |5334  2830 \n 40  18   0  42   0   0|   0    89M| 247k  840k|   0     0 |4698  2977 \n 31  18  11  39   0   2|4096B   42M| 238k 1269k|   0     0 |4270  2474 \n 17   3  13  66   0   0|4096B   49M| 260k  773k|   0     0 |5153  3100 \n 21   2  14  62   0   1|8192B   46M| 269k  948k|   0     0 |6762  3581 \n 24   2  35  39   0   0|4096B   39M| 256k  777k|   0     0 |5313  2761 \n 15   2  10  72   0   1|4096B   49M| 237k  797k|   0     0 |5312  3018 \n 19   4  22  55   0   0|8192B   47M| 307k 1034k|   0     0 |5508  3278 \n 41   3  15  40   0   1|8192B   47M| 293k  727k|   0     0 |5630  3303 \n 16   2  26  54   0   1|4096B   56M| 282k 1750k|   0     0 |5016  2781 \n 17   3  12  67   0   2|8192B   43M| 238k  824k|   0     0 |5751  3147 \n 14  11  50  24   0   1|4096B   39M| 247k 1105k|   0     0 |4454  2389 \n 41   3  20  35   0   1|   0    58M| 152k  481k|   0     0 |4009  2958 \n 52   2   4  41   0   1|4096B   59M| 211k  621k|   0     0 |5449  2846 \n 31   2   0  66   0   1|   0    52M| 255k 1476k|   0     0 |5167  2693 \n 36   2  24  36   0   2|  12k   49M| 311k  888k|   0     0 |4537  2563 \n 47   7   2  43   0   2|4096B   50M| 231k  750k|   0     0 |4083  2165 \n 40   4   6  50   0   0|4096B   86M| 211k  819k|   0     0 |4768  2875 \n 29   5   2  65   0   0|   0    79M| 180k  580k|   0     0 |4271  4461 \n 40   3   0  57   0   0|4096B   58M| 238k 1489k|   0     0 |4366  4480 \n 27   8  26  38   0   1|4096B   33M| 301k  984k|   0     0 |4439  2838 \n 11   2   9  78   0   1|4096B   24M| 230k  646k|   0     0 |4894  4504 \n 10   3  14  72   0   0|4096B   21M| 183k  549k|   0     0 |4066  3952 \n 14   3  27  57   0   0|   0    64M| 147k  339k|   0     0 |3479  2860 \n 10   2  19  69   0   0|4096B   51M| 112k  452k|   0     0 |2847  2300 \n  9   4  18  69   0   0|4096B   37M| 131k  443k|   0     0 |2923  2004 \n  4   2  49  45   0   0|4096B   31M|  97k  230k|   0     0 |2163  1545 \n  1   2  73  24   0   0|   0    33M|  49k  130k|   0     0 |1425   824 \n  1   0  71  28   0   0|   0    26M|  36k   86k|   0     0 |1426   910 \n  0   0  55  45   0   0|   0    32M|  32k  148k|   0     0 |1334   695 \n  4   0  64  32   0   0|   0    39M|  14k   39k|   0     0 |1262   406 \n  0   2  38  60   0   0|   0    44M|  13k   44k|   0     0 |1136   382 \n  1   1  82  16   0   0|   0    47M|  25k   70k|   0     0 |1228   584 \n  1   3  69  27   0   0|4096B   46M|  23k   60k|   0     0 |1576   599 \n----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--\nusr sys idl wai hiq siq| read  writ| recv  send|  in   out | int   csw \n  3   1  70  27   0   0|4096B   43M|  22k   54k|   0     0 |1065   574 \n  1   1  33  65   0   0|   0    46M|6124B   17k|   0     0 |1190   345 \n  1   1  49  50   0   0|   0    47M|  11k   22k|   0     0 |1258   444 \n  2  11  23  64   0   0|  56k   58M|9749B   47k|   0     0 |1143   379 \n  1   1  64  34   0   0|   0    51M| 198B 5914B|   0     0 |1048   234 \n  0   1  63  36   0   0|   0    58M| 662B 1278B|   0     0 | 976   454 \n  1   0  81  18   0   0|   0    50M| 426B 6022B|   0     0 |1304   600 \n  0   1  70  29   0   0|   0    43M| 132B 1868B|   0     0 |1150   210 \n  1   1  79  19   0   0|   0    51M| 198B 5914B|   0     0 | 986   246 \n  1   2  30  66   0   0|   0    54M| 246B  420B|   0     0 |1150   288 \n  1   0  49  50   0   0|   0    55M| 659B 6752B|   0     0 |1038   280 \n  1   2  37  60   0   0|   0    47M|  66B  354B|   0     0 |1191   227 \n  0   0  80  19   0   0|   0    43M| 561B 6044B|   0     0 |1129   256 \n  5  13  44  38   0   0|   0    49M|1558B   19k|   0     0 |1225   243 \n  3   6  48  42   0   0|   0    52M| 705B 6022B|   0     0 | 948   327  \n</pre>\n\n<p>What could cause such a behavior? Are there any techniques to avoid this?</p>\n"},{"tags":["performance","json","r"],"answer_count":3,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":484,"score":4,"question_id":5185990,"title":"Efficient alternative to merge() when building dataframe from json files with R?","body":"<p>I have written the following code which works, but is painfully slow once I start executing it over thousands of records:</p>\n\n<pre><code>require(\"RJSONIO\")\npeople_data &lt;- data.frame(person_id=numeric(0))\n\njson_data &lt;- fromJSON(json_file)\nn_people &lt;- length(json_data)\nfor(person in 1:n_people) {\n        person_dataframe &lt;- as.data.frame(t(unlist(json_data[[person]])))\n        people_data &lt;- merge(people_data, person_dataframe, all=TRUE)\n    }\n\noutput_file &lt;- paste(\"people_data\",\".csv\")\nwrite.csv(people_data, file=output_file)\n</code></pre>\n\n<p>I am attempting to build a unified data table from a series of json-formated files. The <code>fromJSON()</code> function reads in the data as lists of lists. Each element of the list is a person, which then contains a list of the attributes for that person.</p>\n\n<p>For example:</p>\n\n<pre><code>[[1]]\n    person_id\n    name\n    gender\n    hair_color\n[[2]]\n    person_id\n    name\n    location\n    gender\n    height\n\n[[...]]\n\nstructure(list(person_id = \"Amy123\", name = \"Amy\", gender = \"F\",\n               hair_color = \"brown\"), \n          .Names = c(\"person_id\", \"name\", \"gender\", \"hair_color\"))\n\nstructure(list(person_id = \"matt53\", name = \"Matt\", \n               location = structure(c(47231, \"IN\"), \n                                    .Names = c(\"zip_code\", \"state\")), \n               gender = \"M\", height = 172), \n          .Names = c(\"person_id\", \"name\", \"location\", \"gender\", \"height\"))\n</code></pre>\n\n<p>The end result of the code above is matrix where the columns are every person-attribute that appears in the structure above, and the rows are the relevant values for each person. As you can see though, some data is missing for some of the people, so I need to ensure those show up as <code>NA</code> and make sure things end up in the right columns. Further, <code>location</code> itself is a vector with two components: <code>state</code> and <code>zip_code</code>, meaning it needs to be flattened to <code>location.state</code> and <code>location.zip_code</code> before it can be merged with another person record; this is what I use <code>unlist()</code> for. I then keep the running master table in <code>people_data</code>.</p>\n\n<p>The above code works, but do you know of a more efficient way to accomplish what I'm trying to do? It appears the <code>merge()</code> is slowing this to a crawl... I have hundreds of files with hundreds of people in each file.</p>\n\n<p>Thanks!\nBryan</p>\n\n<p>UPDATE:\nBased on the feedback below, I tried to build a list of all the people, and then convert it all at one time into a dataframe. I let it run overnight and still didn't finish making the dataframe. There are around 1/2 million people in the list. That codes looks like this:</p>\n\n<pre><code>require(\"RJSONIO\")\nrequire(\"plyr\")\npeople_data &lt;- data.frame(person_id=numeric(0))\npeople_list &lt;- list()\n\njson_data &lt;- fromJSON(json_file)\nn_people &lt;- length(json_data)\nfor(person in 1:n_people) {\n        people_list[[person]] &lt;- t(unlist(json_data[[person]]))\n    }\n\n#PROBLEM CODE, SLOW, 1/2 million records in people_list\npeople_data &lt;- rbind.fill(lapply(people_list, as.data.frame))\n\noutput_file &lt;- paste(\"people_data\",\".csv\")\nwrite.csv(people_data, file=output_file)\n</code></pre>\n"},{"tags":["java","performance","opengl","theory","graphics2d"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":34,"score":1,"question_id":13092142,"title":"Adjusting repaints in JComponent to improve performance","body":"<p>I am trying to do some tests on 2D \"game\"-programming by trying out different concepts to approach both designing and visualizing the environment.</p>\n\n<p>In my baby-steps I went to go with Swing and using a JComponent's paintComponent() method to retrieve a Graphics2D object that I then use to visualize my game board.</p>\n\n<p>It works quite well but I got to the point where I need to repeatedly check my whole game-model and update the view, basically once every 1/10 second something can change.</p>\n\n<p>I paint the visuals by calling repaint() on my JComponent to cause a complete update of the view: I check every tile my game board has for information and paint that tile according to this data, for every tile on the board. But as I approach about 1000 - 4000 tiles that need to be painted, I come to the point where the painting of the whole view takes more than 100ms and thus a constant lag occurs when doing anything.</p>\n\n<p>Now for the question(s): I am looking for a way, or opinions, on how to improve the performance of this approach. As not every tile on the board changes every \"tick\" I do not need to \"repaint\" this tile. But on the contrary, moving the visual area (camera offset) changes the position of every tile on the screen, so it would need to be repainted at a different position. Also, the later implementation of \"would be\" animations would need a constant update of the visual area, regardless of \"happenings\" or not. When looking at 3D games with high quality graphics (or even simple ones like minecraft) running at > 30 FPS, I am wondering weather or not I should immediately switch to OpenGL before running into even more problems graphics wise, or are there ways to improve the performance with algorithms checking for the right kind of changes both in the view and the model?</p>\n"},{"tags":["c++","performance","c++11","const"],"answer_count":3,"favorite_count":1,"up_vote_count":13,"down_vote_count":0,"view_count":310,"score":13,"question_id":13099942,"title":"Should I still return const objects in C++11?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/12051012/should-i-return-const-objects\">Should I return const objects?</a><br>\n  (The original title of that question was: <strong>int foo() or const int foo()?</strong> explaining why I missed it.)</p>\n</blockquote>\n\n<hr>\n\n<p>Effective C++, Item 3: Use const whenever possible. In particular, returning const objects is promoted to avoid unintended assignment like <code>if (a*b = c) {</code>. I find it a little paranoid, nevertheless I have been following this advice.</p>\n\n<p>It seems to me that returning const objects can degrade performance in C++11.</p>\n\n<pre class=\"lang-cpp prettyprint-override\"><code>#include &lt;iostream&gt;\nusing namespace std;\n\nclass C {\npublic:\n    C() : v(nullptr) { }\n\n    C&amp; operator=(const C&amp; other) {\n        cout &lt;&lt; \"copy\" &lt;&lt; endl;\n        // copy contents of v[]\n        return *this;\n    }\n\n    C&amp; operator=(C&amp;&amp; other) {\n        cout &lt;&lt; \"move\" &lt;&lt; endl;\n        v = other.v, other.v = nullptr;\n        return *this;\n    }\n\nprivate:\n    int* v;\n};\n\nconst C const_is_returned() { return C(); }\n\nC nonconst_is_returned() { return C(); }\n\nint main(int argc, char* argv[]) {\n    C c;\n    c = const_is_returned();\n    c = nonconst_is_returned();\n    return 0;\n}\n</code></pre>\n\n<p>This prints:</p>\n\n<pre><code>copy\nmove\n</code></pre>\n\n<p>Do I implement the move assignment correctly? Or I simply shouldn't return const objects anymore in C++11?</p>\n"},{"tags":["javascript","jquery","performance"],"answer_count":5,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":326,"score":6,"question_id":12978069,"title":"Poor JS/jQuery performance, particularly in IE9 and Firefox","body":"<p>I'm having serious visual &amp; performance issues with the script below. The biggest problem is that the animation of the object is becoming really jerky, almost cripplingly so in IE9, but increasingly annoying in Firefox. </p>\n\n<p>It has been pretty fast until recently - but I'm concerned the complexity is slowing things down. Oddly the <a href=\"http://www.webkit.org/perf/sunspider/sunspider.html\">Sunspider benchmark</a> runs faster in my IE9 instance, than in Firefox.</p>\n\n<p>The script (which is a snippet of a larger collection <em>*</em>):</p>\n\n<ol>\n<li>Checks a HTML5 session storage log of the users progression through\nthe game. </li>\n<li>Depending on the stage, animates an object between two\npoints using <a href=\"https://github.com/MmmCurry/jquery.crSpline\">crSpline</a>. </li>\n<li>Ensures the browser window follows the object\nacross a large canvas, via scrollLeft etc. </li>\n<li>Finally, it loads a popup window via <a href=\"http://www.jacklmoore.com/colorbox\">colorbox</a>. </li>\n<li>When this box is closed, the user progression log is incremented accordingly and the object moves again.</li>\n</ol>\n\n<p>Are there aby obvious speed improvments I could make to my code? There's a fair bit of repition, how can I reduce that? Are there any infinite loops running that I'm missing? Is there software I can use to profile slow points of the JS?</p>\n\n<p><em>*</em> (I can't provide the other JS files or HTML, but I have identified this script as the problem)</p>\n\n<hr>\n\n<p><strong>UPDATE:</strong>\nAfter a fair bit more testing, it appears that the step animate function - which follows the object in the window via scrollLeft - is causing the jerky animation. Removing it improves things considerably.</p>\n\n<p>This isn't a viable long term solution however. A quick fix is to call the follow function on complete, but this is a much less smooth experience for the end user, especially when the object moves longer distances.</p>\n\n<p><strong>So, how would I modify the step function to run a lot 'slower'/more efficiently?</strong> I'm guessing the jerkiness is caused by it using all the available resources to follow the object every millisecond.</p>\n\n<pre><code>(function ($) {\n\n  sessionStorage.gameMainStage = 0 \n\n  moveShip =  function() {\n\n    switch (sessionStorage.gameMainStage)\n\n{\n  case '1':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[715, 425], [582, 524], [556, 646], [722, 688], [963, 629], [1143, 467]]) },{\n      duration: 10000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n          },\n          complete: function() {\n            $.colorbox({href:\"dialog-1.html\", width:\"737px\", height:\"474px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n      }\n    );\n    break;\n\n  case '2':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[1143, 467], [1343, 667], [1443, 367],  [1243, 167], [1499, 285]]) },\n        {\n          duration: 5000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n          },\n          complete: function() {\n            $.colorbox({href:\"dialog-2\", width:\"737px\", height:\"547px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n\n        }\n    );\n    break;\n\n  case '3':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[1499, 285], [1922, 423]]) },\n        {\n          duration: 5000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n          },\n          complete: function() {\n            $.colorbox({href:\"dialog-3.html\", width:\"737px\", height:\"547px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n\n        }\n    );\n    break;  \n\n  case '4':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[1922, 423], [2216, 578]]) },{\n        duration: 5000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n            }, \n\n          complete: function() {\n            $.colorbox({href:\"game-1.html\", width:\"737px\", height:\"547px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n        }\n    );\n    break;\n\n  case '5':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[2216, 578], [2769, 904]]) },{\n      duration: 5000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n          }, \n\n          complete: function() {\n            $.colorbox({href:\"dialog-4.html\", width:\"737px\", height:\"547px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n      }\n    );\n    break;\n\n  case '6':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[2769, 904], [3263, 903]]) },{\n      duration: 5000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n          }, \n\n          complete: function() {\n            $.colorbox({href:\"dialog-5.html\", width:\"737px\", height:\"547px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n      }\n    );\n    break;\n\n  case '7':\n    $.colorbox({href:\"game-2.html\", width:\"500px\", height:\"600px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n  break;\n\n  case '8':\n    $.colorbox({href:\"dialog-6.html\", width:\"737px\", height:\"567px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n  break;\n\n  case '9':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[3263, 903], [4141, 820]]) },{\n      duration: 5000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n          }, \n\n          complete: function() {\n            $.colorbox({href:\"dialog-7.html\", width:\"737px\", height:\"547px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n      }\n    );\n    break;\n\n  case '10':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[4141, 820], [4568, 949], [4447, 1175]]) },{\n      duration: 5000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n          }, \n\n          complete: function() {\n            $.colorbox({href:\"dialog-8.html\", width:\"737px\", height:\"434px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n      }\n    );\n    break;\n\n  case '11':\n    $.colorbox({href:\"dialog-9.html\", width:\"737px\", height:\"567px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n  break;\n\n  case '12':\n    $(\"#object\").animate(\n      { crSpline: $.crSpline.buildSequence([[4447, 1175], [4701, 1124], [4816, 822]]) },{\n      duration: 5000,\n          step: function() {\n            var mover = $('#object'),               \n            posX = mover.position().left;\n            posY = mover.position().top;\n\n            $(window)\n            .scrollLeft(posX - $(window).width() / 2)\n            .scrollTop(posY - $(window).height() / 2);\n          }, \n\n          complete: function() {\n            $.colorbox({href:\"dialog-10.html\", width:\"900px\", height:\"687px\", iframe: true, overlayClose: false, escKey: false, close: \"\"});\n          }\n      }\n    );\n    break;\n}\n\n};\n\n})(jQuery);\n</code></pre>\n"},{"tags":["c","performance","gcc","g++"],"answer_count":2,"favorite_count":1,"up_vote_count":9,"down_vote_count":2,"view_count":92,"score":7,"question_id":13104178,"title":"Program compiled by gcc runs faster than compiled by g++","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/3302657/performance-difference-between-gcc-and-g-for-c-program\">Performance difference between gcc and g++ for C program</a>  </p>\n</blockquote>\n\n\n\n<p>I was checking the improvement of performance with using register storage specifier for the loop control variable when I accidentally noticed that program compiled with gcc runs faster than compiled with g++. Can someone explain it to me?</p>\n\n<p>Here is the code:</p>\n\n<pre><code>#include &lt;stdio.h&gt;\n\nconst unsigned long scope = 1000000000;\n\nint main()\n{\n    register unsigned long i;\n    for (i=0; i &lt; scope; i++);\n    return 0;\n}\n</code></pre>\n\n<p>;</p>\n\n<pre><code>gcc register.c\ntime ./a.out   \nreal    0m0.466s\nuser    0m0.468s\nsys     0m0.000s\n\ng++ register.c\ntime ./a.out \nreal    0m0.923s\nuser    0m0.920s\nsys     0m0.000s\n</code></pre>\n"},{"tags":["mysql","sql","performance","optimization","mariadb"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":78,"score":2,"question_id":13103850,"title":"Why is my MySQL group by so slow?","body":"<p>I am trying to query against a partitioned table (by month) approaching 20M rows.  I need to group by DATE(transaction_utc) as well as country_id.  The rows that get returned if i turn off the group by and aggregates is just over 40k, which isn't too many, however adding the group by makes the query substantially slower unless said GROUP BY is on the transaction_utc column, in which case it gets FAST. </p>\n\n<p>I've been trying to optimize this first query below by tweaking the query and/or the indexes, and got to the point below (about 2x as fast as initially) however still stuck with a 5s query for summarizing 45k rows, which seems way too much.</p>\n\n<p>For reference, this box is a brand new 24 logical core, 64GB RAM, Mariadb-5.5.x server with way more INNODB buffer pool available than index space on the server, so shouldn't be any RAM or CPU pressures.</p>\n\n<p>So, I'm looking for ideas on what is causing this slow down and suggestions on speeding it up.  Any feedback would be greatly appreciated!  :) </p>\n\n<p>Ok, onto the details...</p>\n\n<p>The following query (the one I actually need) takes approx 5 seconds (+/-), and returns less than 100 rows.   </p>\n\n<pre><code>SELECT lss.`country_id` AS CountryId\n, Date(lss.`transaction_utc`) AS TransactionDate\n, c.`name` AS CountryName,  lss.`country_id` AS CountryId\n, COALESCE(SUM(lss.`sale_usd`),0) AS SaleUSD\n, COALESCE(SUM(lss.`commission_usd`),0) AS CommissionUSD  \nFROM `sales` lss  \nJOIN `countries` c ON lss.`country_id` = c.`country_id`  \nWHERE ( lss.`transaction_utc` BETWEEN '2012-09-26' AND '2012-10-26' AND lss.`username` = 'someuser' )  GROUP BY lss.`country_id`, DATE(lss.`transaction_utc`)\n</code></pre>\n\n<p>EXPLAIN SELECT for the same query is as follows.  Notice that it's not using the transaction_utc key.  Shouldn't it be using my covering index instead?</p>\n\n<pre><code>id  select_type table   type    possible_keys   key key_len ref rows    Extra\n1   SIMPLE  lss ref idx_unique,transaction_utc,country_id   idx_unique  50  const   1208802 Using where; Using temporary; Using filesort\n1   SIMPLE  c   eq_ref  PRIMARY PRIMARY 4   georiot.lss.country_id  1   \n</code></pre>\n\n<p>Now onto a couple other options that I've tried to attempt to determine whats going on...</p>\n\n<p>The following query (changed group by) takes about 5 seconds (+/-), and returns only 3 rows:</p>\n\n<pre><code>SELECT lss.`country_id` AS CountryId\n, DATE(lss.`transaction_utc`) AS TransactionDate\n, c.`name` AS CountryName,  lss.`country_id` AS CountryId\n, COALESCE(SUM(lss.`sale_usd`),0) AS SaleUSD\n, COALESCE(SUM(lss.`commission_usd`),0) AS CommissionUSD  \nFROM `sales` lss  \nJOIN `countries` c ON lss.`country_id` = c.`country_id`  \nWHERE ( lss.`transaction_utc` BETWEEN '2012-09-26' AND '2012-10-26' AND lss.`username` = 'someuser' )  GROUP BY lss.`country_id`\n</code></pre>\n\n<p>The following query (removed group by) takes 4-5 seconds (+/-) and returns 1 row:</p>\n\n<pre><code>SELECT lss.`country_id` AS CountryId\n    , DATE(lss.`transaction_utc`) AS TransactionDate\n    , c.`name` AS CountryName,  lss.`country_id` AS CountryId\n    , COALESCE(SUM(lss.`sale_usd`),0) AS SaleUSD\n    , COALESCE(SUM(lss.`commission_usd`),0) AS CommissionUSD  \n    FROM `sales` lss  \n    JOIN `countries` c ON lss.`country_id` = c.`country_id`  \n    WHERE ( lss.`transaction_utc` BETWEEN '2012-09-26' AND '2012-10-26' AND lss.`username` = 'someuser' )\n</code></pre>\n\n<p>The following query takes .00X seconds (+/-) and returns ~45k rows.  This to me shows that at max we're only trying to group 45K rows into less than 100 groups (as in my initial query):</p>\n\n<pre><code>SELECT lss.`country_id` AS CountryId\n    , DATE(lss.`transaction_utc`) AS TransactionDate\n    , c.`name` AS CountryName,  lss.`country_id` AS CountryId\n    , COALESCE(SUM(lss.`sale_usd`),0) AS SaleUSD\n    , COALESCE(SUM(lss.`commission_usd`),0) AS CommissionUSD  \n    FROM `sales` lss  \n    JOIN `countries` c ON lss.`country_id` = c.`country_id`  \n    WHERE ( lss.`transaction_utc` BETWEEN '2012-09-26' AND '2012-10-26' AND lss.`username` = 'someuser' )\nGROUP BY lss.`transaction_utc`\n</code></pre>\n\n<p>TABLE SCHEMA:</p>\n\n<pre><code>CREATE TABLE IF NOT EXISTS `sales` (\n  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,\n  `user_linkshare_account_id` int(11) unsigned NOT NULL,\n  `username` varchar(16) NOT NULL,\n  `country_id` int(4) unsigned NOT NULL,\n  `order` varchar(16) NOT NULL,\n  `raw_tracking_code` varchar(255) DEFAULT NULL,\n  `transaction_utc` datetime NOT NULL,\n  `processed_utc` datetime NOT NULL ,\n  `sku` varchar(16) NOT NULL,\n  `sale_original` decimal(10,4) NOT NULL,\n  `sale_usd` decimal(10,4) NOT NULL,\n  `quantity` int(11) NOT NULL,\n  `commission_original` decimal(10,4) NOT NULL,\n  `commission_usd` decimal(10,4) NOT NULL,\n  `original_currency` char(3) NOT NULL,\n  PRIMARY KEY (`id`,`transaction_utc`),\n  UNIQUE KEY `idx_unique` (`username`,`order`,`processed_utc`,`sku`,`transaction_utc`),\n  KEY `raw_tracking_code` (`raw_tracking_code`),\n  KEY `idx_usd_amounts` (`sale_usd`,`commission_usd`),\n  KEY `idx_countries` (`country_id`),\n  KEY `transaction_utc` (`transaction_utc`,`username`,`country_id`,`sale_usd`,`commission_usd`)\n) ENGINE=InnoDB  DEFAULT CHARSET=utf8\n/*!50100 PARTITION BY RANGE ( TO_DAYS(`transaction_utc`))\n(PARTITION pOLD VALUES LESS THAN (735112) ENGINE = InnoDB,\n PARTITION p201209 VALUES LESS THAN (735142) ENGINE = InnoDB,\n PARTITION p201210 VALUES LESS THAN (735173) ENGINE = InnoDB,\n PARTITION p201211 VALUES LESS THAN (735203) ENGINE = InnoDB,\n PARTITION p201212 VALUES LESS THAN (735234) ENGINE = InnoDB,\n PARTITION pMAX VALUES LESS THAN MAXVALUE ENGINE = InnoDB) */ AUTO_INCREMENT=19696320 ;\n</code></pre>\n"},{"tags":["ruby-on-rails-3","performance"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":215,"score":2,"question_id":12892937,"title":"Improve Rails loading time","body":"<p>This is a bit of a follow-up from <a href=\"http://stackoverflow.com/q/4736546/305019\">a previous question on improving rails console loading time</a>.</p>\n\n<p>The first great suggestion was to figure out <a href=\"http://stackoverflow.com/a/5071198/305019\">which gems take too long</a>.</p>\n\n<p>Next answer, suggested using <code>:require =&gt; nil</code> and <a href=\"http://stackoverflow.com/a/7146810/305019\">loading those gems later</a>.</p>\n\n<p>With some gems however, it's not entirely clear how to accomplish this without breaking things. Here's a list of our 'biggest offenders', I wonder if someone can suggest the best approach to loading them only when necessary?</p>\n\n<pre><code>require gon: 2.730000 (2.870059)\nrequire omniauth-openid: 1.410000 (1.503858)\nrequire cancan: 2.640000 (2.707467)\nrequire fog: 2.730000 (2.846530)\nrequire activeadmin: 3.650000 (3.923877)\n</code></pre>\n\n<p>and of course there are many more that take around 1 second or less, which also adds up... but at least removing the big ones will already improve things.</p>\n\n<h3>how can I selectively load gems later to make rails load faster?</h3>\n"},{"tags":["hash","random","performance"],"answer_count":5,"favorite_count":1,"up_vote_count":9,"down_vote_count":1,"view_count":3481,"score":8,"question_id":2575787,"title":"Looking for a fast hash-function","body":"<p>I'm looking for a special hash-function. Let's say I have a large list of strings, if I order them by their hash-values they should be ordered quasi randomly. </p>\n\n<p>The most important point is: it must be super fast. I've tried md5 and sha1 and they're using to much cpu power.</p>\n\n<p>Clashes are not a problem.</p>\n\n<p>I'm using javascript, so it shouldn't be too complicated to implement.</p>\n"},{"tags":["performance","caching","memory-management","operating-system"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":13091453,"title":"System/OS Caching vs. Application Caching","body":"<p>When developing applications that work with compressed on-disk indexes or on-disk files where parts of the index or the file are accessed repetitively (for arguments sake, let's say with something akin to a Zipfian  distribution), I wonder when is it sufficient/better to rely on OS-level caching (e.g., memory mapping on say a Debian system), and when is it better to implement something on the application layer (e.g., something like <a href=\"http://docs.oracle.com/javase/1.5.0/docs/api/java/nio/channels/FileChannel.html\" rel=\"nofollow\">FileChannel</a> buffering or Memcached or a custom LRU-cache in Java code).</p>\n\n<p>For example, <a href=\"http://juanggrande.wordpress.com/2011/01/05/os-cache-do-matter/\" rel=\"nofollow\">one article</a> (in reference to Solr) argues for leaving memory free for OS-caching:</p>\n\n<blockquote>\n  <p>The OS’s cache is really useful, it decreases significantly the time required to answer a query (even after completely restarting the server!), so always remember to keep some memory free for the OS.</p>\n</blockquote>\n\n<p>This got me wondering whether or not my application-level cache that fills memory with weak maps to LRU Java objects is doing more harm than good, esp. since Java is so greedy in terms of memory overhead ... instead of using that memory to cache a few final result objects, would that space be better used by the OS to cache lots of raw compressed data? On the other hand, the application layer cache would be better for platform independence, allowing for caching no matter what OS the code was running on. </p>\n\n<p>And so I realised that I had no idea how to go about answering that question in a principled way, other than running a couple of specific benchmarks. Which leads me to ask ...</p>\n\n<p><strong>What general guidelines exist for whether to assign available memory for application-level caching, or to leave that memory available for OS-level caching?</strong></p>\n\n<p>In particular, I'd love to be able to better recognise when coding an application-level cache is a waste of time, or even harmful for performance.</p>\n"},{"tags":["php","string","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":3,"view_count":104,"score":0,"question_id":8773270,"title":"Performance overhead for PHP string concatenation?","body":"<p>I'm starting a new project shortly and going to use coding standards. I've always written SQL statements like this:</p>\n\n<pre><code>$sql = sprintf(\"INSERT INTO users (name) VALUES ('%s')\", $name);\n</code></pre>\n\n<p>I'm wondering if there is any performance gained by using one of these:</p>\n\n<pre><code>$sql = \"INSERT INTO users (name) VALUES ('\".$name.\"')\";\n$sql = \"INSERT INTO users (name) VALUES ('$name')\";\n</code></pre>\n\n<p><strong>Also</strong>: Does this performance difference fluctuate with the addition of more \"parameters\" (as in the case of the first line of code) ?</p>\n\n<p>Thanks.</p>\n"},{"tags":["c#",".net","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":9,"down_vote_count":0,"view_count":230,"score":9,"question_id":13054708,"title":"What is the best way to route paths in a large grid?","body":"<p>I'm working on an algorithm to find a set of non intersected paths in a grid for a \ngiven pairs of points..\nLike this for these pairs:\n(9,4) and (12,13)\n<img src=\"http://i.stack.imgur.com/3ZW1M.png\" alt=\"Sample Grid\"></p>\n\n<p>The output should be something like this:</p>\n\n<pre><code>    9,10,11,7,3,4\n\n    13,14,15,16,12\n</code></pre>\n\n<p>and print \"Blocked\" if it can't route all paths</p>\n\n<p>First I searched for an already made algorithm to find all simple paths between 2 \npoints in a graph or a grid. and I found this one by @Casey Watson and @svick <a href=\"http://stackoverflow.com/questions/58306/graph-algorithm-to-find-all-connections-between-two-arbitrary-vertices\">here</a>.. \nIt works really well but for small graphs only.</p>\n\n<p>I converted it to C#.NET and enhanced it a little bit to be able to find paths of \nmaximum length X. and build on it my total algorithm.</p>\n\n<p>The one I built works fine in small graphs..\nHere is routes 9 pairs in a 8x8 grid..\n<img src=\"http://i.stack.imgur.com/i3WJK.png\" alt=\"enter image description here\"></p>\n\n<p>but it takes a huge time in larger ones like the 16x16 or even the final one I intended to do which is a 3D model of 16x16x2\nLike this</p>\n\n<p><img src=\"http://i.stack.imgur.com/vR484.png\" alt=\"8x8x2 grid\"></p>\n\n<p>The algorithm was developed to be a depth first search <strong>RECURSIVE</strong> algorithm, but it \ntook a huge time to return value to the user. so I decided to convert it to loops instead of the recursive calls so that I can benefit from <strong>yield return</strong> feature in .NET\nbut still it didn't help any better.</p>\n\n<p>The loops version of the algorithm find a route for a pair of points in less than a second but the recursive one took more than 90 seconds.</p>\n\n<p><img src=\"http://i.stack.imgur.com/lQT7O.png\" alt=\"enter image description here\"></p>\n\n<p>when I tried with 2 pairs, the loops version took around 342 seconds but the recursive one took around 200..</p>\n\n<p><img src=\"http://i.stack.imgur.com/OBM5M.png\" alt=\"enter image description here\"></p>\n\n<p>So I can't know which is faster..!? the recursive or the loops one..</p>\n\n<p>I really want to know the best way to do this..</p>\n\n<p>Note : the first digit in the number of the node determine the layer (Starts at 1)..</p>\n\n<p>Here is the code</p>\n\n<pre><code>    using System;\n    using System.Collections;\n    using System.Collections.Generic;\n    using System.Diagnostics;\n    using System.IO;\n    using System.Linq;\n\n    namespace AlgorithmTest\n    {\n     struct Connection\n    {\n    public int FirstNode;\n    public int SecondNode;\n\n    public Connection(int N1,int N2)\n    {\n        FirstNode = N1;\n        SecondNode = N2;\n    }\n}\nenum Algorithm\n{ Recursion, Loops }\n\npublic class Search\n{\n\n    private const int MAX = 15;\n\n    private const int Width = 16;\n    private const int Length = 16;\n    private const int Height = 2;\n\n\n\n    private static void Main(string[] args)\n    {\n\n\n        var graph = new Graph();\n\n\n        var str = new int[Height,Length, Width];\n        var level = ((int)Math.Pow(10, (Length * Width).ToString().Length) &gt;= 100) ? (int)Math.Pow(10, (Length * Width).ToString().Length) : 100;              \n        for (var i = 0; i &lt; Height; i++)\n        {\n            int num = 0;\n            for (var j = 0; j &lt; Length; j++)\n                for (var k = 0; k &lt; Width; k++)\n            {\n                str[i, j, k] = ++num + level;\n\n            }\n            level += level;\n        }\n\n\n        for (var i = 0; i &lt; Height; i++)\n        {\n            for (var j = 0; j &lt; Length; j++)\n            {\n                for (var k = 0; k &lt; Width; k++)\n                {\n\n                    if (i &lt; Height - 1) graph.addEdge(str[i, j, k], str[i + 1, j, k]);\n                    if (i &gt; 0) graph.addEdge(str[i, j, k], str[i - 1, j, k]);\n\n                    if (k &lt; Width - 1) graph.addEdge(str[i, j, k], str[i, j, k + 1]);\n                    if (k &gt; 0) graph.addEdge(str[i, j, k], str[i, j, k - 1]);\n\n                    if (j &lt; Length - 1) graph.addEdge(str[i, j, k], str[i, j + 1, k]);\n                    if (j &gt; 0) graph.addEdge(str[i, j, k], str[i, j - 1, k]);\n\n\n                }\n            }\n        }\n\n\n\n        var wt = new Stopwatch();\n\n       wt.Start();\n        var connectedNodes = new List&lt;Connection&gt;()\n                                 {\n\n\n\n                                     new Connection(1030, 1005),\n       //                              new Connection(1002, 1044),\n    //                                         new Connection(1015, 1064),\n    //                                        new Connection(1041, 1038),\n    //                                         new Connection(1009, 1027),\n    //                                         new Connection(1025, 1018),\n    //                                         new Connection(1037, 1054),\n    //                                         new Connection(1049, 1060),\n    //                                         new Connection(1008, 1031),\n    //                                         new Connection(1001, 1035),\n\n                                 };\n        wt.Start();\n        Console.WriteLine(\"Using Loops:\");\n        Console.WriteLine();\n        var allPaths = new Search().FindAllPaths(connectedNodes, graph, MAX, Algorithm.Loops);\n        wt.Stop();\n        foreach (var path in allPaths)\n        {\n            PrintPath(path);\n        }\n        Console.WriteLine(\"Total Seconds: \" + wt.Elapsed.TotalSeconds + \", Number of paths: \" + allPaths.Count());\n        Console.WriteLine(\"***************************************************************************************************\");\n        Console.WriteLine(\"Using Recursion:\");\n        Console.WriteLine();\n        wt.Reset();\n        wt.Start();\n        allPaths = new Search().FindAllPaths(connectedNodes, graph, MAX, Algorithm.Recursion);\n        wt.Stop();\n        foreach (var path in allPaths)\n        {\n            PrintPath(path);\n        }\n        Console.WriteLine(\"Total Seconds: \" + wt.Elapsed.TotalSeconds + \", Number of paths: \" + allPaths.Count());\n        Console.WriteLine();\n\n    }\n\n    private IEnumerable&lt;List&lt;int&gt;&gt; FindAllPaths(List&lt;Connection&gt; connectedNodes, Graph graph, int max, Algorithm algorithm)\n    {\n        var paths=new Stack&lt;List&lt;int&gt;&gt;();\n        var blocked=new List&lt;int&gt;();\n\n        for (var i = 0; i &lt; connectedNodes.Count; i++)\n        {\n            if (!blocked.Contains(connectedNodes[i].FirstNode)) blocked.Add(connectedNodes[i].FirstNode);\n            if (!blocked.Contains(connectedNodes[i].SecondNode)) blocked.Add(connectedNodes[i].SecondNode);\n        }\n\n        if (algorithm == Algorithm.Recursion)\n        {\n            if (FindAllPaths(connectedNodes, 0, max, graph, paths, blocked))\n            {\n                Console.WriteLine(\"BLOCKED\");\n                return new List&lt;List&lt;int&gt;&gt;();\n            }\n        }\n        else if(algorithm==Algorithm.Loops)\n        {\n            if (!FindAllPaths2(connectedNodes, 0, max, graph, paths, blocked))\n            {\n                Console.WriteLine(\"BLOCKED\");\n                return new List&lt;List&lt;int&gt;&gt;();\n            }\n        }\n\n        return paths;\n\n    }\n    private static bool FindAllPaths(List&lt;Connection&gt; connectedNodes,int order,int max, Graph graph, Stack&lt;List&lt;int&gt;&gt; allPaths, List&lt;int&gt; blocked)\n    {\n\n        if (order &gt;= connectedNodes.Count) return false;\n\n\n        var paths = SearchForPaths(graph, connectedNodes[order].FirstNode, connectedNodes[order].SecondNode, max, blocked);\n        if (paths.Count == 0) return true;\n        int i;\n        for (i = 0; i &lt; paths.Count; i++)\n        {\n            var path = paths[i];\n            allPaths.Push(path);\n            blocked.AddRange(path);\n\n\n            if (!FindAllPaths(connectedNodes, order + 1,max, graph, allPaths, blocked)) break;\n\n            allPaths.Pop();\n            foreach (var j in path)\n            {\n                blocked.RemoveAll(num =&gt; num==j);\n            }\n\n            paths.RemoveAll(list =&gt; IsListsSimilar(list,path));\n\n            i--;\n\n        }\n        if (i == paths.Count) return true;\n\n\n        return false;\n\n    }\n\n    private static bool IsListsSimilar(List&lt;int&gt; L1,List&lt;int&gt; L2)\n    {\n        if (L2.Count &gt; L1.Count) return false;\n\n        for (int i = 0; i &lt; L2.Count - 1; i++)\n        {\n            if (L1[i] != L2[i]) return false;\n        }\n        return true;\n    }\n\n    private static List&lt;List&lt;int&gt;&gt; SearchForPaths(Graph graph, int start, int end, int max, List&lt;int&gt; blocked)\n    {\n        blocked.Remove(start);\n        blocked.Remove(end);\n\n\n\n\n        var nodePaths = new List&lt;List&lt;int&gt;&gt;();\n        var visited = new LinkedList&lt;int&gt;();\n        visited.AddLast(start);\n        DepthFirstSearch(graph, visited, end, max, blocked, nodePaths);\n\n\n\n        nodePaths = nodePaths.OrderBy(list =&gt; list.Count).ToList();\n\n        return nodePaths;\n\n    }\n    private static void DepthFirstSearch(Graph graph, LinkedList&lt;int&gt; visited, int end, int max, List&lt;int&gt; blocked, List&lt;List&lt;int&gt;&gt; paths)\n    {\n        var nodes = graph.adjacentNodes(visited.Last.Value);\n        // examine adjacent nodes\n        var nodeCount = blocked.Count;\n        for (int i = 0; i &lt; nodeCount; i++)\n        {\n            if (visited.Contains(blocked[i])) return;\n        }\n\n        if (visited.Count &gt; max) return;\n\n\n        nodeCount = nodes.Count;\n        for (var i = 0; i &lt; nodeCount; i++)\n        {\n            if (visited.Contains(nodes[i]) || nodes[i] != end) continue;\n\n            visited.AddLast(nodes[i]);\n\n            {\n                paths.Add(new List&lt;int&gt;(visited));\n\n            }\n            visited.RemoveLast();\n            break;\n        }\n\n\n\n        nodeCount = nodes.Count;\n        for (var i = 0; i &lt; nodeCount; i++)\n        {\n            if (visited.Contains(nodes[i]) || nodes[i] == end) continue;\n\n            visited.AddLast(nodes[i]);\n            DepthFirstSearch(graph, visited, end, max, blocked, paths);\n            visited.RemoveLast();\n        }\n\n    }\n\n    private static bool FindAllPaths2(List&lt;Connection&gt; connectedNodes, int order, int max, Graph graph, Stack&lt;List&lt;int&gt;&gt; allPaths, List&lt;int&gt; blocked)\n    {\n\n        if (order &gt;= connectedNodes.Count) return false;\n\n\n        foreach (var path in SearchForPaths2(graph, connectedNodes[order].FirstNode, connectedNodes[order].SecondNode, max, blocked))\n        {\n\n            allPaths.Push(path);\n            blocked.AddRange(path);\n\n\n            if (!FindAllPaths2(connectedNodes, order + 1, max, graph, allPaths, blocked)) break;\n\n            allPaths.Pop();\n            foreach (var j in path)\n            {\n                blocked.RemoveAll(num =&gt; num == j);\n            }\n\n\n        }\n\n\n\n\n        return true;\n\n    }\n    private static IEnumerable&lt;List&lt;int&gt;&gt; SearchForPaths2(Graph graph, int start, int end, int max, List&lt;int&gt; blocked)\n    {\n        blocked.Remove(start);\n        blocked.Remove(end);\n\n\n        var visited = new LinkedList&lt;int&gt;();\n        visited.AddLast(start);\n        foreach (var VARIABLE in DepthFirstSearch(graph, visited, end, max, blocked))\n        {\n            yield return VARIABLE;\n        }\n\n    }\n    private static IEnumerable&lt;List&lt;int&gt;&gt; DepthFirstSearch(Graph graph, LinkedList&lt;int&gt; visited, int end, int max, List&lt;int&gt; blocked)\n    {\n\n\n\n\n\n        var nodes = graph.adjacentNodes(visited.Last.Value);\n\n\n        var nodeCount = blocked.Count;\n        for (int i = 0; i &lt; nodeCount; i++)\n        {\n            if (visited.Contains(blocked[i])) yield break;\n        }\n\n\n        if (visited.Count &gt; max) yield break;\n\n        nodeCount = nodes.Count;\n        for (var i = 0; i &lt; nodeCount; i++)\n        {\n            if (visited.Contains(nodes[i]) || nodes[i] != end) continue;\n\n            visited.AddLast(nodes[i]);\n\n            yield return (new List&lt;int&gt;(visited));\n            visited.RemoveLast();\n            break;\n        }\n\n\n\n\n        nodeCount = nodes.Count;\n        for (var i = 0; i &lt; nodeCount; i++)\n        {\n            if (visited.Contains(nodes[i]) || nodes[i] == end) continue;\n\n            visited.AddLast(nodes[i]);\n            foreach (var P in DepthFirstSearch(graph, visited, end, max, blocked))\n            {\n\n                yield return P;\n\n            }\n\n            visited.RemoveLast();\n\n        }\n\n\n\n\n\n\n    }\n\n\n    private static void PrintPath(List&lt;int&gt; visited)\n    {\n\n        for (int i = 0; i &lt; visited.Count()-1; i++)\n        {\n            Console.Write(visited[i]);\n            Console.Write(\" --&gt; \");\n        }\n        Console.Write(visited[visited.Count() - 1]);\n\n        Console.WriteLine();\n        Console.WriteLine();\n\n    }\n\n\n}\npublic class Graph\n{\n    private readonly Dictionary&lt;int, HashSet&lt;int&gt;&gt; map = new Dictionary&lt;int, HashSet&lt;int&gt;&gt;();\n\n    public void addEdge(int node1, int node2)\n    {\n        HashSet&lt;int&gt; adjacent = null;\n\n        map.TryGetValue(node1, out adjacent);\n\n        if (adjacent == null)\n        {\n            adjacent = new HashSet&lt;int&gt;();\n            map.Add(node1, adjacent);\n        }\n        adjacent.Add(node2);\n    }\n\n    public List&lt;int&gt; adjacentNodes(int last)\n    {\n        HashSet&lt;int&gt; adjacent = null;\n\n        map.TryGetValue(last, out adjacent);\n\n        if (adjacent == null)\n        {\n            return new List&lt;int&gt;();\n        }\n        return new List&lt;int&gt;(adjacent);\n    }\n}\n    }\n</code></pre>\n"},{"tags":["javascript","jquery","performance","optimization","jquery-plugins"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":112,"score":0,"question_id":7973726,"title":"JQuery plugin to shard domain names for <img>'s","body":"<p>I was reading Stoyan Stefanov's <a href=\"http://www.phpied.com/simple-sharding-logic/\" rel=\"nofollow\">excellent article about JS domain sharding</a> for image files and wanted to improve his recipe.</p>\n\n<p>The below script will iterate through the images in a page, assign each to a bucket based on the length of the \"src\" value, and assign it to a bucket (logging this to the console).</p>\n\n<p>What I'm trying to do is create a JQuery plugin that can accept a collection like $('img') or $('link') and rewrite the \"src\" attribute with a sharded domain like \"http://images1.mydomain.com/path/to/image.png\".</p>\n\n<p>If my page has:</p>\n\n<pre><code>&lt;img src=\"/Images/file1.png\" /&gt;\n&lt;img src=\"/Images/filenumber2.png\" /&gt;\n&lt;img src=\"/Images/footer.png\" /&gt;\n</code></pre>\n\n<p>I want to do something like:</p>\n\n<pre><code>$(document).ready(function(){\n  $('img').domainShard(3, \"subdomain\", \"mydomain.com\")\n});\n</code></pre>\n\n<p>To roughly produce:</p>\n\n<pre><code>&lt;img src=\"http://subdomain1.mydomain.com/Images/file1.png\" /&gt;\n&lt;img src=\"http://subdomain2.mydomain.com/Images/filenumber2.png\" /&gt;\n&lt;img src=\"http://subdomain3.mydomain.com/Images/footer.png\" /&gt;\n</code></pre>\n\n<p>Using a JQuery plugin like so:</p>\n\n<pre><code>(function ($) {\n    $.fn.domainShard = function (buckets, subdomain, domain) {\n        var numBuckets =  buckets || 3;\n        var subdomain = subdomain || \"images\";\n        var domain = domain || \"mydomain.com\";\n        return this.each(function () {\n            // look at the src\n                // var src = $(this).attr('src');\n            // compute bucket assignment\n                // do stuff\n            // set the new path\n                // newSrc = $(this).attr('src', path);\n        });\n    };\n})(jQuery);\n</code></pre>\n\n<p>Here is Stoyan's Javascript:</p>\n\n<pre><code>function getBucket(url, numbuckets) {\n  var number = url.length,\n  group = number % numbuckets;\n  return group;\n}\n\nfunction toBuckets(stuff, numbuckets) {\n  var numbuckets = parseInt(numbuckets, 10),\n  url, group,\n  buckets = Array(numbuckets),\n  cache = {};\n    for (var i = 0, max = stuff.length; i &lt; max; i++) {\n        url = stuff[i].src;\n\n        if (typeof cache[url] === 'number') {\n            continue;\n        }\n        group = getBucket(url, numbuckets);\n        if (!buckets[group]) {\n            buckets[group] = [];\n        }\n        buckets[group].push(url);\n        cache[url] = group;\n    }\n    return buckets;\n}\n\nconsole.log(toBuckets(document.images, 3));\n</code></pre>\n\n<p>I'll keep hacking away and report back when I have something working -- any advice or assistance is greatly appreciated.</p>\n\n<p>Thanks!</p>\n"},{"tags":["c","performance","algorithm","mathematical-optimization"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":286,"score":2,"question_id":11809502,"title":"Which is better way to calculate nCr","body":"<p>Approach 1:<br>\nC(n,r) = n!/(n-r)!r!</p>\n\n<p>Approach 2:<br>\nIn the book <a href=\"http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CF0QFjAA&amp;url=http://www.math.upenn.edu/~wilf/website/CombinatorialAlgorithms.pdf&amp;ei=aTcdUJiDHojsrAfAn4C4Aw&amp;usg=AFQjCNGgowD1UYCUKKvzibbMHWzfnSGSBQ&amp;sig2=SA2ePQJ-b-0FRkz1RnlGVA\" rel=\"nofollow\">Combinatorial Algorithms by wilf</a>, i have found this:<br>\nC(n,r) can be written as <code>C(n-1,r) + C(n-1,r-1)</code>.</p>\n\n<p>e.g.     </p>\n\n<pre><code>C(7,4) = C(6,4) + C(6,3) \n       = C(5,4) + C(5,3) + C(5,3) + C(5,2)\n       .   .\n       .   .\n       .   .\n       .   .\n       After solving\n       = C(4,4) + C(4,1) + 3*C(3,3) + 3*C(3,1) + 6*C(2,1) + 6*C(2,2)\n</code></pre>\n\n<p>As you can see, the final solution doesn't need any multiplication. In every form C(n,r), either n==r or r==1.</p>\n\n<p>Here is the sample code i have implemented:</p>\n\n<pre><code>int foo(int n,int r)\n{\n     if(n==r) return 1;\n     if(r==1) return n;\n     return foo(n-1,r) + foo(n-1,r-1);\n}\n</code></pre>\n\n<p>See <a href=\"http://ideone.com/KIUbg\" rel=\"nofollow\">output</a> here.</p>\n\n<p>In the approach 2, there are overlapping sub-problems where we are calling recursion to solve the same sub-problems again. We can avoid it by using <a href=\"http://en.wikipedia.org/wiki/Dynamic_programming\" rel=\"nofollow\">Dynamic Programming</a>.</p>\n\n<p>I want to know which is the better way to calculate C(n,r)?. </p>\n"},{"tags":["c#","performance","events"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":29,"score":0,"question_id":13101794,"title":"Event add and remove implementation details","body":"<p>I'm developing on a application which has a lot of performance critial code. When looking at my code i noticed that i also have a lot of <code>+=</code> and <code>-=</code>on events with many invokations, so i ask myself ( and now you ) how <code>+=</code> and <code>-=</code>are implemented and how fast it is when have a lot of invokations.</p>\n"},{"tags":["java","performance","synchronized","simpledateformat"],"answer_count":3,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":91,"score":2,"question_id":12984345,"title":"Java 7 Calendar.getInstance, TimeZone.getTimeZone got synchronized and slow, any work arounds?","body":"<p>After upgrading my runtime to Java 7 I see incredible slowness...and my program is spending all of its time in the SimpleDateFormat constructor.  As described in a great post here: <a href=\"http://coffeedriven.org/?p=83\" rel=\"nofollow\">http://coffeedriven.org/?p=83</a> the TimeZone code is now checking for the presence of an application context in the static synchronized method getDefaultInAppContext.</p>\n\n<p>The problem for me is that its Spring Batch file reader code that is creating a new SimpleDateFormat object for each line that it reads!</p>\n\n<p>Anyone got a work around for this?</p>\n"},{"tags":["performance","autofac","castle-dynamicproxy","dynamic-proxy"],"answer_count":2,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":624,"score":2,"question_id":5256676,"title":"Autofac: Tips for increasing performance when using DynamicProxy?","body":"<p>I just start using DynamicProxy2 today. And found it caused significant performance drop.</p>\n\n<p>See the code below. Test1 is 10 times slower than Test2.</p>\n\n<p>Any tips for increasing performance when using DynamicProxy?</p>\n\n<pre><code>class Program\n{\n    public void Main()\n    {\n        for (int i = 0; i &lt; 3; i++)\n        {\n            var stopWatch = Stopwatch.StartNew();\n            int count = 1 * 1000 * 1000;\n\n            Test1(count);\n            //Test2(count);\n\n            long t = stopWatch.ElapsedMilliseconds;\n            Console.WriteLine(t.ToString() + \" milliseconds\");\n            Console.WriteLine(((double)count/(t/1000)).ToString() + \" records/1 seconds\");\n        }\n    }\n\n    void Test1(int count)\n    {\n        var builder = new ContainerBuilder();\n        builder.RegisterType&lt;TestViewModel&gt;()\n            .EnableClassInterceptors()\n            .InterceptedBy(typeof(NotifyPropertyChangedInterceptor));\n        builder.RegisterType&lt;NotifyPropertyChangedInterceptor&gt;();\n\n        var container = builder.Build();\n        for (int i = 0; i &lt; count; i++)\n        {\n            container.Resolve&lt;TestViewModel&gt;();\n        }\n    }\n\n    void Test2(int count)\n    {\n        var builder = new ContainerBuilder();\n        builder.RegisterType&lt;TestViewModel&gt;();\n\n        var container = builder.Build();\n        for (int i = 0; i &lt; count; i++)\n        {\n            container.Resolve&lt;TestViewModel&gt;();\n        }\n    }\n}\n\npublic class TestViewModel : INotifyPropertyChanged\n{\n    [Notify]\n    public virtual string Value { get; set; }\n    public event PropertyChangedEventHandler PropertyChanged;\n}\n\n/// &lt;summary&gt;\n/// Copied from: http://serialseb.blogspot.com/2008/05/implementing-inotifypropertychanged.html\n/// &lt;/summary&gt;\npublic class NotifyPropertyChangedInterceptor : IInterceptor\n{\n    public void Intercept(IInvocation invocation)\n    {\n        // let the original call go through first, so we can notify *after*\n        invocation.Proceed();\n        if (invocation.Method.Name.StartsWith(\"set_\"))\n        {\n            string propertyName = invocation.Method.Name.Substring(4);\n            var pi = invocation.TargetType.GetProperty(propertyName);\n\n            // check that we have the attribute defined\n            if (Attribute.GetCustomAttribute(pi, typeof(NotifyAttribute)) == null)\n                return;\n\n            // get the field storing the delegate list that are stored by the event.\n            FieldInfo info = invocation.TargetType.GetFields(BindingFlags.Instance | BindingFlags.NonPublic)\n                .Where(f =&gt; f.FieldType == typeof(PropertyChangedEventHandler))\n                .FirstOrDefault();\n\n            if (info != null)\n            {\n                // get the value of the field\n                PropertyChangedEventHandler evHandler = info.GetValue(invocation.InvocationTarget) as PropertyChangedEventHandler;\n                // invoke the delegate if it's not null (aka empty)\n                if (evHandler != null)\n                    evHandler.Invoke(invocation.TargetType, new PropertyChangedEventArgs(propertyName));\n            }\n        }\n    }\n}\n</code></pre>\n\n<p><strong>Update:</strong></p>\n\n<p>On my machine, Test1 takes about 45 seconds, Test2 takes about 4.5 seconds. After read <a href=\"http://stackoverflow.com/users/13163/krzysztof-kozmic\">Krzysztof Koźmic</a>'s answer, I tried to put <em>NotifyPropertyChangedInterceptor</em> into singleton scope:</p>\n\n<pre><code>builder.RegisterType&lt;NotifyPropertyChangedInterceptor&gt;().SingleInstance();\n</code></pre>\n\n<p>that saved me about 4 seconds. Now Test1 takes about 41 seconds.</p>\n\n<p><strong>Update 2:</strong></p>\n\n<p>Test3 takes about 8.3 seconds on my machine. So it seems using Autofac or DynamicProxy alone performance is not a very big problem (in my project), but combining them together would cause great performance drop.</p>\n\n<pre><code>    public void Test3(int count)\n    {\n        var generator = new Castle.DynamicProxy.ProxyGenerator();\n        for (int i = 0; i &lt; count; i++)\n        {\n            generator.CreateClassProxy(typeof(TestViewModel), \n                new NotifyPropertyChangedInterceptor());\n        }\n    }\n</code></pre>\n"},{"tags":["javascript","performance","node.js","object","garbage-collection"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":107,"score":1,"question_id":12539574,"title":"What's the best way (most efficient) to turn all the keys of an object to lower case?","body":"<p>I've come up with</p>\n\n<pre><code>function keysToLowerCase (obj) {\n  var keys = Object.keys(obj);\n  var n = keys.length;\n  while (n--) {\n    var key = keys[n]; // \"cache\" it, for less lookups to the array\n    if (key !== key.toLowerCase()) { // might already be in its lower case version\n        obj[key.toLowerCase()] = obj[key] // swap the value to a new lower case key\n        delete obj[key] // delete the old key\n    }\n  }\n  return (obj);\n}\n</code></pre>\n\n<p>But I'm not sure how will v8 behave with that, for instance, will it really delete the other keys or will it only delete references and the garbage collector will bite me later ?</p>\n\n<p>Also, I created <a href=\"http://jsperf.com/object-keys-to-lower-case\" rel=\"nofollow\">these tests</a>, I'm hoping you could add your answer there so we could see how they match up.</p>\n\n<p><strong>EDIT 1:</strong>\nApparently, according to the tests, it's faster if we don't check if the key is already in lower case, but being faster aside, will it create more clutter by ignoring this and just creating new lower case keys ? Will the garbage collector be happy with this ?</p>\n"},{"tags":["performance","azure","sql-azure","latency"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":314,"score":1,"question_id":11574516,"title":"Latency between Azure Web Role and SQL Azure and Application performance","body":"<p><strong>Azure Web Role</strong> and <strong>Sql Azure Latency</strong> </p>\n\n<p>Hi Just to know that there is a latency and timeout between the <strong>web worker role</strong> and <strong>SQL Azure</strong>  , Event a Timeout at some times (These a are not random having frequently ) 40% of 100 pings does not have 0ms timeout </p>\n\n<p>if Web worker role and SQL Azure in the same data center WHY there is a timeout as they are communicating using their internal network </p>\n\n<p>Pls Refer the attached screenshots :</p>\n\n<p><img src=\"http://i.stack.imgur.com/Ji8en.png\" alt=\"enter image description here\"></p>\n\n<p>Application which runs on this web worker role has a mysteries performance ups and downs .. if may be due to various reasons but what i need  to know is that does theses statistics on latency  and timeout affects web application performance ?.</p>\n\n<p>Thanks,</p>\n"},{"tags":["java","performance","google-app-engine","gwt"],"answer_count":4,"favorite_count":4,"up_vote_count":9,"down_vote_count":0,"view_count":504,"score":9,"question_id":10016909,"title":"Is google app engine 1.6.4 slower in local?","body":"<p><strong>Original issue</strong></p>\n\n<p>Since I changed the version from 1.6.3. to 1.6.4 I get serious performance problems working together with GWT in hosted mode.</p>\n\n<p><strong>Update 18/04/2012</strong></p>\n\n<p>The issue is reproductible also in 1.6.4.1 in dev environment.\nBy now the best is to downgrade to 1.6.3</p>\n\n<p><strong>Update 09/07/2012</strong></p>\n\n<p>According to Kris Giesing:\nThis is still not fixed in 1.7.0. A request that takes 330ms to process in 1.4.3, and 415ms to process in 1.6.3, takes 13740ms to process in 1.7.0. That's from timing the Java analysis code (no I/O) - almost a 40x slowdown.</p>\n\n<p><strong>Update 09/08/2012</strong></p>\n\n<p>Google acknowledged the problem in the <a href=\"http://code.google.com/p/googleappengine/issues/detail?id=7282\" rel=\"nofollow\">issue 7282</a> of google appengine's public issue list.</p>\n"},{"tags":["android","performance","sqlite","profiling"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":35,"score":2,"question_id":13094931,"title":"How can I find how much time is being spent in the SQLite engine on Android?","body":"<p>We are trying to track down a performance issue in our app when we do a batch of database updates.  Using the Android DDMS Profiler in Eclipse seems to only show the amount of CPU time spent in the java code, it appears to not include the time spent in the SQLite engine.\nThis makes it VERY hard to determine where all the wall-clock time is being spent.</p>\n\n<p>Is there any way to find out how much wall-time is being spent in the various SQLite engine calls?</p>\n"},{"tags":["xml","performance","query","qt","xquery"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":20,"score":0,"question_id":13098726,"title":"Qt: how to make generic (X)Query in Qt to different XML documents?","body":"<p>I'm new to QXmlQuery lib and I want to learn what is capable on with XML documents. I have maybe more than 1000 XML documents, some of them are the same (the xml scheme is the same), some of them are different.. But all of them have one common element called 'list' and this element have always children called 'item' (many items). I don't know the absolute path to the list element in each document, but I know that it is somewhere in. I want to make one query to all of this files and extract the 'items'.\nMay be this will illustrate my idea better:</p>\n\n<pre><code>QXmlQuery xquery;\nxquery.setQuery( \"doc('some_doc.xml')/list/item\");\nif (xquery.isValid())\n{\n    QXmlResultItems itemResult;\n    xquery.evaluateTo( &amp;itemResult );\n    //take an iterator or QStringList.. nevermind\n}\n</code></pre>\n\n<p>I have a code somewhere like this.</p>\n\n<p>There are two examples of these xml docs:</p>\n\n<pre><code>&lt;globalroot&gt;\n  &lt;list&gt;\n    &lt;item&gt;20.15.14447.2214&lt;/item&gt;\n    &lt;item&gt;21.15.14447.2214&lt;/item&gt;\n    &lt;item&gt;22.15.14447.2214&lt;/item&gt;\n  &lt;/list&gt;\n&lt;/globalroot&gt;\n</code></pre>\n\n<p>and the second example:</p>\n\n<pre><code>&lt;root&gt;\n &lt;sdmlt&gt;random text, not important&lt;/sdmlt&gt;\n &lt;localroot&gt;\n  &lt;list&gt;\n    &lt;item&gt;40.15.14447.2214&lt;/item&gt;\n    &lt;item&gt;41.15.14447.2214&lt;/item&gt;\n    &lt;item&gt;42.15.14447.2214&lt;/item&gt;\n  &lt;/list&gt;\n &lt;/localroot&gt;\n&lt;/root&gt;\n</code></pre>\n\n<p>The output must look like this:</p>\n\n<pre><code> //I need an iterator or QStringList that contains following down elements:\n //from the first doc:\n 20.15.14447.2214\n 21.15.14447.2214\n 22.15.14447.2214\n\n //from the second doc:\n 40.15.14447.2214\n 41.15.14447.2214\n 42.15.14447.2214\n</code></pre>\n\n<p>How can I make such a generic query? I know how to iterate over the tree and just run the xQuery over each node until (isValid() == true), but I need some mechanism in Qt that allows me to do that implicitly. Is it possible? If you know something about it you can give me a short example; suggestions, similar topics are welcomed too.</p>\n"},{"tags":["performance","oracle"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":39,"score":0,"question_id":13098707,"title":"DBMS_ALERT.WAITANY uses up to 100% cpu","body":"<p>We have an application that uses <code>DBMS_ALERT.WAITANY</code>. Because  the users complained that the application becomes so slow at certain intervals we looked at the enterprise manager of our Oracle 11 DB. We saw that in the same intervals <code>DBMS_ALERT.WAITANY</code> uses up to 100% of one cpu.</p>\n\n<p>Is there a way to know what  notification causes <code>DBMS_ALERT.WAITANY</code> to use so much resource ?</p>\n\n<p>Thanks\nAndreas</p>\n"},{"tags":["javascript","jquery","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":37,"score":1,"question_id":13098649,"title":"How to calculate javascript functionality working speed?","body":"<p>In my Application, many places i used javascript and jquery functions.\nSome times javascript progress making to slow to perform actions.\nI able to see the speed after progress is complete via firefox tool.</p>\n\n<p>Is there any way to calculate javascript working speed while script on progress.\nThanks..</p>\n"},{"tags":["mysql","database","performance","load","stress"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":3473,"score":2,"question_id":2128824,"title":"Check load on mysql database","body":"<p>What would be the best ways to monitor mysql performance and load, queries per second, total queries over a hour etc?</p>\n\n<p>Thanks!</p>\n"},{"tags":["java","performance","graphics","drawimage"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":44,"score":1,"question_id":13097866,"title":"Graphics.drawImage() is too slow","body":"<p><code>Graphics.drawImage()</code> is slowing down my program significantly.</p>\n\n<p>As the background of my game, I am constantly drawing the image <strong>(700x500)</strong> then drawing the objects on top of it.</p>\n\n<p>Without drawing the background, the program runs perfectly. With the <code>drawImage()</code> it runs at less than half of that speed.</p>\n\n<p>I have tried</p>\n\n<pre><code>GraphicsEnvironment env = GraphicsEnvironment.getLocalGraphicsEnvironment();\nGraphicsDevice device = env.getDefaultScreenDevice();\nGraphicsConfiguration config = device.getDefaultConfiguration();\nBufferedImage buffy = config.createCompatibleImage(width, height);\n</code></pre>\n\n<p>But it doesn't (seem to) make any difference.</p>\n"},{"tags":["performance","concurrency","webserver"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":1,"view_count":454,"score":0,"question_id":8176501,"title":"Web Server: Theoretical limit to concurrent connections?","body":"<p>Is there a theoretical limit to how many concurrent connections a single web server can handle?</p>\n\n<p>I've been reading a lot about the <a href=\"http://gwan.ch/\" rel=\"nofollow\">G-WAN</a> web server that claims to be the fastest in the world, and the <a href=\"http://www.kegel.com/c10k.html\" rel=\"nofollow\">C10k</a> problem.</p>\n\n<p><strong>UPDATE</strong>:</p>\n\n<p>Another way to state this question, what is the ranked order of most likely bottlenecks that will prevent additional concurrent connections?</p>\n"},{"tags":["php","performance","caching","apc","eaccelerator"],"answer_count":5,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":1475,"score":4,"question_id":4821978,"title":"Choosing a PHP caching technique: output caching into files vs. opcode caching","body":"<p>I've heard of two caching techniques for the PHP code:</p>\n\n<ol>\n<li><p>When a PHP script generates output it stores it into local files. When the script is called again it check whether the file with previous output exists and if true returns the content of this file. It's mostly done with playing around the \"output buffer\". Somthing like this is described in <a href=\"http://www.theukwebdesigncompany.com/articles/php-caching.php\" rel=\"nofollow\">this</a> article.</p></li>\n<li><p>Using a kind of opcode caching plugin, where the compiled PHP code is stored in memory. The most popular of this one is APC, also eAccelerator.</p></li>\n</ol>\n\n<p>Now the question is whether it make any sense to use both of the techniques or just use one of them. I think that the first method is a bit complicated and time consuming in the implementation, when the second one seem to be a simple one where you just need to install the module.</p>\n\n<p>I use PHP 5.3 (PHP-FPM) on Ubuntu/Debian.</p>\n\n<p>BTW, are there any other methods to cache PHP code or output, which I didn't mention here? Are they worth considering?</p>\n"},{"tags":["c#","performance","random","generator","rate"],"answer_count":4,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":60,"score":1,"question_id":13094875,"title":"Generate random text files at a specific speed in C#","body":"<p>I have created a C# console app which continuously creates text files with randomly created strings. Time (in minute) is given as an input to the app and it runs for that number of minutes to generate continuous text files. I understand that the amount of data generated will vary depending upon the processor speed and other configurations. My machine is able to generate a total of 25 mb of data in text format in 1 minute.</p>\n\n<p>Now, my question is, can I control the rate of data generated per minute if given as another input to the app through my code? </p>\n\n<p>Any help would be very much appreciated.</p>\n"},{"tags":["php","mysql","arrays","performance","sleep"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":13094991,"title":"How do I ensure that php script gives enough time for execution of MySQL query?","body":"<p>I want to give my php script time to check queries.</p>\n\n<p>My cycle:</p>\n\n<ol>\n<li>Choose value</li>\n<li>--- I want (maybe here?) some delay, sleep or something ---</li>\n<li>MySQL SELECT Query</li>\n<li>Compare values depending on SELECT results</li>\n</ol>\n\n<p>Points 3 and 4 can theoretically take about 2 seconds. </p>\n\n<p>Will PHP wait until the SELECT is complete?</p>\n\n<p>Or will php wait for filling big arrays? (multidimensional, about 250 * 5 * 2 values..)</p>\n"},{"tags":["c#",".net","performance","linq"],"answer_count":4,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":132,"score":0,"question_id":13087328,"title":"Are there some disadvantages in using a lot of LINQ to Objects statements?","body":"<p>I come from Java programming and switching to C# programming I discovered the extreme powerful of LINQ.</p>\n\n<p>In my recent implementation I noticed that I use it (expecially LINQ to Objects) very often in my code to avoid <code>foreach</code> loops, to search elements in lists and for similar tasks.</p>\n\n<p>Now I'm wondering if there is some performance disadvantage in massively use Linq to Objects...</p>\n"}]}
{"total":25592,"page":6,"pagesize":100,"questions":[{"tags":["performance","cuda","gpu","nvidia"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":132,"score":0,"question_id":13035505,"title":"Converting a C for-loop to a CUDA for-loop","body":"<p>I have this low level for loop I've written in C that a friend suggested I write in CUDA. I've set up my CUDA enviroment and have been looking at the docs, but i'm still struggling with the syntax for what's been well over 2 weeks now. Can anyone help me out? What would this look like in CUDA? </p>\n\n<pre><code>float* red = new float [N];\nfloat* green = new float [N];\nfloat* blue = new float [N];\n\nfor (int y = 0; y &lt; h; y++)\n{\n    // Get row ptr from the color image\n    const unsigned char* src = rowptr&lt;unsigned char&gt;(color, 0, y, w);\n\n    // Get row ptrs for the destination channel features\n    float* rptr = rowptr&lt;float&gt;(red, 0, y, w);\n    float* gptr = rowptr&lt;float&gt;(green, 0, y, w);\n    float* bptr = rowptr&lt;float&gt;(blue, 0, y, w);\n\n    for (int x = 0; x &lt; w; x++)\n    {\n        *rptr++ = (float)*src++;\n        *gptr++ = (float)*src++;\n        *bptr++ = (float)*src++;\n    }\n}\n</code></pre>\n"},{"tags":["python","performance","algorithm","optimization","cython"],"answer_count":13,"favorite_count":7,"up_vote_count":31,"down_vote_count":0,"view_count":710,"score":31,"question_id":12926575,"title":"speeding up paring of strings into objects in Python","body":"<p>I'm trying to find an efficient way to pair together rows of data containing integer points, and storing them as Python objects. The data is made up of <code>X</code> and <code>Y</code> coordinate points, represented as a comma separated strings. The points have to be paired, as in <code>(x_1, y_1), (x_2, y_2), ...</code> etc. and then stored as a list of objects, where each point is an object. The function below <code>get_data</code> generates this example data:</p>\n\n<pre><code>def get_data(N=100000, M=10):\n    import random\n    data = []\n    for n in range(N):\n        pair = [[str(random.randint(1, 10)) for x in range(M)],\n                [str(random.randint(1, 10)) for x in range(M)]]\n        row = [\",\".join(pair[0]),\n               \",\".join(pair[1])]\n        data.append(row)\n    return data\n</code></pre>\n\n<p>The parsing code I have now is:</p>\n\n<pre><code>class Point:\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n\ndef test():\n    import time\n    data = get_data()\n    all_point_sets = []\n    time_start = time.time()\n    for row in data:\n        point_set = []\n        first_points, second_points = row\n        # Convert points from strings to integers\n        first_points = map(int, first_points.split(\",\"))\n        second_points = map(int, second_points.split(\",\"))\n        paired_points = zip(first_points, second_points)\n        curr_points = [Point(p[0], p[1]) \\\n                       for p in paired_points]\n        all_point_sets.append(curr_points)\n    time_end = time.time()\n    print \"total time: \", (time_end - time_start)\n</code></pre>\n\n<p>Currently, this takes nearly 7 seconds for 100,000 points, which seems very inefficient. Part of the inefficiency seems to stem from the calculation of <code>first_points</code>, <code>second_points</code> and <code>paired_points</code> - and the conversion of these into objects. </p>\n\n<p>Another part of the inefficiency seems to be the building up of <code>all_point_sets</code>. Taking out the <code>all_point_sets.append(...)</code> line seems to make the code go from ~7 seconds to 2 seconds!</p>\n\n<p>How can this be sped up? thanks.</p>\n\n<p><strong>FOLLOWUP</strong> Thanks for everyone's great suggestions - they were all helpful. but even with all the improvements, it's still about 3 seconds to process 100,000 entries. I'm not sure why in this case it's not just instant, and whether there's an alternative representation that would make it instant.  Would coding this in Cython change things?  Could someone offer an example of that?  thanks again.</p>\n"},{"tags":["sql","xml","query","performance"],"answer_count":4,"favorite_count":3,"up_vote_count":1,"down_vote_count":0,"view_count":700,"score":1,"question_id":3701616,"title":"Speed Up XML Queries in SQL Server 2005","body":"<p>I store all my data in on XML column in SQL Server 2005.</p>\n\n<p>As more and more records are being inserted, I notice the queries are slowing down.  I've tried creaeting a Primary XML Index, as well as a Secondary VALUE index and this did not do anything to help the speed.</p>\n\n<p>Any tips,thoughts, or tricks that I'm missing?</p>\n\n<p>Sample View that I query:</p>\n\n<pre><code>SELECT Id\n, CaseNumber\n, XmlTest.value('(/CodeFiveReport/ReportEvent/StartDate)[1]', 'varchar(25)') + ' ' + XmlTest.value('(/CodeFiveReport/ReportEvent/StartTime)[1]', 'varchar(25)') as StartDate\n, XmlTest.value('(/CodeFiveReport/@Status)[1]', 'varchar(10)') as [Status]\n, XmlTest.value('(/CodeFiveReport/ReportEvent/Address/PatrolDistrict/@Name)[1]', 'varchar(100)') as PatrolDistrict\n, XmlTest.value('(/CodeFiveReport/PrimaryUnit/@Name)[1]', 'varchar(40)') as PrimaryUnit\n, XmlTest.value('(/CodeFiveReport/ReportEvent/Address/@StreetNumber)[1]', 'varchar(50)') + ' ' + XmlTest.value('(/CodeFiveReport/ReportEvent/Address/@StreetName)[1]', 'varchar(50)') + ' ' + XmlTest.value('(/CodeFiveReport/ReportEvent/Address/StreetSuffix/@Name)[1]', 'varchar(50)') + ' ' + XmlTest.value('(/CodeFiveReport/ReportEvent/Address/@City)[1]', 'varchar(50)') + ' ' + XmlTest.value('(/CodeFiveReport/ReportEvent/Address/State/@Abbreviation)[1]', 'varchar(50)') + ' '  + XmlTest.value('(/CodeFiveReport/ReportEvent/Address/@ZipCode)[1]', 'varchar(50)') as Location\n, XmlTest.value('(/CodeFiveReport/ReportEvent/ReportType/@Name)[1]', 'varchar(50)') as ReportType\n, XmlTest.value('(/CodeFiveReport/ReportEvent/Offenses/OffenseDescription/OffenseType/@CodeAndDescription)[1]', 'varchar(50)') as IncidentType\n, XmlTest as Report\n, CreatedBy as UserId\n, XmlTest.value('(/CodeFiveReport/PrimaryUnit/@ID)[1]', 'integer') as UnitId\n, XmlTest.value('(/CodeFiveReport/PrimaryUnit/@Code)[1]', 'varchar(6)') as UnitCode\n, XmlTest.value('(/CodeFiveReport/Owner/AgencyID)[1]', 'char(2)') as AgencyId   \n, IsLocked\n, LockedBy\n, XmlTest.value('(/CodeFiveReport/VersionUsed)[1]', 'varchar(20)') as VersionUsed\nFROM UploadReport\nWHERE XmlTest.value('(/CodeFiveReport/Owner/AgencyID)[1]', 'char(2)') = '06'\n</code></pre>\n"},{"tags":["performance","hibernate","jpa"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":1574,"score":3,"question_id":4196921,"title":"Dynamic-update with JPA","body":"<p>I was surprised to recently learn that the default hibernate behavior is to update all of the fields in an object if only a single change is made and merge is called.</p>\n\n<p>Dynamic-update is the field that allows you to configure an alternative approach of just updating the changed field...</p>\n\n<p><a href=\"http://www.mkyong.com/hibernate/hibernate-dynamic-update-attribute-example/\" rel=\"nofollow\">http://www.mkyong.com/hibernate/hibernate-dynamic-update-attribute-example/</a></p>\n\n<p>I am using JPA with hibernate and I tried to add the following </p>\n\n<pre><code>@javax.persistence.Entity\n@org.hibernate.annotations.Entity(dynamicInsert=true, dynamicUpdate=true) \n</code></pre>\n\n<p>to my class (previously it only had the JPA annotation)</p>\n\n<p>Anyway, i've been monitoring the sql and unfortunately it didn't change it and I'm still seeing every field updated.</p>\n\n<p>This is my java that updates the object...</p>\n\n<pre><code>    @Transactional(readOnly = false, propagation = Propagation.REQUIRES_NEW)    \npublic void setAccountStatusForUser(String username, AccountStatus act){\n    User u = this.getUser(username);\n    u.setAccountStatus(act);\n    this.update(u);\n}\n</code></pre>\n\n<p>and the update method does the following:</p>\n\n<pre><code>    @Transactional(readOnly = false, propagation = Propagation.REQUIRES_NEW)\n  public Object update(Object o) {\n      Object a = this.entityManager.merge(o);\n      this.entityManager.flush();\n      return a;\n}\n</code></pre>\n\n<p>Any help would greatly be appreciated.</p>\n"},{"tags":["objective-c","cocoa-touch","performance","instruments"],"answer_count":2,"favorite_count":7,"up_vote_count":10,"down_vote_count":0,"view_count":3773,"score":10,"question_id":1488601,"title":"How to find out what mach_msg_trap waits for?","body":"<p>I a profiling my iPhone application on target, and according to Instruments 65% of the time is spent in <code>mach_msg_trap</code>.</p>\n\n<p>I have a background thread that runs-forever and send results back to the main thread using <code>performSelectorOnMainThread:withObject:waitUntilDone:</code>, aproximately every 2 seconds. I am not waiting until done.</p>\n"},{"tags":["python","performance","list","optimization","code-efficiency"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":102,"score":3,"question_id":13084619,"title":"First items in inner list efficiently as possible","body":"<p>I have a coordinated storage list in python   <code>A[row,col,value]</code> for storing non-zeros values.</p>\n\n<p>How can I get the list of all the row indexes? I expected this <code>A[0:][0]</code> to work as <code>print A[0:]</code> prints the whole list but <code>print A[0:][0]</code> only prints <code>A[0]</code>.</p>\n\n<p>The reason I ask is for efficient calculation of the number of non-zero values in each row <em>i.e</em> iterating over <code>range(0,n)</code> where n is the total number of rows. This should be much <em>cheaper</em> than my current way of <code>for i in range(0,n): for j in A: ...</code>. </p>\n\n<p>Something like:</p>\n\n<pre><code>c = []\n# for the total number of rows\nfor i in range(0,n):\n     # get number of rows with only one entry in coordinate storage list\n     if A[0:][0].count(i) == 1: c.append(i)                \nreturn c\n</code></pre>\n\n<p>Over:</p>\n\n<pre><code>c = []\n# for the total number of rows \nfor i in range(0,n):\n    # get the index and initialize the count to 0 \n    c.append([i,0])\n    # for every entry in coordinate storage list \n    for j in A:\n        # if row index (A[:][0]) is equal to current row i, increment count  \n        if j[0] == i:\n           c[i][1]+=1\nreturn c\n</code></pre>\n\n<p><strong>EDIT:</strong> </p>\n\n<p>Using Junuxx's answer, <a href=\"http://stackoverflow.com/questions/2600191/how-to-calculate-the-occurrences-of-a-list-item-in-python\">this question</a> and <a href=\"http://www.daniweb.com/software-development/python/code/217019/search-a-python-dictionary-both-ways\" rel=\"nofollow\">this post</a> I came up with the following <em>(for returning the number of singleton rows)</em> which is much faster for my current problems size of <code>A</code> than my original attempt. However it still grows with the number of rows and columns. I wonder if it's possible to not have to iterate over <code>A</code> but just upto <code>n</code>? </p>\n\n<pre><code># get total list of row indexes from coordinate storage list\nrow_indexes = [i[0] for i in A]\n# create dictionary {index:count}\nc = Counter(row_indexes)    \n# return only value where count == 1 \nreturn [c[0] for c in c.items() if c[1] == 1]\n</code></pre>\n"},{"tags":["sql","performance","sqlite","select","insert"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":57,"score":0,"question_id":13090775,"title":"Optimize this insert SQL Query","body":"<p>I've been trying to find out why my SQLite database is performing relatively slowly (4 seconds to insert 1500 records) and I think I've narrowed it down to this query. Is there a way to optimise this? </p>\n\n<pre><code> \"INSERT OR REPLACE INTO MainFrame(WID,PName,PAlias,PModel,FriendID, UniverseID, GalaxyID) VALUES\n  ((SELECT WID FROM Worlds WHERE WName= ?),\n  @pname,\n  @palias,\n  @pmodel,\n  (SELECT FriendID FROM Friend WHERE FriendName = @eFriend),\n  (SELECT UniverseID FROM Universes WHERE UniverseName = @eUniverse),\n  (SELECT GalaxyID FROM Galaxies WHERE GalaxyName = @eGalaxy ))\";\n</code></pre>\n\n<p>As you can see, there are a few <code>Selects</code> being used in an insert query. The reason for this is because the loop inserts data into other tables (<code>WID</code>, <code>FriendID</code>, <code>UniverseID</code>, <code>GalaxyID</code>) so I don't have that data until it's been inserted. I need this data to insert into the <code>MainFrame</code> table but this feels like a brute force approach. Any advice?</p>\n"},{"tags":["performance","assembly","x86","intel"],"answer_count":0,"favorite_count":1,"up_vote_count":3,"down_vote_count":1,"view_count":64,"score":2,"question_id":13092829,"title":"Any way to move 2 bytes in 32-bit x86 using MOV without causing a mode switch or cpu stall?","body":"<p>If I want to move 2 unsigned bytes from memory into a 32-bit register, can I do that with a MOV instruction and no mode switch?</p>\n\n<p>I notice that you CAN do that with the MOVSE and MOVZE instructions. For example, with MOVSE the encoding 0F B7 moves 16 bits to a 32 bit register. It is a 3 cycle instruction, though.</p>\n\n<p>Alternatively I guess I could move 4 bytes into the register and then somehow CMP just two of them somehow. What is the fastest strategy for retrieving and comparing 16-bit data on 32-bit x86? Note that I am mostly doing 32-bit operations so I can't switch to 16-bit mode and stay there.</p>\n\n<p>----- FYI to the uninitiated: the issue here is that 32-bit Intel x86 processors can MOV 8-bit data and 16-bit OR 32-bit data depending on what mode they are in. This mode is called the \"D-bit\" setting. You can use special prefixes 0x66 and 0x67 to use a non-default mode. For example, if you are in 32-bit mode, and you prefix the instruction with 0x66 this will cause the operand to be treated as 16-bit. The only problem is that doing this causes a big performance hit.</p>\n"},{"tags":["java","performance","coding-style"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":34,"score":0,"question_id":13092524,"title":"Setting a value only on first access -- best practice, (micro)performance?","body":"<p>In the below code, assume that <code>getAndClear()</code> will get called billions of times, i.e. assume that performance matters. It will return an array only during its first call. It must return null in all further calls. (That is, my question is about micro-optimization in some sense, and I'm aware of the fact it's bad practice, but you can also consider it as a question of \"which code is nicer\" or \"more elegant\".)</p>\n\n<pre><code>public class Boo {\n   public static int[] anything = new int[] { 2,3,4 };\n   private static int[] something = new int[] { 5,6,7 }; // this may be much bigger as well\n\n   public static final int[] getAndClear() {\n      int[] st = something;\n      something = null;\n      // ... (do something else, useful)            \n\n      return st;\n   }\n}\n</code></pre>\n\n<p>Is the below code faster? Is it better practice?</p>\n\n<pre><code>public static int[] getAndClear() {\n   int[] array = sDynamicTextIdList;\n   if (array != null) {\n      sDynamicTextIdList = null;\n      // ... (do something else, useful)     \n      return array;\n   }\n   // ... (do something else, useful)     \n\n   return null;\n}\n</code></pre>\n\n<p>A further variant could be this:</p>\n\n<pre><code>public static int[] getAndClear() {\n   int[] array = sDynamicTextIdList;\n   if (array != null) {\n      sDynamicTextIdList = null;\n   }\n   // ... (do something else, useful)     \n   return array;\n}\n</code></pre>\n\n<p>I know it probably breaks down to hardware architecture level and CPU instructions (setting something to 0 vs. checking for 0), and performance-wise, it doesn't matter, but then I would like to know which is the \"good practive\" or more quality code. In this case, the question can be reduced to this:</p>\n\n<pre><code>private static boolean value = true;       \n\npublic static int[] getTrueOnlyOnFirstCall() {\n   boolean b = value;\n   value = false;\n   return b;\n}\n</code></pre>\n\n<p>If the method is called 100000 times, this means that <code>value</code> will be set to <code>false</code> 99999 times unnecessarily. The other variant (faster? nicer?) would look like this:</p>\n\n<pre><code>public static int[] getTrueOnlyOnFirstCall() {\n   boolean b = value;\n   if (b) { \n      value = false;\n      return true;\n   }\n   return false;\n}\n</code></pre>\n\n<p>Moreover, compile-time and JIT-time optimizations may also play a role here, so this question could be extended by \"and what about in C++\". (If my example is not applicable to C++ in this form, then feel free to subtitute the statics with member fields of a class.)</p>\n"},{"tags":["mysql","performance","indexing","innodb","clustered-index"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":43,"score":2,"question_id":13057095,"title":"Behavior of InnoDB clustered compound index","body":"<p>We are running MySQL/ISAM database with a following table:</p>\n\n<pre><code>create table measurements (\n  `tm_stamp` int(11) NOT NULL DEFAULT '0',\n  `fk_channel` int(11) NOT NULL DEFAULT '0',\n  `value` int(11) DEFAULT NULL,\n  PRIMARY KEY (`tm_stamp`,`fk_channel`)\n);\n</code></pre>\n\n<p>The <code>tm_stamp</code>-<code>fk_channel</code> combination is required unique, hence the compound primary key. Now, for certain irrelevant reason, the database will be migrated to InnoDB engine. Upon googling something about it, i found out that the key will dictate the physical ordering of the data on the disk. 90% of the queries currently go as follows:</p>\n\n<pre><code>SELECT value FROM measurements\nWHERE fk_channel=A AND tm_stamp&gt;=B and tm_stamp&lt;=C\nORDER BY tm_stamp ASC\n</code></pre>\n\n<p>Inserts are 99% in order of <code>tm_stamp</code>, it's a storage for dataloggers network. The table has low millions of rows but growing steadily. The questions are</p>\n\n<ol>\n<li>Should the sole change of storage engine result in any significant performance change, better or worse?</li>\n<li>Does the order of columns in the index matter with regards to the most popular SELECT? <a href=\"http://www.cumps.be/nl/blog/read/efficient-compound-index-usage\" rel=\"nofollow\">This blog</a> suggest something along that line.</li>\n<li>Thanks to the nature of clustered index, may we perhaps leave out the ORDER BY clause and gain some performance?</li>\n</ol>\n"},{"tags":["c#","visual-studio-2010","colors","richtextbox","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":416,"score":1,"question_id":3545935,"title":"Why does the Rich Text Box freeze when loading a large string?","body":"<p>I have a program where I basically need to load Rich Text from a StringBuilder.\nThe problem is, somethimes I get a string that is 100,000 lines long (and this is a possible situation for the program), including Rtf codes and colours. </p>\n\n<p>The problem isn't building the string, it's when I asign the Rtf property to the StringBuilder.ToString(), it takes a solid <strong>4 minutes</strong> to load.</p>\n\n<pre><code>TextBox.Rtf = Build.ToString();\n</code></pre>\n\n<p>If I copy this same string from the StringBuilder, and load it in WordPad, it takes about <strong>2 or 3 seconds</strong>. I am diabling the RTB's redrawing by using SendMessage() and WM_SETREDRAW, but that doesn't change anything.</p>\n\n<p>Any suggestions?</p>\n"},{"tags":["python","performance","queue","multiprocessing","pipe"],"answer_count":1,"favorite_count":6,"up_vote_count":19,"down_vote_count":0,"view_count":2134,"score":19,"question_id":8463008,"title":"Python multiprocessing - Pipe vs Queue","body":"<p>What are the fundamental differences between queues and pipes in <a href=\"http://docs.python.org/library/multiprocessing.html\">Python's multiprocessing package</a>?</p>\n\n<p>In what scenarios should one choose one over the other?  When is it advantageous to use <code>Pipe()</code>?  When is it advantageous to use <code>Queue()</code>?</p>\n"},{"tags":["c#","performance","richtextbox"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":4,"view_count":81,"score":-2,"question_id":13091739,"title":"Inserting text into RichTextBox causes program to freeze","body":"<p>I have a RichTextBox control on my Form and I am trying to load about 1,800,000 characters into it. When I do, my application freezes.</p>\n\n<p>I know that my usage of this control is an edge case scenario, but does anyone have any recommendations for how I can prevent the application from freezing?</p>\n"},{"tags":["c#","performance","memory","memory-leaks","profiling"],"answer_count":3,"favorite_count":0,"up_vote_count":6,"down_vote_count":0,"view_count":75,"score":6,"question_id":13084585,"title":"Process.GetProcessesByName(String, String) Memory Leak","body":"<p>I have a piece of code that gets a list of processes on a remote computer using the static method <a href=\"http://msdn.microsoft.com/en-us/library/725c3z81%28v=vs.100%29.aspx\" rel=\"nofollow\">Process.GetProcessesByName(String, String)</a>, this runs on a lot of computers (a few thousands) and I've noticed it's a cause of a major memory leak.</p>\n\n<p>I ran ANTS memory profiler which told me that most of my memory is taken by strings, strings containing strage values like \"% Idle Time\", \"Processor Information\", and \"Cache Faults/sec\". I've recognized those strings as probably being a part of Performance Counters in the program, the problem is I don't have any performance counters in the program.</p>\n\n<p>Digging deeper found out those strings are held in hashtables that are held by PerformanceCounterLib which are held by ANOTHER hashtable that is stored inside an internal static member of the PerformanceCounterLib class (which in itself is internal).</p>\n\n<p>Digging even deeper into the rabbit hole, I've found out that Process.GetProcesesByName uses PerformanceCounterLib to get the process list running on a distant computer and that for each remote computer another PerformanceCounterLib instance is created and referenced in the static internal variable of PerformanceCounterLib. Each of those instances hold that hashtable of strings that I found out is clogging my memory (each of them is between 300-700 kb, meaning it's clogging up my Large Object Heap).</p>\n\n<p>I did not find a way to delete those unused PerformanceCounterLib instances, they are all internal and the user has no access to them.</p>\n\n<p>How can I fix my memory problem? This is REALLY bad, my program hits 5GB (my server's limit) within 24 hours.</p>\n\n<p><em>EDIT</em>: added a piece of code (not tested) that should reproduce the problem. For clarification:</p>\n\n<pre><code>/// computerNames is a list of computers that you have access to\npublic List&lt;string&gt; GetProcessesOnAllComputers(List&lt;string&gt; computerNames)\n{\n    var result = new List&lt;string&gt;();\n    foreach(string compName in computernames)\n    {\n        Process[] processes = Process.GetProcesses(compName); // Happens with every     method that gets processes on a remote computer\n        string processString = processes.Aggregate(new StringBuilder(), (sb,s) =&gt; sb.Append(';').Append(s), sb =&gt; sb.ToString());\n        result.Add(processString);\n        foreach (var p in processes)\n        {\n            p.Close();\n            p.Dispose();\n        }\n        processes = null;\n    }\n}\n</code></pre>\n"},{"tags":["performance","apache","benchmarking"],"answer_count":2,"favorite_count":2,"up_vote_count":12,"down_vote_count":0,"view_count":1684,"score":12,"question_id":2820306,"title":"Definition of Connect, Processing, Waiting in apache bench","body":"<p>When I run apache bench I get results like:</p>\n\n<pre><code>Command: abs.exe -v 3 -n 10 -c 1 https://mysite\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:      203  213   8.1    219     219\nProcessing:    78  177  88.1    172     359\nWaiting:       78  169  84.6    156     344\nTotal:        281  389  86.7    391     564\n</code></pre>\n\n<p>I can't seem to find the definition of Connect, Processing and Waiting.  What do those numbers mean?</p>\n"},{"tags":["ruby","performance","syntax"],"answer_count":9,"favorite_count":15,"up_vote_count":40,"down_vote_count":0,"view_count":4521,"score":40,"question_id":1836467,"title":"Is there a performance gain in using single quotes vs double quotes in ruby?","body":"<p>Do you know if using double quotes instead of single quotes in ruby decreases performance in any meaningful way in ruby 1.8 and 1.9.</p>\n\n<p>so if I type </p>\n\n<pre><code>question = 'my question'\n</code></pre>\n\n<p>is it faster than </p>\n\n<pre><code>question = \"my question\"\n</code></pre>\n\n<p>I imagine that ruby tries to figure out if something needs to be evaluated when it encounters double quotes and probably spends some cycles doing just that.</p>\n"},{"tags":["performance","compiler","cuda"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":13091705,"title":"Differences between CUDA 4.2 and 4.0 front-ends","body":"<p>This is from an experiment I did a while back, sadly I do not have access to the code or the machine, so I won't be able to check your profiling suggestions anytime soon. I used a high level directive based approach to parallelize two applications, one memory bound and the other was compute bound (which performed much better on Tesla M2090). The compute bound kernel was heavy on arithmetic operations (primarily multiplications). \nThese are my results for some data sizes for the compute bound kernel. The compiler options\nfor all these versions are minimal and consistent.</p>\n\n<pre><code>Size    cu4.2-llvm  cu4.2-open64    cu4.0   cu4.0-fastmath  cu4.2-fastmath\n--------------------------------------------------------------------------\n150       0.26          0.18         0.18       0.17             0.19\n250       1.04          0.55         0.55       0.53             0.67\n350       2.88          1.35         1.33       1.29             1.78\n450       6.31          2.8          2.78       2.69             3.88\n</code></pre>\n\n<p>From the above data, I have some questions:</p>\n\n<ol>\n<li><p>Since the results of <code>cu4.2-open64</code> (CUDA 4.2 with -open64 flag to use the Open64 front end), <code>cu4.0</code> and <code>cu4.0-fastmath</code> are almost the same, does CUDA 4.0 implicitly turn on the fast math options or is this because of FMA or something else ?</p></li>\n<li><p>When I provide <code>fastmath</code> options to CUDA 4.2 (cu4.2-fastmath), my results are close to the CUDA 4.0 group, so this difference between 4.2 and 4.0 could be related to optimization of arithmetic operations by the two compilers? </p></li>\n<li><p>As I said, I am not using CUDA directly, rather a high level pragma based approach (HMPP, PGI, OpenACC) wherein I specify clauses to distribute loops to GPU grid blocks, and specify data scoping of variables. \nCould this performance difference between compilers be related to how the high level \napproach translates the user code?</p></li>\n</ol>\n\n<p>For the memory intensive code, I see very minimal difference between these versions, which is why I don't show their results. </p>\n"},{"tags":["sql","performance","oracle","plsql"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":36,"score":0,"question_id":13091634,"title":"Oracle TimeStamp with Timezone vs timestamp vs datetime vs offsets","body":"<p>So i am trying to improve the performance of my reporting queries where time is being used as a filter for these sql statements. The columns that the time we're querying on are currently stored as timestamps with timezone. I have found that indexing on these columns seems to auto generate a function based index where it performs a sys_extract_utc on the column. I have also found that filtering on these columns seems to slow down the performance of my queries greatly. Considering i need these columns with the timestamp with timezone to perform calculations on other things beside the reporting, i had planned to create a new column for each of the timestamp with timezone fields.</p>\n\n<p>What i want to know is what would be the best way to go/ what would be the pros/cons with the creation of these two new columns. As in what type of field would give me the best performance on filtering my queries. a timestamped column with no timezone, a datetime, or would using an offset give me the best results for my queries?</p>\n\n<p><strong>NOTE:</strong> by offset what i mean is i would give what we consider as the beginning of time in our database a numeric value and from then on the time values would be given a numeric comparison to filter against. </p>\n\n<p>and help or suggestions are greatly appreciated. Thank you.</p>\n"},{"tags":["c#",".net","performance","webrequest"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":64,"score":0,"question_id":13078241,"title":"Understanding WebRequests","body":"<p>In my code (C#) I'm using a WebRequest:</p>\n\n<pre><code>var request = WebRequest.Create(url);\n</code></pre>\n\n<p>For some reason this line takes about 2-3 seconds to run. I've looked for a solution, but couldn't find any for the creation of the request. I've tried different urls (http and https, google, etc.) but nothing seems to help. Did anyone else experience such a behavior? Can anyone explain what exactly is going on during the creation of the request? Any alternatives?</p>\n\n<p>BTW - I'm on a 64bit Win 7 (Bootcamp)</p>\n\n<p>Edit - measured 3.126 seconds with Stopwatch for the following code (wireless):</p>\n\n<pre><code>var request = WebRequest.Create(\"http://www.google.com\");\n</code></pre>\n\n<p>On the same network, only wired, it took 0.01 seconds.</p>\n\n<p>Thanks everyone!</p>\n"},{"tags":["android","performance","android-ui"],"answer_count":1,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":71,"score":4,"question_id":12812003,"title":"What could cause an Android app to run slow on an identical device to one which it runs fast on?","body":"<p>I, and a few other of my Android app users, run a Galaxy Nexus. Most of us find the app to be blazing fast, but a couple are reporting that it is unusably slow <strong><em>also on a Galaxy Nexus</em></strong>. I'm shocked to hear them tell me that the buttons, scrolling, etc. are all slow. The main view of the app is a <code>ListView</code> containing many images, textviews, etc. In fact, you can check out <a href=\"https://play.google.com/store/apps/details?id=com.streamified.streamified\" rel=\"nofollow\">the app for free on Google Play</a> if you feel like digging deeper. I'm trying to compile a checklist of what might cause this issue.</p>\n\n<p>Here's what I have so far:</p>\n\n<ul>\n<li>Low memory</li>\n<li>Low disk space</li>\n<li>Uncaught errors</li>\n<li>Rooted device (?)</li>\n</ul>\n\n<p>Any other ideas?</p>\n\n<p>More importantly, is there any way to detect (or even adjust for!) potential problems?</p>\n"},{"tags":["python","sql","performance","parsing","obiee"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":13072593,"title":"Is there a good log parser for OBIEE's nqquery.log files?","body":"<p>I would like to extract a list of all the Logical SQLs executed by OBIEE. This information is present in OBIEE's nqquery.log log files. I am looking for a script which can parse this log file and also provide the following information for each Logical SQL, in a CSV file</p>\n\n<ul>\n<li>Hash Id of the Logical SQL and the complete query</li>\n<li>Time Taken to execute the logical sql</li>\n<li>Ability to group related Logical SQLs by Subject Area</li>\n</ul>\n\n<p>It should be able to collect all the Physical SQLs for a given Logical SQL after I increase the log level and disable cache.</p>\n\n<p>Added bonus, provide an Explain Plan for the Physical SQLs if I provide the Database connection information.</p>\n\n<p>Does such a script exist or is it asking for too much?</p>\n"},{"tags":["mysql","performance","insert"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":28,"score":0,"question_id":13088441,"title":"Efficient multiple row inserts in mysql","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/1793169/which-is-faster-multiple-single-inserts-or-one-multiple-row-insert\">Which is faster: multiple single INSERTs or one multiple-row INSERT?</a>  </p>\n</blockquote>\n\n\n\n<p>While going through a book on mysql, I found out two ways to insert a row in a database.</p>\n\n<pre><code>Method 1 \n\nINSERT INTO tableName (col1, col2, col3) VALUES('a', 'b', 'c');\nINSERT INTO tableName (col1, col2, col3) VALUES('d', 'b', 'c');\nINSERT INTO tableName (col1, col2, col3) VALUES('e', 'b', 'c');\n\nMethod 2\n\nINSERT INTO tableName (col1, col2, col3) VALUES('a', 'b', 'c'), ('d', 'b', 'c'), ('e', 'b', 'c');\n</code></pre>\n\n<p>Is the second method more efficient than the first one ? Or does it simply calls the <code>Method 1</code> multiple times ?</p>\n"},{"tags":["multithreading","performance","sql-server-2008","parallel-processing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":291,"score":0,"question_id":9561903,"title":"The query processor could not start the necessary thread resources","body":"<p>I having this error on my mssql server especially when there are many records inserted at the same time:\n <em>\"100|The query processor could not start the necessary thread resources for parallel query execution.\"</em> </p>\n\n<p>i try google search the problem to upgrade my sql performance so that can avoid this error, and plenty of it suggests to set maxdrop on the query or the max degree of parallelism on the mssql. </p>\n\n<p>But how can i know how many the value of the maxdrop or max degree of parallelism i should set to? \nand how i check the maximum value of max degree of parallelism of my server can run?\nif i change the max degree of parallelism of the server, means all the store proc will run following that setting, so will it affect other performance? or i just need to set the maxdrop on certain store proc?</p>\n"},{"tags":["ios","performance","screenshot","blur","low-level"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":41,"score":0,"question_id":13088131,"title":"Fast screenshot ios","body":"<p>In my project I have to make a screenshot of the screen and apply blur to create the effect of frosted glass. Content can be moved under the glass and blured picture changed. </p>\n\n<p>I'v used Accelerate.framework to speedup blurring, also i,v used OpenGL to draw CIImage directly to GLView.</p>\n\n<p>Now I'm looking for a way to optimize getting screenshot of the screen.\nI use this method to get screenshot of some area at the bottom of the screen:</p>\n\n<pre><code> CGSize size = CGSizeMake(rect.size.width, rect.size.height);\n\n    // get screenshot of self.view\n    CGColorSpaceRef colorSpaceRef = CGColorSpaceCreateDeviceRGB();\n    CGContextRef ctx = CGBitmapContextCreate(nil, size.width, size.height, 8, 0, colorSpaceRef, kCGImageAlphaPremultipliedFirst);\n    CGContextClearRect(ctx, rect);\n    CGColorSpaceRelease(colorSpaceRef);\n    CGContextSetInterpolationQuality(ctx, kCGInterpolationNone);\n    CGContextSetShouldAntialias(ctx, NO);\n    CGContextSetAllowsAntialiasing(ctx, NO);\n    CGContextTranslateCTM(ctx, 0.0, someView.frame.size.height);\n    CGContextScaleCTM(ctx, 1, -1);\n\n    //add mask\n    CGImageRef maskImage = [UIImage imageNamed:@\"mask.png\"].CGImage;\n    CGContextClipToMask(ctx, rect, maskImage);\n\n    [someView.layer renderInContext:ctx];\n\n    //get screenshot image\n    CGImageRef imageRef = CGBitmapContextCreateImage(ctx);\n</code></pre>\n\n<p>It works fine and fast if self.view has 1-2 subviews, but if there are several subviews (or it is tableview), then everything starts to slow down.</p>\n\n<p>So i try to find a fast way to get pixels from some rect on screen. Maybe using a low-level API.</p>\n"},{"tags":["mysql","performance","join"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":30,"score":0,"question_id":13087608,"title":"How should I write this MySQL query containing multiple Left Joins","body":"<p>I have a query consisting of multiple joins and I am wondering whether it can be re-written to improve performance.</p>\n\n<p>I have 2 tables as follows (I have removed non-important columns for this example):</p>\n\n<p>slots</p>\n\n<pre><code>------------------------------------------\n| id   | name | slot_1 | slot_2 | slot_3 |\n------------------------------------------\n| 1    | Bob  | 1      | 2      | 3      |\n| 2    | Jim  | 4      | 3      | 3      |\n| 3    | Alf  | 1      | 2      | 5      |\n------------------------------------------\n</code></pre>\n\n<p>(There are 25 slots in total, each in it's own column)</p>\n\n<p>slot_details</p>\n\n<pre><code>-----------------------------------\n| id   | stat_1 | stat_2 | stat_3 |\n-----------------------------------\n| 1    | 1      | 5      | 6      |\n| 2    | 4      | 31     | 23     |\n| 3    | 6      | 5      | 7      |\n| 4    | 7      | 4      | 9      |\n| 5    | 2      | 3      | 5      |\n-----------------------------------\n</code></pre>\n\n<p>(There are 10 stats in total)</p>\n\n<p>The query is as follows:</p>\n\n<pre><code>SELECT\n    slots.name,\n    slot_1_details.stat_1 AS slot_1_stat_1,\n    slot_1_details.stat_2 AS slot_1_stat_2,\n    slot_1_details.stat_3 AS slot_1_stat_3,\n    slot_2_details.stat_1 AS slot_2_stat_1,\n    slot_2_details.stat_2 AS slot_2_stat_2,\n    slot_2_details.stat_3 AS slot_2_stat_3,\n    slot_3_details.stat_1 AS slot_3_stat_1,\n    slot_3_details.stat_2 AS slot_3_stat_2,\n    slot_3_details.stat_3 AS slot_3_stat_3\nFROM\n    slots\nLEFT JOIN\n    slot_details AS slot_1_details\nON (\n    slot_1_details.id = slots.slot_1\n)\nLEFT JOIN\n    slot_details AS slot_2_details\nON (\n    slot_2_details.id = slots.slot_2\n)\nLEFT JOIN\n    slot_details AS slot_3_details\nON (\n    slot_3_details.id = slots.slot_3\n)\nWHERE (\n    slots.id = 1\n)\n</code></pre>\n\n<p>The expected outcome of this query would be as follows:</p>\n\n<pre><code>| name | slot_1_stat_1 | slot_1_stat_2 | slot_1_stat_3 | slot_2_stat_1 | slot_2_stat_2 | slot_2_stat_3 | slot_3_stat_1 | slot_3_stat_2 | slot_3_stat_3 |\n|bob   | 1             | 5             | 6             | 4             | 31            | 23            | 6             | 5             | 7             |\n</code></pre>\n\n<p>Unfortunately I am not in a situation where I can change the tables.</p>\n\n<p>Thank you for the help!</p>\n"},{"tags":["c#","performance","timer"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":112,"score":0,"question_id":13056349,"title":"is timer efficient?","body":"<p>I have created an application that is used to read a mail box at certain intervals. If there is a new mail it downloads the attachment creates pdf files say 100 + combines it and mail it back to a particular list. Due to some server policies am in a position to convert it to a window service. I have used a timer my code given below</p>\n\n<pre><code>private System.Threading.Timer timer;\ntimer = new System.Threading.Timer(TimerTick, null, TimeSpan.Zero, TimeSpan.FromMinutes(1));\n\nvoid TimerTick(object state)\n{\n  var minute = DateTime.Now.Minute;\n  if (minute != lastMinute &amp;&amp; minute % 5 == 0)\n  {\n    //check mail here\n  }\n}\n</code></pre>\n\n<p>Is implementing a timer like this an efficient way of doing this? Is there any better way to handle this? I am worried about the performance because the applications need to run 24 x7 and hence can end up in utilizing more cpu memory if inefficient.</p>\n\n<p><em><strong>is timer the only best available option in this scenario ?</em></strong></p>\n"},{"tags":[".net","silverlight","performance","http","httpwebrequest"],"answer_count":3,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":569,"score":4,"question_id":2400949,"title":"What might cause the big overhead of making a HttpWebRequest call?","body":"<p>When I send/receive data using <b>HttpWebRequest</b> (on <b>Silverlight</b>) in small blocks, I measure the <em>very small throughput</em> of 500 bytes/s over a \"localhost\" connection. When sending the data in large blocks, I get 2 MB/s, which is some <b>5000 times faster</b>.  </p>\n\n<p>Does anyone know what could cause this incredibly big overhead?</p>\n\n<p><b>Additional info</b>: </p>\n\n<ul>\n<li>I'm using the HTTP POST method</li>\n<li>I did the performance measurement on both Firefox 3.6 and Internet Explorer 7. Both showed similar results.</li>\n<li>My CPU is loaded for only 10% (quad core, so 40% actually)</li>\n<li>WebClient showed similar results</li>\n<li><a href=\"http://stackoverflow.com/questions/2407418/what-can-be-done-to-speed-up-synchronous-wcf-calls\">WCF/SOAP showed similar results</a></li>\n</ul>\n\n<p><b>Update</b>: The Silverlight client-side code I use is essentially my own implementation of the WebClient class. The reason I wrote it is because I noticed the same performance problem with WebClient, and I thought that the HttpWebRequest would allow to tweak the performance issue. Regrettably, this did not work. The implementation is as follows:</p>\n\n<pre><code>public class HttpCommChannel\n{\n    public delegate void ResponseArrivedCallback(object requestContext, BinaryDataBuffer response);\n\n    public HttpCommChannel(ResponseArrivedCallback responseArrivedCallback)\n    {\n        this.responseArrivedCallback = responseArrivedCallback;\n        this.requestSentEvent = new ManualResetEvent(false);\n        this.responseArrivedEvent = new ManualResetEvent(true);\n    }\n\n    public void MakeRequest(object requestContext, string url, BinaryDataBuffer requestPacket)\n    {\n        responseArrivedEvent.WaitOne();\n        responseArrivedEvent.Reset();\n\n        this.requestMsg = requestPacket;\n        this.requestContext = requestContext;\n\n        this.webRequest = WebRequest.Create(url) as HttpWebRequest;\n        this.webRequest.AllowReadStreamBuffering = true;\n        this.webRequest.ContentType = \"text/plain\";\n        this.webRequest.Method = \"POST\";\n\n        this.webRequest.BeginGetRequestStream(new AsyncCallback(this.GetRequestStreamCallback), null);\n        this.requestSentEvent.WaitOne();\n    }\n\n    void GetRequestStreamCallback(IAsyncResult asynchronousResult)\n    {\n        System.IO.Stream postStream = webRequest.EndGetRequestStream(asynchronousResult);\n\n        postStream.Write(requestMsg.Data, 0, (int)requestMsg.Size);\n        postStream.Close();\n\n        requestSentEvent.Set();\n        webRequest.BeginGetResponse(new AsyncCallback(this.GetResponseCallback), null);\n    }\n\n    void GetResponseCallback(IAsyncResult asynchronousResult)\n    {\n        HttpWebResponse response = (HttpWebResponse)webRequest.EndGetResponse(asynchronousResult);\n        Stream streamResponse = response.GetResponseStream();\n        Dim.Ensure(streamResponse.CanRead);\n        byte[] readData = new byte[streamResponse.Length];\n        Dim.Ensure(streamResponse.Read(readData, 0, (int)streamResponse.Length) == streamResponse.Length);\n        streamResponse.Close();\n        response.Close();\n\n        webRequest = null;\n        responseArrivedEvent.Set();\n        responseArrivedCallback(requestContext, new BinaryDataBuffer(readData));\n    }\n\n    HttpWebRequest webRequest;\n    ManualResetEvent requestSentEvent;\n    BinaryDataBuffer requestMsg;\n    object requestContext;\n    ManualResetEvent responseArrivedEvent;\n    ResponseArrivedCallback responseArrivedCallback;\n}\n</code></pre>\n\n<p>I use this code to send data back and forth to an HTTP server.</p>\n\n<p><b>Update</b>: after extensive research, I conclude that <a href=\"http://stackoverflow.com/questions/2407418/what-can-be-done-to-speed-up-synchronous-wcf-calls\">the performance problem is inherent to Silverlight v3</a>.</p>\n"},{"tags":["performance","algorithm","primes"],"answer_count":19,"favorite_count":12,"up_vote_count":23,"down_vote_count":4,"view_count":13089,"score":19,"question_id":622,"title":"Most efficient code for the first 10000 prime numbers?","body":"<p>I want to print the first 10000 prime numbers.\r\nCan anyone give me the most efficient code for this?\r\nClarifications:</p>\r\n\r\n<ol>\r\n<li>It does not matter if your code is inefficient for n &gt;10000.</li>\r\n<li>The size of the code does not matter.</li>\r\n<li>You cannot just hard code the values in any manner.</li>\r\n</ol>"},{"tags":["performance","benchmarking","neo4j"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":1105,"score":0,"question_id":7866832,"title":"Neo4j Benchmark","body":"<p>Does anyone know a simple benchmark for neo4j?</p>\n\n<p>I tried to build it by myself but it spends 1s to create 1 node.. maybe it isn't the right way to build it!</p>\n"},{"tags":["windows","performance","winapi","file-io","io"],"answer_count":2,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":383,"score":2,"question_id":7430959,"title":"How to make CreateFile as fast as possible","body":"<p>I need to read the contents of several thousands of small files at startup. On linux, just using fopen and reading is very fast. On Windows, this happens very slowly.</p>\n\n<p>I have switched to using Overlapped I/O (Asynchronous I/O) using ReadFileEx, where Windows does a callback when data is ready to read.</p>\n\n<p>However, the actual thousands of calls to CreateFile itself are still a bottleneck. Note that I supply my own buffers, turn on the NO_BUFFERING flag, give the SERIAL hint, etc. However, the calls to CreateFile take several 10s of seconds, whereas on linux everything is done much faster.</p>\n\n<p>Is there anything that can be done to get these files ready for reading more quickly?</p>\n\n<p>The call to CreateFile is:</p>\n\n<pre><code>            hFile = CreateFile(szFullFileName,\n                GENERIC_READ,\n                FILE_SHARE_READ | FILE_SHARE_WRITE,\n                NULL,\n                OPEN_EXISTING,\n                FILE_ATTRIBUTE_NORMAL | FILE_FLAG_OVERLAPPED | FILE_FLAG_NO_BUFFERING | FILE_FLAG_SEQUENTIAL_SCAN,\n                NULL);\n</code></pre>\n"},{"tags":["sql","database","performance","postgresql"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":13085521,"title":"PostgreSQL code optimalization","body":"<p>I have this code:</p>\n\n<pre><code>SELECT \n    rv_storage.m_product_id AS n_product_id,\n    rv_storage.value,\n    rv_storage.name,\n    m_warehouse.name AS warehouse_name,\n    rv_storage.qtyonhand,\n    rv_transaction.m_transaction_id,\n\n    CASE WHEN rv_transaction.movementtype = 'V+' THEN movementdate\n    ELSE NULL END AS last_in,\n\n    CASE WHEN rv_transaction.movementtype = 'C-' THEN movementdate\n    ELSE NULL END AS last_out,\n\n    rv_transaction.movementagedays,\n\n    CASE WHEN (movementagedays &lt; -90) AND (movementagedays &gt;= -180)  THEN qtyonhand\n    ELSE NULL END AS more_than_90,\n\n    CASE WHEN movementagedays &lt; -180 THEN qtyonhand\n    ELSE NULL END AS more_than_180\nFROM\n    adempiere.rv_storage\n    INNER JOIN\n    adempiere.rv_transaction ON\n        rv_transaction.m_product_id = rv_storage.m_product_id \n        AND rv_transaction.movementagedays = (\n            SELECT MAX(movementagedays) \n            FROM adempiere.rv_transaction \n            WHERE\n                rv_transaction.m_product_id = rv_storage.m_product_id \n                AND rv_transaction.movementtype = 'C-'\n                OR rv_transaction.movementtype = 'V+'\n            )\n    INNER JOIN\n    adempiere.m_warehouse ON\n        m_warehouse.m_warehouse_id = rv_storage.m_warehouse_id\nWHERE rv_storage.m_product_id IN (\n    SELECT m_product_id\n    FROM adempiere.rv_transaction\n    WHERE movementagedays &lt; -90\n    )\nORDER BY n_product_id;\n</code></pre>\n\n<p>That results in following table:</p>\n\n<p><img src=\"http://img9.imageshack.us/img9/4506/table3n.png\" alt=\"table\"></p>\n\n<p>But on the server with live data (100k+ rows on each table ) it is too slow.\nCan somebody tell me how can the code be optimized?</p>\n\n<p>Thank You</p>\n"},{"tags":["java","performance","garbage-collection","jvm","tuning"],"answer_count":4,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":1453,"score":7,"question_id":7916723,"title":"Full GC becoming very frequent","body":"<p>I've got a Java webapp running on one tomcat instance. During peak times the webapp serves around 30 pages per second and normally around 15.</p>\n\n<p>My environment is:</p>\n\n<pre><code>O/S: SUSE Linux Enterprise Server 10 (x86_64)\nRAM: 16GB\n\nserver: Tomcat 6.0.20\nJVM: Java HotSpot(TM) 64-Bit Server VM 1.6.0_14\nJVM options:\nCATALINA_OPTS=\"-Xms512m -Xmx1024m -XX:PermSize=128m -XX:MaxPermSize=256m\n               -XX:+UseParallelGC\n               -Djava.awt.headless=true\n               -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps\"\nJAVA_OPTS=\"-server\"\n</code></pre>\n\n<p>After a couple of days of uptime the Full GC starts occurring more frequently and it becomes a serious problem to the application's availability. After a tomcat restart the problem goes away but, of course, returns after 5 to 10 or 30 days (not consistent).</p>\n\n<p>The Full GC log before and after a restart is at <a href=\"http://pastebin.com/raw.php?i=4NtkNXmi\" rel=\"nofollow\">http://pastebin.com/raw.php?i=4NtkNXmi</a></p>\n\n<p>It shows a log before the restart at 6.6 days uptime where the app was suffering because Full GC needed 2.5 seconds and was happening every ~6 secs.</p>\n\n<p>Then it shows a log just after the restart where Full GC only happened every 5-10 minutes.</p>\n\n<p>I've got two dumps using <code>jmap -dump:format=b,file=dump.hprof PID</code> when the Full GCs where occurring (I'm not sure whether I got them exactly right when a Full GC was occurring or between 2 Full GCs) and opened them in <a href=\"http://www.eclipse.org/mat/\" rel=\"nofollow\">http://www.eclipse.org/mat/</a> but didn't get anything useful in Leak Suspects:</p>\n\n<ul>\n<li>60MB: 1 instance of \"org.hibernate.impl.SessionFactoryImpl\" (I use hibernate with ehcache)</li>\n<li>80MB: 1,024 instances of \"org.apache.tomcat.util.threads.ThreadWithAttributes\" (these are probably the 1024 workers of tomcat)</li>\n<li>45MB: 37 instances of \"net.sf.ehcache.store.compound.impl.MemoryOnlyStore\" (these should be my ~37 cache regions in ehcache)</li>\n</ul>\n\n<p>Note that I never get an OutOfMemoryError.</p>\n\n<p>Any ideas on where should I look next?</p>\n\n<p>thanks</p>\n"},{"tags":["linux","performance","ubuntu","kernel"],"answer_count":4,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":742,"score":6,"question_id":5399030,"title":"clone()/fork()/process creation is slow on some machines","body":"<p>Creating new processes is very slow on some of my machines, and not others. </p>\n\n<p>The machines are all similar, and some of the slow machines are running the exact same workloads on the same hardware and kernel (2.6.32-26, Ubuntu 10.04) as some of the fast machines. Tasks that do not involve process creation are the same speeds on all machines.</p>\n\n<p>For example, this program executes ~50 times slower on the affected machines:</p>\n\n<pre><code>int main()\n{\n    int i;\n    for (i=0;i&lt;10000;i++)\n    {\n        int p = fork();\n        if (!p) exit(0);\n        waitpid(p);\n    }\n    return 0;\n}\n</code></pre>\n\n<p>What could be causing task creation to be much slower, and what other differences could I look for in the machines?</p>\n\n<p>Edit1: Running bash scripts (as they spawn a lot of subprocesses) is also very slow on these machines, and strace on the slow scripts shows the slowdown in the <code>clone()</code> kernel call.</p>\n\n<p>Edit2: <code>vmstat</code> doesn't show any significant differences on the fast vs slow machines. They all have more than enough RAM for their workloads and don't go to swap.</p>\n\n<p>Edit3: I don't see anything suspicious in <code>dmesg</code></p>\n\n<p>Edit4: I'm not sure why this is on stackoverflow now, I'm not asking about the example program above (just using it to demonstrate the problem), but linux administration/tuning, but if people think it belongs here, cool.</p>\n"},{"tags":["performance","script","multiple","requests","loadui"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":13,"score":1,"question_id":13084710,"title":"How to run multiple test cases at the same time in LoadUI?","body":"<p>I have created a testsuite in SoapUI and a testsuite contains two testcases. I need to execute a testsuite at the same time from loadUI.</p>\n\n<p>Either command line or from loadUI.</p>\n\n<p>Thanks</p>\n"},{"tags":["performance","matlab","optimization","vectorization","mex"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":65,"score":2,"question_id":13070637,"title":"Optimizing a Vectorized Matlab Function","body":"<p>When i run profiler it tell me that the most time consuming code is the function <code>vdist</code>. Its a program that measures distance between two points on earth considering earth as an ellipsoid. The code looks standard and i don't know where and how it can be improved upon. The initial comments say, it has already been vectorized. Is there a counterpart to it  in some other language which can be used as a MEX file. All i want is improvement in terms of time efficiency. Here is a link to the code from Matlab FEX. </p>\n\n<p><a href=\"http://www.mathworks.com/matlabcentral/fileexchange/8607-vectorized-geodetic-distance-and-azimuth-on-the-wgs84-earth-ellipsoid/content/vdist.m\" rel=\"nofollow\">http://www.mathworks.com/matlabcentral/fileexchange/8607-vectorized-geodetic-distance-and-azimuth-on-the-wgs84-earth-ellipsoid/content/vdist.m</a>   </p>\n\n<p>The function is called from within a loop as- (You can find the function as its the most time consuming line here)</p>\n\n<pre><code>              109 for i=1:polySize \n              110    % find the two vectors needed\n       11755  111    if i~=1 \n0.02   11503  112        if i&lt;polySize \n0.02   11251  113         p0=Polygon(i,:); p1=Polygon(i-1,:); p2=Polygon(i+1,:);    \n         252  114        else \n         252  115         p0=Polygon(i,:); p1=Polygon(i-1,:); p2=Polygon(1,:); %special case for i=polySize \n         252  116        end \n         252  117    else \n         252  118         p0=Polygon(i,:); p1=Polygon(polySize,:); p2=Polygon(i+1,:); %special case for i=1 \n         252  119    end \n 0.02  11755  120    Vector1=(p0-p1); Vector2=(p0-p2); \n 0.06  11755  121    if ~(isequal(Vector1,Vector2) || isequal(Vector1,ZeroVec) || isequal(Vector2,ZeroVec)); \n              122        %determine normals and normalise and\n 0.17  11755  123        NV1=rotateVector(Vector1, pi./2); NV2=rotateVector(Vector2, -pi./2); \n 0.21  11755  124        NormV1=normaliseVector(NV1); NormV2=normaliseVector(NV2); \n              125        %determine rotation by means of the atan2 (because sign matters!)\n       11755  126        totalRotation = vectorAngle(NormV2, NormV1); % Bestimme den Winkel totalRotation zwischen den normierten Vektoren \n       11755  127      if totalRotation&lt;10 \n       11755  128          totalRotation=totalRotation*50; \n       11755  129      end \n0.01   11755  130      for res=1:6 \n0.07   70530  131         U_neu=p0+NV1; \n17.01  70530  132         [pos,a12] = vdist(p0(:,2),p0(:,1),U_neu(:,2),U_neu(:,1)); \n0.02   70530  133         a12=a12+1/6.*res*totalRotation; \n       70530  134         ddist=1852*safety_distance; \n4.88   70530  135         [lat2,lon2] = vreckon(p0(:,2),p0(:,1),ddist, a12); \n0.15   70530  136         extendedPoly(f,:)=[lon2,lat2];f=f+1; \n&lt; 0.01 70530  137      end \n       11755  138    end \n       11755  139 end \n</code></pre>\n"},{"tags":["java",".net","string","performance"],"answer_count":5,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":178,"score":3,"question_id":12590097,"title":"Is checking whether string.length == 0 still faster than checking string == \"\"?","body":"<p>I read from a programming book about 7-8 years ago that checking <code>string.length == 0</code> is a faster way to check for empty strings. I'm wondering if that statement still holds true today (or if it has ever been true at all), because I personally think <code>string == \"\"</code> is more straightforward and more readable. I mostly deal with high-level languages such as .NET and java.</p>\n"},{"tags":["performance","hibernate","oracle11g"],"answer_count":0,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":34,"score":4,"question_id":13074500,"title":"Hibernate Query Running Slow","body":"<p>I am currently facing a problem with a slow running Hibernate query that is being kicked off from a Java program. It is making a call to Oracle 11g at the back end.</p>\n\n<p>This query is taking anything from 40-90 seconds the first time it is run. On subsequent executions however the query is returned in a fraction of the time (and I don't even see the database being hit, so assume that hibernate is caching it).</p>\n\n<p>If I copy and paste the query from Enterprise Manager into a SQL client and run the very same query directly (even changing a few parameters) the query returns in a fraction of a second.</p>\n\n<p>If I look at the performance tuning tab in EM I see that the time taken is primarily taken up with User I/O Waits (97.5%), and CPU (2.5%). Could this mean that the fetch size I am using in hibernate is configured at too small a value?</p>\n\n<p>If there is any other information you might need to help me get to the bottom of this issue then please let me know.</p>\n\n<p>=====</p>\n\n<h2>Additional information:</h2>\n\n<p>We do have an index on the table and I can see that it is being used as part of the execution of the query, unfortunately it isn't very readable but I am not sure how else to include it:</p>\n\n<p>Id  Operation   Name    Rows\n(Estim)     Cost    Time\nActive(s)   Start\nActive  Execs   Rows\n(Actual)    Read\nReqs    Read\nBytes   Mem\n(Max)   Activity\n(%)     Activity Detail\n(# samples)\n0   SELECT STATEMENT                        1<br>\n1   . FILTER                        1<br>\n2   .. HASH JOIN RIGHT OUTER        2674    7223    1   +4  1   0           1M<br>\n3   ... TABLE ACCESS FULL   TOTEM_EQ_EXPIRYCODES    475     4   1   +4  1   481<br>\n4   ... HASH JOIN RIGHT OUTER       2674    7219    1   +4  1   0           399K<br>\n5   .... TABLE ACCESS BY INDEX ROWID    TOTEM_EQ_UNDERLYINGS    1   2   1   +4  1   1<br>\n6   ..... INDEX UNIQUE SCAN     TOTEM_EQ_UND_PK     1   1   1   +4  1   1<br>\n7   .... NESTED LOOPS                       1<br>\n8   ..... NESTED LOOPS      2674    7216    42  +4  1   0<br>\n9   ...... TABLE ACCESS BY GLOBAL INDEX ROWID   EQUITIES_MONTHLY_INSTRUMENTS    2671    1871    45  +1  1   8438    3517    27MB        24.44   db file sequential read (11)\n10  ....... INDEX RANGE SCAN    EQ_MON_INS_UNDERLYING_INDX  2671    12  42  +4  1   8438    27  216KB<br>\n11  ...... PARTITION RANGE ITERATOR         1   2           8438<br>\n12  ....... INDEX RANGE SCAN    EQ_MON_RESULT_INSPT_UNQ     1   2   44  +2  8438    0   5403    42MB        75.56   Cpu (1)\ndb file sequential read (33)\n13  ..... TABLE ACCESS BY LOCAL INDEX ROWID     EQUITIES_MONTHLY_RESULTS    1   3                   </p>\n\n<p>Here are the Global stats:</p>\n\n<p>Elapsed\nTime(s)     Cpu\nTime(s)     IO\nWaits(s)    Fetch\nCalls   Buffer\nGets    Read\nReqs    Read\nBytes\n45  0.73    45  1   22980   9740    76MB            </p>\n"},{"tags":["c#",".net","performance","datetime","optimization"],"answer_count":3,"favorite_count":2,"up_vote_count":10,"down_vote_count":0,"view_count":754,"score":10,"question_id":1561791,"title":"Optimizing alternatives to DateTime.Now","body":"<p>A colleague and I are going back and forth on this issue and I'm hoping to get some outside opinions as to whether or not my proposed solution is a good idea.</p>\n\n<p>First, a disclaimer: I realize that the notion of \"optimizing <code>DateTime.Now</code>\" sounds crazy to some of you. I have a couple of pre-emptive defenses:</p>\n\n<ol>\n<li>I sometimes suspect that those people who always say, \"Computers are fast; readability <em>always</em> comes before optimization\" are often speaking from experience developing applications where performance, though it may be <em>important</em>, is not <strong>critical</strong>. I'm talking about needing things to happen as close to instantaneously as possible -- like, within nanoseconds (in certain industries, this <em>does</em> matter -- for instance, real-time high-frequency trading).</li>\n<li>Even with that in mind, the alternative approach I describe below is, in fact, quite readable. It is not a bizarre hack, just a simple method that works reliably and fast.</li>\n<li>We <strong>have</strong> runs tests. <code>DateTime.Now</code> <strong>is</strong> slow (relatively speaking). The method below <strong>is</strong> faster.</li>\n</ol>\n\n<p>Now, onto the question itself.</p>\n\n<p>Basically, from tests, we've found that <code>DateTime.Now</code> takes roughly 25 ticks (around 2.5 microseconds) to run. This is averaged out over thousands to millions of calls, of course. It appears that the first call actually takes a significant amount of time and subsequent calls are much faster. But still, 25 ticks is the average.</p>\n\n<p>However, my colleague and I noticed that <code>DateTime.UtcNow</code> takes substantially less time to run -- on average, a mere 0.03 microseconds.</p>\n\n<p><em>Given that our application will never be running while there is a change in Daylight Savings Time</em>, my suggestion was to create the following class:</p>\n\n<pre><code>public static class FastDateTime {\n    public static TimeSpan LocalUtcOffset { get; private set; }\n\n    public static DateTime Now {\n        get { return DateTime.UtcNow + LocalUtcOffset; }\n    }\n\n    static FastDateTime() {\n        LocalUtcOffset = TimeZone.CurrentTimeZone.GetUtcOffset(DateTime.Now);\n    }\n}\n</code></pre>\n\n<p>In other words, determine the UTC offset for the local timezone <strong>once</strong> -- at startup -- and from that point onward leverage the speed of <code>DateTime.UtcNow</code> to get the current time a lot faster via <code>FastDateTime.Now</code>.</p>\n\n<p>I could see this being a problem if the UTC offset changed during the time the application was running (if, for example, the application was running overnight); but as I stated already, in <em>our</em> case, that will not happen.</p>\n\n<p>My colleague has a different idea about how to do it, which is a bit too involved for me to explain here. Ultimately, <em>as far as I can tell</em>, both of our approaches return an accurate result, mine being slightly faster (~0.07 microseconds vs. ~0.21 microseconds).</p>\n\n<p>What I want to know is:</p>\n\n<ol>\n<li>Am I missing something here? Given the abovementioned fact that the application will only run within the time frame of a single date, is <code>FastDateTime.Now</code> safe?</li>\n<li>Can anyone else perhaps think of an even <em>faster</em> way of getting the current time?</li>\n</ol>\n"},{"tags":["performance","query","tsql","sql-server-2005"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":74,"score":3,"question_id":12854990,"title":"Why would ISNULL improve the performance of a query?","body":"<p>We're currently trying to speedup some queries and we've run into something which I (not a DBA just a .NET developer) cannot explain or comprehent. We're running this query on SQL Server 2005.</p>\n\n<p>We have the following query (made small and simple for the sake of argument);</p>\n\n<pre><code>SELECT    \n    *\nFROM    \n    RandomTable \nWHERE   \n    MoneyColumn &lt;&gt; 0\nGROUP BY\n    SomeColumn\n</code></pre>\n\n<p>This query run in around <strong>three</strong> seconds, then we randomly tried to following to speed it up (shot in the dark really)</p>\n\n<pre><code>SELECT    \n    *\nFROM    \n    RandomTable \nWHERE   \n    isnull(MoneyColumn,0) &lt;&gt; 0\nGROUP BY\n    SomeColumn\n</code></pre>\n\n<p>This reduces the query speed to around <strong>one</strong> second..</p>\n\n<p>This column has no NULL values (yet due to the database design beeing HORRIBLE) it is however NULLABLE...</p>\n\n<p>Is the fact that it's NULLABLE making SQL Server do something to account for this making it slow where the ISNULL isn't beeing mentioned? I simply have no idea why the ISNULL would make it perform faster (and by such a big margin). I'd think SQL would actually have more to do when there is a ISNULL statement in the query.</p>\n\n<p>Can anyone shed some light on this?</p>\n\n<p><strong>EDIT</strong> Execution plans added</p>\n\n<p><strong>With ISNULL</strong></p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-16\"?&gt;\n&lt;ShowPlanXML xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" Version=\"1.0\" Build=\"9.00.5000.00\" xmlns=\"http://schemas.microsoft.com/sqlserver/2004/07/showplan\"&gt;\n  &lt;BatchSequence&gt;\n    &lt;Batch&gt;\n      &lt;Statements&gt;\n        &lt;StmtSimple StatementCompId=\"1\" StatementEstRows=\"9019.76\" StatementId=\"1\" StatementOptmLevel=\"FULL\" StatementSubTreeCost=\"1.48105\" StatementText=\"SELECT    debiteur_id, MIN(Faktuurdatum) AS OldestOpenInvoiceDate, ISNULL(SUM(Totaal_Open),0) AS TotalOpenAmount&amp;#xD;&amp;#xA;FROM    dbo.tbl_Faktuur &amp;#xD;&amp;#xA;WHERE   (Afgehandeld_NeeJa = 0 OR Afgehandeld_NeeJa IS NULL)&amp;#xD;&amp;#xA;AND        (ISNULL(Totaal_Open,0) &amp;lt;&amp;gt; 0) &amp;#xD;&amp;#xA;--AND        (Totaal_Open &amp;lt;&amp;gt; 0) &amp;#xD;&amp;#xA;GROUP BY debiteur_id\" StatementType=\"SELECT\"&gt;\n          &lt;StatementSetOptions ANSI_NULLS=\"false\" ANSI_PADDING=\"false\" ANSI_WARNINGS=\"false\" ARITHABORT=\"true\" CONCAT_NULL_YIELDS_NULL=\"false\" NUMERIC_ROUNDABORT=\"false\" QUOTED_IDENTIFIER=\"false\" /&gt;\n          &lt;QueryPlan DegreeOfParallelism=\"1\" MemoryGrant=\"1520\" CachedPlanSize=\"54\" CompileTime=\"11\" CompileCPU=\"11\" CompileMemory=\"704\"&gt;\n            &lt;RelOp AvgRowSize=\"23\" EstimateCPU=\"0.000901976\" EstimateIO=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimateRows=\"9019.76\" LogicalOp=\"Compute Scalar\" NodeId=\"0\" Parallel=\"false\" PhysicalOp=\"Compute Scalar\" EstimatedTotalSubtreeCost=\"1.48105\"&gt;\n              &lt;OutputList&gt;\n                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                &lt;ColumnReference Column=\"Expr1003\" /&gt;\n                &lt;ColumnReference Column=\"Expr1005\" /&gt;\n              &lt;/OutputList&gt;\n              &lt;ComputeScalar&gt;\n                &lt;DefinedValues&gt;\n                  &lt;DefinedValue&gt;\n                    &lt;ColumnReference Column=\"Expr1005\" /&gt;\n                    &lt;ScalarOperator ScalarString=\"isnull([Expr1004],($0.0000))\"&gt;\n                      &lt;Intrinsic FunctionName=\"isnull\"&gt;\n                        &lt;ScalarOperator&gt;\n                          &lt;Identifier&gt;\n                            &lt;ColumnReference Column=\"Expr1004\" /&gt;\n                          &lt;/Identifier&gt;\n                        &lt;/ScalarOperator&gt;\n                        &lt;ScalarOperator&gt;\n                          &lt;Const ConstValue=\"($0.0000)\" /&gt;\n                        &lt;/ScalarOperator&gt;\n                      &lt;/Intrinsic&gt;\n                    &lt;/ScalarOperator&gt;\n                  &lt;/DefinedValue&gt;\n                &lt;/DefinedValues&gt;\n                &lt;RelOp AvgRowSize=\"23\" EstimateCPU=\"0.291662\" EstimateIO=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimateRows=\"9019.76\" LogicalOp=\"Compute Scalar\" NodeId=\"1\" Parallel=\"false\" PhysicalOp=\"Compute Scalar\" EstimatedTotalSubtreeCost=\"1.48014\"&gt;\n                  &lt;OutputList&gt;\n                    &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                    &lt;ColumnReference Column=\"Expr1003\" /&gt;\n                    &lt;ColumnReference Column=\"Expr1004\" /&gt;\n                  &lt;/OutputList&gt;\n                  &lt;ComputeScalar&gt;\n                    &lt;DefinedValues&gt;\n                      &lt;DefinedValue&gt;\n                        &lt;ColumnReference Column=\"Expr1004\" /&gt;\n                        &lt;ScalarOperator ScalarString=\"CASE WHEN [Expr1013]=(0) THEN NULL ELSE [Expr1014] END\"&gt;\n                          &lt;IF&gt;\n                            &lt;Condition&gt;\n                              &lt;ScalarOperator&gt;\n                                &lt;Compare CompareOp=\"EQ\"&gt;\n                                  &lt;ScalarOperator&gt;\n                                    &lt;Identifier&gt;\n                                      &lt;ColumnReference Column=\"Expr1013\" /&gt;\n                                    &lt;/Identifier&gt;\n                                  &lt;/ScalarOperator&gt;\n                                  &lt;ScalarOperator&gt;\n                                    &lt;Const ConstValue=\"(0)\" /&gt;\n                                  &lt;/ScalarOperator&gt;\n                                &lt;/Compare&gt;\n                              &lt;/ScalarOperator&gt;\n                            &lt;/Condition&gt;\n                            &lt;Then&gt;\n                              &lt;ScalarOperator&gt;\n                                &lt;Const ConstValue=\"NULL\" /&gt;\n                              &lt;/ScalarOperator&gt;\n                            &lt;/Then&gt;\n                            &lt;Else&gt;\n                              &lt;ScalarOperator&gt;\n                                &lt;Identifier&gt;\n                                  &lt;ColumnReference Column=\"Expr1014\" /&gt;\n                                &lt;/Identifier&gt;\n                              &lt;/ScalarOperator&gt;\n                            &lt;/Else&gt;\n                          &lt;/IF&gt;\n                        &lt;/ScalarOperator&gt;\n                      &lt;/DefinedValue&gt;\n                    &lt;/DefinedValues&gt;\n                    &lt;RelOp AvgRowSize=\"23\" EstimateCPU=\"0.291662\" EstimateIO=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimateRows=\"9019.76\" LogicalOp=\"Aggregate\" NodeId=\"2\" Parallel=\"false\" PhysicalOp=\"Hash Match\" EstimatedTotalSubtreeCost=\"1.48014\"&gt;\n                      &lt;OutputList&gt;\n                        &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                        &lt;ColumnReference Column=\"Expr1003\" /&gt;\n                        &lt;ColumnReference Column=\"Expr1013\" /&gt;\n                        &lt;ColumnReference Column=\"Expr1014\" /&gt;\n                      &lt;/OutputList&gt;\n                      &lt;MemoryFractions Input=\"1\" Output=\"1\" /&gt;\n                      &lt;RunTimeInformation&gt;\n                        &lt;RunTimeCountersPerThread Thread=\"0\" ActualRows=\"156794\" ActualEndOfScans=\"1\" ActualExecutions=\"1\" /&gt;\n                      &lt;/RunTimeInformation&gt;\n                      &lt;Hash&gt;\n                        &lt;DefinedValues&gt;\n                          &lt;DefinedValue&gt;\n                            &lt;ColumnReference Column=\"Expr1003\" /&gt;\n                            &lt;ScalarOperator ScalarString=\"MIN([directpay].[dbo].[tbl_Faktuur].[Faktuurdatum])\"&gt;\n                              &lt;Aggregate AggType=\"MIN\" Distinct=\"false\"&gt;\n                                &lt;ScalarOperator&gt;\n                                  &lt;Identifier&gt;\n                                    &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Faktuurdatum\" /&gt;\n                                  &lt;/Identifier&gt;\n                                &lt;/ScalarOperator&gt;\n                              &lt;/Aggregate&gt;\n                            &lt;/ScalarOperator&gt;\n                          &lt;/DefinedValue&gt;\n                          &lt;DefinedValue&gt;\n                            &lt;ColumnReference Column=\"Expr1013\" /&gt;\n                            &lt;ScalarOperator ScalarString=\"COUNT_BIG([directpay].[dbo].[tbl_Faktuur].[Totaal_Open])\"&gt;\n                              &lt;Aggregate AggType=\"COUNT_BIG\" Distinct=\"false\"&gt;\n                                &lt;ScalarOperator&gt;\n                                  &lt;Identifier&gt;\n                                    &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                                  &lt;/Identifier&gt;\n                                &lt;/ScalarOperator&gt;\n                              &lt;/Aggregate&gt;\n                            &lt;/ScalarOperator&gt;\n                          &lt;/DefinedValue&gt;\n                          &lt;DefinedValue&gt;\n                            &lt;ColumnReference Column=\"Expr1014\" /&gt;\n                            &lt;ScalarOperator ScalarString=\"SUM([directpay].[dbo].[tbl_Faktuur].[Totaal_Open])\"&gt;\n                              &lt;Aggregate AggType=\"SUM\" Distinct=\"false\"&gt;\n                                &lt;ScalarOperator&gt;\n                                  &lt;Identifier&gt;\n                                    &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                                  &lt;/Identifier&gt;\n                                &lt;/ScalarOperator&gt;\n                              &lt;/Aggregate&gt;\n                            &lt;/ScalarOperator&gt;\n                          &lt;/DefinedValue&gt;\n                        &lt;/DefinedValues&gt;\n                        &lt;HashKeysBuild&gt;\n                          &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                        &lt;/HashKeysBuild&gt;\n                        &lt;RelOp AvgRowSize=\"23\" EstimateCPU=\"0.255\" EstimateIO=\"0.634196\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimateRows=\"27420\" LogicalOp=\"Index Seek\" NodeId=\"4\" Parallel=\"false\" PhysicalOp=\"Index Seek\" EstimatedTotalSubtreeCost=\"0.889196\"&gt;\n                          &lt;OutputList&gt;\n                            &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                            &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Faktuurdatum\" /&gt;\n                            &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                          &lt;/OutputList&gt;\n                          &lt;RunTimeInformation&gt;\n                            &lt;RunTimeCountersPerThread Thread=\"0\" ActualRows=\"298726\" ActualEndOfScans=\"1\" ActualExecutions=\"1\" /&gt;\n                          &lt;/RunTimeInformation&gt;\n                          &lt;IndexScan Ordered=\"true\" ScanDirection=\"FORWARD\" ForcedIndex=\"false\" NoExpandHint=\"false\"&gt;\n                            &lt;DefinedValues&gt;\n                              &lt;DefinedValue&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                              &lt;/DefinedValue&gt;\n                              &lt;DefinedValue&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Faktuurdatum\" /&gt;\n                              &lt;/DefinedValue&gt;\n                              &lt;DefinedValue&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                              &lt;/DefinedValue&gt;\n                            &lt;/DefinedValues&gt;\n                            &lt;Object Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Index=\"[_dta_index_tbl_Faktuur_5_583009158__K13_K9_K19_K2_5]\" /&gt;\n                            &lt;SeekPredicates&gt;\n                              &lt;SeekPredicate&gt;\n                                &lt;Prefix ScanType=\"EQ\"&gt;\n                                  &lt;RangeColumns&gt;\n                                    &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Afgehandeld_NeeJa\" /&gt;\n                                  &lt;/RangeColumns&gt;\n                                  &lt;RangeExpressions&gt;\n                                    &lt;ScalarOperator ScalarString=\"(0)\"&gt;\n                                      &lt;Const ConstValue=\"(0)\" /&gt;\n                                    &lt;/ScalarOperator&gt;\n                                  &lt;/RangeExpressions&gt;\n                                &lt;/Prefix&gt;\n                              &lt;/SeekPredicate&gt;\n                            &lt;/SeekPredicates&gt;\n                            &lt;Predicate&gt;\n                              &lt;ScalarOperator ScalarString=\"isnull([directpay].[dbo].[tbl_Faktuur].[Totaal_Open],($0.0000))&amp;lt;($0.0000) OR isnull([directpay].[dbo].[tbl_Faktuur].[Totaal_Open],($0.0000))&amp;gt;($0.0000)\"&gt;\n                                &lt;Logical Operation=\"OR\"&gt;\n                                  &lt;ScalarOperator&gt;\n                                    &lt;Compare CompareOp=\"LT\"&gt;\n                                      &lt;ScalarOperator&gt;\n                                        &lt;Intrinsic FunctionName=\"isnull\"&gt;\n                                          &lt;ScalarOperator&gt;\n                                            &lt;Identifier&gt;\n                                              &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                                            &lt;/Identifier&gt;\n                                          &lt;/ScalarOperator&gt;\n                                          &lt;ScalarOperator&gt;\n                                            &lt;Const ConstValue=\"($0.0000)\" /&gt;\n                                          &lt;/ScalarOperator&gt;\n                                        &lt;/Intrinsic&gt;\n                                      &lt;/ScalarOperator&gt;\n                                      &lt;ScalarOperator&gt;\n                                        &lt;Const ConstValue=\"($0.0000)\" /&gt;\n                                      &lt;/ScalarOperator&gt;\n                                    &lt;/Compare&gt;\n                                  &lt;/ScalarOperator&gt;\n                                  &lt;ScalarOperator&gt;\n                                    &lt;Compare CompareOp=\"GT\"&gt;\n                                      &lt;ScalarOperator&gt;\n                                        &lt;Intrinsic FunctionName=\"isnull\"&gt;\n                                          &lt;ScalarOperator&gt;\n                                            &lt;Identifier&gt;\n                                              &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                                            &lt;/Identifier&gt;\n                                          &lt;/ScalarOperator&gt;\n                                          &lt;ScalarOperator&gt;\n                                            &lt;Const ConstValue=\"($0.0000)\" /&gt;\n                                          &lt;/ScalarOperator&gt;\n                                        &lt;/Intrinsic&gt;\n                                      &lt;/ScalarOperator&gt;\n                                      &lt;ScalarOperator&gt;\n                                        &lt;Const ConstValue=\"($0.0000)\" /&gt;\n                                      &lt;/ScalarOperator&gt;\n                                    &lt;/Compare&gt;\n                                  &lt;/ScalarOperator&gt;\n                                &lt;/Logical&gt;\n                              &lt;/ScalarOperator&gt;\n                            &lt;/Predicate&gt;\n                          &lt;/IndexScan&gt;\n                        &lt;/RelOp&gt;\n                      &lt;/Hash&gt;\n                    &lt;/RelOp&gt;\n                  &lt;/ComputeScalar&gt;\n                &lt;/RelOp&gt;\n              &lt;/ComputeScalar&gt;\n            &lt;/RelOp&gt;\n          &lt;/QueryPlan&gt;\n        &lt;/StmtSimple&gt;\n      &lt;/Statements&gt;\n    &lt;/Batch&gt;\n  &lt;/BatchSequence&gt;\n&lt;/ShowPlanXML&gt;\n</code></pre>\n\n<p><strong>Without ISNULL</strong></p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-16\"?&gt;\n&lt;ShowPlanXML xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" Version=\"1.0\" Build=\"9.00.5000.00\" xmlns=\"http://schemas.microsoft.com/sqlserver/2004/07/showplan\"&gt;\n  &lt;BatchSequence&gt;\n    &lt;Batch&gt;\n      &lt;Statements&gt;\n        &lt;StmtSimple StatementCompId=\"1\" StatementEstRows=\"1322.43\" StatementId=\"1\" StatementOptmLevel=\"FULL\" StatementOptmEarlyAbortReason=\"GoodEnoughPlanFound\" StatementSubTreeCost=\"0.274954\" StatementText=\"SELECT    debiteur_id, MIN(Faktuurdatum) AS OldestOpenInvoiceDate, ISNULL(SUM(Totaal_Open),0) AS TotalOpenAmount&amp;#xD;&amp;#xA;FROM    dbo.tbl_Faktuur &amp;#xD;&amp;#xA;WHERE   (Afgehandeld_NeeJa = 0 OR Afgehandeld_NeeJa IS NULL)&amp;#xD;&amp;#xA;--AND        (ISNULL(Totaal_Open,0) &amp;lt;&amp;gt; 0) &amp;#xD;&amp;#xA;AND        (Totaal_Open &amp;lt;&amp;gt; 0) &amp;#xD;&amp;#xA;GROUP BY debiteur_id\" StatementType=\"SELECT\"&gt;\n          &lt;StatementSetOptions ANSI_NULLS=\"false\" ANSI_PADDING=\"false\" ANSI_WARNINGS=\"false\" ARITHABORT=\"true\" CONCAT_NULL_YIELDS_NULL=\"false\" NUMERIC_ROUNDABORT=\"false\" QUOTED_IDENTIFIER=\"false\" /&gt;\n          &lt;QueryPlan CachedPlanSize=\"47\" CompileTime=\"9\" CompileCPU=\"9\" CompileMemory=\"528\"&gt;\n            &lt;RelOp AvgRowSize=\"23\" EstimateCPU=\"0.000132243\" EstimateIO=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimateRows=\"1322.43\" LogicalOp=\"Compute Scalar\" NodeId=\"0\" Parallel=\"false\" PhysicalOp=\"Compute Scalar\" EstimatedTotalSubtreeCost=\"0.274954\"&gt;\n              &lt;OutputList&gt;\n                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                &lt;ColumnReference Column=\"Expr1003\" /&gt;\n                &lt;ColumnReference Column=\"Expr1005\" /&gt;\n              &lt;/OutputList&gt;\n              &lt;ComputeScalar&gt;\n                &lt;DefinedValues&gt;\n                  &lt;DefinedValue&gt;\n                    &lt;ColumnReference Column=\"Expr1005\" /&gt;\n                    &lt;ScalarOperator ScalarString=\"isnull([Expr1004],($0.0000))\"&gt;\n                      &lt;Intrinsic FunctionName=\"isnull\"&gt;\n                        &lt;ScalarOperator&gt;\n                          &lt;Identifier&gt;\n                            &lt;ColumnReference Column=\"Expr1004\" /&gt;\n                          &lt;/Identifier&gt;\n                        &lt;/ScalarOperator&gt;\n                        &lt;ScalarOperator&gt;\n                          &lt;Const ConstValue=\"($0.0000)\" /&gt;\n                        &lt;/ScalarOperator&gt;\n                      &lt;/Intrinsic&gt;\n                    &lt;/ScalarOperator&gt;\n                  &lt;/DefinedValue&gt;\n                &lt;/DefinedValues&gt;\n                &lt;RelOp AvgRowSize=\"23\" EstimateCPU=\"0.167304\" EstimateIO=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimateRows=\"1322.43\" LogicalOp=\"Aggregate\" NodeId=\"1\" Parallel=\"false\" PhysicalOp=\"Hash Match\" EstimatedTotalSubtreeCost=\"0.274822\"&gt;\n                  &lt;OutputList&gt;\n                    &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                    &lt;ColumnReference Column=\"Expr1003\" /&gt;\n                    &lt;ColumnReference Column=\"Expr1004\" /&gt;\n                  &lt;/OutputList&gt;\n                  &lt;MemoryFractions Input=\"0\" Output=\"0\" /&gt;\n                  &lt;Hash&gt;\n                    &lt;DefinedValues&gt;\n                      &lt;DefinedValue&gt;\n                        &lt;ColumnReference Column=\"Expr1003\" /&gt;\n                        &lt;ScalarOperator ScalarString=\"MIN([directpay].[dbo].[tbl_Faktuur].[Faktuurdatum])\"&gt;\n                          &lt;Aggregate AggType=\"MIN\" Distinct=\"false\"&gt;\n                            &lt;ScalarOperator&gt;\n                              &lt;Identifier&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Faktuurdatum\" /&gt;\n                              &lt;/Identifier&gt;\n                            &lt;/ScalarOperator&gt;\n                          &lt;/Aggregate&gt;\n                        &lt;/ScalarOperator&gt;\n                      &lt;/DefinedValue&gt;\n                      &lt;DefinedValue&gt;\n                        &lt;ColumnReference Column=\"Expr1004\" /&gt;\n                        &lt;ScalarOperator ScalarString=\"SUM([directpay].[dbo].[tbl_Faktuur].[Totaal_Open])\"&gt;\n                          &lt;Aggregate AggType=\"SUM\" Distinct=\"false\"&gt;\n                            &lt;ScalarOperator&gt;\n                              &lt;Identifier&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                              &lt;/Identifier&gt;\n                            &lt;/ScalarOperator&gt;\n                          &lt;/Aggregate&gt;\n                        &lt;/ScalarOperator&gt;\n                      &lt;/DefinedValue&gt;\n                    &lt;/DefinedValues&gt;\n                    &lt;HashKeysBuild&gt;\n                      &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                    &lt;/HashKeysBuild&gt;\n                    &lt;RelOp AvgRowSize=\"23\" EstimateCPU=\"0.030319\" EstimateIO=\"0.0771991\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimateRows=\"27420\" LogicalOp=\"Index Seek\" NodeId=\"2\" Parallel=\"false\" PhysicalOp=\"Index Seek\" EstimatedTotalSubtreeCost=\"0.107518\"&gt;\n                      &lt;OutputList&gt;\n                        &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                        &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Faktuurdatum\" /&gt;\n                        &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                      &lt;/OutputList&gt;\n                      &lt;IndexScan Ordered=\"true\" ScanDirection=\"FORWARD\" ForcedIndex=\"false\" NoExpandHint=\"false\"&gt;\n                        &lt;DefinedValues&gt;\n                          &lt;DefinedValue&gt;\n                            &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Debiteur_ID\" /&gt;\n                          &lt;/DefinedValue&gt;\n                          &lt;DefinedValue&gt;\n                            &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Faktuurdatum\" /&gt;\n                          &lt;/DefinedValue&gt;\n                          &lt;DefinedValue&gt;\n                            &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                          &lt;/DefinedValue&gt;\n                        &lt;/DefinedValues&gt;\n                        &lt;Object Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Index=\"[_dta_index_tbl_Faktuur_5_583009158__K13_K9_K19_K2_5]\" /&gt;\n                        &lt;SeekPredicates&gt;\n                          &lt;SeekPredicate&gt;\n                            &lt;Prefix ScanType=\"EQ\"&gt;\n                              &lt;RangeColumns&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Afgehandeld_NeeJa\" /&gt;\n                              &lt;/RangeColumns&gt;\n                              &lt;RangeExpressions&gt;\n                                &lt;ScalarOperator ScalarString=\"(0)\"&gt;\n                                  &lt;Const ConstValue=\"(0)\" /&gt;\n                                &lt;/ScalarOperator&gt;\n                              &lt;/RangeExpressions&gt;\n                            &lt;/Prefix&gt;\n                            &lt;EndRange ScanType=\"LT\"&gt;\n                              &lt;RangeColumns&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                              &lt;/RangeColumns&gt;\n                              &lt;RangeExpressions&gt;\n                                &lt;ScalarOperator ScalarString=\"($0.0000)\"&gt;\n                                  &lt;Const ConstValue=\"($0.0000)\" /&gt;\n                                &lt;/ScalarOperator&gt;\n                              &lt;/RangeExpressions&gt;\n                            &lt;/EndRange&gt;\n                          &lt;/SeekPredicate&gt;\n                          &lt;SeekPredicate&gt;\n                            &lt;Prefix ScanType=\"EQ\"&gt;\n                              &lt;RangeColumns&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Afgehandeld_NeeJa\" /&gt;\n                              &lt;/RangeColumns&gt;\n                              &lt;RangeExpressions&gt;\n                                &lt;ScalarOperator ScalarString=\"(0)\"&gt;\n                                  &lt;Const ConstValue=\"(0)\" /&gt;\n                                &lt;/ScalarOperator&gt;\n                              &lt;/RangeExpressions&gt;\n                            &lt;/Prefix&gt;\n                            &lt;StartRange ScanType=\"GT\"&gt;\n                              &lt;RangeColumns&gt;\n                                &lt;ColumnReference Database=\"[directpay]\" Schema=\"[dbo]\" Table=\"[tbl_Faktuur]\" Column=\"Totaal_Open\" /&gt;\n                              &lt;/RangeColumns&gt;\n                              &lt;RangeExpressions&gt;\n                                &lt;ScalarOperator ScalarString=\"($0.0000)\"&gt;\n                                  &lt;Const ConstValue=\"($0.0000)\" /&gt;\n                                &lt;/ScalarOperator&gt;\n                              &lt;/RangeExpressions&gt;\n                            &lt;/StartRange&gt;\n                          &lt;/SeekPredicate&gt;\n                        &lt;/SeekPredicates&gt;\n                      &lt;/IndexScan&gt;\n                    &lt;/RelOp&gt;\n                  &lt;/Hash&gt;\n                &lt;/RelOp&gt;\n              &lt;/ComputeScalar&gt;\n            &lt;/RelOp&gt;\n          &lt;/QueryPlan&gt;\n        &lt;/StmtSimple&gt;\n      &lt;/Statements&gt;\n    &lt;/Batch&gt;\n  &lt;/BatchSequence&gt;\n&lt;/ShowPlanXML&gt;\n</code></pre>\n"},{"tags":["performance","oracle","oracle11gr2","impdp"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":25,"score":0,"question_id":13078530,"title":"How can I identify and fix bottlenecks in an Oracle11gR2 import","body":"<p>I've recently been tasked with moving our Oracle 11g database from a linux CentOS desktop to a windows 2008 server. Given I've not done much behind the scenes stuff with oracle before I'm a bit out of my depth.</p>\n\n<p>I did a full expdp from the source server and imported it to the target server (i built tablespaces before the load but nothing else).</p>\n\n<p>Everything went well enough (a few errors that were related to having a different global_name which, from documentation, appeared ignorable.</p>\n\n<p>The problem I am having is that the new server appears to take longer to run queries compared to the old server. This is in spite of more cores and more RAM. I think the problem is due to I/O limitations as the old server ran on an SSD.</p>\n\n<p>I get the same 'explain plans' on queries yet getting through the blocks on v$sql_longops just seems to take longer... Can anyone provide details on what steps to take to check and compare output so I can try to ascertain the underlying issue?</p>\n"},{"tags":["performance","oracle","hibernate","index","order"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":453,"score":2,"question_id":8729760,"title":"Query from Hibernate App does not use DB indexes","body":"<p>I try to solve a performance issue of my application. The query hibernate generates, is of the form:</p>\n\n<pre><code>select * \nfrom ( \n    select this.a, this.b, this.state, this.id \n    from view_user this \n    where this.state=:1 order by this.a asc, this.b\n) \nwhere rownum &lt;= :2\n</code></pre>\n\n<p>where</p>\n\n<ul>\n<li>id is the primary key</li>\n<li>there is a combined, unique index on (a, b, id).</li>\n<li>view_user has ~ 2 million entries</li>\n<li>view_user performs some further joins to other tables</li>\n</ul>\n\n<p><strong>Issue</strong></p>\n\n<p>The above query performs\n - fast from SQLDeveloper\n - fast from a small Java app with hibernate\n - extremely slow (>100x slower) from the application with hibernate\n - values for the bind variables are 2 respectively 30 (rownum origins from paging)\n - the hibernate query is \"of the form\" above. There are actually about 20 columns in the view.</p>\n\n<p><strong>Current state of analysis</strong></p>\n\n<ul>\n<li>query plan shows that index is used when query comes from SQlDeveloper or \"small java app\".</li>\n<li>query plan shows that full table scans are performed if query comes from hibernate app</li>\n<li>DB tracing shows only two differences: NLS settings (from SQLDeveloper) and slightly different formatting (whitespaces). Everything else seems to be the same...</li>\n</ul>\n\n<p><strong>Versions</strong></p>\n\n<ul>\n<li>hibernate: 2.1.8</li>\n<li>jdbc driver: used ojdbc14, 5 and 6. Makes no difference</li>\n<li>Oracle: 10.2 and 11. Makes no difference</li>\n</ul>\n\n<p>=> I'm glad about every hint somebody might have concerning this issue. What troubles me is the fact that the DB tracing did not show any differences... Yes, it looks like it is something about hibernate. But what? How to detect?</p>\n\n<hr>\n\n<p>For the sake of completeness, here the hibernate query (from the log):</p>\n\n<pre><code>Select * from ( \n    select this.USER_ID as USER_ID0_, this.CLIENT_ID as CLIENT_ID0_, \n    this.USER_NAME as USER_NAME0_, this.USER_FIRST_NAME as USER_FIR5_0_, this.USER_REMARKS as \n    USER_REM6_0_, this.USER_LOGIN_ID as USER_LOG7_0_, this.USER_TITLE as USER_TITLE0_, \n    this.user_language_code as user_lan9_0_, this.USER_SEX as USER_SEX0_, \n    this.USER_BIRTH_DATE as USER_BI11_0_, this.USER_TELEPHONE as USER_TE12_0_, \n    this.USER_TELEFAX as USER_TE13_0_, this.USER_MOBILE as USER_MO14_0_, \n    this.USER_EMAIL as USER_EMAIL0_, this.USER_ADDRESSLINE1 as USER_AD16_0_, \n    this.USER_ADDRESSLINE2 as USER_AD17_0_, this.USER_POSTALCODE as USER_PO18_0_, \n    this.USER_CITY as USER_CITY0_, this.USER_COUNTRY_CD as USER_CO20_0_, \n    this.USER_COUNTRY_NAME as USER_CO21_0_, this.USER_STATE_ID as USER_ST24_0_, \n    this.USER_STATE as USER_STATE0_, this.USER_TEMP_COLL_ID as USER_TE26_0_, \n    this.USER_TEMP_COLL_NAME as USER_TE27_0_, this.UNIT_ID as UNIT_ID0_, \n    this.CLIENT_NAME as CLIENT_38_0_, this.PROFILE_EXTID as PROFILE39_0_\n    from VIEW_USER this\n    where this.USER_STATE_ID=:1 order by this.USER_NAME asc, this.USER_FIRST_NAME asc\n) \nwhere rownum &lt;= :2\n</code></pre>\n\n<p>Unique index is over user_name, user_first_name, user_id.</p>\n"},{"tags":["mysql","performance","query"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":43,"score":0,"question_id":13082066,"title":"to improve performance on query","body":"<p>I hava nest query:</p>\n\n<pre><code>SELECT PIXEL_X as 'X_Coord', PIXEL_Y as 'Y_Coord', \nCONVERTWATTS2DBM_udf(SUM(L2_VALUE)/SUM(L3_VALUE)) as 'Pixel_Value' \n   FROM table  \n   WHERE  \n      ('GSM 850/900' like CONCAT('%',FILTER2,'/%') OR \n       'GSM 850/900' like CONCAT('%/',FILTER2,'%') ) \nGROUP BY X_Coord, Y_Coord;\n</code></pre>\n\n<p>but takes a long time, could you help me to improve their performance?</p>\n\n<p>Thanks</p>\n"},{"tags":["arrays","performance","r"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":30,"score":0,"question_id":13081611,"title":"Most efficient way to fill out a matrix of elements given by function f(i,j) in R?","body":"<p>I have a function of two scalar values <code>x_i</code>, <code>y_j</code>.  I have a vector of n <code>x_i</code> values, X and n <code>y_j</code> values, e.g.</p>\n\n<pre><code>myfunction &lt;- function(x,y) min(x,y)\nX &lt;- 1:3\nY &lt;- 2:4\n</code></pre>\n\n<p>I want to fill out the $n$ by $n$ matrix whose elements <code>(i,j)</code> are given by <code>myfunction(x_i, y_j)</code>.  There's a lot of ways to do this in <code>R</code>, and I'm curious about their relative performance.</p>\n\n<p>For instance, this seems like a task for <code>outer</code>, but it seems to get confused whether it is passing a vector or scalar to <code>myfunction</code>.  First consider: </p>\n\n<p><code>r\n    outer(X, Y, paste)\n</code></p>\n\n<p>gives me each of the pairs</p>\n\n<pre><code>     [,1]  [,2]  [,3] \n[1,] \"1 2\" \"1 3\" \"1 4\"\n[2,] \"2 2\" \"2 3\" \"2 4\"\n[3,] \"3 2\" \"3 3\" \"3 4\"\n</code></pre>\n\n<p>Looks good.  But </p>\n\n<pre><code>outer(X, Y, myfunction)\n</code></pre>\n\n<p>throws the error:</p>\n\n<pre><code>Error: dims [product 9] do not match the length of object [1]\n</code></pre>\n\n<p>Meanwhile other possible functions seem to behave as I expected with scalars, such as:</p>\n\n<pre><code>myfunction &lt;- function(x,y) exp((x-y)^2)\n</code></pre>\n\n<p>which works fine</p>\n\n<pre><code>outer(X, Y, myfunction)\n\n\n         [,1]      [,2]        [,3]\n[1,] 2.718282 54.598150 8103.083928\n[2,] 1.000000  2.718282   54.598150\n[3,] 2.718282  1.000000    2.718282\n</code></pre>\n\n<p>In a few quick numerical experiments, it seems this is slightly faster than <code>expand.grid</code>, and the function call more compact, but I don't seem to understand why some functions appear to work as I anticipate and others do not.  </p>\n\n<p>The classic <code>expand.grid</code> solution also requires the function to work with vector arguments, which means a very different thing for my example with <code>min</code>; a different version of the same problem.  Is there a way to enforce the fact that the arguments to my function must be scalars rather than vectors?</p>\n"},{"tags":["mysql","performance","join"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":34,"score":0,"question_id":13081434,"title":"Multiple Joins really slow down MySQL query","body":"<p>This is my first time doing such a large query and so many joins. When I do this query it is super slow. I am not sure how to use Foreign Keys and if it would even help... but if anyone knows a way I could speed this up, it would be very helpful. Here is my query:     </p>\n\n<pre><code>SELECT FLOOR(AVG(ra.double)) AS price, hi.hname,  hi.hotel_id as hid, hi.hstars, hi.haddress, im.image_file, hd.short_desc, pr.promo_name, pr.discount, pr.discount_type, pr.book_start, pr.book_end \n    FROM hotel_info AS hi \n    JOIN images AS im ON im.foreign_id = hi.hotel_id \n    LEFT JOIN hotel_desc AS hd ON hd.hotel_id = hi.hotel_id \n    RIGHT JOIN rates AS ra ON ra.hotel_id = hi.hotel_id \n    RIGHT JOIN promotions AS pr ON pr.hotel_id = ra.hotel_id \n    WHERE ra.booking_date  BETWEEN '2012-01-01' AND '2012-01-06' \n    AND ra.double != '0.00' \n    AND hi.status = '1' \n    AND hi.destination_id = '$destination' \n    GROUP BY hi.hotel_id\n</code></pre>\n"},{"tags":["python","algorithm","performance","recommendation-engine"],"answer_count":3,"favorite_count":5,"up_vote_count":4,"down_vote_count":0,"view_count":831,"score":4,"question_id":2136941,"title":"efficient library to recommend product based on user history","body":"<p>I have a database of which products every user has viewed and I want to recommend a product based on what similar users have viewed. Is there a Python library that can achieve this? I don't need Netflix quality results, just products that are more likely than not of interest. Any ideas?</p>\n"},{"tags":["java","performance","opengl","png","jogl"],"answer_count":5,"favorite_count":5,"up_vote_count":5,"down_vote_count":0,"view_count":5095,"score":5,"question_id":1927419,"title":"Loading PNGs into OpenGL performance issues - Java & JOGL much slower than C# & Tao.OpenGL","body":"<p>I am noticing a large performance difference between Java &amp; JOGL and C# &amp; Tao.OpenGL when both loading PNGs from storage into memory, and when loading that BufferedImage (java) or Bitmap (C# - both are PNGs on hard drive) 'into' OpenGL.</p>\n\n<p>This difference is quite large, so I assumed I was doing something wrong, however after quite a lot of searching and trying different loading techniques I've been unable to reduce this difference.</p>\n\n<p>With Java I get an image loaded in 248ms and loaded into OpenGL in 728ms\nThe same on C# takes 54ms to load the image, and 34ms to load/create texture.</p>\n\n<p>The image in question above is a PNG containing transparency, sized 7200x255, used for a 2D animated sprite. I realise the size is really quite ridiculous and am considering cutting up the sprite, however the large difference is still there (and confusing).</p>\n\n<p>On the Java side the code looks like this:</p>\n\n<pre><code>BufferedImage image = ImageIO.read(new File(fileName));\ntexture = TextureIO.newTexture(image, false);\ntexture.setTexParameteri(GL.GL_TEXTURE_MIN_FILTER, GL.GL_LINEAR);\ntexture.setTexParameteri(GL.GL_TEXTURE_MAG_FILTER, GL.GL_LINEAR);\n</code></pre>\n\n<p>The C# code uses:</p>\n\n<pre><code>Bitmap t = new Bitmap(fileName);\n\nt.RotateFlip(RotateFlipType.RotateNoneFlipY);\nRectangle r = new Rectangle(0, 0, t.Width, t.Height);\n\nBitmapData bd = t.LockBits(r, ImageLockMode.ReadOnly, PixelFormat.Format32bppArgb);\n\nGl.glBindTexture(Gl.GL_TEXTURE_2D, tID);\nGl.glTexImage2D(Gl.GL_TEXTURE_2D, 0, Gl.GL_RGBA, t.Width, t.Height, 0, Gl.GL_BGRA, Gl.GL_UNSIGNED_BYTE, bd.Scan0);\nGl.glTexParameteri(Gl.GL_TEXTURE_2D, Gl.GL_TEXTURE_MIN_FILTER, Gl.GL_LINEAR);\nGl.glTexParameteri(Gl.GL_TEXTURE_2D, Gl.GL_TEXTURE_MAG_FILTER, Gl.GL_LINEAR);\n\nt.UnlockBits(bd);\nt.Dispose();\n</code></pre>\n\n<p>After quite a lot of testing I can only come to the conclusion that Java/JOGL is just slower here - PNG reading might not be as quick, or that I'm still doing something wrong.</p>\n\n<p>Thanks.</p>\n\n<p>Edit2:</p>\n\n<p>I have found that creating a new BufferedImage with format TYPE_INT_ARGB_PRE decreases OpenGL texture load time by almost half - this includes having to create the new BufferedImage, getting the Graphics2D from it and then rendering the previously loaded image to it.</p>\n\n<p>Edit3: Benchmark results for 5 variations.\nI wrote a small benchmarking tool, the following results come from loading a set of 33 pngs, most are very wide, 5 times.</p>\n\n<pre><code>testStart: ImageIO.read(file) -&gt; TextureIO.newTexture(image)  \nresult: avg = 10250ms, total = 51251  \ntestStart: ImageIO.read(bis) -&gt; TextureIO.newTexture(image)  \nresult: avg = 10029ms, total = 50147  \ntestStart: ImageIO.read(file) -&gt; TextureIO.newTexture(argbImage)  \nresult: avg = 5343ms, total = 26717  \ntestStart: ImageIO.read(bis) -&gt; TextureIO.newTexture(argbImage)  \nresult: avg = 5534ms, total = 27673  \ntestStart: TextureIO.newTexture(file)  \nresult: avg = 10395ms, total = 51979\n</code></pre>\n\n<p>ImageIO.read(bis) refers to the technique described in James Branigan's answer below.\nargbImage refers to the technique described in my previous edit:</p>\n\n<pre><code>img = ImageIO.read(file);\nargbImg = new BufferedImage(img.getWidth(), img.getHeight(), TYPE_INT_ARGB_PRE);\ng = argbImg.createGraphics();\ng.drawImage(img, 0, 0, null);\ntexture = TextureIO.newTexture(argbImg, false);\n</code></pre>\n\n<p>Any more methods of loading (either images from file, or images to OpenGL) would be appreciated, I will update these benchmarks.</p>\n"},{"tags":["c++","c","performance","memory-management","hardware"],"answer_count":2,"favorite_count":2,"up_vote_count":5,"down_vote_count":0,"view_count":308,"score":5,"question_id":11401717,"title":"Why do integers process faster than bytes on NDS?","body":"<p>i've noticed that my nds application works a little faster when I replace all the instances of bytes with integers. all the examples online put u8/u16 instances whenever possible. is there a specific reason as to why this is the case?</p>\n"},{"tags":["xcode","performance","osx","instruments","performance-counters"],"answer_count":1,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":59,"score":6,"question_id":13075113,"title":"Is there anyway to read performance counters on OS X Mountain Lion?","body":"<p>Shark, Apple's profiler which let you configure custom performance counters, is no longer supported in OSX Mountain Lion since it can't run a 32-bit kernel. Instruments.app, Apple's replacement for Shark, doesn't seem to support reading performance counters such as L1 cache hits/misses**. Is there anyway to actually setup and read performance counters on OS X? Even if there is no application, is there some user-land API to do this?</p>\n\n<p>**Instruments.app does seem to have an interface for performance counters, but on my Retina MacBook Pro, the PM Events window lists no events, and indicates \"Device: Unknown.\" Are there any other alternatives to Instruments?</p>\n"},{"tags":["android","json","performance","gridview","adapter"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":72,"score":0,"question_id":13080143,"title":"Make application load faster with ViewHolder class","body":"<p>I've made gridview application that get path of image from json. it work find but when it load it's so slow. and i try to find solution and they recommended me to use ViewHolder but I don't know how to use in my gridview application.</p>\n\n<p>How can I use ViewHolder in my application? how I speed up my application?</p>\n\n<p>This code work find but it load so slow.</p>\n\n<pre><code>public class MainActivity extends Activity {\n\nString strUrl = \"http://192.168.10.104/adchara1/\";\nGridView gridView;\n\n@Override\npublic void onCreate(Bundle savedInstanceState) {\n    super.onCreate(savedInstanceState);\n    setContentView(R.layout.activity_main);\n\n    // Creating a new non-ui thread task to download json data\n    DownloadTask downloadTask = new DownloadTask();\n\n    // Starting the download process\n    downloadTask.execute(strUrl);\n\n    gridView = (GridView) findViewById(R.id.lv_countries);        \n    gridView.setOnItemClickListener(new OnItemClickListener() {\n\n        public void onItemClick(AdapterView&lt;?&gt; parent, View v, int positon,\n                long id) {\n\n             HashMap&lt;String, Object&gt; hm = (HashMap&lt;String, Object&gt;) gridView.getAdapter().getItem(positon);\n             String imgPath = (String) hm.get(\"photo\"); //get downloaded image path\n             Intent i = new Intent(MainActivity.this, DisplayActivity.class); //start new Intent to another Activity.\n             i.putExtra(\"ClickedImagePath\", imgPath ); //put image link in intent.\n             startActivity(i);\n        }\n    });\n}\n\n/** A method to download json data from url */\nprivate String downloadUrl(String strUrl) throws IOException{\n  String data = \"\";\n  InputStream iStream = null;\n\n    HttpClient httpClient = new DefaultHttpClient();\n    HttpPost httpPost = new HttpPost(strUrl);\n    ArrayList&lt;NameValuePair&gt; param = new ArrayList&lt;NameValuePair&gt;();\n    try {\n        httpPost.setEntity(new UrlEncodedFormEntity(param));\n        HttpResponse httpResponse = httpClient.execute(httpPost);\n        HttpEntity httpEntity = httpResponse.getEntity();\n        iStream = httpEntity.getContent();\n    } catch (Exception e) {\n        Log.e(\"log_tag\", \"Error in http connection \" + e.toString());\n    }\n    try {\n        BufferedReader br = new BufferedReader(new InputStreamReader(iStream));\n        StringBuilder sb = new StringBuilder();\n        String line = \"\";\n        while((line = br.readLine()) != null){\n            sb.append(line + \"\\n\");\n        }\n        iStream.close();\n        data = sb.toString();\n    } catch (Exception e) {\n        Log.e(\"log_tag\", \"Error converting result \" + e.toString());\n    }\n    return data;\n}\n\n/** AsyncTask to download json data */\nprivate class DownloadTask extends AsyncTask&lt;String, Integer, String&gt;{\n    String data = null;\n    @Override\n    protected String doInBackground(String... url) {\n        try{\n            data = downloadUrl(url[0]);\n        }catch(Exception e){\n            Log.d(\"Background Task\",e.toString());\n        }\n        return data;\n    }\n\n    @Override\n    protected void onPostExecute(String result) {\n\n        // The parsing of the xml data is done in a non-ui thread\n        GridViewLoaderTask gridViewLoaderTask = new GridViewLoaderTask();\n\n        // Start parsing xml data\n        gridViewLoaderTask.execute(result);\n    }\n}\n\n/** AsyncTask to parse json data and load ListView */\nprivate class GridViewLoaderTask extends AsyncTask&lt;String, Void, SimpleAdapter&gt;{\n\n    JSONObject jObject;\n    // Doing the parsing of xml data in a non-ui thread\n    @Override\n    protected SimpleAdapter doInBackground(String... strJson) {\n        try{\n            jObject = new JSONObject(strJson[0]);\n            CountryJSONParser countryJsonParser = new CountryJSONParser();\n            countryJsonParser.parse(jObject);\n        }catch(Exception e){\n            Log.d(\"JSON Exception1\",e.toString());\n        }\n\n        // Instantiating json parser class\n        CountryJSONParser countryJsonParser = new CountryJSONParser();\n\n        // A list object to store the parsed countries list\n        List&lt;HashMap&lt;String, Object&gt;&gt; countries = null;\n\n        try{\n            // Getting the parsed data as a List construct\n            countries = countryJsonParser.parse(jObject);\n        }catch(Exception e){\n            Log.d(\"Exception\",e.toString());\n        }\n\n        // Keys used in Hashmap\n        String[] from = { \"frame\",\"photo\"};\n\n        // Ids of views in listview_layout\n        int[] to = { R.id.iv_frame,R.id.iv_photo};\n\n        // Instantiating an adapter to store each items\n        // R.layout.listview_layout defines the layout of each item\n        SimpleAdapter adapter = new SimpleAdapter(getBaseContext(), countries, R.layout.lv_layout, from, to);\n        return adapter;\n    }\n\n    /** Invoked by the Android on \"doInBackground\" is executed */\n    @Override\n    protected void onPostExecute(SimpleAdapter adapter) {\n\n        // Setting adapter for the listview\n        gridView.setAdapter(adapter);\n\n        for(int i=0;i&lt;adapter.getCount();i++){\n            HashMap&lt;String, Object&gt; hm = (HashMap&lt;String, Object&gt;) adapter.getItem(i);\n            String frameUrl = (String) hm.get(\"frame_path\");\n            String imgUrl = (String) hm.get(\"photo_path\");\n            ImageLoaderTask imageLoaderTask = new ImageLoaderTask();\n\n            HashMap&lt;String, Object&gt; hmDownload = new HashMap&lt;String, Object&gt;();\n            hm.put(\"frame_path\", frameUrl);\n            hm.put(\"photo_path\",imgUrl);\n            hm.put(\"position\", i);\n\n            // Starting ImageLoaderTask to download and populate image in the listview\n            imageLoaderTask.execute(hm);\n        }\n    }\n} \n\n    /** AsyncTask to download and load an image in ListView */\n    private class ImageLoaderTask extends AsyncTask&lt;HashMap&lt;String, Object&gt;, Void, HashMap&lt;String, Object&gt;&gt;{\n\n        @Override\n        protected HashMap&lt;String, Object&gt; doInBackground(HashMap&lt;String, Object&gt;... hm) {\n\n            InputStream iStream = null;\n            String imgUrl;\n            String frameUrl;\n            imgUrl = (String) hm[0].get(\"photo_path\");\n            frameUrl = (String) hm[0].get(\"frame_path\");\n            int position = (Integer) hm[0].get(\"position\");\n\n            URL url;\n            URL urlFrame;\n            try {\n                url = new URL(imgUrl);\n                urlFrame = new URL(frameUrl);\n                // Creating an http connection to communicate with url\n                HttpURLConnection urlConnection = (HttpURLConnection) url\n                        .openConnection();\n\n                // Connecting to url\n                urlConnection.connect();\n\n                // Reading data from url\n                iStream = urlConnection.getInputStream();\n\n                // Getting Caching directory\n                File cacheDirectory = getBaseContext().getCacheDir();\n\n                // Temporary file to store the downloaded image\n                File tmpFile = new File(cacheDirectory.getPath() + \"/wpta_\"+ position + \".png\");\n\n                // The FileOutputStream to the temporary file\n                FileOutputStream fOutStream = new FileOutputStream(tmpFile);\n\n                // Creating a bitmap from the downloaded inputstream\n                Bitmap b = BitmapFactory.decodeStream(iStream);\n\n                // Writing the bitmap to the temporary file as png file\n                b.compress(Bitmap.CompressFormat.PNG, 100, fOutStream);\n\n                // Flush the FileOutputStream\n                fOutStream.flush();\n\n                // Close the FileOutputStream\n                fOutStream.close();\n\n                // Create a hashmap object to store image path and its position in the listview\n                HashMap&lt;String, Object&gt; hmBitmap = new HashMap&lt;String, Object&gt;();\n\n                // Storing the path to the temporary image file\n                hmBitmap.put(\"photo\", tmpFile.getPath());\n                hmBitmap.put(\"frame\", tmpFile.getPath());\n\n                // Storing the position of the image in the listview\n                hmBitmap.put(\"position\", position);\n\n                // Returning the HashMap object containing the image path and position\n                return hmBitmap;\n            } catch (MalformedURLException e) {\n                return null;\n            } catch (FileNotFoundException e) {\n                return null;\n            } catch (IOException e) {\n                return null;\n            }\n        }\n\n    @Override\n    protected void onPostExecute(HashMap&lt;String, Object&gt; result) {\n        // Getting the path to the downloaded image\n        String path = (String) result.get(\"photo\");\n        String framePath = (String) result.get(\"frame\");\n        // Getting the position of the downloaded image\n        int position = (Integer) result.get(\"position\");\n\n        // Getting adapter of the listview\n        SimpleAdapter adapter = (SimpleAdapter ) gridView.getAdapter();\n\n        // Getting the hashmap object at the specified position of the listview\n        HashMap&lt;String, Object&gt; hm = (HashMap&lt;String, Object&gt;) adapter.getItem(position);\n\n        // Overwriting the existing path in the adapter\n        hm.put(\"photo\",path);\n        hm.put(\"frame\", framePath);\n        // Noticing listview about the dataset changes\n        adapter.notifyDataSetChanged();\n    }\n}\n\n\n\n@Override\npublic boolean onCreateOptionsMenu(Menu menu) {\n    getMenuInflater().inflate(R.menu.activity_main, menu);\n    return true;\n}\n</code></pre>\n\n<p>}</p>\n"},{"tags":["c","performance","algorithm","opencl","measurement"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":389,"score":2,"question_id":10155579,"title":"C vs OpenCL, how to compare results of time measurement?","body":"<p>So, in the other post I questioned about C time measurement. Now, I wanna know how to compare the result of the C \"function\" vs the OpenCL \"function\"</p>\n\n<p>This is the code of the host OpenCL and C</p>\n\n<pre><code>#define PROGRAM_FILE \"sum.cl\"\n#define KERNEL_FUNC \"float_sum\"\n#define ARRAY_SIZE 1000000\n\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\n#include &lt;CL/cl.h&gt;\n\nint main()\n{\n    /* OpenCL Data structures */\n\n    cl_platform_id platform;\n    cl_device_id device;\n    cl_context context;\n    cl_program program;\n    cl_kernel kernel;    \n    cl_command_queue queue;\n    cl_mem vec_buffer, result_buffer;\n\n    cl_event prof_event;;\n\n    /* ********************* */\n\n    /* C Data Structures / Data types */\n    FILE *program_handle; //Kernel file handle\n    char *program_buffer; //Kernel buffer\n\n    float *vec, *non_parallel;\n    float result[ARRAY_SIZE];\n\n    size_t program_size; //Kernel file size\n\n    cl_ulong time_start, time_end, total_time;\n\n    int i;\n    /* ****************************** */\n\n    /* Errors */\n    cl_int err;\n    /* ****** */\n\n    non_parallel = (float*)malloc(ARRAY_SIZE * sizeof(float));\n    vec          = (float*)malloc(ARRAY_SIZE * sizeof(float));\n\n    //Initialize the vector of floats\n    for(i = 0; i &lt; ARRAY_SIZE; i++)\n    vec[i] = i + 1;\n\n    /************************* C Function **************************************/\n    clock_t start, end;\n\n    start = clock();\n\n    for( i = 0; i &lt; ARRAY_SIZE; i++) \n    {\n    non_parallel[i] = vec[i] * vec[i];\n    }\n    end = clock();\n    printf( \"Number of seconds: %f\\n\", (clock()-start)/(double)CLOCKS_PER_SEC );\n\n    free(non_parallel);\n    /***************************************************************************/\n\n\n\n\n    clGetPlatformIDs(1, &amp;platform, NULL);//Just want NVIDIA platform\n    clGetDeviceIDs(platform, CL_DEVICE_TYPE_GPU, 1, &amp;device, NULL);\n    context = clCreateContext(NULL, 1, &amp;device, NULL, NULL, &amp;err);\n\n    // Context error?\n    if(err)\n    {\n    perror(\"Cannot create context\");\n    return 1;\n    }\n\n    //Read the kernel file\n    program_handle = fopen(PROGRAM_FILE,\"r\");\n    fseek(program_handle, 0, SEEK_END);\n    program_size = ftell(program_handle);\n    rewind(program_handle);\n\n    program_buffer = (char*)malloc(program_size + 1);\n    program_buffer[program_size] = '\\0';\n    fread(program_buffer, sizeof(char), program_size, program_handle);\n    fclose(program_handle);\n\n    //Create the program\n    program = clCreateProgramWithSource(context, 1, (const char**)&amp;program_buffer, \n                    &amp;program_size, &amp;err);\n\n    if(err)\n    {\n    perror(\"Cannot create program\");\n    return 1;\n    }\n\n    free(program_buffer);\n\n    clBuildProgram(program, 0, NULL, NULL, NULL, NULL);\n\n    kernel = clCreateKernel(program, KERNEL_FUNC, &amp;err);\n\n    if(err)\n    {\n    perror(\"Cannot create kernel\");\n    return 1;\n    }\n\n    queue = clCreateCommandQueue(context, device, CL_QUEU_PROFILING_ENABLE, &amp;err);\n\n    if(err)\n    {\n    perror(\"Cannot create command queue\");\n    return 1;\n    }\n\n    vec_buffer = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR,\n                sizeof(float) * ARRAY_SIZE, vec, &amp;err);\n    result_buffer = clCreateBuffer(context, CL_MEM_WRITE_ONLY, sizeof(float)*ARRAY_SIZE, NULL, &amp;err);\n\n    if(err)\n    {\n    perror(\"Cannot create the vector buffer\");\n    return 1;\n    }\n\n    clSetKernelArg(kernel, 0, sizeof(cl_mem), &amp;vec_buffer);\n    clSetKernelArg(kernel, 1, sizeof(cl_mem), &amp;result_buffer);\n\n    size_t global_size = ARRAY_SIZE;\n    size_t local_size = 0;\n\n    clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &amp;global_size, NULL, 0, NULL, &amp;prof_event);\n\n    clEnqueueReadBuffer(queue, result_buffer, CL_TRUE, 0, sizeof(float)*ARRAY_SIZE, &amp;result, 0, NULL, NULL);\n    clFinish(queue);\n\n\n\n     clGetEventProfilingInfo(prof_event, CL_PROFILING_COMMAND_START,\n           sizeof(time_start), &amp;time_start, NULL);\n     clGetEventProfilingInfo(prof_event, CL_PROFILING_COMMAND_END,\n           sizeof(time_end), &amp;time_end, NULL);\n     total_time += time_end - time_start;\n\n    printf(\"\\nAverage time in nanoseconds = %lu\\n\", total_time/ARRAY_SIZE);\n\n\n\n    clReleaseMemObject(vec_buffer);\n    clReleaseMemObject(result_buffer);\n    clReleaseKernel(kernel);\n    clReleaseCommandQueue(queue);\n    clReleaseProgram(program);\n    clReleaseContext(context);\n\n    free(vec);\n\n    return 0;\n}\n</code></pre>\n\n<p>And the kernel is:</p>\n\n<pre><code>__kernel void float_sum(__global float* vec,__global float* result){\n    int gid = get_global_id(0);\n    result[gid] = vec[gid] * vec[gid];\n}\n</code></pre>\n\n<p>Now, the results are:</p>\n\n<blockquote>\n  <p>Number of seconds: 0.010000 &lt;- This is the for the C code</p>\n  \n  <p>Average time in nanoseconds = 140737284 &lt;- OpenCL function </p>\n</blockquote>\n\n<p>0,1407 seconds is the time of the OpenCL time kernel execution, and it's more than the C function, is it correct? Beacause I think OpenCL should be fastest than C non parallel algorithm...</p>\n"},{"tags":["sql","performance","postgresql","plpgsql","window-functions"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":68,"score":1,"question_id":13078964,"title":"Ordered count of consecutive repeats / duplicates","body":"<p>I highly doubt I'm doing this in the most efficient manner, which is why I tagged <code>plpgsql</code> on here. I need to run this on <strong>2 billion rows</strong> for a <strong>thousand  measurement systems</strong>.</p>\n\n<p>You have measurement systems that often report the previous value when they lose connectivity, and they lose connectivity for spurts often but sometimes for a long time. You need to aggregate but when you do so, you need to look at how long it was repeating and make various filters based on that information. Say you are measuring mpg on a car but it's stuck at 20 mpg for an hour than moves around to 20.1 and so on. You'll want to evaluate the accuracy when it's stuck. You could also place some alternative rules that look for when the car is on the highway, and with window functions you can generate the 'state' of the car and have something to group on. Without further ado:</p>\n\n<pre><code>--here's my data, you have different systems, the time of measurement, and the actual measurement\n--as well, the raw data has whether or not it's a repeat (hense the included window function\nselect * into temporary table cumulative_repeat_calculator_data\nFROM\n    (\n    select \n    system_measured, time_of_measurement, measurement, \n    case when \n     measurement = lag(measurement,1) over (partition by system_measured order by time_of_measurement asc) \n     then 1 else 0 end as repeat\n    FROM\n    (\n    SELECT 5 as measurement, 1 as time_of_measurement, 1 as system_measured\n    UNION\n    SELECT 150 as measurement, 2 as time_of_measurement, 1 as system_measured\n    UNION\n    SELECT 5 as measurement, 3 as time_of_measurement, 1 as system_measured\n    UNION\n    SELECT 5 as measurement, 4 as time_of_measurement, 1 as system_measured\n    UNION\n    SELECT 5 as measurement, 1 as time_of_measurement, 2 as system_measured\n    UNION\n    SELECT 5 as measurement, 2 as time_of_measurement, 2 as system_measured\n    UNION\n    SELECT 5 as measurement, 3 as time_of_measurement, 2 as system_measured\n    UNION\n    SELECT 5 as measurement, 4 as time_of_measurement, 2 as system_measured\n    UNION\n    SELECT 150 as measurement, 5 as time_of_measurement, 2 as system_measured\n    UNION\n    SELECT 5 as measurement, 6 as time_of_measurement, 2 as system_measured\n    UNION\n    SELECT 5 as measurement, 7 as time_of_measurement, 2 as system_measured\n    UNION\n    SELECT 5 as measurement, 8 as time_of_measurement, 2 as system_measured\n    ) as data\n) as data;\n\n--unfortunately you can't have window functions within window functions, so I had to break it down into subquery\n--what we need is something to partion on, the 'state' of the system if you will, so I ran a running total of the nonrepeats\n--this creates a row that stays the same when your data is repeating - aka something you can partition/group on\nselect * into temporary table cumulative_repeat_calculator_step_1\nFROM\n    (\n    select \n    *,\n    sum(case when repeat = 0 then 1 else 0 end) over (partition by system_measured order by time_of_measurement asc) as cumlative_sum_of_nonrepeats_by_system\n    from cumulative_repeat_calculator_data\n    order by system_measured, time_of_measurement\n) as data;\n\n--finally, the query. I didn't bother showing my desired output, because this (finally) got it\n--I wanted a sequential count of repeats that restarts when it stops repeating, and starts with the first repeat\n--what you can do now is take the average measurement under some condition based on how long it was repeating, for example  \nselect *, \ncase when repeat = 0 then 0\nelse\nrow_number() over (partition by cumlative_sum_of_nonrepeats_by_system, system_measured order by time_of_measurement) - 1\nend as ordered_repeat\nfrom cumulative_repeat_calculator_step_1\norder by system_measured, time_of_measurement\n</code></pre>\n\n<p>So, what would you do differently in order to run this on a huge table, or what alternative tools would you use? I'm thinking plpgsql because I suspect this needs to done in-database, or during the data insertion process, although I generally work with the data after it's loaded. Is there any way to get this in one sweep without resorting to sub-queries? </p>\n\n<p>I have tested one <em>alternative method</em>, but it still relies on a sub-query and I think this is faster. For that method you create a \"starts and stops\" table with start_timestamp, end_timestamp, system. Then you join to the larger table and if the timestamp is between those, you classify it as being in that state, which is essentially an alternative to <code>cumlative_sum_of_nonrepeats_by_system</code>. But when you do this, you join on 1=1 for thousands of devices and thousands or millions of 'events'. Do you think that's a better way to go?</p>\n"},{"tags":["php","mysql","ajax","performance","query"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":29,"score":1,"question_id":13079933,"title":"Many datbase calls that load less data or fewer database calls that load more data?","body":"<p>I have a website I'm working on that loads 8 rows of data by making an ajax call to a php content loader that calls my database each time the user hits a next button to load the new content.</p>\n\n<p>This method works very fast for me right now as a single user of the site. </p>\n\n<p>My question is though, if I have many users on the site, would it be better to load the next 16,32, etc rows of data up front when they hit next or keep the way it is now by loading in 8 rows each time they hit next? Does it matter?</p>\n"},{"tags":["performance","mongodb","spring-data"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":31,"score":1,"question_id":13062897,"title":"MongoDB + SpringData Query very slow","body":"<p>I have this simple class: </p>\n\n<pre><code>@Document (collection = \"advertise\")\npublic class AdvertiseCache {\n    @Id\n    private int id;\n\n    private int brandId;\n    private String brandName;\n    private String modelName;\n\n    @Indexed\n    private int odometer;\n\n    @Indexed\n    private int price;\n    private boolean learner;\n    private int manufacturedYear;\n    private double engineSize;\n    private String transmissionTypeName;\n    private String stateName;\n    private String ownerTypeName; //private/dealer\n    private String conditionTypeName; //new/used\n}\n</code></pre>\n\n<p>I have another class with same attributes but annotated with @Entity.</p>\n\n<p>They are stored in MongoDB and respectively in PostgreSQL.</p>\n\n<p>I am using Spring Data JPA for PostgreSQL and Spring Data MongoDB ... for mongo.</p>\n\n<p>Both databases contain same data, 30 rows.</p>\n\n<ul>\n<li><p>10000 queries of type findAll will cost: Mongo ~8000-9000ms and PostgreSQL ~10000-11000ms</p></li>\n<li><p>10000 queries of type findAll where price >= 1 and price &lt;=9000 and odometer >=1 and odometer &lt;= 40000 will cost: Mongo: ~7000ms and PostgreSQL ~7200ms</p></li>\n</ul>\n\n<p><strong>WHY? Am I doing something wrong?</strong> I was expecting mongo much faster. (I my application rarely I am using just find all. Most of the times I use filters for sorting)</p>\n\n<p>Both servers are running in a FreeBSD 9 virtual machine. I tested this on another VM with CentOS 6.3. Similar results +-100ms.</p>\n\n<p>Tnx</p>\n\n<p>/// more code for explanations (my filter builder will contain only odometerMin, odometerMax for between criteria and priceMin and priceMax for between criteria:</p>\n\n<pre><code>public List&lt;AdvertiseCache&gt; findByFilter(FilterBuilder filter) {\n    List&lt;AdvertiseCache&gt; result = null;\n    Query query = new Query();\n    Criteria criteria = new Criteria();\n    criteria = criteria.and(\"price\").gte(filter.getPriceMin()).lte(filter.getPriceMax());\n    criteria = criteria.and(\"odometer\").gte(filter.getOdometerMin()).lte(filter.getOdometerMax()); \n    query.addCriteria(criteria);\n    query.limit(filter.getLimit());\n    query.skip(filter.getOffset());\n    result = mongoTemplate.find(query, AdvertiseCache.class, collectionName);\n    return result;\n}\n</code></pre>\n"},{"tags":["jquery","performance","google-chrome","dom"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":79,"score":4,"question_id":12797416,"title":"Chrome event triggering takes ages in large DOM","body":"<p>I have a relatively large DOM and have noticed an incredible performance degradation in chrome when triggering events on an element.  A single event e.g:</p>\n\n<pre><code>myElem.trigger('myevent.myscope',arguments);\n</code></pre>\n\n<p>takes 14ms!! (22.0.1229.92 m)</p>\n\n<p>the same event in firefox 15.0.1 and msie 9 take less than 1ms to trigger!</p>\n\n<p>The element is a jquery object and has been cached so there is no DOM lookup taking place prior to the trigger.  I am using console.time() </p>\n\n<pre><code>console.time('trigger');\nmyElem.trigger('myevent.myscope',arguments);\nconsole.timeEnd('trigger');\n</code></pre>\n\n<p>Can someone shed a bit of light on this situation</p>\n\n<p>Thanks</p>\n\n<p>Gary</p>\n"},{"tags":[".net","performance","windows-server-2008-r2","couchbase"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":86,"score":0,"question_id":12790110,"title":"Couchbase client is too slow on production server","body":"<p>We recently purchased a new server for our project. After that I've noticed a performance problem with Couchbase client. Then I wrote a simple load tool to compare performance on different machines:</p>\n\n<pre><code>internal class Program {\n    private static IMemcachedClient _client;\n    private static string _key = \"mykey\";\n    private static bool _value = false;\n    private static void Main() {\n        _client = new CouchbaseClient();\n\n        _client.Store(StoreMode.Set, _key, _value);\n        while (true) {\n            _client.Get(_key);\n        }\n    }\n}\n</code></pre>\n\n<p>On my development machine this tool makes 35k gets per sec to localhost memcached instance. \nBut on the server it's much slower - 4k gets per sec with same settings.</p>\n\n<p>It is very big difference and I don't understand the reason.</p>\n\n<p>Dev machine configuration:</p>\n\n<ul>\n<li>Windows 7 Professional x64 </li>\n<li>Core i7-2600 3.4GHz </li>\n<li>8Gb RAM</li>\n</ul>\n\n<p>Production server configuration:</p>\n\n<ul>\n<li>Windows 2008 R2 Enterprise x64</li>\n<li>2x Xeon E5645 2.4GHz</li>\n<li>48Gb RAM</li>\n</ul>\n\n<p>Can you help me to understand why server is so slow?</p>\n"},{"tags":["linux","performance","real-time","scheduling"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":107,"score":1,"question_id":11111852,"title":"how to shield a cpu from the linux scheduler (prevent it scheduling threads onto that cpu)?","body":"<p>It is possible to use <code>sched_setaffinity</code> to pin a thread to a cpu, increasing performance (in some situations)</p>\n\n<p>From the linux man page:</p>\n\n<blockquote>\n  <p>Restricting a process to run on a single CPU also avoids the\n  performance cost caused by the cache invalidation that occurs when a\n  process ceases to execute on one CPU and then recommences execution on\n  a different CPU</p>\n</blockquote>\n\n<p>Further, if I desire a more real-time response, I can change the scheduler policy for that thread to <code>SCHED_FIFO</code>, and up the priority to some high value (up to <code>sched_get_priority_max</code>), meaning the thread in question should always pre-empt any other thread running on its cpu when it becomes ready.</p>\n\n<p>However, at this point, the thread running on the cpu which the real-time thread just pre-empted will possibly have evicted much of the real-time thread's level-1 cache entries.</p>\n\n<p>My questions are as follows:</p>\n\n<ol>\n<li>Is it possible to prevent the scheduler from scheduling any threads onto a given cpu? (eg: either hide the cpu completely from the scheduler, or some other way)</li>\n<li>Are there some threads which absolutely have to be able to run on that cpu? (eg: kernel threads / interrupt threads)</li>\n<li>If I need to have kernel threads running on that cpu, what is a reasonable maximum priority value to use such that I don't starve out the kernel threads?</li>\n</ol>\n"},{"tags":["c#","performance","linq","list","iqueryable"],"answer_count":1,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":82,"score":5,"question_id":13078208,"title":"List queries 20 times faster than IQueryable?","body":"<p>Here is a test that i have setup this evening. It was made to prove something different, but the outcome was not quite as i expected. </p>\n\n<p>I'm running a test with 10000 random queries on an IQueryable and while testing i found out that if i do the same on a List, my test is 20 times faster.</p>\n\n<p>See below. My CarBrandManager.GetList originally returns an IQueryable, but now i first issue a ToList(), and then it's way faster. </p>\n\n<p>Can anyone tell me something about why i see this big difference? </p>\n\n<pre><code>var sw = new Stopwatch();\nsw.Start();\n\nint queries = 10000;\n\n//IQueryable&lt;Model.CarBrand&gt; carBrands = CarBrandManager.GetList(context);\nList&lt;Model.CarBrand&gt; carBrands = CarBrandManager.GetList(context).ToList();\n\nRandom random = new Random();\nint randomChar = 65;\n\nfor (int i = 0; i &lt; queries; i++)\n{\n    randomChar = random.Next(65, 90);\n    Model.CarBrand carBrand = carBrands.Where(x =&gt; x.Name.StartsWith(((char)randomChar).ToString())).FirstOrDefault();\n}\n\nsw.Stop();\nlblStopWatch.Text = String.Format(\"Queries: {0} Elapsed ticks: {1}\", queries, sw.ElapsedTicks);\n</code></pre>\n"},{"tags":["c#","c++",".net","performance","comparison"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":6,"view_count":233,"score":-2,"question_id":13063714,"title":"Why does game dev use C++ and why doesn't the rest of the industry?","body":"<p>Its a fact today that almost every AAA video game released for the PC, Xbox or PS3 is written entirely in C++. Some vendors say they can't move to C# because large codebases for the various engines they reuse internally are already built and tested in C++ (physics, rendering, etc). And performance-wise, its a generally accepted fact that <a href=\"http://www.codeproject.com/Articles/212856/Head-to-head-benchmark-Csharp-vs-NET\" rel=\"nofollow\">C++ code runs faster than C#</a>.</p>\n\n<p>To re-iterate : <strong>Why does the professional game-dev industry in general use C++, and why don't we?</strong> Is it because the general line-of-business application doesn't require the \"best performance you can get\" and trading reliability (memory leaks, OOBs, stack overflows) for performance \"isn't important\"? Or is it just because C++ is generally \"harder to maintain\" (templates, pointers, malloc, etc) than the same code written in C#?</p>\n\n<p>Or is it because development in C++ is \"more time consuming\" than developing the same featureset in C#, so you can't implement as many features as you'd like to? But even today, many high-end game-dev kits such as <a href=\"http://www.unrealengine.com/en/\" rel=\"nofollow\">UDK</a> and <a href=\"http://www.crytek.com/cryengine\" rel=\"nofollow\">CryENGINE</a> are written entirely in C++ and their <a href=\"http://www.unrealengine.com/features/\" rel=\"nofollow\">featureset</a> is among the best you can get on any platform. <strong>So if C++ isn't \"harder\" or \"limiting\" or \"buggier\" for such companies then why is it used so little outside the game-dev or scientific/HPC industries?</strong> </p>\n"},{"tags":["performance","node.js","openssl","spawn"],"answer_count":0,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":13,"score":0,"question_id":13077869,"title":"What are the drawbacks of using child_process.spawn vs. native bindings in NodeJS","body":"<p>I'm currently working on a NodeJS SCEP server. It heavily uses the <code>openssl</code> executable to work.</p>\n\n<p>Basically, I'm using <code>child_process.spawn</code> to wrap openssl functionality.</p>\n\n<p>I'm probably sure it would be better to use native bindings to the openssl lib, but since it is not available (<code>crypto</code> does not have all the smime.verify smime.decrypt req.verify stuff), I had to find an easy way to make it work, but I'm not sure it is a viable solution.</p>\n\n<p>To me the first drawback is the unstable API, I do rely on a cli output that can change between servers, etc. But what about performance?</p>\n"},{"tags":["sql","performance","ms-access"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":45,"score":0,"question_id":13075009,"title":"typeperf logging to SQL","body":"<p>I am trying to log typeperf to an ACCESS database and am not having any luck.  I created an access database, added the File DSN, and when I try to run the command:</p>\n\n<p>typeperf -si 15 -f SQL \"\\Process(*)\\ID Process\" -o SQL:accessDSN!log</p>\n\n<p>the output just says Unknown.  </p>\n"},{"tags":["c++","performance"],"answer_count":5,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":105,"score":0,"question_id":13073909,"title":"How to optimize input/output in C++","body":"<p>I'm solving a problem which requires very fast input/output. More precisely, the input data file will be up to 15MB. Is there a fast, way to read/print integer values.</p>\n\n<p><em>Note:</em> I don't know if it helps, but the input file has the following form: </p>\n\n<ul>\n<li>line 1: a number n</li>\n<li>line 2..n+1: three numbers a,b,c;</li>\n<li>line n+2: a number r</li>\n<li>line n+3..n+4+r: four numbers a,b,c,d</li>\n</ul>\n\n<p><em>Note 2:</em> The input file will be <code>stdin</code>.</p>\n\n<p><strong>Edit:</strong> Something like the following isn't fast enough:</p>\n\n<pre><code>void fast_scan(int &amp;n) {\n  char buffer[10];\n  gets(buffer);\n  n=atoi(buffer);\n}\n\nvoid fast_scan_three(int &amp;a,int &amp;b,int &amp;c) {\n  char buffval[3][20],buffer[60];\n  gets(buffer);\n  int n=strlen(buffer);\n  int buffindex=0, curindex=0;\n  for(int i=0; i&lt;n; ++i) {\n    if(!isdigit(buffer[i]) &amp;&amp; !isspace(buffer[i]))break;\n    if(isspace(buffer[i])) {\n      buffindex++;\n      curindex=0;\n    } else {\n      buffval[buffindex][curindex++]=buffer[i];\n    }\n  }\n  a=atoi(buffval[0]);\n  b=atoi(buffval[1]);\n  c=atoi(buffval[2]);\n}\n</code></pre>\n"},{"tags":["java","android","c","performance","android-ndk"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":109,"score":0,"question_id":12998245,"title":"JAVA Matrix-Vector-Multiplication 100 times slower than C-Version","body":"<p>im working on performance-differences between Android JAVA- and Android NDK-applications.\nI performed a Matrix4D-Vector4D Transformation on more than 90000 vertices as an example for 3D Graphics.</p>\n\n<p>It seemes, that the JAVA Version is nearly <strong>100 times slower</strong> than the C-Version. Did i something wrong? Does anyone have similar experiences?</p>\n\n<p>my Java-Code for transformation:</p>\n\n<pre><code>        long t1 = System.nanoTime();\n        for ( int i = 0; i &lt; vCount; i++)\n        {\n\n            Vector4 vOut = new Vector4();\n            Vector4 v = vertices[i];\n\n            vOut.v_[0] = v.v_[0] * matrix[0].v_[0];\n            vOut.v_[1] = v.v_[0] * matrix[0].v_[1];\n            vOut.v_[2] = v.v_[0] * matrix[0].v_[2];\n            vOut.v_[3] = v.v_[0] * matrix[0].v_[3];\n\n            vOut.v_[0] += v.v_[1] * matrix[1].v_[0];\n            vOut.v_[1] += v.v_[1] * matrix[1].v_[1];\n            vOut.v_[2] += v.v_[1] * matrix[1].v_[2];\n            vOut.v_[3] += v.v_[1] * matrix[1].v_[3];\n\n            vOut.v_[0] += v.v_[2] * matrix[2].v_[0];\n            vOut.v_[1] += v.v_[2] * matrix[2].v_[1];\n            vOut.v_[2] += v.v_[2] * matrix[2].v_[2];\n            vOut.v_[3] += v.v_[2] * matrix[2].v_[3];\n\n            vOut.v_[0] += v.v_[3] * matrix[3].v_[0];\n            vOut.v_[1] += v.v_[3] * matrix[3].v_[1];\n            vOut.v_[2] += v.v_[3] * matrix[3].v_[2];\n            vOut.v_[3] += v.v_[3] * matrix[3].v_[3]; \n\n            vertices[i] = vOut;\n\n        }\n        long t2 = System.nanoTime();        \n        long diff = t2 - t1;        \n        double ms = (double)(diff / 1000000.0f);\n        Log.w(\"GL2JNIView\", String.format(\"ms %.2f \", ms));\n</code></pre>\n\n<p>Performance (Transform > 90 000 Vertices | Android 4.0.4 SGS II):\n(Median-value of 200 runs)</p>\n\n<pre><code>JAVA-Version:   2 FPS\nC-Version:    190 FPS\n</code></pre>\n"},{"tags":["performance","mongodb","indexing","schema","geospatial"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":123,"score":1,"question_id":12908871,"title":"MongoDB geospatial query with sort - performance issues","body":"<p>I have query (which is very slow <strong>~2,5s</strong>):</p>\n\n<pre><code>db.markers.find({ latlng: { '$within': { '$box': [ [ -16, -140 ], [ 75, 140 ] ] } } }).sort({_id: -1}).limit(1000)\n</code></pre>\n\n<p>When I run explain for this query I get</p>\n\n<pre><code>{\n   \"cursor\" : \"GeoBrowse-box\",\n   \"isMultiKey\" : false,\n   \"n\" : 1000,\n   \"nscannedObjects\" : 242331,\n   \"nscanned\" : 242331,\n   \"nscannedObjectsAllPlans\" : 242331,\n   \"nscannedAllPlans\" : 242331,\n   \"scanAndOrder\" : true,\n   \"indexOnly\" : false,\n   \"nYields\" : 1383,\n    \"nChunkSkips\" : 0,\n    \"millis\" : 2351,\n    \"indexBounds\" : {\n        \"latlng\" : [ ]\n    },\n    \"lookedAt\" : NumberLong(262221),\n    \"matchesPerfd\" : NumberLong(242331),\n    \"objectsLoaded\" : NumberLong(242331),\n    \"pointsLoaded\" : NumberLong(0),\n    \"pointsSavedForYield\" : NumberLong(0),\n    \"pointsChangedOnYield\" : NumberLong(0),\n    \"pointsRemovedOnYield\" : NumberLong(0),\n    \"server\" : \"xx:27017\"\n}\n</code></pre>\n\n<p>When I remove <strong>sort({_id: -1})</strong> explain gives me (fast query <strong>5 milis</strong>):</p>\n\n<pre><code>{\n    \"cursor\" : \"GeoBrowse-box\",\n    \"isMultiKey\" : false,\n    \"n\" : 1000,\n    \"nscannedObjects\" : 1000,\n    \"nscanned\" : 1000,\n    \"nscannedObjectsAllPlans\" : 1000,\n    \"nscannedAllPlans\" : 1000,\n    \"scanAndOrder\" : false,\n    \"indexOnly\" : false,\n    \"nYields\" : 0,\n    \"nChunkSkips\" : 0,\n    \"millis\" : 5,\n    \"indexBounds\" : {\n        \"latlng\" : [ ]\n    },\n    \"lookedAt\" : NumberLong(1000),\n    \"matchesPerfd\" : NumberLong(1000),\n    \"objectsLoaded\" : NumberLong(1000),\n    \"pointsLoaded\" : NumberLong(0),\n    \"pointsSavedForYield\" : NumberLong(0),\n    \"pointsChangedOnYield\" : NumberLong(0),\n    \"pointsRemovedOnYield\" : NumberLong(0),\n        \"server\" : \"xx:27017\"\n}\n</code></pre>\n\n<p>I have  2d index on latlng, desc index on _id and <strong>compound</strong> indexes.</p>\n\n<pre><code>db.markers.ensureIndex({latlng: '2d', _id:-1})\ndb.markers.ensureIndex({ latlng: '2d' })\ndb.markers.ensureIndex({ _id: -1 })\n</code></pre>\n\n<p>What I want to achieve is to get markers from a particular area sorted from newest.</p>\n\n<p>Any ideas or suggestions how to do a lot less than <strong>2.5 seconds</strong>??</p>\n\n<p>If someone wants to do their own tests</p>\n\n<pre><code>var i = 0,\n  lat = 0,\n  lng = 0;\n\nfor (i; i &lt; 260000; i++) {\n  lat = parseFloat(Math.min(-90 + (Math.random() * 180), 90).toFixed(6));\n  lng = parseFloat(Math.min(-180 + (Math.random() * 360), 180).toFixed(6));\n  collection.insert({latlng: [lat, lng]}, function () {});\n}\n\ncollection.find({ latlng: { '$within': { '$box': [ [ -90, -180 ], [ 90, 180 ] ] } } }, {latlng: 1, _id: 1 }).sort({_id: -1}).limit(1000).explain()\n</code></pre>\n\n<p>On my local machine I receives (<strong>~ 2,6s</strong>):</p>\n\n<pre><code>{\n    \"cursor\" : \"GeoBrowse-box\",\n    \"isMultiKey\" : false,\n    \"n\" : 1000,\n    \"nscannedObjects\" : 260000,\n    \"nscanned\" : 260000,\n    \"nscannedObjectsAllPlans\" : 260000,\n    \"nscannedAllPlans\" : 260000,\n    \"scanAndOrder\" : true,\n    \"indexOnly\" : false,\n    \"nYields\" : 1612,\n    \"nChunkSkips\" : 0,\n    \"millis\" : 2613,\n    \"indexBounds\" : {\n            \"latlng\" : [ ]\n    },\n    \"lookedAt\" : NumberLong(260000),\n    \"matchesPerfd\" : NumberLong(260000),\n    \"objectsLoaded\" : NumberLong(260000),\n    \"pointsLoaded\" : NumberLong(0),\n    \"pointsSavedForYield\" : NumberLong(0),\n    \"pointsChangedOnYield\" : NumberLong(0),\n    \"pointsRemovedOnYield\" : NumberLong(0),\n    \"server\" : \"xx:27017\"\n}\n</code></pre>\n\n<p>Thx</p>\n"},{"tags":["python","performance","optimization","if-statement"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":65,"score":2,"question_id":13073777,"title":"Python: improve in elegant way (code saving) a function in order to avoid several Statements","body":"<p>I wrote this function to read <a href=\"http://www.liblas.org/tutorial/python.html\" rel=\"nofollow\">Las</a> file and save a shapefile. The function creates a shapefile with 8 fields. What i wish insert a <strong>parse</strong> element in the function in order to select the fields i wish to save LAS2SHP(inFile,outFile=None,parse=None). if None all fields are saved. if <strong>parse</strong> is \nparse=\"irn\" the fields intensity, return_number, and number_of_returns are saved. following the legend</p>\n\n<pre><code>\"i\": p.intensity,\n\"r\": p.return_number,\n\"n\": p.number_of_returns,\n\"s\": p.scan_direction,\n\"e\": p.flightline_edge,\n\"c\": p.classification,\n\"a\": p.scan_angle, \n</code></pre>\n\n<p>I wrote a solution if....ifelse....else really code consuming (and not elegant). Thanks for all helps and suggestions for saving code </p>\n\n<p>thanks in advance\nGianni</p>\n\n<p>here the original function in python</p>\n\n<pre><code>import shapefile\nfrom liblas import file as lasfile\n\ndef LAS2SHP(inFile,outFile=None):\n    w = shapefile.Writer(shapefile.POINT)\n    w.field('Z','C','10')\n    w.field('Intensity','C','10')\n    w.field('Return','C','10')\n    w.field('NumberRet','C','10')\n    w.field('ScanDir','C','10')\n    w.field('FlightEdge','C','10')\n    w.field('Class','C','10')\n    w.field('ScanAngle','C','10')\n    for p in lasfile.File(inFile,None,'r'):\n        w.point(p.x,p.y)\n        w.record(float(p.z),float(p.intensity),float(p.return_number),float(p.number_of_returns),float(p.scan_direction),float(p.flightline_edge),float(p.classification),float(p.scan_angle))\n    if outFile == None:\n        inFile_path, inFile_name_ext = os.path.split(os.path.abspath(inFile))\n        inFile_name = os.path.splitext(inFile_name_ext)[0]\n        w.save(\"{0}\\\\{1}.shp\".format(inFile_path,inFile_name))\n    else:\n        w.save(outFile)\n</code></pre>\n"},{"tags":["json","performance","servlets","jersey","response"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":56,"score":0,"question_id":13043590,"title":"Jersey response very slow","body":"<p>I am using jersey for REST service, and during measuring request time I encountered a delay in the response which seems to come from jersey itself.</p>\n\n<p>While the computation of the request is about 8 seconds (testsystem), the time until jersey delivers the request is additionally 12 seconds. The 8 seconds are measured from the first jersey \"Server in-bound request\" after the request ist build in our code:</p>\n\n<pre><code>Response.ok( entity ).build( );\n</code></pre>\n\n<p>Then, some magic in jersey happens and this magic seems to last 12 sec until the \" Server out-bound response\" log appears and the json stuff appears on the console. cURL says, that the request lasted 20secs. The resonse contains 32 more or less big json data strings.</p>\n\n<p>I have already turned off the link and logging filter in web.xml, no improvements.</p>\n\n<p>I have no idea where to start searching for. Thanks for your ideas!</p>\n\n<p>We</p>\n"},{"tags":["c++","c","performance","optimization"],"answer_count":3,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":93,"score":1,"question_id":13058315,"title":"How to optimize simple gaussian filter for performance?","body":"<p>I am trying to write an android app which needs to calculate gaussian and laplacian pyramids for multiple full resolution images, i wrote this it on C++ with NDK, the most critical part of the code is applying gaussian filter to images abd i am applying this filter with horizontally and vertically.   </p>\n\n<p>The filter is (0.0625, 0.25, 0.375, 0.25, 0.0625)\nSince i am working on integers i am calculating (1, 4, 6, 4, 1)/16</p>\n\n<pre><code>dst[index] = ( src[index-2] + src[index-1]*4 + src[index]*6+src[index+1]*4+src[index+2])/16;\n</code></pre>\n\n<p>I have made a few simple optimization however it still is working slow than expected and i was wondering if there are any other optimization options that i am missing. </p>\n\n<p>PS: I should mention that i have tried to write this filter part with inline arm assembly however it give 2x slower results.</p>\n\n<pre><code>//horizontal  filter\nfor(unsigned y = 0; y &lt; height;  y++) {\n    for(unsigned x = 2; x &lt; width-2;  x++) {\n        int index = y*width+x;\n            dst[index].r = (src[index-2].r+ src[index+2].r + (src[index-1].r + src[index+1].r)*4 + src[index].r*6)&gt;&gt;4;\n            dst[index].g = (src[index-2].g+ src[index+2].g + (src[index-1].g + src[index+1].g)*4 + src[index].g*6)&gt;&gt;4;\n            dst[index].b = (src[index-2].b+ src[index+2].b + (src[index-1].b + src[index+1].b)*4 + src[index].b*6)&gt;&gt;4;                \n     }\n}\n//vertical filter\nfor(unsigned y = 2;  y &lt; height-2;  y++) {\n    for(unsigned x = 0;  x &lt; width;  x++) {\n        int index = y*width+x;\n            dst[index].r = (src[index-2*width].r + src[index+2*width].r  + (src[index-width].r + src[index+width].r)*4 + src[index].r*6)&gt;&gt;4;\n            dst[index].g = (src[index-2*width].g + src[index+2*width].g  + (src[index-width].g + src[index+width].g)*4 + src[index].g*6)&gt;&gt;4;\n            dst[index].b = (src[index-2*width].b + src[index+2*width].b  + (src[index-width].b + src[index+width].b)*4 + src[index].b*6)&gt;&gt;4;\n     }\n}\n</code></pre>\n"},{"tags":["performance","cassandra"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":42,"score":1,"question_id":13020195,"title":"cassandra 1 big column vs multi small columns in read performance?","body":"<p>I'm getting around 1000 distinct events per second, (4 nodes cluster). After each event I will need to increase some counters. My question is, is it better to have a normal column family which has only one column and all the counters are treated like string with comma \",\" separated (example: \"1,3,5,6,0,2\") or it is better to create a Counter Column family with multiple columns? I read some document it says that counter column family can do read and write with consistency level 1 which is fast for reading. I don't really care much about write performance.</p>\n"},{"tags":["performance","multithreading","jvm","multicore"],"answer_count":4,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":4254,"score":2,"question_id":1649402,"title":"Would a multithreaded Java application exploit a multi-core machine very well?","body":"<p>If I write a multi-threaded java application, will the JVM take care of utilizing all available cores? Do I have to do some work?</p>\n"},{"tags":["performance","node.js","redis"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":794,"score":4,"question_id":5709866,"title":"Redis performance issues?","body":"<p>I was trying to put some heavy load on my Redis for testing purposes and find out any upper limits. First I loaded it with 50,000 and 100,000 keys of size 32characters with values around 32 characters. It took no more than 8-15 seconds in both key sizes. Now I try to put 4kb of data as value for each key. First 10000 keys take 800 milli seconds to set. But from that point it slows down gradually and to set whole 50,000 keys it takes aroudn 40 minutes. I am loading the database using NodeJs with <a href=\"http://github.com/mranney/node_redis\" rel=\"nofollow\">node_redis (Mranney)</a> . Is there any mistake I am doing or is Redis just that slow with big values of size 4 KB?</p>\n\n<p>One more thing I found now is when I run another client parallel to the current one and update keys this 2nd client finishes up loading the 50000 keys with 4kb values within 8 seconds while the first client still does its thing forever. Is it a bug in node or the redis library? This is alarming and not acceptable for production.</p>\n"},{"tags":["javascript","performance","optimization","coding-style"],"answer_count":7,"favorite_count":8,"up_vote_count":34,"down_vote_count":0,"view_count":8645,"score":34,"question_id":143486,"title":"Unobtrusive JavaScript: <script> at the top or the bottom of the HTML code?","body":"<p>I've recently read the Yahoo manifesto <a href=\"http://developer.yahoo.com/performance/rules.html#postload\" rel=\"nofollow\">Best Practices for Speeding Up Your Web Site</a>. They recommend to put the JavaScript inclusion at the bottom of the HTML code when we can.</p>\n\n<p>But where exactly and when?</p>\n\n<p>Should we put it before closing <code>&lt;/html&gt;</code> or after ? And above all, when should we still put it in the <code>&lt;head&gt;</code> section?</p>\n"},{"tags":["php","performance","caching","apc"],"answer_count":6,"favorite_count":29,"up_vote_count":55,"down_vote_count":0,"view_count":37807,"score":55,"question_id":911158,"title":"How to clear APC cache entries?","body":"<p>I need to clear all APC cache entries when I deploy a new version of the site.\nAPC.php has a button for clearing all opcode caches, but I don't see buttons for clearing all User Entries, or all System Entries, or all Per-Directory Entries.</p>\n\n<p>Is it possible to clear all cache entries via the command-line, or some other way?</p>\n"},{"tags":["ruby-on-rails","ruby-on-rails-3","performance","where"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":47,"score":2,"question_id":13054759,"title":"Rails efficiency of where clause","body":"<p>I'm worried about the efficiency of this line in the controller of my Rails project</p>\n\n<pre><code>posts_list = Post.where(:title =&gt; params[:title])\n</code></pre>\n\n<p>If the number of \"Posts\" in the database grows, will the line become slow to execute? Is there any possible optimization?</p>\n"},{"tags":["c#","multithreading","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":100,"score":4,"question_id":13070339,"title":"Parallel.For vs regular threads","body":"<p>I'm trying to understand why Parallel.For is able to outperform a number of threads in the following scenario: consider a batch of jobs that can be processed in parallel. While processing these jobs, new work may be added, which then needs to be processed as well. The <code>Parallel.For</code> solution would look as follows:</p>\n\n<pre><code>var jobs = new List&lt;Job&gt; { firstJob };\nint startIdx = 0, endIdx = jobs.Count;\nwhile (startIdx &lt; endIdx) {\n  Parallel.For(startIdx, endIdx, i =&gt; WorkJob(jobs[i]));\n  startIdx = endIdx; endIdx = jobs.Count;\n}\n</code></pre>\n\n<p>This means that there are multiple times where the Parallel.For needs to synchronize. Consider a bread-first graph algorithm algorithm; the number of synchronizations would be quite large. Waste of time, no?</p>\n\n<p>Trying the same in the old-fashioned threading approach:</p>\n\n<pre><code>var queue = new ConcurrentQueue&lt;Job&gt; { firstJob };\nvar threads = new List&lt;Thread&gt;();\nvar waitHandle = new AutoResetEvent(false);\nint numBusy = 0;\nfor (int i = 0; i &lt; maxThreads; i++) \n  threads.Add(new Thread(new ThreadStart(delegate {\n    while (!queue.IsEmpty || numBusy &gt; 0) {\n      if (queue.IsEmpty)\n        // numbusy &gt; 0 implies more data may arrive\n        waitHandle.WaitOne();\n\n      Job job;\n      if (queue.TryDequeue(out job)) {\n        Interlocked.Increment(ref numBusy);\n        WorkJob(job); // WorkJob does a waitHandle.Set() when more work was found\n        Interlocked.Decrement(ref numBusy);\n      }\n    }\n    // others are possibly waiting for us to enable more work which won't happen\n    waitHandle.Set(); \n})));\nthreads.ForEach(t =&gt; t.Start());\nthreads.ForEach(t =&gt; t.Join());\n</code></pre>\n\n<p>The <code>Parallel.For</code> code is of course much cleaner, but what I cannot comprehend, it's even faster as well! Is the task scheduler just that good? The synchronizations were eleminated, there's no busy waiting, yet the threaded approach is consistently slower (for me). What's going on? Can the threading approach be made faster?</p>\n\n<p>Edit: thanks for all the answers, I wish I could pick multiple ones. I chose to go with the one that also shows an actual possible improvement.</p>\n"},{"tags":["c","performance","load-testing","qa","loadrunner"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":45,"score":1,"question_id":13063840,"title":"web_reg_find vs strstr(). Performance","body":"<p>What do you think which operation is fastest and creating less loading, LR <code>web_reg_find()</code> or C <code>strstr()</code>? Which is more preferable for a very strong loading test?</p>\n\n<p>And if somebody knows how does <code>web_reg_find()</code> works, please tell me.</p>\n"},{"tags":["performance","java-ee","glassfish-3","cpu-usage"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":1325,"score":1,"question_id":9158842,"title":"Glassfish - high CPU usge","body":"<p>I'm using Glassfish 3.1.1 web profile in production envirnoment and it's eating too much CPU.\nHere is server settings: Windows Server 2008 R2 64bit, Intel Xeon 8core @ 3,2GHz, 8GB ram.\nI'm using JDK 1.7u2 64bit. Glassfish JVM settings:</p>\n\n<pre><code>&lt;jvm-options&gt;-XX:+UseCompressedOops&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-Xmn1g&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-Xss128k&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-XX:+UseParallelOldGC&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-XX:ParallelGCThreads=4&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-Xmx3g&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-XX:+DisableExplicitGC&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-d64&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-XX:PermSize=256m&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-Xms3g&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-XX:MaxPermSize=256m&lt;/jvm-options&gt;\n&lt;jvm-options&gt;-XX:+AggressiveHeap&lt;/jvm-options&gt;\n</code></pre>\n\n<p>I've also tune some options according to <a href=\"http://jfarcand.wordpress.com/2009/11/27/putting-glassfish-v3-in-production-essential-surviving-guide/\" rel=\"nofollow\">http://jfarcand.wordpress.com/2009/11/27/putting-glassfish-v3-in-production-essential-surviving-guide/</a>\nRunning for few hours or 1 day Glassfish use 15% to 40% CPU, someties 90% and application dosen't respond.</p>\n\n<pre><code>[#|2012-02-03T10:30:46.837+0100|WARNING|glassfish3.1.1|com.sun.grizzly.config.GrizzlyServiceListener|_ThreadID=43;_ThreadName=Thread-2;|GRIZZLY0023: Interrupting idle Thread: http-thread-pool-80(32).|#]\n\n[#|2012-02-03T10:30:55.074+0100|WARNING|glassfish3.1.1|com.sun.grizzly.config.GrizzlyServiceListener|_ThreadID=41;_ThreadName=Thread-2;|GRIZZLY0023: Interrupting idle Thread: http-thread-pool-80(41).|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|java.nio.channels.ClosedChannelException\n    at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:249)\n    at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:440)\n    at com.sun.grizzly.util.OutputWriter.flushChannel(OutputWriter.java:108)\n    at com.sun.grizzly.util.OutputWriter.flushChannel(OutputWriter.java:76)\n    at com.sun.grizzly.http.SocketChannelOutputBuffer.flushChannel(SocketChannelOutputBuffer.java:326)\n    at com.sun.grizzly.http.SocketChannelOutputBuffer.flushBuffer(SocketChannelOutputBuffer.java:398)\n    at com.sun.grizzly.http.SocketChannelOutputBuffer.realWriteBytes(SocketChannelOutputBuffer.java:282)\n    at com.sun.grizzly.tcp.http11.InternalOutputBuffer$OutputStreamOutputBuffer.doWrite(InternalOutputBuffer.java:898)\n    at com.sun.grizzly.tcp.http11.filters.ChunkedOutputFilter.doWrite(ChunkedOutputFilter.java:167)\n    at com.sun.grizzly.tcp.http11.filters.GzipOutputFilter$FakeOutputStream.write(GzipOutputFilter.java:223)\n    at java.util.zip.GZIPOutputStream.finish(GZIPOutputStream.java:169)\n    at java.util.zip.DeflaterOutputStream.close(DeflaterOutputStream.java:238)\n    at com.sun.grizzly.tcp.http11.filters.GzipOutputFilter.recycle(GzipOutputFilter.java:186)\n    at com.sun.grizzly.http.SocketChannelOutputBuffer.recycle(SocketChannelOutputBuffer.java:417)\n    at com.sun.grizzly.http.ProcessorTask.finishResponse(ProcessorTask.java:817)\n    at com.sun.grizzly.http.ProcessorTask.postResponse(ProcessorTask.java:750)\n    at com.sun.grizzly.http.ProcessorTask.doProcess(ProcessorTask.java:726)\n    at com.sun.grizzly.http.ProcessorTask.process(ProcessorTask.java:1019)\n    at com.sun.grizzly.http.DefaultProtocolFilter.execute(DefaultProtocolFilter.java:225)\n    at com.sun.grizzly.DefaultProtocolChain.executeProtocolFilter(DefaultProtocolChain.java:137)\n    at com.sun.grizzly.DefaultProtocolChain.execute(DefaultProtocolChain.java:104)\n    at com.sun.grizzly.DefaultProtocolChain.execute(DefaultProtocolChain.java:90)\n    at com.sun.grizzly.http.HttpProtocolChain.execute(HttpProtocolChain.java:79)\n    at com.sun.grizzly.ProtocolChainContextTask.doCall(ProtocolChainContextTask.java:54)\n    at com.sun.grizzly.SelectionKeyContextTask.call(SelectionKeyContextTask.java:59)\n    at com.sun.grizzly.ContextTask.run(ContextTask.java:71)\n    at com.sun.grizzly.util.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:532)\n    at com.sun.grizzly.util.AbstractThreadPool$Worker.run(AbstractThreadPool.java:513)\n    at java.lang.Thread.run(Thread.java:722)\n|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:249)|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:440)|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.util.OutputWriter.flushChannel(OutputWriter.java:108)|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.util.OutputWriter.flushChannel(OutputWriter.java:76)|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.SocketChannelOutputBuffer.flushChannel(SocketChannelOutputBuffer.java:326)|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.SocketChannelOutputBuffer.flushBuffer(SocketChannelOutputBuffer.java:398)|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.SocketChannelOutputBuffer.realWriteBytes(SocketChannelOutputBuffer.java:282)|#]\n\n[#|2012-02-03T10:30:56.665+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.tcp.http11.InternalOutputBuffer$OutputStreamOutputBuffer.doWrite(InternalOutputBuffer.java:898)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.tcp.http11.filters.ChunkedOutputFilter.doWrite(ChunkedOutputFilter.java:167)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.tcp.http11.filters.GzipOutputFilter$FakeOutputStream.write(GzipOutputFilter.java:223)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at java.util.zip.GZIPOutputStream.finish(GZIPOutputStream.java:169)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at java.util.zip.DeflaterOutputStream.close(DeflaterOutputStream.java:238)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.tcp.http11.filters.GzipOutputFilter.recycle(GzipOutputFilter.java:186)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.SocketChannelOutputBuffer.recycle(SocketChannelOutputBuffer.java:417)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.ProcessorTask.finishResponse(ProcessorTask.java:817)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.ProcessorTask.postResponse(ProcessorTask.java:750)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.ProcessorTask.doProcess(ProcessorTask.java:726)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.ProcessorTask.process(ProcessorTask.java:1019)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.DefaultProtocolFilter.execute(DefaultProtocolFilter.java:225)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.DefaultProtocolChain.executeProtocolFilter(DefaultProtocolChain.java:137)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.DefaultProtocolChain.execute(DefaultProtocolChain.java:104)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.DefaultProtocolChain.execute(DefaultProtocolChain.java:90)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.http.HttpProtocolChain.execute(HttpProtocolChain.java:79)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.ProtocolChainContextTask.doCall(ProtocolChainContextTask.java:54)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.SelectionKeyContextTask.call(SelectionKeyContextTask.java:59)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.ContextTask.run(ContextTask.java:71)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.util.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:532)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at com.sun.grizzly.util.AbstractThreadPool$Worker.run(AbstractThreadPool.java:513)|#]\n\n[#|2012-02-03T10:30:56.681+0100|SEVERE|glassfish3.1.1|javax.enterprise.system.std.com.sun.enterprise.server.logging|_ThreadID=138;_ThreadName=Thread-2;|    at java.lang.Thread.run(Thread.java:722)|#]\n\n[#|2012-02-03T10:31:00.440+0100|WARNING|glassfish3.1.1|com.sun.grizzly.config.GrizzlyServiceListener|_ThreadID=40;_ThreadName=Thread-2;|GRIZZLY0023: Interrupting idle Thread: http-thread-pool-80(38).|#]\n\n[#|2012-02-03T10:33:49.170+0100|WARNING|glassfish3.1.1|com.sun.grizzly.config.GrizzlyServiceListener|_ThreadID=40;_ThreadName=Thread-2;|GRIZZLY0023: Interrupting idle Thread: http-thread-pool-80(11).|#]\n\n[#|2012-02-03T10:33:57.235+0100|WARNING|glassfish3.1.1|com.sun.grizzly.config.GrizzlyServiceListener|_ThreadID=43;_ThreadName=Thread-2;|GRIZZLY0023: Interrupting idle Thread: http-thread-pool-80(16).|#]\n</code></pre>\n\n<p>Edit: I solved the problem, there is bug in Glassfish gzip compression, so I turned it off.\n<a href=\"http://www.java.net/forum/topic/glassfish/glassfish/glassfish-301-gzip-problem-threads-apparently-spinning-100-cpu-use\" rel=\"nofollow\">http://www.java.net/forum/topic/glassfish/glassfish/glassfish-301-gzip-problem-threads-apparently-spinning-100-cpu-use</a></p>\n\n<p>Instead I would use this: <a href=\"http://www.servletsuite.com/servlets/gzipflt.htm\" rel=\"nofollow\">http://www.servletsuite.com/servlets/gzipflt.htm</a> to compress content.</p>\n"},{"tags":["ruby-on-rails","performance","views","newrelic-rpm"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":42,"score":1,"question_id":13047992,"title":"Slow rendering of Rails view according to newrelic statistics","body":"<p>I am seeing a performance problem with rails view rendering.\nAccording to Newrelic Stats:</p>\n\n<p><img src=\"http://s10.postimage.org/e155cw0iv/Screen_Shot_2012_10_24_at_1_51_05_PM.png\" alt=\"newrelic stats\"></p>\n\n<p>This slow rendering <em>randomly</em> appears during requests.</p>\n\n<ul>\n<li>Could this problem be connected to slow file reading from the file system?</li>\n<li>Is there any way to debug it in lower level?</li>\n</ul>\n"},{"tags":["c#","windows","performance","hardware"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":35,"score":1,"question_id":13068759,"title":"How do I best-determine system requirements for a new app?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/170939/how-to-specify-the-hardware-your-software-needs\">How to specify the hardware your software needs?</a>  </p>\n</blockquote>\n\n\n\n<p>How do you determine the system requirements of a user's PC in order for them to install and run your software?</p>\n\n<p>I am aware of the obvious, such as Windows, .NET Framework [version number]. But how do you come up with the correct RAM, Processor and all of that?</p>\n\n<p>Is this just something that you observe while you're debugging your app? Do you just check out the Resource Monitor and watch for how much Disk usage your app is using, or how much memory it is taking up?</p>\n\n<p>Are there any tools, or would you recommend I use tools to help determine system requirements for my applications?</p>\n\n<p>I've searched for this but I have not been able to find much information.</p>\n\n<p><em>More importantly, what about the Windows Experience Index? I've seen a few box apps in the shop say you need a Windows Exp. Index of N, but are there tools that determine what index is required for my app to run?</em></p>\n"},{"tags":["performance","class","struct",".net-4.0"],"answer_count":5,"favorite_count":5,"up_vote_count":19,"down_vote_count":0,"view_count":1487,"score":19,"question_id":2410710,"title":"Why is the new Tuple type in .Net 4.0 a reference type (class) and not a value type (struct)","body":"<p>Does anyone know the answer and/or have an oppinion about it?</p>\n\n<p>Since tuples would normally not be very large I would assume it would make more sense to use structs than classes for these. What say you?</p>\n"},{"tags":["c#","performance","clr","gettype"],"answer_count":5,"favorite_count":6,"up_vote_count":28,"down_vote_count":0,"view_count":11660,"score":28,"question_id":686412,"title":"C# 'is' operator performance","body":"<p>I have a program that requires fast performance.  Within one of its inner loops, I need to test the type of an object to see whether it inherits from a certain interface.</p>\n\n<p>One way to do this would be with the CLR's built-in type-checking functionality.  The most elegant method there probably being the 'is' keyword:</p>\n\n<pre><code>if (obj is ISpecialType)\n</code></pre>\n\n<p>Another approach would be to give the base class my own virtual GetType() function which returns a pre-defined enum value (in my case, actually, i only need a bool).  That method would be fast, but less elegant.</p>\n\n<p>I have heard that there is an IL instruction specifically for the 'is' keyword, but that doesn't mean it executes fast when translated into native assembly.  Can anyone share some insight into the performance of 'is' versus the other method?</p>\n\n<p><strong>UPDATE:</strong>  Thanks for all the informed answers!  It seem a couple helpful points are spread out among the answers:  Andrew's point about 'is' automatically performing a cast is essential, but the performance data gathered by Binary Worrier and Ian is also extremely useful.  It would be great if one of the answers were edited to include <em>all</em> of this information.</p>\n"},{"tags":["c++","performance","function","object","functional-programming"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":140,"score":2,"question_id":13067432,"title":"Fast function objects in C++?","body":"<p>I'm writing a program for scientific computing and my foremost interest (after correctness) is speed. Recently I have noticed that I need readable code too. :)</p>\n\n<p>Instead of writing</p>\n\n<pre><code>for (int k=0;k!=10;k+=1)\n   array[k] = fun(a, k);\n</code></pre>\n\n<p>I'm considering writing</p>\n\n<pre><code>class fun_t {\nprivate:\n   type a;\npublic:\n   fun_t(type in) : a(in) {};\n\n   type operator() (int k) {\n      ...computation...\n   }\n};\n...\nfun_t fun(a);\nfor (int k=0;k!=10;k+=1)\n   array[k] = fun(k);\n</code></pre>\n\n<p>Will the function object style be as fast as the first example? Can I expect the same inlinings in both? Is there a better a way? (Note that I'm only presenting the idea here, this is not my actual code.)</p>\n"},{"tags":["c++","performance","void-pointers"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":98,"score":1,"question_id":3224467,"title":"What's the best way to make a bitwise function work for any type of integer input c++?","body":"<p>So I need to read flags in bits and set flags in bits. These bits are in various sizes of integer: int16, int32, int64, etc.</p>\n\n<p>I would like to have a function that does something like this:</p>\n\n<pre><code>static integertype function(integertype data, char startbit, char endbit);\n</code></pre>\n\n<p>I don't want to code what will be the same code to isolate bits from for different sizes of integers in separate but identical functions (for the multitude of bit functions I want to write).</p>\n\n<p>I thought about using a void pointer for the data so everything could run through one function. Is this a bad design? What about as far as efficiency goes? I have no concept of bad/good design due to my inexperience.</p>\n\n<pre><code>static int function(void *data, char startbit, char endbit)\n</code></pre>\n\n<p>These flags have to be looked at very often as this is for a data acquisition system. Would a void pointer implementation be reasonably efficient?</p>\n\n<p>I know premature optimization is bad, but I would like to know what things are generally less or more efficient than others so I can make good decisions.</p>\n\n<p>Thanks in advance for taking me to school.</p>\n"},{"tags":["python","performance","swap"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":126,"score":2,"question_id":13040891,"title":"Python Swap two digits in a number?","body":"<p>What is the fastest way to swap two digits in a number in Python?  I am given the numbers as strings, so it'd be nice if I could have something as fast as</p>\n\n<pre><code>string[j] = string[j] ^ string[j+1] \nstring[j+1] = string[j] ^ string[j+1]\nstring[j] = string[j] ^ string[j+1]\n</code></pre>\n\n<p>Everything I've seen has been much more expensive than it would be in C, and involves making a list and then converting the list back or some variant thereof.</p>\n"},{"tags":["performance","query","optimization","expressionengine"],"answer_count":6,"favorite_count":1,"up_vote_count":9,"down_vote_count":0,"view_count":139,"score":9,"question_id":13041463,"title":"What's the optimal amount of queries an ExpressionEngine page should load?","body":"<p>I saw @parscale tweet: <a href=\"https://twitter.com/parscale/status/260808185424781312\">How many queries are you happy with for a home page? When do you say this is Optimized?</a></p>\n\n<p>I saw responses that &lt; 50 is good, 30 or less is best, and 100+ is danger zone. Is there really any proper number? And if say you do have > 50 queries running on your pages, what are some ways to bring it down?</p>\n\n<p>I generally have sites that run the gamut that are under 50 queries and some more, though the \"more\" don't seem to be too slow, I'm always interested in making it faster. How?</p>\n"},{"tags":["java","performance","jsf","profiling","profiler"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":2,"view_count":26,"score":-2,"question_id":13066057,"title":"creating own web profiler","body":"<p>i need to create my own profiler which could profile some thing like netbeans profiler, which shows execution time of each method and class. i found a link which gives basics of  profiling web application. </p>\n\n<p><a href=\"http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/servletFilters/servlet-filters.html\" rel=\"nofollow\">http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/servletFilters/servlet-filters.html</a></p>\n\n<p>is it possible to use the above method and create a tool which can profile any war files. </p>\n"},{"tags":["android","performance","hierarchy"],"answer_count":1,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":379,"score":2,"question_id":4437796,"title":"hierarchy viewer results interpretation","body":"<p>Some days ago I've installed the updates for ADT Plugin in Eclipse and I've just tried the new <strong>Hierarchy viewer</strong> tool. Beside it took me a while to find that the measurements are calculated only at the first (parent) view, I have observed that the values shown are not the same every time, even if there are no changes in my application.</p>\n\n<p>For example, I've launched in the emulator one of my applications, loaded it in the Hierarchy viewer and got the next results: (I've wanted to include the screen shots, but my reputation doesn't allow me to post pictures yet, so I'll just write the values)</p>\n\n<p><strong>Measure:</strong> 175.340 ms<br>\n<strong>Layout:</strong> 5.179 ms<br>\n<strong>Draw:</strong> 47.115 ms       </p>\n\n<p>Then, without any changes, I launched again the application, and got the following results:</p>\n\n<p><strong>Measure:</strong> 98.696 ms<br>\n<strong>Layout:</strong> 4.819 ms<br>\n<strong>Draw:</strong> 50.923 ms </p>\n\n<p>Could someone tell me why there is such a big difference between the values of Measure, for example?</p>\n\n<p>Also, did someone know the meaning / the difference between the 3 values provided: <strong>Measure</strong>, <strong>Layout</strong> and <strong>Draw</strong>? Are each related with some specific attributs of the views?  </p>\n\n<p>The Android developers page doesn't provide too many explanations for that, and all I know is those values must be as small as possible.</p>\n"},{"tags":["c++","performance","thrust","code-timing"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":67,"score":2,"question_id":12869725,"title":"performance tuning a thrust application","body":"<p>I am running a small C++/thrust program (below) on my macbook pro w/ 9600M GT gpu and am interested in understanding where the time is spent in the function h, because the goal is to run this code as quickly as possible for larger values of NEPS.</p>\n\n<p>For that purpose, I have littered the function with clock() calls.</p>\n\n<p>The times printed indicate that almost all of the time is spent in thrust::reduce.\nIndeed, the reported time for thrust::reduce is several hundred times greater than that for thrust::transform, which invokes three calls to cosine per element.  Why?</p>\n\n<p>Naturally, I'm suspicious of the measured times.\nI inserted a 2nd call to thrust::reduce just to see if the time reported would be similar:  it's not.  The time reported for the 2nd call has much higher variance and is smaller.\nMore confusion:  why?</p>\n\n<p>I had also tried using thrust::transform_reduce (commented out) in place of the two kernel calls expecting that to run faster -- instead, it was 4% slower.  Why?</p>\n\n<p>Suggestions appreciated!</p>\n\n<pre><code>#include &lt;thrust/host_vector.h&gt;\n#include &lt;thrust/device_vector.h&gt;\n#include &lt;thrust/sequence.h&gt;\n#include &lt;iostream&gt;\n\n#include &lt;stdio.h&gt;\n#include &lt;stdint.h&gt;\n\n\n float NEPS = 6.0;\n __device__ float EPS;\n __device__ float SQEPS;\n\n __device__ float CNV_win;\n __device__ float CNV_dt;\n int CNV_n;\n float EU_dt;\n\n__host__ __device__ float f(float x,float t){\n    return x*cos(t)+x*cos(t/SQEPS)+cos(t/EPS);\n}\n\nstruct h_functor\n{\n  const float x, t;\n  h_functor(float _x, float _t) : x(_x),t(_t) {}\n  __host__ __device__\n  float operator()(const float &amp; t_f) const {\n    return f(x,   t-CNV_win+CNV_dt*(t_f+1)   )*CNV_dt;\n  } \n};\n\n\nclock_t my_clock() __attribute__ ((noinline));\nclock_t my_clock() {\n  return clock();\n}\nfloat h(float x,float t){\n    float sum;\n\n    sum = CNV_dt*(f(x,t-CNV_win/2)+f(x,t+CNV_win/2))/2;\n    clock_t start = my_clock(), diff1, diff2, diff3, diff4, diff5;\n    thrust::device_vector&lt;float&gt; t_f(CNV_n-2);\n    diff1 = my_clock() - start;\n    /* initialize t_f to 0.. CNV_n-3 */\n    start = my_clock();\n    thrust::sequence(t_f.begin(), t_f.end());\n    diff2 = my_clock() - start;\n\n    start = my_clock();\n    thrust::transform(t_f.begin(), t_f.end(), t_f.begin(), h_functor(x,t));\n    diff3 = my_clock() - start;\n    start = my_clock();\n    sum += thrust::reduce(t_f.begin(), t_f.end());\n    diff4 = my_clock() - start;\n    start = my_clock();\n    sum += thrust::reduce(t_f.begin(), t_f.end());\n    diff5 = my_clock() - start;\n#define usec(d) (d)\n    fprintf(stderr, \"Time taken %ld %ld %ld %ld %ld usecs\\n\", usec(diff1), usec(diff2), usec(diff3), usec(diff4), usec(diff5));\n        /* a bit slower, surprisingly:\n       sum += thrust::transform_reduce(t_f.begin(), t_f.end(), h_functor(x,t), 0, thrust::plus&lt;float&gt;());\n       */\n\n    return sum;\n}\nmain(int argc, char ** argv) {\n  if (argc &gt;= 1) NEPS = strtod(argv[1], 0);\n  fprintf(stderr, \"NEPS = %g\\n\", NEPS);\n\n  EPS= powf(10.0,-NEPS);\n  SQEPS= powf(10.0,-NEPS/2.0);\n  CNV_win= powf(EPS,1.0/4.0);\n  CNV_dt = EPS;\n  CNV_n = powf(EPS,-3.0/4.0);\n  EU_dt = powf(EPS,3.0/4.0);\n\n  cudaMemcpyToSymbol(CNV_win, &amp;CNV_win, sizeof(float));\n  cudaMemcpyToSymbol(CNV_dt, &amp;CNV_dt, sizeof(float));\n  cudaMemcpyToSymbol(SQEPS, &amp;SQEPS, sizeof(float));\n  cudaMemcpyToSymbol(EPS, &amp;EPS, sizeof(float));\n\n  float x=1.0;\n  float t = 0.0;\n  int n = floor(1.0/EU_dt);\n  fprintf(stderr, \"CNV_n = %d\\n\", CNV_n);\n  while (n--) {\n    float sum = h(x,t);\n    x=x+EU_dt*sum;\n    t=t+EU_dt;\n  }\n  printf(\"%f\\n\",x);\n}\n</code></pre>\n"},{"tags":["iphone","performance","opengl-es","computer-vision"],"answer_count":1,"favorite_count":5,"up_vote_count":4,"down_vote_count":0,"view_count":2292,"score":4,"question_id":4270737,"title":"Convolving an image with OpenGL ES on iPhone: possible?","body":"<p>I've googled around a few times, but I have not gotten a straight answer.  I have a matrix that I would like to convolve with a discrete filter (e.g. the Sobel operator for edge detection).  Is it possible to do this in an accelerated way with OpenGL ES on the iPhone?</p>\n\n<p>If it is, how how?  If it is not, are there other high-performance tricks I can use to speed up the operation?  Wizardly ARM assembly operations that can do it fast?  Ultimately I want to perform as fast of a convolution as possible on an iPhone's ARM processor.</p>\n"},{"tags":["java","performance","ssd","ext4"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":72,"score":1,"question_id":13043849,"title":"Whats the fastest way to write a journal to an SSD in Java?","body":"<p>I have small transactions which I need to sync to the filesystem (ext4) onto a SSD within a java program.</p>\n\n<p>What is the fastest way to write these transactions sequencial into a file, if I need to sync with the filesystem after each transaction?\nThe data per transaction is realy small, but it would be ok to write more (to fill a page for example), if that increases transactions/s.</p>\n\n<p>After the data was written once, it is only read.</p>\n\n<p>Also, are there any tweaks to the filesystem that increase the performance?\nIs there a stable filesystem that is suited better for that task?</p>\n\n<p><strong>UPDATE:</strong> This seems to be a filesystem problem. Ext4 is much slower than Ext3, when using sync?\nSuggestion?\n<strong>UPDATE</strong> The solution to the problem is, preallocation the file (in java _file.setLength(size) ). This will cause on ext4 that the space is preallocated and all metadata is writen once. After this, writeing to the file will only edit the user data and metadata is keeped unchanged.\nThis causeed a speedup by factor 10 in my case.</p>\n"},{"tags":["mysql","performance","query","select-query"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":33,"score":1,"question_id":13048951,"title":"I need to speed up specific mysql query on large table","body":"<p>Hi I know there is a lot of topics dedicated to query optimizing strategies, but this one is so specific I couldnt find the answer anywhere on the interenet.</p>\n\n<p>I have large table of product in eshop (appx. 180k rows) and the table has 65 columns. Yeah yeah I know its quite a lot, but I store there information about books, dvds, bluerays and games.</p>\n\n<p>Still I am not considering a lot of cols into query, but the select is still quite tricky. There are many conditions that need to be considered and compared. Query below</p>\n\n<pre><code>SELECT *\nFROM products\nWHERE production = 1 \nAND publish_on &lt; '2012-10-23 11:10:06' \nAND publish_off &gt; '2012-10-23 11:10:06' \nAND price_vat &gt; '0.5' \nAND ean &lt;&gt; ''\nAND publisher LIKE '%Johnny Cash%'\nORDER BY bought DESC, datec DESC, quantity_storage1 DESC, quantity_storege2 DESC, quantity_storage3 DESC\nLIMIT 0, 20\n</code></pre>\n\n<p>I have already tried to put there indexes one by one on cols in where clause and even in order by clause, then I tried to create compound index on (production, publish_on, publish_off, price_vat, ean). </p>\n\n<p>Query is still slow (couple of seconds) and it need to be fast since its eshop solution and people are leaving as they are not getting their results fast. And I am still not counting the time I need to perform the search for all found rows so I can make paging.</p>\n\n<p>I mean, the best way to make it quick is to simplify the query, but all the conditions and sorting is a must in this case. </p>\n\n<p>Can anyone help with this kind of issue? Is it even possible to speed this kind of query up, or is there any other way how I can for example simplify the query and leave the rest on php engine to sort the results.. </p>\n\n<p>Oh, Iam really clueless in this.. Share your wisdom peple, please...</p>\n\n<p>Many thanks in advance</p>\n"},{"tags":["performance","excel","vba","optimization","excel-vba"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":7,"view_count":140,"score":-7,"question_id":13039069,"title":"Optimize VBA Code","body":"<p>Can somebody help me optimize this code.  It is a macro to input formulas into a was-is spreadsheet (deltas, rows subtotals, column subtotals).  It works well with small amounts of data, but with large datadumps it takes upward of 15 minutes and sometimes bombs out excel.  I wrote this code myself, so i am sure there are many ways to make it more efficient.  </p>\n\n<p>Here is the code (Sorry about format):</p>\n\n<pre><code>'Variables\nDim Rcnt As Long\nDim Ccnt As Long\nDim i As Long\nDim j As Long\nDim n As Long\nDim rValues As Long\nDim msgComplete As Long\n\n 'Delete extra rows\nActiveSheet.UsedRange\nRcnt = Cells.SpecialCells(xlLastCell).Row\nCcnt = Cells.SpecialCells(xlLastCell).Column\n\nFor n = 1 To Ccnt\n    If IsError(Application.VLookup(\"Values\", Columns(n), 1, False)) Then\n        'do nothing\n    Else\n        If WorksheetFunction.Match(\"Values\", Columns(n), 0) = 1 Then\n            MsgBox (\"Error.  Did not paste updated pivot table data.  Exiting Macro.\")\n            Exit Sub\n        Else\n            Rows(1 &amp; \":\" &amp; (WorksheetFunction.Match(\"Values\", Columns(n), 0)) - 1).Delete\n        End If\n    End If\nNext n\n\n'Copy down pivot data\nActiveSheet.UsedRange\nRcnt = Cells.SpecialCells(xlLastCell).Row\nCcnt = Cells.SpecialCells(xlLastCell).Column\n\nFor n = WorksheetFunction.Match(\"Values\", Rows(1), 0) - 1 To 1 Step -1\n    For i = 1 To Rcnt\n        If Cells(i, n).Value = \"\" Then\n            Cells(i, n) = Cells(i, n).Offset(-1, 0)\n            Cells(i, n).Font.Color = RGB(255, 255, 255)\n        Else\n            'do nothing\n        End If\n    Next i\nNext n\n\n'Input delta formulas\nFor n = WorksheetFunction.Match(\"Values\", Rows(1), 0) To 1 Step -1\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"BAC Delta\") _\n        Or InStr(1, Cells(i, n), \"EAC Delta\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=R[-1]C-R[-2]C\"\n            Next j\n        End If\n    Next i\nNext n\n\n'Input grand total sum formulas\nFor n = WorksheetFunction.Match(\"Values\", Rows(1), 0) To 1 Step -1\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Revised BAC\") Or InStr(1, Cells(i, n), \"BAC Delta\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Or InStr(1, Cells(i, n), \"EAC Delta\") Then\n            Cells(i, Ccnt) = \"=Sum(\" &amp; Cells(i, Ccnt).Offset(0, -1).Address(False, False) &amp; \":\" _\n            &amp; Cells(i, WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1).Address(False, False) &amp; \")\"\n        End If\n    Next i\nNext n\n\n'Input sumifs formulas\n n = 10\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(n).Offset(0, -2).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -2).Address &amp; \",\" &amp; Columns(n).Offset(0, -3).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -3).Address &amp; \",\" &amp; Columns(n).Offset(0, -4).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -4).Address &amp; \",\" &amp; Columns(n).Offset(0, -5).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -5).Address &amp; \",\" &amp; Columns(n).Offset(0, -6).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -6).Address &amp; \",\" &amp; Columns(n).Offset(0, -7).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -7).Address &amp; \",\" &amp; Columns(n).Offset(0, -8).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -8).Address &amp; \",\" &amp; Columns(n).Offset(0, -9).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -9).Address &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 9\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(n).Offset(0, -2).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -2).Address &amp; \",\" &amp; Columns(n).Offset(0, -3).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -3).Address &amp; \",\" &amp; Columns(n).Offset(0, -4).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -4).Address &amp; \",\" &amp; Columns(n).Offset(0, -5).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -5).Address &amp; \",\" &amp; Columns(n).Offset(0, -6).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -6).Address &amp; \",\" &amp; Columns(n).Offset(0, -7).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -7).Address &amp; \",\" &amp; Columns(n).Offset(0, -8).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -8).Address &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 8\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(n).Offset(0, -2).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -2).Address &amp; \",\" &amp; Columns(n).Offset(0, -3).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -3).Address &amp; \",\" &amp; Columns(n).Offset(0, -4).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -4).Address &amp; \",\" &amp; Columns(n).Offset(0, -5).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -5).Address &amp; \",\" &amp; Columns(n).Offset(0, -6).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -6).Address &amp; \",\" &amp; Columns(n).Offset(0, -7).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -7).Address &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 7\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(n).Offset(0, -2).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -2).Address &amp; \",\" &amp; Columns(n).Offset(0, -3).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -3).Address &amp; \",\" &amp; Columns(n).Offset(0, -4).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -4).Address &amp; \",\" &amp; Columns(n).Offset(0, -5).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -5).Address &amp; \",\" &amp; Columns(n).Offset(0, -6).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -6).Address &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 6\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(n).Offset(0, -2).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -2).Address &amp; \",\" &amp; Columns(n).Offset(0, -3).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -3).Address &amp; \",\" &amp; Columns(n).Offset(0, -4).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -4).Address &amp; \",\" &amp; Columns(n).Offset(0, -5).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -5).Address &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 5\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(n).Offset(0, -2).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -2).Address &amp; \",\" &amp; Columns(n).Offset(0, -3).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -3).Address &amp; \",\" &amp; Columns(n).Offset(0, -4).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -4).Address &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 4\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(n).Offset(0, -2).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -2).Address &amp; \",\" &amp; Columns(n).Offset(0, -3).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -3).Address &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 3\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(n).Offset(0, -2).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -2).Address &amp; \", \" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 2\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)), \" &amp; Columns(n).Offset(0, -1).Address &amp; _\n                \",\" &amp; Cells(i, n).Offset(0, -1).Address &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 1\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(n).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")-2),LEFT(\" &amp; Cells(i, n).Address &amp; _\n                \",search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")-2)),\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n\nn = 1\nIf IsError(Application.Find(\"Values\", Cells(1, n))) Then\n    For i = Rcnt To 1 Step -1\n        If InStr(1, Cells(i, n), \"Total Current BAC\") _\n        Or InStr(1, Cells(i, n), \"Total Revised BAC\") _\n        Or InStr(1, Cells(i, n), \"Total Current EAC\") _\n        Or InStr(1, Cells(i, n), \"Total Revised EAC\") Then\n            For j = Ccnt - 1 To WorksheetFunction.Match(\"Values\", Rows(1), 0) + 1 Step -1\n                Cells(i, j) = \"=Sumifs(\" &amp; Columns(j).Address(False, False) &amp; \",\" &amp; Columns(WorksheetFunction.Match(\"Values\", Rows(1), 0)).Address &amp; _\n                \",If(not(iserror(search(\"\"Current\"\" ,\" &amp; Cells(i, n).Address &amp; \"))), right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Current\"\",\" &amp; Cells(i, n).Address &amp; \")+1)),right(\" &amp; Cells(i, n).Address &amp; _\n                \",(len(\" &amp; Cells(i, n).Address &amp; \")-search(\"\"Revised\"\",\" &amp; Cells(i, n).Address &amp; \")+1))))\"\n            Next j\n        End If\n    Next i\nElse\n    'do nothing\nEnd If\n</code></pre>\n"},{"tags":["iphone","connection","performance","ios-simulator"],"answer_count":4,"favorite_count":10,"up_vote_count":22,"down_vote_count":0,"view_count":4332,"score":22,"question_id":2593971,"title":"iPhone Simulator - SImulate a Slow Connection?","body":"<p>Is there a way to slow down the internet connection to the iPhone Simulator, so as to mimic how the App might react when you are in a slow spot on the cellular network?</p>\n"},{"tags":["performance","linq","select","for-loop"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":44,"score":0,"question_id":13064224,"title":"Performance for vs. select","body":"<p>I read in Bill Wagners Book \"Effective C#\" that one should favor more declarative <em>select</em> statements over traditional loops (<em>for</em>). </p>\n\n<p>For example:</p>\n\n<pre><code> int[] foo = new int[1000];\n for (int i = 0; i &lt; foo.Length; i++)\n     foo[i] = i * i;\n</code></pre>\n\n<p>is traditional imperative code, whereas this would be declarative Linq code:</p>\n\n<pre><code> int[] foo2 = (from i in Enumerable.Range(0, 1000)\n               select i * i).ToArray();\n</code></pre>\n\n<p>Being an old-fashioned programmer, I prefer the first version. </p>\n\n<p>The question is how about performance? I suppose the first version is also faster.</p>\n"},{"tags":["mysql","sql","performance","many-to-many"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":53,"score":2,"question_id":13063772,"title":"Is it better to use INNER JOIN or EXISTS to find belonging to several in m2m relation?","body":"<p>Given m2m relation: <strong>items-categories</strong> I have three tables: </p>\n\n<ul>\n<li><strong>items</strong>, </li>\n<li><strong>categories</strong> and </li>\n<li><strong>items_categories</strong> that hold references to both</li>\n</ul>\n\n<p>I want to find an item belonging to <strong>all given</strong> category sets:</p>\n\n<pre><code>Find Item \nbelonging to a category in [1,3,6] \nand belonging to a category in [7,8,4] \nand belonging to a category in [12,66,42]\nand ...\n</code></pre>\n\n<p>There are two ways I can think of to accomplish this in mySQL.</p>\n\n<p><strong>OPTION A: INNER JOIN:</strong></p>\n\n<pre><code>SELECT id from items \nINNER JOIN category c1 ON (item.id = c1.item_id)\nINNER JOIN category c2 ON (item.id = c2.item_id)\nINNER JOIN category c3 ON (item.id = c3.item_id)\n...\nWHERE\nc1.category_id IN [1,3,6] AND\nc2.category_id IN [7,8,4] AND\nc3.category_id IN [12,66,42] AND\n...;\n</code></pre>\n\n<p><strong>OPTION B: EXISTS:</strong></p>\n\n<pre><code>SELECT id from items\nWHERE\nEXISTS(SELECT category_id FROM category WHERE category.item_id = id AND category_id in [1,3,6] AND\nEXISTS(SELECT category_id FROM category WHERE category.item_id = id AND category_id in [7,8,4] AND\nEXISTS(SELECT category_id FROM category WHERE category.item_id = id AND category_id in [12,66,42] AND\n...;\n</code></pre>\n\n<p>Both options work. The question is: <strong>Which is the fastest / most optimal for large item table?</strong> Or is there an OPTION C I am missing?</p>\n"},{"tags":["javascript","jquery","performance","jquery-selectors","jquery-context"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":53,"score":2,"question_id":13057781,"title":"jQuery context slows down search","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/2421782/performance-of-jquery-selector-with-context\">Performance of jQuery selector with context</a>  </p>\n</blockquote>\n\n\n\n<p>In the <a href=\"http://api.jquery.com/jQuery/\" rel=\"nofollow\">jQuery DOCS</a> it says </p>\n\n<blockquote>\n  <p>By default, selectors perform their searches within the DOM starting\n  at the document root. However, an alternate context can be given for\n  the search by using the optional second parameter to the $() function.</p>\n</blockquote>\n\n<p>Based on that my understanding is that a selection using a <code>context</code> passed in as the second parameter should be faster then the same selection without the <code>context</code> passed in. However I ran some tests and it seems as if this isn't the case, or at least isn't always the case.</p>\n\n<p>To elaborate, I originally wanted to see if searching for multiple elements at once (<code>$(\"div1, #div2\")</code>) was faster then searching for the two separately (<code>$(\"#div1\") $(\"div2\")</code>). I then decided to test it with the <code>context</code> and without to see how much faster it was with the <code>context</code>, but was surprised when it turned out that the <code>context</code> seemed to be slowing it down.</p>\n\n<p>For example given the following basic HTML markup</p>\n\n<pre><code>&lt;div id=\"testCnt\"&gt;\n    &lt;div id=\"Div0\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div1\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div2\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div3\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div4\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div5\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div6\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div7\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div8\"&gt;&lt;/div&gt;\n    &lt;div id=\"Div9\"&gt;&lt;/div&gt;\n&lt;/div&gt;\n</code></pre>\n\n<p>And the following JavaScript (jQuery 1.8.2, and tested using FireBug) </p>\n\n<pre><code>$(function () {    \n    var $dvCnt = $('#testCnt');\n    var dvCnt = $dvCnt[0];\n\n    console.time('Individual without cache');\n    for (var i = 0; i &lt; 10000; i++) {\n        $('#Div0').text('Test');\n        $('#Div1').text('Test');\n        $('#Div2').text('Test');\n        $('#Div3').text('Test');\n        $('#Div4').text('Test');\n        $('#Div5').text('Test');\n        $('#Div6').text('Test');\n        $('#Div7').text('Test');\n        $('#Div8').text('Test');\n        $('#Div9').text('Test');\n\n    }\n    console.timeEnd('Individual without cache');\n\n    console.time('Individual with $cache');\n    for (var i = 0; i &lt; 10000; i++) {\n        $('#Div0', $dvCnt).text('Test');\n        $('#Div1', $dvCnt).text('Test');\n        $('#Div2', $dvCnt).text('Test');\n        $('#Div3', $dvCnt).text('Test');\n        $('#Div4', $dvCnt).text('Test');\n        $('#Div5', $dvCnt).text('Test');\n        $('#Div6', $dvCnt).text('Test');\n        $('#Div7', $dvCnt).text('Test');\n        $('#Div8', $dvCnt).text('Test');\n        $('#Div9', $dvCnt).text('Test');\n\n    }\n    console.timeEnd('Individual with $cache');\n\n    console.time('Individual with DOM cache');\n    for (var i = 0; i &lt; 10000; i++) {\n        $('#Div0', dvCnt).text('Test');\n        $('#Div1', dvCnt).text('Test');\n        $('#Div2', dvCnt).text('Test');\n        $('#Div3', dvCnt).text('Test');\n        $('#Div4', dvCnt).text('Test');\n        $('#Div5', dvCnt).text('Test');\n        $('#Div6', dvCnt).text('Test');\n        $('#Div7', dvCnt).text('Test');\n        $('#Div8', dvCnt).text('Test');\n        $('#Div9', dvCnt).text('Test');\n\n    }\n    console.timeEnd('Individual with DOM cache');\n\n\n    console.time('Multiple without cache');\n    for (var i = 0; i &lt; 10000; i++) {\n        $('#Div0,#Div1 ,#Div2 ,#Div3 ,#Div4 ,#Div5 ,#Div6, #Div7, #Div8, #Div9').text('Test');\n    }\n    console.timeEnd('Multiple without cache');\n\n    console.time('Multiple with $cache');\n    for (var i = 0; i &lt; 10000; i++) {\n        $('#Div0,#Div1 ,#Div2 ,#Div3 ,#Div4 ,#Div5 ,#Div6, #Div7, #Div8, #Div9', $dvCnt).text('Test');\n    }\n    console.timeEnd('Multiple with $cache');\n\n    console.time('Multiple with DOM cache');\n    for (var i = 0; i &lt; 10000; i++) {\n        $('#Div0,#Div1 ,#Div2 ,#Div3 ,#Div4 ,#Div5 ,#Div6, #Div7, #Div8, #Div9', dvCnt).text('Test');\n    }\n    console.timeEnd('Multiple with DOM cache');\n});\n</code></pre>\n\n<p>Here's a <a href=\"http://jsbin.com/ehumic/3/edit\" rel=\"nofollow\">jsbin</a></p>\n\n<p>I'm getting something like the following results</p>\n\n<p>Individual without cache: 11490ms<br>\nIndividual with $cache: 13315ms<br>\nIndividual with DOM cache: 14487ms   </p>\n\n<p>Multiple without cache: 7557ms<br>\nMultiple with $cache: 7824ms<br>\nMultiple with DOM cache: 8589ms   </p>\n\n<p>Can someone shed some insight on whats going on? Specifically why the search is slowing down when the jQuery context is passed in? </p>\n\n<p><em>EDIT:</em></p>\n\n<p>Most of the anwsers here (as well as <a href=\"http://stackoverflow.com/q/2421782/384985\">Performance of jQuery selector with context</a>) basically say that that either the DOM in this example is too small to really gain much or that selecting by <code>ID</code> is going to be fast regardless. I understand both points, the main point of my question is why would the <code>context</code> <strong>slow</strong> down the search, the size of the <code>DOM</code> shouldn't make a difference for that, and neither should the fact that searching by ID is already very fast. </p>\n\n<p><a href=\"http://stackoverflow.com/users/1490904/pebbl\">@pebble</a> suggested that the reason that its slower is because jQuery can't use the native browser methods (<code>getElementByID</code>), this seems to make sense to me, but then why is it faster to search for multiple elements in one selection? </p>\n\n<p>Anyway I dumped the tests into a <a href=\"http://jsperf.com/jquery-selection-with-context\" rel=\"nofollow\">jsPerf</a> adding cases to search by class and was again surprised to see that the search for multiple classes with a cache this time was the fastest.</p>\n"},{"tags":["c++","performance","visual-c++","binary","byte"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":71,"score":1,"question_id":13063306,"title":"C++: Fastest way to merge and split a uint into 4 bytes","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/741212/fastest-method-to-split-a-32-bit-number-into-bytes-in-c\">Fastest Method to Split a 32 Bit number into Bytes in C++</a>  </p>\n</blockquote>\n\n\n\n<p>Is there a way to merge/split a uint into 4 bytes quicker than what I'm currently doing? Maybe some inline assembler that has a native opcode that can do it in a single instruction?</p>\n\n<pre><code>// merge into x0\nunsigned int x0 = (data[i] &lt;&lt; 24) | (data[i+1] &lt;&lt; 16) | (data[i+2] &lt;&lt; 8) | data[i+3]; \n\n\n// split x0\noutputBuffer[i] = (x0 &gt;&gt; 24);\noutputBuffer[i+1]  = (x0 &gt;&gt; 16) &amp; 0xFF;\noutputBuffer[i+2]  = (x0 &gt;&gt; 8) &amp; 0xFF;\noutputBuffer[i+3]  = (x0) &amp; 0xFF;\n</code></pre>\n"},{"tags":["c#",".net","performance","sqlite","indexing"],"answer_count":2,"favorite_count":0,"up_vote_count":6,"down_vote_count":0,"view_count":98,"score":6,"question_id":13056193,"title":"Escape wildcards (%, _) in SQLite LIKE without sacrificing index use?","body":"<p>I have a couple of issues with SQLite query. Actually I start thinking that SQLite is not designed for tables with more then 10 rows, really, SQLite is a nightmare.</p>\n\n<p>The following query</p>\n\n<pre><code>SELECT * FROM [Table] WHERE [Name] LIKE 'Text%'\n</code></pre>\n\n<p>It works fine. <code>EXPLAIN</code> shows that the index is used and result is returned after about <code>70ms</code>.</p>\n\n<p>Now I need to run this query from .NET SQLite driver, so I'm changing query</p>\n\n<pre><code>SELECT * FROM [Table] WHERE [Name] LIKE @Pattern || '%'\n</code></pre>\n\n<p>Index is not used. When I run the following query in any SQLite tool the index is not used as well</p>\n\n<pre><code>SELECT * FROM [Table] WHERE [Name] LIKE 'Text' || '%'\n</code></pre>\n\n<p>So I guess SQLite doesn't have any kind of preprocessing logic implemented.</p>\n\n<p>OK. Let's try to solve it, I'm still binding variables and doing the following</p>\n\n<pre><code>SELECT * FROM [Table] WHERE [Name] LIKE @Pattern\n</code></pre>\n\n<p>But now I append <code>%</code> wildcard symbol to the end of my pattern string, like this</p>\n\n<pre><code>command.Parameters.Add(new SQLiteParameter(\"@Pattern\", pattern + '%'));\n</code></pre>\n\n<p>It works very slow. I can't say why, because when I run this query from SQLite tool it works fine, however when I bind this variable from .NET code it works slow.</p>\n\n<p>OK. I'm still trying to solve this. I'm getting rid of the pattern parameter binding and building this condition dynamically.</p>\n\n<pre><code>pattern = pattern.Replace(\"'\", \"''\");\npattern = pattern.Replace(\"%\", \"\\\\%\");\nwhere = string.Format(\"LIKE '{0}%' ESCAPE '\\\\'\", pattern);\n</code></pre>\n\n<p>Index is not used again. It's not used because of <code>ESCAPE</code>. I see that when I run </p>\n\n<pre><code>EXPLAIN QUERY PLAN SELECT * FROM [Table] WHERE [Name] LIKE 'Text%' ESCAPE '\\'\n</code></pre>\n\n<p>As soon as I remove <code>ESCAPE</code> it starts using index again and the query finishes in 60-70ms.</p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>Here are the results.</p>\n\n<pre><code>EXPLAIN QUERY PLAN\nSELECT * FROM [RegistryValues]\nWHERE\n     [ValueName] LIKE 'windir%' ESCAPE '\\' \n</code></pre>\n\n<p><code>SCAN TABLE RegistryValues (~3441573 rows)</code></p>\n\n<p>and the one without <code>ESCAPE</code></p>\n\n<pre><code>EXPLAIN QUERY PLAN\nSELECT * FROM [RegistryValues]\nWHERE\n     [ValueName] LIKE 'windir%'\n</code></pre>\n\n<p><code>SEARCH TABLE RegistryValues USING INDEX IdxRegistryValuesValueNameKeyIdKeyHiveFileId (ValueName&gt;? AND ValueName&lt;?) (~31250 rows)</code></p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>Just found this</p>\n\n<p><a href=\"http://www.sqlite.org/optoverview.html\" rel=\"nofollow\">http://www.sqlite.org/optoverview.html</a></p>\n\n<p><strong>4.0 The LIKE optimization</strong></p>\n\n<p><code>The ESCAPE clause cannot appear on the LIKE operator</code></p>\n\n<p>So what should I do then?</p>\n\n<p>Do I understand it right? I can't search string containing wildcards using <code>LIKE</code> operator in <code>SQLite</code>. By saying wildcards I mean <code>_</code> <code>%</code> <code>^</code> <code>!</code></p>\n\n<p>It's impossible simply because I can't escape them.\nActually I can, but I can't use indexes in this case, so the query will not be efficient.</p>\n\n<p>Am I right?</p>\n"},{"tags":["php","performance","oracle"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":54,"score":1,"question_id":12967784,"title":"Define value edit by another php file","body":"<p>I have 2 questions</p>\n\n<p>1.) how to write <code>update_defile($array_value){...}</code> function?</p>\n\n<p>define_file.php</p>\n\n<pre><code>&lt;?php\n  define(\"FIST_NAME\", \"something1\");\n  define(\"LAST_NAME\", \"something2\");\n  define(\"ADDRESS\", \"something3\");\n?&gt;\n</code></pre>\n\n<p>\"something\" is not a constant value that can be change every method <code>Call(update_defile($array_value)</code></p>\n\n<p>value set</p>\n\n<pre><code>$array_value = (\"FIST_NAMe\" =&gt; \"duleep\", \"LAST_NAME\" =&gt; \"dissnayaka\", \"AGE\" =&gt; \"28\" );\n</code></pre>\n\n<p>after call method(update_defile($array_value){.....}) \"define_file.php\"\nfile want to be look like bellow</p>\n\n<pre><code>&lt;?php\n  define(\"FIST_NAME\", \"duleep\");\n  define(\"LAST_NAME\", \"dissnayaka\");\n  define(\"ADDRESS\", \"something3\");\n  define(\"AGE\", \"28\");\n?&gt;\n</code></pre>\n\n<p>2). </p>\n\n<p>My datbase is Oracle. I already saved configuration value in the data base but frequently use these configuration value for my application. So i get value form database and save in the define_file.php as increase performance(down rate database call) but I'm not sure i can increase performance keep configuration value in the PHP file please explain. what is the best way increase performance my application and other alternative solutions welcome.</p>\n"},{"tags":["html","css","performance","pagespeed"],"answer_count":3,"favorite_count":2,"up_vote_count":6,"down_vote_count":0,"view_count":3159,"score":6,"question_id":1800137,"title":"Google Page Speed - what do these messages mean?","body":"<p>I ran the Google Page Speed Firefox extension on a few pages, and under \"efficient CSS selectors\" it listed various things that are inefficient in my CSS.</p>\n\n<p>But some of the messages seem a bit cryptic - what do these (in bold) mean:</p>\n\n<blockquote>\n  <p>div#menu h3.soon small<br>\n  <strong>Tag key with 2 descendant selectors and ID overly qualified with tag and Class overly qualified with tag</strong></p>\n  \n  <p>table.data tr:nth-child(2n) td<br>\n  <strong>Tag key with 2 descendant selectors and Class overly qualified with tag</strong></p>\n  \n  <p>table.data tr.disabled td<br>\n  <strong>Tag key with 2 descendant selectors and Class overly qualified with tag and Class overly qualified with tag</strong></p>\n</blockquote>\n\n<p>I'm assuming they think descendant selectors are bad but there are lots of \"overly qualified\" as well. I probably won't go to too much effort fixing all these up (there are many) but it would be nice to know what Google actually means here!</p>\n"},{"tags":["performance","memory-management","language-agnostic","distributed-computing","bandwidth"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":46,"score":2,"question_id":13062114,"title":"What is bandwidth demand?","body":"<p>What does bandwidth demand mean?\nI've seen it being used in this paragraph: </p>\n\n<blockquote>\n  <p>\"Memory must be distributed among the processors rather than\n  centralized; otherwise the memory system would not be able to support\n  the <strong>bandwidth demands</strong> of a large number of processors without\n  incurring long access latency\"</p>\n</blockquote>\n"},{"tags":["performance","selenium","web","web-performance-test"],"answer_count":0,"favorite_count":2,"up_vote_count":0,"down_vote_count":0,"view_count":31,"score":0,"question_id":13058907,"title":"web performance testing that renders javascript","body":"<p>I would like to load test pages AND render javascript. It seems there are 3 categories of applications which might solve this problem, and all three miss the mark.</p>\n\n<p>1) Jmeter, apachebench, tsung, Grinder, Iago\nWhy it wont work: It doesn't render javascript. These are handy tools, but they won't work for this purpose.</p>\n\n<p>2) Watir, Selenium\nThese tools are excellent, they use real browsers and render javascript / ajax, but alas, they are designed mostly for functional testing, and not performance testing. You could create your own app to use these things for a load test, but it would be a massive pain in the ass to collect all of the performance metrics and aggregate them. </p>\n\n<p>If only there was a combination of the first two types of web performance tests, that would be great.</p>\n\n<p>The third option, solves this problem, but unfortunately you have to pay for it.\n3) Web load testing services\nServices like Keynote Load Pro, BrowserMob and others are great, and they solve the problem of using real browsers and rendering JavaScript. The only problem is, I don't want to pay $300 ever time I run a damn load test (this is an exaggeration, but not really, depending on how many virtual users you use).</p>\n\n<p>So that option won't work, unless I want to hemorrhage money.</p>\n\n<p>Isn't there a test harness out there that solves this problem? It seems like a big gaping hole where there is a need for an open source tool that no one has solved yet. The commercial companies dominate this space (and those who want to employ a full time developer to write a selenium performance test framework).</p>\n"}]}
{"total":25592,"page":7,"pagesize":100,"questions":[{"tags":["java","performance","memory-management"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":47,"score":2,"question_id":13061234,"title":"Storing Data in a Variable vs Inline Arithmetic","body":"<p>Is there any difference between these two code snippets in terms of memory usage and performance/overhead?</p>\n\n<pre><code>final float xPos = (CAMERA_WIDTH / 2) - (mSprite.getWidth() / 2);\nfinal float yPos = (CAMERA_HEIGHT / 2) - (mSprite.getHeight() / 2);\nmSprite.setPosition(xPos, yPos);\n</code></pre>\n\n<p>and the other case:</p>\n\n<pre><code>mSprite.setPosition(((CAMERA_WIDTH / 2) - (mSprite.getWidth() / 2)), ((CAMERA_HEIGHT / 2) - (mSprite.getHeight() / 2)));\n</code></pre>\n\n<p>The only difference I can see is that the first snippet is storing the variable in what I assume to be a different area of memory than the second snippet, but I'm not very familiar with Java memory allocation (I'm more of a C/C++ person). </p>\n\n<p>My question is: is there any benefit to one way or the other? Does using the <code>final</code> keyword in the first example affect it at all?</p>\n\n<p>Thank you!</p>\n"},{"tags":["performance","parallel-processing","cuda"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":66,"score":3,"question_id":13060620,"title":"A couple of CUDA-performance questions","body":"<p>This is the first time i ask question here so thanks very much in advance and please forgive my ignorance. And also I've just started to CUDA programming.</p>\n\n<p>Basically, i have a bunch of points, and i want to calculate all the pair-wise distances. Currently my kernel function just holds on one point, and iteratively read in all other points (from global memory), and conduct the calculation. Here's some of my confusions:</p>\n\n<ul>\n<li><p>I'm using a Tesla M2050 with 448 cores. But my current parallel version (kernel&lt;&lt;&lt;128,16,16>>>) achieves a much higher parallelism (about 600x faster than kernel&lt;&lt;&lt;1,1,1>>>). Is it possibly due to the multithreading thing or pipeline issue, or they actually indicate the same thing?</p></li>\n<li><p>I want to further improve the performance. So i figure to use shared memory to hold some input points for each multiprocessing block. But the new code is just as fast. What's the possible cause? Could it be related to the fact that i set too many threads?</p></li>\n<li><p>Or, is it because i have a if-statement in the code? The thing is, i only consider and count the short distances, so i have a statement like (if dist &lt; 200). How much should i worry about this one?</p></li>\n</ul>\n\n<p>A million thanks!\nBin</p>\n"},{"tags":["python","performance","matplotlib"],"answer_count":1,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":90,"score":3,"question_id":13046127,"title":"matplotlib very slow. Is it normal?","body":"<p>I am creating a couple of pdf plots with matplotlib which is composed of 400 subplots. Each one has only 5 data points. It takes 420 s on a good computer to save 5 pdf picture. Is there any way to optimize the code or it is just normal  for matplotlib?</p>\n\n<p>Portion of code for plotting:</p>\n\n<pre><code>plot_cnt = 1\nfor k in np.arange(K_min, K_max + 1):\n    for l in np.arange(L_min, L_max + 1):\n        ax = plt.subplot(grid[0], grid[1], plot_cnt)\n        plot_cnt += 1\n        plt.setp(ax, 'frame_on', False)\n        ax.set_ylim([-0.1, 1.1])\n        ax.set_xlabel('K={},L={}'.format(k, l), size=3)\n        ax.set_xlim([-0.1, 4.1])\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.grid('off')\n        ax.plot(np.arange(5), (data['S1']['Azimuth'][:, k - 1, l + offset_l] + \\\n                data['S1']['Delta Speed'][:, k - 1, l + offset_l] + \\\n                data['S1']['Speed'][:, k - 1, l + offset_l]) / 3,\n                'r-o', ms=1, mew=0, mfc='r')\n        ax.plot(np.arange(5), data['S2'][case][:, k - 1, l + offset_l],\n                'b-o', ms=1, mew=0, mfc='b')\nplt.savefig(os.path.join(os.getcwd(), 'plot-average.pdf'))\nplt.clf()\nprint 'Final plot created.'\n</code></pre>\n\n<p>Final Picture:\n<img src=\"http://i.stack.imgur.com/xgzdN.png\" alt=\"enter image description here\"></p>\n"},{"tags":[".net","performance","testing"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":21,"score":0,"question_id":13024739,"title":"Efficiently storing performance across multiple tests","body":"<p>I have many different games (test simulations) that will hopefully be run by several thousand computers monthly; they all will have varying hardware. I want to be able to store the data from all these computers for processing later such as figuring out what processors, GPUs and other factors affect performance the most. </p>\n\n<p>I've considered sending down the hardware everytime this test is run, but that seems wasteful</p>\n\n<p>The entire configuration is re-sent and re-added upon test run; everytime. And it still leaves me a lot on how to parse it all.</p>\n\n<p>However, all these varying pieces of information still don't get me where I want to be. I don't neccesarily need the data - but a way to calculate how performant a particular hardware configuration is. I need to be able to guess how well a given computer configuration coming in will do based on previous results - and do so in a way that won't harm bandwidth.</p>\n\n<p>Is there an algorithm I can use to track performance, specifically that will give accurate similar results?</p>\n"},{"tags":["performance","decompression","in-memory","hdd"],"answer_count":3,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":13052043,"title":"which is faster: in-memory decompression or accessing uncompressed data in HDD","body":"<p>I have a dataset larger than main memory. After compression, it fits into memory. However, in-memory decompression is kind of compute-intensive. \nCompared to accessing uncompressed data in hard drive, does in-memory decompression have any advantage in term of time-to-completion? assuming data from HDD will loaded into memory in its entirety (i.e. no random access to HDD during processing). Anyone has done any benchmark before. Thanks. </p>\n"},{"tags":["python","performance","range","max","min"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":64,"score":4,"question_id":13060458,"title":"Efficiently find the range of an array in python?","body":"<p>Is there an accepted efficient way to find the range (ie. max value - min value) of a list of numbers in python? I have tried using a loop and I know I can use the <code>min</code> and <code>max</code> functions with subtraction. I am just wondering if there is some kind of built-in that is faster.</p>\n"},{"tags":["javascript","performance","jsperf"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":46,"score":-1,"question_id":13059144,"title":"jsPerf accuracy","body":"<p>Having spent an entire evening being baffled by some reasonably straightforward benchmarking tests I've whittled things down to a curious test case - the same function twice. It seems there can be some pretty big performance differences (up to 44%), between THE SAME FUNCTION TWICE.</p>\n\n<p>TEST: <a href=\"http://jsperf.com/it-s-the-same\" rel=\"nofollow\">http://jsperf.com/it-s-the-same</a></p>\n\n<pre><code>var ns = {i:0};\n\n(function(){\n\n    function add(n1, n2){\n            ns.i = n1 + n2;\n            return ns.i;\n    };\n\n    ns.test1 = function(){\n            add(2, 12);\n    }\n\n    ns.test2 = function(){\n            add(2, 12);\n    }\n\n})();\n</code></pre>\n\n<p>Any arguments in the defence of jsPerf? Which I might add I have spent many hours glued to. Could 44% be caused by the ns lookup? Is there a better way to structure a test like this? Here I am trying to 'do something' with the output - as per this discussion <a href=\"http://stackoverflow.com/questions/12662497/performance-penalty-for-undefined-arguments\">Performance penalty for undefined arguments</a></p>\n"},{"tags":["database","performance","postgresql","monitoring"],"answer_count":4,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":214,"score":3,"question_id":1452807,"title":"What are the good resources for database monitoring?","body":"<p>I was wondering if anyone could point me in the direction of some good resources (web sites, technical articles/journals, books, etc.) related to database monitoring/performance?</p>\n\n<p>I am looking to write a paper that explores what kinds of statistics are useful in database monitoring, and to whom they are useful for. </p>\n\n<p>So, I figured I'd ask the experts ...</p>\n\n<p>(In my research, I will be focusing mostly on a PostgreSQL database, but I am looking to gather resources on many different databases).</p>\n\n<p>Any suggestions would be appreciated, thanks!</p>\n"},{"tags":["java","python","performance","project-euler"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":117,"score":2,"question_id":13054325,"title":"Why is there such a huge performance different between the same Python/Java code?","body":"<p>I'm currently trying to work through the ProjectEuler questions with Python (something I've only picked up today).  The question I'm working is question 5, where</p>\n\n<pre><code>2520 is the smallest number that can be divided by each of the numbers from 1 to 10 without any remainder.\n\nWhat is the smallest positive number that is evenly divisible by all of the numbers from 1 to 20?\n</code></pre>\n\n<p>I have worked through the problems using Java before, so using the same method as I did before, I just created a loop that iterated, however it seems that my code never ends.</p>\n\n<p>Python:</p>\n\n<pre><code>i = 1\nwhile 1:\n    if i%2 == 0 and i%3==0 and i%4==0 and i%5==0 and i%6==0 and i%7==0 and i%8==0 and i%9==0 and i%10==0 and i%11==0 and i%12==0 and i%13==0 and i%14==0 and i%15==0 and i%16==0 and i%17==0 and i%18==0 and i%19==0:\n        print i\n        break\n    i+=1\n</code></pre>\n\n<p>Java:</p>\n\n<pre><code>public class p5\n{\n    public static void main(String[] args)\n    {\n        for (int i=1;;i++)\n        {\n            if (i%1==0&amp;&amp;i%2==0&amp;&amp;i%3==0&amp;&amp;i%4==0&amp;&amp;i%5==0&amp;&amp;i%6==0&amp;&amp;i%7==0&amp;&amp;i%8==0&amp;&amp;i%9==0&amp;&amp;i%10==0&amp;&amp;i%11==0&amp;&amp;i%12==0&amp;&amp;i%13==0&amp;&amp;i%14==0&amp;&amp;i%15==0&amp;&amp;i%16==0&amp;&amp;i%17==0&amp;&amp;i%18==0&amp;&amp;i%19==0&amp;&amp;i%20==0)\n            {\n                System.out.println(i);\n                break;\n            }\n        }\n    }\n}\n</code></pre>\n\n<p>Java executed that in under 3 seconds on my computer, whereas the Python code never seemed to end.  Any tips?</p>\n\n<p><strong>Edit:</strong></p>\n\n<p>Apparently I typed something wrong, which caused it to never end.  However, even with the entire thing written correctly (with the same output as my Java), it still took <strong>1 minute and 20 seconds</strong>, whereas for Java it took around 1 - 2 seconds.  Am I doing anything wrong?  Or is Python's performance that bad (which shouldn't be afaik)</p>\n"},{"tags":["performance","excel-vba"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":39,"score":-1,"question_id":13057510,"title":"Performance of VBA-2003 vs. VBA-2007","body":"<p>I have upgraded Excel 2003 to Excel 2007 on my office desktop, but I still keeping Excel 2003 on my laptop. What I found out is that it takes significantly more time to run the same VBA application with the same input parameters on VBA-2007 than on VBA-2003.</p>\n\n<p>I have some simulation tool, that should (1) read input data, (2) run several thousand iterations in memory and then (3) printout the results. The second step is very CPU intensive, and takes the most of the time. I measured a processing time for this step, and it occurs that it takes ~3 times longer to run the same scenario on my desktop (with VBA-2007) than on my laptop (with VBA-2003), e.g. 125 sec. vs. 37 sec.</p>\n\n<p>Both computers use Windows XP Professional, Version 2002, Service Pack 3.</p>\n"},{"tags":["java","performance","arraylist","primes"],"answer_count":5,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":93,"score":2,"question_id":13056199,"title":"Most efficient way to print an ArrayList","body":"<p>I'm trying to print an ArrayList with upwards of a couple thousand entries (it has to find all the prime numbers between 1 and 1000000). At the end of the program, I call this method:</p>\n\n<pre><code>println(myArrayList);\n</code></pre>\n\n<p>While this works with only a couple hundred entries, it takes more time to print the array list than to find the primes once there gets to be more entries.</p>\n\n<p>Would it be more efficient to iterate over it? Or to use a different class?</p>\n"},{"tags":["performance","matlab","math","matrix","vectorization"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":3,"view_count":85,"score":-2,"question_id":12977219,"title":"Improve Performance of Matlab Function","body":"<p>I would like to improve one of my simple matlab functions.\nIs there any arithmetic way to implement this function? I think that would perform much better.</p>\n\n<pre><code>function img_output = cutchannels(img_input, min, max)\n[r c l] = size(img_input);\nimg_output = double(img_input);\n\nfor i = 1:r\n    for j = 1:c\n        for k = 1:l\n            if(img_output(i:j:k)&gt; max)\n                img_output(i:j:k) = max;\n            elseif(img_output(i:j:k) &lt; min)\n                img_output(i:j:k) = min;\n            end\n        end\n    end\nend\nend\n</code></pre>\n"},{"tags":["performance","matlab","interpolation","vectorization"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":46,"score":2,"question_id":13054340,"title":"Improving performance of interpolation (Barycentric formula)","body":"<p>I have been given an assignment in which I am supposed to write an algorithm which performs polynomial interpolation by the barycentric formula.  The formulas states that:</p>\n\n<p>p(x) = (SIGMA_(j=0 to n) w(j)*f(j)/(x - x(j)))/(SIGMA_(j=0 to n) w(j)/(x - x(j)))</p>\n\n<p>I have written an algorithm which works just fine, and I get the polynomial output I desire.  However, this requires the use of some quite long loops, and for a large grid number, lots of nastly loop operations will have to be done.  Thus, I would appreciate it greatly if anyone has any hints as to how I may improve this, so that I will avoid all these loops.</p>\n\n<p>In the algorithm, <code>x</code> and <code>f</code> stand for the given points we are supposed to interpolate.  <code>w</code> stands for the barycentric weights, which have been calculated before running the algorithm.  And <code>grid</code> is the linspace over which the interpolation should take place:</p>\n\n<pre><code>function p = barycentric_formula(x,f,w,grid)\n\n%Assert x-vectors and f-vectors have same length.\nif length(x) ~= length(f)\n    sprintf('Not equal amounts of x- and y-values. Function is terminated.')\n    return;\nend\n\nn = length(x);\nm = length(grid);\np = zeros(1,m);\n\n% Loops for finding polynomial values at grid points.  All values are\n% calculated by the barycentric formula.\nfor i = 1:m\n    var = 0;\n    sum1 = 0;\n    sum2 = 0;\n    for j = 1:n\n        if grid(i) == x(j)\n            p(i) = f(j);\n            var = 1;\n        else\n            sum1 = sum1 + (w(j)*f(j))/(grid(i) - x(j));\n            sum2 = sum2 + (w(j)/(grid(i) - x(j)));\n        end\n    end\n    if var == 0\n        p(i) = sum1/sum2;\n    end    \nend\n</code></pre>\n"},{"tags":["php","performance","apache","session"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":84,"score":2,"question_id":12417769,"title":"Simultaneous connections to single browser through PHP and Apache2","body":"<p>I have at the moment an AJAX request going to sendMail.php, it closes the connection immediately(using the header <em>Connection: Close</em>) and continues processing for approx 30 seconds. </p>\n\n<p>But the problem I'm experincing now is that when that same client trys to load any PHP page from that server, it has to wait until sendMail.php has finished processing. </p>\n\n<p><strong>Is there any way around this?</strong></p>\n\n<p>I was reading on some other SO questions that it may be session related, but I'm not using any sessions, I even tried calling <strong>session_write_close()</strong> at the start of the sendMail.php script.</p>\n\n<p>Example code (this is hacky and over done, but it works):</p>\n\n<pre><code>   //File: SendMail.php\n   //error_reporting(E_ALL);\n    error_reporting(0);\n    session_write_close();\n    set_time_limit(0); \n    ignore_user_abort(1);\n    ignore_user_abort(true);\n    ini_set('ignore_user_abort','1');\n    apache_setenv('no-gzip', 1);\n    apache_setenv('KeepAlive',0);\n    ini_set('zlib.output_compression', 0);\n    ini_set('output_buffering', 0);\n\n    $size = ob_get_length();\n\n    // send headers to tell the browser to close the connection\n    header(\"Content-Length: $size\");\n    header('Connection: Close');\n        // flush all output\n    ignore_user_abort(true);\n    ob_end_flush();\n    ob_flush();\n    flush();\n    sleep(30);//The real code has more stuff, but just for example lets just say it sleeps for 30 seconds\n</code></pre>\n\n<p>The rest of the referenced material is a normal navigation via GET. </p>\n"},{"tags":["jquery","performance","jquery-selectors"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":29,"score":0,"question_id":13055707,"title":"Is there any performance advantage to using the E#C CSS selector over #C?","body":"<p>I'm currently reading up on jQuery to fill some gaps in knowledge.</p>\n\n<p>Looking at the various CSS selectors, I see:-</p>\n\n<ul>\n<li><p><code>#C</code> (any element with an ID of C) </p>\n\n<pre><code>$('#profile')\n</code></pre></li>\n<li><p><code>E#C</code> (any element of type E with an ID of #C).</p>\n\n<pre><code>$('div#profile')\n</code></pre></li>\n</ul>\n\n<p>I know specificity is a big deal in the application of CSS rules, but given that it is bad practice to have duplicate IDs on a page, I'm wondering why the second form exists and how it is treated in jQuery.</p>\n\n<p>Does it confer a performance advantage when interrogating the DOM? ( i.e. immediately limiting the scope of the selection ).  This question applies mostly to jQuery, but I'd also be interested to know if it had any bearing on rendering engines, etc.</p>\n"},{"tags":["performance","prolog","dcg"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":74,"score":1,"question_id":10683843,"title":"Prolog - Do plain rules have better performance than lists?","body":"<p>I have a set of DCG rules (in this case german personal pronouns):</p>\n\n<pre><code>% personal pronoun (person, case, number, genus)\nppers(1,0,sg,_) --&gt; [ich].\nppers(1,1,sg,_) --&gt; [meiner].\nppers(1,2,sg,_) --&gt; [mir].\nppers(1,3,sg,_) --&gt; [mich].\nppers(2,0,sg,_) --&gt; [du].\nppers(2,1,sg,_) --&gt; [deiner].\nppers(2,2,sg,_) --&gt; [dir].\nppers(2,3,sg,_) --&gt; [dich].\n...\n</code></pre>\n\n<p>Because they are semantically connected, it would make sense to me to keep this information by moving them into a list (grouped by person for example) instead of unrelated rules. This also makes things a bit neater:</p>\n\n<pre><code>ppers(1,sg,_,[ich, meiner, mir, mich]).\nppers(2,sg,_,[du,deiner,dir,dich]).\n...\n</code></pre>\n\n<p>I would then select the item I want with <code>nth0()</code> where the case I need is the index within the list.</p>\n\n<p>However, I noticed when tracing through the program, that when checking a german sentence for correct grammar and trying to find if a part is a personal pronoun, Prolog will not step through every instanc when I use the upper version (plain rules), but will crawl through every list when I use the list version below.</p>\n\n<p>Does this mean that performance will be worse if I use lists and nth0 versus plain rules? Or does the Prolog tracer just not show the crawling for plain rules as it does for lists?</p>\n\n<p>(I hope I could make my question obvious enough, if not I will expand.)</p>\n"},{"tags":["android","performance","graphics","android-canvas","scenegraph"],"answer_count":2,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":98,"score":5,"question_id":13012514,"title":"Canvas manipulation vs element manipulation","body":"<p>I am developing a small library as a basis for some applications. As I am about to create a scenegraph (2D) I am wondering which of the following approaches looks more promising under the view of performance, maintainability, easy to use etc.</p>\n\n<ol>\n<li>I could give each drawable element a matrix where I perform translation, rotation and more.</li>\n<li>I could do everything on the canvas instead of the elements.</li>\n</ol>\n\n<p>The first solution has a disadvantages: For primitive elements like circles, where I can't pass a matrix in the draw call, I must access the translated values from the matrix like this:</p>\n\n<pre><code>private float get(int index) {\n    final float[] values = new float[9];\n    getValues(values);\n    return values[index];\n}\n\npublic float getX() {\n    return get(Matrix.MTRANS_X);\n}\n\npublic float getY() {\n    return get(Matrix.MTRANS_Y);\n}\n</code></pre>\n\n<p>So on every draw call I create a float array for each getter call (one for getX(), one for getY()). Assuming that I have plenty of elements on the screen, this could lead to a memory and performance impact.</p>\n\n<p>The second approach has the disadvantage of \"negative\" thinking. If I want an element be drawn at point 100/100 I must translate the canvas to -100/-100 as I would draw on 0/0. If I restore the canvas after that, the result would be the element be drawn on the wanted 100/100. I am not sure if this negative thinking would result in a heavy impact on code maintainability and decreased understanding (never even started to think about introducing bugs by simply forgetting to negate something...).</p>\n\n<p>Does someone have a tip which way should be preferred?</p>\n"},{"tags":["performance","search","full-text-search"],"answer_count":5,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":181,"score":5,"question_id":12882000,"title":"Hybrid of fulltext and property-based search engine","body":"<h3>Background:</h3>\n\n<p>SQL database representing different types of events (concerts, football matches, charity collections etc.), where each contain event-related data (concert - artist name, match - host/visitor team). All of these events inherit from one, general table <code>event</code>, which contains data related to all of them (name, description, location, start/end date).\nInheritance is implemented using table-per-subclass model known from <a href=\"http://docs.jboss.org/hibernate/orm/3.3/reference/en/html/inheritance.html#inheritance-tablepersubclass\" rel=\"nofollow\">Hibernate</a> or <a href=\"http://docs.doctrine-project.org/en/2.0.x/reference/inheritance-mapping.html#class-table-inheritance\" rel=\"nofollow\">Doctrine</a>. The database also stores tables <code>artists</code> (<code>id</code>, <code>name</code>, <code>birth_date</code>) and <code>football_teams</code> (<code>id</code>, <code>name</code>, <code>country</code>, <code>coach_name</code>) used in <code>event_concerts</code> and <code>event_football_matches</code> tables (through FKs).</p>\n\n<h3>Problem:</h3>\n\n<p>Create a search engine that given some criteria (<code>{name: \"manchester\", startDate: \"01.01.2012 - 01.02.2012\"}</code> or <code>{location: \"london\", description: \"artists +metallica -bieber\"}</code>) will return all events that meet the criteria, as well as results from <code>artists</code>/<code>football_teams</code> tables.</p>\n\n<p>Some properties of those events contain large pieces of text, that should be searched through in fulltext-search manner.</p>\n\n<h3>Example:</h3>\n\n<p>Given following search criteria:</p>\n\n<pre><code>{ location: \"london\", startDate: \"05.11.2012 - 07.11.2012\" }\n</code></pre>\n\n<p>Search engine should return:</p>\n\n<ol>\n<li>(football event) Arsenal vs Manchester United match, Emirates Stadium, London, 06.11.2012</li>\n<li>(concert event) Metallica concert, Some-Fancy-Location, 05.11.2012</li>\n<li>(football team/not an event) Arsenal, founded: 1886, league: Premier League</li>\n<li>(football team/not an event) Chelsea, founded: 1905, league: Premier League</li>\n<li>(festival event) Halloween in London, 07.11.2012</li>\n<li>(dance event) Sleeping Beauty at Sadler's Wells, £45, 07.11.2012</li>\n<li>(musician, not an event) Neil Christian, 1943 - 2012, Rock'n'Roll vocalist</li>\n</ol>\n\n<p>As you can see, <em>startDate</em> (event-related property) is considered only in case of events.</p>\n\n<hr>\n\n<p>Search engine has to scan lots of tables, that's why I believe I should use dedicated software (Sphinx, Lucene, ...?) and create separate index just for the searching.</p>\n\n<hr>\n\n<p>Could anyone suggest some solution for building such an index? What software could I use as a base for that search engine?</p>\n\n<hr>\n\n<h3>EDIT:</h3>\n\n<p>Just to clarify: none of the properties is required. Some of them contain dates which will be searched using exact-match, some of them contain short text (like a location) that also will be searched using exact-match. But some of them contain long pieces of text, and that needs to be searched in full-text manner.</p>\n"},{"tags":["php","ajax","performance","forms","table"],"answer_count":3,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":73,"score":2,"question_id":12974317,"title":"1500 row table with each row containing a form - Ajax or something else?","body":"<p>I have a very large HTML table containing 1500 rows (markup produced by PHP).  Each row has a 5-element checkbox and one  textarea.  </p>\n\n<p>Here's an example row:</p>\n\n<pre><code>&lt;tr id=\"abc123\"&gt;\n    &lt;td&gt;abc123&lt;/td&gt;\n    &lt;td valign=\"top\"&gt;&lt;input name=\"v1\" value=1 type=\"checkbox\"&gt;&lt;/td&gt;\n    &lt;td valign=\"top\"&gt;&lt;input name=\"v2\" value=2 type=\"checkbox\"&gt;&lt;/td&gt;\n    &lt;td valign=\"top\"&gt;&lt;input name=\"v3\" value=3 type=\"checkbox\"&gt;&lt;/td&gt;\n    &lt;td valign=\"top\"&gt;&lt;input name=\"v4\" value=4 type=\"checkbox\"&gt;&lt;/td&gt;\n    &lt;td valign=\"top\"&gt;&lt;input name=\"v5\" value=5 type=\"checkbox\"&gt;&lt;/td&gt;\n    &lt;td valign=\"top\"&gt;&lt;textarea name=\"notes\"&gt;&lt;/textarea&gt;&lt;/td&gt;\n    &lt;td valign=\"top\"&gt;&lt;input type=\"submit\" name=\"submit\"&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n</code></pre>\n\n<p>There are more columns but this is the important part. </p>\n\n<p>What would be the best way of setting this up so that the submit button is checked/redrawn every 60 seconds?   If the row/form has been submitted, the submit button would need to change to a link.   </p>\n\n<p>Would having that many writes occurring all at once lead to horrible performance?  Our server can handle it, but I'm more concerned about client capabilities (who are using mid-grade or worse desktop machines).</p>\n\n<p>Other than breaking up the records onto separate pages (client not interested in that), is there a better way of handling this many forms on a single page? </p>\n\n<p><strong>Update</strong>\nI'm thinking it might make more sense to let users click on the row they want to update, which would then convert only that row to a form.  If a row loses focus, I'll display a confim box so they don't lose or submit incomplete data.</p>\n"},{"tags":["performance","automapper","emitmapper"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":24,"score":0,"question_id":13053590,"title":"Emit mapper vs valueinjecter or automapper performance","body":"<p>I have spent some time comparing this three mappers and it is interesting why so big performance diffrenece between emitmapper and any of valueinjecter or automapper(last two comparable by performance). From benchmark test in emitmapper solution(1000000 iterations):</p>\n\n<pre><code>    Auto Mapper (simple): 38483 milliseconds\n    Emit Mapper (simple): 118 milliseconds\n    Handwritten Mapper (simple): 37 milliseconds\n    Auto Mapper (Nested): 53800 milliseconds\n    Emit Mapper (Nested): 130 milliseconds\n    Handwritten Mapper (Nested): 128 milliseconds\n    Auto Mapper (Custom): 49587 milliseconds\n    Emit Mapper (Custom): 231 milliseconds\n</code></pre>\n\n<p>Also some benchmarks from valueinjecter runned with added emitmapper(for 10000 iterations):</p>\n\n<pre><code>    Convention: 00:00:00.5016074\n    Automapper: 00:00:00.1992945 \n    Smart convention: 00:00:00.2132185\n    Emit mapper(each time new mapper): 00:00:00.1168676\n    Emit mapper(one mapper): 00:00:00.0012337\n</code></pre>\n\n<p>There in first emit mapper test - it was created each time, in second - one mapper for all conversions. </p>\n\n<p>Taking this into account, have result as valueinjecter(also as automapper) slower than in 100 times than emit mapper. What is a reason of so huge performance difference? As for me object to object mapper cannot took so much time comparing to handwritten mapper as it be a bottleneck of project(if we need to map collection of objects for example).</p>\n\n<p>At this moment I'm thinking about using emit mapper, but only one reason why I'm not ready to decide : emit mapper not supported at all by first developers, but I'm not sure that this is very important(very low possibility to requirement of some additional functionality).</p>\n"},{"tags":["mysql","performance"],"answer_count":6,"favorite_count":1,"up_vote_count":7,"down_vote_count":0,"view_count":422,"score":7,"question_id":3840906,"title":"Extra column ruins MySQL performance","body":"<p>I have a warehouse table that looks like this:</p>\n\n<pre><code>CREATE TABLE Warehouse (\n  id BIGINT(20) UNSIGNED NOT NULL AUTO_INCREMENT,\n  eventId BIGINT(20) UNSIGNED NOT NULL,\n  groupId BIGINT(20) NOT NULL,\n  activityId BIGINT(20) UNSIGNED NOT NULL,\n  ... many more ids,\n  \"txtProperty1\" VARCHAR(255),\n  \"txtProperty2\" VARCHAR(255),\n  \"txtProperty3\" VARCHAR(255),\n  \"txtProperty4\" VARCHAR(255),\n  \"txtProperty5\" VARCHAR(255),\n  ... many more of these\n  PRIMARY KEY (\"id\")\n  KEY \"WInvestmentDetail_idx01\" (\"groupId\"),\n  ... several more indices\n) ENGINE=INNODB;\n</code></pre>\n\n<p>Now, the following query spends about 0.8s in <em>query time</em> and 0.2s in <em>fetch time</em>, for a total of about one second.  The query returns ~67,000 rows.</p>\n\n<pre><code>SELECT eventId\nFROM Warehouse\nWHERE accountId IN (10, 8, 13, 9, 7, 6, 12, 11)\n  AND scenarioId IS NULL\n  AND insertDate BETWEEN DATE '2002-01-01' AND DATE '2011-12-31'\nORDER BY insertDate;\n</code></pre>\n\n<p>Adding more ids to the select clause doesn't really change the performance at all.</p>\n\n<pre><code>SELECT eventId, groupId, activityId, insertDate\nFROM Warehouse\nWHERE accountId IN (10, 8, 13, 9, 7, 6, 12, 11)\n  AND scenarioId IS NULL\n  AND insertDate BETWEEN DATE '2002-01-01' AND DATE '2011-12-31'\nORDER BY insertDate;\n</code></pre>\n\n<p>However, adding a \"property\" column does change it to 0.6s fetch time and 1.8s query time.</p>\n\n<pre><code>SELECT eventId, txtProperty1\nFROM Warehouse\nWHERE accountId IN (10, 8, 13, 9, 7, 6, 12, 11)\n  AND scenarioId IS NULL\n  AND insertDate BETWEEN DATE '2002-01-01' AND DATE '2011-12-31'\nORDER BY insertDate;\n</code></pre>\n\n<p>Now to really blow your socks off.  Instead of <em>txtProperty1</em>, using <em>txtProperty2</em> changes the times to 0.8s fetch, 24s query!</p>\n\n<pre><code>SELECT eventId, txtProperty2\nFROM Warehouse\nWHERE accountId IN (10, 8, 13, 9, 7, 6, 12, 11)\n  AND scenarioId IS NULL\n  AND insertDate BETWEEN DATE '2002-01-01' AND DATE '2011-12-31'\nORDER BY insertDate;\n</code></pre>\n\n<p>The two columns are pretty much identical in the type of data they hold: mostly non-null, and neither are indexed (not that that should make a difference anyways).  To be sure the table itself is healthy I ran analyze/optimize against it.</p>\n\n<p>This is really mystifying to me.  I can see why adding columns to the select clause only can slightly increase fetch time, but it should not change query time, especially not significantly.  I would appreciate any ideas as to what is causing this slowdown.</p>\n\n<p>EDIT - More data points</p>\n\n<p>SELECT * actually outperforms txtProperty2 - 0.8s query, 8.4s fetch.  Too bad I can't use it because the fetch time is (expectedly) too long.</p>\n"},{"tags":["performance","amazon-ec2","monitoring"],"answer_count":3,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":1025,"score":2,"question_id":5808797,"title":"Know of any tools to monitor the REAL performance of an amazon ec2 instance?","body":"<p>There are times (like this morning for 15 minutes) when the performance of my cc1.4xlarge is horrible and it is not anything running on my instance as evidenced by the aws console monitoring graphs and my own investigation.</p>\n\n<p>It seems like the physical box that it is on is having problems or maybe Xen isn't properly allocating resources.</p>\n\n<p>Are there any tools that will monitor not the \"reported\" stats on the CPU and disk, but do actual performance tests, e.g. divide some floats 1,000 times and make sure it completes within a typical time, write/read stuff to disk?</p>\n"},{"tags":["android","iphone","performance","opengl-es","glsl"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":554,"score":2,"question_id":10702065,"title":"OpenGL ES shader degrades too much performance","body":"<p>I'm optimizing a game that works for both, iPhone and Android. I'm using 4 shader to draw the scene and I noticed that if I change one of them to another the fps goes from 32 to 42, even though there's only 1 sprite being drawn with that shader, and the only difference in this 2 shaders is just a product in the fragmente shader.</p>\n\n<p>These are the shaders:  </p>\n\n<p>default-2d-tex.shader</p>\n\n<pre><code>#ifdef GL_ES\nprecision highp float;\nprecision lowp int;\n#endif\n\n#ifdef VERTEX\n\nuniform mat4        umvp;\n\nattribute vec4      avertex;\nattribute vec2      auv;\n\nvarying vec2        vuv;\n\nvoid main()\n{\n    // Pass the texture coordinate attribute to a varying.\n    vuv = auv;\n\n    // Here we set the final position to this vertex.\n    gl_Position = umvp * avertex;\n}\n\n#endif\n\n#ifdef FRAGMENT\n\nuniform sampler2D   map0;\nuniform vec4 ucolor;\n\nvarying vec2        vuv;\n\nvoid main()\n{\n    gl_FragColor =  texture2D(map0, vuv) * ucolor;\n}\n\n#endif\n</code></pre>\n\n<p>default-2d-tex-white.shader</p>\n\n<pre><code>#ifdef GL_ES\nprecision highp float;\nprecision lowp int;\n#endif\n\n#ifdef VERTEX\n\nuniform mat4        umvp;\n\nattribute vec4      avertex;\nattribute vec2      auv;\n\nvarying vec2        vuv;\n\nvoid main()\n{\n    // Pass the texture coordinate attribute to a varying.\n    vuv = auv;\n\n    // Here we set the final position to this vertex.\n    gl_Position = umvp * avertex;\n}\n\n#endif\n\n#ifdef FRAGMENT\n\nuniform sampler2D   map0;\n\nvarying vec2        vuv;\n\nvoid main()\n{\n    gl_FragColor =  texture2D(map0, vuv);\n}\n\n#endif\n</code></pre>\n\n<p>Again,<br>\nIf I modify default-2d-tex.shader and remove the product \"* ucolor\", the fps goes from 32 to 42, and I'm using it for just one sprite in the scene!</p>\n\n<p>Is this normal? Why is this shader being so slow and how can I improve it?</p>\n\n<p><strong>EDIT:</strong></p>\n\n<p>I see this performance slowdown on both iPod and Android in an equal ratio. Both are PowerVr SGX GPUs (iPod 3rd gen and Samsung Galaxy SL -PowerVR SGX 530-). iOS version is 4.1 and Android is 2.3.3</p>\n\n<p>The sprite I'm drawing is scaled to fill the screen (scaled to 4x) and I'm drawing it once per frame. It's taken from a texture map so the texture is actually larger (1024x1024) but the portion taken is 80x120. Alpha blending is enabled.</p>\n\n<p><strong>EDIT 2</strong></p>\n\n<p>I made a mistake. The sprite is scaled 11x: its 32x48.\nIf I don't draw that sprite at all, fps goes to 45. I'm drawing a lot of sprites in the scene, why is that one taking so much time? Could it be because it's scaled so much?</p>\n"},{"tags":["performance","html5","udp","websocket"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":91,"score":1,"question_id":13040752,"title":"WebSockets, UDP, and benchmarks","body":"<p>HTML5 websockets currently use a form of TCP communication. However, for real-time games, TCP just won't cut it (and is great reason to use some other platform, like native). As I probably need UDP to continue a project, I'd like to know if the specs for HTML6 or whatever will support UDP?</p>\n\n<p>Also, are there any reliable benchmarks for WebSockets that would compare the WS protocol to a  low-level, direct socket protocol?</p>\n"},{"tags":["performance","sql-server-2005","computed-columns"],"answer_count":0,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":41,"score":3,"question_id":13051507,"title":"Computed column index","body":"<p>I have a table <code>Com_Main</code> which contains column <code>CompanyName nvarchar(250)</code>. It has average length of 19, max length = 250.</p>\n\n<p>To improve performance I want to add a computed column <code>left20_CompanyName</code> which holds the first 20 characters of <code>CompanyName</code>:</p>\n\n<pre><code>alter table Com_main \nadd left20_CompanyName as LEFT(CompanyName, 20) PERSISTED\n</code></pre>\n\n<p>Then I create Index on this column:</p>\n\n<pre><code>create index ix_com_main_left20CompanyName \non Com_main (LEFT20_CompanyName)\n</code></pre>\n\n<p>So when I use</p>\n\n<pre><code>select CompanyName from Com_Main\nwhere LEFT20_CompanyName LIKE '122%'\n</code></pre>\n\n<p>it uses this nonclustered index, but when the query is like:</p>\n\n<pre><code>select CompanyName from Com_Main \nwhere CompanyName LIKE '122%'\n</code></pre>\n\n<p>It uses full table scan, and don't use this index. </p>\n\n<p>So the question:</p>\n\n<p>Is it possible to make SQL Server use this index on computable column in last query?</p>\n"},{"tags":["java","performance","memory","enums"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":60,"score":4,"question_id":13051223,"title":"ENUM storage (memory, etc)","body":"<p>I was just asking myself a little question, and I'm not sure if I can find the right answer to this:</p>\n\n<p>If I use an ENUM in Java, with an own constructor (and many, many, maaaaany parameters) - are those stored in Memory every time the program gets executed, or are they only 'loaded' in Memory, if they get used?</p>\n\n<p>What I mean is, if I have an ENUM with 400 entries, and only use one of the entries - are all others still present in Memory?</p>\n\n<p><strong>some pseudocode:</strong></p>\n\n<pre><code>public enum Type {\n    ENTRY_A(val1, val2, val3, val4, new Object(val5, val6, val7, ...)),\n    ENTRY_B(val1, val2, val3, val4, new Object(val5, val6, val7, ...)),\n    ENTRY_C(val1, val2, val3, val4, new Object(val5, val6, val7, ...)),\n    ...\n}\n</code></pre>\n\n<p>If I only use ENTRY_A, and dont touch ENTRY_B, ENTRY_C, etc - how will Java handle that exactly?</p>\n\n<p>Thanks for the Answer - and yes, this is mainly curiousity</p>\n"},{"tags":["performance","cuda","cublas"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":62,"score":0,"question_id":13051205,"title":"cuBLAS performance","body":"<p>I use CUDA 5.0, and I want to compare matrix multiplication in C and cuBLAS. I already wrote a program in which matrix multiplication in C and cuBLAS both gave correct answers. </p>\n\n<p>Now I want to compare their performance. For implementation in C, I used the <code>clock()</code>, but I found that cutil doesn't exist in CUDA 5.0, so I used <code>cudaEvent</code>. Both implementations use the same matrix, and in C, I just measured the time when C do the matrix multiplication, while in cuBLAS I began the measurement from <code>createhandle</code> till <code>destroyhandle</code>. </p>\n\n<p>I got this result:<br>\nWhen C spends just 0.08ms, cuBLAS spend 59ms, and then I used <code>clock()</code> to measure time for cuBLAS, cuBLAS became faster than C. I don't know whether the method I used to measure time is correct. Why do <code>cudaevent</code> and <code>clock()</code> give different answers? </p>\n\n<p>I use cuBLAS, cudaevent just following Nvidia's documentation. I'm really puzzled about how to measure time correctly.</p>\n"},{"tags":["performance","linux-kernel","i2c"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":26,"score":0,"question_id":13050877,"title":"I2C Data Rate Configuration 100/400KHz","body":"<p>I am currently using an I2C device within an android application and think it may be operating at 100KHz. I would like it to run at 400KHz. Assuming the I2C device accommodates this How would I go about doing this.</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["javascript","performance","asynchronous","nonblocking"],"answer_count":0,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":48,"score":0,"question_id":13050723,"title":"Way to detect if script was loaded asynch?","body":"<p>Is there a way to detect how a script was loaded without looking at the html source?</p>\n\n<p>Either via browser addon or with a little javascript or sth. like that without interfering with the page? Pagespeed for example detects if google's analytics snippet was loaded asynch, but I don't now how they do it.</p>\n\n<p>Any ideas? I'd like to do automated performance tests like this one.</p>\n"},{"tags":["c#","performance","performance-measurement"],"answer_count":9,"favorite_count":2,"up_vote_count":9,"down_vote_count":0,"view_count":2757,"score":9,"question_id":2072361,"title":"What is the best way to measure how long code takes to execute?","body":"<p>I'm trying to determine which approach to removing a string is the <strong>fastest</strong>.</p>\n\n<p>I simply get the <strong>start</strong> and <strong>end</strong> time and show the difference.</p>\n\n<p>But the results are so <strong>varied</strong>, e.g. as shown below the same method can take from 60 ms to 231 ms.</p>\n\n<p><strong>What is a better method to get more accurate results?</strong></p>\n\n<p><img src=\"http://www.deviantsart.com/upload/1q4t3rl.png\" alt=\"alt text\"></p>\n\n<pre><code>using System;\nusing System.Collections;\nusing System.Collections.Generic;\n\nnamespace TestRemoveFast\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            for (int j = 0; j &lt; 10; j++)\n            {\n                string newone = \"\";\n                List&lt;string&gt; tests = new List&lt;string&gt;();\n                for (int i = 0; i &lt; 100000; i++)\n                {\n                    tests.Add(\"{http://company.com/Services/Types}ModifiedAt\");\n                }\n\n                DateTime start = DateTime.Now;\n                foreach (var test in tests)\n                {\n                    //newone = ((System.Xml.Linq.XName)\"{http://company.com/Services/Types}ModifiedAt\").LocalName;\n                    newone = Clean(test);\n                }\n\n                Console.WriteLine(newone);\n                DateTime end = DateTime.Now;\n                TimeSpan duration = end - start;\n                Console.WriteLine(duration.ToString());\n            }\n\n            Console.ReadLine();\n        }\n\n        static string Clean(string line)\n        {\n            int pos = line.LastIndexOf('}');\n            if (pos &gt; 0)\n                return line.Substring(pos + 1, line.Length - pos - 1);\n                //return line.Substring(pos + 1);\n            else\n                return line;\n        }\n    }\n}\n</code></pre>\n"},{"tags":["c#","performance","datetime","timing","stopwatch"],"answer_count":3,"favorite_count":2,"up_vote_count":26,"down_vote_count":0,"view_count":4471,"score":26,"question_id":2923283,"title":"Stopwatch vs. using System.DateTime.Now for timing events","body":"<p>I wanted to track the performance of my code so I stored the start and end time using <code>System.DateTime.Now</code>. I took the difference between the two as the time my code to execute. </p>\n\n<p>I noticed though that the difference didn't appear to be accurate. So I tried using a <code>Stopwatch</code> object. This turned out to be much, much more accurate.</p>\n\n<p>Can anyone tell me why <code>Stopwatch</code> would be more accurate than calculating the difference between a start and end time using <code>System.DateTime.Now</code>?</p>\n\n<p>BTW, I'm not talking about a tenths of a percent. I get about a 15-20% difference.</p>\n"},{"tags":["performance","sql-server-2008","c#-4.0","table","entity-framework-4"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":33,"score":0,"question_id":13048907,"title":"Sql Server heavily queried Table - should I store secondary info (html text) in another table","body":"<p>The Overview:</p>\n\n<p>I have a table \"category\" that is for the most part used to categorise products and currently looks like this:</p>\n\n<pre><code>CREATE TABLE [dbo].[Category]\n( \nCategoryId int IDENTITY(1,1) NOT NULL, \nCategoryNode hierarchyid NOT NULL UNIQUE,\nCategoryString AS CategoryNode.ToString() PERSISTED,\nCategoryLevel AS CategoryNode.GetLevel() PERSISTED,\nCategoryTitle varchar(50) NOT NULL,\nIsActive bit NOT NULL DEFAULT 1\n)\n</code></pre>\n\n<p>This table is heavily queried to display the category hierarchy on a shopping website (typically every page view) and can have a substantial number of items.</p>\n\n<p>I'm using the Entity Framework in my data layer.</p>\n\n<p>The Question:</p>\n\n<p>I have a need to add what could potentially be a fairly large \"description\" which could come in the form of the entire contents of a web-page and I'm wondering whether I should store this in a related table rather than adding it to the existing category table given that the entity framework will drag the \"description\" column out of the database 100% of the time when 99.5% of the time I'll only want the CategoryTitle and CategoryId.</p>\n\n<p>Typically I wouldn't worry about the overhead of the Entity Framework, but in the case I think it might be important to take it into consideration. I could work around this with a view or a complex type from a stored proc, but this means a lot of refactoring that I'd prefer to avoid.</p>\n\n<p>I'm just interested to know if anyone has any thoughts, suggestions or a desire to slap my wrists in relation to this scenario...</p>\n\n<p>EDIT:</p>\n\n<p>I should add that the reason I'm hesitating to set up a secondary table is because I don't like the idea of adding an additional table that has a 1 to 1 relationship with the Category table - it seems somewhat pointless. But I'm also not a DBA so I'm not sure whether this is an acceptable practice or not.</p>\n"},{"tags":["java","database","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":56,"score":1,"question_id":13048762,"title":"Result Set to Multi Hash Map","body":"<p>I have a situation here. I have a huge database with >10 columns and millions of rows. I am using a matching algorithm which matches each input records with the values in database. </p>\n\n<p>The database operation is taking lot of time when there are millions of records to match. I am thinking of using a multi-hash map or any resultset alternative so that i can save the whole table in memory and prevent hitting database again....</p>\n\n<p>Can anybody tell me what should i do??</p>\n"},{"tags":["java","search","lucene","performance"],"answer_count":2,"favorite_count":1,"up_vote_count":7,"down_vote_count":0,"view_count":1250,"score":7,"question_id":4919001,"title":"Fastest way to count all results in Lucene (java)","body":"<p>What is the fastest way to count all results for a given Query in Lucene?</p>\n\n<ol>\n<li>TopDocs.totalHits </li>\n<li>implement and manage a Filter, using QueryFilter</li>\n<li>implement a custom 'counting' Collector. This simply increments a count in the collect(int doc) method and returns true for the acceptsDocOutOfOrder() method. All other methods are NOOPS.</li>\n</ol>\n\n<p>Since 1. will do scoring on all docs, and 2. could have an upfront hit due to loading of the FieldCache, I assume the answer is 3. It just seems odd that Lucene doesn't provide such a collector out of the box?</p>\n"},{"tags":["performance","apache","node.js","apache2"],"answer_count":6,"favorite_count":5,"up_vote_count":17,"down_vote_count":0,"view_count":2383,"score":17,"question_id":6634299,"title":"Node.js slower than Apache","body":"<p>I am comparing performance of Node.js (0.5.1-pre) vs Apache (2.2.17) for a very simple scenario - serving a text file.</p>\n\n<p>Here's the code I use for node server:</p>\n\n<pre><code>var http = require('http')\n  , fs = require('fs')\n\nfs.readFile('/var/www/README.txt',\n    function(err, data) {\n        http.createServer(function(req, res) {\n            res.writeHead(200, {'Content-Type': 'text/plain'})\n            res.end(data)\n        }).listen(8080, '127.0.0.1')\n    }\n)\n</code></pre>\n\n<p>For Apache I am just using whatever default configuration which goes with Ubuntu 11.04</p>\n\n<p>When running Apache Bench with the following parameters against <strong>Apache</strong></p>\n\n<pre><code>ab -n10000 -c100 http://127.0.0.1/README.txt\n</code></pre>\n\n<p>I get the following runtimes:</p>\n\n<pre><code>Time taken for tests:   1.083 seconds\nComplete requests:      10000\nFailed requests:        0\nWrite errors:           0\nTotal transferred:      27630000 bytes\nHTML transferred:       24830000 bytes\nRequests per second:    9229.38 [#/sec] (mean)\nTime per request:       10.835 [ms] (mean)\nTime per request:       0.108 [ms] (mean, across all concurrent requests)\nTransfer rate:          24903.11 [Kbytes/sec] received\n\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        0    0   0.8      0       9\nProcessing:     5   10   2.0     10      23\nWaiting:        4   10   1.9     10      21\nTotal:          6   11   2.1     10      23\n\nPercentage of the requests served within a certain time (ms)\n  50%     10\n  66%     11\n  75%     11\n  80%     11\n  90%     14\n  95%     15\n  98%     18\n  99%     19\n 100%     23 (longest request)\n</code></pre>\n\n<p>When running Apache bench against <strong>node</strong> instance, these are the runtimes:</p>\n\n<pre><code>Time taken for tests:   1.712 seconds\nComplete requests:      10000\nFailed requests:        0\nWrite errors:           0\nTotal transferred:      25470000 bytes\nHTML transferred:       24830000 bytes\nRequests per second:    5840.83 [#/sec] (mean)\nTime per request:       17.121 [ms] (mean)\nTime per request:       0.171 [ms] (mean, across all concurrent requests)\nTransfer rate:          14527.94 [Kbytes/sec] received\n\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        0    0   0.9      0       8\nProcessing:     0   17   8.8     16      53\nWaiting:        0   17   8.6     16      48\nTotal:          1   17   8.7     17      53\n\nPercentage of the requests served within a certain time (ms)\n  50%     17\n  66%     21\n  75%     23\n  80%     25\n  90%     28\n  95%     31\n  98%     35\n  99%     38\n 100%     53 (longest request)\n</code></pre>\n\n<p>Which is clearly slower than Apache. This is especially surprising if you consider the fact that Apache is doing a lot of other stuff, like logging etc.</p>\n\n<p>Am I doing it wrong? Or is Node.js really slower in this scenario?</p>\n\n<p><strong>Edit 1</strong>: I do notice that node's concurrency is better - when increasing a number of simultaneous request to 1000, Apache starts dropping few of them, while node works fine with no connections dropped.</p>\n"},{"tags":["performance","internet-explorer","gwt","grid","smartgwt"],"answer_count":3,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":5327,"score":2,"question_id":1070856,"title":"SmartGWT ListGrid is slow, but only in Internet Explorer","body":"<p>we have migrated from gwtext to smartgwt and overall the experience is ok. However, we have big problems with the ListGrid component of SmartGWT. </p>\n\n<p>It is very slow if both of the following conditions are met:</p>\n\n<ul>\n<li>Internet Explorer is used</li>\n<li>5 or more columns</li>\n</ul>\n\n<p>the speed will decrease if you add more columns up to the point where the whole thing is unusable and you have to kill the browser, e.g. through the windows task manager.</p>\n\n<p>Grids with 1 column are fine in internet explorer</p>\n\n<p>Grids with a large number of columns are fine in firefox, opera</p>\n\n<p>In the smartgwt forums I've found two entries that are about this problem, but none of the suggested workarounds has fixed the problem. </p>\n\n<p><a href=\"http://forums.smartclient.com/showthread.php?t=5896\" rel=\"nofollow\">http://forums.smartclient.com/showthread.php?t=5896</a></p>\n\n<p>Since I am only allowed to post one hyperlink, here's the number of the second thread:</p>\n\n<p>t=5193</p>\n\n<p>Any help is greatly appreciated</p>\n"},{"tags":["java","performance","java-ee","websphere"],"answer_count":8,"favorite_count":2,"up_vote_count":7,"down_vote_count":1,"view_count":4903,"score":6,"question_id":1178210,"title":"Websphere Application Server - What on earth will it take to start any fast?","body":"<p>I am using Rational Application Developer v7.0 that ships with an integrated test environment. When I get to debugging my webapp, the server startup time in debug mode is close to 5-6 minutes - enough time to take a coffe break! </p>\n\n<p>At times, it so pisses me off that I start cursing IBM for building an operating system! instead of an app server: Spawning 20+ processes and useless services with no documented configuration to tuning it, to starting any faster. </p>\n\n<p>I am sure there are many java developers out there, who would agree with me on this. Now, I tried to disable the default apps and a set of services via my admin console, however, that hasn't helped much.</p>\n\n<p>I have no webservices, no enterprise beans, no queues, just a simple web app which requires a connection pool. Have you done something in the past to make your integrated test environment, start fast in debug mode and there by consume less RAM? </p>\n\n<p>UPDATE: \nI tried disabling a few services (internationaliztion, default apps etc...) and now the websphere server went from bad to worse. Not only doesn't it take horrifying startup time, it keeps freezing every now and then for upto 2 minutes. :-( Sounds like, optimization is not such a good thing, always!</p>\n"},{"tags":["c++","performance"],"answer_count":6,"favorite_count":0,"up_vote_count":11,"down_vote_count":0,"view_count":186,"score":11,"question_id":5528569,"title":"repeated calling - coding practice","body":"<p>which one do you prefer? (of course getSize doesn't make any complicated counting, just returning member value)</p>\n\n<pre><code>void method1(Object &amp; o)\n{\n    int size = o.getSize();\n\n    someAction(size);\n    someOtherAction(size);\n}\n</code></pre>\n\n<p>or</p>\n\n<pre><code>void method2(Object &amp; o)\n{\n    someAction(o.getSize());\n    someOtherAction(o.getSize());\n}\n</code></pre>\n\n<p>I know I can measure which one is faster but I want some comments... Not just executing time related... eg. if you are prefer method2, how many times maximally do you use o.getSize and what is the number what make you use method1 way?\nAny best practices? (imagine even different types then int)\nTY</p>\n"},{"tags":["ios","performance","image-processing","opengl-es"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":177,"score":1,"question_id":12052151,"title":"Image analysis through GPU (with OpenGL ES) or CPU?","body":"<p>I'd like to analyze the constantly updating image feed that comes from an iPhone camera to determine a general \"lightness coefficient\". Meaning: if the coefficient returns 0.0, the image is completely black, if it returns 1.0 the image is completely white. Of course all values in between are the ones that I care about the most (background info: I'm using this coefficient to calculate the intensity of some blending effects in my fragment shader).</p>\n\n<p>So I'm wondering if I should run a for loop over my pixelbuffer and analyze the image every frame (30 fps) and send the coeff as a uniform to my fragment shader or is there a way to analyze my image in OpenGL. If so, how should I do that?</p>\n"},{"tags":["ios","performance","opengl-es","texture"],"answer_count":2,"favorite_count":2,"up_vote_count":1,"down_vote_count":0,"view_count":225,"score":1,"question_id":10296149,"title":"(iPhone, OpenGL) direct texture data storage in files","body":"<p>At this moment I use this scenario to load OpenGL texture from PNG:</p>\n\n<ul>\n<li>load PNG via UIImage</li>\n<li>get pixels data via bitmap context</li>\n<li>repack pixels to new format (currently RGBA8 -> RGBA4, RGB8 -> RGB565, using ARM NEON instructions)</li>\n<li>create OpenGL texture with data</li>\n</ul>\n\n<p>(this approach is commonly used in Cocos2d engine)</p>\n\n<p>It takes much time and seems to do extra work that may be done once per build. So I want to save repacked pixels data back into file and load it directly to OpenGL on second time.</p>\n\n<p>I would know the practical advantages. Does anyone tried it? Is it worth to compress data via zip (as I know, current iDevices have bottleneck in file access)? Would be very thankful for real experience sharing.</p>\n"},{"tags":["ios","performance","opengl-es"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":241,"score":-1,"question_id":9255118,"title":"What is the fastest drawing method on iOS?","body":"<p>I'm developing a 2D game on iOS, but I'm finding it difficult getting drawing to run fast (60 FPS on Retina display).</p>\n\n<p>I've first used UIKit for drawing, which is of course not suitable for a game. I coulnd't draw a couple of sprites without slowdown.</p>\n\n<p>Then I moved on to OpenGL, because I read it's the closest I can get to the GPU (which I think it means it's the fastest possible). I was using glDrawArrays(). When I ran it on the Simulator, FPS dropped when I was reaching over 200 triangles. People said it was because the Simulator or the computer are not optimized to run iOS OpenGL. Then I tested it on a real device, and to my surprise, the performance difference was really small. It still couldn't run that few triangles smoothly - and I know other games on iOS use a lot more polygons, shaders, 3D graphics, etc.</p>\n\n<p>When I ran it through Instruments to check OpenGL performance, it told me I could speed it up by using VBOs. So I rewrote my code to use VBO instead, updating all vertices each frame. Performance increased very little, and I still can't surpass 200 triangles at consistent 60 FPS. And that is 2D drawing alone without context changes/transformations. I also didn't write the game yet - there are no objects making no CPU-intensive tasks.</p>\n\n<p>Everyone I ask says OpenGL is top performance. What could I possibly be doing wrong? I am assuming OpenGL can handle LOTS of polygons that are updated each frame - is that right? Which method other games use that I see they run fine, like Infinity Blade which is 3D, or even Angry Birds which has lots of ever-updating sprites? What is recommended when making a game?</p>\n"},{"tags":["c++","performance","opengl","optimization","opengl-es"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":107,"score":2,"question_id":12894397,"title":"Opengl performance issue when drawing polygon meshes","body":"<p>I'm using the following code to draw some polygon meshes in a 3D game.</p>\n\n<pre><code>void drawModelFace(const MeshFace *face, float *vertices, float *vertNormals, float *textureVerts)\n{   \n    glBegin(GL_POLYGON);\n    for (int i = 0; i &lt; face-&gt;_numVertices; i++) \n    {\n       glNormal3fv(&amp;vertNormals[3 * face-&gt;_vertices[i]]);\n\n       if (face-&gt;_texVertices)\n       {\n           glTexCoord2fv(&amp;textureVerts[2 * face-&gt;_texVertices[i]]);\n       }\n\n       glVertex3fv(&amp;vertices[3 * face-&gt;_vertices[i]]);\n    }\n    glEnd();\n}\n</code></pre>\n\n<p>My problem is that I'm experiencing some performance issue ingame when this function is called a lot of time. </p>\n\n<p>This function is called on average 50000 times per second which gives a constant 60fps but on some places it's called 100000 times per second which gives a 15fps. (I'm using a today's computer underclocked to 1Ghz to simulate the performance of today's phone)</p>\n\n<p>I heard that immediate mode could be slow that's why I tried using glDrawArrays instead. Here's the code:</p>\n\n<pre><code>void drawModelFace(const MeshFace *face, float *vertices, float *vertNormals, float *textureVerts)\n{   \n    GLfloat vert[3*face-&gt;_numVertices];\n    GLfloat normal[3*face-&gt;_numVertices];\n    GLfloat tex[2*face-&gt;_numVertices];\n\n    glEnableClientState(GL_VERTEX_ARRAY);\n    glEnableClientState(GL_TEXTURE_COORD_ARRAY);\n    glEnableClientState(GL_NORMAL_ARRAY);\n    glVertexPointer(3, GL_FLOAT, 0, vert);\n    glTexCoordPointer(2, GL_FLOAT, 0, tex);\n    glNormalPointer(GL_FLOAT, 0, normal);\n\n    for (int i = 0; i &lt; face-&gt;_numVertices; i++) \n    {\n        vert[0 + (i*3)] = vertices[3 * face-&gt;_vertices[i]];\n        vert[1 + (i*3)] = vertices[3 * face-&gt;_vertices[i]+1];\n        vert[2 + (i*3)] = vertices[3 * face-&gt;_vertices[i]+2];\n\n        normal[0 + (i*3)] = vertNormals[3 * face-&gt;_vertices[i]];\n        normal[1 + (i*3)] = vertNormals[3 * face-&gt;_vertices[i]+1];\n        normal[2 + (i*3)] = vertNormals[3 * face-&gt;_vertices[i]+2];\n\n            if (face-&gt;_texVertices)\n            {\n                tex[0 + (i*2)] = textureVerts[2 * face-&gt;_texVertices[i]];\n                tex[1 + (i*2)] = textureVerts[2 * face-&gt;_texVertices[i]+1];\n            }\n    }\n\n    glDrawArrays(GL_TRIANGLE_FAN ,0, face-&gt;_numVertices);\n    glDisableClientState(GL_VERTEX_ARRAY);\n    glDisableClientState(GL_TEXTURE_COORD_ARRAY);\n    glDisableClientState(GL_NORMAL_ARRAY); \n}\n</code></pre>\n\n<p>But the performance results are exactly the same.</p>\n\n<p>How can I optimize my code to gain some fps?</p>\n\n<p>Note that my final goal is to use this code an android devices thus glBegin and glEnd are not allowed anymore. </p>\n"},{"tags":["python","performance","numpy","scipy","cython"],"answer_count":1,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":117,"score":4,"question_id":13044515,"title":"Cython code 3-4 times slower than Python / Numpy code?","body":"<p>I am trying to convert my Python / Numpy code to Cython code for speedup purposes. However, Cython is MUCH slower (3-4 times) than the Python / Numpy code. Am I using Cython correctly? Am I passing arguments correctly to myc_rb_etc() in my Cython code? What about when I call the integrate function? Thank you in advance for your help.\nHere is my Python / Numpy code: </p>\n\n<pre><code>from pylab import * \nimport pylab as pl\nfrom numpy import *\nimport numpy as np\nfrom scipy import integrate\n\ndef myc_rb_e2f(y,t,k,d):\n\n    M = y[0]\n    E = y[1]\n    CD = y[2]\n    CE = y[3]\n    R = y[4]\n    RP = y[5] \n    RE = y[6]\n\n    S = 0.01\n    if t &gt; 300:\n        S = 5.0\n    #if t &gt; 400\n        #S = 0.01\n\n    t1 = k[0]*S/(k[7]+S);\n    t2 = k[1]*(M/(k[14]+M))*(E/(k[15]+E));\n    t3 = k[5]*M/(k[14]+M);\n    t4 = k[11]*CD*RE/(k[16]+RE);\n    t5 = k[12]*CE*RE/(k[17]+RE);\n    t6 = k[2]*M/(k[14]+M);\n    t7 = k[3]*S/(k[7]+S);\n    t8 = k[6]*E/(k[15]+E);\n    t9 = k[13]*RP/(k[18]+RP);\n    t10 = k[9]*CD*R/(k[16]+R);\n    t11 = k[10]*CE*R/(k[17]+R);\n\n    dM = t1-d[0]*M\n    dE = t2+t3+t4+t5-k[8]*R*E-d[1]*E\n    dCD = t6+t7-d[2]*CD\n    dCE = t8-d[3]*CE\n    dR = k[4]+t9-k[8]*R*E-t10-t11-d[4]*R\n    dRP = t10+t11+t4+t5-t9-d[5]*RP\n    dRE = k[8]*R*E-t4-t5-d[6]*RE\n\n    dy = [dM,dE,dCD,dCE,dR,dRP,dRE]\n\n    return dy\n\nt = np.zeros(10000)\nt = np.linspace(0.,3000.,10000.)\n\n# Initial concentrations of [M,E,CD,CE,R,RP,RE]\ny0 = np.array([0.,0.,0.,0.,0.4,0.,0.25])\nE_simulated = np.zeros([10000,5000])\nE_avg = np.zeros([10000])\nk = np.zeros([19])\nd = np.zeros([7])\n\nfor i in range (0,5000):\n    k[0] = 1.+0.1*randn(1)\n    k[1] = 0.15+0.05*randn(1)\n    k[2] = 0.2+0.05*randn(1)\n    k[3] = 0.2+0.05*randn(1)\n    k[4] = 0.35+0.05*randn(1)\n    k[5] = 0.001+0.0001*randn(1)\n    k[6] = 0.5+0.05*randn(1)\n    k[7] = 0.3+0.05*randn(1)\n    k[8] = 30.+5.*randn(1)\n    k[9] = 18.+3.*randn(1)\n    k[10] = 18.+3.*randn(1)\n    k[11] = 18.+3.*randn(1)\n    k[12] = 18.+3.*randn(1)\n    k[13] = 3.6+0.5*randn(1)\n    k[14] = 0.15+0.05*randn(1)\n    k[15] = 0.15+0.05*randn(1)\n    k[16] = 0.92+0.1*randn(1)\n    k[17] = 0.92+0.1*randn(1)\n    k[18] = 0.01+0.001*randn(1)\n    d[0] = 0.7+0.05*randn(1)\n    d[1] = 0.25+0.025*randn(1)\n    d[2] = 1.5+0.05*randn(1)\n    d[3] = 1.5+0.05*randn(1)\n    d[4] = 0.06+0.01*randn(1)\n    d[5] = 0.06+0.01*randn(1)\n    d[6] = 0.03+0.005*randn(1)\n    r = integrate.odeint(myc_rb_e2f,y0,t,args=(k,d))\n    E_simulated[:,i] = r[:,1]\n\nfor i in range(0,10000):\n    E_avg[i] = sum(E_simulated[i,:])/5000.\n\npl.plot(t,E_avg,'-ro')\npl.show()\n</code></pre>\n\n<p>Here is the code converted into Cython:</p>\n\n<pre><code>cimport numpy as np\nimport numpy as np\nfrom numpy import *\nimport pylab as pl\nfrom pylab import * \nfrom scipy import integrate\n\ndef myc_rb_e2f(y,t,k,d):\n\n    cdef double M = y[0]\n    cdef double E = y[1]\n    cdef double CD = y[2]\n    cdef double CE = y[3]\n    cdef double R = y[4]\n    cdef double RP = y[5] \n    cdef double RE = y[6]\n\n    cdef double S = 0.01\n    if t &gt; 300.0:\n        S = 5.0\n    #if t &gt; 400\n        #S = 0.01\n\n    cdef double t1 = k[0]*S/(k[7]+S)\n    cdef double t2 = k[1]*(M/(k[14]+M))*(E/(k[15]+E))\n    cdef double t3 = k[5]*M/(k[14]+M)\n    cdef double t4 = k[11]*CD*RE/(k[16]+RE)\n    cdef double t5 = k[12]*CE*RE/(k[17]+RE)\n    cdef double t6 = k[2]*M/(k[14]+M)\n    cdef double t7 = k[3]*S/(k[7]+S)\n    cdef double t8 = k[6]*E/(k[15]+E)\n    cdef double t9 = k[13]*RP/(k[18]+RP)\n    cdef double t10 = k[9]*CD*R/(k[16]+R)\n    cdef double t11 = k[10]*CE*R/(k[17]+R)\n\n    cdef double dM = t1-d[0]*M\n    cdef double dE = t2+t3+t4+t5-k[8]*R*E-d[1]*E\n    cdef double dCD = t6+t7-d[2]*CD\n    cdef double dCE = t8-d[3]*CE\n    cdef double dR = k[4]+t9-k[8]*R*E-t10-t11-d[4]*R\n    cdef double dRP = t10+t11+t4+t5-t9-d[5]*RP\n    cdef double dRE = k[8]*R*E-t4-t5-d[6]*RE\n\n    dy = [dM,dE,dCD,dCE,dR,dRP,dRE]\n\n    return dy\n\n\ndef main():\n    cdef np.ndarray[double,ndim=1] t = np.zeros(10000)\n    t = np.linspace(0.,3000.,10000.)\n    # Initial concentrations of [M,E,CD,CE,R,RP,RE]\n    cdef np.ndarray[double,ndim=1] y0 = np.array([0.,0.,0.,0.,0.4,0.,0.25])\n    cdef np.ndarray[double,ndim=2] E_simulated = np.zeros([10000,5000])\n    cdef np.ndarray[double,ndim=2] r = np.zeros([10000,7])\n    cdef np.ndarray[double,ndim=1] E_avg = np.zeros([10000])\n    cdef np.ndarray[double,ndim=1] k = np.zeros([19])\n    cdef np.ndarray[double,ndim=1] d = np.zeros([7])\n    cdef int i\n    for i in range (0,5000):\n        k[0] = 1.+0.1*randn(1)\n        k[1] = 0.15+0.05*randn(1)\n        k[2] = 0.2+0.05*randn(1)\n        k[3] = 0.2+0.05*randn(1)\n        k[4] = 0.35+0.05*randn(1)\n        k[5] = 0.001+0.0001*randn(1)\n        k[6] = 0.5+0.05*randn(1)\n        k[7] = 0.3+0.05*randn(1)\n        k[8] = 30.+5.*randn(1)\n        k[9] = 18.+3.*randn(1)\n        k[10] = 18.+3.*randn(1)\n        k[11] = 18.+3.*randn(1)\n        k[12] = 18.+3.*randn(1)\n        k[13] = 3.6+0.5*randn(1)\n        k[14] = 0.15+0.05*randn(1)\n        k[15] = 0.15+0.05*randn(1)\n        k[16] = 0.92+0.1*randn(1)\n        k[17] = 0.92+0.1*randn(1)\n        k[18] = 0.01+0.001*randn(1)\n        d[0] = 0.7+0.05*randn(1)\n        d[1] = 0.25+0.025*randn(1)\n        d[2] = 1.5+0.05*randn(1)\n        d[3] = 1.5+0.05*randn(1)\n        d[4] = 0.06+0.01*randn(1)\n        d[5] = 0.06+0.01*randn(1)\n        d[6] = 0.03+0.005*randn(1)\n        r = integrate.odeint(myc_rb_e2f,y0,t,args=(k,d))\n        E_simulated[:,i] = r[:,1]\n    for i in range(0,10000):\n        E_avg[i] = sum(E_simulated[i,:])/5000.\n    pl.plot(t,E_avg,'-ro')\n    pl.show()\n</code></pre>\n\n<p>Here are some pstats from cProfile on my Python / Numpy code:</p>\n\n<p><code>ncalls tottime   percall  cumtime  percall</code></p>\n\n<p><code>5000   82.505    0.017  236.760    0.047 {scipy.integrate._odepack.odeint}</code></p>\n\n<p><code>1    1.504    1.504  238.949  238.949 myc_rb_e2f.py:1(&lt;module&gt;)</code></p>\n\n<p><code>5000    0.025    0.000  236.855    0.047 C:\\Python27\\lib\\site-packages\\scipy\\integrate\\odepack.py:18(odeint)</code></p>\n\n<p><code>12291237  154.255    0.000  154.255    0.000 myc_rb_e2f.py:7(myc_rb_e2f)</code></p>\n"},{"tags":["performance","hadoop","shuffle","reduce"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":227,"score":2,"question_id":12717659,"title":"Hadoop reduce shuffle merge in memory","body":"<p>I'm having some performance issues with Reduce Merge phase and I wonder if someone can take a look. I have a 6 GB dataset (text), evenly distributed on the cluster, The dataset has two keys that I then GroupBy into two reducers (I'm using cascading). So each reducer has 3GB of data. I give each reducer 12 GB of memory, but I'm still seeing a 20 minute merge phase.</p>\n\n<p>Two questions: Shouldn't this merge be done entirely in memory (if I have 12 GB of Heap). Even without an in memory merge, 20 minutes seems like way way too long to merge 3GB, especially with 12 disks(JBOD) and 12 cores on a node. I'm wondering if I'm writing the partial merge data to the wrong place (HDFS, vs local?).</p>\n\n<p>the MAPRFS_BYTES_READ, and MAPRFS_BYTES_WRITTEN are interesting. The initial dataset is 6GB (which it shows in the Map column). Somehow sorting increases it to 17GB, which seems odd. Then in the reduce Phase it's reading 23GB from MapRfs, and writing 17GB. Should the reduce phase merge data be written to MapRFS or to the local FS? WHy would the size grow so much over the initial dataset (no compression is used, it's straight text)</p>\n\n<hr>\n\n<pre><code>    Counter     Map     Reduce  Total\nJob Counters    Aggregate execution time of mappers(ms)     0   0   29,887,359\nLaunched reduce tasks   0   0   2\nRack-local map tasks    0   0   4\nLaunched map tasks  0   0   353\nData-local map tasks    0   0   311\ncascading.flow.SliceCounters    Read_Duration   329,399     366,004     695,403\nTuples_Read     252,000,000     67,896,295  319,896,295\nTuples_Written  252,000,000     0   252,000,000\nProcess_End_Time    476,294,761,317,139     0   476,294,761,317,139\nWrite_Duration  2,713,840   0   2,713,840\nProcess_Begin_Time  476,294,753,764,176     2,698,557,228,678   478,993,310,992,854\nFileSystemCounters  MAPRFS_BYTES_READ   6,651,978,400   21,721,014,791  28,372,993,191\nMAPRFS_BYTES_WRITTEN    17,044,716,578  17,044,701,398  34,089,417,976\nFILE_BYTES_WRITTEN  19,046,005  107,748     19,153,753\nMap-Reduce Framework    Map input records   252,000,000     0   252,000,000\nReduce shuffle bytes    0   16,980,659,887  16,980,659,887\nSpilled Records     252,000,000     0   252,000,000\nMap output bytes    16,540,701,046  0   16,540,701,046\nCPU_MILLISECONDS    18,861,020  7,640,360   26,501,380\nMap input bytes     6,644,947,675   0   6,644,947,675\nCombine input records   0   0   0\nSPLIT_RAW_BYTES     97,428  0   97,428\nReduce input records    0   67,896,295  67,896,295\nReduce input groups     0   2   2\nCombine output records  0   0   0\nPHYSICAL_MEMORY_BYTES   324,852,019,200     15,041,486,848  339,893,506,048\nReduce output records   0   0   0\nVIRTUAL_MEMORY_BYTES    626,863,038,464     26,729,230,336  653,592,268,800\nMap output records  252,000,000     0   252,000,000\nGC time elapsed (ms)    1,568,523   76,636  1,645,159\ncascading.flow.StepCounters     Tuples_Read     252,000,000     0   252,000,000\n</code></pre>\n\n<hr>\n\n<pre><code>name    value\nfs.s3n.impl org.apache.hadoop.fs.s3native.NativeS3FileSystem\nmapreduce.heartbeat.100 1000\nmapred.task.cache.levels    2\nhadoop.tmp.dir  /tmp/hadoop-${user.name}\nhadoop.native.lib   true\nmap.sort.class  org.apache.hadoop.util.QuickSort\nmapreduce.jobtracker.recovery.dir   /var/mapr/cluster/mapred/jobTracker/recovery\nmapreduce.heartbeat.1000    10000\nipc.client.idlethreshold    4000\nmapred.system.dir   /var/mapr/cluster/mapred/jobTracker/system\nmapreduce.cluster.reduce.userlog.retain-size    10485760\nmapred.job.tracker.persist.jobstatus.hours  0\nio.skip.checksum.errors false\nfs.default.name maprfs:///\nmapred.cluster.reduce.memory.mb -1\nmapred.child.tmp    ./tmp\nfs.har.impl.disable.cache   true\nmapred.jobtracker.jobhistory.lru.cache.size 5\nmapred.skip.reduce.max.skip.groups  0\ncascading.flow.step.num 1\nmapred.jobtracker.instrumentation   org.apache.hadoop.mapred.JobTrackerMetricsInst\nmapr.localvolumes.path  /var/mapr/local\nmapred.tasktracker.dns.nameserver   default\nio.sort.factor  50\nmapred.output.value.groupfn.class   cascading.tuple.hadoop.util.GroupingComparator\nmapreduce.use.maprfs    true\nmapred.task.timeout 600000\nmapred.max.tracker.failures 4\nhadoop.rpc.socket.factory.class.default org.apache.hadoop.net.StandardSocketFactory\nmapred.mapoutput.key.class  cascading.tuple.io.TuplePair\nfs.hdfs.impl    org.apache.hadoop.hdfs.DistributedFileSystem\nmapred.queue.default.acl-administer-jobs    \nmapred.output.key.class org.apache.hadoop.io.Text\nmapred.skip.map.auto.incr.proc.count    true\nmapred.map.runner.class cascading.flow.hadoop.FlowMapper\nmapreduce.job.complete.cancel.delegation.tokens true\nmapreduce.tasktracker.heapbased.memory.management   false\nio.mapfile.bloom.size   1048576\ntasktracker.http.threads    2\nmapred.job.shuffle.merge.percent    0.70\ncascading.flow.id   853276BF02049D394C31880B08C9E6CC\nmapred.child.renice 10\nfs.ftp.impl org.apache.hadoop.fs.ftp.FTPFileSystem\nuser.name   jdavis\nmapred.fairscheduler.smalljob.max.inputsize 10737418240\nmapred.output.compress  false\nio.bytes.per.checksum   512\nmapred.healthChecker.script.timeout 600000\ntopology.node.switch.mapping.impl   org.apache.hadoop.net.ScriptBasedMapping\nmapred.reduce.slowstart.completed.maps  0.95\nmapred.reduce.max.attempts  4\nfs.ramfs.impl   org.apache.hadoop.fs.InMemoryFileSystem\nmapr.localoutput.dir    output\nmapred.skip.map.max.skip.records    0\nmapred.jobtracker.port  9001\nmapred.cluster.map.memory.mb    -1\nmapreduce.tasktracker.prefetch.maptasks 1.0\nhadoop.security.group.mapping   org.apache.hadoop.security.ShellBasedUnixGroupsMapping\nmapreduce.tasktracker.task.slowlaunch   false\nmapred.job.tracker.persist.jobstatus.dir    /var/mapr/cluster/mapred/jobTracker/jobsInfo\nmapred.jar  /var/mapr/cluster/mapred/jobTracker/staging/jdavis/.staging/job_201210022148_0086/job.jar\nfs.s3.buffer.dir    ${hadoop.tmp.dir}/s3\njob.end.retry.attempts  0\nfs.file.impl    org.apache.hadoop.fs.LocalFileSystem\ncascading.app.name  omeg\nmapred.local.dir.minspacestart  0\nmapred.output.compression.type  RECORD\nfs.mapr.working.dir /user/$USERNAME/\nfs.maprfs.impl  com.mapr.fs.MapRFileSystem\nfs.https.impl   cascading.tap.hadoop.io.HttpFileSystem\ntopology.script.number.args 100\nio.mapfile.bloom.error.rate 0.005\nmapred.cluster.max.reduce.memory.mb -1\nmapred.max.tracker.blacklists   4\nmapred.task.profile.maps    0-2\nmapred.userlog.retain.hours 24\nmapred.job.tracker.persist.jobstatus.active false\nhadoop.security.authorization   false\nlocal.cache.size    10737418240\nmapred.min.split.size   0\nmapred.map.tasks    353\nmapred.tasktracker.task-controller.config.overwrite true\ncascading.app.appjar.path   /home/jdavis/tmp/omeg.jar\nmapred.output.value.class   org.apache.hadoop.io.Text\nmapred.partitioner.class    cascading.tuple.hadoop.util.GroupingPartitioner\nmapreduce.maprfs.use.compression    true\nmapred.job.queue.name   default\nmapreduce.tasktracker.reserved.physicalmemory.mb.low    0.90\ncascading.group.comparator.size 3\nipc.server.listen.queue.size    128\ngroup.name  common\nmapred.inmem.merge.threshold    0\njob.end.retry.interval  30000\nmapred.fairscheduler.smalljob.max.maps  10\nmapred.skip.attempts.to.start.skipping  2\nfs.checkpoint.dir   ${hadoop.tmp.dir}/dfs/namesecondary\nmapred.reduce.tasks 2\nmapred.merge.recordsBeforeProgress  10000\nmapred.userlog.limit.kb 0\nmapred.job.reduce.memory.mb -1\nwebinterface.private.actions    true\nio.sort.spill.percent   0.99\nmapred.job.shuffle.input.buffer.percent 0.80\nmapred.job.name [853276BF02049D394C31880B08C9E6CC/DCB7B555F1FC65C767B8E2CD716607AA] copyr/(1/1) /user/jdavis/ctest/end\nmapred.map.tasks.speculative.execution  false\nhadoop.util.hash.type   murmur\nmapred.map.max.attempts 4\nmapreduce.job.acl-view-job\n\nmapred.job.tracker.handler.count    10\nmapred.input.format.class   cascading.tap.hadoop.io.MultiInputFormat\nmapred.tasktracker.expiry.interval  600000\nmapred.jobtracker.maxtasks.per.job  -1\nmapred.jobtracker.job.history.block.size    3145728\nkeep.failed.task.files  false\nmapred.output.format.class  org.apache.hadoop.mapred.TextOutputFormat\nipc.client.tcpnodelay   false\nmapred.task.profile.reduces 0-2\nmapred.output.compression.codec org.apache.hadoop.io.compress.DefaultCodec\nio.map.index.skip   0\nmapred.working.dir  /user/jdavis\nipc.server.tcpnodelay   false\nhadoop.proxyuser.root.hosts \nmapred.reducer.class    cascading.flow.hadoop.FlowReducer\ncascading.app.id    A593B4669179BB6F06771249E7ADFA48\nmapred.used.genericoptionsparser    true\njobclient.progress.monitor.poll.interval    1000\nmapreduce.tasktracker.jvm.idle.time 10000\nmapred.job.map.memory.mb    -1\nhadoop.logfile.size 10000000\nmapred.reduce.tasks.speculative.execution   false\nmapreduce.job.dir   maprfs:/var/mapr/cluster/mapred/jobTracker/staging/jdavis/.staging/job_201210022148_0086\nmapreduce.tasktracker.outofband.heartbeat   true\nmapreduce.reduce.input.limit    -1\nmapred.tasktracker.ephemeral.tasks.ulimit   4294967296&gt;\nfs.s3n.block.size   67108864\nfs.inmemory.size.mb 200\nmapred.fairscheduler.smalljob.max.reducers  10\nhadoop.security.authentication  simple\nfs.checkpoint.period    3600\ncascading.flow.step.id  DCB7B555F1FC65C767B8E2CD716607AA\nmapred.job.reuse.jvm.num.tasks  -1\nmapred.jobtracker.completeuserjobs.maximum  5\nmapreduce.cluster.map.userlog.retain-size   10485760\nmapred.task.tracker.task-controller org.apache.hadoop.mapred.LinuxTaskController\nmapred.output.key.comparator.class  cascading.tuple.hadoop.util.GroupingSortingComparator\nfs.s3.maxRetries    4\nmapred.cluster.max.map.memory.mb    -1\nmapred.mapoutput.value.class    cascading.tuple.Tuple\nmapred.map.child.java.opts  -XX:ErrorFile=/opt/cores/mapreduce_java_error%p.log\nmapred.job.tracker.history.completed.location   /var/mapr/cluster/mapred/jobTracker/history/done\nmapred.local.dir    /tmp/mapr-hadoop/mapred/local\nfs.hftp.impl    org.apache.hadoop.hdfs.HftpFileSystem\nfs.trash.interval   0\nfs.s3.sleepTimeSeconds  10\nmapred.submit.replication   10\nfs.har.impl org.apache.hadoop.fs.HarFileSystem\nmapreduce.heartbeat.10  300\ncascading.version   Concurrent, Inc - Cascading 2.0.5\nmapred.map.output.compression.codec org.apache.hadoop.io.compress.DefaultCodec\nmapred.tasktracker.dns.interface    default\nhadoop.proxyuser.root.groups    root\nmapred.job.tracker  maprfs:///\nmapreduce.job.submithost    c10-m001.wowrack.upstream.priv\nmapreduce.tasktracker.cache.local.numberdirectories 10000\nio.seqfile.sorter.recordlimit   1000000\nmapreduce.heartbeat.10000   100000\nmapred.line.input.format.linespermap    1\nmapred.jobtracker.taskScheduler org.apache.hadoop.mapred.FairScheduler\nmapred.tasktracker.instrumentation  org.apache.hadoop.mapred.TaskTrackerMetricsInst\nmapred.tasktracker.taskmemorymanager.killtask.maxRSS    false\nmapred.child.taskset    true\njobclient.completion.poll.interval  5000\nmapred.fairscheduler.smalljob.max.reducer.inputsize 1073741824\nmapred.local.dir.minspacekill   0\nio.sort.record.percent  0.28\nmapr.localspill.dir spill\nio.compression.codec.lzo.class  com.hadoop.compression.lzo.LzoCodec\nfs.kfs.impl org.apache.hadoop.fs.kfs.KosmosFileSystem\nmapred.tasktracker.reduce.tasks.maximum (CPUS &gt; 2) ? (CPUS * 0.70): 1\nmapred.temp.dir ${hadoop.tmp.dir}/mapred/temp\nmapred.tasktracker.ephemeral.tasks.maximum  1\nfs.checkpoint.edits.dir ${fs.checkpoint.dir}\nmapred.tasktracker.tasks.sleeptime-before-sigkill   5000\nmapred.job.reduce.input.buffer.percent  0.0\nmapred.tasktracker.indexcache.mb    10\nmapreduce.task.classpath.user.precedence    false\nmapreduce.job.split.metainfo.maxsize    -1\nhadoop.logfile.count    10\nfs.automatic.close  true\nmapred.skip.reduce.auto.incr.proc.count true\nmapreduce.job.submithostaddress 10.100.0.99\nmapred.child.oom_adj    10\nio.seqfile.compress.blocksize   1000000\nfs.s3.block.size    67108864\nmapred.tasktracker.taskmemorymanager.monitoring-interval    3000\nmapreduce.tasktracker.volume.healthcheck.interval   60000\nmapred.cluster.ephemeral.tasks.memory.limit.mb  200\nmapreduce.jobtracker.staging.root.dir   /var/mapr/cluster/mapred/jobTracker/staging\nmapred.acls.enabled false\nmapred.queue.default.state  RUNNING\nmapred.fairscheduler.smalljob.schedule.enable   false\nmapred.queue.names  default\nfs.hsftp.impl   org.apache.hadoop.hdfs.HsftpFileSystem\nmapred.fairscheduler.eventlog.enabled   false\nmapreduce.jobtracker.recovery.maxtime   480\nmapred.task.tracker.http.address    0.0.0.0:50060\nmapreduce.jobtracker.inline.setup.cleanup   false\nmapred.reduce.parallel.copies   40\nio.seqfile.lazydecompress   true\nmapred.tasktracker.ephemeral.tasks.timeout  10000\nmapred.output.dir   maprfs:/user/jdavis/ctest/end\nmapreduce.tasktracker.group root\nhadoop.workaround.non.threadsafe.getpwuid   false\nio.sort.mb  512\nmapred.reduce.child.java.opts   -Xmx12000m\nipc.client.connection.maxidletime   10000\nmapred.compress.map.output  false\nhadoop.security.uid.cache.secs  14400\nmapred.task.tracker.report.address  127.0.0.1:0\nmapred.healthChecker.interval   60000\nipc.client.kill.max 10\nipc.client.connect.max.retries  10\nfs.http.impl    cascading.tap.hadoop.io.HttpFileSystem\nfs.s3.impl  org.apache.hadoop.fs.s3.S3FileSystem\nmapred.fairscheduler.assignmultiple true\nmapred.user.jobconf.limit   5242880\nmapred.input.dir    maprfs:/user/jdavis/ctest/mid\nmapred.job.tracker.http.address 0.0.0.0:50030\nio.file.buffer.size 131072\nmapred.jobtracker.restart.recover   true\nio.serializations   cascading.tuple.hadoop.TupleSerialization,org.apache.hadoop.io.serializer.WritableSerialization\nmapreduce.use.fastreduce    false\nmapred.reduce.copy.backoff  300\nmapred.task.profile false\nmapred.jobtracker.retiredjobs.cache.size    300\njobclient.output.filter FAILED\nmapred.tasktracker.map.tasks.maximum    (CPUS &gt; 2) ? (CPUS * 0.80) : 1\nio.compression.codecs   org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec\nfs.checkpoint.size  67108864\ncascading.sort.comparator.size  3\n</code></pre>\n\n<hr>\n\n<pre><code>2012-10-02 19:30:50,676 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processName=SHUFFLE, sessionId=\n2012-10-02 19:30:50,737 INFO org.apache.hadoop.mapreduce.util.ProcessTree: setsid exited with exit code 0\n2012-10-02 19:30:50,742 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: /proc/&lt;pid&gt;/status does not have information about swap space used(VmSwap). Can not track swap usage of a task.\n2012-10-02 19:30:50,742 INFO org.apache.hadoop.mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.mapreduce.util.LinuxResourceCalculatorPlugin@27b62aab\n2012-10-02 19:30:50,903 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 9115 may have finished in the interim.\n2012-10-02 19:31:01,663 INFO org.apache.hadoop.mapred.Merger: Merging 37 sorted segments\n2012-10-02 19:31:01,672 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 36 segments left of total size: 1204882102 bytes\n2012-10-02 19:31:03,079 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 7596 may have finished in the interim.\n2012-10-02 19:31:15,487 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 4803 may have finished in the interim.\n2012-10-02 19:31:15,489 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 11069 may have finished in the interim.\n2012-10-02 19:33:37,821 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 20846 may have finished in the interim.\n2012-10-02 19:33:59,274 INFO org.apache.hadoop.mapred.Merger: Merging 35 sorted segments\n2012-10-02 19:33:59,275 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 35 segments left of total size: 1176895576 bytes\n2012-10-02 19:34:02,131 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 21791 may have finished in the interim.\n2012-10-02 19:34:29,927 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 22847 may have finished in the interim.\n2012-10-02 19:36:32,181 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 30438 may have finished in the interim.\n2012-10-02 19:37:18,243 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 3852 may have finished in the interim.\n2012-10-02 19:37:26,292 INFO org.apache.hadoop.mapred.Merger: Merging 37 sorted segments\n2012-10-02 19:37:26,293 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 37 segments left of total size: 1233203028 bytes\n2012-10-02 19:39:07,695 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 9813 may have finished in the interim.\n2012-10-02 19:39:10,764 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 10045 may have finished in the interim.\n2012-10-02 19:39:56,829 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 17383 may have finished in the interim.\n2012-10-02 19:40:18,295 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 19584 may have finished in the interim.\n2012-10-02 19:40:32,307 INFO org.apache.hadoop.mapred.Merger: Merging 58 sorted segments\n2012-10-02 19:40:32,308 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 44 segments left of total size: 1206978885 bytes\n2012-10-02 19:41:35,154 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 26361 may have finished in the interim.\n2012-10-02 19:43:53,644 INFO org.apache.hadoop.mapred.Merger: Merging 56 sorted segments\n2012-10-02 19:43:53,645 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 56 segments left of total size: 1217287352 bytes\n2012-10-02 19:46:55,246 INFO org.apache.hadoop.mapred.Merger: Merging 44 sorted segments\n2012-10-02 19:46:55,246 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 44 segments left of total size: 1221163604 bytes\n2012-10-02 19:49:57,894 INFO org.apache.hadoop.mapred.Merger: Merging 85 sorted segments\n2012-10-02 19:49:57,895 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 62 segments left of total size: 1229975233 bytes\n2012-10-02 19:52:09,914 WARN org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree: The process 25247 may have finished in the interim.\n2012-10-02 19:52:52,620 INFO org.apache.hadoop.mapred.Merger: Merging 1 sorted segments\n2012-10-02 19:52:52,620 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32065409 bytes\n2012-10-02 19:52:53,327 INFO org.apache.hadoop.mapred.Merger: Merging 8 sorted segments\n2012-10-02 19:52:53,345 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 8 segments left of total size: 8522450575 bytes\n2012-10-02 19:52:53,366 INFO cascading.flow.hadoop.FlowReducer: cascading version: Concurrent, Inc - Cascading 2.0.5\n</code></pre>\n"},{"tags":["mysql","performance","perl","security","cgi"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":69,"score":0,"question_id":13043648,"title":"Perl: Handing $dbh off to module. Security and performance?","body":"<p>So I have a cgi script,</p>\n\n<pre><code>#!/usr/bin/perl -T\n\nuse strict;\nuse warnings;\nuse DBI;\nuse WebEngine;\n\n\nmy $dbh = DBI-&gt;connect('DBI:mysql:database', $username, $password)\n    || die \"Could not connect to database: $DBI::errstr\";\n\nmy $we = WebEngine-&gt;new($dbh)\n    or die(\"Failed to instantiate WebEngine object:\\n$!\\n\");\n\n$userID = $we-&gt;register(\"MyUsername\", $dbh);\n</code></pre>\n\n<p>This script creates a database handler and then uses a module I made to deal with most of the back-end of the site to register a username and return a userID number.</p>\n\n<p>I have three questions about this.</p>\n\n<ul>\n<li><p>Does creating this $dbh in this script increase performance by keeping a database connection open?  </p></li>\n<li><p>Could I put the $dbh in my module and not fear being inefficient? </p></li>\n<li><p>Is there a security benefit to keeping the $dbh (and the associated info(I keep my pass in plain text in the code; is that bad?)) in the module that is not directly interacted with through my website?</p></li>\n</ul>\n"},{"tags":["performance","algorithm","puzzle"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":97,"score":1,"question_id":13037826,"title":"brute forcing a brain puzzle","body":"<p>I was given a brain puzzle from lonpos.cc as a present. I was curius of how many different solutions there were, and I quite enjoy writing algorithms and code, so I started writing an application to brute force it.</p>\n\n<p>The puzzle looks like this : <a href=\"http://www.lonpos.cc/images/LONPOSdb.jpg\" rel=\"nofollow\">http://www.lonpos.cc/images/LONPOSdb.jpg</a> / <a href=\"http://cdn100.iofferphoto.com/img/item/191/498/944/u2t6.jpg\" rel=\"nofollow\">http://cdn100.iofferphoto.com/img/item/191/498/944/u2t6.jpg</a></p>\n\n<p>It's a board of 20x14 \"points\". And all puzzle pieces can be flipped and turned. I wrote an application where each piece (and the puzzle) is presented like this:</p>\n\n<pre><code>01010\n00100\n01110\n01110\n11111\n01010\n</code></pre>\n\n<p>Now my application so far is reasonably simple.</p>\n\n<p>It takes the list of pieces and a blank board, pops of piece #0\nflips it in every direction, and for that piece tries to place it for every x and y coordinate. If it successfully places a piece it passes a copy of the new \"board\" with some pieces taken to a recursive function, and tries all combinations for their pieces.</p>\n\n<p>Explained in pseudocode:</p>\n\n<pre><code>bruteForce(Board base, List pieces) {\n    for (Piece in pieces.pop, piece.pop.flip, piece.pop.flip2...) {\n        int x,y = 0;\n        if canplace(piece, x, y) {\n            Board newBoard = base.clone();\n            newBoard.placePiece(piece, x, y);\n            bruteForce(newBoard, pieces);\n        }\n        ## increment x until x &gt; width, then y\n    }\n}\n</code></pre>\n\n<p>Now I'm trying to find out ways to make this quicker. Things I've thought of so far:</p>\n\n<ol>\n<li>Making it solve in parallel - Implemented, now using 4 threads.</li>\n<li>Sorting the pieces, and only trying to place the pieces that will fit in the x,y space we're trying to fit. (Aka if we're on the bottom row, and we only have 4 \"points\" from our position to the bottom, dont try the ones that are 8 high).</li>\n<li>Not duplicating the board, instead using placePiece and removePiece or something like it.</li>\n<li>Checking for \"invalid\" boards, aka if a piece is impossible to reach (boxed in completely).</li>\n</ol>\n\n<p>Anyone have any creative ideas on how I can do this quicker? Or any way to mathematically calculate how many different combinations there are? </p>\n"},{"tags":["php","sql","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":49,"score":1,"question_id":13044162,"title":"SQL Efficiency and speed","body":"<p>I have to delete records that contain the id in an array.</p>\n\n<p>I just don't know which is better to use. Which is faster and more efficient?</p>\n\n<p><strong>code 1</strong></p>\n\n<pre><code>$array = array('123','456','789' ...)//over 900 entries\n$id = implode(',', $array);\n$sql = 'DELETE FROM email WHERE id IN ('.$id.')';\n//execute sql\n</code></pre>\n\n<p>or</p>\n\n<p><strong>code2</strong></p>\n\n<pre><code>$array = array('123','456','789' ...)//over 900 entries\nfor($x=0;$x&lt;count($array);$x++){\n  $sql='DELETE FROM email WHERE id = '.$array[$x].' ';\n  //execute sql\n}\n</code></pre>\n\n<p>Which from the two is faster and more efficient when executed?</p>\n"},{"tags":["performance","sockets","web-applications"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":38,"score":0,"question_id":13044133,"title":"Is Socket Programing in combination with Web Application, Good?","body":"<p>I have to develop a Service which is to interact with a Server Socket at remote for activating a subscriber request, i have achieved the it with help of plain socket programming. now, my Service is to be available for other application to initiate the requests sent by the subscriber, i wanted to create a web application for this so that using a URL hit they can initiate the request. but, i m under perception that web application in combination with Socket programming is not that good, my concern is with performance as of now.</p>\n\n<p>Would request to guide me a proper way so as to make my service available either as web application or a Standalone app.</p>\n"},{"tags":["performance","optimization","profiling"],"answer_count":17,"favorite_count":17,"up_vote_count":38,"down_vote_count":16,"view_count":12411,"score":22,"question_id":266373,"title":"One could use a profiler, but why not just halt the program?","body":"<p>If something is making a single-thread program take, say, 10 times as long as it should, you could run a profiler on it. You could also just halt it with a \"pause\" button, and you'll see exactly what it's doing. </p>\n\n<p>Even if it's only 10% slower than it should be, if you halt it more times, before long you'll see it repeatedly doing the unnecessary thing. Usually the problem is a function call somewhere in the middle of the stack that isn't really needed. This doesn't measure the problem, but it sure does find it.</p>\n\n<p>Edit: The objections mostly assume that you only take 1 sample. If you're serious, take 10. Any line of code causing some percentage of wastage, like 40%, will appear on the stack on that fraction of samples, on average. Bottlenecks (in single-thread code) can't hide from it.</p>\n\n<p>EDIT: To show what I mean, many objections are of the form \"there aren't enough samples, so what you see could be entirely spurious\" - vague ideas about chance. But if something of <em>any recognizable description</em>, not just being in a routine or the routine being active, is in effect for 30% of the time, then the probability of seeing it on any given sample is 30%. </p>\n\n<p>Then suppose only 10 samples are taken. The number of times the problem will be seen in 10 samples follows a <a href=\"http://en.wikipedia.org/wiki/Binomial_distribution\" rel=\"nofollow\">binomial distribution</a>, and the probability of seeing it 0 times is .028. The probability of seeing it 1 time is .121. For 2 times, the probability is .233, and for 3 times it is .267, after which it falls off. Since the probability of seeing it less than two times is .028 + .121 = .139, that means the probability of seeing it two or more times is 1 - .139 = .861. The general rule is if you see something you could fix on two or more samples, it is worth fixing. </p>\n\n<p>In this case, the chance of seeing it in 10 samples is 86%. If you're in the 14% who don't see it, just take more samples until you do. (If the number of samples is increased to 20, the chance of seeing it two or more times increases to more than 99%.) So it hasn't been precisely measured, but it has been precisely found, and it's important to understand that it could easily be something that a profiler could not actually find, such as something involving the state of the data, not the program counter.</p>\n"},{"tags":["networking","network-programming","performance","ping"],"answer_count":7,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":2956,"score":1,"question_id":4575537,"title":"Calculate upload/download speed by ping","body":"<p>How to calculate the speed of an internet connection by some average ping rates.What are the calculations involved in it.IS it possible to calculate upload/download limit by ping rate</p>\n\n<p><strong>EDIT</strong>\nIf ping is not a solution what else is?</p>\n"},{"tags":["c#","performance","math","matrix","usability"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":88,"score":0,"question_id":13035464,"title":"Fast and useful way of storing matrix values in C#","body":"<p>I need to create a 4x4 matrix class for a 3D engine in C#.\nI have seen some other engines storing the matrix values in single float member variables / fields like this:</p>\n\n<pre><code>float m11, m12, m13, m14\nfloat m21, m22, m23, m24\nfloat m31, m32, m33, m34\nfloat m41, m42, m43, m44\n</code></pre>\n\n<p>However, I thought storing them in a two dimensional array would be more useful for transformations / calculations with the matrices:</p>\n\n<pre><code>float[4][4];\n</code></pre>\n\n<p>I also thought of a one dimensional array - but it looks less self explanatory and wouldn't give me an advantage over the first option:</p>\n\n<pre><code>float[16];\n</code></pre>\n\n<p>In C++, I always used the \"union\" keyword to have all of the above storing possibilites at once. However, C# does not seem to have this keyword, so I have to decide which one I want to use.</p>\n\n<p>What is the fastest way of storing the 4x4 matrix when applying transformations etc.?\nWhich option would you choose when thinking about usability?</p>\n"},{"tags":["c++","c","performance","math","sin"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":154,"score":3,"question_id":13042110,"title":"Create a Fast Sin() function to improve fps ? Fast sin() function?","body":"<p>I am rendering 500x500 points in real-time. \nI have to compute the position of points using atan() and sin() functions. By using atan() and sin() I am getting <strong>24 fps</strong> (frames per second).</p>\n\n<pre><code>float thetaC = atan(value);\nfloat h = (value) / (sin(thetaC)));\n</code></pre>\n\n<p>If I don't use sin() I am getting <strong>52 fps</strong>.</p>\n\n<p>and if I dont use atan() I am <strong>30 fps</strong>.</p>\n\n<p>So, the big problem is with sin(). How can I use Fast Sin version. Can I create a Look Up Table for that ? I don't have any specific values to create LUT. what can I do in this situation ? </p>\n\n<p><em><strong>PS: I have also tried fast sin function of ASM but not getting any difference.</em></strong></p>\n\n<p>Thanks.</p>\n"},{"tags":["performance","time-complexity","asymptotic-complexity"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":90,"score":-1,"question_id":13040950,"title":"Time complexity, binary (search) tree","body":"<p>assume I have a complete binary tree up-to a certain depth <em>d</em>. What would the time complexity be to traverse (pre-order traversal) this tree.</p>\n\n<p>I am confused because I know that the amount of nodes in the tree is 2^d, so therefore the time complexity would be <code>BigO(2^d)</code> ? because the tree is growing exponentially. </p>\n\n<p>But, upon research on the internet, Everyone states that's traversal is <code>BigO(n)</code> where n is the number of elements (which would be <code>2^d</code> in this case), not <code>BigO(2^d)</code>, what am I missing?</p>\n\n<p>thanks  </p>\n"},{"tags":["sql-server","performance","sql-server-2000","time-series"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":58,"score":1,"question_id":13037557,"title":"SQL Server : time-series data performance","body":"<p>I have a table of a little over 1 billion rows of time-series data with fantastic insert performance but (sometimes) awful select performance.</p>\n\n<p>Table <code>tblTrendDetails</code> (PK is ordered as shown):</p>\n\n<pre><code>PK  TrendTime    datetime\nPK  CavityId     int\nPK  TrendValueId int\n    TrendValue   real\n</code></pre>\n\n<p>The table is continuously pulling in new data and purging old data, so insert and delete performance needs to remain snappy.</p>\n\n<p>When executing a query such as the following, performance is poor (30 sec):</p>\n\n<pre><code>SELECT * \nFROM tblTrendDetails\nWHERE TrendTime BETWEEN @inMinTime AND @inMaxTime\n  AND CavityId = @inCavityId\n  AND TrendValueId = @inTrendId\n</code></pre>\n\n<p>If I execute the same query again (with similar times, but any <code>@inCavityId</code> or <code>@inTrendId</code>), performance is very good (1 sec). Performance counters show that disk access is the culprit the first time the query is run.  </p>\n\n<p>Any recommendations regarding how to improve performance without (significantly) adversely affecting the insert or delete performance?  Any suggestions (including completely changing the underlying database) are welcome.</p>\n"},{"tags":["java","performance","spring","rest","scalability"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":157,"score":1,"question_id":12787275,"title":"Tomcat Performance with Huge String Responses","body":"<p>I've a springRest web service endpoint that returns a string text of size 4MB. As we do load test of this endpoint we constantly see heap spikes and ultimately the system crashes. I'm thinking - as we make requests each request is serviced by a thread separately. My hypothesis is: Because the string is saved in a global static variable, each thread takes a copy of the 4MB and after around 3000 requests the heap is all consumed and the system crashes because 3000 threads taking each 4MB is around 12GB and hence the system goes out of memory. But this is my hypothesis.</p>\n\n<p>My question: doesn't tomcat reclaim the memory after each thread that processes a request has done it's job?\nIs this related to GC (garbage collection)?\nIn the request life cycle - as a request comes, a thread is created (per that request) does the thread get it's own copy of the response or it just references the response? if that huge string response is copied to each thread then may be that's why the heap spike is showing.\nWhen the response is given back to the client how does tomcat reclaim the resources of that thread? when does it do it? is claiming request threads related to GC?</p>\n\n<p>Another aspect that i observed is: delay on the method socketWrite0() - this takes from 70-95% of the response time. It is a bottle neck i think. So in the flow of request response - who writes to the socket? the thread? or the thread hands the response to tomcat and tomcat writes it?</p>\n\n<p>If any of you could give me a hint or an aspect to look at that relates memory spikes with huge string responses, i'd really appreciate it. thanks guys!</p>\n\n<p>rose</p>\n"},{"tags":["java","performance","reflection","interface"],"answer_count":3,"favorite_count":0,"up_vote_count":7,"down_vote_count":1,"view_count":4642,"score":6,"question_id":1122506,"title":"Interpreting Java reflection performance: Why is it surprisingly very fast?","body":"<p>I've seen other threads saying java reflection performance is 10-100x slower than when using non-reflection calls.</p>\n\n<p>My tests in 1.6 have shown that this is not the case but I found some other interesting things that I need someone to explain to me.</p>\n\n<p>I have objects that implement my interface.  I did three things 1) using a reference to an Object I cast that object to the interface and call the method through the interface 2) using a reference to the actual object call the method directly and 3) call the method through reflection.  I saw that #1 interface call was fastest followed closely by #3 reflection but I noticed that the direct method call was the slowest by a good margin.</p>\n\n<p>I don't understand that, I would've expected the direct call to be fastest, then the interface, then reflection would be much much more slow.</p>\n\n<p>Blah and ComplexClass are in a different package from the main class and both have a doSomething(int x) method that implements the interface and just prints the integer x.</p>\n\n<p>Here are my results (times in ms, results very similar w/ multiple trials):\ncalling a method directly: 107194\ncalling a method directly from an object cast to an interface: 89594\ncalling a method through reflection: 90453</p>\n\n<p>Here is my code:</p>\n\n<pre><code>public class Main\n{\n\n    /**\n     * @param args the command line arguments\n     */\n    public static void main(String[] args)\n    {\n        Blah x = new Blah();\n        ComplexClass cc = new ComplexClass();\n        test((Object) x, cc);\n    }\n\n    public static void test(Object x, ComplexClass cc)\n    {\n        long start, end;\n        long time1, time2, time3 = 0;\n        int numToDo = 1000000;\n        MyInterface interfaceClass = (MyInterface) x;\n\n        //warming up the cache\n        for (int i = 0; i &lt; numToDo; i++)\n        {\n            cc.doSomething(i); //calls a method directly\n        }\n\n        start = System.currentTimeMillis();\n        for (int i = 0; i &lt; numToDo; i++)\n        {\n            cc.doSomething(i); //calls a method directly\n        }\n        end = System.currentTimeMillis();\n        time1 = end - start;\n\n        start = System.currentTimeMillis();\n        for (int i = 0; i &lt; numToDo; i++)\n        {\n            interfaceClass.doSomething(i); //casts an object to an interface then calls the method\n        }\n        end = System.currentTimeMillis();\n        time2 = end - start;\n\n\n        try\n        {\n            Class xClass = x.getClass();\n            Class[] argTypes =\n            {\n                int.class\n            };\n            Method m = xClass.getMethod(\"doSomething\", argTypes);\n            Object[] paramList = new Object[1];\n            start = System.currentTimeMillis();\n            for (int i = 0; i &lt; numToDo; i++)\n            {\n                paramList[0] = i;\n                m.invoke(x, paramList); //calls via reflection\n            }\n            end = System.currentTimeMillis();\n            time3 = end - start;\n\n        } catch (Exception ex)\n        {\n        }\n\n        System.out.println(\"calling a method directly: \" + time1);\n        System.out.println(\"calling a method directly from an object cast to an interface: \" + time2);\n        System.out.println(\"calling a method through reflection: \" + time3);\n    }\n</code></pre>\n"},{"tags":["linux","performance","libevent","c10k"],"answer_count":7,"favorite_count":9,"up_vote_count":31,"down_vote_count":0,"view_count":2388,"score":31,"question_id":3129608,"title":"Is there any modern review of solutions to the 10000 client/sec problem","body":"<p>(Commonly called the C10K problem)</p>\n\n<p>Is there a more contemporary review of solutions to the <a href=\"http://www.kegel.com/c10k.html\" rel=\"nofollow\">c10k</a> problem (Last updated: 2 Sept 2006), specifically focused on Linux (epoll, signalfd, eventfd, timerfd..) and libraries like libev or libevent?</p>\n\n<p>Something that discusses all the solved and still unsolved issues on a modern Linux server?</p>\n"},{"tags":["asp.net","performance","security","https","asp.net-mvc-4"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":45,"score":0,"question_id":13040442,"title":"implications of having a site in full https","body":"<p>I am currently developing an MVC4 web application for eCommerce. The site will contain a login and users can visit the site, input their details and submit orders etc. This is a traditional eCommerce site.</p>\n\n<p>To boost the security of the site, I am looking to set up the entire site in https. As the user will be supplying their log in credentials and storing personal information in cookies, I would like the site to be fully secured. </p>\n\n<p>I have concerns though, these being if I set up the site in https, will it detriment performance? Will it impact negatively on search engine optimization? Are there any other implications of having an entire site in https?</p>\n\n<p>I use output caching to cache the content of my views - with https will these still get cached?</p>\n\n<p>I have been reviewing security guidelines and documentation, such as <a href=\"http://asafaweb.com/OWASP%20Top%2010%20for%20.NET%20developers.pdf\" rel=\"nofollow\">this</a> from OWASP and they recommend this. Also, I see that sites such as twitter are fully https.</p>\n"},{"tags":["android","performance","quicksearch"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":205,"score":1,"question_id":6499347,"title":"Populating Quick Search Box via a web service","body":"<p>I am thinking of supporting Quick Search Box in my Android app.  I would take what the user has typed and run a query on a remote server via web services.  Is this pattern efficient enough or is it expected that any data I am querying should be displayed from locally accessible data?</p>\n"},{"tags":["performance","uiimageview","uicollectionview"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":109,"score":0,"question_id":12994964,"title":"Making UICollectionView With Images From URLs More Efficient","body":"<p>I have a UICollection View with cells that contain images from the web. I use the EGOImageView class to load the images in the background, however with really large images there is about a half a second freeze on the iPhone and on the iPad it is very noticeable. Is there anything I can do to make it more efficient? Is there a way to download a compressed version of large images?</p>\n"},{"tags":["sql","performance","oracle","plsql","data-warehouse"],"answer_count":1,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":109,"score":5,"question_id":12991346,"title":"Refresh strategy for materialized views in a data warehouse","body":"<p>I have a system that has a materialized view that contains roughly 1 billion items, on a consistent two hour basis I need to update about 200 million (20% of the records).  My question is what should the refresh strategy on my materialized view be?  As of right now it is refresh with an interval.  I am curious as to the performance impacts between refreshing on an interval vice refresh never and rename/replace the old materialized view with the new one.  The underlying issue is the indices that are used by Oracle which creates a massive amount of redo.  Any suggestions are appreciated.</p>\n\n<p><strong>UPDATE</strong><br>\nSince some people seem to think this is off topic my current view point is to do the following:  </p>\n\n<p>Create an Oracle Schedule Chain that invokes a series of PL/SQL (programming language I promise) functions to refresh materialized view in a pseudo-parallel fashion.    However, being as though I fell into the position of a DBA of sorts, I am looking to solve a data problem with an algorithm and/or some code.</p>\n"},{"tags":["performance","apache","caching","memcached","centos"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":35,"score":0,"question_id":13039691,"title":"Google PageSpeed - Setting an expiry date","body":"<p>I'm using a CentOS 5 32bit and I've just scanned my site on Google for page speed and it gave me the following:</p>\n\n<p>\"Setting an expiry date or a maximum age in the HTTP headers for static resources instructs the browser to load previously downloaded resources from local disk rather than over the network.\"</p>\n\n<p>Can someone please let me know how can I enable this within my Apache server?</p>\n"},{"tags":["asp.net","performance","viewstate"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":1692,"score":3,"question_id":600919,"title":"Running ASP.NET without viewstate turned on","body":"<p>We're about to start rebuilding one of our ASP.NET projects and I would like to try developing it without viestate turned on (disabled in web.config).</p>\n\n<p>I know about the upsides and downsides of viewstate and generally speaking what it keeps track of in comparison to control state, however I would like to know:</p>\n\n<ol>\n<li><p>What are the principle development process differences? Ie how differently do you structure your Page_Load etc?</p></li>\n<li><p>Is there any functionality in the standard ASP.NET controls that really will just not work without viewstate turned on?</p></li>\n</ol>\n\n<p>Also, are there any detailed articles on the workflow differences between working with and without VS?</p>\n"},{"tags":["python","linux","performance","osx","benchmarking"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":87,"score":0,"question_id":13038745,"title":"Speed of Python on Linux vs Mac","body":"<p>Out of curiosity: If I run a program on MacOS 10.8 (Python 2.7.2) it takes 21 seconds to finish. The same code running under a virtual Linux machine on the same computer takes 13 seconds (Python 2.7.3).</p>\n\n<p>Anyone knows why? I'm not asking about this specific code - why does it happen at all? I would expect the speed to be roughly the same as it's the same computer or a bit slower on Linux as it runs virtualized...</p>\n\n<p>Edit: the code calculates some statistics over data in RAM. Think of calculating the mean of some integers or similar.</p>\n\n<p>Edit 2:</p>\n\n<pre><code>from time import time\nt = time()\nx = 0\nl = list(range(1000000))\nr = list(range(1000000))\nfor i in range(100):\n    l = map(lambda a,b: a+b, l, r)\n    x = x+sum(l)\nprint time()-t\n</code></pre>\n\n<p>MacOS: 22s, Linux VM: 15s..?</p>\n"},{"tags":["c","performance","optimization","data-structures","computational-geometry"],"answer_count":5,"favorite_count":1,"up_vote_count":8,"down_vote_count":0,"view_count":1097,"score":8,"question_id":78045,"title":"Spatial Data Structures in C","body":"<p>I do work in theoretical chemistry on a high performance cluster, often involving molecular dynamics simulations.  One of the problems my work addresses involves a static field of N-dimensional (typically N = 2-5) hyper-spheres, that a test particle may collide with.  I'm looking to optimize (read: overhaul) the the data structure I use for representing the field of spheres so I can do rapid collision detection.  Currently I use a dead simple array of pointers to an N-membered struct (doubles for each coordinate of the center) and a nearest-neighbor list.  I've heard of oct- and quad- trees but haven't found a clear explanation of how they work, how to efficiently implement one, or how to then do fast collision detection with one.  Given the size of my simulations, memory is (almost) no object, but cycles are.</p>\n"},{"tags":["performance","sql-server-2008-r2","recoverymodel"],"answer_count":0,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":27,"score":0,"question_id":13038686,"title":"Performance slow down in SQL Server 2008 R2 after setting reovery to simple","body":"<p>I have a SQL Server 2008 R2 database which applications write to it intensively (I guess not less than 20 write operations per second).</p>\n\n<p>It was 3 months ago when I changed my recovery model to <code>Simple</code>. But after that the database becomes slower and slower. </p>\n\n<p>Now I set the recovery model to <code>Full</code>. But can I get the performance gain right now? Or do I have to wait till the Log file grows once again to reserve enough space to work in? </p>\n\n<p>I am not even sure why setting my recovery mode to  <code>Simple</code> reduces my performance (I am thankful if someone enlighten me on this too). </p>\n\n<p>Thanks a lot</p>\n"},{"tags":["php","performance","file-rename"],"answer_count":1,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":300,"score":7,"question_id":12944728,"title":"Renaming a 900kb pdf file takes long time","body":"<p>I am trying to <code>rename()</code> a 900 KiB PDF file in PHP. It is taking a long time to rename it for some reason. I thought it should be instant.</p>\n\n<p>This is on a CentOS server. While the file is being renamed I can get properties and it seems like <code>rename()</code> is copying and replacing the old file with new renamed file.</p>\n\n<p>The old name and new name paths are in the same directory.</p>\n\n<p>Has anyone stumbled upon this issue before?</p>\n\n<p><hr /></p>\n\n<h2>Code:</h2>\n\n<pre><code>    //If exists change name and then return path\n    $pieces = explode(\"@\", $filename);\n    $newName = $pieces[0].' '.$pieces[2];\n\n    rename($uidPath.$filename, $uidPath.$newName);\n\n    if (preg_match('/pdf/', $pieces[2]))\n    {\n        $result['status'] = '1';\n        $result['path'] = 'path to file';\n    } \n    else \n    {\n        $result['status'] = '1';\n        $result['path'] = 'path to file';\n    }\n</code></pre>\n"},{"tags":["performance","algorithm","r","index","sets"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":53,"score":2,"question_id":13037560,"title":"What is an efficient way to convert sets to a column index in R?","body":"<p><strong>Overview</strong></p>\n\n<p>Give a large (nrows > 5,000,000+) data frame, <strong>A</strong>, with string row names and a list of disjoint sets (n = 20,000+), <strong>B</strong>, where each set consists of row names from <strong>A</strong>, what is the best way to create a vector representing the sets in <strong>B</strong> via a unique value?</p>\n\n<p><strong>Illustration</strong></p>\n\n<p>Below is an example illustrating this problem:</p>\n\n<pre><code># Input\nA &lt;- data.frame(d = rep(\"A\", 5e6), row.names = as.character(sample(1:5e6)))\nB &lt;- list(c(\"4655297\", \"3177816\", \"3328423\"), c(\"2911946\", \"2829484\"), ...) # Size 20,000+\n</code></pre>\n\n<p>The desired result would be:</p>\n\n<pre><code># An index of NA represents that the row is not part of any set in B.\n&gt; A[,\"index\", drop = F]\n        d index\n4655297 A     1\n3328423 A     1\n2911946 A     2\n2829484 A     2\n3871770 A    NA\n2702914 A    NA\n2581677 A    NA\n4106410 A    NA\n3755846 A    NA\n3177816 A     1\n</code></pre>\n\n<p><strong>Naive Attempt</strong></p>\n\n<p>Something like this can be achieved using the following method.</p>\n\n<pre><code>n &lt;- 0\nA$index &lt;- NA\nlapply(B, function(x){\n  n &lt;&lt;- n + 1\n  A[x, \"index\"] &lt;&lt;- n\n})\n</code></pre>\n\n<p><strong>Problem</strong></p>\n\n<p>However this is unreasonably slow (several hours) due to indexing A multiple times and is not very R-esque or elegant.</p>\n\n<p>How can the desired result be generated in a quick and efficient manner?</p>\n"},{"tags":["c#","performance","linq"],"answer_count":7,"favorite_count":8,"up_vote_count":68,"down_vote_count":0,"view_count":1254,"score":68,"question_id":7499384,"title":"Does the order of LINQ functions matter?","body":"<p>Basically, as the question states... does the order of LINQ functions matter in terms of <b>performance</b>? Obviously the results would have to be identical still...</p>\n\n<p>Example:</p>\n\n<pre><code>myCollection.OrderBy(item =&gt; item.CreatedDate).Where(item =&gt; item.Code &gt; 3);\nmyCollection.Where(item =&gt; item.Code &gt; 3).OrderBy(item =&gt; item.CreatedDate);\n</code></pre>\n\n<p>Both return me the same results, but are in a different LINQ order. I realize that reordering some items will result in different results, and I'm not concerned about those. What my main concern is in knowing if, in getting the same results, ordering can impact performance. And, not just on the 2 LINQ calls I made (OrderBy, Where), but on any LINQ calls.</p>\n"},{"tags":["python","performance","recursion"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":89,"score":3,"question_id":13036272,"title":"python - compare two directories recursively and flag equivalent structure","body":"<p>I have read many questions in stackoverflow, related to python compare directory. However, my current question is bit different.<br>\nI have two directories, which contains two different version release package contents. Now I want to compare to ensure the contents are same. <strong>However</strong> few files have version name embedded into them. Now which is the best possible way to compare them and conclude (except the version difference, all files match).  </p>\n\n<hr>\n\n<p>For example:<br>\nVersion <strong>V1R1C1</strong> contains directory structure as below  </p>\n\n<pre><code>pmt&gt; find . -name \"*\"\n.\n./c1\n./c1/c2\n./c1/c1_V1R1C1.cfg\n./a1\n./a1/a1_V1R1C1.cfg\n./a1/a2\n./a1/a2/a1a2_V1R1C1.cfg\n./b1/a_best_file.txt\n./b1/b2/a_test_file.txt\n./b1/b2/b1b2_V1R1C1.cfg\n./a_V1R1C1.cfg\n</code></pre>\n\n<p>Version <strong>V2R3C1</strong> may contain below structure    </p>\n\n<pre><code>pmt&gt; find . -name \"*\"\n.\n./c1\n./c1/c2\n./c1/c1_V2R3C1.cfg\n./a1\n./a1/a1_V2R3C1.cfg\n./a1/a2\n./a1/a2/a1a2_V2R3C1.cfg\n./b1/a_best_file.txt\n./b1/b2/a_test_file.txt\n./b1/b2/b1b2_V2R3C1.cfg\n./a_V2R3C1.cfg\n</code></pre>\n\n<hr>\n\n<p>In the above case, the program must flag it as <strong>equivalent</strong> structure.  </p>\n\n<p>I can think of few solutions - like for example, read both the directory structure recursively into cache (dict), rip the version information and compare etc. But looks like not a completely effective mechanism because of two reason 1. It does not utilize the inbuilt directory compare 2.The multiple read/rip/compare is bound to cost (especially with huge directory tree structure).  </p>\n\n<p>I am looking for ideas, which are simple and efficient than the one above. </p>\n\n<hr>\n\n<p>PS :<br>\n1. In case of any difference (except the version unlike the above example), I would like to use the left/right etc to get diffed list.<br>\n2. We can assume before hand which is the version name in both directories (like V1R1C1 in first case and V2R3C1 in second case).</p>\n"},{"tags":["performance","orient-db"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":44,"score":1,"question_id":13034247,"title":"OrientDB and mmap (performance issue)","body":"<p>If I iterate over a cluster, using the following code:</p>\n\n<pre><code>for (ODocument document : m_database.browseCluster(clusterName)) {\n    // ...\n}\n</code></pre>\n\n<p>I observe very poor performance, (ca. 10 records per seconds are fetched), but only if sufficient memory is allocated to Java process. If I restrict memory to 64mb - everything runs very fast.</p>\n\n<p>As I can see from profiler, OFileMMap.map is called VERY often in the case when performance is poor. And channel.map(....) consumes most of the time.</p>\n\n<p>If I disable mmap completely by setting file.mmap.strategy=4 - everything is getting fast, but not as fast, as with mmap and restricted memory allocation.</p>\n\n<p>Any ideas? </p>\n"},{"tags":["c++","python","performance","simulation","simpy"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":53,"score":0,"question_id":13035296,"title":"Python (SimPy - Simulation) speed performance","body":"<p>I opt to implement a very simple open-source discrete event simulation package to model enterprise (manufacturing/logistics etc) procedures. As I said I want it to be simple, but expandable. I spent some days in learning about SimPy and I really like the way things are constructed with it (a great tutorial here-> <a href=\"http://heather.cs.ucdavis.edu/~matloff/simcourse.html\" rel=\"nofollow\">http://heather.cs.ucdavis.edu/~matloff/simcourse.html</a>). </p>\n\n<p>Nevertheless my concerns have to do with speed: since python is generally much slower than c and discrete event simulations are notorious for requiring much time and resources, should I expect that if I manage to model big systems with SimPy the performance of my application will be crappy? Are there means of making python work at better speed (eg Numpy and Scipy) and will that be equivalent to the effectiveness of c?</p>\n\n<p>Also is there some other open-source framework (maybe based in c or c++?) that you believe will be better for my needs? (I have done a quick overview by searching in the web, but still I am confused). Of course I can just use c or c++, but that seems too complicated. </p>\n\n<p>Thank you for any insight!</p>\n"},{"tags":["python","multithreading","performance","concurrency","parallel-processing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":69,"score":0,"question_id":13031886,"title":"Python concurrency performance","body":"<p>I'm facing concurrency in Python for the first time with the goal in mind to optimize my script. Basically I have a script which invokes third party jar tools using os.system against some files. My first \"procedural\" version took about 135 seconds to complete, after using threads (<strong>threading.Thread</strong> and <strong>threading.Queue</strong>) 127 and now switching to multiprocess (<strong>multiprocessing.Process</strong> and <strong>multiprocessing.JoinableQueue</strong>) 113 seconds... but it's still a lot of time... could you give me some feedback and/or point me to an article that may solve my problem?</p>\n\n<p>(I'm using Python 2.7.1 and I would like to avoid 3d party modules)</p>\n"},{"tags":["html","css","performance","web"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":52,"score":-1,"question_id":13034407,"title":"Custom fonts on website, why not?","body":"<p>I would like to ask, whether it is advisable, using custom fonts on a website. \nSure, it maybe a nicer optical solution e.g. using SeroWeb-Light instead of Arial. But what are the reasons against it?\nMy idea: it needs more request and more traffic. Websites with performance problems may have more problems, is this right? I am interested in the opinion of professionals, what do you thin? Please help me to find arguments!</p>\n\n<p>Many Thanks!</p>\n"},{"tags":["performance","jsf","configuration","myfaces","mojarra"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":27,"score":1,"question_id":13033070,"title":"What is the default for FACELETS_REFRESH_PERIOD?","body":"<p>In a JSF application, the parameter javax.faces.FACELETS_REFRESH_PERIOD can be used to enable/disable automatic reloading of XHTML files.</p>\n\n<p>I am currently researching the right configuration for production deployments, and accidentally found out that we currently run with FACELETS_REFRESH_PERIOD=1 even in production, which is obviously not a good idea.</p>\n\n<p>This lead to the question: What is the default value for this parameter?</p>\n\n<p>Ideally, I'd like to just omit FACELETS_REFRESH_PERIOD from our production config for simplicity's sake, and hoped it would use a \"safe\" default value of -1. However, this does not seem to be the case, because without the parameter, refreshing seems to be enabled (with both Mojarra and MyFaces).</p>\n\n<p>I checked the JSF spec, and while it describes the parameter, it does not give a default. Is this a deliberate omission in the spec?</p>\n"},{"tags":["mysql","performance","left-join"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":42,"score":1,"question_id":13034163,"title":"MySQL join slow even after indexing","body":"<p>I have two tables members and dep whose description are as follows:</p>\n\n<pre><code>TABLE members:\n+-----------------+------------+------+-----+---------+-------+\n| Field           | Type       | Null | Key | Default | Extra |\n+-----------------+------------+------+-----+---------+-------+\n| MemberID_M      | varchar(8) | YES  | MUL | NULL    |       |\n| Age             | varchar(5) | YES  |     | NULL    |       |\n| Sex             | varchar(1) | YES  |     | NULL    |       |\n| SomeInfo        | int(11)    | YES  |     | NULL    |       |\n+-----------------+------------+------+-----+---------+-------+\nTABLE dep:\n+-----------------+------------+------+-----+---------+-------+\n| Field           | Type       | Null | Key | Default | Extra |\n+-----------------+------------+------+-----+---------+-------+\n| MemberID_t      | int(11)    | YES  | MUL | NULL    |       |\n| YEAR            | varchar(2) | NO   |     |         |       |\n| Days            | tinyint(4) | YES  |     | NULL    |       |\n| train           | bigint(20) | NO   |     | 0       |       |\n+-----------------+------------+------+-----+---------+-------+\n</code></pre>\n\n<p>I want to perform the following query:</p>\n\n<pre><code>CREATE TABLE table2 \nSELECT a.*,b.* \nFROM dep AS a LEFT OUTER JOIN members AS b \n          ON  a.MemberID_t = b.Memberid_M;\n</code></pre>\n\n<p>Intially, the ids in both the tables were not indexed and the query did not return for hours. Now, even after indexing it is taking a lot of time.</p>\n\n<pre><code>EXPLAIN for the SELECT part of the query is:\n+----+-------------+-------+------+---------------+------+---------+------+--------+-------+\n| id | select_type | table | type | possible_keys | key  | key_len | ref  | rows   | Extra |\n+----+-------------+-------+------+---------------+------+---------+------+--------+-------+\n|  1 | SIMPLE      | a     | ALL  | NULL          | NULL | NULL    | NULL | 436689 |       |\n|  1 | SIMPLE      | b     | ALL  | memid2        | NULL | NULL    | NULL | 226127 |       |\n+----+-------------+-------+------+---------------+------+---------+------+--------+-------+\n</code></pre>\n"},{"tags":["java","performance","playframework","playframework-1.x"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":263,"score":2,"question_id":11288712,"title":"CPU load with play framework","body":"<p>Since a few days, on a system which has been in development for about a year, I have a constant CPU load from the play! server. I have two servers, one active and one as a hot spare. In the past, the hot-spre server showed no load, or a neglectable load. But now it consumes a constant 50-110% CPU (using top on Linux).</p>\n\n<p>Is there an easy way to find out what the cause it? I don't see this behavior on my MacBook when debugging (usually 0.1-1%).This is something that only happened in the past few days as far as I am aware.</p>\n\n<p>This is a status print of the hot-spare. As can be seen no controllers are queried apart from the scheduled tasks (which do not perform on this server due to a flag, but are launched):</p>\n\n<pre><code>~        _            _ \n~  _ __ | | __ _ _  _| |\n~ | '_ \\| |/ _' | || |_|\n~ |  __/|_|\\____|\\__ (_)\n~ |_|            |__/   \n~\n~ play! 1.2.4, http://www.playframework.org\n~ framework ID is prod-frontend\n~\n~ Status from http://localhost:xxxx/@status,\n~\nJava:\n~~~~~\nVersion: 1.6.0_26\nHome: /usr/lib/jvm/java-6-sun-1.6.0.26/jre\nMax memory: 64880640\nFree memory: 11297896\nTotal memory: 29515776\nAvailable processors: 2\n\nPlay framework:\n~~~~~~~~~~~~~~~\nVersion: 1.2.4\nPath: /opt/play\nID: prod-frontend\nMode: PROD\nTmp dir: /xxx/tmp\n\nApplication:\n~~~~~~~~~~~~\nPath: /xxx/server\nName: iDoms Server\nStarted at: 07/01/2012 12:05\n\nLoaded modules:\n~~~~~~~~~~~~~~\nsecure at /opt/play/modules/secure\npaginate at /xxx/server/modules/paginate-0.14\n\nLoaded plugins:\n~~~~~~~~~~~~~~\n0:play.CorePlugin [enabled]\n100:play.data.parsing.TempFilePlugin [enabled]\n200:play.data.validation.ValidationPlugin [enabled]\n300:play.db.DBPlugin [enabled]\n400:play.db.jpa.JPAPlugin [enabled]\n450:play.db.Evolutions [enabled]\n500:play.i18n.MessagesPlugin [enabled]\n600:play.libs.WS [enabled]\n700:play.jobs.JobsPlugin [enabled]\n100000:play.plugins.ConfigurablePluginDisablingPlugin [enabled]\n\nThreads:\n~~~~~~~~\nThread[Reference Handler,10,system] WAITING\nThread[Finalizer,8,system] WAITING\nThread[Signal Dispatcher,9,system] RUNNABLE\nThread[net.sf.ehcache.CacheManager@449278d5,5,main] WAITING\nThread[Timer-0,5,main] TIMED_WAITING\nThread[com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread-#0,5,main] TIMED_WAITING\nThread[com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread-#1,5,main] TIMED_WAITING\nThread[com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread-#2,5,main] TIMED_WAITING\nThread[jobs-thread-1,5,main] TIMED_WAITING\nThread[jobs-thread-2,5,main] TIMED_WAITING\nThread[jobs-thread-3,5,main] TIMED_WAITING\nThread[New I/O server boss #1 ([id: 0x7065ec20, /0:0:0:0:0:0:0:0:9001]),5,main] RUNNABLE\nThread[DestroyJavaVM,5,main] RUNNABLE\nThread[New I/O server worker #1-3,5,main] RUNNABLE\n\nRequests execution pool:\n~~~~~~~~~~~~~~~~~~~~~~~~\nPool size: 0\nActive count: 0\nScheduled task count: 0\nQueue size: 0\n\nMonitors:\n~~~~~~~~\ncontrollers.ReaderJob.doJob(), ms.         -&gt;      114 hits;      4.1 avg;      0.0 min;    463.0 max;\ncontrollers.MediaCoderProcess.doJob(), ms. -&gt;     4572 hits;      0.1 avg;      0.0 min;    157.0 max;\ncontrollers.Bootstrap.doJob(), ms.         -&gt;        1 hits;      0.0 avg;      0.0 min;      0.0 max;\n\nDatasource:\n~~~~~~~~~~~\nJdbc url: jdbc:mysql://xxxx\nJdbc driver: com.mysql.jdbc.Driver\nJdbc user: xxxx\nJdbc password: xxxx\nMin pool size: 1\nMax pool size: 30\nInitial pool size: 3\nCheckout timeout: 5000\n\nJobs execution pool:\n~~~~~~~~~~~~~~~~~~~\nPool size: 3\nActive count: 0\nScheduled task count: 4689\nQueue size: 3\n\nScheduled jobs (4):\n~~~~~~~~~~~~~~~~~~~~~~~~~~\ncontrollers.APNSFeedbackJob run every 24h. (has never run)\ncontrollers.Bootstrap run at application start. (last run at 07/01/2012 12:05:32)\ncontrollers.MediaCoderProcess run every 15s. (last run at 07/02/2012 07:10:46)\ncontrollers.ReaderJob run every 600s. (last run at 07/02/2012 07:05:36)\n\nWaiting jobs:\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\ncontrollers.MediaCoderProcess will run in 2 seconds\ncontrollers.APNSFeedbackJob will run in 17672 seconds\ncontrollers.ReaderJob will run in 276 seconds\n</code></pre>\n"},{"tags":["windows","performance"],"answer_count":5,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":304,"score":3,"question_id":3202337,"title":"How to simulate a reboot in Win7 for testing application cold-start time","body":"<p>In order to measure application' cold-start time, I have to reboot my machine every time, which is really time-consuming. I understand it is mission impossible to simulate a real reboot, but what I want is something rough, ex, put out cache in standby list as many as possible so the warm start won't be so warm anymore.</p>\n\n<p>Any ideas on this?</p>\n\n<p>Thanks</p>\n"},{"tags":["javascript","performance","apache","networking"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":600,"score":1,"question_id":10938682,"title":"How to reduce server \"Wait\" time?","body":"<p>I am trying to optimize my site's speed and I'm using the great tool at <a href=\"http://tools.pingdom.com/fpt/\" rel=\"nofollow\">pingdom.com</a>.  Right now, over 50% of the time it takes to load the page is \"Wait\" time as shown in the screenshot below.  What can I do to reduce this?  Also, how typical is this figure?  are there benchmarks on this?  Thanks!</p>\n\n<p><img src=\"http://i.stack.imgur.com/RgsFY.png\" alt=\"high server wait time\"></p>\n\n<p><strong>EDIT:</strong>\nOk.. let me clarify a few things.  There are no server side scripts or database calls going on. Just HTML, CSS, JS, and images.  I have already done some things like push js to the end of the body tag to get parallel downloads.  I am aware that the main.html and templates.html are adding to the overall wait time by being done synchronously after js.js downloads, that's not the problem.  I am just surprised at how much \"wait\" time there is for each request.  Does server distance affect this?  what about being on a shared server, does that affect the wait time?  Is there any low-hanging fruit to remedy those issues?</p>\n\n<p><img src=\"http://i.stack.imgur.com/rsBlu.png\" alt=\"enter image description here\"></p>\n"},{"tags":["sql","sql-server","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":6,"down_vote_count":0,"view_count":126,"score":6,"question_id":12976486,"title":"SQL Server uses scan instead of seek when using a window function and predicate contains a variable","body":"<p>Take a look at this fiddle: <a href=\"http://sqlfiddle.com/#!6/18324/2\" rel=\"nofollow\">http://sqlfiddle.com/#!6/18324/2</a></p>\n\n<p>Expand the very first execution plan, for the queries against view <code>B</code>.<br>\nNotice that the first query executes using index seek, while the second one - using index scan. In my real setup, with thousands of rows, this produces a performance hit that is quite considerable.  </p>\n\n<p>WTF???</p>\n\n<p>The queries are equivalent, aren't they? Why does a literal produce seek and a variable - scan?<br>\nBut more importantly: <strong>how can I work around this</strong>?  </p>\n\n<p><a href=\"http://stackoverflow.com/questions/4459425/sql-server-query-fast-with-literal-but-slow-with-variable\">This post</a> comes closest to the problem, and the solution that works from there is using <code>option(recompile)</code> (thank you, Martin Smith). However, that does not work for me, because my queries are being generated by my ORM library (which is Entity Framework) and I cannot amend them manually.<br>\nRather what I'm looking for is a way to reformulate the <code>B</code> view so that the problem would not occur.</p>\n\n<p>While fiddling with this problem, I have noticed that it is always the \"Segment\" block in the execution plan that loses the predicate. To verify this, I reformulated the query in terms of a subquery with <code>min</code> function (see view <code>D</code>). And voila! - both queries against the <code>D</code> view produce identical plans.</p>\n\n<p>The bad news, however, is that I cannot use this <code>min</code>-powered trick, because in my real setup, the column <code>Y</code> is actually <strong>several columns</strong>, so that I can order by them, but I cannot take a <code>min()</code> of them.<br>\nSo the second question would be: <strong>can anyone come up with a trick that is similar to min-powered subquery, but works for several columns?</strong></p>\n\n<p><strong>NOTE 1</strong>: this is definitely not related to the tipping point, because there are just 2 records in the table.<br>\n<strong>NOTE 2</strong>: it also doesn't have to do with the presence of a view. See an example with view <code>C</code>: the server is happily using seek in that case.</p>\n"},{"tags":["mysql","performance","database-performance"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":79,"score":3,"question_id":13029946,"title":"Mysql self inner join query?","body":"<p>When i run the below query in mysql it took 78 sec to display record. Is there other way to write this query. Here is my \nmysql query -> \"</p>\n\n<pre><code>select distinct nuqta1.post_id from wp_postmeta as nuqta1 \ninner join wp_postmeta as nuqta2 on (nuqta1.post_id = nuqta2.post_id) \ninner join wp_postmeta as nuqta4 on (nuqta1.post_id = nuqta4.post_id) \ninner join wp_postmeta as nuqta5 on (nuqta1.post_id = nuqta5.post_id) \ninner join wp_postmeta as nuqta6 on (nuqta1.post_id = nuqta6.post_id) \ninner join wp_postmeta as nuqta7 on (nuqta1.post_id = nuqta7.post_id) \ninner join wp_postmeta as nuqta8 on (nuqta1.post_id = nuqta8.post_id) \ninner join wp_postmeta as nuqta9 on (nuqta1.post_id = nuqta9.post_id) \ninner join wp_postmeta as nuqta10 on (nuqta1.post_id = nuqta10.post_id) \ninner join wp_postmeta as nuqta11 on (nuqta1.post_id = nuqta11.post_id) \ninner join wp_postmeta as nuqta12 on (nuqta1.post_id = nuqta12.post_id) \nwhere (nuqta2.meta_key = 'checkin' and nuqta2.meta_value LIKE '%10/31/2012%') \nand (nuqta4.meta_key = 'guests' and nuqta4.meta_value ='1') \nand (nuqta5.meta_key = 'roomtype' and nuqta5.meta_value LIKE '%Entire home/apt%') \nand (nuqta6.meta_key = 'price' and cast(nuqta6.meta_value as signed) BETWEEN '10' and '99999') \nand (nuqta7.meta_key = 'amenities' and nuqta7.meta_value LIKE '%Wireless Internet%') \nand (nuqta8.meta_key = 'amenities' and nuqta8.meta_value LIKE '%TV%') \nand (nuqta9.meta_key = 'amenities' and nuqta9.meta_value LIKE '%Kitchen%') \nand (nuqta10.meta_key = 'amenities' and nuqta10.meta_value LIKE '%Wireless Internet%') \nand (nuqta11.meta_key = 'amenities' and nuqta11.meta_value LIKE '%TV%') \nand (nuqta12.meta_key = 'amenities' and nuqta12.meta_value LIKE '%Kitchen%') \nand 1=1 order by nuqta1.post_id asc\n</code></pre>\n\n<p>\".\nAnd and i am using wordpress table wp_postmeta to run this query</p>\n"},{"tags":["java","performance","graphics2d"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":50,"score":1,"question_id":13030891,"title":"Graphics2D Drawing Performance","body":"<p>I am trying some things out with manually drawing \"things\" with a Java <code>Graphics2D</code> object within a Swing component and as I reach about >2000 squares that I order the object to draw it gets really slow.</p>\n\n<p>I have no clue whether or not this is \"common\". Are 2000 objects to render really \"a lot\"? Is the <code>Graphics2D</code> object just not very performant? Should I just stop where I am now and rather switch to JOGL before I try out more complex stuff and it is too late?</p>\n"},{"tags":["java","performance","visualvm"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":77,"score":1,"question_id":13030317,"title":"Debugging a slow Java method","body":"<p>VisualVM is showing me that a particular method is taking a long time to execute.</p>\n\n<p>Are there any widely used strategies for looking at the performance (in regards to time) of a Java method?</p>\n\n<p>My gut feeling is that the sluggish response time will come from a method that is somewhere down the call hierarchy from the one VisualVM is reporting but I think getting some hard numbers is better than fishing around in the code based on an assumption when it comes to performance.</p>\n"},{"tags":["sql","performance","between"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":248,"score":1,"question_id":10834194,"title":"SQL query multiple ranges without using multiple OR clauses (nesting LIKE, BETWEEN)","body":"<p>I commonly have to query an extremely large table for multiple variables with many conditions per variable.  Often times, a variable will need to be queried for multiple ranges.  For example, I could need all records of VAR_1 where VAR_1 is between 200-300, 350-400, 450-500.</p>\n\n<p>Normally I would write this as follows, but have been told that using <code>IN()</code> instead of the multiple ORs would be much more efficient.\n</p>\n\n<pre><code>SELECT * FROM table\nWHERE VAR_1 BETWEEN '200' AND '300' OR\n      VAR_1 BETWEEN '350' AND '400' OR\n      VAR_1 BETWEEN '450' AND '500'\n</code></pre>\n\n<p>Is there any way to condense this information and get rid of the <code>OR</code>s by nesting <code>LIKE</code> or <code>BETWEEN</code> clauses within an <code>IN()</code>? \nSomething along the lines of:\n</p>\n\n<p><code>WHERE VAR_1 IN (BETWEEN '200' AND '300', BETWEEN '350' AND '400', BETWEEN '450' AND '500')</code></p>\n\n<p>or</p>\n\n<p><code>WHERE VAR_1 IN ('[200-300]','[350-400]','[450-500]')</code></p>\n\n<p>I have tried things like these, but the syntax is clearly incorrect. Any ideas or directions you can point me in would be great, still very new to SQL.</p>\n"},{"tags":["css","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":35,"score":1,"question_id":13028891,"title":"how to bench a css pseudo-elements","body":"<p>Is there a way of measuring the performance (of displaying) for a web page with a lots of <code>:after</code> and <code>:before</code> in the CSS ?</p>\n\n<p>I think there is a small impact on rendering time of each element, and there is no problem on a modern computer but on a slow mobile.  </p>\n"},{"tags":["performance","node.js","loggly"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":47,"score":0,"question_id":13020546,"title":"What is the maximum throughput of Loggly?","body":"<p>How many requests per second from a client can Loggly handle? I am only able to get around 10–20 requests processed per second and I am wondering if this is normal.</p>\n"},{"tags":["c++","performance","loops","macros","overloading"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":136,"score":0,"question_id":13027550,"title":"C++ repeating codes with a loops","body":"<p>Im not sure if this is a stupid question, so shoot me if it is!</p>\n\n<p>I am having this \"dilemna\" which I encounter very often. I have say two overloaded functions in C++</p>\n\n<p>say we have this two overloads of <em>F</em> (just a pseudocode below)</p>\n\n<pre><code>void F(A a, .../*some other parameters*/)\n{ \n  //some code part\n  //loop Starts here\n  G1(a,.../* some other parameter*/)\n  //loop ends here\n  //some code part\n}\n\n\nvoid F(B b, .../*some other parameters*/)\n{\n  //some code part\n  //loop Starts here\n  G2(b,.../* some other parameter*/)\n  //loop ends here\n  //some code part\n}\n</code></pre>\n\n<p>where <em>A</em> and <em>B</em> are different types and <em>G1</em> and <em>G2</em> are different functions doing different things. The code part of the overloads except for <em>G1</em> and <em>G2</em> lines are the same and they are sometimes very long and extensive. Now the question is.. how can I write my code more efficiently. Naturally I want NOT to repeat the code (even if it's easy to do that, because its just a copy paste routine). A friend suggested macro... but that would look dirty. Is this simple, because if it is Im quite stupid to know right now. Would appreciate any suggestions/help.</p>\n\n<p><strong>Edit:</strong> Im sorry for those wanting a code example. The question was really meant to be abstract as I encounter different \"similar\" situation in which I ask myself how I am able to make the code shorter/cleaner. In most cases codes are long otherwise I wouldn't bother asking this in the first place. As KilianDS pointed out, it's also good to make sure that the function itself isn't very long. But sometimes this is just unavoidable. Many cases where I encounter this, the loop is even nested (i.e. several loops within each other) and the beginning of <em>F</em> we have the start of a loop and the end of <em>F</em> we end that loop.</p>\n\n<p>Jose </p>\n"},{"tags":["mysql","performance","database-performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":29,"score":0,"question_id":13027911,"title":"MySQL Query Execution Time Comparison","body":"<p>i am doing one select qry in my mysql database which contains \" order by field desc\".\nI get the resultset within 16 minutes.</p>\n\n<p>Now in the same qry i put  \"order by field desc limit  5\" to get the first 5 records..\nBut my qry is running for more than 20 minutes.</p>\n\n<p>How can i make the execution faster?\nAny Ideas?</p>\n\n<p>Thanks-\njithkaran</p>\n"},{"tags":["python","performance","matplotlib","scientific-computing"],"answer_count":4,"favorite_count":4,"up_vote_count":6,"down_vote_count":0,"view_count":1739,"score":6,"question_id":5854515,"title":"Large plot: ~20 million samples, gigabytes of data","body":"<p>I have got a problem (with my RAM) here: it's not able to hold the data I want to plot. I do have sufficient HD space. Is there any solution to avoid that \"shadowing\" of my data-set?</p>\n\n<p>Concretely I deal with Digital Signal Processing and I have to use a high sample-rate. My framework (GNU Radio) saves the values (to avoid using too much disk space) in binary. I unpack it. Afterwards I need to plot. I need the plot zoomable, and interactive. And that is an issue. </p>\n\n<p>Is there any optimization potential to this, or another software/programming language (like R or so) which can handle larger data-sets? Actually I want much more data in my plots. But I have no experience with other software. GNUplot fails, with a similar approach to the following. I don't know R (jet).</p>\n\n<pre><code>import matplotlib.pyplot as plt\nimport matplotlib.cbook as cbook\nimport struct\n\n\"\"\"\nplots a cfile\n\ncfile - IEEE single-precision (4-byte) floats, IQ pairs, binary\ntxt - index,in-phase,quadrature in plaintext\n\nnote: directly plotting with numpy results into shadowed functions\n\"\"\"\n\n# unpacking the cfile dataset\ndef unpack_set(input_filename, output_filename):\n    index = 0   # index of the samples\n    output_filename = open(output_filename, 'wb')\n\n    with open(input_filename, \"rb\") as f:\n\n        byte = f.read(4)    # read 1. column of the vector\n\n        while byte != \"\":\n        # stored Bit Values\n            floati = struct.unpack('f', byte)   # write value of 1. column to a variable\n            byte = f.read(4)            # read 2. column of the vector\n            floatq = struct.unpack('f', byte)   # write value of 2. column to a variable\n            byte = f.read(4)            # next row of the vector and read 1. column\n            # delimeter format for matplotlib \n        lines = [\"%d,\" % index, format(floati), \",\",  format(floatq), \"\\n\"]\n            output_filename.writelines(lines)\n            index = index + 1\n    output_filename.close\n    return output_filename.name\n\n# reformats output (precision configuration here)\ndef format(value):\n    return \"%.8f\" % value            \n\n# start\ndef main():\n\n    # specify path\n    unpacked_file = unpack_set(\"test01.cfile\", \"test01.txt\")\n    # pass file reference to matplotlib\n    fname = str(unpacked_file)\n    plt.plotfile(fname, cols=(0,1)) # index vs. in-phase\n\n    # optional\n    # plt.axes([0, 0.5, 0, 100000]) # for 100k samples\n    plt.grid(True)\n    plt.title(\"Signal-Diagram\")\n    plt.xlabel(\"Sample\")\n    plt.ylabel(\"In-Phase\")\n\n    plt.show();\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>\n\n<p>Something like plt.swap_on_disk() could cache the stuff on my SSD ;)</p>\n"},{"tags":["javascript","performance","browser","pagespeed"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":51,"score":1,"question_id":13015421,"title":"JavaScript browser parsing speed testing","body":"<p>I'm looking into the speed of JavaScript parsers in web browsers, importantly it needs to be easy to demonstrate. I came up with a simple test - the idea being that each script block is parsed and executed individually, so a large block of script could be timed:</p>\n\n<pre><code>&lt;script&gt;var start = new Date().getTime();&lt;/script&gt;\n\n&lt;script&gt;\n    /*! jQuery v1.8.2 jquery.com | jquery.org/license */\n    ...\n&lt;/script&gt;\n\n&lt;script&gt;alert ( new Date().getTime() - start );&lt;/script&gt;\n</code></pre>\n\n<p>Superficially this appears to work, removing the middle script block will result in a negligible time.</p>\n\n<p>However I'm not certain that my logic is not fundamentally flawed.</p>\n"},{"tags":["c++","c","performance","optimization","compiler-optimization"],"answer_count":6,"favorite_count":1,"up_vote_count":32,"down_vote_count":2,"view_count":1342,"score":30,"question_id":12919184,"title":"Why doesn't a compiler optimize floating-point *2 into an exponent increment?","body":"<p>I've often noticed gcc converting multiplications into shifts in the executable. Something similar might happen when multiplying an <code>int</code> and a <code>float</code>. For example, <code>2 * f</code>, might simply increment the exponent of <code>f</code> by 1, saving some cycles. Do the compilers, perhaps if one requests them to do so (e.g. via <code>-ffast-math</code>), in general, do it?</p>\n\n<p>Are compilers generally smart enough to do this, or do I need to do this myself using the <code>scalb*()</code> or <code>ldexp()/frexp()</code> function family?</p>\n"},{"tags":["java","performance","eclipse-plugin","eclipse-rcp","jface"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":61,"score":0,"question_id":12918603,"title":"Eclipse plugin performance degrades over time","body":"<p>I have 2 <code>views</code> A and B, each has a <code>treeViewer</code>. \nB has a <code>listener</code> to tree <code>selectionChanged</code> in <code>view</code> A.\nso each time I select  a <code>tree</code> item in A , the <code>selectionChanged</code> action is:</p>\n\n<p>1- <code>setInput</code> to <code>tree</code> B</p>\n\n<p>2- apply filter </p>\n\n<p>3- expand all elements</p>\n\n<p>The problem here is each time I click on a tree item in <code>View</code> A, the time it takes for <code>View</code> B to show up the tree items is increasing every time for the same selection in tree A.and items show up slower each time.</p>\n\n<p>which part of the <code>selectionChanged</code> action could result in this performance problem !?</p>\n\n<p>After some investigations, I found out that the method <code>TreeViewer.expandAll()</code> is the one causes that big delay each time. \nIs this an <code>Eclipse</code> issue or should I modify it ?</p>\n"},{"tags":["java","performance","multiplication"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":46,"score":-1,"question_id":13023017,"title":"Need a more optimized karatsuba code java","body":"<p>Here is the code that I have submitted for fast multiplication problem using Karatsuba algorithm but still it gives TLE.The code deploys recursive Karatsuba.Kindly suggest optimizations.The karatsuba function takes in two BigInteger variables as parameter and returns a BigInteger as a result for multiplication.If the bitlength is less than 2000 ,it returns the result of simple multtiplication.Or else it goes for recursion</p>\n\n<pre><code>import java.math.BigInteger;\nimport java.util.Scanner;\n\n\npublic class Main {\n\n     public static BigInteger karatsuba(BigInteger x, BigInteger y) {\n\n            // cutoff to brute force\n            int N = Math.max(BitLength(x), BitLength(y));\n         //   System.out.println(N);\n\n        if (N &lt;= 2000) return x.multiply(y);                                        \n\n        // number of bits divided by 2, rounded up\n        N = (N / 2) + (N % 2);\n\n        // x = a + 2^N b,   y = c + 2^N d\n        BigInteger b = x.shiftRight(N);\n      //  System.out.println(b);\n\n        BigInteger a = x.subtract(b.shiftLeft(N));\n      //  System.out.println(a);\n        BigInteger d = y.shiftRight(N);\n      //  System.out.println(d);\n        BigInteger c = y.subtract(d.shiftLeft(N));\n     //   System.out.println(c);\n\n        // compute sub-expressions\n        BigInteger ac    = karatsuba(a, c);\n        BigInteger bd    = karatsuba(b, d);\n        BigInteger abcd  = karatsuba(a.add(b), c.add(d));\n\n            return ac.add(abcd.subtract(ac).subtract(bd).shiftLeft(N)).add(bd.shiftLeft(2*N));\n    }\n\n public static int BitLength(BigInteger n)\n {\n     byte[] Data = n.toByteArray();\n     int result = (Data.length - 1) * 8;\n     byte Msb = Data[Data.length - 1];\n     while (Msb != 0) {\n         result += 1;\n         Msb &gt;&gt;= 1;\n     }\n     return result;\n }\n\n\npublic static void main(String[] args) {\n    BigInteger A,B,C;\n    int  t;\n    Scanner sc = new Scanner(System.in);\n\n    t = sc.nextInt();\n    while(t &gt; 0){\n\n\n         A = sc.nextBigInteger();\n         B = sc.nextBigInteger();\n         C =  karatsuba(A, B);\n         System.out.println(C);\n         --t;\n\n    }\n    sc.close();\n}\n</code></pre>\n\n<p>}</p>\n"},{"tags":["iphone","objective-c","ios","performance","cocoa-touch"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":54,"score":2,"question_id":12990722,"title":"How do I test the load time of my application?","body":"<p>I would like to benchmark the load time of my application but am not sure how to do this. I could start by inserting a couple of <code>NSLog</code>s and comparing the time stamps. But where should I place these?</p>\n\n<p>Should I place the second log in my root view controller's <code>viewDidAppear</code> since this is the first time the application is available to the user? And what about the first log? I was considering applicationDidFinishLaunchingWithOptions: but the name suggests this is a little late.</p>\n\n<p>Any references to tutorials, instruments, or anything else would be appreciated.</p>\n"},{"tags":["linux","performance","cpu","performancecounter","perf"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":13004300,"title":"perf record what is relation between -c option and overflow events?","body":"<p>I am trying to use perf record to record 15 hardware and trace point events.</p>\n\n<p>I want to understand the following:</p>\n\n<ol>\n<li><p>event based sampling: from the docs I understood perf record will sample whenever 64-bit counter corresponding to that events will over flow. Is that right? The counter will overflow after ~ 2^64 such events?</p></li>\n<li><p>When I have more events to measure than the number of PMUs/counters, do I have to pass any specific switch to use multiplexing. How does overflow-event behave in the presence of multiplexing.</p></li>\n<li><p>What is the purpose/use of the switch \"-c\" ? Can I make the counters to overflow every n-events using this switch?</p></li>\n</ol>\n\n<p>Please help. </p>\n"},{"tags":["objective-c","ios","performance","cocoa-touch","core-data"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":29,"score":0,"question_id":13022065,"title":"Indexing a Core Data property to improve NSSortDescriptor performance","body":"<p>I have a core data relationship that is unordered. For various reasons I can not use an NSOrderedSet, but I do provide an additional method on the <code>NSManagedObject</code> that uses an <code>NSSortDescriptor</code> to order the items by date.</p>\n\n<p>I am hitting a performance issue and have identified that this sort descriptor is the root cause. I have read that <code>NSSortDescriptor</code> is not particularly fast. I am considering adding and maintaining an index on the relationship since my insert/update/delete performance is much less of an issue.</p>\n\n<p>Should I expect a noticeable improvement if my <code>NSSortDescriptor</code> sorts by index instead of date? Or is the performance issue more related to the <code>NSSortDescriptor</code> itself and not the attribute being sorted on?</p>\n\n<p>If it might help, how do I do this? Do I simply select the 'Indexed' check box on my data model editor and re-generate the <code>NSManagedObject</code>? What kind of code will it add to the class? Or do I need to do some manual work also?</p>\n"},{"tags":["php","performance","button","reload","counter"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":271,"score":1,"question_id":10528638,"title":"PHP Button Counter Issue","body":"<p>I'm fairly new to PHP, but have managed to set up a script that simply displays the amount of times a button is clicked each time a user clicks on it.\n<br>Here is the PHP:</p>\n\n<pre><code>&lt;?php\n$f = fopen('counter.txt', 'r+');\nflock($f, LOCK_EX);\n$total = (int) fread($f, max(1, filesize('counter.txt')));\nif (isset($_POST['submit'])) {\nrewind($f);\nfwrite($f, ++$total);\n}\nfclose($f);\n?&gt;\n</code></pre>\n\n<p>Here is the HTML:</p>\n\n<pre><code>&lt;form action='' method=\"post\"&gt;\n&lt;input type=\"submit\" name=\"submit\" value=\"click\" /&gt;\n&lt;/form&gt;\nThis button has been clicked &lt;?php echo $total; ?&gt; times.\n</code></pre>\n\n<p>The counter works and everything, but there are three issues I hope you guys can help me with:</p>\n\n<ol>\n<li><p>The counter increases each time the page is reloaded. I only want the counter to increase when the button is clicked. Is there a way I can fix this?</p></li>\n<li><p>Each time I refresh the page, Firefox asks me to confirm that I want the page to be reloaded. I know there is an option in my browser settings to prevent this, but was wondering if there was a way to refine my php so that this message does not occur for the user as well.</p></li>\n<li><p>If you click the button a couple of times and then try to use the Back button, it takes you through each one of the previous clicks. Again, is there a way to fix my code so that it does not do this and instead goes to the previous page?</p></li>\n</ol>\n\n<p>HUGE thanks!!</p>\n"},{"tags":["objective-c","ios","performance","cocoa","core-data"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":33,"score":0,"question_id":13020684,"title":"Persisting a subset of Core Data objects","body":"<p>This is a little strange, but I want to store an array of <code>NSManagedObjects</code> outside of Core Data. </p>\n\n<p>Core Data is managing all of my model's properties and relationships. One such relationship is to all of an <code>Author</code>'s <code>Books</code>. Sometimes it is useful to know that same list in a certain order, so I have added <code>booksByDate</code> (for example) to the <code>NSManagedObject</code> Author.</p>\n\n<p>Because the sort descriptor proved expensive, I implemented a cache, using an iVar on <code>Author</code>. This helped a lot with some laggy UI issues I was experiencing. But the cache was only useful after it was first loaded, so when my application launches, I now go and tell each <code>Author</code> to cache its <code>booksByDate</code>. This adds a few seconds to my launch time, but drastically speeds up the performance once the app is running.</p>\n\n<p>I would like to reduce that launch time. One area I am experimenting with is somehow storing each <code>Author</code>'s cached <code>booksByDate</code>. At launch, instead of telling each <code>Author</code> to generate its cache using the expensive sort descriptor, I could just pass out each stored cache to it's correct <code>Author</code>.</p>\n\n<p>How might I store these caches so they persist between executions? </p>\n"},{"tags":["database","performance","hibernate","jpa"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":12898638,"title":"Database commit performance","body":"<p>I develop an App and recently I've caught me on the thought. What of the following has better performance?</p>\n\n<ol>\n<li>Perform single commit of 1000 entities.</li>\n<li>Perform 20 commits with 50 entities at each commit.</li>\n</ol>\n\n<p>And how do you think will it be implementation dependable?</p>\n"},{"tags":[".net","performance","memory-management","garbage-collection"],"answer_count":17,"favorite_count":11,"up_vote_count":37,"down_vote_count":0,"view_count":12141,"score":37,"question_id":118633,"title":"What's so wrong about using GC.Collect()?","body":"<p>Although I do understand the serious implications of playing with this function (or at least that's what I think), I fail to see why it's becoming one of these things that respectable programmers wouldn't ever use, even those who don't even know what it is for.</p>\n\n<p>Let's say I'm developing an application where memory usage varies extremely depending on what the user is doing. The application life cycle can be divided into two main stages: editing and real-time processing. During the editing stage, suppose that billions or even trillions of objects are created; some of them small and some of them not, some may have finalizers and some may not, and suppose their lifetimes vary from a very few milliseconds to long hours. Next, the user decides to switch to the real-time stage. At this point, suppose that performance plays a fundamental role and the slightest alteration in the program's flow could bring catastrophic consequences. Object creation is then reduced to the minimum possible by using object pools and the such but then, the GC chimes in unexpectedly and throws it all away, and someone dies.</p>\n\n<p>The question: In this case, wouldn't it be wise to call GC.Collect() before entering the second stage?</p>\n\n<p>After all, these two stages never overlap in time with each other and all the optimization and statistics the GC could have gathered would be of little use here...</p>\n\n<p>Note: As some of you have pointed out, .NET might not be the best platform for an application like this, but that's beyond the scope of this question. The intent is to clarify whether a GC.Collect() call can improve an application's overall behaviour/performance or not. We all agree that the circumstances under which you would do such a thing are extremely rare but then again, the GC tries to guess and does it perfectly well most of the time, but it's still about guessing.</p>\n\n<p>Thanks.</p>\n"}]}
{"total":25592,"page":8,"pagesize":100,"questions":[{"tags":["performance","optimization","memory","global-variables"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":52,"score":1,"question_id":6838602,"title":"Are there any benefits to declaring a commonly declared variable as a global?","body":"<p>For example, I have:</p>\n\n<pre><code>char query[512];\n</code></pre>\n\n<p>declared about 27 times in my application which connects to a mysql database.</p>\n\n<p>The same size each time, and declared in many different functions.</p>\n\n<p>This application will never use threads.</p>\n\n<p>The query is ALWAYS executed immediately after the query is set with snprintf.\nThere is no function in between the setting and the execution of the query to mess it up.</p>\n\n<p>Are their any benefits or performance gains to declaring this as a global variable?</p>\n"},{"tags":["c#","arrays","winforms","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":116,"score":1,"question_id":10820108,"title":"Difference in speed while accessing an array","body":"<p><strong>Is any difference (in speed of my program - execution time) between?? <br/></strong></p>\n\n<p><strong>1 st option:</strong></p>\n\n<pre><code> private void particleReInit(int loop)\n        {\n            this.particle[loop].active = true;\n            this.particle[loop].life = 1.0f;\n            this.particle[loop].fade = 0.3f * (float)(this.random.Next(100)) / 1000.0f + 0.003f; \n            this.particle[loop].r = colors[this.col][0];    // Select Red Rainbow Color\n            this.particle[loop].g = colors[this.col][1];    // Select Red Rainbow Color\n            this.particle[loop].b = colors[this.col][2];    // Select Red Rainbow Color\n            this.particle[loop].x = 0.0f;\n            this.particle[loop].y = 0.0f;\n            this.particle[loop].xi = 10.0f * (this.random.Next(120) - 60.0f);\n            this.particle[loop].yi = (-50.0f) * (this.random.Next(60)) - (30.0f);\n            this.particle[loop].xg = 0.0f; \n            this.particle[loop].yg = 0.8f; \n            this.particle[loop].size = 0.2f;\n            this.particle[loop].center = new PointF(particleTextures[0].Width / 2, particleTextures[0].Height / 2);\n\n    }\n</code></pre>\n\n<p><strong>2 nd option: <br/></strong></p>\n\n<pre><code>Particle p = particle[loop];\np.active = true;\np.life = 1.0f;\n...\n</code></pre>\n\n<p><br/>\nWhere <code>Particle particle[] = new Particle[NumberOfParticles];</code> is just an array of Particles which have some properties like life, position.</p>\n\n<p><br/>\nI'm doing it in Visual Studio 2010 Like WFA (Windows Form Aplication) and need to enhance performance (we're not able to use OpenGL, so for more particles my program tends to be slow).</p>\n"},{"tags":["sql-server","performance","stored-procedures"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":83,"score":0,"question_id":13005076,"title":"SQL Server stored procedure a lot slower than straight query","body":"<p>I have a table with over 100MM records in it. The table has a clustered index and a nonclustered index.</p>\n\n<p>I can run a basic count using T-SQL on the table and it takes 1 second to run. When I put the same exact count query inside of a stored procedure it then takes 12 seconds to run.</p>\n\n<p>I have looked at the execution plan for both the standard query and the stored procedure and they both are using the nonclustered index.</p>\n\n<p>I am not sure why the stored procedure is so slow compared to the standard query.</p>\n\n<p>I have read some stuff about reindexing in a situation like this but I am not sure why I need to do that. Also, it takes a few hours to reindex so I want to make sure that will work.</p>\n\n<p>Any help on this would be great.</p>\n\n<p>Thanks</p>\n\n<p>UPDATE </p>\n\n<p>Here is the stored procedure:</p>\n\n<pre><code>SET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\n\nALTER PROCEDURE quickCount \n\n@sYID INT,\n@eYID INT\n\nAS\nBEGIN\n\nSET NOCOUNT ON;\n\n\n    SELECT COUNT(leadID)\n    FROM dbo.leads\n    WHERE yearID &gt;= @sYID\n    AND yearID &lt;= @eYID\n\nEND\nGO\n</code></pre>\n\n<p>and here is the standard query:</p>\n\n<pre><code>SELECT COUNT(leadID)\nFROM leads\nWHERE yearID &gt;= 0\nAND yearID &lt;= 99\n</code></pre>\n\n<p>I did try to run it with no parameters and the SP runs way faster (1 second). So I am assuming that it has something to do with the parameters.</p>\n"},{"tags":["c++","database","performance","compression","bit-manipulation"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":34,"score":0,"question_id":11924954,"title":"Modifying WAH Compress bitmap performence","body":"<p>I have developing word align bitmap compression algorithm for data indexing.algorithm is based on the WAH compression research paper.compression bitmap perform well on bit-wise operation and it's very space efficient. but modifying the compressed bitmap not very efficient ,because modifying need splitting compressed word size block and several memmove cause performance bottleneck. </p>\n\n<blockquote>\n  <p>please look at the following example.</p>\n  \n  <p>example: data set -\n  [1000000,34,9,23456,6543,10000000,23440004,100,345]</p>\n</blockquote>\n\n<p>performance reduce  due to the random nature of the data set , in the real application scenario this can happened.</p>\n\n<ol>\n<li>can anyone give me a hint on how to overcome this performance problem?.</li>\n</ol>\n"},{"tags":["performance","algorithm","open","bigdata"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":53,"score":1,"question_id":13016866,"title":"BigData analysis: scalability but quick development","body":"<p>Which do you consider is the best language to processing (getting statistical results) of analysing some GB of data information, taking into account these limitations:</p>\n\n<ul>\n<li>Open source code.</li>\n<li>Data can be analyzed in matrixes.</li>\n<li>Developing time limited.</li>\n<li>Cost of processing also limited.</li>\n</ul>\n\n<p>For, example, Octave, Fortran, C++, C, Python, etc.</p>\n\n<p>Thanks.</p>\n"},{"tags":["performance","matlab","time","for-loop"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":80,"score":0,"question_id":13012914,"title":"How can i make this code more time efficient?","body":"<p>This is a part of the Profiler report, and shows how these lines are eating up the time. Can it be improved upon?      </p>\n\n<pre><code>         434 %clean up empty cells in subPoly\n         228  435 if ~isempty(subPoly) \n         169  436     subPoly(cellfun(@isempty,subPoly)) = []; \n              437 \n              438     %remove determined subpoly points from the hull polygon\n         169  439     removeIndex = zeros(size(extendedPoly,1),1); \n         169  440     for i=1:length(subPoly) \n         376  441         for j=1:size(subPoly{i}(:,1)) \n       20515  442             for k=1:size(extendedPoly,1) \n6.12 5644644  443                 if extendedPoly(k,:)==subPoly{i}(j,:) \n       30647  444                     removeIndex(k,1)=1; \n       30647  445                 end \n1.08 5644644  446             end \n0.02   20515  447         end \n         376  448     end \n         169  449     extendedPoly = extendedPoly(~removeIndex(:,1),:);  \n         169  450 end \n</code></pre>\n"},{"tags":["java","php","c","performance","code-efficiency"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":4,"view_count":102,"score":-3,"question_id":13016603,"title":"Programming Challenge: Finding 2 Similar arrays in Minimum O(n) Time","body":"<p>I think its quite a challenging programming issue..</p>\n\n<p>I Have a group of arrays in DB. </p>\n\n<p>Each Array has a list of ID numbers.</p>\n\n<p>I get one array and I need to find the most identical array to this specific one.  </p>\n\n<p>Each array has 20 IDs. </p>\n\n<p>How do I find it in minimum time? The fast answer will be compare each cell .. but, what the content of the cell are not in the same location?</p>\n\n<p>Example:</p>\n\n<pre><code>Array MyArray = [**1122, 2233, 4455**, 6677, **8899..**. , N]\n\nArray OtherArray1 = [3454, 2233, 678, 6677, 8899... , N]\n\nArray OtherArray2 = [1122, 2233, 4455, 6677, 8899... , N]\n\nArray OtherArray3 = [**1122, 2233, 4455,** 678, **8899...** , N] ***&lt;-- THE CHOSEN ONE***\n\nArray OtherArray4 = [1122, 678, 4455, 6677, 8899... , N]\n\nArray OtherArray5 = [68, 2233, 4455, 678, 8899... , N]\n...\n..\n\nArray OtherArrayN = [2223, 7777, 5573, 7776, 8899... , N]\n</code></pre>\n\n<hr>\n\n<p>Thank You!</p>\n\n<p>Elik</p>\n"},{"tags":["c++","performance","boost","c++11","parameter-passing"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":85,"score":0,"question_id":13016139,"title":"When should I use boost call_traits::param_type versus a universal reference?","body":"<p>In C++98 I got used to using <a href=\"http://www.boost.org/doc/libs/1_51_0/libs/utility/call_traits.htm\" rel=\"nofollow\">call_traits</a> in my templated functions to automatically pick the best way to pass parameters, e.g.:</p>\n\n<pre><code>template&lt;class T&gt;\nvoid foo(typename boost::call_traits&lt;T&gt;::param_type arg)\n{\n    // .. do stuff with arg ..\n}\n</code></pre>\n\n<p>The advantage being that for primitives it would pass by value and for more complex objects it would pass by reference, so I'd have the least amount of overhead possible. C++11 now has a concept of 'universal references':</p>\n\n<pre><code>template&lt;class T&gt;\nvoid foo(T&amp;&amp; arg)\n{\n    // .. do stuff with arg ..\n}\n</code></pre>\n\n<p>As I understand it I need to use a universal reference in order to get perfect forwarding with std::forward, so if I plan to use that the choice is clear. But when I don't plan to, which should I prefer? Will a universal reference always be as good or better?</p>\n"},{"tags":["java","performance","oracle","stored-procedures"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":48,"score":0,"question_id":13015991,"title":"Processing a text file from PL/SQL vs Java","body":"<p>I need to implement a store procedure in an Oracle database that will do the following:</p>\n\n<ul>\n<li>Read an external file that needs to be processed (extract data from file and validate)</li>\n<li>Call another store procedure in the database in charge to validate/insert the data.</li>\n<li>Manage exceptions.</li>\n<li>Write to another file with the results of the operations executed.</li>\n</ul>\n\n<p>I know I can do all these things with PL/SQL or Java (store procedure), but which will be more efficient/faster or better? most of the operations are reading/writing a file, and the database operations are done in a store procedure already. </p>\n\n<p>I have read other posts about PL/SQL vs Java (like <a href=\"http://stackoverflow.com/questions/6821841/java-stored-procedure-vs-pl-sql-stored-procedure\">this</a> and <a href=\"http://www.coderanch.com/t/550834/Oracle-OAS/Native-Java-Oracle-vs-PL\" rel=\"nofollow\">this</a>) but none talks about this. </p>\n"},{"tags":["performance","io","fortran","implied-do"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":80,"score":0,"question_id":12567087,"title":"Implied do. vs explicit loop with IO","body":"<p>I realize this question has been asked <a href=\"http://stackoverflow.com/questions/4563918/are-implied-do-loops-inneficient\">before</a>, but not in the context of IO.  Is there any reason to believe that:</p>\n\n<pre><code>!compiler can tell that it should write the whole array at once?\n!but perhaps compiler allocates/frees temporary array?\nwrite(UNIT) (/( arr(i), i=1,N )/)\n</code></pre>\n\n<p>would be any more efficient than:</p>\n\n<pre><code>!compiler does lots of IO here?\ndo i=1,N\n   write(UNIT) arr(i)\nenddo\n</code></pre>\n\n<p>for a file which is opened as:</p>\n\n<pre><code>open(unit=UNIT,access='STREAM',file=fname,status='UNKNOWN')\n</code></pre>\n\n<p>There is a possibly that this will be used with compiler options to turn off buffered writing as well ...</p>\n"},{"tags":["performance","hashtable"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":62,"score":1,"question_id":12631413,"title":"How to select a good hashing function (for a hashtable)","body":"<p>I was wondering how one would best approach the task of deciding upon the operations a hashing function should perform on it's input, based on the probable input format of course.</p>\n\n<p>Are there any rule(book)s i have yet to find?</p>\n\n<p>How could i estimate the cost of such a function?</p>\n\n<p>Can i somehow foresee the likelihood of collisions knowing the charset used for inputs?</p>\n\n<p>Thanks for your food for my thought in advance. :)</p>\n"},{"tags":["wcf","performance","concurrency","instance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":34,"score":1,"question_id":13015000,"title":"Single Instance VS PerCall in WCF","body":"<p>There are a lot of posts saying that <code>SingleInstance</code> is a bad design. But I think it is the best choice in my situation.<br>\nIn my service I have to return a list of currently logged-in users (with additional data). This list is identical for all clients. I want to retrieve this list from database every 5 seconds (for example) and return a copy of it to the client, when needed.<br>\nIf I use <code>PerCall</code> instancing mode, I will retrieve this list from database every single time. This list is supposed to contain ~200-500 records, but can grow up to 10 000 in the future. Every record is complex and contains about 10 fields. </p>\n\n<p>So what about performance? Is it better to use \"bad design\" and get list once or to use \"good approach\" and get list from database on every call?</p>\n"},{"tags":["c#",".net","performance","linq","optimization"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":98,"score":0,"question_id":13014357,"title":"Optimizing A LINQ To Objects Query","body":"<p>I'm trying to optimize the below LINQ query to improve it's speed performance.  The number of objects it's searching against could be in the tens of thousands.</p>\n\n<pre><code>var lQuery = from o in oEvents\nwhere (o.oSalesEvent != null &amp;&amp; o.oSalesEvent.OccurDate &lt; oCalcMgr.OccurDate &amp;&amp; (\n                (oCalcMgr.InclTransTypes == Definitions.TransactionTypes.SalesAll) ?\n                   (o.oSalesEvent.EventStateID == ApprovedID || o.oSalesEvent.EventStateID == PendingID) : \n                   o.oSalesEvent.EventStateID == ApprovedID)) &amp;&amp;\n      ((oCalcMgr.InclTransTypes == Definitions.TransactionTypes.SalesAll) ? \n                (o.oSalesMan.oEmployment.EventStateID == ApprovedID || o.oSalesMan.oEmployment.EventStateID == PendingID) : \n                 o.oSalesMan.oEmployment.EventStateID == ApprovedID)\nselect new { SaleAmount = o.SaleAmount.GetValueOrDefault(), CompanyID = o.oSalesEvent.CompanyID };\n</code></pre>\n\n<p>The query basically says, give me the sales amounts and company ids from all sale events that occurred prior to a certain date.  The sale event's status and the salesman's employment status should either always be \"approved\" or they can be also \"pending\" if specified.</p>\n\n<p>As you can see there's a date comparison and a couple of integer comparisons.  Which integer comparison used is based on whether or not a property matches a certain Enum value.</p>\n\n<p>I have some ideas of my own on ways to go about the optimization, but I want to hear others thoughts, who might have more insight into how LINQ would translate this query behind the scenes.</p>\n\n<p>Thanks</p>\n"},{"tags":["web-services","performance","rest","soap"],"answer_count":7,"favorite_count":4,"up_vote_count":6,"down_vote_count":0,"view_count":4832,"score":6,"question_id":4163066,"title":"Rest vs. Soap. Has REST a better performance?","body":"<p>I read some questions already posted here regarding Soap and Rest\nand I didn't find the answer I am looking for.\nWe have a system which has been built using Soap web services.\nThe system is not very performant and it is under discussion\nto replace all Soap web services for REST web services.\nSomebody has argued that Rest has a better performance.\nI don't know if this is true. (This was my first question)\nAssuming that this is true, is there any disadvantage using \nREST instead of Soap? (Are we loosing something?)</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["javascript","jquery","performance","single-page-application"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":84,"score":0,"question_id":7752781,"title":"<script> tag at bottom causing $(document).ready to fail","body":"<p>I was following the snippets from html5boilerplate.com (as well as something that is recommended by yahoo) that we should put the scripts at the bottom of the page (of my site's main page).</p>\n\n<p>I am also following SPI (Single page interface) - meaning I will load the content area only via ajax for any new page visit.</p>\n\n<p>My page structure is..</p>\n\n<pre><code>&lt;body&gt;\n&lt;header /&gt;\n&lt;div id=\"content\" /&gt;\n&lt;footer /&gt;\n&lt;/body&gt;\n</code></pre>\n\n<p>Now issue is ..\nI load JQuery at the end of main page. But I need some javascript to get executed on the child page which would be loaded in content div. So if I write \n    <code>$(document).ready(..)</code>\nin the child page, it will crib about \"$ is not defined\".</p>\n\n<p>Any suggestions?</p>\n"},{"tags":["java","xml","performance","parsing"],"answer_count":10,"favorite_count":5,"up_vote_count":10,"down_vote_count":0,"view_count":15760,"score":10,"question_id":530064,"title":"Fastest XML parser for small, simple documents in Java","body":"<p>I have to objectify very simple and small XML documents (less than 1k, and it's almost SGML: no namespaces, plain UTF-8, you name it...), read from a stream, in Java.</p>\n\n<p>I am using JAXP to process the data from my stream into a Document object. I have tried Xerces, it's way too big and slow... I am using Dom4j, but I am still spending way too much time in org.dom4j.io.SAXReader.</p>\n\n<p>Does anybody out there have any suggestion on a faster, more efficient implementation, keeping in mind I have very tough CPU <em>and</em> memory constraints?</p>\n\n<p>[Edit 1] Keep in mind that my documents are very small, so the overhead of <em>staring</em> the parser can be important. For instance I am spending as much time in org.xml.sax.helpers.XMLReaderFactory.createXMLReader as in org.dom4j.io.SAXReader.read</p>\n\n<p>[Edit 2] The result <em>has to</em> be in Dom format, as I pass the document to decision tools that do arbitrary processing on it, like switching code based on the value of arbitrary XPaths, but also extracting lists of values packed as children of a predefined node.</p>\n\n<p>[Edit 3] In any case I eventually need to load/parse the complete document, since all the information it contains is going to be used at some point.</p>\n\n<p>(This question is related to, but different from, <a href=\"http://stackoverflow.com/questions/373833/best-xml-parser-for-java\">http://stackoverflow.com/questions/373833/best-xml-parser-for-java</a> )</p>\n"},{"tags":["objective-c","ios","performance","cocoa-touch","core-data"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":36,"score":0,"question_id":13014474,"title":"Using a cache, populating it at launch, and optimizing launch time","body":"<p>I have an <code>NSManagedObject</code> 'Recipe' with an unordered, to-many relationship to <code>items</code>. I often need the recipe's items in a specific order, so I added some methods to <code>Recipe</code> - <code>itemsByName</code>, <code>itemsByDate</code>, etc. These use a <code>sortDescriptor</code> to sort everything in the <code>items</code> set and return an <code>NSArray</code>.</p>\n\n<p>Accessing <code>sortedArrayUsingDescriptors</code> during runtime was proving to be a noticeable bottleneck in my UI, so I added a caching mechanism. <code>Recipe</code> now has an iVar for each sorted array: <code>_cachedItemsByName</code>, <code>_cachedItemsByDate</code>, etc. If the cache exists (and is not flagged as dirty), the <code>itemsByName</code> method returns the iVar instead of re-sorting the set each time.</p>\n\n<p>This worked fine once my application had been running a while, but still lead to noticeable UI issues each time a recipe's ordered items was requested for the first time (since the cache had to be created using <code>sortedArrayUsingDescriptors</code>). So, I decided that when the application launches, it should update the cache for all of my recipes immediately - sacrificing load time for a better in-app experience. This has worked very well and my UI issues are now resolved.</p>\n\n<p>However, I would really like to chip away at that initial load time. But I'm not sure where to turn. I have considered the following options:</p>\n\n<ul>\n<li>On launch, update all of the caches in a background thread. After a few hours of research into GCD this seems like a non-starter. I've read that I can't interact with the NSManagedObjectContext or any of it's objects in a different thread (my processing just involves iterating through each <code>Recipe</code> object and telling it to update it's iVar. This involves sorting through it's <code>items</code> relationship).</li>\n<li>Just make <code>items</code> an ordered relationship so that the order doesn't have to be calculated at run time. But I have multiple orders in which I need to return the items, so I would need multiple relationships to the same objects, just in different orders. This seems icky - I would be responsible for keeping each <code>NSOrderedSet</code> up to date. For example, when an item is added, I would have to insert it multiple times into different relationships. Also, I have read that the NSOrderedSet introduces it's own performance overhead.</li>\n<li>Some other way to persist the cache - I don't really know where to start on this one. Perhaps write each Recipe's cache to file somehow, and then try to match them up at load time? I'm not sure this would be any quicker than re-sorting each recipe on launch, but I would be willing to try it out if someone could suggest how and where to store this.</li>\n</ul>\n\n<p>I would love some feedback on each of the options above, and even better, some alternatives. I am pleased with my solution for improving in-app performance, but would really like to speed up my initial launch.</p>\n"},{"tags":["sql","sql-server","performance","query"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":63,"score":1,"question_id":12558483,"title":"SQL query slow calculating cumulative totals","body":"<p>We are storing data readings at 5 minute intervals for a large number of gauges.</p>\n\n<p>The data tables involved are:</p>\n\n<pre><code>Table1 - GaugeData\nColumns - \n    GaugeID (int, primary key)\n    Timestamp (datetime, primary key)\n    Value (decimal)\n\nTable2 - GaugeSummaryData\nColumns - \n    GaugeID (int, primary key)\n    DayTimestamp (date, primary key)\n    DayTotal (decimal) - total for the current date/day\n    CumulativeTotal (decimal) - total up to and including the current date\n</code></pre>\n\n<p>Without changing the table structure in any way, what would be the most efficient way to copy and aggregate data from GaugeData into GaugeSummaryData?</p>\n\n<p>I have attempted this two ways already. Using a cursor takes 40 minutes to copy all data from GaugeData to GaugeSummaryData. Using insert/update statements took 2hrs+.</p>\n\n<p>Could somebody please suggest the most efficient way? Pseudocode or SQL appreciated.</p>\n"},{"tags":["mysql","database","performance","optimization","subquery"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":50,"score":2,"question_id":13012989,"title":"Slow MySQL query with AS and subquery","body":"<p>I have a problem with this slow query that runs for 10+ seconds:</p>\n\n<pre><code>SELECT DISTINCT siteid,\n                storyid,\n                added,\n                title,\n                subscore1,\n                subscore2,\n                subscore3,\n                ( 1 * subscore1 + 0.8 * subscore2 + 0.1 * subscore3 ) AS score\nFROM   articles\nWHERE  added &gt; '2011-10-23 09:10:19'\n       AND ( articles.feedid IN (SELECT userfeeds.siteid\n                                 FROM   userfeeds\n                                 WHERE  userfeeds.userid = '1234')\n              OR ( articles.title REGEXP '[[:&lt;:]]keyword1[[:&gt;:]]' = 1\n                    OR articles.title REGEXP '[[:&lt;:]]keyword2[[:&gt;:]]' = 1 ) )\nORDER  BY score DESC\nLIMIT  0, 25 \n</code></pre>\n\n<p>This outputs a list of stories based on the sites that a user added to his account. The ranking is determined by score, which is made up out of the subscore columns. </p>\n\n<p>The query uses filesort and uses indices on PRIMARY and feedid.\nResults of an EXPLAIN:</p>\n\n<pre><code>1   PRIMARY articles    \nrange   \nPRIMARY,added,storyid   \nPRIMARY  729263 rows    \nUsing where; Using filesort\n\n2   DEPENDENT SUBQUERY  \nuserfeeds   \nindex_subquery  storyid,userid,siteid_storyid   \nsiteid  func    \n1 row   \nUsing where\n</code></pre>\n\n<p>Any suggestions to improve this query? Thank you.</p>\n"},{"tags":["c++","c","performance","optimization"],"answer_count":12,"favorite_count":2,"up_vote_count":14,"down_vote_count":1,"view_count":773,"score":13,"question_id":12982884,"title":"C/C++ optimizing away checks to see if a function has already been run before","body":"<p>Let's say you have a function in C/C++, that behaves a certain way the first time it runs. And then, all other times it behaves another way (see below for example). After it runs the first time, the if statement becomes redundant and could be optimized away if speed is important.  Is there any way to make this optimization?</p>\n\n<pre><code>bool val = true; \n\nvoid function1() {\n\n   if (val == true) {\n      // do something\n      val = false; \n   }\n   else {\n      // do other stuff, val is never set to true again \n   }\n\n}\n</code></pre>\n"},{"tags":["css","performance","netbeans","syntax-highlighting","bug"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":28,"score":1,"question_id":13013247,"title":"Netbeans Syntax Highlighting Lag","body":"<p>I'm working on a pretty big project that uses a CMS and whenever I'm editing a file with a lot of lines of code (especially in a CSS file), the syntax highlighting takes a couple of seconds after I'm finished typing to change colors.</p>\n\n<p>Netbeans feels a lot faster than Eclipse, but why is this bug persisting.</p>\n\n<p>Anyone else having this issue?</p>\n"},{"tags":["android","performance","andengine"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":99,"score":0,"question_id":12998860,"title":"Andengine - Always keep the same speed of the body","body":"<p>I am developing a game where you are on side scroll on screen middle (player) and enemyes appears from left or right continusly.</p>\n\n<p>I create a enemy, set a Linearvelocity to right (or left) on this way:\ngetBody.setLinearVelocity(v*this.getDireccion(), 0);</p>\n\n<p>Then, when the enemy collides with another enemy or player, sometimes change direction (he slides back) or get more slow or fast. I need that always have the same LinearVelocity, and if this enemy is colliding with a player or another monster, dont stop, dont slow, dont fast... same velocity and direction.</p>\n\n<p>Anybody can help me with this? \nSorry my english.</p>\n"},{"tags":["c#","performance","cpu-usage","wait","performance-testing"],"answer_count":4,"favorite_count":3,"up_vote_count":3,"down_vote_count":0,"view_count":135,"score":3,"question_id":13001578,"title":"I need a slow C# function","body":"<p>For some testing I'm doing I need a C# function that takes around 10 seconds to execute. It will be called from an ASPX page, but I need the function to eat up CPU time on the server, not rendering time. A slow query into the Northwinds database would work, or some very slow calculations. Any ideas?</p>\n"},{"tags":["c#",".net","performance","optimization","file-size"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":191,"score":4,"question_id":7583688,"title":"Is there a way to get the size of a file in .NET using a static method?","body":"<p>I know the normal way of getting the size of a file would be to use a FileInfo instance:</p>\n\n<pre><code>using System.IO;\nclass SizeGetter\n{\n  public static long GetFileSize ( string filename )\n  {\n    FileInfo fi = new FileInfo ( filename );\n    return fi.Length;\n  }\n}\n</code></pre>\n\n<p>Is there a way to do the same thing without having to create an instance of FileInfo, using a static method? Maybe I'm trying to be overly stingy with creating a new instance every time I want a file size, but take for example trying to calculate the total size of a directory containing 5000+ files. As optimized as the GC may be, shouldn't there be a way to do this without having to tax it unnecessarily?</p>\n"},{"tags":["visual-studio-2010","performance","stackframe"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":29,"score":0,"question_id":13006371,"title":"Does omitting the frame pointers really have a positive effect on performance and a negative effect on debug-ability?","body":"<p>As was advised long time ago, I always build my release executables without frame pointers (which is the default if you compile with /Ox).</p>\n\n<p>However, now I read in the paper <a href=\"http://research.microsoft.com/apps/pubs/default.aspx?id=81176\" rel=\"nofollow\">http://research.microsoft.com/apps/pubs/default.aspx?id=81176</a>, that frame pointers don't have much of an effect on performance.  So optimizing it fully (using /Ox) or optimizing it fully with frame pointers (using /Ox /Oy-) doesn't really make a difference on peformance.</p>\n\n<p>Microsoft seems to indicate that adding frame pointers (/Oy-) makes debugging easier, but is this really the case?</p>\n\n<p>I did some experiments and noticed that:</p>\n\n<ul>\n<li>in a simple test executable (compiled using /Ox /Ob0) the omission of frame pointers does increase performance (with about 10%).  But this test executable only performs some function calls, nothing else.</li>\n<li>in my own application the adding/removing of frame pointers don't seem to have a big effect.  Adding frame pointers seems to make the application about 5% faster, but that could be within the error margin.</li>\n</ul>\n\n<p>What is the general advice regarding frame pointers?</p>\n\n<ul>\n<li>should they be omitted (/Ox) in a release executable because they really have a positive effect on performance?</li>\n<li>should they be added (/Ox /Oy-) in a release executable because they improve debug-ablity (when debugging with a crash-dump file)?</li>\n</ul>\n\n<p>Using Visual Studio 2010.</p>\n"},{"tags":["ios","performance","uiscrollview","photo-gallery"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":58,"score":1,"question_id":13011128,"title":"UIScrollView: Bad performance with large images","body":"<p><strong>TL:DR</strong></p>\n\n<p>What technique does Apple use to make Photo.app so fast, even with large images?</p>\n\n<p><strong>Long Version</strong></p>\n\n<p>I watched Apple's WWDC 2010 video about scroll views to learn how to replicate Photo.app pagination behavior and low memory utilization (PhotoScroller Demo). It works well, but since images are loaded only when they are needed, when I try to paginate to another image, the app locks while the JPEG is being decompressed.</p>\n\n<p>The same video shows a tiling technique to get better performance, but since I'm using photos taken from the camera and stored in the app, that doesn't seem feasible (having multiple copies of each photo, in different resolutions, would consume too much space - 4MB vs 27MB). Also, using iExplorer I noticed Photo.apps has only a copy of each photo (it doesn't even have a small thumbnail copy for the gallery).</p>\n\n<p>What technique did Apple use to make Photos.app so fast? How can I get that same performance in my app?</p>\n\n<blockquote>\n  <p>I'm a bit confused if this should be here or on Programmers,\n  since there's no code in the question, but F.A.Q. says that algorithm\n  questions are part of Stackoverflow, and the tags here match it\n  better.</p>\n</blockquote>\n"},{"tags":["database","performance","ssis","etl"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":2445,"score":0,"question_id":809119,"title":"How to Handling Incremental Load with large datasets ssis","body":"<p>I have 2 tables (~ 4 million rows) that I have to do insert/update actions on matching and unmatching records. I am pretty confused about the method I have to use for incremental load. Should I use Lookup component or new sql server merge statement? and will there be too much performance differences?</p>\n"},{"tags":["performance","function","parameters","call"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":4,"view_count":38,"score":-3,"question_id":13008832,"title":"what is faster: one call to a function with many parameters or more calls to functions with fewer parameters?","body":"<p>I have 9 buffers that I need to send over CAN. I have a function that has 9 parameters, representing the code for which variables I want to have in each buffer. The function contains 9 switch-case. The question is: will this be faster if I have 3 functions each with 3 parameters? So instead of calling one function with 9 parameters, it's faster to call 3 functions with 3 parameters?</p>\n"},{"tags":["iphone","performance","graphics","opengl-es"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":1485,"score":2,"question_id":2412169,"title":"How do you represent a normal or texture coordinate using GLshorts?","body":"<p>A lot of suggestions on improving the performance in iPhone games revolve around sending less data to the GPU. The obvious suggestion is to use GLshorts instead of GLfloat wherever possible, such as for vertices, normals, or texture coordinates.</p>\n\n<p>What are the specifics when using a GLshort for a normal or a texture coordinate? Is it possible to represent a GLfloat texture coordinate of 0.5 when using a GLshort? If so, how do you do that? Would it just be SHRT_MAX/2? That is, does the range of 0 to 1 for a GLfloat map to 0 to SHRT_MAX when using a GLshort texture coordinate?</p>\n\n<p>What about normals? I've always created normals with GLfloats and normalized them to unit length. When using a GLshort for a normal, are you sending a non-normalized vector to the GPU? If so, when and how is it normalized? By dividing by all components by SHRT_MAX?</p>\n"},{"tags":["performance","algorithm","data-structures"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":63,"score":2,"question_id":13009179,"title":"Data Structure for storing ranges of values that allows efficient comparision operation","body":"<p>I am looking for a data structure that allows storing non-overlapping ranges of integers\nand comparing whether a certain range exists[is covered] in[by] the data structure.</p>\n\n<p>For example, after I store (0,9),(10,19),(30,29), \nat some point later I want to check if the range (1,11) is covered, in which case the\nalgorithm gives a \"yes\" whereas with the range (15,25) the algorithm gives a \"no\" as the given range is not covered.</p>\n\n<p>Many thanks in advance.</p>\n"},{"tags":["php","performance","oop","parameters","instance-variables"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":64,"score":1,"question_id":13007658,"title":"PHP OOP Performance : Parameter or Instance Variable?","body":"<p>Im curious about getting an answer for a OOP question but can't find any info so far.</p>\n\n<p>Here goes, writing classes and methods is it faster to pass in parameters for each method or use instance/field variables and $this->x;</p>\n\n<p>Which would be faster at run time?</p>\n\n<pre><code>class ExampleByParameter(){\n\n function SomeMethod($a,$b){\n echo $a.\" \".$b;\n return;\n }\n\n}\n</code></pre>\n\n<p>or</p>\n\n<pre><code>class ExampleByInstance(){\n\n function __construct($a,$b){\n $this-&gt;a=$a;\n $this-&gt;b=$b;\n }\n\n function SomeMehtod(){\n $a=$this-&gt;a;\n $b=$this-&gt;b;\n echo $a.\" \".$b;\n return;\n }\n\n}\n</code></pre>\n\n<p>I would imagine their would be no difference with the examples above but im thinking there might be a significant difference with more complicated code. </p>\n"},{"tags":[".net","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":36,"score":1,"question_id":13008833,"title":"Impact of declaring multiple classes","body":"<p>In my .Net application (MVVM application) i have let's say 10 drop down lists. So i created 10 different classes(Models) containing just Name , Value pair. The reason of creating 10 different classes is just because these drop down lists are functionally independent.</p>\n\n<p>What are the pros and cons (Including memory impact) of 10 different classes instead of just 1 class (Which contains Name Value Pair and Bound to View through ViewModel)</p>\n"},{"tags":["asp.net",".net","performance","sql-server-2005"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":52,"score":0,"question_id":13007716,"title":"Performance of web application .net 2.0","body":"<p>I have just started working over a website written in .net 2.0. Pages take long to load and response time is quite low, not sure where to start from in order to improve performance of the same.</p>\n\n<p>Hardware is not a problem as there is enough memory and processor is also good enough.</p>\n\n<p>Any Idea where should I start from and to improve the performance.</p>\n"},{"tags":["performance","http","file-upload","upload","ftp"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":13006726,"title":"Why am I getting extremely slow upload speeds via http vs ftp?","body":"<p>I'm getting 10mps upload speeds to my server using ftp but when I use http protocol my upload speeds are around 300kpbs, which is about 3% of the ftp file upload speed.  I've heard that http protocol upload speed is about the same as ftp protocol upload speed so why is there such a huge difference when I try to use a script to upload files through a web browser?  </p>\n\n<p>I wanted to create a user content generated site where users can upload massive files, like 1GB or more and I don't want to put my users through a lot of pain by making them use an ftp client, so I'd rather have them upload files through my site after they login to their member area.  Is this an issue with the server itself that is limiting the speed of file uploads when the http protocol is used?  Please help.</p>\n"},{"tags":["c#","asp.net","performance","iis","iis-7.5"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":62,"score":-1,"question_id":13006499,"title":"Measure ASP.NET Website Server Side Executing Time","body":"<p>I've a website hosted on windows hosting IIS 7.5. How can I measure server side (C#) executing time?.. Is it possible to get it without IIS?</p>\n\n<p>I need some tools, software or online tools</p>\n"},{"tags":["performance","drupal","views","drupal-theming"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":1874,"score":1,"question_id":9914220,"title":"How to theme views fields in Drupal 7 correctly","body":"<p>I need to theme views in Drupal 7. There is a content type 'Book' and I need to list 5 books and theme them in special manner(preview image, title and author).</p>\n\n<p>When I override views-view-field.tpl.php and print raw SQL result, I see that all fields are displayed. This code</p>\n\n<pre><code>echo \"&lt;pre&gt;\";\nprint_r($row);\necho \"&lt;/pre&gt;\";\n</code></pre>\n\n<p>gives</p>\n\n<pre><code>[entity] =&gt; stdClass Object\n (\n  [title] =&gt; ...\n  ....\n  [nid] =&gt; 34\n  ...\n  [body] =&gt; Array\n  ...\n</code></pre>\n\n<p>But I don't want pass [body] from database to php side, because it can be huge and cause a performance issue. I haven't selected [body] in view settings. </p>\n\n<p>Is there a way to pass only certain fields to views-view-field.tpl.php?</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["performance","big-o","worst-case"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":36,"score":1,"question_id":13002478,"title":"Nested for loops running time calculating","body":"<p>How can I calculate worst case running time of this loop?</p>\n\n<pre><code>for(int i=1 ; i * i &lt; n ; i*=2)\n\n{\n     //do something\n}\n</code></pre>\n"},{"tags":["performance","actionscript-3","flash","variables","definition"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":55,"score":1,"question_id":12986990,"title":"Does defining variables within a loop matter?","body":"<p>This is a piece of code I am writing.</p>\n\n<pre><code>var cList:XMLList = xml.defines.c;\nvar className:String;\nvar properties:XMLList;\nvar property:XML;\nvar i:int,l:int;\nvar c:XML;\n\nfor each(c in cList)\n{\n    className = String(c.@name);\n\n    if(cDict[className])\n    {\n        throw new Error('class name has been defined' + className);\n    }\n\n    if(className)\n    {\n        cDict[className] = c;\n    }\n\n    properties = c.property;\n\n    i = 0,\n    l = properties.length();\n\n    if(l)\n    {\n        propertyDict[className] = new Dictionary();\n\n        for(;i&lt;l;i++)\n        {\n            // ...\n        }\n    }\n}\n</code></pre>\n\n<p>As you can see, I defined all variables outside of loops. I am always worried, that if I defined them inside the loop, it might slow down the process speed, though I don't have proof - it's just a feeling.</p>\n\n<p>I also don't like that the as3 grammar allows using a variable name before the defintion. So I always define vars at the very beginning of my functions. </p>\n\n<p>Now I am worried these habits might backfire on me someday. Or is it just a matter of personal taste?</p>\n"},{"tags":["php","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":60,"score":1,"question_id":13003340,"title":"Will the code after false \"if\" statements be compiled","body":"<p>if I have code like that</p>\n\n<pre><code>if($abc==true) {\n   //code code code\n} else {\n   // other code, other code\n}\n</code></pre>\n\n<p>So if <code>$abc</code> is <code>true</code> will the \"other code\" be compiled?</p>\n\n<p>So is this irrelevant if there is a lot of \"code\" performance wise?</p>\n"},{"tags":["sql-server","performance","entity-framework","multi-tenant","parameter-sniffing"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":82,"score":1,"question_id":12979493,"title":"Multi-tenant SQL Server databases and parameter sniffing","body":"<p>I have a multi-tenant database in SQL Server 2012 where each tenant's rows are identified by a <code>tenant_id</code> column (aka the <a href=\"http://msdn.microsoft.com/en-us/library/aa479086.aspx\" rel=\"nofollow\">Shared Database, Shared Schema</a> approach). Some tenants, particularly the newer ones, have very few rows, while others have many.</p>\n\n<p>SQL Server's query optimizer normally builds a query plan based on the parameters provided during its first execution, then re-uses this plan for all future queries even if different parameters are provided. This is known as <a href=\"http://www.sommarskog.se/query-plan-mysteries.html\" rel=\"nofollow\">parameter sniffing</a>.</p>\n\n<p>The problem we have with our database is that SQL Server sometimes builds these plans based on parameters that point to a smaller tenant, which works fine for that tenant, but then when it reapplies the cached plan to a larger tenant it fails catastrophically (usually timing out, in fact). Typically we find out about this situation only when one of our larger tenants contacts us about experiencing time-out errors, then we have to get into the system and manually flush all the query plans to correct it.</p>\n\n<p>There is a query hint you can use to prevent SQL Server from caching query plans (<a href=\"http://www.benjaminnevarez.com/2010/06/how-optimize-for-unknown-works/\" rel=\"nofollow\"><code>OPTIMIZE FOR UNKNOWN</code></a>) but this results in some extra overhead since the query plan is being regenerated every time the query is called. An additional problem is that we're using Entity Framework which offers no ability to specify the <code>OPTIMIZE FOR UNKNOWN</code> hint on queries.</p>\n\n<p>So the question is -- what is the best practice for multi-tenant databases with regard to parameter sniffing? Is there a way to disable parameter sniffing database-wide without having to specify it on every query? If so, is that even the best approach? Should I be partitioning the data in some other way? Is there some other approach I'm not thinking of?</p>\n"},{"tags":["java","string","performance","file-io","concatenation"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":276,"score":1,"question_id":9107097,"title":"Java efficient file writing: String concatentation versus extra call to write()","body":"<p>In the code below, which case (1 or 2) is more \"efficient\"?</p>\n\n<pre>\nstatic final String NEWLINE = System.getProperty(\"line.separator\");\nVector text_vec = ...;\nFileWriter file_writer = new FileWriter(path);\nBufferedWriter buffered_writer = new BufferedWriter(file_writer);\ntry {\n    for (String text: text_vec) {\n\n        // Case 1: String concatenation\n        buffered_writer.write(text + NEWLINE);\n\n        // Case 2: Extra call to write()\n        buffered_writer.write(text);\n        buffered_writer.write(NEWLINE);\n    }\n}\nfinally {\n    buffered_writer.close();\n}\n</pre>\n\n<p>In case #1, as I understand, the String concatenation is handled by the Java compiler by automatically allocating a StringBuilder object.  Since the String values are not known at compile time, it is not possible to concatenate \"early\" (during compile time).</p>\n\n<p>So the question stands: Which one is more efficient (CPU/memory/wall clock time)?</p>\n\n<p>I am leaving the exact definition of \"efficient\" to those whom answer.  I am not an expert on the Java virtual machine.  I hope others can enlighten!</p>\n\n<p>Thanks.</p>\n"},{"tags":["performance","cocos2d","collision"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":12996761,"title":"Cocos2d fast collision detection with many sprites","body":"<p>I have a LOT of sprites (shooting game) and I need to test collision with many other objects in the level (Player, walls, crates etc...) and it's working fine if I brute force the CGRectIntersectsRect method. The problem is very apparent with this method as I'm checking every bullet against every object every frame. I know a bit about how to speed things up but I wanted to get some more experience game dev's insight (perhaps cocos2d specific) before I spend a couple days implementing some sort of spacial partitioning hierarchy.</p>\n\n<p>Any help would be greatly appreciated!</p>\n"},{"tags":["eclipse","tips-and-tricks","performance"],"answer_count":23,"favorite_count":227,"up_vote_count":292,"down_vote_count":0,"view_count":142430,"score":292,"question_id":316265,"title":"Tricks to speed up Eclipse","body":"<p>Which tricks do you know to make the experience with Eclipse faster?  </p>\n\n<p>For instance: I disable the all the plugins I don't need (Mylyn, Subclipse, &hellip;).</p>\n\n<p>Instead of using a plugin for Mercurial I configure TortoiseHG as an external tool.</p>\n"},{"tags":["javascript","css","performance","image","sprite"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":94,"score":1,"question_id":13000650,"title":"Any good alternatives to CSS sprites?","body":"<p>I'm working on an app that needs to load a lot of icons. At the moment I'm using <code>&lt;img&gt;</code> tags with the <code>src</code>'s set to the right URLs. That triggers a lot of request. Even if caching is properly set up there are still a lot of HEAD requests.</p>\n\n<p>I'm looking for a solution that will trigger only a minimum of requests (best would be 1) to receive all of the needed icons. But I don't like the concept of spriting, since it's harder to change/replace/add icons in this concept.</p>\n\n<p>Are there any other solutions?</p>\n\n<p>For example, would it be to slow to base64-encode all images to one file on the server, send them to the browser, split them with JavaScript and set base64 src's to the img tags?</p>\n\n<p>Does anyone test this or different approaches?</p>\n"},{"tags":["iphone","xcode","performance","unity3d"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":437,"score":0,"question_id":4558888,"title":"Xcode debugging is slow!","body":"<p>I'm trying to debug an Unity3D iPhone app using Xcode and it's ultra slow. </p>\n\n<p>The application is running smoothly but the Xcode environment crawls to a halt. I can barely get to the debug menu and turn terminate it. I'm running this on a 2GHz, 3GB RAM machine, and Xcode isn't even hogging any cpu in the activity monitor. Any ideas to what might be causing this?</p>\n\n<p>Regards/Per</p>\n"},{"tags":["c++","performance","boost","c++11","std-function"],"answer_count":2,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":271,"score":7,"question_id":11126238,"title":"Have the ideas behind the Fast Delegate (et al) been used to optimize std::function?","body":"<p>There have been proposals for C++ \"delegates\" which have lower overhead than <code>boost::function</code>:</p>\n\n<ul>\n<li><a href=\"http://www.codeproject.com/Articles/7150/Member-Function-Pointers-and-the-Fastest-Possible\" rel=\"nofollow\">Member Function Pointers and the Fastest Possible C++ Delegates</a></li>\n<li><a href=\"http://www.codeproject.com/Articles/13287/Fast-C-Delegate\" rel=\"nofollow\">Fast C++ Delegate</a></li>\n<li><a href=\"http://www.codeproject.com/Articles/11015/The-Impossibly-Fast-C-Delegates\" rel=\"nofollow\">The Impossibly Fast C++ Delegates</a></li>\n</ul>\n\n<p>Have any of those ideas been used to implement <code>std::function</code>, resulting in better performance than <code>boost::function</code>? Has anyone compared the performance of <code>std::function</code> vs <code>boost::function</code>?</p>\n\n<p><strong>I want to know this specifically for the GCC compiler and libstdc++ on Intel 64-bit architectures</strong>, but information on other compilers is welcome (such as Clang).</p>\n"},{"tags":["arrays","performance","scala","map","for-loop"],"answer_count":1,"favorite_count":0,"up_vote_count":5,"down_vote_count":1,"view_count":154,"score":4,"question_id":12999680,"title":"Poor performance of Array.map(f: A => B) in Scala","body":"<p>Please can someone explain to me, why Array.map(f: A=> B) method is implemented in a such way that it is more than 5 times slower than this code:</p>\n\n<pre><code>val list = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)\nval newList = new Array[Int](size)\n\nvar j = 0  \nwhile (j &lt; size) {\n  newList(j) = list(j)\n  j += 1\n}\n</code></pre>\n\n<p>The method map(f: A=> B) in the Array class, which is provided by TraversableLike trait, uses Scala 'for loop' for iterating over the elements of input Array object, which of course is much slower than using 'while loop'.</p>\n\n<p>Scala version: 2.9.2\nJava: jdk1.6.0_23 64bit windows</p>\n"},{"tags":["php","mysql","performance","comparison"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":210,"score":2,"question_id":8473756,"title":"Compare string with values from mysql","body":"<p>I have a database containing the name of bands and other artists related to music. Now i want to check a string containing an artist name against this database and find similar or equal artists to avoid different kind of spelling.</p>\n\n<p>I found the php function 'similar_text' and i am sure, it is no problem to build a script to do this comparison during a loop.</p>\n\n<p>What would be the best and fastest way to do such a comparison?</p>\n"},{"tags":["python","regex","performance","python-2.6","substitution"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":85,"score":0,"question_id":12982805,"title":"Python Speeding Up Retrieving data from extremely large string","body":"<p>See Down farther:</p>\n\n<p>I have a list I converted to a very very long string as I am trying to edit it, as you can gather it's called tempString. It works as of now it just takes way to long to operate, probably because it is several different regex subs. They are as follow:</p>\n\n<pre><code>tempString = ','.join(str(n) for n in coords)\ntempString = re.sub(',{2,6}', '_', tempString)\ntempString = re.sub(\"[^0-9\\-\\.\\_]\", \",\", tempString)\ntempString = re.sub(',+', ',', tempString)\nclean1 = re.findall(('[-+]?[0-9]*\\.?[0-9]+,[-+]?[0-9]*\\.?[0-9]+,'\n                 '[-+]?[0-9]*\\.?[0-9]+'), tempString)\ntempString = '_'.join(str(n) for n in clean1)\ntempString = re.sub(',', ' ', tempString)\n</code></pre>\n\n<p>Basically it's a long string containing commas and about 1-5 million sets of 4 floats/ints (mixture of both possible),:</p>\n\n<pre><code>-5.65500020981,6.88999986649,-0.454999923706,1,,,-5.65500020981,6.95499992371,-0.454999923706,1,,,\n</code></pre>\n\n<p>The 4th number in each set I don't need/want, i'm essentially just trying to split the string into a list with 3 floats in each separated by a space.</p>\n\n<p>The above code works flawlessly but as you can imagine is quite time consuming on large strings.</p>\n\n<p>I have done a lot of research on here for a solution but they all seem geared towards words, i.e. swapping out one word for another.</p>\n\n<hr>\n\n<p><strong>EDIT:</strong>\nOk so this is the solution i'm currently using:</p>\n\n<pre><code>def getValues(s):\n    output = []\n    while s:\n        # get the three values you want, discard the 3 commas, and the \n        # remainder of the string\n        v1, v2, v3, _, _, _, s = s.split(',', 6)\n        output.append(\"%s %s %s\" % (v1.strip(), v2.strip(), v3.strip()))         \n    return output\ncoords = getValues(tempString)\n</code></pre>\n\n<p>Anyone have any advice to speed this up even farther? After running some tests It still takes much longer than i'm hoping for.</p>\n\n<p>I've been glancing at numPy, but I honestly have absolutely no idea how to the above with it, I understand that after the above has been done and the values are cleaned up i could use them more efficiently with numPy, but not sure how NumPy could apply to the above.</p>\n\n<p>The above to clean through 50k sets takes around 20 minutes, I cant imagine how long it would be on my full string of 1 million sets. I'ts just surprising that the program that originally exported the data took only around 30 secs for the 1 million sets</p>\n"},{"tags":["performance","views","multiple-databases","cross-join"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":12983389,"title":"Need some advice on best way to write Large reporting query from a fairly complex source","body":"<p>After searching through other answers to see if this particular question has already been asked (and other flavours have been asked - just not covering off everything I need to enquire about), I would like to pose the following scenario and ask for advise on the most efficient way to create this reporting query. This is a verbose post and I am not allowed to post any of the T-SQL code unfortunately - my employer has expressly forbid that. </p>\n\n<p>One thing I have learned over the years is that there are loads more people out there that know a heck of a lot more than me - so after searching and not finding what you need, just ask someone :)</p>\n\n<p>All tables have primary keys and if there are tables that link to child tables, there are foreign keys in place. The database and report server is SQL 2008 R2. The server runs 128GB RAM and is an 4CPU Quad core hyper threaded beastie.</p>\n\n<p>First, I have a set of tables that contain Locations, Sections and Areas. A Location is linked through a mapping table to a Section - there are multiple locations to a section, sections are linked to Areas through a mapping table and there are multiple sections to an Area. These table are all located in Database \"A\". I have created a view in a separate Database \"B\" that represents the Area - Section - Location linkages in a nice result set that executes quickly. I call this the LocationSectionArea View. </p>\n\n<p>In database \"B\", thousands of users are entering their daily statistics regarding all sorts of categories of their work. There are at present 91 categories. Each category will contain subcategories which vary from about 3 to as many as 25. The number of categories and subcategories can change at any time with more being added or removed through an administration interface. I have created a View that represents these links and returns a result set called the CategorySubCategoryFields view.</p>\n\n<p>The information being collected for each of these categories/Sub category links varies - some require about 8 different fields, while some require 3, and some only 1. These fields are linked to their appropriate sub categories. This is again contained in database \"B\".</p>\n\n<p>I have constructed a view that nicely pulls this data together in this database which ends up with a large matrix of results that presents the data in a way that can be reported on quite easily, and the views execution so far seems to be quite acceptable in speed. I call this the UserStatsView. This is contained in database \"B\".</p>\n\n<p>Now - the users entering the data are assigned to one of the locations mentioned earlier. The results are required to show all Locations within a Section, and all sections within an area, as well as for each location show each of the categories and sub categories for that category. This then needs to be linked to the results from the UserStatsView so you end up with a rather large matrix of these results intermingled with a lot of 0's where there are no results - but they are created for reporting purposes.</p>\n\n<p>The reports have to be allow a user to , for example, select a Section, and then produce a report of all Locations contained within that section, along with a tally of the results entered by users for that location - for each Category-SubCategory combination there is. It must present all locations for the section and all Categories-SubCategories regardless of if there are actual results entered by users or not. So there may be a lot of 0's on some of the reports.</p>\n\n<p>To achieve this I created a results view that cross joins the LocationSectionArea  with the CategorySubCategoryFields view. This created the basic matrix that I want. I then left join this matrix to the results of the UserStatsView, joining on the LocationId, CategoryId and SubCategoryId to insert the results for the users for the locations. This is all created in database \"B\".</p>\n\n<p>Now this all works and is \"ok\" provided you supply enough filters in the final where clause to reduce the number of records. As you can imaging - if they try to run a report with a larger results set - it starts to get very slow. (ie, date range covering 6 months for all locations takes longer than 30 seconds)</p>\n\n<p>Part of my problem is I believe that the Location information is linked in from database \"A\" and the view constructed is contained in database \"B\" along with all the views tables and records for all other information required for this report. </p>\n\n<p>Another part is I believe the very large and complex cross join created that is then linked to the view of users results. The cross join is a Cartesian result with no indexes or related data until it is joined with the UserStatsView result set.</p>\n\n<p>And the third problem I believe is the fact that data is being \"created\" to fill in all the blanks in order to produce the results structure that can then be fed into SSRS. I have seen that table spooling and hash joins take up a lot of the execution time, as well as lookups from the database with the locations, sections and Areas. These were all shown through the query execution estimated plan.</p>\n\n<p>What I am asking is if anyone knows if there is a better way of generating this result set given the criteria outlined above. Have I completely missed a really simple and faster way of doing this, which is highly probable :). If someone can suggest what I should research I will happily go off and do it - just not sure what to research at this stage?</p>\n\n<p>Cheers</p>\n\n<p>Rod.</p>\n\n<p>Update: the following is the table structure that defines the two databases,. Obviously there is more in the real tables, but this is the crux of how they hang together. Please excuse such a large post - I have removed everything except the key fields and some data fields so you can see how the tables are structured. The table names and field names have been changed to generic names with a lot of extra fields removed so I can post the code.</p>\n\n<hr>\n\n<p>Database A</p>\n\n<pre><code>**Location Table**\nLocationId BIGINT PK\nLocationName Varchar(100)\n\n\n**Section Table**\nSectionId Bigint PK\nSectionName varchar(100)\n\n\n\n**Area Table**\nAreaId Bigint PK\nAreaName varchar(100)\n\n\n\n**LocationSectionMap Table**\nLocSecId Bigint PK\nLocationId Bigint FK Index to Location Table\nSectionId BigInt FK Index to Section Table\n\n\n\n\n**SectionAreaMap Table**\nSecAreaId Bigint PK \nSectionId Bigint FK Index to Section Table\nAreaId Bigint FK Index to Area Table\n</code></pre>\n\n<hr>\n\n<p>Database B</p>\n\n<pre><code>**Categories Table** \nCategoryId Bigint PK\ncategoryName varchar(100)\n\n\n\n**SubCategories Table**\nSubCategoryId Bigint\nCategoryId Bigint FK to Categories Table \nSubCategoryType Int\nFieldTypeId Int (1, 2, 3 or 4)\n\n\n**UserStats Table**\nUserStatId bigint PK\nUserId Bigint\nStartDate DateTime\nEndDate DateTime\nLocationId Bigint --&gt; this is the location ID in Location table in database A\nSectionId Bigint --&gt; this is the Section ID in Section Table in database A\nAreaId Bigint --&gt; this is the Area ID of the Area Table in database A\n\n\n\n**FieldType1 Table**\nFieldType1Id bigint PK\nUserStatId  Bigint FK to UserStats Table\nSubCategoryId Bigint FK to Subcategories Table\nValue1 int\nValue2 int\n\n\n\n\n\n**FieldType2 Table**\nFieldType2Id bigint PK\nUserStatId  Bigint FK to UserStats Table\nSubCategoryId Bigint FK to Subcategories Table\nValue1 int \nValue2 int \nValue4 int\nValue5 int\nValue6 int\nValue7 int\nValue8 int\nValue9 int\nValue10 int\n\n\n**FieldType3 Table**\nFieldType3Id bigint PK\nUserStatId  Bigint FK to UserStats Table\nSubCategoryId Bigint FK to Subcategories Table\nValue1 int\nValue2 int\nValue11 int\nValue12 int\nValue13 int\nValue14 int\nValue15 int\nValue16 int\n\n\n**FieldType4 Table**\nFieldType4Id bigint PK\nUserStatId  Bigint FK to UserStats Table\nSubCategoryId Bigint FK to Subcategories Table\nCombinedValue1And2 int\n\n\n\n**SubCategoryAssociations Table**\nSubCategoryAssociationId Int PK\nReportOfSubCategoryId bigint FK to Subcategories Table\nIncludeValuesFromSubcategoryId Bigint FK to Subcategories Table\n</code></pre>\n\n<p>Views:\nArea-Section-Location View in Database A  (vwAreasSectionsAndLocations)</p>\n\n<pre><code>SELECT     A.AreaId, A.AreaName, C.SectionId, C.SectionName, E.LocationId, E.LocationName\nFROM         DatabaseA.dbo.tblAreas AS A INNER JOIN\n                      DatabaseA.dbo.tblAreaSections AS B ON A.AreaId = B.AreaId INNER JOIN\n                      DatabaseA.dbo.tblSections AS C ON B.SectionId = C.SectionId INNER JOIN\n                      DatabaseA.dbo.tblSectionLocations AS D ON C.SectionId = D.SectionId INNER JOIN\n                      DatabaseA.dbo.tblLocations AS E ON D.LocationId = E.LocationId\n</code></pre>\n\n<p>Category-SubCategory View in Database B   (vwCategoryAndSubCategory)</p>\n\n<pre><code>SELECT     TOP (100) PERCENT dbo.tblCategories.CategoryId, \n          dbo.tblCategories.CategoryName, \n                      dbo.tblSubCategory.SubCategoryId, \n                      dbo.tblSubCategory.SubCategoryname, \n              dbo.tblSubCategory.SubCategoryTypeId\nFROM         dbo.tblCategories INNER JOIN\n                      dbo.tblSubCategory ON dbo.tblCategories.CategoryId = dbo.tblSubCategory.CategoryId\nORDER BY dbo.tblCategories.CategoryName, dbo.tblSubCategory.SubCategoryname\n</code></pre>\n\n<p>View that Cross Joins the two views to create the required results matrix (without Results)  (vwAreaSectionLocationCategorySubCategory)</p>\n\n<pre><code>SELECT     dbo.vwCategoryAndSubCategory.CategoryId, dbo.vwCategoryAndSubCategory.CategoryName, \n                      dbo.vwCategoryAndSubCategory.CategoryPlacementOrder, dbo.vwCategoryAndSubCategory.IsStandardDaybookEntryCategory, \n                      dbo.vwCategoryAndSubCategory.SubCategoryId, dbo.vwCategoryAndSubCategory.SubCategoryname, dbo.vwCategoryAndSubCategory.SubCategoryTypeId, \n                      dbo.vwCategoryAndSubCategory.SubCategoryPlacementOrder, dbo.vwAreasSectionsAndLocations.AreaId, dbo.vwAreasSectionsAndLocations.AreaName, \n                      dbo.vwAreasSectionsAndLocations.SectionId, dbo.vwAreasSectionsAndLocations.SectionName, dbo.vwAreasSectionsAndLocations.LocationId, \n                      dbo.vwAreasSectionsAndLocations.LocationName\nFROM         dbo.vwCategoryAndSubCategory CROSS JOIN\n                      dbo.vwAreasSectionsAndLocations\n</code></pre>\n\n<p>View that Creates the Use Stats Results  (vwUserStatsResults)</p>\n\n<pre><code>SELECT      dbo.tblUserStats.UserStatId, \n    dbo.tblUserStats.UserId, \n    dbo.tblUserStats.LocationId, \n        dbo.tblUserStats.SectionId, \n    dbo.tblUserStats.AreaId,                       \n            dbo.tblUserStats.StartDate, \n    dbo.tblUserStats.EndDate, \n    dbo.tblFieldType1.CategoryId, \n    dbo.tblFieldType1.SubCategoryId, \n            1 AS FieldTypeId, \n    (dbo.tblFieldType1.Value1 + ISNULL\n                      ((SELECT     SUM(A.Value1)\n                          FROM         dbo.tblFieldType1 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType1.UserStatId = A.UserStatId), 0) \n                  + ISNULL\n                      ((SELECT     SUM(A.Value1)\n                          FROM         dbo.tblFieldType2 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType1.UserStatId = A.UserStatId), 0) \n                  + ISNULL\n                      ((SELECT     SUM(A.Value1)\n                          FROM         dbo.tblFieldType3 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType1.UserStatId = A.UserStatId), 0)) \n                  AS Value1, (dbo.tblFieldType1.Value2 + ISNULL\n                      ((SELECT     SUM(A.Value2)\n                          FROM         dbo.tblFieldType1 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType1.UserStatId = A.UserStatId), 0) \n                  + ISNULL\n                      ((SELECT     SUM(A.Value2)\n                          FROM         dbo.tblFieldType2 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType1.UserStatId = A.UserStatId), 0) \n                  + ISNULL\n                      ((SELECT     SUM(A.Value2)\n                          FROM         dbo.tblFieldType3 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType1.UserStatId = A.UserStatId), 0)) \n                  AS Value2, Value1 + Value2 AS CombinedValue1And2, 0 AS Value3, 0 AS Value4, \n                  0 AS Value5, 0 AS Value6, 0 AS Value7, 0 AS Value8, \n                  0 AS Value9, 0 AS Value10, 0 AS Value11, 0 AS Value12, 0 AS Value13, \n                  0 AS Value14, 0 AS Value15, 0 AS Value16\nFROM         dbo.tblUserStats INNER JOIN\n                  dbo.tblFieldType1 ON dbo.tblUserStats.UserStatId = dbo.tblFieldType1.UserStatId \n\nUNION ALL\n\nSELECT      dbo.tblUserStats.UserStatId, \n    dbo.tblUserStats.UserId, \n    dbo.tblUserStats.LocationId, \n            dbo.tblUserStats.SectionId, \n    dbo.tblUserStats.AreaId, \n            dbo.tblUserStats.StartDate, \n    dbo.tblUserStats.EndDate, \n    dbo.tblFieldType2.CategoryId, \n    dbo.tblFieldType2.SubCategoryId, \n            2 AS FieldTypeId, \n    dbo.tblFieldType2.Value1 + ISNULL\n                      ((SELECT     SUM(Value1) AS Expr1\n                          FROM         dbo.tblFieldType1 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType2.UserStatId = UserStatId)), 0) \n                  + ISNULL\n                      ((SELECT     SUM(Value1) AS Expr1\n                          FROM         dbo.tblFieldType2 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType2.UserStatId = UserStatId)), 0) \n                  + ISNULL\n                      ((SELECT     SUM(Value1) AS Expr1\n                          FROM         dbo.tblFieldType3 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType2.UserStatId = UserStatId)), 0) \n                  AS Value1, dbo.tblFieldType2.Value2 + ISNULL\n                      ((SELECT     SUM(Value2) AS Expr1\n                          FROM         dbo.tblFieldType1 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType2.UserStatId = UserStatId)), 0) \n                  + ISNULL\n                      ((SELECT     SUM(Value2) AS Expr1\n                          FROM         dbo.tblFieldType2 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType2.UserStatId = UserStatId)), 0) \n                  + ISNULL\n                      ((SELECT     SUM(Value2) AS Expr1\n                          FROM         dbo.tblFieldType3 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType2.UserStatId = UserStatId)), 0) \n                  AS Value2, dbo.tblFieldType2.Value1 + dbo.tblFieldType2.Value2 AS CombinedValue1And2, \n                  ISNULL(dbo.tblFieldType2.Value3, 0), ISNULL(dbo.tblFieldType2.Value4, 0), \n                  ISNULL(dbo.tblFieldType2.Value5, 0), ISNULL(dbo.tblFieldType2.Value6, 0), \n                  ISNULL(dbo.tblFieldType2.Value7, 0), ISNULL(dbo.tblFieldType2.Value8, 0), \n                  ISNULL(dbo.tblFieldType2.Value9, 0), ISNULL(dbo.tblFieldType2.Value10, 0), \n                  0 AS Value11, 0 AS Value12, 0 AS Value13, 0 AS Value14, 0 AS Value15, \n                  0 AS Value16\nFROM         dbo.tblUserStats INNER JOIN\n                  dbo.tblFieldType2 ON dbo.tblUserStats.UserStatId = dbo.tblFieldType2.UserStatId \nUNION ALL\nSELECT      dbo.tblUserStats.UserStatId, \n    dbo.tblUserStats.UserId, \n    dbo.tblUserStats.LocationId, \n    dbo.tblUserStats.SectionId, \n    dbo.tblUserStats.AreaId, \n            dbo.tblUserStats.StartDate, \n    dbo.tblUserStats.EndDate, \n    dbo.tblFieldType3.CategoryId, \n    dbo.tblFieldType3.SubCategoryId, \n            3 AS FieldTypeId, \n    dbo.tblFieldType3.Value1 + ISNULL\n                      ((SELECT     SUM(Value1) AS Expr1\n                          FROM         dbo.tblFieldType1 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType3.UserStatId = UserStatId)), 0) \n                  + ISNULL\n                      ((SELECT     SUM(Value1) AS Expr1\n                          FROM         dbo.tblFieldType2 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType3.UserStatId = UserStatId)), 0) \n                  + ISNULL\n                      ((SELECT     SUM(Value1) AS Expr1\n                          FROM         dbo.tblFieldType3 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType3.UserStatId = UserStatId)), 0) \n                  AS Value1, dbo.tblFieldType3.Value2 + ISNULL\n                      ((SELECT     SUM(Value2) AS Expr1\n                          FROM         dbo.tblFieldType1 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType3.UserStatId = UserStatId)), 0) \n                  + ISNULL\n                      ((SELECT     SUM(Value2) AS Expr1\n                          FROM         dbo.tblFieldType2 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType3.UserStatId = UserStatId)), 0) \n                  + ISNULL\n                      ((SELECT     SUM(Value2) AS Expr1\n                          FROM         dbo.tblFieldType3 AS A\n                          WHERE     (FieldId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      (ReportOnSubCategoryId = A.SubCategoryId))) AND (dbo.tblFieldType3.UserStatId = UserStatId)), 0) \n                  AS Value2, dbo.tblFieldType3.Value1 + dbo.tblFieldType3.Value2 AS CombinedValue1And2, 0 AS Value3, \n                  0 AS Value4, 0 AS Value5, 0 AS Value6, 0 AS Value7, \n                  0 AS Value8, 0 AS Value9, 0 AS Value10, \n                  ISNULL(dbo.tblFieldType3.Value11, 0), ISNULL(dbo.tblFieldType3.Value12, 0), \n                  ISNULL(dbo.tblFieldType3.Value13, 0), ISNULL(dbo.tblFieldType3.Value14, 0), \n                  ISNULL(dbo.tblFieldType3.Value15, 0), ISNULL(dbo.tblFieldType3.Value16, 0)\nFROM         dbo.tblUserStats INNER JOIN\n                  dbo.tblFieldType3 ON dbo.tblUserStats.UserStatId = dbo.tblFieldType3.UserStatId \nUNION ALL\nSELECT     dbo.tblUserStats.UserStatId, \n    dbo.tblUserStats.UserId, \n    dbo.tblUserStats.LocationId, \n            dbo.tblUserStats.SectionId, \n    dbo.tblUserStats.AreaId, \n            dbo.tblUserStats.StartDate, \n    dbo.tblUserStats.EndDate, \n    dbo.tblFieldType4.CategoryId, \n    dbo.tblFieldType4.SubCategoryId, \n            4 AS FieldTypeId, \n    0 AS Value1, 0 AS Value2, dbo.tblFieldType4.CombinedValue1And2 + (ISNULL\n                      ((SELECT     SUM(A.Value1)\n                          FROM         dbo.tblFieldType1 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType4.UserStatId = A.UserStatId), 0) \n                  + ISNULL\n                      ((SELECT     SUM(A.Value1)\n                          FROM         dbo.tblFieldType2 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType4.UserStatId = A.UserStatId), 0) \n                  + ISNULL\n                      ((SELECT     SUM(A.Value1)\n                          FROM         dbo.tblFieldType3 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType4.UserStatId = A.UserStatId), 0)) \n                  + (ISNULL\n                      ((SELECT     SUM(A.Value2)\n                          FROM         dbo.tblFieldType1 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType4.UserStatId = A.UserStatId), 0) \n                  + ISNULL\n                      ((SELECT     SUM(A.Value2)\n                          FROM         dbo.tblFieldType2 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType4.UserStatId = A.UserStatId), 0) \n                  + ISNULL\n                      ((SELECT     SUM(A.Value2)\n                          FROM         dbo.tblFieldType3 A\n                          WHERE     A.SubCategoryId IN\n                                                    (SELECT DISTINCT IncludeValuesFromSubCategoryId\n                                                      FROM          dbo.tblSubCategoryAssociations\n                                                      WHERE      ReportOnSubCategoryId = A.SubCategoryId) AND dbo.tblFieldType4.UserStatId = A.UserStatId), 0)) \n                  AS CombinedValue1And2, 0 AS Value3, 0 AS Value4, 0 AS Value5, 0 AS Value6, \n                  0 AS Value7, 0 AS Value8, 0 AS Value9, 0 AS Value10, \n                  0 AS Value11, 0 AS Value12, 0 AS Value13, 0 AS Value14, 0 AS Value15, \n                  0 AS Value16\nFROM         dbo.tblUserStats INNER JOIN\n                  dbo.tblFieldType4 ON dbo.tblUserStats.UserStatId = dbo.tblFieldType4.UserStatId \n</code></pre>\n\n<p>And finally the view that pulls it all together   (vwReportResults)</p>\n\n<p>Posted in another entry due to character limitations</p>\n"},{"tags":["performance","jmeter","load-testing","visualvm","jvisualvm"],"answer_count":5,"favorite_count":7,"up_vote_count":5,"down_vote_count":0,"view_count":3518,"score":5,"question_id":5645393,"title":"How to do load testing using jmeter and visualVM?","body":"<p>I want to do load testing for 10 million users for my site. The site is a Java based web-app. My approach is to create a Jmeter test plan for all the links and then take a report for the 10 million users. Then use jvisualVM to do profiling and check if there are any bottlenecks.</p>\n\n<p>Is there any better way to do this? Is there any existing demo for doing this? I am doing this for the first time, so any assistance will be very helpful.</p>\n"},{"tags":["c","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":94,"score":0,"question_id":12850912,"title":"Performance down","body":"<p>Can anyone help me with this?</p>\n\n<pre><code>void _vect_mat(float *vect,float **mat){\n  float temp[4];\n\n  temp[0] = vect[0];\n  temp[1] = vect[1];\n  temp[2] = vect[2];\n  temp[3] = vect[3];\n\n  vect[0] = (temp[0] * mat[0][0]) + (temp[1] * mat[1][0]) + (temp[2] * mat[2][0]) + (temp[3] * mat[3][0]);\n  vect[1] = (temp[0] * mat[0][1]) + (temp[1] * mat[1][1]) + (temp[2] * mat[2][1]) + (temp[3] * mat[3][1]);\n  vect[2] = (temp[0] * mat[0][2]) + (temp[1] * mat[1][2]) + (temp[2] * mat[2][2]) + (temp[3] * mat[3][2]);\n  vect[3] = (temp[0] * mat[0][3]) + (temp[1] * mat[1][3]) + (temp[2] * mat[2][3]) + (temp[3] * mat[3][3]);\n}\n\nint main(){\n  int i,j,k;\n\n  float *vect,**mat;\n\n  vect =  (float *)malloc(4 * sizeof(float));\n  mat  = (float **)malloc(4 * sizeof(float *);\n  for(i=0;i&lt;4;i++)mat[i] = (float *)malloc(4 * sizeof(float));\n\n  vect[0] = 1.0;\n  vect[n] = ......etc.\n\n  mat[0][0] = 1.0;\n  mat[n][m] ......etc.     \n\n  while (1){\n    for(i = 0;i &lt; 5;i++){\n      for(j = 0;j&lt; 6;j++){\n        for(k = 0;k &lt; 3600;k++)_vect_mat(vect,mat); \n      }\n    }\n  }\n}\n</code></pre>\n\n<p>when I call the function <code>_vect_mat</code> inside the loop, all performance falls. It is Normal?\nWhat am I doing wrong?</p>\n"},{"tags":["php","arrays","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":50,"score":1,"question_id":12995025,"title":"Is there any advantage in reusing arrays instead of creating new ones?","body":"<p>Is it good practice to try to save a \"generic\" array in the $_SESSION array for the purpose of applyng e.g PHP sorting functions instead of calling a new result set from the server? Seems abit overkill to e.g call a new result set if I want to sort comments based on likes instead of timestamp. Maybe there is another best practice for this \"problem\"? How much efficiency is there to be gained working like this? Is it very much?</p>\n"},{"tags":["java","string","performance","indexof"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":75,"score":1,"question_id":12995895,"title":"Java: Overhead in executing StringBuilder.indexOf() that concatenates a variable and literal","body":"<p>Is there a performance overhead in executing <code>sb.indexOf(c + \"\")</code> \nwhere <code>c</code> is of type <code>Character</code> or <code>char</code> and <code>sb</code> is <code>StringBuilder</code> object?</p>\n"},{"tags":["java","performance","hashset"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":68,"score":0,"question_id":12973867,"title":"Java: Comparing hashsets as efficently as possible","body":"<p>I have 3 hashsets. goodLinkSet, badLinkSet and testLinkSet.</p>\n\n<p>goodLinkSet holds a list of URLs that work and badLinkSet holds a list of URLs that don't work. testLinkSet holds a list of URLs that I need to check if they are good are bad, some of the links in here have been tested already in the other two sets. </p>\n\n<p>What I want to do is remove all the strings/links in testLinkSet that appear in goodLinkSet and badLinkSet so I'm not testing URLs multiple times. I want to do this as efficiently and as fast as possible. A for each loop seems to be a bit slow. </p>\n\n<p>What's the most efficient way of running this? Are there any functions that does this for me? Any advice would be very much appreciated!</p>\n"},{"tags":["java","string","performance","collections"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":103,"score":1,"question_id":12995617,"title":"How can I improve performance of string processing with less memory?","body":"<p>I'm implementing this in Java.</p>\n\n<pre><code>Symbol file     Store data file\n\n1\\item1         10\\storename1\n10\\item20       15\\storename6\n11\\item6        15\\storename9\n15\\item14       1\\storename250\n5\\item5         1\\storename15\n</code></pre>\n\n<p>The user will search store names using wildcards like <code>storename?</code>\nMy job is to search the store names and produce a full string using symbol data. For example:</p>\n\n<p>item20-storename1<br>\nitem14-storename6<br>\nitem14-storename9  </p>\n\n<p>My approach is:</p>\n\n<ol>\n<li>reading the store data file line by line</li>\n<li>if any line contains matching search string (like <code>storename?</code>), I will push that line to an intermediate store result file</li>\n<li>I will also copy the itemno of a matching storename into an arraylist (like 10,15)</li>\n<li>when this arraylist size%100==0 then I will remove duplicate item no's using hashset,  reducing arraylist size significantly</li>\n<li><p>when arraylist size >1000</p>\n\n<ol>\n<li>sort that list using <code>Collections.sort(itemno_arraylist)</code></li>\n<li>open symbol file &amp; start reading line by line</li>\n<li>for each line <code>Collections.binarySearch(itemno_arraylist,itmeno)</code></li>\n<li>if matching then push result to an intermediate symbol result file</li>\n</ol></li>\n<li><p>continue with step1 until EOF of store data file</p></li>\n</ol>\n\n<p>...</p>\n\n<p>After all of this I would combine two result files (symbol result file &amp; store result file) to present actual strings list.</p>\n\n<p>This approach is working but it is consuming more CPU time and main memory.</p>\n\n<p>I want to know a better solution with reduced CPU time (currently 2 min) &amp; memory (currently 80MB). There are many collection classes available in Java. Which one would give a more efficient solution for this kind of huge string processing problem?</p>\n\n<p>If you have any thoughts on this kind of string processing problems that too in Java would be great and helpful.</p>\n\n<p>Note: Both files would be nearly a million lines long.</p>\n"},{"tags":["c++","performance","g++","switch-statement"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":1,"view_count":83,"score":3,"question_id":12992108,"title":"\"crosses initialization of variable\" only when initialization combined with declaration","body":"<p>I've read <a href=\"http://stackoverflow.com/questions/5685471/error-jump-to-case-label\">this question</a> about the \"jump to case label\" error, but I still have some questions. I'm using g++ 4.7 on Ubuntu 12.04.</p>\n\n<p>This code gives an error:</p>\n\n<pre><code>int main() {\n  int foo = 1;\n  switch(foo) {\n  case 1:\n    int i = 0;\n    i++;\n    break;\n  case 2:\n    i++;\n    break;\n  }\n}\n</code></pre>\n\n<p>The error is </p>\n\n<pre><code>jump-to-case-label.cpp: In function ‘int main()’:\njump-to-case-label.cpp:8:8: error: jump to case label [-fpermissive]\njump-to-case-label.cpp:5:9: error:   crosses initialization of ‘int i’\n</code></pre>\n\n<p>However, this code compiles fine,</p>\n\n<pre><code>int main() {\n  int foo = 1;\n  switch(foo) {\n  case 1:\n    int i;\n    i = 0;\n    i++;\n    break;\n  case 2:\n    i++;\n    break;\n  }\n}\n</code></pre>\n\n<p>Is the second code any less dangerous than the first? I'm confused as to why g++ allows it.</p>\n\n<p>Secondly, the fix for this problem is to scope the initialized variable. If the initialized variable is a large object, and the switch statement is in a while loop, won't the constructor and destructor be called each time that scope is entered and left, causing a decrease in efficiency? Or will the compiler optimize this away?</p>\n"},{"tags":["sql-server","performance","sql-server-2008","indexing"],"answer_count":5,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":951,"score":0,"question_id":488258,"title":"Adding a index on my table for this query","body":"<p>This table gets hit with this query the most, so I want add a index to speed things up, this table will have 5 million rows it in.</p>\n\n<p>My query looks like this:</p>\n\n<p>SELECT someID \nFROM someTable \nWHERE \n    myVarChar = @myVarChar AND\n    MyBit = 0 AND MyBit2 = 1 AND MyBit3 = 0</p>\n\n<p>myVarChar is unique also.</p>\n\n<p>What would the best index be for this type of query?</p>\n\n<p>I currently have a single index that covers all of the 4 columns in the above query.</p>\n\n<p>I am using sql server 2008 standard.</p>\n\n<p>Do I have to reindex every so often or its automatic?</p>\n"},{"tags":["java","performance","stringbuilder"],"answer_count":2,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":50,"score":3,"question_id":12994374,"title":"Efficient design for a text-based file that is edited at arbitrary positions?","body":"<p>I'm attempting to develop a simple online editor that allows for real-time collaboration (written in Java). In this editor, I want clients to be able to edit the source code at arbitrary points (e.g. add the letter 'd' to the source code file at row 11, column 20). I'm not sure how to design these source code file objects in an efficient way, while still allowing for letter-by-letter client-server synchronization (similar to how Google Docs works).</p>\n\n<p>I considered using a RandomAccessFile, but after reading <a href=\"http://stackoverflow.com/questions/5786697/how-to-write-content-in-a-specific-position-in-a-file\" title=\"this post\">this post</a>, I don't think that would be an efficient approach. Inserting a letter near the beginning of the file would involve changing everything after it.</p>\n\n<p>My current plan is to represent both the source files on the server and client using a StringBuilder object and its insert/delete/append methods. On the server-side, this StringBuilder would be converted to an actual file as necessary.</p>\n\n<p>I'm curious as to whether there might be a better approach for solving this problem. Any ideas?</p>\n"},{"tags":["sql","performance","mongodb","nosql","document"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":366,"score":3,"question_id":4482972,"title":"Sql database versus document database?","body":"<p>Small introduction:</p>\n\n<ol>\n<li><p>I've tried to develop my project with sql database, entity framework, linq. \nI have table 'Users' and for example i have list of user educations. I ask myself: 'How much educations user can have?  1, 2, 10...' ? And seems at 99% of cases not more than 10. So for such example in sql i need to create referenced table 'Educations'. Right? If i need to display user educations and user i need to join above mentioned tables... But what if user have 10 or even more collections with not more than 10 items in each? I need to create 10 referenced tables in sql? And than join all of them when i need to display? For better performance i've created denormized tables with shape of data that i need to show on ui. And every time when user was updated, i need to update denormilized structure.   </p></li>\n<li><p>Now i redeveloped my project to use document database(<a href=\"http://www.mongodb.org/\" rel=\"nofollow\">MongoDB</a>). And i've created one document for User with all 10 collections inside.</p></li>\n</ol>\n\n<p>May be i've lost something? But seems document database win here. It's very fast and very easy to support. +1 to document database.</p>\n\n<p>So, what is your opinion about what better to use document database or sql database? </p>\n\n<p>When I should use document database and when sql?</p>\n"},{"tags":["ruby-on-rails","performance","optimization","amazon-s3","paperclip"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":63,"score":3,"question_id":12993954,"title":"How to Add Expires headers for amazon s3 images?","body":"<p>In my model, which is using paperclip. I have added <a href=\"http://webonrails.com/2012/06/06/set-cache-control-expires-headers-to-the-content-served-by-s3-using-attachment_fu-or-paperclip/\" rel=\"nofollow\">this</a> line of code but still its not working</p>\n\n<pre><code> has_attached_file :attachment, {\n    :styles =&gt; {\n      :medium =&gt; [\"654x5000000&gt;\", :jpg],\n      :small =&gt; [\"260x50000000&gt;\", :jpg], \n      :thumb =&gt; [\"75x75#\", :jpg],\n      :facebook_meta_tag =&gt;[\"200x200#\", :jpg] \n    },\n    :convert_options =&gt; {\n       :medium =&gt; \"-quality 80 -interlace Plane\",\n       :small =&gt; \"-quality 80 -interlace Plane\",\n       :thumb =&gt; \"-quality 80 -interlace Plane\",\n       :facebook_meta_tag =&gt; \"-quality 80 -interlace Plane\" \n       },\n       :s3_headers =&gt; { 'Cache-Control' =&gt; 'max-age=315576000', 'Expires' =&gt; 10.years.from_now.httpdate } \n    }.merge(PAPERCLIP_STORAGE_OPTIONS)\n</code></pre>\n\n<p>PS: I tested it on <a href=\"http://gtmetrix.com\" rel=\"nofollow\">GTmetrix.com</a> and as per their stats, expiry headers are not there in amazon images.</p>\n"},{"tags":["c#",".net","visual-studio-2010","performance","visual-studio"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":34,"score":2,"question_id":12993112,"title":"Debugging very slow after adding persistent settings","body":"<p>I have a semi-small visual C# solution I am working on in Visual Studio 2010. I recently added in Persistent settings using the <code>settings.Settings</code> file and <code>Properties.Settings.Default</code>. On the Form Load event, I am checking the value using <code>Properties.Settings.Default</code> and am assigning them to a <code>checkbox.Checked</code> variable.</p>\n\n<pre><code>//Load Form1\nprivate void Form1_Load(object sender, System.EventArgs e)\n{\n    cbxShowPass.Checked = Properties.Settings.Default.showFullPassword;\n    checkBox2.Checked = Properties.Settings.Default.showBookmarkFiles;\n}\n</code></pre>\n\n<p>When I start my program in Debug mode, it is extremely slow to start, however, when I remove that line of code, it will start quickly.</p>\n\n<p>What can I do to make my program start up quickly in Debug mode without removing the Settings?</p>\n\n<p>Here are my CheckBox_Changed events, where I assign the value to the Settings.</p>\n\n<pre><code>private void ShowPass_CheckChanged(object sender, EventArgs e)\n{\n    Properties.Settings.Default.showFullPassword = !Properties.Settings.Default.showFullPassword;\n        Properties.Settings.Default.Save();\n        DisplayPassword();\n}\n</code></pre>\n"},{"tags":["python","performance","if-statement","coding-style"],"answer_count":7,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":225,"score":7,"question_id":12680109,"title":"Performance or style difference between \"if\" and \"if not\"?","body":"<p>Is there a performance difference or style preference between these two ways of writing if statements?  It is basically the same thing, the 1 condition will be met only once while the other condition will be met every other time.  Should the condition that is met only once be first or second?  Does it make a difference performance wise?   I prefer the 1st way if the the performance is the same.</p>\n\n<pre><code>data = range[0,1023]\nlength = len(data)\nmax_chunk = 10\n\nfor offset in xrange(0,length,max_chunk):\n    chunk = min(max_chunk,length-offset)\n    if chunk &lt; max_chunk:\n        write_data(data[offset:])\n    else:\n        write_data(data[offset:offset+max_chunk])\n</code></pre>\n\n<p>vs</p>\n\n<pre><code>data = range[0,1023]\nlength = len(data)\nmax_chunk = 10\n\nfor offset in xrange(0,length,max_chunk):\n    chunk = min(max_chunk,length-offset)\n    if not chunk &lt; max_chunk:\n        write_data(data[offset:offset+max_chunk])\n    else:\n        write_data(data[offset:])\n</code></pre>\n"},{"tags":["java","performance","optimization","bit-manipulation"],"answer_count":4,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":463,"score":7,"question_id":9825319,"title":"What's the fastest way to do a right circular bit shift on a byte array","body":"<p>If I have the array:</p>\n\n<pre><code>{01101111,11110000,00001111} // {111, 240, 15}\n</code></pre>\n\n<p>The result for a 1 bit shift is:</p>\n\n<pre><code>{10110111,11111000,00000111} // {183, 248, 7}\n</code></pre>\n\n<p>The array size is not fixed, and the shifting will be from 1 to 7 inclusive. Currently I have the following code (which works fine):</p>\n\n<pre><code>private static void shiftBitsRight(byte[] bytes, final int rightShifts) {\n   assert rightShifts &gt;= 1 &amp;&amp; rightShifts &lt;= 7;\n\n   final int leftShifts = 8 - rightShifts;\n\n   byte previousByte = bytes[0]; // keep the byte before modification\n   bytes[0] = (byte) (((bytes[0] &amp; 0xff) &gt;&gt; rightShifts) | ((bytes[bytes.length - 1] &amp; 0xff) &lt;&lt; leftShifts));\n   for (int i = 1; i &lt; bytes.length; i++) {\n      byte tmp = bytes[i];\n      bytes[i] = (byte) (((bytes[i] &amp; 0xff) &gt;&gt; rightShifts) | ((previousByte &amp; 0xff) &lt;&lt; leftShifts));\n      previousByte = tmp;\n   }\n}\n</code></pre>\n\n<p>Is there a faster way to achieve this than my current approach?</p>\n"},{"tags":["android","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":515,"score":4,"question_id":3048637,"title":"Performance of the Android Virtual Device","body":"<p>The Android virtual device (a simulated Android environment) doesn't run very smoothly on my machine. Scrolling is quite sluggish.</p>\n\n<p>Is that normal?</p>\n\n<p>EDIT: Just noticed that a AVD running Android 1.6 has a significantly better performance compared to the AVDs running on 2.1 and 2.2.</p>\n"},{"tags":["php","mysql","performance","for-loop"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":71,"score":0,"question_id":12992793,"title":"Efficient ways to calculate \"ranks\" for over 350,000 users","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/3333665/mysql-rank-function\">Mysql rank function</a>  </p>\n</blockquote>\n\n\n\n<p>I am making a rank website and each user will have a unique rank based on level. #1 is the best rank and since there's 350,000 users, the worst rank will be #350000.</p>\n\n<p>No user can have the same rank as anyone else. Now, when a new user is added, their level is calculated. After calculation a script will \"rebuild\" the ranks, by going through each user one by one, and calculating their new rank.</p>\n\n<p>Here's an explanation of the queries:</p>\n\n<ul>\n<li>\"Id\" is the member's ID in the database</li>\n<li>\"RankNum\" is their level. It needs to be renamed.</li>\n<li>\"Rank\" is their unique rank, #1 to #350000.</li>\n</ul>\n\n<p>Here's my current script:</p>\n\n<pre><code>function RebuildRanks() {\n    $qD = mysql_query(\"SELECT `Id`,`RankNum` FROM `Members` ORDER BY `Rank` DESC, `Id` ASC\");\n    $rowsD = mysql_num_rows($qD);\n\n    $curRank = 0;\n\n    for($x = 1; $x &lt;= $rowsD; $x++) {\n        $rowD = mysql_fetch_array($qD);\n\n        $curRank++;\n        if($rowD['RankNum'] != $curRank) {\n            if($curRank != 0) {\n                mysql_query(\"UPDATE `Members` SET `RankNum`='$curRank' WHERE `Id`='\".$rowD['Id'].\"'\");\n            }\n        }\n    }\n\n    return true;\n}\n</code></pre>\n\n<p>With 350,000 users, this tends to run very slow. Essentially, in the database the <strong>Rank</strong> (\"ORDER BY `Rank` DESC\") is their level, so the query will order them. Unfortunately, the rest of the process is slow. </p>\n\n<p>It takes about 97 seconds to process all 350,000 users this way. Is there any possible solution to running this more efficiently and quickly?</p>\n"},{"tags":["iphone","performance","ios","viewdidload"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":414,"score":0,"question_id":4963745,"title":"Switching Tabs (initially) very slow - Move Function out of viewDidLoad?","body":"<p>In my iPhone app, I have three tabs laid out using a <code>UITabBarController</code>. The first tab (that loads on app launch) uses local data to load, and is very fast.</p>\n\n<p>The second tab, though, which downloads an XML file from the web and parses it, then displays all the data in a <code>UITableView</code>, takes a long time to load over slower connections (EDGE, 3G). And, since I do call my parser inside <code>viewDidLoad</code>, the app won't switch to my second tab until everything is done—this means it takes a while to load the tab sometimes, and it looks like the app's locked up.</p>\n\n<p>I'd rather be able to have the user switch to that tab, have the view load immediately—even if empty, and then have the data downloaded/parsed/displayed. I have the network activity spinner spinning, so at least the user can know something's happening.</p>\n\n<p>Here's my current <code>viewDidLoad</code>:</p>\n\n<pre><code>// Load in the latest stories when the app is launched.\n- (void)viewDidLoad {\n    [super viewDidLoad];\n    NSLog(@\"Loading news view\");\n\n    articles = [[NSMutableArray alloc] init];\n\n    NSURL *url = [NSURL URLWithString:@\"http://example.com/mobile-app/latest-news.xml\"];\n    NSLog(@\"About to parse URL: %@\", url);\n    parser = [[NSXMLParser alloc] initWithContentsOfURL:url];\n    parser.delegate = self;\n    [parser parse];\n}\n</code></pre>\n\n<p>I found <a href=\"http://www.chrisumbel.com/article/asynchronous_iphone_nsthread_nstimer\" rel=\"nofollow\">this article</a>, which shows how to run threads in the background, but I tried implementing that code and couldn't get the background thread to load the data back into my UITableView - the code was called, but how would I make sure the parsed articles are loaded back into my table view?</p>\n"},{"tags":["java","performance","multithreading"],"answer_count":7,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":221,"score":2,"question_id":878035,"title":"Why don't multiple threads seem to speed up my web application?","body":"<pre><code>class ApplicationContext{\n    private final NetworkObject networkObject = new networkObject();\n\n    public ApplicationContext(){\n      networkObject.setHost(\"host\");\n      networkObject.setParams(\"param\");\n    }\n\n    public searchObjects(ObjectType objType){\n        networkObject.doSearch(buildQuery(objType));\n    }\n}\n\nclass NetworkObject{\n    private final SearchObject searchObject = new SearchObject();\n\n    public doSearch(SearchQuery searchQuery){\n        searchObject.search(searchQuery); //threadsafe, takes 15(s) to return\n    }\n}\n</code></pre>\n\n<p>Consider a webserver running a web application which creates only one ApplicationContext instance (singleton) and uses the same applicationInstance to call searchObjects e.g.</p>\n\n<pre><code> ApplicationContext appInstance = \n                  ApplicationContextFactory.Instance(); //singleton\n</code></pre>\n\n<p>Every new request to a webpage say 'search.jsp' makes a call </p>\n\n<pre><code> appInstance.searchObjects(objectType);\n</code></pre>\n\n<p>I am making 1000 requests to 'search.jsp' page. All the threads are using the same ApplicationContext instance, and searchObject.search() method takes 15 seconds to return. My Question is Do all other threads wait for their turn (15 sec) to execute when one is already executing the searchObject.search() function or All threads will execute searchObject.search() concurrently, Why??</p>\n\n<p>I hope I have made my question very clear??</p>\n\n<p><strong>Update:</strong>\nThanks all for clarifying my doubt. Here is my second Question, what difference in performance should be observe when I do:</p>\n\n<pre><code>public synchronized doSearch(SearchQuery searchQuery){\n    searchObject.search(searchQuery); //threadsafe, takes 15(s) to return\n}\n</code></pre>\n\n<p><strong>OR</strong></p>\n\n<pre><code>public doSearch(SearchQuery searchQuery){\n    searchObject.search(searchQuery); //threadsafe, takes 15(s) to return\n}\n</code></pre>\n\n<p>I believe using the function 'doSearch' without synchronized keyword should be giving more performance. But, when I tested it today, the results came out the other way. The performance was similar or sometimes better when I use synchronized keyword.</p>\n\n<p>Can anyone explain the behavior. How should I debug such cases.</p>\n\n<p>Regards,</p>\n\n<p>Perry</p>\n"},{"tags":["ios","performance","interpolation"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":12992157,"title":"Fast bilinear interpolation on old iOS devices","body":"<p>I've got the following code to do a biliner interpolation from a matrix of 2D vectors, each cell has x and y values of the vector, and the function receives k and l indices telling the bottom-left nearest position in the matrix</p>\n\n<pre><code>// p[1]                     returns the interpolated values\n// fieldLinePointsVerts     the raw data array of fieldNumHorizontalPoints x fieldNumVerticalPoints\n//                          only fieldNumHorizontalPoints matters to determine the index to access the raw data\n// k and l                  horizontal and vertical indices of the point just bellow p[0] in the raw data\n\nvoid interpolate( vertex2d* p, vertex2d* fieldLinePointsVerts, int fieldNumHorizontalPoints, int k, int l ) {\n\n    int index = (l * fieldNumHorizontalPoints + k) * 2;\n\n    vertex2d p11;\n    p11.x = fieldLinePointsVerts[index].x;\n    p11.y = fieldLinePointsVerts[index].y;\n\n    vertex2d q11;\n    q11.x = fieldLinePointsVerts[index+1].x;\n    q11.y = fieldLinePointsVerts[index+1].y;\n\n    index = (l * fieldNumHorizontalPoints + k + 1) * 2;\n\n    vertex2d q21;\n    q21.x = fieldLinePointsVerts[index+1].x;\n    q21.y = fieldLinePointsVerts[index+1].y;\n\n    index = ( (l + 1) * fieldNumHorizontalPoints + k) * 2;\n\n    vertex2d q12;\n    q12.x = fieldLinePointsVerts[index+1].x;\n    q12.y = fieldLinePointsVerts[index+1].y;\n\n    index = ( (l + 1) * fieldNumHorizontalPoints + k + 1 ) * 2;\n\n    vertex2d p22;\n    p22.x = fieldLinePointsVerts[index].x;\n    p22.y = fieldLinePointsVerts[index].y;\n\n    vertex2d q22;\n    q22.x = fieldLinePointsVerts[index+1].x;\n    q22.y = fieldLinePointsVerts[index+1].y;\n\n    float fx = 1.0 / (p22.x - p11.x);\n    float fx1 = (p22.x - p[0].x) * fx;\n    float fx2 = (p[0].x - p11.x) * fx;\n\n    vertex2d r1;\n    r1.x = fx1 * q11.x + fx2 * q21.x;\n    r1.y = fx1 * q11.y + fx2 * q21.y;\n\n    vertex2d r2;\n    r2.x = fx1 * q12.x + fx2 * q22.x;\n    r2.y = fx1 * q12.y + fx2 * q22.y;\n\n    float fy = 1.0 / (p22.y - p11.y);\n    float fy1 = (p22.y - p[0].y) * fy;\n    float fy2 = (p[0].y - p11.y) * fy; \n\n    p[1].x = fy1 * r1.x + fy2 * r2.x;\n    p[1].y = fy1 * r1.y + fy2 * r2.y;\n}\n</code></pre>\n\n<p>Currently this code needs to be run every single frame in old iOS devices, say devices with arm6 processors</p>\n\n<p>I've taken the numeric sub-indices from the wikipedia's equations <a href=\"http://en.wikipedia.org/wiki/Bilinear_interpolation\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Bilinear_interpolation</a></p>\n\n<p>I'd accreciate any comments on optimization for performance, even plain asm code </p>\n"},{"tags":["c++","arrays","performance","stl","vector"],"answer_count":15,"favorite_count":19,"up_vote_count":69,"down_vote_count":3,"view_count":6222,"score":66,"question_id":3664272,"title":"std::vector is so much slower than plain arrays?","body":"<p>I've always thought it's the general wisdom that <code>std::vector</code> is \"implemented as an array,\" blah blah blah. Today I went down and tested it, seems to be not so:</p>\n\n<p>Here's some test results:</p>\n\n<pre><code>UseArray completed in 2.619 seconds\nUseVector completed in 9.284 seconds\nUseVectorPushBack completed in 14.669 seconds\nThe whole thing completed in 26.591 seconds\n</code></pre>\n\n<p>That's about 3 - 4 times slower! Doesn't really justify for the \"<code>vector</code> may be slower for a few nanosecs\" comments.</p>\n\n<p>And the code I used:</p>\n\n<pre><code>#include &lt;cstdlib&gt;\n#include &lt;vector&gt;\n\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n\n#include &lt;boost/date_time/posix_time/ptime.hpp&gt;\n#include &lt;boost/date_time/microsec_time_clock.hpp&gt;\n\nclass TestTimer\n{\npublic:\n    TestTimer(const std::string &amp; name) : name(name),\n        start(boost::date_time::microsec_clock&lt;boost::posix_time::ptime&gt;::local_time())\n    {\n    }\n\n    ~TestTimer()\n    {\n        using namespace std;\n        using namespace boost;\n\n        posix_time::ptime now(date_time::microsec_clock&lt;posix_time::ptime&gt;::local_time());\n        posix_time::time_duration d = now - start;\n\n        cout &lt;&lt; name &lt;&lt; \" completed in \" &lt;&lt; d.total_milliseconds() / 1000.0 &lt;&lt;\n            \" seconds\" &lt;&lt; endl;\n    }\n\nprivate:\n    std::string name;\n    boost::posix_time::ptime start;\n};\n\nstruct Pixel\n{\n    Pixel()\n    {\n    }\n\n    Pixel(unsigned char r, unsigned char g, unsigned char b) : r(r), g(g), b(b)\n    {\n    }\n\n    unsigned char r, g, b;\n};\n\nvoid UseVector()\n{\n    TestTimer t(\"UseVector\");\n\n    for(int i = 0; i &lt; 1000; ++i)\n    {\n        int dimension = 999;\n\n        std::vector&lt;Pixel&gt; pixels;\n        pixels.resize(dimension * dimension);\n\n        for(int i = 0; i &lt; dimension * dimension; ++i)\n        {\n            pixels[i].r = 255;\n            pixels[i].g = 0;\n            pixels[i].b = 0;\n        }\n    }\n}\n\nvoid UseVectorPushBack()\n{\n    TestTimer t(\"UseVectorPushBack\");\n\n    for(int i = 0; i &lt; 1000; ++i)\n    {\n        int dimension = 999;\n\n        std::vector&lt;Pixel&gt; pixels;\n            pixels.reserve(dimension * dimension);\n\n        for(int i = 0; i &lt; dimension * dimension; ++i)\n            pixels.push_back(Pixel(255, 0, 0));\n    }\n}\n\nvoid UseArray()\n{\n    TestTimer t(\"UseArray\");\n\n    for(int i = 0; i &lt; 1000; ++i)\n    {\n        int dimension = 999;\n\n        Pixel * pixels = (Pixel *)malloc(sizeof(Pixel) * dimension * dimension);\n\n        for(int i = 0 ; i &lt; dimension * dimension; ++i)\n        {\n            pixels[i].r = 255;\n            pixels[i].g = 0;\n            pixels[i].b = 0;\n        }\n\n        free(pixels);\n    }\n}\n\nint main()\n{\n    TestTimer t1(\"The whole thing\");\n\n    UseArray();\n    UseVector();\n    UseVectorPushBack();\n\n    return 0;\n}\n</code></pre>\n\n<p>Am I doing it wrong or something? Or have I just busted this performance myth?</p>\n\n<p>Edit: I'm using Release mode in MSVS2005.</p>\n\n<hr>\n\n<p>In MSVC, <code>#define _SECURE_SCL 0</code> reduces <code>UseVector</code> by half (bringing it down to 4 seconds). This is really huge IMO.</p>\n"},{"tags":["java","performance","map"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":82,"score":1,"question_id":12991569,"title":"Efficient java map for values.contains(object) in O(1)?","body":"<p>I've started writing a tool for modelling graphs (nodes and edges, both represented by objects, not adjacency matrices or simply numbers) recently to get some practice in software development next to grad school. I'd like a node to know its neighbors and the edges it's incident with. An obvious way to do this would be to take a HashSet and a HashSet. What I would like to do however is have a method</p>\n\n<pre><code>Node_A.getEdge(Node B)\n</code></pre>\n\n<p>that returns the edge between nodes A and B in O(1). I was thinking of doing this by replacing the two HashSets mentioned above by using one HashMap that maps the neighbors to the edges that connect a node with its neighbors. For example, calling</p>\n\n<pre><code>Node_A.hashmap.get(B)\n</code></pre>\n\n<p>should return the edge that connects A and B. My issue here is whether there's a way to have</p>\n\n<pre><code>HashMap.keySet().contains(Node A);\nHashMap.values().contains(Edge e);\n</code></pre>\n\n<p>both work in O(1)? If that isn't the case with the standard libraries, are there implementations that will give me constant time for add, remove, contains, size for keySet() and values()?</p>\n"},{"tags":["iphone","ios","performance","cocoa-touch","uiviewcontroller"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":53,"score":0,"question_id":12991582,"title":"What can happen between viewWillAppear and viewDidAppear?","body":"<p>I am optimizing a transition that seems to be slow on my device. I am pushing one <code>UIViewController</code> from another when a <code>UITableView</code>'s row is selected. There is a noticeable pause after row selection and before the new view is pushed.</p>\n\n<p>Some logging indicates that all of my code is reasonably quick, from row selection until the pushed controller's <code>viewWillAppear</code>. But then the time between <code>viewWillAppear</code> and <code>viewDidAppear</code> is logged at around 0.7 seconds.</p>\n\n<p>The transition itself (I believe) should only take 0.3 seconds. What could be accounting for the remainder?</p>\n\n<p>I am testing on an iPhone 4, so I'm not expecting the snappiest performance. But I should be able to match the same performance of other similar apps on the same device, no?</p>\n"},{"tags":["php","performance","compression"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":147,"score":-1,"question_id":8222311,"title":"I looking for the best way to compress a file/string with PHP","body":"<p>They are many options on php.net website for compression:</p>\n\n<ul>\n<li>Bzip2</li>\n<li>LZF</li>\n<li>Phar</li>\n<li>Rar</li>\n<li>Zip</li>\n<li>Zlib</li>\n</ul>\n\n<p>I am looking for the best option. In other words witch one is the fastes?</p>\n\n<p>Also I am thinking of splitting the file into multiple ones.</p>\n\n<p>I though about reformatting the files into a giant string and then use a PHP script to split the string after a certain amount of characters. But I am not sure this is the fastes way.</p>\n\n<p>To put the file into a string I was thinking of using base64_encode... Is this the best way?</p>\n\n<p>Thanks in advance for any tip and help.</p>\n"},{"tags":["mysql","performance","relevance","full-text"],"answer_count":3,"favorite_count":4,"up_vote_count":11,"down_vote_count":0,"view_count":1722,"score":11,"question_id":237970,"title":"Full-text search relevance is measured in?","body":"<p>I am making a quiz system, and when quizmakers insert questions into the Question Bank, I am to check the DB for duplicate / very highly similar questions.</p>\n\n<p>Testing MySQL's <a href=\"http://dev.mysql.com/doc/refman/5.0/en/fulltext-search.html#function_match\" rel=\"nofollow\">MATCH() ... AGAINST()</a>, the highest relevance I get is 30+, when I test against a 100% similar string.</p>\n\n<p>So what exactly is the relevance? To quote the <a href=\"http://dev.mysql.com/doc/refman/5.0/en/fulltext-natural-language.html\" rel=\"nofollow\">manual</a>:</p>\n\n<blockquote>\n  <p>Relevance values are non-negative floating-point numbers. Zero relevance means no similarity. Relevance is computed based on the number of words in the row, the number of unique words in that row, the total number of words in the collection, and the number of documents (rows) that contain a particular word. </p>\n</blockquote>\n\n<p>My problem is how to test the relevance value if a string is a duplicate. If it's 100% duplicate, prevent it from being inserter into Question Bank. But if it is only so similar, prompt the quizmaker to verify, insert or not. So how do I do that? 30+ for 100% identical string is not percentage, so I'm stump.</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["ruby-on-rails","database","performance","nosql"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":54,"score":0,"question_id":12971230,"title":"Rails3: Should I switch to NoSQL? Alot of calculations, alot of data for user-matching","body":"<p>I'm at a stage of my project where I'm thinking about moving to NoSQL for performance reasons.  I will definitely have tables with millions of rows, so NoSQL might be useful. But my problem is I'm also doing alot of calculations with this data and I don't know if that would give me that much more performance if rails still has to do all the calculations.</p>\n\n<p>Here's another question of mine where I describe what data I need and how I process it: <a href=\"http://stackoverflow.com/questions/12944813/rails-3-user-matching-algorithm-to-sql-query-complicated/\">Rails 3 user matching-algorithm to SQL Query (COMPLICATED)</a></p>\n\n<p>After I realized most of my code in SQL and match one user with 1000 other users it still took</p>\n\n<pre><code>Completed 200 OK in 104871ms (Views: 2146.0ms | ActiveRecord: 93780.5ms)\n(on my local machine with sqlite)\n</code></pre>\n\n<p>And that's not acceptable for me.\nI would definitely be able to denormalize my tables into one for this to work. But will this give me a performance boost?</p>\n\n<p>I also thought about storing the calculated match percentages IN the database, but that would result in 2.5 billion rows for just 50k users.</p>\n"},{"tags":["performance","entity-framework"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":32,"score":1,"question_id":12990662,"title":"EF5 .Local performance","body":"<p>I'm doing this on a table with ~43k rows:</p>\n\n<pre><code>MyDbContext.Stores.Load();\nMyDbContext.Stores.Local.Count.Dump(); //horrible performance!\n</code></pre>\n\n<p>I can see through the profiler that the first instruction fires up the select statement to fetch all rows. Actually the second instruction returns the correct value but after ~12 seconds, and it is not what I was expecting considering that all data should be in memory.\nWhat is wrong (or what is its real purpose) with .Local in Entity Framework?</p>\n"},{"tags":["ios","performance","transparency"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":12990321,"title":"iOS Performance troubles with transparency","body":"<p>I just generated a gradient with transparency programmatically by adding a solid color and a gradient to an image mask. I then applied the resulting image to my UIView.layer.content. The visual is fine, but when I scroll object under the transparency, the app gets chunky. Is there a way to speed up?</p>\n\n<p>My minital thought was caching the resulting gradient. Another thought was to create a gradient that is only one pixel wide and stretch it to cover the desired area. Will either of these approaches help the performance?</p>\n\n<p>Joe </p>\n"},{"tags":["php","string","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":6,"view_count":158,"score":-5,"question_id":8304341,"title":"Which is faster, concatenating a string or imploding an array of strings?","body":"<p>Got into a discussion about the performance issues of combing a string.</p>\n\n<p>Example</p>\n\n<pre><code>$var1 = 'abc';\n$var2 = 'def';\n$var3 = 'hij';\n</code></pre>\n\n<p>Would it be faster to combine these by doing</p>\n\n<pre><code>implode('', array($var1, $var2, $var3));\n</code></pre>\n\n<p>Or would it be faster to do</p>\n\n<pre><code>$var1.$var2.$var3;\n</code></pre>\n"},{"tags":["mysql","performance","postgresql","database-performance","postgresql-performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":66,"score":1,"question_id":12986656,"title":"Postgres equivalent of MySQL's BENCHMARK() function","body":"<p>I am using Postgresql. I want to test how much time a function takes to execute. Since the fucntion takes only a few milliseconds, I would like to call it in a loop 1000s of times to get an accurate figure. </p>\n\n<p>MySQL has a BENCHMARK() function to do this. Is there an equivalent or do I have to write a procedure with a loop to do this?</p>\n"},{"tags":["asp.net","performance","iis","cassini"],"answer_count":1,"favorite_count":2,"up_vote_count":0,"down_vote_count":0,"view_count":86,"score":0,"question_id":12954150,"title":"Why is IIS slower than ASP.NET Development Server?","body":"<p>I have an ASPX webpage that does some complex operations and database calls. When I view the webpage by running ASP.NET Development Server (Cassini), it takes about <em>200ms</em>.</p>\n\n<p>Then, <em>without any code changes and configuration changes</em>, I deploy the website to my local machine IIS 7 and view the same web page again. It takes <em>2.0sec</em>, which is <strong>10 times slower</strong>.</p>\n\n<p>I thought IIS should be faster than (or at least as fast as) Cassini.</p>\n\n<p>To investigate further, I created a new page, test1.aspx, which contains nothing but an empty for-loop that runs for 90 million times in the Page_Load. In Cassini, it takes about 200ms. In IIS, it takes 300ms (50% slower).</p>\n\n<p>What could be the reason that makes IIS slower than Cassini? Or, perhaps an even better question, how can I make IIS run at least as fast as Cassini? </p>\n"},{"tags":["php","performance","memory-management","apc"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":222,"score":1,"question_id":2884413,"title":"Does PHP's APC reclaim memory after apc_delete() is called?","body":"<p>More generally, does anyone know where the way APC works internally is documented?</p>\n"},{"tags":["php","performance","apc"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":34,"score":1,"question_id":12989424,"title":"PHP APC doesn't cache mixed php/html files","body":"<p>I seen, through the APC manager, which APC caches only \"pure\" php files, while files with php and raw html mixed aren't cached. Does a solution exist for this question?</p>\n\n<p>This a example of mixed code:</p>\n\n<pre><code>&lt;html&gt;\n&lt;p&gt;fgsfsdfs &lt;?php echo \"test\" ?&gt;&lt;/p&gt;\n&lt;/html&gt;\n</code></pre>\n\n<p>Could I resolve, including all raw html in php \"echo function\"? How would be performance in this case?</p>\n"},{"tags":["algorithm","performance","sorting","quicksort"],"answer_count":6,"favorite_count":0,"up_vote_count":12,"down_vote_count":0,"view_count":1932,"score":12,"question_id":4146843,"title":"When should we use Radix sort?","body":"<p>It seems Radix sort has a very good average case performance, i.e. <em>O(kN)</em>: <a href=\"http://en.wikipedia.org/wiki/Radix_sort\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Radix_sort</a></p>\n\n<p>but it seems most people still are using Quick Sort, don't they?</p>\n"},{"tags":["performance","ms-access"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":2695,"score":2,"question_id":2405226,"title":"How to populate an array with recordset data","body":"<p>I am attempting to move data from a recordset directly into an array. I know this is possible, but specifically I want to do this in VBA as this is being done in MS Access 2003.</p>\n\n<p>Typically I would do something like the following to achieve this:</p>\n\n<pre><code>    Dim vaData As Variant \n    Dim rst As ADODB.Recordset\n\n    ' Pull data into recordset code here...\n\n    ' Populate the array with the whole recordset.\n    vaData = rst.GetRows \n</code></pre>\n\n<p>What differences exist between VB and VBA which makes this type of operation not work?</p>\n\n<p>What about performance concerns? Is this an \"expensive\" operations?</p>\n"},{"tags":["mysql","sql","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":73,"score":3,"question_id":12988199,"title":"MySQL database structure: more columns or more rows","body":"<p>I collect how people tag topics with categories in table like:</p>\n\n<pre><code>ID | topic_id | votes_Category_1 | votes_Category_2 |.......... | votes_Category_12\n</code></pre>\n\n<p>I dump this table every hour for history reasons.\nLets say table contain 2 million rows. dumped every hour in history tables.</p>\n\n<p>This solution is not flexible if I want to add column Category_13, so I'm thinking about this one:</p>\n\n<pre><code>ID | topic_id | Category_id | vote_count\n</code></pre>\n\n<p>This solution will create 12 rows per topic, its better structured and more flexible but I'll have to dump every hour 24 million rows.</p>\n\n<p>I need the best 10 topics in each category!\nI wonder in Case 2 if using Max on votes (where category_id=x and topic_id=y) will be slower than in case 1: Order by categoy_x where topic_id=y</p>\n\n<p>Which one would be better JUST!!! from performance stand point:</p>\n\n<ol>\n<li>To have 2 million rows with 14 columns</li>\n<li>To have 24 million rows with 4 columns</li>\n</ol>\n\n<p>Thank you</p>\n"},{"tags":["c#",".net","performance","logging"],"answer_count":6,"favorite_count":7,"up_vote_count":10,"down_vote_count":0,"view_count":2331,"score":10,"question_id":1348643,"title":"How performant is StackFrame?","body":"<p>I am considering using something like <code>StackFrame stackFrame = new StackFrame(1)</code> to log the executing method, but I don't know about its performance implications. Is the stack trace something that is build anyway with each method call so performance should not be a concern or is it something that is only build when asked for it? Do you recommend against it in an application where performance is very important? If so, does that mean I should disable it for the release?</p>\n"},{"tags":["c#","performance","xna"],"answer_count":3,"favorite_count":2,"up_vote_count":6,"down_vote_count":0,"view_count":183,"score":6,"question_id":5432073,"title":"What books, articles, blog posts about XNA performance?","body":"<p>Do you know some books, articles, blogs about how to write code to keep good performance in XNA?<br>\nI know that there are principles in XNA, strict rules what to do and what to avoid, what solutions are better.  </p>\n"},{"tags":["c++","performance","linker","exe"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":58,"score":0,"question_id":12755482,"title":"EXE global slowdown","body":"<p>I have an exe containing Fortran and C++ code that was recently modified to link against some custom static libraries.  The exe now runs significantly slower (~ factor of 2) than before in every function, even though it is not calling any new code in the test run.  The call graph and function hit count has been checked in a profiler and validates the assertion that the new code is not being called.</p>\n\n<p>At link time there are now numerous \"multiple definition\" warnings caused by the new code.  However, optimisations are still enabled and no other compiler or environment settings have been modified.  The exe is not significantly larger than before and the memory footprint is the same in both runs.</p>\n\n<p>Any ideas what might cause this?</p>\n"},{"tags":["c++","performance","matrix","io","binary"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":75,"score":0,"question_id":12987796,"title":"Reading big binary files in c++, performance difference compiling with makefile or not","body":"<p>It's being years I read this forum but this is my first question. I have a program which reads a really big matrix (~500 megabytes) of doubles stored in binary. If I compile my program #including all the libraries etc.. the matrices get read in few seconds, if I use a makefile to link all the object files, the program read the matrix in more than 45 seconds!.</p>\n\n<p>I have changed nothing on the program except for compiling it with a makefile.</p>\n\n<p>I do numerical computation and performance is important. Is there an explanation and hopefully a way out?</p>\n\n<p>I'm using a mac, runing lion, g++ 4.2 </p>\n"},{"tags":["performance","category","opencart"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":1920,"score":1,"question_id":9915941,"title":"Opencart extremely slow loading speed","body":"<p>I'm running Opencart 1.5.2 on my server and after importing a large number of products I got a massive speed down. I tried installing a vq mod <a href=\"http://www.opencart.com/index.php?route=extension/extension/info&amp;extension_id=3444\" rel=\"nofollow\">http://www.opencart.com/index.php?route=extension/extension/info&amp;extension_id=3444</a> which had to speed up the site... it didn't.</p>\n\n<p>Store url: <a href=\"http://abvbooks.com/alba-books/\" rel=\"nofollow\">http://abvbooks.com/alba-books/</a></p>\n\n<p>I know I have some elements on the site which are relatively big but before the import it was running fine.</p>\n\n<p>I appreciate the help.</p>\n"},{"tags":["performance","methods","log4j"],"answer_count":3,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":239,"score":4,"question_id":9979044,"title":"log4j: fastest way to display method name?","body":"<p>In my logging messages, I need to insert the name of the method where the messages were produced. I've looked at Log4J documentation and \"M\" and \"l\" conversion chars that also have warning like \"WARNING Generating caller location information is extremely slow and should be avoided unless execution speed is not an issue\". So I have (at least) two options:</p>\n\n<ol>\n<li>Use these chars but slow down my code</li>\n<li>Manually insert method name into messages, i.e. something like this <code>log.info(\"myMethod:  message\");</code> which will be faster but not as elegant</li>\n</ol>\n\n<p>Are there any other options that would not slow down my code?</p>\n\n<p>Thanks!</p>\n"},{"tags":["performance","postgresql","index"],"answer_count":3,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":2149,"score":3,"question_id":4009062,"title":"Very slow bitmap heap scan in Postgres","body":"<p>I have the following simple table that contains traffic measurement data:</p>\n\n<pre><code>CREATE TABLE \"TrafficData\"\n(\n  \"RoadID\" character varying NOT NULL,\n  \"DateID\" numeric NOT NULL,\n  \"ExactDateTime\" timestamp NOT NULL,\n  \"CarsSpeed\" numeric NOT NULL,\n  \"CarsCount\" numeric NOT NULL\n)\nCREATE INDEX \"RoadDate_Idx\" ON \"TrafficData\" USING btree (\"RoadID\", \"DateID\");\n</code></pre>\n\n<p>The column RoadID uniquely identifies the road whose data is being recorded, while DateID identifies the day of the year (1..365) of the data - basically a rounded off representation of ExactDateTime.</p>\n\n<p>I have about 100.000.000 rows; there are 1.000 distinct values in the column \"RoadID\" and 365 distinct values in the column \"DateID\".</p>\n\n<p>I then run the following query:</p>\n\n<pre><code>SELECT * FROM \"TrafficData\"\nWHERE \"RoadID\"='Station_1'\nAND \"DateID\"&gt;20100610 AND \"DateID\"&lt;20100618;\n</code></pre>\n\n<p>This takes up to three mind-boggling seconds to finish, and I cannot for the life of me figure out WHY.</p>\n\n<p>EXPLAIN ANALYZE gives me the following output:</p>\n\n<pre><code>Bitmap Heap Scan on \"TrafficData\"  (cost=104.84..9743.06 rows=2496 width=47) (actual time=35.112..2162.404 rows=2016 loops=1)\n  Recheck Cond: (((\"RoadID\")::text = 'Station_1'::text) AND (\"DateID\" &gt; 20100610::numeric) AND (\"DateID\" &lt; 20100618::numeric))\n  -&gt;  Bitmap Index Scan on \"RoadDate_Idx\"  (cost=0.00..104.22 rows=2496 width=0) (actual time=1.637..1.637 rows=2016 loops=1)\n        Index Cond: (((\"RoadID\")::text = 'Station_1'::text) AND (\"DateID\" &gt; 20100610::numeric) AND (\"DateID\" &lt; 20100618::numeric))\nTotal runtime: 2163.985 ms\n</code></pre>\n\n<p>My specs:</p>\n\n<ul>\n<li>Windows 7</li>\n<li>Postgres 9.0</li>\n<li>4GB RAM</li>\n</ul>\n\n<p>I'd greatly appreciate any helpful pointers!</p>\n"},{"tags":["mysql","xml","performance","caching"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":46,"score":0,"question_id":12981893,"title":"Is it a good idea to cache/read data with xml instead of reading from MySQL database?","body":"<p>I'v been running my own blog system for a while, and I noticed that for most of the pages, several (5-10) same databse queries need to be performed everytime the page gets visited.</p>\n\n<p>Now I'm thinking of caching data into a xml file. Here's my plan of doing this:</p>\n\n<p>--- When posting/editing a blog, insert/update data into MySQL database, and generate the xml file</p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;data&gt;\n    &lt;id&gt;32&lt;/id&gt;\n    &lt;title&gt;caching data with xml&lt;/title&gt;\n    &lt;date&gt;2012-10-01&lt;/date&gt;\n    &lt;content&gt;&lt;![CDATA[&lt;p&gt;blah...blah...blah...&lt;/p&gt;]]&gt;&lt;/content&gt;\n&lt;/data&gt;\n</code></pre>\n\n<p>--- When the page gets visited, check for file existence first (just in case), and then parse the xml file and format/output data.</p>\n\n<pre><code>if(file_exists(\"path/to/blog/32.xml\")) {\n    $data = simplexml_load_file(\"path/to/blog/32.xml\");\n    echo '&lt;h2'&gt;.$data-&gt;title.'&lt;/h2&gt;';\n    echo '&lt;p'&gt;.$data-&gt;date.'&lt;/p&gt;';\n    echo $data-&gt;content;\n}\nelse {\n    mysql_query(...);\n}\n</code></pre>\n\n<p>By doing this MySQL could do much less work, but I'm not quite sure if this is gonna cause any problems later, like 100 or 300 visitors are visiting the same page at the same time. Can PHP handle that? Am I doing it right with this method?</p>\n\n<p>Thank in advance for any info and tips.</p>\n\n<p>BTW, I'm not thinking of using those templates yet.</p>\n"},{"tags":["c#",".net","performance","algorithm","optimization"],"answer_count":20,"favorite_count":18,"up_vote_count":63,"down_vote_count":1,"view_count":51796,"score":62,"question_id":228038,"title":"Best way to reverse a string","body":"<p>I've just had to write a string reverse function in C# 2.0 (i.e. LINQ not available) and came up with this:</p>\n\n<pre><code>public string Reverse(string text)\n{\n    char[] cArray = text.ToCharArray();\n    string reverse = String.Empty;\n    for (int i = cArray.Length - 1; i &gt; -1; i--)\n    {\n        reverse += cArray[i];\n    }\n    return reverse;\n}\n</code></pre>\n\n<p>Personally I'm not crazy about the function and am convinced that there's a better way to do it. Is there?</p>\n"},{"tags":["c#","performance","caching","sortedlist","mmo"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":126,"score":2,"question_id":12872801,"title":"Improving Game Performance C#","body":"<p>We are all aware of the popular trend of MMO games. where players face each other live. However during gameplay there is a tremendous flow of SQL inserts and queries, as given below</p>\n\n<ul>\n<li>There are average/minimum 100 tournaments online per 12 minutes or 500 players / hour</li>\n<li>In Game Progress table, We are storing each player move \n<ul>\n<li>12 round tournament of 4 player there can be 48 records</li>\n<li>plus around same number for spells or special items</li>\n<li>a total of 96 per tournament or 48000 record inserts per hour (500 players/hour)</li>\n</ul></li>\n</ul>\n\n<p>In reponse to my previous question ( <a href=\"http://stackoverflow.com/questions/12668680/improve-mmo-game-performance\">Improve MMO game performance</a> ), I changed the schema and we are not writing directly to database. </p>\n\n<p>Instead accumulating all values in a <code>DataTable</code>. The process then whenever the <code>DataTable</code> has more than 100k rows (which can sometimes be even within the hour) writes to a text file in csv format. Another background application which frequently scans the folder for CSV files, reads any available CSV file and stores the information into server database.</p>\n\n<p><strong>Questions</strong></p>\n\n<ol>\n<li><p>Can we access the datatable present in the game application from another application, directly (it reads the datatable and clears records that have read). So that the in place of writing and reading from disk, we read and write directly from memory.</p></li>\n<li><p>Is there any method that is quicker that DataTable, that can hold large data and yet be fairly quicker in sorting and updating operation. Because we have to frequenly scan for userids, update game status (almost at every insert). It can be a cache utility OR a fast\nScan/Search algorithm OR even a CollectionModel. Right now, we use a foreach loop to go through all records in a <code>DataTable</code> and update rows if user is present. If not then we create a new row. I tried using <code>SortedList</code> and classes, but then it not only doubles the effort, memory usage increases tremendously slowing down overall game performance.</p></li>\n</ol>\n\n<p>thanks</p>\n\n<p>arvind</p>\n"},{"tags":["performance","cpu","low-level"],"answer_count":3,"favorite_count":1,"up_vote_count":1,"down_vote_count":1,"view_count":57,"score":0,"question_id":12977050,"title":"What has a better performance: multiplication or division?","body":"<p>Which version is faster ?\n<code>x * 0.5\nor\nx / 2</code></p>\n\n<p>Ive had a course at the university called computer systems some time ago. From back then i remember that multiplying two values can be achieved with comparably \"simple\" logical gates but division is not a \"native\" operation and requires a sum register that is in a loop increased by the divisor and compared to the dividend.</p>\n\n<p>Now i have to optimise an algorithm with a lot of divisions. Unfortunately its not just dividing by two so binary shifting is no option. Will it make a difference to change all divisions to multiplications ?</p>\n\n<p>update:</p>\n\n<p>I have changed my code and didnt notice any difference. You're probably right about compiler optimisations. Since all the answers were great ive upvoted them all. I chose rahul's answer because of the great link. </p>\n"},{"tags":["c++","windows","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":43,"score":0,"question_id":12984782,"title":"Improving Delay of Context Switch Monitor","body":"<p>So I've written a program that continually polls the operating system to see if a specific process has experienced any context switches. In addition, this program sends a signal of either \"Process Sleeping\" if it has detected 0 context switches since last polling, or \"Process Context Switching\" if it has detected more than 0 context switches since last polling. It sends this signal to a receiver program running on the same machine. Here is the code I use to do this:</p>\n\n<pre><code>#include \"stdafx.h\"\n#include &lt;iostream&gt;\n#include &lt;windows.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;conio.h&gt;\n#include &lt;pdh.h&gt;\n#include &lt;pdhmsg.h&gt;\n#include &lt;iomanip&gt;\n#include &lt;string.h&gt;\n#pragma comment(lib, \"pdh.lib\")\n\nusing namespace std;\nCONST ULONG SAMPLE_INTERVAL_MS    = 0;\nCONST PWSTR BROWSE_DIALOG_CAPTION = L\"Select a counter to monitor.\";\nstatic SOCKET m_socket;\n\nstatic void sendSignal(char * sendbuf){\n    int bytesSent = send(m_socket, sendbuf, strlen(sendbuf), 0);\n}\n\nstatic int initializeConnection(){\n    WORD wVersionRequested;\n    WSADATA wsaData;\n    int wsaerr;\n\n    // Using MAKEWORD macro, Winsock version request 2.2\n    wVersionRequested = MAKEWORD(2, 2);\n\n    wsaerr = WSAStartup(wVersionRequested, &amp;wsaData);\n    if (wsaerr != 0)\n    {\n        /* Tell the user that we could not find a usable WinSock DLL.*/\n        printf(\"Server: The Winsock dll not found!\\n\");\n        return 0;\n    }\n    else\n    {\n        printf(\"Server: The Winsock dll found!\\n\");\n        printf(\"Server: The status: %s.\\n\", wsaData.szSystemStatus);\n    }\n\n    /* Confirm that the WinSock DLL supports 2.2.*/\n    /* Note that if the DLL supports versions greater    */\n    /* than 2.2 in addition to 2.2, it will still return */\n    /* 2.2 in wVersion since that is the version we      */\n    /* requested.                                        */\n    if (LOBYTE(wsaData.wVersion) != 2 || HIBYTE(wsaData.wVersion) != 2 )\n    {\n        /* Tell the user that we could not find a usable WinSock DLL.*/\n        printf(\"Server: The dll do not support the Winsock version %u.%u!\\n\", LOBYTE(wsaData.wVersion), HIBYTE(wsaData.wVersion));\n        WSACleanup();\n        return 0;\n    }\n    else\n    {\n        printf(\"Server: The dll supports the Winsock version %u.%u!\\n\", LOBYTE(wsaData.wVersion), HIBYTE(wsaData.wVersion));\n        printf(\"Server: The highest version this dll can support: %u.%u\\n\", LOBYTE(wsaData.wHighVersion), HIBYTE(wsaData.wHighVersion));\n    }\n\n    //////////Create a socket////////////////////////\n    //Create a SOCKET object called m_socket.\n    //SOCKET m_socket;\n\n    // Call the socket function and return its value to the m_socket variable.\n    // For this application, use the Internet address family, streaming sockets, and\n    // the TCP/IP protocol.\n    // using AF_INET family, TCP socket type and protocol of the AF_INET - IPv4\n    m_socket = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);\n\n    // Check for errors to ensure that the socket is a valid socket.\n    if (m_socket == INVALID_SOCKET)\n    {\n        printf(\"Server: Error at socket(): %ld\\n\", WSAGetLastError());\n        WSACleanup();\n        return 0;\n    }\n    else\n    {\n        printf(\"Server: socket() is OK!\\n\");\n    }\n\n    ////////////////bind//////////////////////////////\n    // Create a sockaddr_in object and set its values.\n    sockaddr_in service;\n\n    // AF_INET is the Internet address family.\n    service.sin_family = AF_INET;\n    // \"127.0.0.1\" is the local IP address to which the socket will be bound.\n    service.sin_addr.s_addr = inet_addr(\"127.0.0.1\");\n    // 55555 is the port number to which the socket will be bound.\n    service.sin_port = htons(55555);\n\n    // Call the bind function, passing the created socket and the sockaddr_in structure as parameters.\n    // Check for general errors.\n    if (bind(m_socket, (SOCKADDR*)&amp;service, sizeof(service)) == SOCKET_ERROR)\n    {\n        printf(\"Server: bind() failed: %ld.\\n\", WSAGetLastError());\n        closesocket(m_socket);\n        return 0;\n    }\n    else\n    {\n        printf(\"Server: bind() is OK!\\n\");\n    }\n\n    // Call the listen function, passing the created socket and the maximum number of allowed\n    // connections to accept as parameters. Check for general errors.\n    if (listen(m_socket, 10) == SOCKET_ERROR)\n        printf(\"Server: listen(): Error listening on socket %ld.\\n\", WSAGetLastError());\n    else\n    {\n        printf(\"Server: listen() is OK, I'm waiting for connections...\\n\");\n    }\n\n    // Create a temporary SOCKET object called AcceptSocket for accepting connections.\n    SOCKET AcceptSocket;\n\n    // Create a continuous loop that checks for connections requests. If a connection\n    // request occurs, call the accept function to handle the request.\n    printf(\"Server: Waiting for a client to connect...\\n\" );\n    printf(\"***Hint: Server is ready...run your client program...***\\n\");\n    // Do some verification...\n    while (1)\n    {\n        AcceptSocket = SOCKET_ERROR;\n        while (AcceptSocket == SOCKET_ERROR)\n        {\n            AcceptSocket = accept(m_socket, NULL, NULL);\n        }\n        // else, accept the connection...\n        // When the client connection has been accepted, transfer control from the\n        // temporary socket to the original socket and stop checking for new connections.\n        printf(\"Server: Client Connected!\\n\");\n        m_socket = AcceptSocket;\n        break;\n    }\n    return 0;\n}\n\n\n\nvoid wmain(void)\n{\n    PDH_STATUS Status;\n    HQUERY Query = NULL;\n    HCOUNTER Counter;\n    PDH_FMT_COUNTERVALUE DisplayValue;\n    DWORD CounterType;\n    SYSTEMTIME SampleTime;\n    PDH_BROWSE_DLG_CONFIG BrowseDlgData;\n    WCHAR CounterPathBuffer[PDH_MAX_COUNTER_PATH] = L\"\\\\\\\\HP1040915\\\\Thread(tiny/0)\\\\Context Switches/sec\";\n\n    initializeConnection();\n\n\n    //\n    // Create a query.\n    //\n    Status = PdhOpenQuery(NULL, NULL, &amp;Query);\n\n    if (Status != ERROR_SUCCESS) \n    {\n        wprintf(L\"\\nPdhOpenQuery failed with status 0x%x.\", Status);\n        goto Cleanup;\n    }\n\n    //\n    // Initialize the browser dialog window settings.\n    //\n\n\n\n    ZeroMemory(&amp;BrowseDlgData, sizeof(PDH_BROWSE_DLG_CONFIG));\n\n    BrowseDlgData.bIncludeInstanceIndex = FALSE;\n    BrowseDlgData.bSingleCounterPerAdd = TRUE;\n    BrowseDlgData.bSingleCounterPerDialog = TRUE;\n    BrowseDlgData.bLocalCountersOnly = FALSE;\n    BrowseDlgData.bWildCardInstances = TRUE;\n    BrowseDlgData.bHideDetailBox = TRUE;\n    BrowseDlgData.bInitializePath = FALSE;\n    BrowseDlgData.bDisableMachineSelection = FALSE;\n    BrowseDlgData.bIncludeCostlyObjects = FALSE;\n    BrowseDlgData.bShowObjectBrowser = FALSE;\n    BrowseDlgData.hWndOwner = NULL;\n    BrowseDlgData.szReturnPathBuffer = CounterPathBuffer;\n    BrowseDlgData.cchReturnPathLength = PDH_MAX_COUNTER_PATH;\n    BrowseDlgData.pCallBack = NULL;\n    BrowseDlgData.dwCallBackArg = 0;\n    BrowseDlgData.CallBackStatus = ERROR_SUCCESS;\n    BrowseDlgData.dwDefaultDetailLevel = PERF_DETAIL_WIZARD;\n    BrowseDlgData.szDialogBoxCaption = BROWSE_DIALOG_CAPTION;\n\n    //\n    // Add the selected counter to the query.\n    //\n\n    wprintf(L\"\\nCounter selected: %s\\n\", CounterPathBuffer);\n\n    Status = PdhAddCounter(Query, CounterPathBuffer, 0, &amp;Counter);\n    if (Status != ERROR_SUCCESS) \n    {\n        wprintf(L\"\\nPdhAddCounter failed with status 0x%x.\", Status);\n        goto Cleanup;\n    }\n\n    //\n    // Most counters require two sample values to display a formatted value.\n    // PDH stores the current sample value and the previously collected\n    // sample value. This call retrieves the first value that will be used\n    // by PdhGetFormattedCounterValue in the first iteration of the loop\n    // Note that this value is lost if the counter does not require two\n    // values to compute a displayable value.\n    //\n\n    Status = PdhCollectQueryData(Query);\n    while(Status != ERROR_SUCCESS)\n        Status = PdhCollectQueryData(Query);\n    if (Status != ERROR_SUCCESS) \n    {\n        wprintf(L\"\\nPdhCollectQueryData failed with 0x%x.\\n\", Status);\n        goto Cleanup;\n    }\n\n    //\n    // Print counter values until a key is pressed.\n    //\n\n    while (!_kbhit()) \n    {\n        Sleep(SAMPLE_INTERVAL_MS);\n\n        GetLocalTime(&amp;SampleTime);\n\n        Status = PdhCollectQueryData(Query);\n        if (Status != ERROR_SUCCESS) \n        {\n            wprintf(L\"\\nPdhCollectQueryData failed with status 0x%x.\", Status);\n        }\n\n        FILETIME ft;\n        GetSystemTimeAsFileTime(&amp;ft);\n        unsigned long long tt = ft.dwHighDateTime;\n        tt &lt;&lt;=32;\n        tt |= ft.dwLowDateTime;\n        tt /=10;\n        tt -= 11644473600000000ULL;\n        cout&lt;&lt; \"time is \" &lt;&lt; tt / 1000000&lt;&lt;\"\\n\";*/\n\n        wprintf(L\"\\n\\\"%2.2d/%2.2d/%4.4d %2.2d:%2.2d:%2.2d.%3.3d\\\"\",\n            SampleTime.wMonth,\n            SampleTime.wDay,\n            SampleTime.wYear,\n            SampleTime.wHour,\n            SampleTime.wMinute,\n            SampleTime.wSecond,\n            SampleTime.wMilliseconds);\n\n        //\n        // Compute a displayable value for the counter.\n        //\n\n\n    Status = PdhGetFormattedCounterValue(Counter,\n            PDH_FMT_DOUBLE,\n            &amp;CounterType,\n            &amp;DisplayValue);\n\n\n        if (Status != ERROR_SUCCESS) \n        {\n            wprintf(L\"\\nPdhGetFormattedCounterValue failed with status 0x%x.\", Status);\n            goto Cleanup;\n        }\n\n\n        wprintf(L\",\\\"%.20g\\\"\", DisplayValue.doubleValue);\n        if(DisplayValue.doubleValue == 0)\n            sendSignal(\"Process Sleeping\\n\");\n        else{\n            sendSignal(\"Process Context Switching\\n\");\n            getchar();\n        }\n    }\n\nCleanup:\n\n    //\n    // Close the query.\n    //\n\n    if (Query) \n    {\n        PdhCloseQuery(Query);\n    }\n\n    int x;\n    cin &gt;&gt;x;\n}\n</code></pre>\n\n<p>In order to measure the delay between a context switch happening to the process and my program polling it, I have written another program that gets the time immediately before it programmatically runs the specific process for which I am measuring context switches. I then manually compare this time to the time printed by my context switch program above. Here is the code for that program:</p>\n\n<pre><code>// TheOpener.cpp : Defines the entry point for the console application.\n//\n\n#include \"stdafx.h\"\n#include \"PreciseTimer.h\"\n\n#include &lt;fstream&gt;\n#include &lt;iostream&gt;\n#include &lt;windows.h&gt;\n#include &lt;iomanip&gt;\n#include &lt;stdio.h&gt;\n\n#include &lt;string&gt;\n#include &lt;algorithm&gt;\n\nusing namespace std;\nint _tmain(int argc, _TCHAR* argv[])\n{\n    SYSTEMTIME st;\n    GetLocalTime(&amp;st);\n    wprintf(L\"\\n\\\"%2.2d/%2.2d/%4.4d %2.2d:%2.2d:%2.2d.%3.3d\\\"\",\n            st.wMonth,\n            st.wDay,\n            st.wYear,\n            st.wHour,\n            st.wMinute,\n            st.wSecond,\n            st.wMilliseconds);\n\n    system(\"\\\"\\\"c:\\\\program files\\\\tiny.exe\\\"\");\n\n    int x;\n    cin&gt;&gt;x;\n    return 0;\n}\n</code></pre>\n\n<p>From this experiment, I get an average delay of about 50 milliseconds. My actual question is twofold: Is this a valid way of measuring this delay? If so, is there a way to minimize this delay to the order of nanoseconds? If not, could you suggest a more valid way of measuring the delay? The process in question here is called \"tiny.exe\", as indicated in the above code.</p>\n"},{"tags":["c#","performance","fileinfo"],"answer_count":7,"favorite_count":4,"up_vote_count":12,"down_vote_count":0,"view_count":19142,"score":12,"question_id":6061957,"title":"Get all files and directories in specific path fast","body":"<p>I am creating a backup application where c# scans a directory. Before I use to have something like this in order to get all the files and subfiles in a directory:</p>\n\n<pre><code>DirectoryInfo di = new DirectoryInfo(\"A:\\\\\");\nvar directories= di.GetFiles(\"*\", SearchOption.AllDirectories);\n\nforeach (FileInfo d in directories)\n{\n       //Add files to a list so that later they can be compared to see if each file\n       // needs to be copid or not\n}\n</code></pre>\n\n<p>The only problem with that is that sometimes a file could not be accessed and I get several errors. an example of an error that I get is:<img src=\"http://i.stack.imgur.com/5YH4D.png\" alt=\"error\"></p>\n\n<p>As a result I created a recursive method that will scan all files in the current directory. If there where directories in that directory then the method will be called again passing that directory. The nice thing about this method is that I could place the files inside a try catch block giving me the option to add those files to a List if there where no errors and adding the directory to another list if I had errors.</p>\n\n<pre><code>try\n{\n    files = di.GetFiles(searchPattern, SearchOption.TopDirectoryOnly);               \n}\ncatch\n{\n     //info of this folder was not able to get\n     lstFilesErrors.Add(sDir(di));\n     return;\n}\n</code></pre>\n\n<p>So this method works great the only problem is that when I scan a large directory it takes to much times. How could I speed up this process?   My actual method is this in case you need it. </p>\n\n<pre><code>private void startScan(DirectoryInfo di)\n{\n    //lstFilesErrors is a list of MyFile objects\n    // I created that class because I wanted to store more specific information\n    // about a file such as its comparePath name and other properties that I need \n    // in order to compare it with another list\n\n    // lstFiles is a list of MyFile objects that store all the files\n    // that are contained in path that I want to scan\n\n    FileInfo[] files = null;\n    DirectoryInfo[] directories = null;\n    string searchPattern = \"*.*\";\n\n    try\n    {\n        files = di.GetFiles(searchPattern, SearchOption.TopDirectoryOnly);               \n    }\n    catch\n    {\n        //info of this folder was not able to get\n        lstFilesErrors.Add(sDir(di));\n        return;\n    }\n\n    // if there are files in the directory then add those files to the list\n    if (files != null)\n    {\n        foreach (FileInfo f in files)\n        {\n            lstFiles.Add(sFile(f));\n        }\n    }\n\n\n    try\n    {\n        directories = di.GetDirectories(searchPattern, SearchOption.TopDirectoryOnly);\n    }\n    catch\n    {\n        lstFilesErrors.Add(sDir(di));\n        return;\n    }\n\n    // if that directory has more directories then add them to the list then \n    // execute this function\n    if (directories != null)\n        foreach (DirectoryInfo d in directories)\n        {\n            FileInfo[] subFiles = null;\n            DirectoryInfo[] subDir = null;\n\n            bool isThereAnError = false;\n\n            try\n            {\n                subFiles = d.GetFiles();\n                subDir = d.GetDirectories();\n\n            }\n            catch\n            {\n                isThereAnError = true;                                                \n            }\n\n            if (isThereAnError)\n                lstFilesErrors.Add(sDir(d));\n            else\n            {\n                lstFiles.Add(sDir(d));\n                startScan(d);\n            }\n\n\n        }\n\n}\n</code></pre>\n\n<p>Ant the problem if I try to handle the exception with something like:</p>\n\n<pre><code>DirectoryInfo di = new DirectoryInfo(\"A:\\\\\");\nFileInfo[] directories = null;\n            try\n            {\n                directories = di.GetFiles(\"*\", SearchOption.AllDirectories);\n\n            }\n            catch (UnauthorizedAccessException e)\n            {\n                Console.WriteLine(\"There was an error with UnauthorizedAccessException\");\n            }\n            catch\n            {\n                Console.WriteLine(\"There was antother error\");\n            }\n</code></pre>\n\n<p>Is that if an exception occurs then I get no files.</p>\n"},{"tags":["c#",".net","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":106,"score":0,"question_id":12983385,"title":"More efficient way to calculate CPU Usage","body":"<p>So I'm writing a task manager clone, and right now I'm using this to calculate the CPU usage %s of each process. The problem is this is very slow; I was just wondering if there is a way to speed this up.</p>\n\n<p>Also, I'm not allowed to use PerformanceCounter's methods and/or WMI.</p>\n\n<pre><code>//Omitted:\n// - Process[] processes just holds the currently running processes\n// - rows[] is a list of the rows I have in a table which shows the tables data\n// - rows[2] is the column for CPU usage\n// - rows[0] is the column for PID\n//========\n//Do CPU usages\ndouble totalCPUTime = 0;\nforeach (Process p in processes)\n{\n    try\n    {\n        totalCPUTime += p.TotalProcessorTime.TotalMilliseconds;\n    }\n    catch (System.ComponentModel.Win32Exception e)\n    {\n        //Some processes do not give access rights to view their time.\n    }\n}\nforeach (Process p in processes)\n{\n    double millis = 0;\n    try\n    {\n        millis = p.TotalProcessorTime.TotalMilliseconds;\n    }\n    catch (System.ComponentModel.Win32Exception e)\n    {\n        //Some processes do not give access rights to view their time.\n    }\n    double pct = 100 * millis / totalCPUTime;\n    for (int i = 0; i &lt; rows.Count; i++)\n    {\n        if(rows[i].Cells[0].Value.Equals(p.Id))\n        {\n            rows[i].Cells[2].Value = pct;\n            break;\n        }\n    }\n}\n</code></pre>\n"},{"tags":["asp.net-mvc-3","performance","linq-to-sql"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":56,"score":0,"question_id":12973604,"title":"Using LINQ-to-SQL containers","body":"<p>I'm using MVC3 and currently i'm following a practice such that I declare one instance of DB Container for every controller. I use that container instance for every request coming to that controller. If I need to go to my models for a query or sth, I send that instance as a parameter to the model's function. So for the whole application, I create and use 4-5 different instances of DB Container class. My question is, does this have a good or bad effect on my database operations? Does it matter to create a seperate container instance? What is the proper way to use container classes?</p>\n\n<p>I believe the mentioned class was called DBContext before.</p>\n"}]}
{"total":25592,"page":9,"pagesize":100,"questions":[{"tags":["android","performance","sprite","collision"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":33,"score":0,"question_id":12983306,"title":"Android laggs when more than 1 sprite is present","body":"<p>I have a \"simple\" shooting game where I have a cannon in the center bottom that can shoot bitmaps to enemies (sprites). </p>\n\n<p>Now I don't know if its the collision detection or the sprite itself but if I \"spawn\" more than one enemy it starts to lagg, after 15 its completly unplayable.</p>\n\n<p>The only thing the sprite does is moves itself in a direction and changes the picture from a spritesheet and holds some variables, nothing much.</p>\n\n<p>Collision detection is by going through an array of enemies and within that I go through a second array of bullets and for each enemy I check if a bullet has hit it by making a Rect by them both and check for overlaps.</p>\n\n<p>Its a bit of code so I don't know if I should post some of it or not but you can ask me to post it if you need.</p>\n"},{"tags":["php","mysql","query","performance"],"answer_count":10,"favorite_count":1,"up_vote_count":11,"down_vote_count":1,"view_count":1271,"score":10,"question_id":561900,"title":"How many MySQL queries should I limit myself to on a page? PHP / MySQL","body":"<p>Okay, so I'm sure plenty of you have built crazy database intensive pages...  </p>\n\n<p>I am building a page that I'd like to pull all sorts of unrelated database information from.  Here are some sample different queries for this one page:</p>\n\n<ul>\n<li>article content and info</li>\n<li>IF the author is a registered user, their info</li>\n<li>UPDATE the article's view counter</li>\n<li>retrieve comments on the article</li>\n<li>retrieve information for the authors of the comments</li>\n<li>if the reader of the article is signed in, query for info on them</li>\n<li>etc...</li>\n</ul>\n\n<p>I know these are basically going to be pretty lightning quick, and that I could combine some; but I wanted to make sure that this isn't abnormal?</p>\n\n<p>How many fairly normal and un-heavy queries would you limit yourself to on a page?</p>\n"},{"tags":["android","performance","opengl-es","activity"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":140,"score":0,"question_id":10565240,"title":"Android, performance degrading if multiple activities","body":"<p>I have an OpenGL based app, all is working fine so far. The app consists out ouf 2 activities, the main activity containing the OpenGL view and an additional activity that contains a movie player for some cut-scenes.</p>\n\n<p>On some low-end devices (e.g. LG P690, Android 2.3.4) performance degrades significantly after the movie-activity ran. After that the main activity runs so slow that it only reaches about 30 fps instead of 50 fps.</p>\n\n<p>You can \"fix\" it to become fast again, if you leave the app by pushing the home-button and restart it. Then all is fine again - until that movie-view-activity is shown again...</p>\n\n<p>I did some tests and this effect is also happening regardless of what activity #2 actually does, even if it is just a splash-screen or whatever. If I never invoke a second activity then the performance is always high.</p>\n\n<p>Note: the OpenGL activity is <em>not</em> instanciated multiple times, which would probably lead to a similar effect.</p>\n\n<p>Has somebody experienced the same or a similar issue and knows a remedy?</p>\n"},{"tags":["mysql","performance","greatest-n-per-group","filesort"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":83,"score":3,"question_id":12980823,"title":"Improve performance or redesign 'greatest-n-per-group' mysql query","body":"<p>I'm using MySQL5 and I currently have a query that gets me the info I need but I feel like it could be improved in terms of performance.</p>\n\n<p>Here's the query I built (roughly following <a href=\"http://murrayhopkins.wordpress.com/2008/10/28/mysql-left-join-on-last-or-first-record-in-the-right-table/\" rel=\"nofollow\" title=\"http://murrayhopkins.wordpress.com/2008/10/28/mysql-left-join-on-last-or-first-record-in-the-right-table/\">this guide</a>) :</p>\n\n<pre><code>SELECT d.*, dc.date_change, dc.cwd, h.name as hub\nFROM livedata_dom AS d\n      LEFT JOIN ( SELECT dc1.*\n        FROM livedata_domcabling as dc1\n        LEFT JOIN livedata_domcabling AS dc2\n        ON dc1.dom_id = dc2.dom_id AND dc1.date_change &lt; dc2.date_change\n        WHERE dc2.dom_id IS NULL\n        ORDER BY dc1.date_change desc) AS dc ON (d.id = dc.dom_id)\n      LEFT JOIN livedata_hub AS h ON (d.id = dc.dom_id AND dc.hub_id = h.id)\nWHERE d.cluster = 'localhost'\nGROUP BY d.id;\n</code></pre>\n\n<p>EDIT: Using ORDER BY + GROUP BY to avoid getting multiple dom entries in case 'domcabling' has an entry with null date_change and another one with a date for the same 'dom'.</p>\n\n<p>I feel like I'm killing a mouse with a bazooka. This query takes more than 3 seconds with only about 5k entries in 'livedata_dom' and 'livedata_domcabling'. Also, EXPLAIN tells me that 2 filesorts are used:</p>\n\n<pre><code>+----+-------------+------------+--------+-----------------------------+-----------------------------+---------+-----------------+------+----------------------------------------------+\n| id | select_type | table      | type   | possible_keys               | key                         | key_len | ref             | rows | Extra                                        |\n+----+-------------+------------+--------+-----------------------------+-----------------------------+---------+-----------------+------+----------------------------------------------+\n|  1 | PRIMARY     | d          | ALL    | NULL                        | NULL                        | NULL    | NULL            |    3 | Using where; Using temporary; Using filesort |\n|  1 | PRIMARY     | &lt;derived2&gt; | ALL    | NULL                        | NULL                        | NULL    | NULL            |    3 |                                              |\n|  1 | PRIMARY     | h          | eq_ref | PRIMARY                     | PRIMARY                     | 4       | dc.hub_id       |    1 |                                              |\n|  2 | DERIVED     | dc1        | ALL    | NULL                        | NULL                        | NULL    | NULL            |    4 | Using filesort                               |\n|  2 | DERIVED     | dc2        | ref    | livedata_domcabling_dc592d9 | livedata_domcabling_dc592d9 | 4       | live.dc1.dom_id |    2 | Using where; Not exists                      |\n+----+-------------+------------+--------+-----------------------------+-----------------------------+---------+-----------------+------+----------------------------------------------+ \n</code></pre>\n\n<p>How could I change this query to make it more efficient?</p>\n\n<p>Using the dummy data (provided below), this is the expected result:</p>\n\n<pre><code>+-----+-------+---------+--------+----------+------------+-----------+---------------------+------+-----------+\n| id  | mb_id | prod_id | string | position | name       | cluster   | date_change         | cwd  | hub       |\n+-----+-------+---------+--------+----------+------------+-----------+---------------------+------+-----------+\n| 249 | 47    | 47      |     47 |       47 | SuperDOM47 | localhost | NULL                | NULL | NULL      |\n| 250 | 48    | 48      |     48 |       48 | SuperDOM48 | localhost | 2014-04-16 05:23:00 | 32A  | megahub01 |\n| 251 | 49    | 49      |     49 |       49 | SuperDOM49 | localhost | NULL                | 22B  | megahub01 |\n+-----+-------+---------+--------+----------+------------+-----------+---------------------+------+-----------+\n</code></pre>\n\n<p>Basically I need 1 row for every 'dom' entry, with </p>\n\n<ol>\n<li>the 'domcabling' record with the highest date_change\n<ul>\n<li>if record does not exist, I need null fields</li>\n<li>ONE entry may have a null date_change field per dom (null datetime field considered older than any other datetime)</li>\n</ul></li>\n<li>the name of the 'hub', when a 'domcabling' entry is found, null otherwise</li>\n</ol>\n\n<p>CREATE TABLE + dummy INSERT for the 3 tables:</p>\n\n<p><strong>livedata_dom</strong> (about 5000 entries)</p>\n\n<pre><code>CREATE TABLE `livedata_dom` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `mb_id` varchar(12) NOT NULL,\n  `prod_id` varchar(8) NOT NULL,\n  `string` int(11) NOT NULL,\n  `position` int(11) NOT NULL,\n  `name` varchar(30) NOT NULL,\n  `cluster` varchar(9) NOT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `mb_id` (`mb_id`),\n  UNIQUE KEY `prod_id` (`prod_id`),\n  UNIQUE KEY `name` (`name`),\n  UNIQUE KEY `livedata_domgood_string_7bff074107b0e5a0_uniq` (`string`,`position`,`cluster`)\n) ENGINE=InnoDB AUTO_INCREMENT=5485 DEFAULT CHARSET=latin1;\n\nINSERT INTO `livedata_dom` VALUES (251,'49','49',49,49,'SuperDOM49','localhost'),(250,'48','48',48,48,'SuperDOM48','localhost'),(249,'47','47',47,47,'SuperDOM47','localhost');\n</code></pre>\n\n<p><strong>livedata_domcabling</strong> (about 10000 entries and growing slowly)</p>\n\n<pre><code>CREATE TABLE `livedata_domcabling` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `dom_id` int(11) NOT NULL,\n  `hub_id` int(11) NOT NULL,\n  `cwd` varchar(3) NOT NULL,\n  `date_change` datetime DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `livedata_domcabling_dc592d9` (`dom_id`),\n  KEY `livedata_domcabling_4366aa6e` (`hub_id`),\n  CONSTRAINT `dom_id_refs_id_73e89ce0c50bf0a6` FOREIGN KEY (`dom_id`) REFERENCES `livedata_dom` (`id`),\n  CONSTRAINT `hub_id_refs_id_179c89d8bfd74cdf` FOREIGN KEY (`hub_id`) REFERENCES `livedata_hub` (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=5397 DEFAULT CHARSET=latin1;\n\nINSERT INTO `livedata_domcabling` VALUES (1,251,1,'22B',NULL),(2,250,1,'33A',NULL),(6,250,1,'32A','2014-04-16 05:23:00'),(5,250,1,'22B','2013-05-22 00:00:00');\n</code></pre>\n\n<p><strong>livedata_hub</strong> (about 100 entries)</p>\n\n<pre><code>CREATE TABLE `livedata_hub` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `name` varchar(14) NOT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `name` (`name`)\n) ENGINE=InnoDB AUTO_INCREMENT=98 DEFAULT CHARSET=latin;\n\nINSERT INTO `livedata_hub` VALUES (1,'megahub01');\n</code></pre>\n"},{"tags":["c#","asp.net-mvc","performance","iis7","httpmodule"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":57,"score":0,"question_id":12981298,"title":"How to analyze the performance of requests in ASP.NET MVC application?","body":"<p>I would like to capture the hit time, processing time, memory consumption and response time of requests in ASP.NET MVC application.</p>\n\n<p>Is there any way or tool to perform this?</p>\n"},{"tags":["ruby-on-rails","ruby","performance","rubygems"],"answer_count":2,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":65,"score":4,"question_id":12979171,"title":"How number of gems influences on Rails app performance?","body":"<p>I'm new to Rails and this maybe a dumb question, but I wonder how number of gems influences on Rails app performance? Does it become slower the more gems you add? Are all gems get called on every request? </p>\n\n<p>I'm asking this question because, for example, in Django, you import every needed class/method/library in every .py file which calls it. In Rails you're not doing this, everything is \"autoloaded\", but I wonder, what is the cost of such \"autoloading\"? </p>\n\n<p>Does it mean all gems get's called on every request?</p>\n"},{"tags":["c++","performance","sse"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":131,"score":3,"question_id":12971139,"title":"SSE cumulative summing","body":"<p>I have a simple problem. Having a starting uint_32 value (say 125) and a __m128i of operands to add, for example (+5,+10,-1,-5). What I would like to get as fast as possible is a vector (125 + 5, 125 + 5 + 10, 125 + 5 + 10 - 1, 125 + 5 + 10 - 1 - 5), i.e. cumulatively add values from the operands to the starting value. So far the only solution I can think of is doing an addition of 4 __m128i variables. For the example, they would be</p>\n\n<pre><code>/* pseudoSSE code... */\n__m128i src =     (125,125,125,125)\n__m128i operands =(5,10,-1,-5)\n\n/*  Here I omit the partitioning of operands into add1,..add4 for brevity  */\n\n__m128i add1 =    (+05,+05,+05,+05)\n__m128i add2 =    (+00,+10,+10,+10)\n__m128i add3 =    (+00,+00,-01,-01)\n__m128i add4 =    (+00,+00,+00,-05)\n__m128i res1 = _mm_add_epu32( add1, add2 )\n__m128i res2 = _mm_add_epu32( add3, add4 )\n__m128i res3 = _mm_add_epu32( res1, add2 )\n__m128i res  = _mm_add_epu32( res3, src  )\n</code></pre>\n\n<p>Like this, I get what I wanted. For this solution I am going to need to set all add_ variables and then perform 4 additions. What Im really asking is whether this can be done faster. Either via some different algo or maybe using some specialized SSE functions that I do not know yet (something like _mm_cumulative_sum()). Many thanks.</p>\n"},{"tags":["performance","html5","animation","canvas"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":669,"score":3,"question_id":5387706,"title":"What is the fastest way of animating multiple sprites under html5?","body":"<p>I'm trying to animate 50 or so sprites at the same time, using a setInterval of 30ms.</p>\n\n<p>What gives bettter performance?\nUsing canvas? Or using -webkit-transform and divs? Does anyone have any tips on making animations for html5?</p>\n"},{"tags":["java","performance","jvm","hotspot"],"answer_count":2,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":102,"score":4,"question_id":12980029,"title":"Oracle Hotspot JVM: generally, what operations are especially CPU-costly?","body":"<p>I'd like to understand of what types of operations contribute disproportionately to CPU load as well as develop an intuition on relative cost of common operations. To minimize generalizations, please assume Oracle 7 HotSpot JVM. </p>\n\n<p>For example:</p>\n\n<ul>\n<li>does constructing lots of objects cost CPU (I understand it costs memory :-) )?</li>\n<li>does contenting for a monitor cost CPU? ie if we have multiple threads attempting to enter the same synchronized block, do blocked threads also consume CPU cycles? </li>\n<li>relative cost of above operations? For example, \"new'ing a single object costs the same CPU as iterating over a X-element array\"</li>\n</ul>\n\n<p>Any tips on developing an intuition of relative CPU cost of typical operations? </p>\n\n<p>Any good reads on the subject you could recommend?</p>\n\n<p>thank you,</p>\n\n<p><strong>CLARIFICATION</strong></p>\n\n<p>thanks for early responses, but please note I:</p>\n\n<ul>\n<li>am NOT asking 'why is my app slow'</li>\n<li>understand that using a profiler will help identify problems in a specific app and that for example, GC can eat up CPU or that GC'ing tenured generation is more costly than Eden space</li>\n<li>understand that most ops become costly only if executed a lot (ie virtually no op is expensive if used sparingly)</li>\n</ul>\n\n<p>Instead, I am looking for guidance of <em>relative</em> CPU cost, especially w.r.t. above operations (let's assume a 'web-scale' app uses all ops mentioned equal amount - a lot). </p>\n\n<p>For example I already now that:</p>\n\n<ul>\n<li>long method call chains do not contribute significantly to CPU load (so it's generally OK to use method delegation liberally)</li>\n<li>throwing exceptions is more expensive than using conditionals (thus latter is generally preferred for flow-control in highly performance-sensitive code)</li>\n</ul>\n\n<p>...but what about instantiating new objects or contenting for a monitor? Would either of these ops be significant (dominant?) contributors to CPU load (let's say I don't care about latency of heap size) at scale?</p>\n"},{"tags":["javascript","jquery","performance","httprequest","cdn"],"answer_count":16,"favorite_count":44,"up_vote_count":118,"down_vote_count":1,"view_count":28996,"score":117,"question_id":1447184,"title":"Microsoft CDN for jQuery or Google CDN?","body":"<p>Does it actually matter which CDN you use to link to your jquery file or any javascript file for that matter.  Is one potentially faster than the other?  What other factors could play a role in which cdn you decide to use?  I know that Microsoft, Yahoo, and Google all have CDN's now.</p>\n"},{"tags":["ajax","performance","magento"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":46,"score":2,"question_id":12978579,"title":"Is it relevant enough to create a direct .php file as an AJAX target in Magento (instead of standard router path)","body":"<p>I will explain a bit of background here. The AJAX I'm going to use must work very fast, implementing some logic on the backend and returning JSON as response.</p>\n\n<p>I'm not new in Magento development, but I've struggled to create a <em>cheap</em> request using standard Magento request flow. Using profiler I've discovered, that only the <strong>routing</strong> work (including <code>match</code>, <code>preDispatch</code>, <code>rewrite</code> and more light-weighted, but numerous small routing-related functions)  takes almost a second.</p>\n\n<p>Have no doubts, I'm loading all the necessary data to bootstrap application, my script looks very much like scripts in the <code>shell</code> directory.</p>\n\n<p>So using my own <em>.php</em> file instead of Magento router I'm already saving almost a second per request - without even touching the logic. My benchmarking may be not very accurate, but the point is - I'm definitely saving some time, when time is very essential for me.</p>\n\n<p>So the questions are: <strong>is this 1 sec worth neglecting of Magento architecture</strong>? Has anyone implemented something similar? And where is the best place to put such file into, considering Magento Module approach?</p>\n\n<p>I'll be glad, if anyone can point me in the right direction.</p>\n"},{"tags":["php","performance","xdebug","profiler"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":29,"score":0,"question_id":12979830,"title":"PHP performance Target with Xdebug","body":"<p>I begin to work with Xdebug and WinCacheGrind to understand more about the code I wrote.</p>\n\n<p>I'm currently testing a Shopping Cart Object that uses MySQL to store as a persistent session.</p>\n\n<p>Here are the steps the object does on a typical Add to Cart Action:</p>\n\n<ol>\n<li>Construct a Cart Session filled with default values</li>\n<li>Check for an existing Cart Session In MYSQL with\n$_COOKIE['session_id'] AND $_SERVER['REMOTE_ADDR']. If so, mysql\nrow populates the Cart Session  <em>(12ms)</em></li>\n<li>Set Country Code and State Code in the Cart for furter shipping calculation.</li>\n<li>Add an item</li>\n<li>Add item options</li>\n<li>Get Shipping Options (Regular, Express, NextDay) from MYSQL based on Country Code and State Code <em>(9.1ms)</em></li>\n<li>Calculate Shipping Cost for each Options Based on Weight items in the Cart</li>\n<li>Set Discount <em>(0.1ms)</em></li>\n<li>Set User Prefered Shipping Option ex. regular;</li>\n<li>Save Cart Session in MYSQL <em>(93ms)</em>, using php function serialize for cart content.</li>\n<li>Display Cart values in the VIEW.</li>\n</ol>\n\n<p>The only call to db are on step 2, 6, 11.</p>\n\n<p>There will be of course extra DB call to get Item Details, Item Options and Discount Code. But for the example, I keep it minimal.</p>\n\n<p>For This PHP Request, XDebug give a result of \n<strong>Cumulative Time : 130ms.</strong></p>\n\n<p>Is it bad?</p>\n\n<p>And my real question would be, How Fast should a Request should be in \"ms\"? I heard about YouTube who target 200ms Total but, I'm not Google and don't have this team of ultra super genius laser Intelligent 2055 back from the future engineers...</p>\n\n<p>Thanks for the help.</p>\n\n<p>C.</p>\n"},{"tags":["mysql","performance","entity-framework"],"answer_count":3,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":5407,"score":3,"question_id":1233245,"title":"How to optimize Entity Framework Queries","body":"<p>I am using Linq-To-Entities to do a query which is returning only 947 rows but taking 18 seconds to run. I have done a \"ToTraceString\" to get the underlying sql out and ran the same thing directly on the database and get the same timing. </p>\n\n<p>I have used the tuning advisor and created a couple of indexes although with little impact.</p>\n\n<p>Looking at the query execution plan there are a couple of nested loops which are taking up 95% of the time but these are already working on the indexes?</p>\n\n<p>Does anyone have any ideas on how to force some optimisation into the EF query??</p>\n\n<p>EDIT: Supplying additional information</p>\n\n<p>A basic ER diagram with for the three tables is as follows:</p>\n\n<pre><code>People &gt;----People_Event_Link ----&lt; Events\nP_ID        P_ID                    E_ID\n            E_ID\n</code></pre>\n\n<p>The linq that I am running is designed to get all Events back for a particular Person (using the P_ID):</p>\n\n<pre><code>        var query = from ev in genesisContext.Events\n                    join pe in genesisContext.People_Event_Link\n                    on ev equals pe.Event\n                    where pe.P_ID == key\n                    select ev;\n        return query;\n</code></pre>\n\n<p>Here is the generated SQL (deep breath!):</p>\n\n<pre><code>SELECT \n1 AS [C1], \n[Extent1].[E_ID] AS [E_ID], \n[Extent1].[E_START_DATE] AS [E_START_DATE], \n[Extent1].[E_END_DATE] AS [E_END_DATE], \n[Extent1].[E_COMMENTS] AS [E_COMMENTS], \n[Extent1].[E_DATE_ADDED] AS [E_DATE_ADDED], \n[Extent1].[E_RECORDED_BY] AS [E_RECORDED_BY], \n[Extent1].[E_DATE_UPDATED] AS [E_DATE_UPDATED], \n[Extent1].[E_UPDATED_BY] AS [E_UPDATED_BY], \n[Extent1].[ET_ID] AS [ET_ID], \n[Extent1].[L_ID] AS [L_ID]\nFROM  [dbo].[Events] AS [Extent1]\nINNER JOIN [dbo].[People_Event_Link] AS [Extent2] ON  EXISTS (SELECT \n    1 AS [C1]\n    FROM    ( SELECT 1 AS X ) AS [SingleRowTable1]\n    LEFT OUTER JOIN  (SELECT \n    \t[Extent3].[E_ID] AS [E_ID]\n    \tFROM [dbo].[Events] AS [Extent3]\n    \tWHERE [Extent2].[E_ID] = [Extent3].[E_ID] ) AS [Project1] ON 1 = 1\n    LEFT OUTER JOIN  (SELECT \n    \t[Extent4].[E_ID] AS [E_ID]\n    \tFROM [dbo].[Events] AS [Extent4]\n    \tWHERE [Extent2].[E_ID] = [Extent4].[E_ID] ) AS [Project2] ON 1 = 1\n    WHERE ([Extent1].[E_ID] = [Project1].[E_ID]) OR (([Extent1].[E_ID] IS NULL) AND ([Project2].[E_ID] IS NULL))\n)\nWHERE [Extent2].[P_ID] = 291\n</code></pre>\n"},{"tags":["performance","html5","canvas","android-browser","frame-rate"],"answer_count":1,"favorite_count":2,"up_vote_count":6,"down_vote_count":0,"view_count":99,"score":6,"question_id":12921018,"title":"Canvas rendering performance","body":"<p>I am modifying the HTML5 port of the game Jump'n'Bump to run on Apple and Android-based mobile devices. I use a cheap 1 GHz Cortex-A8 Android 4.0.3 tablet for testing. I have encountered strange behaviour in the system's Browser. I normally get a very low frame-rate of about 1 FPS (entire screen is re-drawn every frame, setTimeout is used...). However, when I add a &lt;div&gt; which has a position:fixed CSS attribute before the &lt;canvas&gt; tag, the frame-rate skyrockets and the game becomes playable.</p>\n\n<p>Could someone please explain this odd phenomenon? Are there some rendering modes in the Android Browser which influence canvas performance? Is this a cross-platform issue? How do I make sure the page works efficiently in the user's browser?</p>\n\n<p>An outline of the code I'm working on:</p>\n\n<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;title&gt;Jump'n'Bump - HTML5&lt;/title&gt;\n&lt;meta http-Equiv=\"Cache-Control\" Content=\"no-cache\"&gt;\n&lt;meta http-Equiv=\"Pragma\" Content=\"no-cache\"&gt;\n&lt;meta http-Equiv=\"Expires\" Content=\"0\"&gt;\n&lt;meta name=\"viewport\" content = \"width=400px, user-scaleable=no\"&gt;\n\n&lt;!-- javascript files included here --&gt;\n&lt;script type=\"text/javascript\" src=\"main.js\"&gt;&lt;/script&gt;\n\n&lt;style type=\"text/css\"&gt;\n  body { margin: 0px 0px 0xp 0px }\n  canvas { border: 0px solid black; }\n  img.resource { display:none; }\n  #fixed_div { position: fixed; width: 10px; height: 10px; left: 0px; top: 0px; }\n  #gameArea { position: absolute; left: 0px; top: 0px; width: 400px; height: 256px; background: red; }\n  canvas {\n    image-rendering: optimizeSpeed;             // Older versions of FF\n    image-rendering: -moz-crisp-edges;          // FF 6.0+\n    image-rendering: -webkit-optimize-contrast; // Webkit\n    image-rendering: optimize-contrast;         // Possible future browsers.\n    -ms-interpolation-mode: nearest-neighbor;   // IE\n  }  \n&lt;/style&gt;\n&lt;body onload=\"init()\" text=\"#FFFFFF\" bgcolor=\"#000000\"&gt;\n\n&lt;!-- image resources like this here: --&gt;\n&lt;img class=\"resource\" id='rabbits' src='rabbit.png'/&gt;\n\n&lt;!-- *** remove the line below and the game slows down *** --&gt;\n&lt;div id='fixed_div'&gt;&lt;/div&gt;\n\n&lt;div id=\"gameArea\"&gt;&lt;canvas id=\"screen\" width=\"400\" height=\"256\"&gt;&lt;/canvas&gt;&lt;/div&gt; \n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n"},{"tags":[".net","performance","64bit","32bit-64bit","infragistics"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":81,"score":0,"question_id":12979774,"title":".Net 4.0 app slower on 64 bit than 32 bit (Profiling and possible solutions) (app is using NetAdvantage)","body":"<p>We have got .NET app written in VB .NET 4.0 / VS2010, compiled with all projects set to the AnyCPU setting for both Debug and Release configuration. We have noticed that when this app is run on a 64 bit environment (tested on Windows Server 2003 R2 and 2008 R2) that the app then takes at least double as long (in absolute terms about 25 secs) as opposed to around 6-12 seconds on a 32 bit environment (Win XP and 7) to start up.</p>\n\n<p>I should add that the 64 bit systems are powerful servers, definitely more powerful than the other tested 32 bit systems. All other apps were faster on 64 bit, but not our poor app ;) (And we did test the apps at different times, under different load, and the results are always pretty much the same.)</p>\n\n<p>As said above, the app is built using AnyCPU and it does run as 64 bit assembly under 64 bit OS (checked via TaskManager). The app itself is a WinForms app, making use of NetAdvantage Forms v10.3 and is regularly querying and writing to MS SQL Server 2008.</p>\n\n<p>The different target machines are all on the same network, so the path to the database (same database was used for the performance tests) for example is the same, I don't think the problem is around the database or the network itself. </p>\n\n<p>One thing I did notice, which seemed odd to me, was that when I built in different \"profiling steps\" using a stopwatch during the startup of our MainForm that the InitializeComponent method took twice as long on 64 bit, around 4 seconds opposed to 1.5 on 32 bit.</p>\n\n<p>It's the very same app we deploy on both systems, no different config.</p>\n\n<p>So I have got two questions:</p>\n\n<p>Any idea what the cause of this could be?</p>\n\n<p>And: What's the best way of determining the \"offending\" pieces of code? Currently I use stopwatches and try to narrow it down. But it looks to me like everything is slower on the 64 bit machines as far as our app is concerned, so I am not too sure I can break it down to specific statements.</p>\n\n<p>Thanks all for your help, very much appreciated...</p>\n"},{"tags":["php","mysql","database","performance","overhead"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":61,"score":0,"question_id":12931446,"title":"MySQL connections and PHP","body":"<p>I have a somewhat general question regarding what's best when programming in PHP using also database connections. I am building a project which includes several modules and each module needs to connect to the MySQL sometimes. The module files are included in the main index.php depending on the action selected from the menu by the user. I guess, most projects work this way anyway.</p>\n\n<p>So far what I do is always open the connection at the start of each module file and close it after the queries have run.</p>\n\n<p>My question is this: is it better to open the connection to the database in the beginning of the index.php and close it in the end so to have 1 connection open, or do multiple connections which stay open for less time? What's best for speed and overhead?</p>\n"},{"tags":["java","networking","tcp","sockets","performance"],"answer_count":10,"favorite_count":6,"up_vote_count":7,"down_vote_count":0,"view_count":13563,"score":7,"question_id":1169739,"title":"Java TCP socket: data transfer is slow","body":"<p>I set up a server with a ServerSocket, connect to it with a client machine.  They're directly networked through a switch and the ping time is &lt;1ms.</p>\n\n<p>Now, I try to push a \"lot\" of data from the client to the server through the socket's output stream.  It takes 23 minutes to transfer 0.6Gb.  I can push a much larger file in seconds via scp.</p>\n\n<p>Any idea what I might be doing wrong?  I'm basically just looping and calling writeInt on the socket.  The speed issue doesn't matter where the data is coming from, even if I'm just sending a constant integer and not reading from disk.</p>\n\n<p>I tried setting the send and receive buffer on both sides to 4Mb, no dice.  I use a buffered stream for the reader and writer, no dice.</p>\n\n<p>Am I missing something?</p>\n\n<p>EDIT: code</p>\n\n<p>Here's where I make the socket</p>\n\n<pre><code>System.out.println(\"Connecting to \" + hostname);\n\n\tserverAddr = InetAddress.getByName(hostname);\n\n\t// connect and wait for port assignment\n\tSocket initialSock = new Socket();\n\tinitialSock.connect(new InetSocketAddress(serverAddr, LDAMaster.LDA_MASTER_PORT));\n\tint newPort = LDAHelper.readConnectionForwardPacket(new DataInputStream(initialSock.getInputStream()));\n\tinitialSock.close();\n\tinitialSock = null;\n\n\tSystem.out.println(\"Forwarded to \" + newPort);\n\n\t// got my new port, connect to it\n\tsock = new Socket();\n\tsock.setReceiveBufferSize(RECEIVE_BUFFER_SIZE);\n\tsock.setSendBufferSize(SEND_BUFFER_SIZE);\n\tsock.connect(new InetSocketAddress(serverAddr, newPort));\n\n\tSystem.out.println(\"Connected to \" + hostname + \":\" + newPort + \" with buffers snd=\" + sock.getSendBufferSize() + \" rcv=\" + sock.getReceiveBufferSize());\n\n\t// get the MD5s\n\ttry {\n\t\tbyte[] dataMd5 = LDAHelper.md5File(dataFile),\n\t\t\t   indexMd5 = LDAHelper.md5File(indexFile);\n\n\t\tlong freeSpace = 90210; // ** TODO: actually set this **\n\n\t\toutput = new DataOutputStream(new BufferedOutputStream(sock.getOutputStream()));\n\t\tinput  = new DataInputStream(new BufferedInputStream(sock.getInputStream()));\n</code></pre>\n\n<p>Here's where I do the server-side connection:</p>\n\n<pre><code>\tServerSocket servSock = new ServerSocket();\n\tservSock.setSoTimeout(SO_TIMEOUT);\n\tservSock.setReuseAddress(true);\n\tservSock.bind(new InetSocketAddress(LDA_MASTER_PORT));\n\n\tint currPort = LDA_START_PORT;\n\n\twhile (true) {\n\t\ttry {\n\t\t\tSocket conn = servSock.accept();\n\t\t\tSystem.out.println(\"Got a connection.  Sending them to port \" + currPort);\n\t\t\tclients.add(new MasterClientCommunicator(this, currPort));\n\t\t\tclients.get(clients.size()-1).start();\n\n\t\t\tThread.sleep(500);\n\n\t\t\tLDAHelper.sendConnectionForwardPacket(new DataOutputStream(conn.getOutputStream()), currPort);\n\n\t\t\tcurrPort++;\n\t\t} catch (SocketTimeoutException e) {\n\t\t\tSystem.out.println(\"Done listening.  Dispatching instructions.\");\n\t\t\tbreak;\n\t\t}\n\t\tcatch (IOException e) {\n\t\t\te.printStackTrace();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n</code></pre>\n\n<p>Alright, here's where I'm shipping over ~0.6Gb of data.</p>\n\n<pre><code>public static void sendTermDeltaPacket(DataOutputStream out, TIntIntHashMap[] termDelta) throws IOException {\n\tlong bytesTransferred = 0, numZeros = 0;\n\n\tlong start = System.currentTimeMillis();\n\n\tout.write(PACKET_TERM_DELTA); // header\t\t\n\tout.flush();\n\tfor (int z=0; z &lt; termDelta.length; z++) {\n\t\tout.writeInt(termDelta[z].size()); // # of elements for each term\n\t\tbytesTransferred += 4;\n\t}\n\n\tfor (int z=0; z &lt; termDelta.length; z++) {\n\t\tfor (int i=0; i &lt; termDelta[z].size(); i++) {\n\t\t\tout.writeInt(1);\n\t\t\tout.writeInt(1);\n\t\t}\n\t}\n</code></pre>\n\n<p>It seems pretty straightforward so far...</p>\n"},{"tags":["performance","flash","animation","adobe"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":49,"score":1,"question_id":12977553,"title":"most effieciant way to run a flash animation/application for a long period of time","body":"<p>I need to run a flash animation, (swf file) for about 12 hours. This is for a exhibition.</p>\n\n<p>Due to constraint outside of my control Im running this from a laptop (currently windows vista(!) but I can reinstall all but mac os's onto it, through a VGA, at a 1024x768 resolution.</p>\n\n<p>Does anyone have any previus experience of best practices/advice on this?</p>\n\n<p>would any os be better exquiped, am I better with a comandline linux running only a flash player? would there be any benifit converting the swf to native code and running it?</p>\n\n<p>any pitfalls you can think of?</p>\n\n<p>any advice greatly appriciated.</p>\n\n<p>N.b I hve seen the question <a href=\"http://stackoverflow.com/questions/1504580/running-the-flash-player-over-long-time-period\">Running the Flash Player over long time period</a> however I'm looking for best practices and tips etc on a wide range of topics</p>\n\n<p>If anyone feels another 'overflow' would get me better advice please let me know</p>\n\n<p>thanks for any suggestions</p>\n\n<p>Andrew</p>\n\n<p>EDIT:</p>\n\n<p>just come across this:\n<a href=\"http://www.adobe.com/devnet/flash/articles/efficiency-tips.html\" rel=\"nofollow\">http://www.adobe.com/devnet/flash/articles/efficiency-tips.html</a>\nSome good tips here on optimizing the file contents.</p>\n"},{"tags":["c++","linux","performance","g++","intel"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":34,"score":1,"question_id":12977128,"title":"fmaf weird performance","body":"<p>I'm experiencing an huge performance decrease using the <code>fmaf</code> function over the usage of <code>*</code> and <code>+</code>. I'm on two Linux machines and using g++ 4.4.3 and g++ 4.6.3</p>\n\n<p>On two different machines the following code runs faster if the <code>myOut</code> vector is filled without the usage of <code>fmaf</code>.</p>\n\n<p>server with g++ 4.6.3 and Intel(R) Xeon(R) CPU E5-2650 @ 2.00GHz</p>\n\n<pre><code>$ ./a.out fmaf\nTime: 1.55008 seconds.\n$ ./a.out muladd\nTime: 0.403018 seconds.\n</code></pre>\n\n<p>server with g++ 4.4.3 and Intel(R) Xeon(R) CPU X5650  @ 2.67GHz</p>\n\n<pre><code>$ ./a.out fmaf\nTime: 0.547544 seconds.\n$ ./a.out muladd\nTime: 0.34955 seconds.\n</code></pre>\n\n<p>Shouldn't the <code>fmaf</code> version (apart to avoid an extra roundup and then be more precise) be faster?</p>\n\n<pre><code>#include &lt;stddef.h&gt;\n#include &lt;iostream&gt;\n#include &lt;math.h&gt;\n#include &lt;string.h&gt;\n#include &lt;stdlib.h&gt;\n\n#include &lt;sys/time.h&gt;\n\nint main(int argc, char** argv) {\n  if (argc != 2) {\n    std::cout &lt;&lt; \"missing parameter: 'muladd' or 'fmaf'\"\n              &lt;&lt; std::endl;\n    exit(-1);\n  }\n  struct timeval start,stop,result;\n  const size_t mySize = 1e6*100;\n\n  float* myA = new float[mySize];\n  float* myB = new float[mySize];\n  float* myC = new float[mySize];\n  float* myOut = new float[mySize];\n\n  gettimeofday(&amp;start,NULL);\n  if (!strcmp(argv[1], \"muladd\")) {\n    for (size_t i = 0; i &lt; mySize; ++i) {\n      myOut[i] = myA[i]*myB[i]+myC[i];\n    }\n  } else if (!strcmp(argv[1], \"fmaf\")) {\n    for (size_t i = 0; i &lt; mySize; ++i) {\n      myOut[i] = fmaf(myA[i], myB[i], myC[i]);\n    }\n  } else {\n    std::cout &lt;&lt; \"specify 'muladd' or 'fmaf'\" &lt;&lt; std::endl;\n    exit(-1);\n  }\n\n  gettimeofday(&amp;stop,NULL);\n  timersub(&amp;stop,&amp;start,&amp;result);\n  std::cout &lt;&lt; \"Time: \" &lt;&lt;  result.tv_sec + result.tv_usec/1000.0/1000.0\n            &lt;&lt; \" seconds.\" &lt;&lt; std::endl;\n\n  delete []myA;\n  delete []myB;\n  delete []myC;\n  delete []myOut;\n}\n</code></pre>\n"},{"tags":["performance","oracle","query","database-design","data-warehouse"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":1,"view_count":65,"score":0,"question_id":12973618,"title":"Oracle explain plan over simple select performs multiple hash joins when multiple columns are indexed in a table","body":"<p>I am currently running into an issue with my Oracle instance.  I have two simple select statements:  </p>\n\n<pre><code>select * from dog_vets  \n</code></pre>\n\n<p>and  </p>\n\n<pre><code>select * from dog_statuses\n</code></pre>\n\n<p>and the following <a href=\"http://sqlfiddle.com/#!4/37754\" rel=\"nofollow\">fiddle</a></p>\n\n<p>My explain plan on <code>dog_vets</code> is as follows:  </p>\n\n<pre><code> 0 | Select Statement  \n 1 | Table Access Full Scan dog_vets\n</code></pre>\n\n<p>my explain plan on <code>dog_statuses</code>  is as follows:  </p>\n\n<pre><code>ID|Operation | Name | Rows |Bytes | cost | time    \n0 | Select Statement |  | 20G | 500M | 100000 | 999:99:17  \n1 | View  | index%_join_001 | 20G | 500M | 100000 | 999:99:17  \n2 | Hash Join  |  | | | |   \n3 | Hash Join  |  | | | |   \n4 | Index fast full scan dog_statuses_check_up  |  | 20G | 500M | 100000 | 32:15:00  \n5 | Index fast full scan dog_statuses_sick|  | 20G | 500M | 100000 | 35:19:00  \n</code></pre>\n\n<p>To get this type of output execute the following statement:  </p>\n\n<pre><code>explain plan for   \nselect * from dog_vets;\n</code></pre>\n\n<p>OR</p>\n\n<pre><code> explain plan for   \n    select * from dog_statuses;\n</code></pre>\n\n<p>and then  </p>\n\n<pre><code>select * from table(dbms_xplan.display);\n</code></pre>\n\n<p>Now my question is, why do multiple indexes imply a view (materialized  I assume) being created in my above statements and further what type of performance hit am I suffering on this type of query?  As it stands now <code>dog_vets</code> has ~300 million records and <code>dog_Statuses</code> has about 500 million.  I have yet to be able to get <code>select * from dog_statuses</code> to return in under 10 hours.  This is primarily because the query dies before it completes.  </p>\n\n<p><strong>DDL</strong>  </p>\n\n<p>In case sql fiddle dies:  </p>\n\n<pre><code>create table dog_vets\n(\n     name varchar2(50),\n     founded timestamp,\n     staff_count number\n  );\n\ncreate table dog_statuses\n(\n      check_up timestamp,\n      sick varchar2(1)\n  );\n\n\ncreate index dog_vet_name\non dog_vets(name);\n\ncreate index dog_status_check_up\non dog_statuses(check_up);\n\ncreate index dog_status_sick\non dog_statuses(sick);\n</code></pre>\n"},{"tags":["performance","parallel-processing","data-modeling","simulation","hpc"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":39,"score":0,"question_id":12907569,"title":"Meaning of axis of figures of simulation or performance modeling papers","body":"<p>I am reading some papers on simulation and performance modeling. The Y axis in some figures is labeled \"Seconds per Simulation Day\". I am not sure what it actually means. It span from 0, 20, 40 to 120.</p>\n\n<p>Another label is \"Simulation years per day\". I guess it means the guest OS inside simulation environment thinks it has passed several years while actually it just passed a day in the real world? But I guess simulation should slow down the execution, so I guess inside simulation environment passed several hours while actually it just passed a day in the real world would be more reasonable.</p>\n\n<p>Thanks.</p>\n"},{"tags":["performance","optimization","jsoup"],"answer_count":2,"favorite_count":0,"up_vote_count":6,"down_vote_count":0,"view_count":126,"score":6,"question_id":11291414,"title":"A way to estimate or predict Jsoup processing time of a chunk of HTML?","body":"<p>Some web pages that I process in Jsoup are heavy. By \"heavy\" I mean the page either contains lots of HTML (let's assume the page has already been downloaded), or it requires several iterations on the <em>same</em> document (created only once via <a href=\"http://jsoup.org/cookbook/input/parse-document-from-string\">Jsoup.parse()</a>).</p>\n\n<p>For that reason, I would like to present to the user a progress bar with a guesstimate of how much time is left.</p>\n\n<p>One approach is to just measure the volume of HTML (in KB or MB) and come up with a speed factor (unfortunately, totally dependent on speed of the system this code runs on).</p>\n\n<p>Another approach is to count the <a href=\"http://stackoverflow.com/a/11291133/1357272\">number of nodes</a>?</p>\n\n<p>Due to the obvious in-deterministic nature of this, am I calling for trouble?</p>\n\n<p>Ideas of better ways to handles this?</p>\n"},{"tags":["ajax","performance","jsf","jquery-ajax","datatable"],"answer_count":1,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":70,"score":4,"question_id":12976215,"title":"How to write <table> markup without JSF tag libraries (h:datatable or ui:repeat) but still use JSF for controlling page flow","body":"<p>I have various tables with the following size : 12 columns and up to 1800 rows. It takes 8 seconds to render it to the user. I currently use <code>h:dataTable</code>. I tried <code>ui:repeat</code> to get the row data from a Java List object, managed by JSF.  Although this works fine, the 8 seconds to render the table is unacceptable. I'm trying to find other ways to do this, but need to keep <code>JSF</code> as my <code>controller</code> for action buttons on the page.  In other words I want to create the 'table markup<code>to send to the</code>page myself<code>and then still associate actions on</code>h:commandButtons` to the managed bean methods. Is there a way to do this?  </p>\n\n<p>The only ways I can think of is to use <code>jquery</code> or <code>ajax</code> to create the table markup, although I am new to technologies other than JSF for UI development.Maybe then I would somehow pass that to the client for render.  The only problem is I don't know how to generate the markup from my list, and second how I would inject it between <code>h:commandButtons</code> that are in my <code>XHTML</code> file currently. </p>\n\n<p>Does any one know how I can solve this without having to completely rip OFF JSF?  One main problem I have is that the  <strong>business requirement that says we can't page the datatable (i.e: Next / Back buttons displaying 100 at a time for example)</strong>.  So, possibly I was thinking I could do this by Ajax calls to the server and get 100 rows at a time after page ready, and append new rows behind the scenes to the user.  This would be a \"perceived\" speed of load, but I don't know how to do this at all.</p>\n"},{"tags":["performance","r"],"answer_count":4,"favorite_count":2,"up_vote_count":6,"down_vote_count":0,"view_count":110,"score":6,"question_id":12973963,"title":"Using sapply vs. for to efficiently write to preallocated data structures","body":"<p>Assume I have a preallocated data structure that I want to write into for the sake of performance vs. growing the data structure over time. First I tried this using sapply:</p>\n\n<pre><code>set.seed(1)\ncount &lt;- 5\npre &lt;- numeric(count)\n\nsapply(1:count, function(i) {\n  pre[i] &lt;- rnorm(1)\n})\npre\n# [1] 0 0 0 0 0\n\n\nfor(i in 1:count) {\n  pre[i] &lt;- rnorm(1)\n}\npre\n# [1] -0.8204684  0.4874291  0.7383247  0.5757814 -0.3053884\n</code></pre>\n\n<p>I assume this is because the anonymous function in <code>sapply</code> is in a different scope (or is it environment in R?) and as a result the <code>pre</code> object isn't the same. The for loop exists in the same scope/environment and so it works as expected.</p>\n\n<p>I've generally tried to adopt the R mechanisms for iteration with apply functions vs. for, but I don't see a way around it here. Is there something different I should be doing or a better idiom for this type of operation?</p>\n\n<p>As noted, my example is highly contrived, I have no interested in generaring normal deviates. Instead my actual code is dealing with a 4 column 1.5 million row dataframe. Previously I was relying on growing and merging to get a final dataframe and decided to try to avoid merges and preallocate based on benchmarking.</p>\n"},{"tags":["asp.net","database","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":6,"view_count":28,"score":-6,"question_id":12975862,"title":"Free performance monitoring tool in asp.net","body":"<blockquote>\n  <p>That tool should not consume bulk space while monitoring.  </p>\n  \n  <p>Suggest free one.  </p>\n  \n  <p>Thanks in advance</p>\n</blockquote>\n"},{"tags":["c++","performance","c++-amp"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":83,"score":0,"question_id":12974389,"title":"When should I use C++ AMP","body":"<p>When should I use the C++ AMP (or shouldn't use it)?</p>\n\n<p>What is an overhead of AMP? How long it takes to copy data to GPU memory and back? What is a minimal data size when AMP starts to decrease performance?</p>\n"},{"tags":["c++","c","performance","algorithm","profiling"],"answer_count":6,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":202,"score":4,"question_id":12964396,"title":"What is the most efficient method to find x contiguous values of y in an array?","body":"<p>Running my app through callgrind revealed that this line dwarfed everything else by a factor of about 10,000. I'm probably going to redesign around it, but it got me wondering; Is there a better way to do it?</p>\n\n<p>Here's what I'm doing at the moment:</p>\n\n<pre><code>int i = 1;\nwhile\n(\n    (\n        (*(buffer++) == 0xffffffff &amp;&amp; ++i) || \n        (i = 1)\n    )\n    &amp;&amp;\n    i &lt; desiredLength + 1\n    &amp;&amp;\n    buffer &lt; bufferEnd\n);\n</code></pre>\n\n<p>It's looking for the offset of the first chunk of desiredLength 0xffffffff values in a 32 bit unsigned int array.</p>\n\n<p>It's significantly faster than any implementations I could come up with involving an inner loop. But it's still too damn slow.</p>\n"},{"tags":["windows","performance","splash-screen","launcher"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":35,"score":0,"question_id":12974184,"title":"writing fast launcher for windows","body":"<p>I'm writing WPF application </p>\n\n<p>application targets all sort of windows and low performance computers\nso I want to write launcher/splash screen for it which will be displayed before application loads</p>\n\n<p>I'm not sure what language to use or what technology\nI want it to be very fast and lightweight </p>\n\n<p>can you suggest anything ?</p>\n"},{"tags":["performance","internet-explorer","internet-explorer-9","ie-compatibility-mode"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":30,"score":0,"question_id":12974055,"title":"Internet explorer compatibility view performance","body":"<p>I use IE9 and I know that the compatibility mode uses IE7 render engine but what about performances ? If the compatibility mode is enabled, does it slow the page load and the javascript ?</p>\n\n<p>I don't see major differences when switching from one to another.</p>\n"},{"tags":["eclipse","performance","jsp","indexing","cpu-usage"],"answer_count":2,"favorite_count":2,"up_vote_count":1,"down_vote_count":0,"view_count":2090,"score":1,"question_id":4320088,"title":"Eclipse indexing takes forever","body":"<p>I have Eclipse Helios SR1 installed on my Ubuntu 10.04 desktop and I'm having to work with a huge set of web projects in it.</p>\n\n<p>When I import the projects eclipse builds the workspace successfully but it keeps consuming 100% of the CPU invariably.</p>\n\n<p>After checking what could be happening I found that in Eclipse's progress tab there is a couple of endless tasks:</p>\n\n<ul>\n<li>System: Java indexing... n files to index</li>\n<li>System: Updating JSP index</li>\n<li>System: Persisting JSP translations</li>\n</ul>\n\n<p>These tasks seem to never end and makes my Eclipse unusable.</p>\n\n<p>I doubt it is a memory issue, I have 2GB in this machine and Eclipse's heap size does not get greater than 350MB and Xmx is set currently to 1024MB.</p>\n\n<p>Also tried running Eclipse with different VM versions: Sun's 1.6, Sun's 1.5, and Open JDK 1.6. No changes.</p>\n\n<p>I have an Athlon X2 2.2GHz processor and a 7200 rpm Samsung hard drive.</p>\n\n<p>The source code is shared via SVN.</p>\n\n<p>Does anyone have any idea of what could be going on?</p>\n\n<p>This is my eclipse.ini just in case:</p>\n\n<pre><code>-startup\nplugins/org.eclipse.equinox.launcher_1.1.0.v20100507.jar\n--launcher.library\nplugins/org.eclipse.equinox.launcher.gtk.linux.x86_1.1.1.R36x_v20100810\n-product\norg.eclipse.epp.package.jee.product\n--launcher.defaultAction\nopenFile\n-showsplash\norg.eclipse.platform\n--launcher.XXMaxPermSize\n256m\n--launcher.defaultAction\nopenFile\n-vmargs\n-Dosgi.requiredJavaVersion=1.5\n-XX:MaxPermSize=256m\n-Xms40m\n-Xmx1024m\n-Djava.library.path=/usr/lib/jni\n</code></pre>\n\n<p>Thanks a lot.</p>\n"},{"tags":["asp.net","performance","c#-4.0"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":53,"score":0,"question_id":11031233,"title":"Assembly.GetExecutingAssembly() performance","body":"<p>I'm looking into creating a automated way to check the running version of our system(s). Whenever we release code to test we version is using </p>\n\n<pre><code>[assembly: AssemblyVersion(\"1.21.0.13329\")]\n[assembly: AssemblyFileVersion(\"1.21.0.13329\")]\n</code></pre>\n\n<p>I now want to extend our ASP.Net <strong>Web forms</strong> and <strong>Web Services</strong> to display this version number. So the Web forms app will put it somewhere on the home page and the web service will probably have a web method that returns the version number.</p>\n\n<p>I was thinking of using </p>\n\n<pre><code>Version version = Assembly.GetExecutingAssembly().GetName().Version;\n</code></pre>\n\n<p>to do this. But I'm concerned that because this is using reflection it could add a performance overhead to my code. It could be getting called several times, especially on the home page and I don't want this to slow response times down. </p>\n\n<p>I have another solution so I don't want alternatives, I just want to know if anyone knows if the performance overhead of this is going to be relatively significant?</p>\n"},{"tags":["performance","algorithm","sorting","data-structures","quicksort"],"answer_count":3,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":93,"score":1,"question_id":12970688,"title":"Quicksort recursion depth Stack space of O(n) doesnot cause stackoverflow?","body":"<p>In Worst case Quicksort recursion Depth requires Stack  space  of  O(n). Why it doesn't cause a stack overflow for large set in the worst case? (reversed sequence)</p>\n"},{"tags":["c#",".net","performance","linq"],"answer_count":4,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":155,"score":5,"question_id":12819792,"title":"LINQ query is slow","body":"<p>During application profiling I found that function checking for pattern match is very slow. It is written using LINQ. Simple replacement of this LINQ expression with loop makes really huge difference. What is it? Is LINQ really such a bad thing and works so slow or I misunderstand something?</p>\n\n<pre><code>private static bool PatternMatch1(byte[] buffer, int position, string pattern)\n{\n    int i = 0;\n\n    foreach (char c in pattern)\n    {\n        if (buffer[position + i++] != c)\n        {\n            return false;\n        }\n    }\n\n    return true;\n}    \n</code></pre>\n\n<p>version 2 with LINQ (suggested by Resharper)</p>\n\n<pre><code>private static bool PatternMatch2(byte[] buffer, int position, string pattern)\n{\n    int i = 0;\n    return pattern.All(c =&gt; buffer[position + i++] == c);\n}\n</code></pre>\n\n<p>version 3 with LINQ</p>\n\n<pre><code>private static bool PatternMatch3(byte[] buffer, int position, string pattern)\n{\n    return !pattern.Where((t, i) =&gt; buffer[position + i] != t).Any();\n}\n</code></pre>\n\n<p>version 4 using lambda</p>\n\n<pre><code>private static bool PatternMatch4(byte[] buffer, int position, string pattern, Func&lt;char, byte, bool&gt; predicate)\n{\n    int i = 0;\n\n    foreach (char c in pattern)\n    {\n        if (predicate(c, buffer[position + i++]))\n        {\n            return false;\n        }\n    }\n\n    return true;\n}\n</code></pre>\n\n<p>and here is the usage with large buffer</p>\n\n<pre><code>const int SIZE = 1024 * 1024 * 50;\n\nbyte[] buffer = new byte[SIZE];\n\nfor (int i = 0; i &lt; SIZE - 3; ++i)\n{\n    if (PatternMatch1(buffer, i, \"xxx\"))\n    {\n        Console.WriteLine(i);\n    }\n}\n</code></pre>\n\n<p>Calling <code>PatternMatch2</code> or <code>PatternMatch3</code> is phenomenally slow. It takes about 8 seconds for <code>PatternMatch3</code> and about 4 seconds for <code>PatternMatch2</code>, while calling <code>PatternMatch1</code> takes about 0.6. As far as I understand it is the same code and I see no difference. Any ideas?</p>\n"},{"tags":["java","arrays","list","performance"],"answer_count":21,"favorite_count":23,"up_vote_count":69,"down_vote_count":0,"view_count":48646,"score":69,"question_id":716597,"title":"Array or List in Java. Which is faster?","body":"<p>I have to keep thousands of strings in memory to be accessed serially in Java. Should I store them in an array or should I use some kind of List ?</p>\n\n<p>Since arrays keep all the data in a contiguous chunk of memory (unlike Lists), would the use of an array to store thousands of strings cause problems ?</p>\n\n<p><strong>Answer:</strong> The common consensus is that the performance difference is minor. List interface provides more flexibility. </p>\n"},{"tags":["android","performance","view","touch"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":50,"score":1,"question_id":12972222,"title":"Slow response onTouchEvent in Android View","body":"<p>I created a View that listens for touch events and then draw a circle whenever you touched. Currently, it only tracks vertical position of the touch. </p>\n\n<p>There are the interesting code parts: </p>\n\n<p>onTouchEvent:</p>\n\n<pre><code>public boolean onTouchEvent(MotionEvent event) {\n\n    int actionType = event.getAction();\n    if (actionType == MotionEvent.ACTION_MOVE || actionType == MotionEvent.ACTION_DOWN ) {\n\n        int px = getMeasuredWidth() / 2;\n        int py = getMeasuredHeight() / 2;\n\n        touchY = (event.getY() - py);\n\n        if (touchY &lt; 25 &amp;&amp; touchY &gt; -25) {\n            touchY = 0;\n        }\n        invalidate();\n    }\n    return true;\n}\n</code></pre>\n\n<p>Also, the onDraw:</p>\n\n<pre><code>@Override\nprotected void onDraw(Canvas canvas) {\n    int px = getMeasuredWidth() / 2;\n    int py = getMeasuredHeight() / 2;\n    int radius = px - innerPadding;\n\n    canvas.drawCircle(px, touchY+py, radius - innerPadding, circlePaint);\n    canvas.save();\n}\n</code></pre>\n\n<p>It works but I find that the circle following the finger is quite slow considering I have a Galaxy S3. It is smooth but it has a delay.</p>\n\n<p>Any advice of how to implement this better?</p>\n\n<p>Thank you. </p>\n"},{"tags":["c#",".net","performance","stream","filestream"],"answer_count":3,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":159,"score":2,"question_id":12969555,"title":"File I/O best practice - byte[] or FileStream?","body":"<p>I'm currently working with a lot of different file types (txt, binary, office, etc). I typically use a <code>byte[]</code> or <code>string</code> to hold the file data in memory (while it is being written/parsed) and in order to read/write it into files I write the entire data using a <code>FileStream</code> after the data has been completely processed. </p>\n\n<ul>\n<li>Should I be using a <code>TextStream</code> instead of a <code>string</code> while generating data for a text file?</li>\n<li>Should I be using a <code>FileStream</code> instead of a <code>byte[]</code> while generating data for a binary file?</li>\n<li>Would using streams give me better performance instead of calculating the entire data and outputting it in one go at the end?</li>\n<li>Is it a general rule that File I/O should always use streams or is my approach fine in some cases?</li>\n</ul>\n"},{"tags":["performance","nginx","control"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":31,"score":2,"question_id":12969958,"title":"nginx speed control based on cookies [or any other variable I can set on the fly]?","body":"<p>I have this config in my Nginx server:</p>\n\n<pre><code>limit_rate 500k;\nlocation ~ \\.mp4$ {\n    mp4;\n    limit_rate_after 4m;\n    limit_rate 90k;\n    limit_req zone=one burst=5;\n    limit_conn addr 2;\n}\n</code></pre>\n\n<p>I want the speed and the burst controlled on the fly by a mean of cookies (can't alter URL as I need it in certain length/structure)</p>\n\n<p>Is there a way to put something like this?</p>\n\n<pre><code>if($cookie_burst){\n    limit_rate_after {$cookie_burst}m;//how can I use cookie value here along with m[Megabytes]?\n}\n</code></pre>\n\n<p>Edit: I cahnged the URL requesting the mp4 file, adding a burst argument, now using $arg_burst in Nginx config file.</p>\n\n<p>However, I can test for $arg_burst but I can't use it in a line like this:</p>\n\n<p>This works:</p>\n\n<pre><code>if ($arg_burst = \"1m\"){\n   limit_rate_after 1m;\n}\n</code></pre>\n\n<p>This doesn't :( :</p>\n\n<pre><code>if ($arg_burst != \"\"){\n   limit_rate_after $arg_burst;\n}\n</code></pre>\n\n<p>Any workaround to put whatever value I want there, on the fly?</p>\n"},{"tags":["mysql","performance","amazon-ec2","amazon-elasticache"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":476,"score":1,"question_id":11335366,"title":"Using ElastiCache with RDS for improving read/write performance","body":"<p>I am using RDS on amazon with a MySQL interface. My application runs on EC2 nodes and read/update the database, but the number of reads and writes are too many and that reduces performance. Most of the time the number of connections exceed the allowed limit. \nI was considering using Elasticache to improve performance, however I did not find resources on web, how to configure database to use this effectively.\nIs this the best way to improve my read/write performance?\nAny suggestions?</p>\n"},{"tags":["html","css","performance","html5","optimization"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":40,"score":-1,"question_id":12970857,"title":"CSS/HTML Optimizations tips","body":"<p>Can someone overview the code on our web site and offer some tips to optimize it further, we want to be able to achieve a grade A or B in yslow performance. </p>\n\n<p>Website at <a href=\"http://www.wirenine.com/\" rel=\"nofollow\">http://www.wirenine.com/</a></p>\n"},{"tags":["mysql","performance","query","join"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":47,"score":0,"question_id":12970402,"title":"Slow MySQL query with multiple sub-select joins","body":"<p>This has been bothering me all day:</p>\n\n<p>I have this query which works fine but is quite slow in some cases. The explain result tells me mysql is scanning 40000 rows 3 times. The query joins on approximatily similar sub-selects but there all a little different.</p>\n\n<p>How can I improve this?</p>\n\n<pre><code>SET @lastchecktime = (SELECT max(tresults.StartTime) FROM tresults);\nSELECT tresults.shopID\n , tshops.OfficialName\n , tresults.Price, tresults.starttime\n , Sub2.minprice\n , Sub2.StartTime\n , Sub3.daystoolow\n , (sub2.minprice/tsupplierproducts.lowestprice) -1 as afwijking\n , If(maxstarttime = @lastchecktime,'yes' ,'no') as notavailable     \nFROM\n  tresults\nINNER JOIN tshops\nON tshops.shopID = tresults.shopID\ninner JOIN (SELECT tresults.shopID\n                 , max(tresults.StartTime) AS MaxStartTime\n            FROM\n            tresults\n            WHERE\n              tresults.pID = 7\n              AND tresults.websiteID = 1\n              AND tresults.StartTime BETWEEN \"2012-08-01\" AND \"2012-12-01\"              \n            GROUP BY\n              tresults.shopID) Sub1\nON tresults.StartTime = Sub1.MaxStartTime AND Sub1.shopID = tshops.shopID\nINNER JOIN (SELECT tresults.shopID\n             , tresults.StartTime\n             , min(tresults.Price) AS minprice\n        FROM\n          tresults\n        WHERE\n          tresults.pID = 7\n          AND tresults.websiteID = 1\n          AND tresults.StartTime BETWEEN \"2012-08-01\" AND \"2012-12-01\"\n        GROUP BY\n          tresults.shopID) Sub2\nON Sub2.shopID = tshops.shopID\nINNER JOIN (SELECT tresults.shopID\n             , round(count(tresults.StartTime)/3,0) AS daystoolow\n        FROM\n          tsupplierproducts\n        INNER JOIN tresults\n        ON tsupplierproducts.pID = tresults.pID AND tresults.Price &lt; tsupplierproducts.LowestPrice\n        WHERE\n          tresults.pID = 7\n          AND tresults.websiteID = 1\n          AND tresults.StartTime BETWEEN \"2012-08-01\" AND \"2012-12-01\"\n          AND tsupplierproducts.supplierID = 2\n        GROUP BY\n          tresults.shopID) Sub3\nON Sub3.shopID = tshops.shopID\nINNER JOIN tsupplierproducts\nON tsupplierproducts.pID = tresults.pID AND tsupplierproducts.supplierID = 2\nWHERE\n  tresults.pID = 7\n  AND tresults.websiteID = 1\nORDER BY\nnotavailable desc, tresults.Price DESC\n</code></pre>\n\n<p>The explain result:</p>\n\n<pre><code>1, PRIMARY, tsupplierproducts, const, PRIMARY,fk_SupplierID,fk_pID, PRIMARY, 8, const,const, 1, Using temporary; Using filesort\n1, PRIMARY, &lt;derived4&gt;, ALL, , , , , 27, \n1, PRIMARY, &lt;derived2&gt;, ALL, , , , , 43, Using where; Using join buffer\n1, PRIMARY, &lt;derived3&gt;, ALL, , , , , 43, Using where; Using join buffer\n1, PRIMARY, tshops, eq_ref, PRIMARY, PRIMARY, 4, Sub1.shopID, 1, Using where\n1, PRIMARY, tresults, eq_ref,     PRIMARY,idxPID,idxWebsite,idxStartTimeASC,idxStartTimeDESC,fk_shopID, PRIMARY, 20, Sub1.MaxStartTime,const,Sub3.shopID,const, 1, \n4, DERIVED, tsupplierproducts, const, PRIMARY,fk_SupplierID,fk_pID, PRIMARY, 8, , 1, Using temporary; Using filesort\n4, DERIVED, tresults, ref, PRIMARY,idxPID,idxWebsite,idxStartTimeASC,idxStartTimeDESC,     idxPID, 4, , 42048, Using     where\n3, DERIVED, tresults, ref, PRIMARY,idxPID,idxWebsite,idxStartTimeASC,idxStartTimeDESC,     idxPID, 4, , 42048,     Using     where; Using temporary; Using filesort\n2, DERIVED, tresults, ref, PRIMARY,idxPID,idxWebsite,idxStartTimeASC,idxStartTimeDESC, idxPID, 4, , 42048, Using where; Using index; Using temporary; Using filesort\n</code></pre>\n\n<p>Any help would be appreciated!</p>\n"},{"tags":["c#","performance","silverlight"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":31,"score":0,"question_id":12953722,"title":"Making image-source swap faster","body":"<p>I have 4 image controls and a button, which swaps all those images within those image-controls. But this is going too slow.</p>\n\n<p>The image swap happens after a 1.5s Storyboard animation. So imagine those four-image controls making a move down and then calling this method:</p>\n\n<pre><code> BLOCK4.Source = stack[3];\n BLOCK3.Source = stack[2];\n BLOCK2.Source = stack[1];\n BLOCK1.Source = stack[0];\n</code></pre>\n\n<p>stack is a private <code>BitmapImage[] stack;</code> array which contains random images after every animation-call.</p>\n\n<p>Do you see a way to tune this code in order to make the swap seemingly faster?</p>\n\n<p><strong>This is what happens:</strong> Animation starts -> stops -> I can see old images -> milliseconds pass by -> I can see new images.</p>\n"},{"tags":["arrays","performance","matlab","structure"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":48,"score":2,"question_id":12949665,"title":"Rearrange structure arrays of uneven lengths to single 1d array","body":"<p>I've got an struct array, with three fields - an array, the array's length, and a number.</p>\n\n<pre><code>N = 5;\ndata = struct;\nfor i=1:N\n    n = ceil(rand * 3);\n    data(i).len = n;\n    data(i).array = rand(1,n);\n    data(i).number = i;\nend\n</code></pre>\n\n<p>The data looks like this:</p>\n\n<pre><code>data = \n1x5 struct array with fields:\n    len    = [ 1 3 3 1 1 ]\n    array  = [[0.8]; [0.7 0.9 0.4]; [0.7 0 0.3]; [0.1]; [0.3]]\n    number = [ 1 2 3 4 5 ]\n</code></pre>\n\n<p>I can return array as a 1x9 array in several ways:</p>\n\n<pre><code>&gt;&gt;&gt; [data.array] \n&gt;&gt;&gt; cat(2,data.array)\n[0.8 | 0.7 0.9 0.4 | 0.7 0 0.3 | 0.1 | 0.3]     %  | shows array separation\n</code></pre>\n\n<p>I'd like to repeat the number (<code>data.number</code>) <code>len</code> times, to produce the same length array as the concatenated array. </p>\n\n<p>I'm currently doing this with <code>arrayfun</code> then <code>cell2mat</code>:</p>\n\n<pre><code>&gt;&gt; x = arrayfun(@(x) repmat(x.number, 1, x.len), data, 'UniformOutput', false)\nx = \n    [1]    [1x3 double]    [1x3 double]    [4]    [5]\n&gt;&gt; cell2mat(x)\n[ 1 2 2 2 3 3 3 4 5]\n</code></pre>\n\n<p>This makes the numbers line up with the arrays. </p>\n\n<pre><code>arrays =  [ 0.8 | 0.7 0.9 0.4 | 0.7 0 0.3 | 0.1 | 0.3 ] \nnumbers = [ 1   | 2   2   2   | 3   3   3 | 4   | 5   ]\n</code></pre>\n\n<hr>\n\n<p>The idea behind this is to feed the data to the GPU for processing - but rearranging the data takes orders of magnitude longer than the actual processing. </p>\n\n<p><code>Arrayfun</code> takes ~5 seconds when N=100,000, and a for loop calling <code>repmat</code> takes ~4 seconds. </p>\n\n<p>Is there a faster way to rearrange data from uneven arrays in structures into matching length 1d arrays? I'm open to using a different data structure. </p>\n\n<hr>\n\n<p><strong>Testing vectorised method:</strong></p>\n\n<pre><code>data = struct;\ndata(1).len = 1;\ndata(1).array = [1 2 3];\ndata(1).number = 11;\ndata(2).len = 0;\ndata(2).array = [];\ndata(2).number = 12;\ndata(3).len = 2;\ndata(3).array = [4 5 6; 7 8 9];\ndata(3).number = 13;\n\nlist_of_array = cat(1,data.array)\n\nidx = zeros(1,size(list_of_array,1));\n% Set start of each array to 1\nlen = cumsum([data.len])\nidx(len) = 1\n% Flat indices\nidx = cumsum([1 idx(1:end-1)])\n\nnf = [data.number]\nrepeated_num_faces = nf(idx)\n</code></pre>\n\n<p>Gives the output:</p>\n\n<pre><code>list_of_array =\n     1     2     3\n     4     5     6\n     7     8     9\nlen =\n     1     1     3    % Cumulative lengths\nidx =\n     1     0     1    % Ones at start\nidx =\n     1     2     2    % Flat indexes - should be [1 3 3]\nnf =\n    11    12    13    % Numbers expanded\nrepeated_num_faces =\n    11    12    12    % Wrong .numbers - should be [11 13 13]\n</code></pre>\n"},{"tags":["python","performance","memory-management","coding-style"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":79,"score":3,"question_id":12953125,"title":"Python: How to improve performance of a script to obtain the Bounding Box of a set of points","body":"<p>I have a set of points (x and y) and i wish to know the maximum and Minimum values of X and Y (the Bounding Box). I coded these lines where i read all points with list comprehension and after i use max and min on X and Y. In the end i delete the points.</p>\n\n<p>This solution is not memory efficency because i need to read all points</p>\n\n<pre><code>points = [(p.x,p.y) for p in lasfile.File(inFile,None,'r')] # read in list comprehension\nX_Max = max(zip(*points)[0])\nX_Min = min(zip(*points)[0])\nY_Max = max(zip(*points)[1])\nY_Min = min(zip(*points)[1])\ndel points\n</code></pre>\n\n<p>I am asking a suggetion to avoid this step (store all points in the memory).\nThanks in advance\nGianni</p>\n"},{"tags":["javascript","json","performance","node.js"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":60,"score":0,"question_id":12969597,"title":"Javascript: Removing an object from array by checking it's attribute","body":"<p>I might have written a pretty confusing title but my question is rather simple. </p>\n\n<p>I'm looking for an efficient way to remove an item from an array. But my array is full objects that has been <b>stringified</b> (I'm writing my app on Node.js and I'm using <b>JSON.stringify</b> method). So my array is like this;</p>\n\n<pre><code>\"{\\\"userID\\\":\\\"15\\\",\n  \\\"possibleFollowers\\\":[\n      {\\\"followerID\\\":\\\"201\\\",\\\"friends\\\":716},\n      {\\\"followerID\\\":\\\"202\\\",\\\"friends\\\":11887},\n      {\\\"followerID\\\":\\\"203\\\",\\\"friends\\\":11887}],\n  \\\"name\\\":\\\"John\\\",\n  \\\"lon\\\":\\\"Doe\\\"}\"\n</code></pre>\n\n<p><P>My question is on Javascript(or Node). If I wanted to remove the from possibleFollowers with \"followerID: 202\", how would I be able to do that <b>efficiently</b>?</p></p>\n"},{"tags":["performance","assembly","pic","pic18","8bit"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":84,"score":1,"question_id":12960652,"title":"fast 8x8bit multiplication assembly pic18","body":"<p>Im trying to multiply two 8 bit numbers and store them in a 16 bit location for results larger than 255. The fastest way to accomplish this is through shifting, which I have tried to implement via the rrcf function and using bcf to clear unwanted carries. </p>\n\n<p>This is what ive came up with. Ive tried to comment all the code so you are able to see my thought process. Im  fairly new to both the PIC18 and programming in ASM in general. Please keep that in mind when (hopefully) providing help. I need to get put in a better spot than I am in, when I run MPLAB SIM, all I get is the counter decrementing...? </p>\n\n<p>I think this is due to the last bit of multiplier being repeatedly tested, which will be zeros and and therefore skip my add instruction every time. Can you help me create a loop to move BTFSC progressively from bit 0-7? I think this is the problem, but i can't figure out the code. I could essentailly write main 8 times, but im trying to save code space</p>\n\n<pre><code>            LIST P=18F4550\n            #include &lt;P18F4550.INC&gt;\n\n            ;equates\n            counter equ 0x00 ;set counter\n            multiplicand equ 0x01 ;set multiplicand\n            multiplier equ 0x02 ;set multiplier\n            resultHigh equ 0x03 ;set resultHigh\n            resultLow equ 0x04 ;set resultLow\n\n            ;test program\n\n            movlw d'100' ;move literal d'100' to wreg\n            movwf multiplicand ;set multiplicand\n            movlw d'400'       ;move literal d'400'\n            movlw multiplier   ;set multiplier\n            movlw d'8'         ;counter\n            movwf counter      ;set counter\n\n            main:\n            btfsc multiplier, 0          ;test LSB if 0,skip next if 0\n            addwf multiplier, resultLow  ;add if 1 to resultLow\n            bcf STATUS,C                 ;clear carry flag\n            rlcf multiplier              ;rotate multiplier left\n            bcf STATUS,C                 ;clear carry\n            rlcf resultLow               ;rotate resultLow w/ carry\n            rlcf resultHigh              ;rotate resultHigh \n                                                                  ;w/carry from resultLow\n\n            decf counter                 ;dec counter\n            goto main                    ;repeat for 8 bits\n            end\n</code></pre>\n"},{"tags":["ios","performance","uitableview","uitableviewcell"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":12969346,"title":"Improving UITableView scrolling","body":"<p>i stuck with some issue of UITableView scrolling. I know that there are numbers of this question and some solutions. However from technical view like (how exactly to do that) i can't find solutions.</p>\n\n<p>Well in my case scrolling is not so smooth because of images in it. It takes lots of time to load it from file. Also i create custom UITableViewCell and using XiB. </p>\n\n<p>These images is using not 1 time but several, the good idea would be to keep them in memory and to load them just once from file. However in my case there could lots of images and app don't have so many memory to keep them. Maybe there are other way to load these images ?</p>\n\n<p>Also images is PNG, maybe i should change type to JPG ?</p>\n\n<p>I would appreciate if someone would give other solutions to improve this scrolling.</p>\n\n<p>Thanks.  </p>\n"},{"tags":["performance","cpu","scaling","ondemand"],"answer_count":0,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":48,"score":4,"question_id":12969081,"title":"Performance governor doesn't locks the CPU frequency at max","body":"<p>I'm running real-time application on the dedicated X3440 server and wondering why the application performance is worse than my former VPS box. Then I download <a href=\"http://i7z.googlecode.com/svn/trunk/i7z_64bit\" rel=\"nofollow\">http://i7z.googlecode.com/svn/trunk/i7z_64bit</a> and execute it to see that under normal usage (in top around 10% cpu), all CPU core only stays around 900Mhz ~ 1200Mhz, and it fluctuates rapidly and inconsistent.</p>\n\n<p>Then I try to set governor from ondemand to performance (echo performance > /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor), and my application performs better because now all CPU core stays around 1700 ~ 1900Mhz (still fluctuating), but the application performance still doesn't meet my standard yet.</p>\n\n<p>I have trying to disable any scaling completely by typing \"service cpuspeed stop\", and it's still showing the same result as performance governor. I was wondering why Performance governor didn't lock the CPU frequency at max.</p>\n\n<p>Then I try to run 4 separate dummy nice low-priority tasks in each of the cores:\nyum install gcc</p>\n\n<p>nano dummy.c</p>\n\n<p>content:</p>\n\n<pre><code>int main() {\n    while(1);\n}\n</code></pre>\n\n<p>gcc dummy.c -o dummy</p>\n\n<pre><code>nice taskset -c 0 ./dummy &amp; nice taskset -c 1 ./dummy &amp; nice taskset -c 2 ./dummy &amp; nice taskset -c 3 ./dummy &amp;\n</code></pre>\n\n<p>This way, all 4 cores will stay at 2533mhz and never fluctuates anymore (regardless of the governor settings), and now my application performs very stellar and no complain whatsoever. But, I don't like the way these tasks waste resource, even though it's not disturbing the main application since it's on low priority.</p>\n\n<p>My question: \nIn my home desktop windows computer, somehow my CPU speed is always going stable at max frequency. But why it doesn't happened on this CentOS 6 2.6.32 x86_64 dedicated server? Is there any way to set all cpu cores to max frequency without using any nice low-priority tasks?</p>\n"},{"tags":["c","performance","numerical"],"answer_count":2,"favorite_count":1,"up_vote_count":4,"down_vote_count":2,"view_count":91,"score":2,"question_id":12937412,"title":"How does the size of a binary influence the execution speed","body":"<p>How does the size of a binary influence the execution speed? Specifically I am talking about code written in ANSI-C translated into machine language using the gnu or intel compiler. The target platform for the binary are modern computers with intel or AMD multi-core CPU's running a Linux operating system. The code performs numerical computations possibly in parallel using openMP and the binary could have several mega bytes. </p>\n\n<p>Note that the execution time will in any case be much larger than the time needed to load code and libraries. I think of very specific codes used to solve large systems of ordinary differential equations for simulations of kinetic equations which are typically CPU-bound for a moderate system size but can also become memory-bound.</p>\n\n<p>I am asking whether small binary size should be a design criterion for highly efficient code or if I can always give preference to explicit code (which eventually repeats code blocks which could be implemented as functions) and compiler optimizations such as loop unrolling etc.</p>\n\n<p>I am aware of profiling technics and how I can apply them to specific problems, but I wonder to which extent general statements can be made.</p>\n"},{"tags":["database","performance","google-app-engine","gae-datastore"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":56,"score":1,"question_id":12966253,"title":"Which database model to store data in?","body":"<p>I am writing an application in Google App Engine with python and I want to sort users and user posts into groups.  Users will be able to tag a post with a group ID and then that post will be displayed on the group page.  </p>\n\n<p>I would also like to relate the users to the groups so that only members of a group can tag a post with that group ID and so that I can display all the users of a group on the side.  I am wondering if it would be more efficient to have a property on the user which will have all of the groups listed (I am thinking max 10 or so) or would it be better to have a property on the Group model which lists all of the users (possibly a few hundred).</p>\n\n<p>Is there much of a difference here?</p>\n"},{"tags":["performance","ubuntu","symfony2","virtualbox"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":100,"score":1,"question_id":12905404,"title":"Symfony2 Slow Initialization Time","body":"<p>I have Symfony2 running on an Ubuntu Server 12.04 (64-bit) VM (VirtualBox).  The host is a MacBook pro.  For some reason I am getting really long request times in development mode (app_dev.php).  I know its slower in dev mode, but I'm talking 5-7 seconds per request (sometimes even slower).  On my Mac I get request times of 200ms or so in development mode.</p>\n\n<p>After looking at my timeline in the Symfony2 profiler, I noticed that ~95% of the request time is \"initialization time\".  What is this?  What are some reasons it could be so slow?</p>\n\n<p>This issue only applies to Symfony2 in dev mode, not any other sites I'm running on the VM, and not even to Symfony2 in production mode.</p>\n\n<p>I saw this (http://stackoverflow.com/questions/11162429/whats-included-in-the-initialization-time-in-the-symfony2-web-profiler), but it doesn't seem to answer my questions.</p>\n"},{"tags":["json","performance","web-applications","web"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":26,"score":0,"question_id":12966871,"title":"Performance impact of using short key in JSON file","body":"<p>In a <strong>web server application</strong>, I use some JSON configuration file. Does anyone have any idea, if using shorter key, what's the performance benefit?</p>\n\n<p>For instance, a long key is \"country_code\", which value is a country code, an int.\nA shorter key is \"cc\". </p>\n\n<p><strong>How much performance benefit can we get from using the shorter key? considering network bandwidth, memory...etc</strong></p>\n\n<p>Thanks!</p>\n"},{"tags":["performance","memory","opencl"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":61,"score":0,"question_id":12938520,"title":"Maximize OpenCL memory output","body":"<p>Using OpenCL, I can't seem to pull more than 7MB/sec of data off of a Radeon 7970 into the main memory of my i5 Desktop.</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;Windows.h&gt;\n#include &lt;CL/cl.h&gt;\n\nint main(int argc, char ** argv)\n{\n    cl_platform_id platform;\n    clGetPlatformIDs(1, &amp;platform, NULL);\n    cl_device_id device;\n    clGetDeviceIDs(platform, CL_DEVICE_TYPE_GPU, 1, &amp;device, NULL);\n    cl_context context = clCreateContext(NULL, 1, &amp;device, NULL, NULL, NULL);\n    cl_command_queue queue = clCreateCommandQueue(context, device, 0, NULL);\n    const char *source =\n    \"__kernel void copytest(__global short* dst) {\\n\"\n    \"    __local short buff[1024];\\n\"\n    \"    for (int i = 0; i &lt; 1024; i++) {\\n\"\n    \"        for (int j = 0; j &lt; 1024; j++)\\n\"\n    \"            buff[j] = j;\\n\"\n    \"        (void)async_work_group_copy(&amp;dst[i*1024], buff, 1024, 0);\\n\"\n    \"    }\\n\"\n    \"}\\n\";\n    cl_program program = clCreateProgramWithSource(context, 1, &amp;source, NULL, NULL);\n    clBuildProgram( program, 1, &amp;device, NULL, NULL, NULL);\n    cl_kernel kernel = clCreateKernel( program, \"copytest\", NULL);\n    cl_mem buf = clCreateBuffer(context, CL_MEM_WRITE_ONLY, 1024 * 1024 * 2, NULL, NULL);\n    const size_t global_work_size = 1;\n    clSetKernelArg(kernel, 0, sizeof(buf), (void*)&amp;buf);\n    LARGE_INTEGER pcFreq = {}, pcStart = {}, pcEnd = {};\n    QueryPerformanceFrequency(&amp;pcFreq);\n    QueryPerformanceCounter(&amp;pcStart);\n    clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &amp;global_work_size, NULL, 0, NULL, NULL);\n    clFinish(queue);\n    QueryPerformanceCounter(&amp;pcEnd);\n    std::cout &lt;&lt; 2.0 * pcFreq.QuadPart / (pcEnd.QuadPart-pcStart.QuadPart) &lt;&lt; \"MB/sec\";\n}\n</code></pre>\n\n<p>As you can see, it's operating all just on a single work unit.  I tried replacing the async_work_group_copy() with a loop distributed amongst multiple (64) work units, but that did not help.</p>\n\n<p>Is there some way to pull memory off faster from the Radeon than 7MB/sec?  I'm interested in the hundreds of MB/sec.  Would NVidia be faster?</p>\n"},{"tags":["multithreading","performance","qt","timer","signals"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":78,"score":1,"question_id":12924960,"title":"Timer makes UI unresponsive?","body":"<p>I use a timer to emit a signal and call a slot function which updates the UI according to new data.</p>\n\n<p>I've calculated the time to run that function as being quite short. When the timer is set to 1000ms, the UI responds quite slow. </p>\n\n<p>I tried to move that functionality to a thread but I'm finding it difficult since a lot of the functionality needs to access the UI class's protected values.</p>\n\n<p>I'm going to try moving the timer to another thread and leave the update functionality in the UI class (main window) but I don't know if it will help.</p>\n\n<p>Why is the timer causing the UI to be slow and unresponsive? Will a thread be lighter and consume less CPU time? How can I fix this?</p>\n\n<pre><code>    initTimer()\n    {\n        refreshTimer = new QTimer(this);\n        connect(refreshTimer, SIGNAL(timeout()), this, SLOT(refreshDisplay));\n        refreshTimer-&gt;start(1000);\n    }\n</code></pre>\n\n<p>refresh slot function called every 1000ms:</p>\n\n<pre><code>    void refreshDisplay()\n    {\n        ui-&gt;tableWidget-&gt;setUpdatesEnabled(false);\n\n        for(int queue_i = size_1, index = 0; queue_i &gt;= 0; queue_i--, index++)\n        {\n            LogInfoItem* logItem = (LogInfoItem*)logDisplayQueue.at(queue_i);\n\n            QString BITS_str = bits2Hexs(logItem-&gt;BITS);\n\n\n            ui-&gt;tableWidget-&gt;item(index, 0)-&gt;setText(logItem-&gt;time);//time\n            ui-&gt;tableWidget-&gt;item(index, 1)-&gt;setText(logItem-&gt;name);//name\n            ui-&gt;tableWidget-&gt;item(index, 2)-&gt;setText(BITS_str);//BITS\n\n            if(queue_i == oldRowItemNo)ui-&gt;tableWidget-&gt;selectRow(index);\n        }\n\n        ui-&gt;tableWidget-&gt;setUpdatesEnabled(true);\n\n        Q_FOREACH(Page* p, PageInfoList)\n        {\n            p-&gt;refresh();\n        }\n\n        Q_FOREACH(IconLabel* icl, iconLabelList)\n        {\n            icl-&gt;refresh();\n        }\n    }\n</code></pre>\n\n<p>What 'refresh()' does is just changing icons and texts in the ui according to the data inside.   Besides, i've made the data static, but still cannot fix it.\n    (I've tested the function, almost no time consuming...)</p>\n"},{"tags":["c++","windows","linux","performance","qt"],"answer_count":4,"favorite_count":0,"up_vote_count":9,"down_vote_count":0,"view_count":236,"score":9,"question_id":12878980,"title":"Speed performance of a Qt program: Windows vs Linux","body":"<p>I've already posted this question <a href=\"http://qt-project.org/forums/viewthread/21092/\" rel=\"nofollow\">here</a>, but since it's maybe not that Qt-specific, I thought I might try my chance here as well. I hope it's not inappropriate to do that (just tell me if it is).</p>\n\n<p>I’ve developed a small scientific program that performs some mathematical computations. I’ve tried to optimize it so that it’s as fast as possible. Now I’m almost done deploying it for Windows, Mac and Linux users. But I have not been able to test it on many different computers yet.</p>\n\n<p>Here’s what troubles me: To deploy for Windows, I’ve used a laptop which has both Windows 7 and Ubuntu 12.04 installed on it (dual boot). I compared the speed of the app running on these two systems, and I was shocked to observe that <strong>it’s at least twice as slow on Windows!</strong> I wouldn’t have been surprised if there were a small difference, but how can one account for such a difference?</p>\n\n<p>Here are a few precisions:</p>\n\n<ul>\n<li>The computation that I make the program do are just some brutal and stupid mathematical calculations, basically, it computes products and cosines in a loop that is called a billion times. On the other hand, the computation is multi-threaded: I launch something like 6 QThreads.</li>\n<li>The laptop has two cores @1.73Ghz. At first I thought that Windows was probably not using one of the cores, but then I looked at the processor activity, according to the small graphic, both cores are running 100%.</li>\n<li>Then I thought the C++ compiler for Windows didn’t the use the optimization options (things like -O1 -O2) that the C++ compiler for Linux automatically did (in release build), but apparently it does.</li>\n</ul>\n\n<p>I’m bothered that the app is so mush slower (2 to 4 times) on Windows, and it’s really weird. On the other hand I haven’t tried on other computers with Windows yet. Still, do you have any idea why the difference?</p>\n\n<p><strong>Additional info:</strong> some data…</p>\n\n<p>Even though Windows seems to be using the two cores, I’m thinking this might have something to do with threads management, here’s why:</p>\n\n<p>Sample Computation n°1 (this one launches 2 QThreads):</p>\n\n<ul>\n<li>PC1-windows: 7.33s</li>\n<li>PC1-linux: 3.72s</li>\n<li>PC2-linux: 1.36s</li>\n</ul>\n\n<p>Sample Computation n°2 (this one launches 3 QThreads):</p>\n\n<ul>\n<li>PC1-windows: 6.84s</li>\n<li>PC1-linux: 3.24s</li>\n<li>PC2-linux: 1.06s</li>\n</ul>\n\n<p>Sample Computation n°3 (this one launches 6 QThreads):</p>\n\n<ul>\n<li>PC1-windows: 8.35s</li>\n<li>PC1-linux: 2.62s</li>\n<li>PC2-linux: 0.47s</li>\n</ul>\n\n<p>where:</p>\n\n<ul>\n<li>PC1-windows = my 2 cores laptop (@1.73Ghz) with Windows 7</li>\n<li>PC1-linux = my 2 cores laptop (@1.73Ghz) with Ubuntu 12.04</li>\n<li>PC2-linux = my 8 cores laptop (@2.20Ghz) with Ubuntu 12.04</li>\n</ul>\n\n<p>(Of course, it's not shocking that PC2 is faster. What's incredible to me is the difference between PC1-windows and PC1-linux).</p>\n\n<p>Note: I've also tried running the program on a recent PC (4 or 8 cores @~3Ghz, don't remember exactly) under Mac OS, speed was comparable to PC2-linux (or slightly faster).</p>\n\n<p>EDIT: I'll answer here a few questions I was asked in the comments.</p>\n\n<ul>\n<li><p>I just installed Qt SDK on Windows, so I guess I have the latest version of everything (including MinGW?). The compiler is MinGW. Qt version is 4.8.1.</p></li>\n<li><p>I use no optimization flags because I noticed that they are automatically used when I build in release mode (with Qt Creator). It seems to me that if I write something like QMAKE_CXXFLAGS += -O1, this only has an effect in debug build.</p></li>\n<li><p>Lifetime of threads etc: this is pretty simple. When the user clicks the \"Compute\" button, 2 to 6 threads are launched simultaneously (depending on what he is computing), they are terminated when the computation ends. Nothing too fancy. Every thread just does brutal computations (except one, actually, which makes a (not so) small\"computation every 30ms, basically checking whether the error is small enough).</p></li>\n</ul>\n\n<p><strong>EDIT: latest developments and partial answers</strong></p>\n\n<p>Here are some new developments that provide answers about all this:</p>\n\n<ul>\n<li><p>I wanted to determine whether the difference in speed really had something to do with threads or not. So I modified the program so that the computation only uses 1 thread, that way we are pretty much comparing the performance on \"pure C++ code\". It turned out that now Windows was only slightly slower than Linux (something like 15%). So I guess that <strong>a small (but not unsignificant) part of the difference is intrinsic to the system, but the largest part is due to threads management</strong>.</p></li>\n<li><p>As someone (Luca Carlon, thanks for that) suggested in the comments, I tried building the application with the compiler for Microsoft Visual Studio (MSVC), instead of MinGW. And suprise, the computation (with all the threads and everything) was now \"only\" 20% to 50% slower than Linux! I think I'm going to go ahead and be content with that. I noticed that weirdly though, the \"pure C++\" computation (with only one thread) was now even slower (than with MinGW), which must account for the overall difference. So as far as I can tell, <strong>MinGW is slightly better than MSVC except that it handles threads like a moron</strong>.</p></li>\n</ul>\n\n<p>So, I’m thinking either there’s something I can do to make MinGW (ideally I’d rather use it than MSVC) handle threads better, or it just can’t. I would be amazed, how could it not be well known and documented ? Although I guess I should be careful about drawing conclusions too quickly, I’ve only compared things on one computer (for the moment).</p>\n"},{"tags":["sql","ruby-on-rails","performance"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":87,"score":0,"question_id":12944813,"title":"Rails 3 user matching-algorithm to SQL Query (COMPLICATED)","body":"<p>I'm currently working on an app that matches users based on answered questions.\nI realized my algorithm in normal RoR and ActiveRecord queries but it's waaay to slow to use it. To match one user with 100 other users takes</p>\n\n<pre><code>Completed 200 OK in 17741ms (Views: 106.1ms | ActiveRecord: 1078.6ms)\n</code></pre>\n\n<p>on my local machine. But still...\nI now want to realize this in raw SQL in order to gain some more performance. But I'm really having trouble getting my head around SQL queries inside of SQL queries and stuff like this plus calculations etc. My head is about to explode and I don't even know where to start.</p>\n\n<p>Here's my algorithm:</p>\n\n<pre><code>def match(user)\n  @a_score = (self.actual_score(user).to_f / self.possible_score(user).to_f) * 100\n  @b_score = (user.actual_score(self).to_f / user.possible_score(self).to_f) * 100\n\n  if self.common_questions(user) == []\n    0.to_f\n  else\n    match = Math.sqrt(@a_score * @b_score) - (100 / self.common_questions(user).count)\n    if match &lt;= 0\n      0.to_f\n    else\n      match\n    end\n  end\nend\n\ndef possible_score(user)\n  i = 0\n  self.user_questions.select(\"question_id, importance\").find_each do |n|\n    if user.user_questions.select(:id).find_by_question_id(n.question_id)\n      i += Importance.find_by_id(n.importance).value\n    end\n  end\n  return i\nend\n\ndef actual_score(user)\n  i = 0\n  self.user_questions.select(\"question_id, importance\").includes(:accepted_answers).find_each do |n|\n    @user_answer = user.user_questions.select(\"answer_id\").find_by_question_id(n.question_id)\n    unless @user_answer == nil\n      if n.accepted_answers.select(:answer_id).find_by_answer_id(@user_answer.answer_id)\n        i += Importance.find_by_id(n.importance).value\n      end\n    end\n  end\n  return i\nend\n</code></pre>\n\n<p>So basically a user answers a questions, picks what answers he accepts and how important that question is to him. The algorithm then checks what questions 2 users have in common, if user1 gave an answer user2 accepts, if yes then the importance user2 gave for each question is added which makes up the score user1 made. Also the other way around for user2. Divided by the possible score gives the percentage and both percentages applied to the geometric mean gives me one total match percentage for both users. Fairly complicated I know. Tell if I didn't explain it good enough. I just hope I can express this in raw SQL. Performance is everything in this.</p>\n\n<p>Here are my database tables:</p>\n\n<pre><code>CREATE TABLE \"users\" (\"id\" INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, \"username\" varchar(255) DEFAULT '' NOT NULL); (left some unimportant stuff out, it's all there in the databse dump i uploaded)\n\nCREATE TABLE \"user_questions\" (\"id\" INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, \"user_id\" integer, \"question_id\" integer, \"answer_id\" integer(255), \"importance\" integer, \"explanation\" text, \"private\" boolean DEFAULT 'f', \"created_at\" datetime);\n\nCREATE TABLE \"accepted_answers\" (\"id\" INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, \"user_question_id\" integer, \"answer_id\" integer);\n</code></pre>\n\n<p>I guess the top of the SQL query has to look something like this?</p>\n\n<pre><code>SELECT u1.id AS user1, u2.id AS user2, COALESCE(SQRT( (100.0*actual_score/possible_score) * (100.0*actual_score/possible_score) ), 0) AS match\nFROM \n</code></pre>\n\n<p>But since I'm not an SQL master and can only do the usual stuff my head is about to explode.\nI hope someone can help me figure this out. Or atleast improve my performance somehow! Thanks so much!</p>\n\n<h1>EDIT:</h1>\n\n<p>So based on Wizard's answer I've managed to get a nice SQL statement for \"possible_score\"</p>\n\n<pre><code>SELECT SUM(value) AS sum_id \nFROM user_questions AS uq1\nINNER JOIN importances ON importances.id = uq1.importance\nINNER JOIN user_questions uq2 ON uq1.question_id = uq2.question_id AND uq2.user_id = 101\nWHERE uq1.user_id = 1\n</code></pre>\n\n<p>I've tried to get the \"actual_score\" with this but it didn't work. My database manager crashed when I executed this.</p>\n\n<pre><code>SELECT SUM(imp.value) AS sum_id \nFROM user_questions AS uq1\nINNER JOIN importances imp ON imp.id = uq1.importance\nINNER JOIN user_questions uq2 ON uq2.question_id = uq1.question_id AND uq2.user_id = 101\nINNER JOIN accepted_answers as ON as.user_question_id =  uq1.id AND as.answer_id = uq2.answer_id\nWHERE uq1.user_id = 1\n</code></pre>\n\n<h1>EDIT2</h1>\n\n<p>Okay I'm an idiot! I can't use \"as\" as an alias of course. Changed it to aa and it worked! W00T!</p>\n"},{"tags":["javascript","jquery","performance","browser","rendering"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":1133,"score":1,"question_id":5190296,"title":"How to test rendering time (not DomContentLoaded nor onLoad)?","body":"<p>I need to have a way or tools to test the actual perceived rendering time for the browser to render the entire page to users. Any suggestions? </p>\n\n<p>The reason I ask is because firbug and Yslow only reports the DomContentLoaded and OnLoad time. </p>\n\n<p>For instance, my application reports 547ms (onLoad:621ms) for the contents. But the actual content is rendered around 3 seconds. I know so because I actually counted 1, 2, 3 slowly  from the moment I hit enter in the url field of the browser to the moment when content appears in front of my eyes. So I know 547ms nor 621ms DOES NOT represents the actual time it takes for the page to load. </p>\n\n<p>Not sure if this helps. But my application </p>\n\n<ol>\n<li><p>renders json data on the server side, save the data as a javascript variable along with the rest of the page's html before server returns the entire html to browser</p></li>\n<li><p>page loads Jquery 1.5 and Jquery template</p></li>\n<li><p>jquery code grabs the json data from the variable defined at step 1</p></li>\n<li><p>use jquery template to render the page. </p></li>\n</ol>\n\n<p>Technically, no Ajax involved here and images on the page are all cached. I don't see firebug downloads any of them. </p>\n\n<p>[Edit]</p>\n\n<p>What i'm trying to figure out is after the firebug reported onLoad time which in my case is 621ms, to the time the page is completed and loaded in my eyes (which is at least 3 seconds), what happened to the 2.4s in between? What took place there? Browser is doing something? Something is blocking? Network? what is it?</p>\n"},{"tags":["performance","matlab","optimization","vectorization"],"answer_count":1,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":129,"score":4,"question_id":12949936,"title":"Vectorization of matlab code","body":"<p>i'm kinda new to vectorization. Have tried myself but couldn't. Can somebody help me vectorize this code as well as give a short explaination on how u do it, so that i can adapt the thinking process too. Thanks. </p>\n\n<pre><code>function [result] = newHitTest (point,Polygon,r,tol,stepSize)\n%This function calculates whether a point is allowed.\n\n%First is a quick test is done by calculating the distance from point to \n%each point of the polygon. If that distance is smaller than range \"r\", \n%the point is not allowed. This will slow down the algorithm at some \n%points, but will greatly speed it up in others because less calls to the \n%circleTest routine are needed.\npolySize=size(Polygon,1);\ntestCounter=0;\n\nfor i=1:polySize\nd = sqrt(sum((Polygon(i,:)-point).^2));\n\nif d &lt; tol*r\n    testCounter=1;\n    break\nend\nend\n\nif testCounter == 0\ncircleTestResult = circleTest (point,Polygon,r,tol,stepSize);\ntestCounter = circleTestResult;\nend\n\nresult = testCounter;\n</code></pre>\n"},{"tags":["java","multithreading","performance","parallel-processing","fork-join"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":122,"score":2,"question_id":11809652,"title":"Performance problems using new fork join framework of jdk 7","body":"<p>I'm using the new forkjoin framework of jdk 7.\nI got a task, which has to be performed multiple times with different parameters.</p>\n\n<p>This task extends <code>RecursiveTask</code>. there are more than 100 tasks to perform, which can performed concurrently. the tasks are independent, so there should be no need for any synchronisation. \nTherefore I created at first the needed tasks and passed them to forkjoin thread pool.\nbut the application becomes slower, than running it without any parallelism.</p>\n\n<p>My first thought was, that i create to much threads..  Thats why i tried to recycle the threads to reduce object creation overhead, but this has no effect on the performance. for recycling im using the reinitialize() method. Also with recycling the performance is slower than running it without any parallelism.</p>\n\n<p>The operations performed in the tasks are not trivial, the duration of running threads are from 5 to 150 ms.   The application runs on a dualcore machine and im using ubuntu and oracle jdk 7.</p>\n"},{"tags":["java","regex","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":54,"score":0,"question_id":12962091,"title":"Java RegEx Performance with Pattern.CASE_INSENSITIVE","body":"<p>I got a pretty simple regular expression I am using </p>\n\n<pre><code>%%(products?)%%\n</code></pre>\n\n<p>Now I want it to be able to match both products? and Products?. The obvious answer is to use the CASE_INSENSITIVE tag when compiling a pattern: </p>\n\n<pre><code>Pattern.compile(\"%%(products?)%%\", Pattern.CASE_INSENSITIVE)\n</code></pre>\n\n<p>But on the <a href=\"http://docs.oracle.com/javase/1.4.2/docs/api/java/util/regex/Pattern.html#CASE_INSENSITIVE\" rel=\"nofollow\">documentation</a> it says \"Specifying this flag may impose a slight performance penalty.\" I therefore thought of an alternative, without the flag:</p>\n\n<pre><code>Pattern.compile(\"%%([Pp]roducts?)%%\")\n</code></pre>\n\n<p>My question is: Which one would have better performance?</p>\n"},{"tags":["ruby-on-rails","ruby","multithreading","performance"],"answer_count":2,"favorite_count":3,"up_vote_count":7,"down_vote_count":0,"view_count":602,"score":7,"question_id":5426540,"title":"How to speed up the processing of a large CSV using ruby","body":"<p>For a project I need to parse some pretty large CSV files. The contents of some of the entries is stored in a MySQL database. I am trying to speed this up using multithreading, but up to now this only slows things down.</p>\n\n<p>I parse a CSV file (up to 10GB), and some of these records (aprox. 5M out of a 20M+ record CSV) need to be inserted into a MySQL database. To determine which record needs to be inserted we use a Redis server with sets that contain the correct id's / references.</p>\n\n<p>Since we process around 30 of these files at any given time, and there are some dependencies, we store each file on a Resque queue and have multiple servers handling these (prioritized) queues.</p>\n\n<p>In a nutshell:</p>\n\n<pre><code>class Worker\n  def self.perform(file)\n    CsvParser.each(file) do |line|\n      next unless check_line_with_redis(line)\n      a = ObjectA.find_or_initialize_by_reference(line[:reference])\n      a.object_bs.destroy_all\n      a.update_attributes(line)\n    end\n  end\n</code></pre>\n\n<p>This works, scales nice horizontally (more CSV files = more servers), but larger CSV files pose a problem. We currently have files that take over 75 hours to parse this way. There are a number of optimizations I thought of already:</p>\n\n<p>One is cutting down on the MySQL queries; we instantiate AR objects while an insert with plain SQL, if we know the objects Id, is much faster. This way we can probably get rid of most of AR and maybe even Rails to remove overhead this way. We can't use a plain MySQL load data since we have to map the CSVs records to other entities that might have different Ids by now (we combine a dozen legacy databases into a new database).</p>\n\n<p>The other is trying to do more in the same time. There is some IO wait time, network wait time for both Redis and MySQL, and even while MRI uses green threads this might allow us to schedule our MySQL queries at the same time as the IO reads etc. But using the following code:</p>\n\n<pre><code>class Worker\n  def self.perform(file)\n    CsvParser.each(file) do |line|\n      next unless check_line_with_redis(line)\n      create_or_join_thread(line) do |myLine|\n        a = ObjectA.find_or_initialize_by_reference(myLine[:reference])\n        a.object_bs.destroy_all\n        a.update_attributes(myLine)\n      end\n    end\n\n    def self.create_or_join_thread(line)\n      @thread.join if @thread.present?\n      @thread = Thread.new(line) do |myLine|\n        yield myLine\n      end\n    end\n  end\n</code></pre>\n\n<p>This slowly slows down the process. When I <code>ps au</code> it starts off at 100% CPU, but as time progresses it goes down to just 2-3%. At that moment it does not insert new records at all, it just appears to hang.</p>\n\n<p>I have <code>strace</code>d the process, and at first I see MySQL queries pass by, after a while it appears it is not executing my ruby code at all. Could be a deadlock (it hung after parsing the <em>last</em> line of the CSV, but the process kept on running at 5% CPU and did not quit), or something I read here: <a href=\"http://timetobleed.com/ruby-threading-bugfix-small-fix-goes-a-long-way/\" rel=\"nofollow\">http://timetobleed.com/ruby-threading-bugfix-small-fix-goes-a-long-way/</a></p>\n\n<p>I am using Rails 2.3.8, REE, 1.8.7-2010.02 on Ubuntu 10.10. Any insights on how to handle large numbers of threads (or maybe why not to use threads here at all) is greatly appreciated!</p>\n"},{"tags":["c#","performance","optimization"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":72,"score":1,"question_id":12963203,"title":"Why is this List.Except so slow in some cases (and how to speed it up)?","body":"<p>I have the following two lists, which are pairs of strings.  One is what I expect and the other is what I found.  I want to find out what is missing.  The code works, but some cases are much, much slower than others.</p>\n\n<ul>\n<li>When n = 1, it takes 21 seconds for the <code>.Except()</code> call.  </li>\n<li>When n = 10, it takes 2 seconds for the <code>.Except()</code> call.  </li>\n</ul>\n\n<p>In both cases, it is the same number of elements.  Is this just some hash table collisions? What can I do to make all cases equally quick?</p>\n\n<pre><code>List&lt;KeyValuePair&lt;string, string&gt;&gt; FoundItems = new List&lt;KeyValuePair&lt;string, string&gt;&gt;();\nList&lt;KeyValuePair&lt;string, string&gt;&gt; ExpectedItems = new List&lt;KeyValuePair&lt;string, string&gt;&gt;();\n\nint n = 1;\nfor (int k1 = 0; k1 &lt; n; k1 ++)\n{\n    for (int k2 = 0; k2 &lt; 3500/n; k2++)\n    {\n        ExpectedItems.Add(new KeyValuePair&lt;string, string&gt;( k1.ToString(), k2.ToString()));\n        if (k2 != 0)\n        {\n            FoundItems.Add(new KeyValuePair&lt;string, string&gt;(k1.ToString(), k2.ToString()));\n        }\n    }\n}\n\nStopwatch sw = new Stopwatch();\nsw.Start();\n\n//!!!! This is the slow line.\nList&lt;KeyValuePair&lt;string, string&gt;&gt; MissingItems = ExpectedItems.Except(FoundItems).ToList();\n//!!!! \n\nstring MatchingTime = \"Matching Time: \" + sw.ElapsedMilliseconds.ToString() + \" (\" + sw.ElapsedMilliseconds / 1000 + \" sec)\";\nMessageBox.Show(MatchingTime + \", \" + ExpectedItems.Count() + \" items\");\n</code></pre>\n\n<p>My data really are strings, I just use integers in this test case because it's easy.</p>\n"},{"tags":["jquery","performance","jquery-validate"],"answer_count":3,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":657,"score":4,"question_id":5542014,"title":"jQuery validate large forms - script running slowly","body":"<p>I'm using jQuery Validate plugin 1.8.0 with jQuery 1.5. Works great for small to medium sized forms. For larger forms the performance degrades significantly (even in IE8 and FF4), sometimes causing the \"script is running too slowly\" message. It appears that the plugin scans the entire DOM within the form looking for attributes and classes to validate, even if you specified custom rules. Anyone know how to turn this off completely? There is an ignore option as well, but it still would scan the DOM, skipping those with the ignore attr.</p>\n\n<p>Here is what ASP.NET renders, except there are about 120 rows of data. Paging the results is not an option, unfortunately.</p>\n\n<pre><code>&lt;table id=\"GridView1\"&gt;\n    &lt;tbody&gt;\n        &lt;tr&gt;\n            &lt;th scope=\"col\"&gt;Header 1&lt;/th&gt;\n            &lt;th scope=\"col\"&gt;Header 2&lt;/th&gt;\n            &lt;th scope=\"col\"&gt;Header 3&lt;/th&gt;\n            &lt;th scope=\"col\"&gt;Header 4&lt;/th&gt;\n            &lt;th scope=\"col\"&gt;Header 5&lt;/th&gt;\n            &lt;th scope=\"col\"&gt;Header 6&lt;/th&gt;\n            &lt;th style=\"width: 60px; white-space: nowrap\" scope=\"col\"&gt;Header 7&lt;/th&gt;\n            &lt;th style=\"width: 60px; white-space: nowrap\" scope=\"col\"&gt;Header 8&lt;/th&gt;\n        &lt;/tr&gt;        \n        &lt;tr class=\"gridRow\" jquery1507811088779756411=\"3\"&gt;\n            &lt;td style=\"width: 50px\" align=\"middle\"&gt;\n                &lt;span id=\"GridView1_ctl03_Label1\"&gt;XXX&lt;/span&gt;\n            &lt;/td&gt;\n            &lt;td&gt;\n                &lt;span id=\"GridView1_ctl03_Label2\"&gt;YYY&lt;/span&gt;\n            &lt;/td&gt;\n            &lt;td style=\"width: 50px\" align=\"middle\"&gt;\n                &lt;span id=\"GridView1_ctl03_Label3\"&gt;ZZZ&lt;/span&gt;\n            &lt;/td&gt;\n            &lt;td align=\"middle\"&gt;\n                &lt;select style=\"width: 70px\" id=\"GridView1_ctl03_Dropdown4\" name=\"GridView1$ctl03$Dropdown4\"&gt;\n                    &lt;option selected value=\"Y\"&gt;Y&lt;/option&gt;\n                    &lt;option value=\"N\"&gt;N&lt;/option&gt;\n                &lt;/select&gt;\n            &lt;/td&gt;\n            &lt;td style=\"width: 50px\" align=\"middle\"&gt;\n                &lt;input id=\"GridView1_ctl03_hidId1\" value=\"100\" type=\"hidden\" name=\"GridView1$ctl03$hidId1\" /&gt;\n                &lt;input id=\"GridView1_ctl03_hidId2\" value=\"100\" type=\"hidden\" name=\"GridView1$ctl03$hidId2\" /&gt;\n                &lt;input id=\"GridView1_ctl03_hidId3\" value=\"100\" type=\"hidden\" name=\"GridView1$ctl03$hidId3\" /&gt;\n                &lt;input id=\"GridView1_ctl03_hidId4\" value=\"100\" type=\"hidden\" name=\"GridView1$ctl03$hidId4\" /&gt;\n                &lt;select style=\"width: 40px\" id=\"GridView1_ctl03_Dropdown5\" name=\"GridView1$ctl03$Dropdown5\"&gt;\n                    &lt;option selected value=\"A\"&gt;A&lt;/option&gt;\n                    &lt;option value=\"B\"&gt;B&lt;/option&gt;\n                &lt;/select&gt;\n            &lt;/td&gt;\n            &lt;td style=\"width: 50px\" align=\"middle\"&gt;\n                &lt;span id=\"GridView1_ctl03_Label6\"&gt;101&lt;/span&gt;\n            &lt;/td&gt;\n            &lt;td align=\"middle\"&gt;\n                &lt;input style=\"width: 60px\" id=\"GridView1_ctl03_Textbox8\" class=\"date required\"\n                    title=\"Please enter a valid start date.\" type=\"text\" name=\"GridView1$ctl03$Textbox8\"\n                    jquery1507811088779756411=\"122\" /&gt;\n            &lt;/td&gt;\n            &lt;td align=\"middle\"&gt;\n                &lt;input style=\"width: 60px\" id=\"GridView1_ctl03_Textbox9\" class=\"date\"\n                    title=\"Please enter a valid end date.\" type=\"text\" name=\"GridView1$ctl03$Textbox9\"\n                    jquery1507811088779756411=\"123\" /&gt;\n            &lt;/td&gt;\n        &lt;/tr&gt;\n    &lt;/tbody&gt;\n&lt;/table&gt;\n</code></pre>\n"},{"tags":["javascript","c++","performance","windows-8"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":57,"score":0,"question_id":12962177,"title":"Windows 8 project type performance differences","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/7466303/c-c-sharp-and-javascript-on-winrt\">C++, C# and JavaScript on WinRT</a>  </p>\n</blockquote>\n\n\n\n<p>I am wanting to create an app for Windows 8 and am equally well versed in C++ and JavaScript. I was wondering what (if any) performance differences there would be between a JavaScript/HTML or C++/XAML. Do they both get compiled to the same source or is the JavaScript version interpreted?</p>\n\n<p>Thanks!</p>\n"},{"tags":["iphone","ios","performance","uitableview","memory"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":35,"score":1,"question_id":12962051,"title":"UITableView Dynamic vs. Static Cell Allocation Performance / Memory Usage","body":"<p>Could anyone tell me the tradeoff in performance/memory usage between using static and dynamic cells in a UITableView?</p>\n\n<p>Here's my situation: I have a TableView with 6 different sections. The first section is the only section in my tableView that holds a different number of cells each time the view loads, depending on the current state of the app. i.e. I have declared 12 static cells for that section in interface builder, however I only display a certain number of those cells depending on the user's interaction with the app thus far. The other 5 tableView sections all contain UISwitches and textFields that never change.</p>\n\n<p>So say I statically allocated 50 cells for that first section, but still only displayed maybe just half of them depending on the state of the app. I would want to be able to display up to 50 cells though. How would this affect the speed or performance of my app? Would doing the entire tableView dynamically and redrawing the switches and textFields for the other sections each time lead to a better application performance?</p>\n"},{"tags":["sql-server","performance","sql-server-2008","tsql","insert"],"answer_count":3,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":120,"score":1,"question_id":12956286,"title":"Optimal insert/update batch for SQL Server","body":"<p>I'm making frequent inserts and updates in large batches from c# code and I need to do it as fast as possible, please help me find all ways to speed up this process.</p>\n\n<ol>\n<li>Build command text using <code>StringBuilder</code>, separate statements with <code>;</code> </li>\n<li>Don't use <code>String.Format</code> or <code>StringBuilder.AppendFormat</code>, it's slower then multiple <code>StringBuilder.Append</code> calls</li>\n<li>Reuse <code>SqlCommand</code> and <code>SqlConnection</code></li>\n<li>Don't use <code>SqlParameter</code>s (limits batch size)</li>\n<li>Use <code>insert into table values(..),values(..),values(..)</code> syntax (1000 rows per statement)</li>\n<li>Use as few indexes and constraints as possible</li>\n<li>Use simple recovery model if possible</li>\n<li>?</li>\n</ol>\n\n<p>Here are questions to help update the list above</p>\n\n<ol>\n<li>What is optimal number of statements per command (per one <code>ExecuteNonQuery()</code> call)?</li>\n<li>Is it good to have inserts and updates in the same batch, or it is better to execute them separately?</li>\n</ol>\n\n<p>My data is being received by tcp, so please don't suggest any Bulk Insert commands that involve reading data from file or external table. </p>\n\n<p>Insert/Update statements rate is about 10/3.</p>\n"},{"tags":["jquery","performance","table"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":41,"score":0,"question_id":12961526,"title":"How to show table rows after they've been hidden","body":"<p>I have a large table and allow the user to hide rows based upon a lot of complicated rules.\nI also have a 'reset' button that is to reveal all the rows back again, but it is running very slowly:</p>\n\n<pre><code>$('#myTable tbody tr').show('fast');\n</code></pre>\n\n<p>Q: Is there a fast way to show table rows that have been previously hidden with the hide method?</p>\n\n<p>Perhaps I should add class=\"hide\" and removeClass instead.</p>\n"},{"tags":[".net","performance"],"answer_count":3,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":163,"score":5,"question_id":1637903,"title":"Odd performance degradation in application","body":"<p>We have an application that is mixed .NET 2.0 and native C++. In our testing, we have a mode which can automatically loop through a set of projects. A project opens, runs, closes, repeat. Each of these steps requires creation/destruction of windows (winforms to be precise). Recently we've experienced some odd behavior in performance. After running for a few hours the opening and closing parts slow down (blocking the gui thread and showing half drawn screens etc). Now it would be easy to chock this up to a resource leak...but we're tracking handles and memory, and while memory grows slightly there's nothing to indicate this level of problem. Handles are stable. So maybe dangling event handlers...still need to investigate that. But the kicker, which perplexes me, is that shutting down the application and restarting it doesn't bring back the initial performance. It's still slow until I reboot the OS (win XP) and then performance starts out snappy again. This really perplexes me as I assume shutting down the application will reclaim all resources. Any thoughts?</p>\n"},{"tags":["android","performance","android-layout"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":47,"score":0,"question_id":12960967,"title":"Can my layout be improved for efficiency?","body":"<p>I have a layout as shown below. It is inflated by code and added to a HorizontalScrollView, sometimes a few hundred times, and causing getting memory issues.</p>\n\n<p>I'm wondering if there's anything that can be done to make it more efficient? Originally I used LinearLayouts, and replacing that with RelativeLayout made a huge difference to the scrolling. Now I'm wondering if it can be further improved?</p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"156dp\"\n    android:layout_height=\"254dp\"\n    android:paddingLeft=\"7dip\"\n    android:paddingRight=\"7dip\"&gt;\n\n    &lt;FrameLayout android:id=\"@+id/button_frame\"\n    android:layout_width=\"156dp\"\n    android:layout_height=\"218dp\"&gt;\n\n    &lt;ImageView android:id=\"@+id/button_bg\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"\n        android:src=\"@drawable/image_bg\"\n        /&gt;\n\n        &lt;ImageView android:id=\"@+id/button_image\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"\n        android:layout_gravity=\"bottom|right\"\n        /&gt;\n\n        &lt;TextView android:id=\"@+id/button_select\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"\n        android:layout_marginLeft=\"2dip\"\n        android:layout_marginRight=\"2dip\"\n        android:background=\"@drawable/btn_selector_bg_selected\"\n        /&gt;\n\n        &lt;TextView android:id=\"@+id/button_selected\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"wrap_content\"\n        android:layout_gravity=\"bottom\"\n        android:layout_marginLeft=\"2dip\"\n        android:layout_marginRight=\"2dip\"\n        android:background=\"@drawable/title_bg_selected\"/&gt;\n\n    &lt;/FrameLayout&gt;\n\n    &lt;TextView\n    android:id=\"@+id/button_title\"\n    android:layout_width=\"156dp\"\n    android:layout_height=\"36dp\"\n    android:textColor=\"#ffffff\"\n    android:singleLine=\"true\"\n    android:textSize=\"13sp\"\n    android:textStyle=\"bold\"\n    android:gravity=\"center_vertical|left\"\n    android:ellipsize=\"end\"\n    android:paddingLeft=\"30dip\"\n    android:paddingRight=\"5dip\"\n    android:layout_below=\"@id/button_frame\"\n    android:background=\"@drawable/title_bg\"/&gt;\n\n&lt;/RelativeLayout&gt;\n</code></pre>\n\n<p>The ImageView button_image is populated using AQuery image caching, so I'm not sure if there's much more I can do to improve the way the image is handled. But any tips on improvements greatly appreciated.</p>\n"},{"tags":["performance","matlab","nested-loops","matrix-multiplication"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":124,"score":1,"question_id":12944026,"title":"MATLAB - Help speed up the code","body":"<p>I need your help to speed up my MATLAB code. Line 17 is the most expensive part. It is because of the two nested loops. I need help with removing the loops and rewrite it into just one matrix multiplication expression. Note that I have taken dKdx as a cell, which is causing problems to replace nested loops with simple matrix multiplication term. Any ideas?\nBelow if a simplified code. May be dKdx doesn't need to be an cell? Idea behind cell was to be able to store lot of matrices of size [2*(nelx+1)<em>(nely+1),2</em>(nelx+1)*(nely+1)].</p>\n\n<pre><code>    clc\n    nelx = 16; nely = 8;\n    dKdx = cell(2*(nelx+1)*(nely+1),1);\n    Hess = zeros(nelx*nely,nelx*nely);\n    U = rand(2*(nelx+1)*(nely+1),1);\n    dUdx = rand(2*(nelx+1)*(nely+1),nelx*nely);\n\n    for elx = 1:nelx\n        for ely = 1:nely\n            elm = nely*(elx-1)+ely;               \n            dKdx{elm,1} = rand(2*(nelx+1)*(nely+1),2*(nelx+1)*(nely+1));\n        end\n    end\n\n    for i = 1:nelx*nely    \n        for j = i:nelx*nely\n            Hess(i,j) = U'*dKdx{j,1}*dUdx(:,i);\n            if i ~= j\n                Hess(j,i) = Hess(i,j);\n            end\n        end\n    end\n</code></pre>\n"},{"tags":["java","performance","profiling","cpu"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":44,"score":0,"question_id":12960167,"title":"Can CPU timing in Java VisualVM show total method durations?","body":"<p>I am not sure if Java VisualVM does not do this, or if I am not using the correct terminology, or seeing the menu option.</p>\n\n<p>Say one has done a CPU profile of an app with 3 methods.</p>\n\n<pre><code>time 0001: method A called\ntime 0002:   method B called\ntime 0004:   method B exits\ntime 0005:   method B called\ntime 0007:   method B exits\ntime 0008: method A exits\ntime 0009: method C called\ntime 0010:   method B called\ntime 0012:   method B exits\ntime 0013: method C exits\n</code></pre>\n\n<p>The only view I can get to shows me:</p>\n\n<pre><code>method B duration 6\nmethod A duration 2\nmethod C duration 2\n</code></pre>\n\n<p>But one should be able to flip the perspective (and does this have a standard name?) to see:</p>\n\n<pre><code>method A duration 7\nmethod B duration 6\nmethod C duration 4\n</code></pre>\n\n<p>How can I get this second view of the CPU timing out of Java VisualVM? Am I not seeing the option, or is it not there?</p>\n\n<p>I see this: <a href=\"http://stackoverflow.com/questions/1892038/total-method-time-in-java-visualvm\">Total method time in Java VisualVM</a></p>\n\n<p>But I do not see anything in the Call Stack that is from any of my objects. Would I have to be comparing two snapshots? This is incredibly non-obvious....</p>\n"},{"tags":["c#","wpf","performance","mediaelement"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":70,"score":0,"question_id":12957121,"title":"WPF - limitation for playing mediaelement at once","body":"<p>I have function for searching videos in folder. For every video file in that folder I am adding MediaElement and starts playing. When I have cca 10 videos it was all right. Then I added some videos, change view element from grid to canvas (because of performance) and now some mediaelement didn´t show (there is blank place where they should be). It is not one and same video. Mostly these happens to videos that are later procesing but not always. Does anyone know where could be problem? I think it is not problem with performance because the rest of videos are playing all right and application working fine. So is there some limitations? Or what I am doing wrong? Testing videos are .wmv and low quality (320x240).</p>\n\n<p>View:</p>\n\n<pre><code>&lt;Canvas x:Name=\"mainCanvas\"&gt;\n            &lt;ScrollViewer HorizontalScrollBarVisibility=\"Auto\" Width=\"1680\" Height=\"750\"&gt;\n                &lt;Canvas x:Name=\"videoCanvas\"&gt;\n                &lt;/Canvas&gt;\n            &lt;/ScrollViewer&gt;\n&lt;/Canvas&gt;\n</code></pre>\n\n<p>and code from MainWindow.xaml.cs</p>\n\n<pre><code> public MainWindow()\n        {\n            InitializeComponent();\n\n            this.WindowStyle = WindowStyle.None;\n            this.WindowState = WindowState.Maximized;\n\n            getAllVideosFromFolder(System.IO.Path.GetFullPath(@\"Videos\\\"));\n        }\n\n        private void getAllVideosFromFolder(string path)\n        {\n            try\n            {\n                var videoFiles = DirectoryHelper.GetFilesByExtensions(new DirectoryInfo(path), \".wmv\", \".mp4\", \".avi\", \".mov\");\n                int i = 0, j = 0;\n\n                foreach (var item in videoFiles)\n                {\n                    MediaElement melem = createMediaElementForPreview(item.FileName);\n\n                    Canvas.SetTop(melem, i * 250);\n                    Canvas.SetLeft(melem, j * 340);\n                    videoCanvas.Children.Add(melem);\n\n                    i++;\n                    if (i &gt; 2)\n                    {\n                        i = 0;\n                        j++;\n                    }\n\n                    videos.Add(videoClass);\n                }\n                videoCanvas.Width = j * 340;\n            }\n            catch (Exception ex)\n            {\n                MessageBox.Show(ex.Message);\n            }\n        }\n\n        private MediaElement createMediaElementForPreview(string sourcePath)\n        {\n            MediaElement melem = new MediaElement();\n            melem.LoadedBehavior = MediaState.Manual;\n            melem.Source = new Uri(sourcePath, UriKind.Relative);\n        melem.Width = 320;\n        melem.Height = 240;\n        melem.Volume = 0;\n        melem.Play();\n        melem.MouseDown += melem_MouseDown;\n        return melem;\n    }\n</code></pre>\n"},{"tags":["javascript","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":12957540,"title":"javascript optimization to avoid function context","body":"<p>I have the following code (trucated for clarity):</p>\n\n<pre><code>if ( typeof CPM == \"undefined\" || !CPM) {var CPM={};}\n\n(function(){\n  CPM.process = {\n    createStyles: function() {\n      //Do style processing\n      alert(\"processing styles\");\n    },\n    createClasses = {\n      //Do style processing\n      alert(\"processing classes\");      \n    },\n    init: function(){CPM.process.createStyles()}\n  };\n})();\n</code></pre>\n\n<p>init is a convention used in the company and must be there for other reasons. What I am trying to avoid is an additional function call for init. Is there an alternative way to write the same init and avoid creation of the function context? Like:</p>\n\n<pre><code>init: CPM.process.createStyles\n</code></pre>\n\n<p>The above fails, but am trying to see if there is any other way of achieving the optimization</p>\n"},{"tags":["performance","matlab","optimization","for-loop","vectorization"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":55,"score":-1,"question_id":12957703,"title":"Need to optimize this matlab code..vectorization will help or not?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/12895609/optimization-a-recurring-matlab-code\">Optimization a recurring matlab code</a>  </p>\n</blockquote>\n\n\n\n<p>Is vectorization a good option in optimizing this piece of code? What criteria decides, whether we vectorize a code or not? What else can be done?</p>\n\n<pre><code>function [oddNodes] = pointInPolygon (point,thePolygon)\n% determine if a point is in the polygon (faster than matlab \"inpolygon\"\n% command\n\n polyPoints=size(thePolygon,1);     %number of polygon points\n oddNodes = false;\n\nj=polyPoints;\nx=point(1); y=point(2);\n\nfor i=1:polyPoints\nif (thePolygon(i,2)&lt;y &amp;&amp; thePolygon(j,2)&gt;=y ||  thePolygon(j,2)&lt;y &amp;&amp; thePolygon(i,2)&gt;=y)\n    if (thePolygon(i,1)+(y-thePolygon(i,2))/(thePolygon(j,2)-thePolygon(i,2))*(thePolygon(j,1)-thePolygon(i,1))&lt;x)\n        oddNodes=~oddNodes;\n    end\nend\nj=i; \nend\n</code></pre>\n"},{"tags":["c#",".net","windows","performance","filesystems"],"answer_count":7,"favorite_count":7,"up_vote_count":9,"down_vote_count":0,"view_count":13874,"score":9,"question_id":1602578,"title":"C#: What is the fastest way to generate a unique filename?","body":"<p>I've seen several suggestions on naming files randomly, including using </p>\n\n<pre><code>System.IO.Path.GetRandomFileName()\n</code></pre>\n\n<p>or using a </p>\n\n<pre><code>System.Guid\n</code></pre>\n\n<p>and appending a file extension.  </p>\n\n<p>My question is:  <strong>What is the fastest way to generate a unique filename?</strong></p>\n"},{"tags":["database","performance","latency"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":1674,"score":1,"question_id":605648,"title":"Database Network Latency","body":"<p>I am currently working on an n-tier system and battling some database performance issues. \nOne area we have been investigating is the latency between the database server and the application server. In our test environment the\naverage ping times between the two boxes is in the region of 0.2ms however on the clients site its more in the region of 8.2 ms. Is that\nsomthing we should be worried about?</p>\n\n<p>For your average system what do you guys consider a resonable latency and how would you go about testing/measuring the latency?</p>\n\n<p>Karl</p>\n"},{"tags":["objective-c","ios","c","performance","zooming"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":51,"score":0,"question_id":12954741,"title":"Math Graph Zoom-In - coordinates and gridlines (for-loop get too slow)","body":"<p>maybe someone could help ?</p>\n\n<p>I develop a graphic calculator for iphone and have a problem\nwith the zoom In funktion.</p>\n\n<p>I have some coordinate system width gridlines and x-axis and y-axis.\ni can move the graph left, right, up, down.\nAnd i can zoom in and out.</p>\n\n<p>If i pan left and then i zoom in, the centre is in the middle and the gridlines go\nto left and right side. Also the Y-axis pan left out.</p>\n\n<p>I realized the gridlines with a for-loop, which starts from the y-axis and end to the bounds of the screen.</p>\n\n<p>The Problem is, if i zoom in very deep, the for-loop have to handle a lot of iterations and the performance get very slow.\nAbove all, if the y-axis is panned very left Side. So the iterations get very big.</p>\n\n<p>Hope someone understand my problem and can help.\nI try to post some code here:</p>\n\n<pre><code>- (void)drawRect:(CGRect)rect\n{\n\n    CGContextRef context = UIGraphicsGetCurrentContext();\n\n\n    // Pan Left and Right\n    x_Global = 160.0 + (lastTouch.x - FirstTouch.x);\n    y_Global = 210.0 + (lastTouch.y - FirstTouch.xy);\n\n    // Zoom\n    x_GlobalAfterZoom = 160.0 - ((160.0 - x_Global) * factor_x_Zoom);\n    y_GlobalAfterZoom = 160.0 - ((160.0 - y_Global) * factor_y_Zoom);\n\n\n    // draw axis\n    CGContextSetLineWidth(context, 0.5);\n    CGContextSetStrokeColorWithColor(context, [UIColor blackColor].CGColor);\n    CGContextMoveToPoint(context, x_MinusBounds, y_GlobalAfterZoom);\n    CGContextAddLineToPoint(context, x_PlusBounds, y_GlobalAfterZoom);\n    CGContextMoveToPoint(context, x_GlobalAfterZoom, y_MinusBounds);\n    CGContextAddLineToPoint(context, x_GlobalAfterZoom, y_PlusBounds);\n    CGContextSetAllowsAntialiasing(context, true);\n    CGContextStrokePath(context);\n\n\n    //draw Gidlines Positive\n    double gitterStepSize = gridLines Distance * gridlinesZoomFactor; //     gridlinesZoomFactor depends on factor_x_Zoom\n\n    for (x_Gridlines = x_GlobalAfterZoom; x_Gridlines &lt;= x_PlusBounds; x_Gridlines +=  GitterStepSize)\n    {\n        if (x_Gridlines &gt;= x_MinusBounds &amp;&amp; x_Gridlines &lt;= x_PlusBounds)\n        {\n              CGContextSetLineWidth(context, 0.15);\n              CGContextSetStrokeColorWithColor(context, [UIColor blackColor].CGColor);\n              CGContextMoveToPoint(context, x_Gridlines, y_MinusBounds);\n              CGContextAddLineToPoint(context, x_Gridlines, y_PlusBounds);\n              CGContextSetAllowsAntialiasing(context, true);\n              CGContextStrokePath(context);\n        }\n    }\n\n}\n</code></pre>\n"},{"tags":["ruby-on-rails","performance","windows-7","64bit","ubuntu-11.10"],"answer_count":3,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":418,"score":3,"question_id":9383308,"title":"Slow rails server inside VMware using RVM","body":"<p>I'm running Windows 7 with a Ubuntu 11.10 x86_64/desktop virtual machine (2gb ram) where I use rvm to manage my rails application (rvm is using 1.9.2-p290).  Myself and another co-worker are both experiencing brutally slow server responses - some of the simple pages are taking anywhere from 2 to 7 minutes to load, and I'm connecting from both inside the VM and on my host computer, so there isn't a network issue.\nI was running Ubuntu 11.10 32-bit server, and rails was running very well with no slow down on the same host.  Unfortunately for me, because of the work that I'm doing, I need to be using 64 bit Ubuntu, which is unusable slow.</p>\n\n<p>Does anyone have any suggestions</p>\n"},{"tags":["android","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":33,"score":0,"question_id":12957730,"title":"Canvas.drawBitmap extremely high cpu consumption","body":"<p>I have a simple Android application which consists of one View, that holds a Gallery widget.\nI load up 10 images to the Gallery object upon view creation.</p>\n\n<p>The FPS during the image transitions (when I swipe right for next image, left for previous) is <em>low</em> - it's just not smooth.</p>\n\n<p>I've used the Method profiling function built in eclipse to profile the application, and noticed that <code>Canvas.drawBitmap</code> consumes a lot of CPU.</p>\n\n<p>I'm currently targeting Android 2.3.3, so <code>hardwareacceleration</code> is irrelevant.</p>\n\n<p>Furthermore, downsampling the bitmaps did not change anything - even when downsampling by a large scale.</p>\n\n<p>Currently using Samsung Galaxy TAB</p>\n\n<p>Any insights on this?</p>\n"},{"tags":["java","performance","logging","slf4j"],"answer_count":6,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":271,"score":4,"question_id":8444266,"title":"Even with slf4j, should you guard your logging?","body":"<p>Help me out in a debate here.. :)</p>\n\n<p>The slf4j site here <a href=\"http://www.slf4j.org/faq.html#logging_performance\" rel=\"nofollow\">http://www.slf4j.org/faq.html#logging_performance</a> indicates that because of the parameterized logging, logging guards are not necessary.  I.e. instead of writing:</p>\n\n<pre><code>if(logger.isDebugEnabled()) {\n  logger.debug(\"Entry number: \" + i + \" is \" + String.valueOf(entry[i]));\n}\n</code></pre>\n\n<p>You can get away with:</p>\n\n<pre><code>Object entry = new SomeObject();\nlogger.debug(\"The entry is {}.\", entry);\n</code></pre>\n\n<p>Is this really okay, or does it incur the (albeit lower) cost of creating the static string that's passed to the trace method..?</p>\n"},{"tags":["java","performance","arraylist"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":75,"score":0,"question_id":12956503,"title":"Java merge arraylist is slow","body":"<p>So I have an application that I start on 2PC. Each instance launches an HSQLDB in server mode.</p>\n\n<p>I'm trying to get the sales of different products.</p>\n\n<p>So I query the local database and fill an arraylist with the name of the product and the number of sales.</p>\n\n<p>Then I execute the same query on the other database on the other PC.</p>\n\n<p>For 1 product, I so have two lines (each one correspond to one database).\nHere, results are false but the execution time is ok.</p>\n\n<p>In order to manage that, I did the following : </p>\n\n<pre><code>ResultSet rs2 = state2.executeQuery(produitsQuery);\nwhile (rs2.next()) {\n   for (int i = 0; i &lt; produits.size(); i++) {\n       obj = ((Object[]) produits.get(i));\n       idpdt = (Integer) obj[1];\n\n       if (idpdt == rs2.getInt(1)) {\n           nb = (Integer) obj[3];\n           valo = (Double) obj[4];\n\n           nb += rs2.getDouble(4);\n           valo += rs2.getDouble(5);\n           produits.set(i, new Object[]{\n               rs2.getString(\"famille\"),\n               rs2.getInt(\"id_pdt\"),\n               rs2.getString(\"nom_pdt\"),\n               nb,\n               valo,\n               s2.getString(\"sous_famille\")});\n           k = 1;\n       }\n    }\n    if (k == 0) \n        produits.add(new Object[]{\n            rs2.getString(\"famille\"),\n            rs2.getInt(\"id_pdt\"),\n            rs2.getString(\"nom_pdt\"),\n            rs2.getInt(\"nb\"),\n            rs2.getDouble(\"valo\"),\n            rs2.getString(\"sous_famille\")});\n\n}\n</code></pre>\n\n<p>Results are perfect but the execution time is very very slow and that's a problem. I think it's because I loop the entire arraylist at every row of the resultset.</p>\n\n<p>What others solutions might I use to make the execution time faster?</p>\n"},{"tags":["ios","performance","optimization","uiimage","nsurlconnection"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":38,"score":0,"question_id":12940176,"title":"iOS optimize image download speed from own server","body":"<p>I have a gallery and detailed view controllers in my application, when a user taps a thumbnail in the gallery I redirect them to the detailed controller and begin to asynchronously download original-sized image, which is 640x640 px and it takes like 5-6 seconds to download. Is there a way to optimize the image download speed?</p>\n\n<p>For example Instagram does this in around 0.2-1.0 second, like incredibly fast. My thinking is that they use some kickass compression on the server side and then unarchive the image they got in the Application. Are there any ways to do something like that?</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["multithreading","performance","d","thread-local-storage"],"answer_count":6,"favorite_count":7,"up_vote_count":16,"down_vote_count":0,"view_count":5126,"score":16,"question_id":506093,"title":"Why is thread local storage so slow?","body":"<p>I'm working on a custom mark-release style memory allocator for the D programming language that works by allocating from thread-local regions.  It seems that the thread local storage bottleneck is causing a huge (~50%) slowdown in allocating memory from these regions compared to an otherwise identical single threaded version of the code, even after designing my code to have only one TLS lookup per allocation/deallocation.  This is based on allocating/freeing memory a large number of times in a loop, and I'm trying to figure out if it's an artifact of my benchmarking method.  My understanding is that thread local storage should basically just involve accessing something through an extra layer of indirection, similar to accessing a variable via a pointer.  Is this incorrect?  How much overhead does thread-local storage typically have?</p>\n\n<p>Note:  Although I mention D, I'm also interested in general answers that aren't specific to D, since D's implementation of thread-local storage will likely improve if it is slower than the best implementations.</p>\n"},{"tags":["python","performance","bash","sed","logging"],"answer_count":4,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":372,"score":5,"question_id":9653507,"title":"trim big log file","body":"<p>i perform performance tests for a few java applications. Applications produce very big log files( it can be 7-10 GB) during the test . I need to trim these log files between specific dates and time. currently, i use python script, which parses log timestamps in datetime python object and print only matched strings. But this solution is very slow. 5 GB log is parsed about 25 minutes\nObviously entries in log file is sequentially and i don't need to read all file line by line. \nI thought about reading file from the start and from the end, until condition is matched and print files between matched number of lines. But i don't know how can i read file from the backwards, without downloading it to the memory.</p>\n\n<p>Please, can you suggest me any suitibale solution for this case.</p>\n\n<p>here is part of python script:</p>\n\n<pre><code>      lfmt = '%Y-%m-%d %H:%M:%S'\n      file = open(filename, 'rU')\n      normal_line = ''\n      for line in file:\n        if line[0] == '[':\n          ltimestamp = datetime.strptime(line[1:20], lfmt)\n\n          if ltimestamp &gt;= str and ltimestamp &lt;= end:\n            normal_line = 'True'\n        else:\n          normal_line = ''\n\n      if normal_line:\n        print line,\n</code></pre>\n"},{"tags":["performance","big-o","quicksort","worst-case"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":12946837,"title":"How to prove Quicksort is O(n*lg n) with special conditions?","body":"<p>This is one the questions on my Data Structures homework:</p>\n\n<p>Suppose that we have a linear-time procedure that is guaranteed to find a pivot element for Quicksort such that\nat least 1% of the array is less than or equal to the pivot and at least 1% is greater than or equal to the pivot.\nShow that Quicksort will then have worst-case complexity O(n lg n).</p>\n\n<p>I know that worst-case complexity for quicksort in general is O(n^2). I read that this happens when all the values of the pivot chosen is either the largest or smallest of the taken set.</p>\n\n<p>My guess is that, because of the given condition that at least 1% of the array be bigger and at least 1% of the array be smaller than the pivot, this eliminates the situation where the pivot is the smallest or largest element in the set. Thus it can never be O(n^2)</p>\n\n<p>Does this sound correct? </p>\n"},{"tags":[".net","performance","iis7","64bit"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":1346,"score":0,"question_id":6182667,"title":".Net Application running slow in IIS7 / Server 2008 64bit","body":"<p>We have a ASP .Net Application (built for 3.5 SP1) which when compiled is done so in Visual Studio 2008 using the \"All CPU's\" option.  It is currently hosted in a Windows 2003 (32bit) IIS6 environment (Virtual server) and connects to a SQL 2008 (64bit) Server. The current Application Server is running x2 Xeon  E5520 CPU's @2.27GHz with 4GM RAM.</p>\n\n<p>With this current setup, the application performs as well as it should.  Recently I have setup a new virtual server running Windows Server 2008 R2 (64bit) and IIS7 running on x2 Xeon E5530 CPU's running @ 2.4GHz with 6GB RAM.  I have setup our existing .Net application on this new server which is still connecting through to the same database server.</p>\n\n<p>Unfortunately though, for reasons beyond my understanding our application performs really poorly on this new server (which when looking at the specs should operate better than the old server)???  Pages seem to take twice as long to load (possibly taking longer to query the DB..?) etc..</p>\n\n<p>Could anyone provide any insight for me that might indicate why this might be?  Our networking guys profess that the new server is setup exactly the same as the old one, so I can't see it being an issue when the application server not being able to access the sql server on the same ports etc.. as the old 32bit server.. all very strange :s</p>\n\n<p>Cheers</p>\n\n<p>Greg</p>\n"},{"tags":["performance","computer-science","performance-testing","measurement","perf"],"answer_count":0,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":38,"score":1,"question_id":12930276,"title":"How to measure interference between processes","body":"<p>In parallel systems every process has an impact onto other processes, because they all compete for several scarce resources like cpu-caches, memory, disk I/O, network, etc.</p>\n\n<p>What method is best suited for measuring interference between processes? Such as Process A &amp; B each access the disk heavily. So running them parallel will probably slower then running sequential (individual runtime). Because the bottleneck is the hard drive.</p>\n\n<p>If I don't know exactly the behaviour of a process (disk-, memory- or cpu- intensive), what method would be best to analyse that?</p>\n\n<p>Measure individual runtime and compare the relative share of each parallel process?</p>\n\n<p>Like process A runs on average 30s alone, when 100% parallel with B 45s, when 20% parallel 35s.. etc ??</p>\n\n<p>Would it be better to compare several indicators like L1 &amp; LLC cache misses, page faults, etc.??</p>\n"},{"tags":["performance","r","mean"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":82,"score":1,"question_id":12759937,"title":"Why are `colMeans()` and `rowMeans()` functions faster than using the mean function with `lapply()`?","body":"<p>What I want to ask is, algorithmically, what do the <code>rowMeans()</code> and <code>colMeans()</code> functions do to optimize speed?</p>\n"},{"tags":["python","regex","performance","perl","text-processing"],"answer_count":6,"favorite_count":1,"up_vote_count":15,"down_vote_count":0,"view_count":468,"score":15,"question_id":12793562,"title":"text processing - python vs perl performance","body":"<p>Here is my perl and python script to do some simple text processing from about 21 log files each about 300KB to 1MB (max) x 5 times repeated (total of 125 files, due to the <em>log</em> repeated 5 times).</p>\n\n<p><strong>Python Code</strong> (code modified to use compiled re and using re.I)</p>\n\n<pre><code>#!/usr/bin/python\n\nimport re\nimport fileinput\n\nexists_re = re.compile(r'^(.*?) INFO.*Such a record already exists', re.I)\nlocation_re = re.compile(r'^AwbLocation (.*?) insert into', re.I)\n\nfor line in fileinput.input():\n    fn = fileinput.filename()\n    currline = line.rstrip()\n\n    mprev = exists_re.search(currline)\n\n    if(mprev):\n        xlogtime = mprev.group(1)\n\n    mcurr = location_re.search(currline)\n\n    if(mcurr):\n        print fn, xlogtime, mcurr.group(1)\n</code></pre>\n\n<p><strong>Perl Code</strong></p>\n\n<pre><code>#!/usr/bin/perl\n\nwhile (&lt;&gt;) {\n    chomp;\n\n    if (m/^(.*?) INFO.*Such a record already exists/i) {\n        $xlogtime = $1;\n    }\n\n    if (m/^AwbLocation (.*?) insert into/i) {\n        print \"$ARGV $xlogtime $1\\n\";\n    }\n\n}\n</code></pre>\n\n<p>And, on my PC both code generates exactly the same result file of 10,790 lines. And, here is the timing done on cygwin perl and python</p>\n\n<pre><code>User@UserHP /cygdrive/d/tmp/Clipboard\n# time /tmp/scripts/python/afs/process_file.py *log* *log* *log* *log* *log* &gt;\nsummarypy.log\n\nreal    0m8.185s\nuser    0m8.018s\nsys     0m0.092s\n\nUser@UserHP /cygdrive/d/tmp/Clipboard\n# time /tmp/scripts/python/afs/process_file.pl *log* *log* *log* *log* *log* &gt;\nsummarypl.log\n\nreal    0m1.481s\nuser    0m1.294s\nsys     0m0.124s\n</code></pre>\n\n<p>Originally, It took 10.2 secs using Python and only 1.9 secs using Perl for this simple text processing.</p>\n\n<p><strong>(UPDATE) but, after the compiled re version of python, it now takes 8.2 seconds in python and 1.5 seconds in perl. Still perl is much faster.</strong></p>\n\n<p>Is there anyway to improve the speed of Python at all OR it is obvious you expert guys that Perl will be the speedy one for simple text processing.</p>\n\n<p>By the way this was not the only test I did for simple text processing... And, each different way I make the source code, always always Perl wins by a large margin. And, not once did Python performed better for simple <code>m/regex/</code> match and print stuff.</p>\n\n<p>Thanks for your input.</p>\n\n<blockquote>\n  <p>Please do not suggest to use C, C++, Assembly, other flavours of\n  Python, etc.</p>\n  \n  <p>I am looking for a solution using Standard Python with its built-in\n  modules compared against Standard Perl (not even using the modules).\n  Boy, I wish to use Python for all my tasks due to its readability, but\n  to give up speed, I don't think so.</p>\n  \n  <p>So, please suggest how can the code be improved to have comparable\n  results with perl.</p>\n</blockquote>\n\n<p><strong>UPDATE: 18OCT2012</strong> </p>\n\n<p>As other users suggested, Perl has its place and Python has its.</p>\n\n<p>So, for this question, one can safely conclude that for simple regex match on each line for hundreds or thousands of text files and writing the results to a file (or printing to screen), <strong>Perl will always, always WIN in performance for this job, as simple as that.</strong></p>\n\n<p>Please note that when I say Perl wins in performance.. only standard Perl and Python is compared... not resorting to some obscure modules (obscure for a normal user like me) and also not calling C, C++, Assembly libraries from python or perl. We don't have time to learn all these extra steps and installation for a simple text matching job.</p>\n\n<p>So, Perl rocks for text processing and regex.</p>\n\n<p>Python has its place to rock in other places.</p>\n"},{"tags":["mysql","performance","query","order"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":55,"score":1,"question_id":12952903,"title":"MySQL ORDER BY - Slowing Down Query","body":"<p>Question pertains to a listing site that holds listing data and lead data in two tables.  When doing a query for listings and total number of leads for each listing, the results are extremely slow after adding an ORDER BY. Without the ORDER BY, the results are retrieved very fast.  Any advice or help with restructuring the query below would be awesome!! Fyi, there are 20k listings and 100k leads.</p>\n\n<pre><code>SELECT ls.*, IFNULL(ld.total_leads, 0) AS total_leads\nFROM listing ls \nLEFT JOIN (SELECT listing_id, COUNT(listing_id) AS total_leads \n            FROM lead GROUP BY listing_id) ld\nON (ls.listing_id = ld.listing_id)\nORDER BY ls.listing_id DESC LIMIT 0,20\n</code></pre>\n"},{"tags":["asp.net-mvc","asp.net-mvc-3","performance","mini-profiler"],"answer_count":3,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":82,"score":0,"question_id":11154966,"title":"Strange MVC3 performance issue","body":"<p>I have a strange issue that is occurring before my code appears to reach my controller.</p>\n\n<p>From using the SO Mini-MVC-Profiler, I've found that a certain request is taking 500ms (500ms!!!), and for what I know it is doing; it's excessively high!</p>\n\n<p><img src=\"http://i.stack.imgur.com/QUz2B.png\" alt=\"enter image description here\"></p>\n\n<p>We do use unity IoC to create our dependencies, and in the ase of the below EntityController being created, there is a AppServices dependently to be created, which does require some service classes to be instantiated, but I wouldn't expect this takes 500ms, it's simply constructors.</p>\n\n<p>Is there any way I could debug to find out specifically where time is being taken up?</p>\n\n<p>Thanks,</p>\n"},{"tags":["performance","64bit","32bit"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":55,"score":0,"question_id":12944500,"title":"32 bit/64 bit Compiling and Application Speed","body":"<p>I just got a new 64 bit computer and I am still trying to understand the differences between 32 bit and 64 bit. I understand that applications built using 64-bit dependencies can only run on a 64 bit, but applications built with 32-bit dependencies can run on both 32 and 64 bit systems. </p>\n\n<p>However, is there any other differences? I know some programs have two different windows versions you can download, one for 64 bit and one for 32 bit. Why do they provide the two different types? Is there a speed increase for compiling a program with 64 bit dependencies for a program to run on a 64 bit system?</p>\n"},{"tags":["django","performance","templates","render","nodes"],"answer_count":0,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":31,"score":1,"question_id":12952707,"title":"Is there a performance hit for Django tags using takes_context?","body":"<p>I sometimes access the context by supplying 'takes_context=True' for Django tags; Usually to acess the request.</p>\n\n<p>But are there performance implications.</p>\n\n<p>My mental model of how templates worke is that the tag function assembles the nodes representing the template, and that thereafter these nodes can render content without recreating the node, or reparsing the template.</p>\n\n<p>But surely, if the tag function can be made to return different nodes, depending on\nSomething in the context, then the nodes will have to be recreated everytime the context is different (I.E every time).</p>\n\n<p>Either that, or you are stuck with what nodes you get first time round, in which case you shouldn't return nodes based on anything in the context (in which case, what's the point?).</p>\n\n<p>Can someone clear this up for me? I' using Django 1.4.2.</p>\n"},{"tags":["mysql","datatable","table","performance"],"answer_count":3,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":2149,"score":3,"question_id":2887619,"title":"Mysql medium int vs. int performance","body":"<p>I have a simple users table, I guess the maximum users I am going to have is 300,000.</p>\n\n<p>Currently I am using:</p>\n\n<pre><code> CREATE TABLE users\n (\n         id INT UNSIGNED AUTOINCREMENT PRIMARY KEY,\n         ....\n</code></pre>\n\n<p>Of course I have many other tables for which users(id) is a FOREIGN KEY. </p>\n\n<p>I read that since the id is not going to use the full maximum of INT it is better to use:\nMEDIUMINT and it will give better performance.</p>\n\n<p>Is it true?</p>\n\n<p>(I am using mysql on Windows Server 2008)</p>\n"},{"tags":["performance","matlab","mex"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":111,"score":4,"question_id":12921705,"title":"Matlab: Does calling the same mex function repeatedly from a loop incur too much overhead?","body":"<p>I have some Matlab code which needs to be speeded up. Through profiling, I've identified a particular function as the culprit in slowing down the execution. This function is called hundreds of thousands of times within a loop. </p>\n\n<p>My first thought was to convert the function to mex (using Matlab Coder) to speed it up. However, common programming sense tells me the interface between Matlab and the mex code would lead to some overhead, which means calling this mex function thousands of times might not be a good idea. Is this correct? Or does Matlab do some magic when it's the same mex being called repeatedly to remove the overhead? </p>\n\n<p>If there <em>is</em> significant overhead, I'm thinking of restructuring the code so as to add the loop to the function itself and <em>then</em> creating a mex of that. Before doing that, I would like to validate my assumption to justify the time spent on this. </p>\n\n<p>Update: </p>\n\n<p>I tried @angainor's suggestion, and created donothing.m with the following code: </p>\n\n<pre><code>function nothing = donothing(dummy) %#codegen\nnothing = dummy;\nend\n</code></pre>\n\n<p>Then, I created a mex function from this as donothing_mex, and tried the following code: </p>\n\n<pre><code>tic;\nfor i=1:1000000\n    donothing_mex(5);\nend\ntoc;\n</code></pre>\n\n<p>The result was that a million calls to the function took about 9 seconds. This is not a significant overhead for our purposes, so for now I think I will convert the called function alone to mex. However, calling a function from a loop that executes about a million times does seem a pretty stupid idea in retrospect, considering this is performance critical code, so moving the loop to the mex function is still in the books, but with much lesser priority. </p>\n"},{"tags":["iphone","performance","optimization","opengl-es","rendering"],"answer_count":6,"favorite_count":8,"up_vote_count":8,"down_vote_count":0,"view_count":7961,"score":8,"question_id":2785640,"title":"optimizing iPhone OpenGL ES fill rate","body":"<p>I have an Open GL ES game on the iPhone. My framerate is pretty sucky, ~20fps. Using the Xcode OpenGL ES performance tool on an iPhone 3G, it shows:</p>\n\n<p>Renderer Utilization: 95% to 99%</p>\n\n<p>Tiler Utilization: ~27%</p>\n\n<p>I am drawing a lot of pretty large images with a  lot of blending. If I reduce the number of images drawn, framerates go from ~20 to ~40, though the performance tool results stay about the same (renderer still maxed). I think I'm being limited by the fill rate of the iPhone 3G, but I'm not sure.</p>\n\n<p>My questions are: How can I determine with more granularity where the bottleneck is? That is my biggest problem, I just don't know what is taking all the time. If it is fillrate, is there anything I do to improve it besides just drawing less?</p>\n\n<p>I am using texture atlases. I have tried to minimize image binds, though it isn't always possible (drawing order, not everything fits on one 1024x1024 texture, etc). Every frame I do 10 image binds. This seem pretty reasonable, but I could be mistaken.</p>\n\n<p>I'm using vertex arrays and glDrawArrays. I don't really have a lot of geometry. I can try to be more precise if needed. Each image is 2 triangles and I try to batch things were possible, though often (maybe half the time) images are drawn with individual glDrawArrays calls. Besides the images, I have ~60 triangles worth of geometry being rendered in ~6 glDrawArrays calls. I often glTranslate before calling glDrawArrays.</p>\n\n<p>Would it improve the framerate to switch to VBOs? I don't think it is a huge amount of geometry, but maybe it is faster for other reasons?</p>\n\n<p>Are there certain things to watch out for that could reduce performance? Eg, should I avoid glTranslate, glColor4g, etc?</p>\n\n<p>I'm using glScissor in a 3 places per frame. Each use consists of 2 glScissor calls, one to set it up, and one to reset it to what it was. I don't know if there is much of a performance impact here.</p>\n\n<p>If I used PVRTC would it be able to render faster? Currently all my images are GL_RGBA. I don't have memory issues.</p>\n\n<p>One of my fullscreen textures is 256x256. Would it be better to use 480x320 so the phone doesn't have to do any scaling? Are there any other general performance advice for texture sizes?</p>\n\n<p>Here is a rough idea of what I'm drawing, in this order:</p>\n\n<p>1) Switch to perspective matrix.\n2) Draw a full screen background image\n3) Draw a full screen image with translucency (this one has a scrolling texture).\n4) Draw a few sprites.\n5) Switch to ortho matrix.\n6) Draw a few sprites.\n7) Switch to perspective matrix.\n8) Draw sprites and some other textured geometry.\n9) Switch to ortho matrix.\n10) Draw a few sprites (eg, game HUD).</p>\n\n<p>Steps 1-6 draw a bunch of background stuff. 8 draws most of the game content. 10 draws the HUD.</p>\n\n<p>As you can see, there are many layers, some of them full screen and some of the sprites are pretty large (1/4 of the screen). The layers use translucency, so I have to draw them in back-to-front order. This is further complicated by needing to draw various layers in ortho and others in perspective.</p>\n\n<p>I will gladly provide additional information if reqested. Thanks in advance for any performance tips or general advice on my problem!</p>\n\n<p><strong>Edit:</strong></p>\n\n<p>I added some logging to see how many glDrawArrays calls I am doing, and with how much data. I do about 20 glDrawArray calls per frame. Usually about 1 to up to 6 of these has about 40 vertices each. The rest of the calls are usually just 2 vertices (one image). I'm just using glVertexPointer and glTexCoordPointer.</p>\n"},{"tags":["mysql","performance","hadoop"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":70,"score":1,"question_id":12935771,"title":"Which technology should I use to handle 1 million * 1 million calculation per 30 seconds","body":"<p>I am doing a Project. I have developed a GPS application where all the devices (Moving on the road) send their coordinates to the server in every 30 seconds. Now I have to calculate the distance between these devices so if any device comes in the range of another device then both the devices get a notification. </p>\n\n<p>I know how to calculate the distance between two coordinates (thanks to google) but I am not sure how to implement it because if we  have 1 million devices simultaneously sending data to the server then the server needs to execute distance calculation 1 million * (1 million -1) times in every 30 seconds. </p>\n\n<p>Please let me how to implement it. Do I need to use anything like hadoop or a mysql database procedure to do the job. </p>\n\n<p>Calculation is not a problem here but handling and calculating this much data is a problem.</p>\n\n<p>Regards,</p>\n"},{"tags":["c++","performance","qt","xml-parsing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":62,"score":0,"question_id":12948152,"title":"Fastest way of parsing XML files for specified Tags using the Qt Framework","body":"<p>As far es I know there a 4 ways of parsing XML files using C++ with Qt.</p>\n\n<pre><code>QDom\nQSax\nQXMLStreamReader\nQXMLQuery\n</code></pre>\n\n<p>I search in my file for a node with a specific attribute, if I've found it, I abort the parsing save the file name to a list and parse the next file. \nI accomplished that using QDom, but since i search up to 10k files with each about 400lines. it takes some time to parse them all.</p>\n\n<p>My question is whether anyone of you knows about the performance of this different approaches?\nOr if you have any tips to improve the performance of such a program?</p>\n\n<p>I appreciate any information!</p>\n"},{"tags":[".net","performance","timer","timing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":51,"score":0,"question_id":12931733,"title":"Is the timer resolution of the System.Diagnostics.Stopwatch class stable?","body":"<p>.Net has support for high resolution timing using the <code>System.Diagnostics.Stopwatch</code> class. I understand that the specific resolution that this class uses differs depending on the underlying hardware and can be obtained via the static property <code>Stopwatch.Frequency</code>.</p>\n\n<p>This frequency appears to be related to CPU frequency and <code>Stopwatch</code> reads this value and stores it in a static class variable within a static initializer/constructor. Hence I'm now wondering if this class will report incorrect timings if the CPU clock changes? e.g. in systems that alter the CPU clock depending system load.</p>\n"},{"tags":["android","performance","ice-cream-sandwich","ormlite"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":144,"score":0,"question_id":12909489,"title":"Ormlite performance on 4.0.3 (ICS)","body":"<p>I experienced a huge performance difference between android 2.3.4 and 4.0.3 on HTC Sensation. </p>\n\n<p>Some additional information:</p>\n\n<ul>\n<li>ormlite version 4.42</li>\n<li>to getting dao I use DaoManager and a dao singleton.</li>\n<li>using batch task to insert</li>\n<li>I'm trying createorupdate 30 objects (only creating takes the same time)</li>\n<li>These are single objects (without relations), but have long string fields.</li>\n</ul>\n\n<p>Time logs:</p>\n\n<p>ICS (4.0.3)</p>\n\n<pre><code>10-16 09:17:06.206: 1 getting dao\n10-16 09:17:06.206: 2 got dao\n10-16 09:17:06.206: 2 start call batch task\n10-16 09:17:06.216: 3 start initializing batch_task\n10-16 09:17:06.326: 121 finished initializing batchtask  \n10-16 09:17:06.836: 623 end batch task\n</code></pre>\n\n<p>2.3.4</p>\n\n<pre><code>10-16 09:20:00.355: 0 getting dao\n10-16 09:20:00.355: 1 got dao\n10-16 09:20:00.355: 1 start call batch task\n10-16 09:20:00.355: 1 start initializing batch_task\n10-16 09:20:00.435: 87 finished initializing batchtask  \n10-16 09:20:00.445: 96 end batch task \n</code></pre>\n\n<p>As you can see on ICS takes creating much more time.</p>\n\n<p>What should I do to get the similar performance on ICS?</p>\n"},{"tags":["php","performance","profiling","production-environment"],"answer_count":2,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":57,"score":2,"question_id":12364622,"title":"PHP: How to run sampling profiler in production?","body":"<p><strong>Production environment:</strong> A load balancer / HTTP reverse proxy in front of cluster of worker machines running Apache 2.2 with <code>mod_php</code> 5.3 on 64 bit Linux. All worker machines are running identical fully custom PHP code and speak to single backend PostgreSQL database. The PHP code is optimized to spend CPU over talking to the database. The database machine has been verified to still have lots of idle.</p>\n\n<p><strong>What I'm looking for:</strong> sampling profiler that can attach to PHP process by PID and periodically stop the process (e.g. with <code>SIGSTOP</code>), collect PHP stack via memory inspection and the continue the process (e.g. with <code>SIGCONT</code>). The stopping period should be adjustable but I think stopping interval should be around 1-10 ms.</p>\n\n<p>A single worker machine PHP process is expected to run a single request always in less than 100 ms. I'm mostly interested collecting profile data for those processes that take more than 100 ms. The best case scenario would be a sampling profiler that would be notified at the start of the request and if the PHP process handling the request is still running 100 ms later, start collecting samples at 1 ms intervals. This way any normally running process would be run to the end without interrupts and I would still get profiles for problematic cases.</p>\n\n<p><strong>Does this kind of sampling profiler for PHP exist?</strong> The intent is to not use instrumenting profiler because the overhead is too high and the instrumentation messes the statistics (been there, done that).</p>\n\n<p>I'm already aware of XHProf and Xdebug but as far as I know, both are instrumenting profilers and affect the actual opcodes of PHP program. I'd highly prefer something that runs the normal PHP opcodes instead.</p>\n\n<p>The closest I know would work is to run PHP code with HipHop and use sampling profiler for C/C++ code but I'm not yet ready to port the software to HipHop. And in that case, the profiling result would be representative only for HipHop, not for mod_php.</p>\n"}]}
{"total":25592,"page":10,"pagesize":100,"questions":[{"tags":["performance","xna","2d","sprite"],"answer_count":4,"favorite_count":3,"up_vote_count":1,"down_vote_count":0,"view_count":3289,"score":1,"question_id":1637086,"title":"XNA - Merge sprites for better drawing performance?","body":"<p>I read somewhere that when rendering a lot of 3D objects one could merge these to form a giant mesh so that only one draw call would be made. Therefore letting the, quote: \"GPU do its magic\", while the <em>CPU is free for other calls</em> than draw.</p>\n\n<p>So to my question, would this be feasible to do in a <strong>2D environment</strong> with <strong>performance</strong> in mind?</p>\n\n<p>For example, say we have a simple tile-system and instead of making a draw call for each tile in view, one would instead <em>merge all tiles</em> to form a large sprite and then call a draw on it.</p>\n\n<p>Any insight into the matter - either tips, links or whatnot - is greatly appreciated since I have no prior experience in graphics performance.</p>\n\n<p><strong>Edit:</strong> Sorry for my poor explanation. I am creating a tileengine for personal use and want it to be as versatile as possible. Therefore I want to optimize just in case I have to draw lots of tiles in the near future.</p>\n\n<p>I <em>do</em> use a tilesheet but what I meant with the question is if merging all tiles that are to be drawn from the sheet into a new Texture2D will gain performance. For example:</p>\n\n<p>We have 128x72 tiles to draw on the screen. Instead of looping through all tiles and calling draw for each tile to be drawn, we merge all tiles into a new sprite 1280x720 in size and draws it. This way the draw() method will only be called <em>once per frame</em>. My <em>question</em> was if this will improve performance as it would merging 3d objects into a single mesh when doing 3D.</p>\n\n<p>Because the info I have gathered is that calling draw() is a performance hog and should be called as little as possible. Anyone to confirm or deny? :)</p>\n"},{"tags":["xcode","multithreading","performance","instruments","xnu"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":31,"score":1,"question_id":12709995,"title":"XNU Thread States color-coded in Xcode/Instruments","body":"<p>I am doing some analysis of a multi-threaded application using the Apple Instruments tools, which give a lot of information I'm trying to make sense out of. I am trying to find a good resource to describe the thread states which are color-coded in the tool. I've been looking into XNU Kernel documentation and books but without much luck.</p>\n\n<p>There is a lot of yellow and purple which correspond to the \"preempted\" and \"supervisor\" modes (the full color chart is described in the upper right pop up in the attached image). Given I'm spending so much time in these states as opposed to the \"running\" state (in blue), I would be particularly interested in knowing what they refer to and whether it is possible/desirable to minimise the time spent in these states.</p>\n\n<p><img src=\"http://i.stack.imgur.com/I1auN.png\" alt=\"enter image description here\"></p>\n"},{"tags":["android","performance","background","onpause"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":67,"score":0,"question_id":12944603,"title":"Android - Resume application from background programmatically","body":"<p>i need to resume my app as soon as the application goes in background.</p>\n\n<p>I don't like my solution cause has a few bugs and is not too performant, this is what i try:</p>\n\n<pre><code>    @Override\nprotected void onPause() {\n       super.onPause();\n       Intent intent = new Intent(this, MainActivity.class);\n       intent.setFlags(Intent.FLAG_ACTIVITY_NEW_TASK | Intent.FLAG_ACTIVITY_CLEAR_TOP);\n       intent.setAction(Intent.ACTION_MAIN);\n       startActivity(intent);\n}\n</code></pre>\n\n<p>This is a single Activity application which load a webview with many dynamic and flash content  while update his state every few seconds and every few second get information from web.\nSo is not to simple and fast every time it goes in background to recreate the activity.\nTo relaunch application needs about 3-4 seconds, too much for me. If users between that seconds clicks the settings icon in the home, application doesn't start again. I don't know why and i'm writing here after a lot of googling :) Help me please!</p>\n"},{"tags":["php","mysql","performance","insert"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":12944132,"title":"PHP/MySQL: What is a reasonable execution time for inserts?","body":"<p><strong>Same result with PDO and standard MySQL.</strong></p>\n\n<p>I understand if you're going to do mass inserts, it's better to do it in bulk. I know that doing it individually is a terrible idea. I don't even have a need to do mass-inserts so I'm just wondering if the results I'm getting are to be expected.</p>\n\n<p><strong>Standard (deprecated) mysql code (I'm using PDO in my site):</strong></p>\n\n<p><strong>493.9</strong> seconds for <strong>10,000</strong> inserts.</p>\n\n<pre><code>$c = mysql_connect ( 'localhost', 'root', 'password', true ) or die ( mysql_error () );\nmysql_select_db ( 'testdb', $c );\nset_time_limit ( 600 );\n$iterations = 10000;\n$store = 0;\n$amount = 0;\n$startTime = microtime ( true );\n$endTime = 0.00;\n\nfor ( $i = 1;   $i &lt;= $iterations;  $i ++ ) :\n    $store = rand ( 1, 2 );\n    $amount = rand ( -5000, 5000 );\n\n    mysql_query ( \"INSERT INTO checkbook (employee_id,store_id,amount,balance) VALUES\n    (6,$store,$amount,0.00)\", $c ) or die ( mysql_error () );\n\nendfor;\n\n$endTime = microtime ( true ) - $startTime;\n\necho '&lt;p&gt;&lt;strong&gt;', $endTime, '&lt;/strong&gt;&lt;/p&gt;';\n</code></pre>\n"},{"tags":["c#","wpf","performance","richtextbox"],"answer_count":1,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":1366,"score":4,"question_id":6418643,"title":"Storing and displaying rich text efficiently","body":"<p>I need to store significant amounts of rich text in a SQL database, retrieve it and display it.</p>\n\n<p>One font throughout is OK but I need different font sizes/bold/colors.</p>\n\n<p>For now I am using a RichTextBox (WPF) to display it, and XamlWriter.Save/XamlReader.Parse to serialize it to strings to store in the DB. It works well but the RichTextBox is so HUGELY SLOW at displaying the text that it's basically unusable.</p>\n\n<p>Is there a quick way to do this with acceptable performance?</p>\n\n<p>I'm considering doing it with GlyphRun objects, drawing each character as a bitmap and computing all the alignment requirements to fit the destination image etc... But reinventing the wheel on simple colored/sizable text seems really strange in 2011.</p>\n\n<p><strong>EDIT</strong>:\nThanks for the answers, didn't see them until now, sorry.</p>\n\n<p>Text is entered by the user from <code>RichTextBox</code>es as well, basically I just save the resulting <code>string</code> <code>XamlWriter.Save(richTextBox.Document)</code> in the database. Other fields (double/int etc) are also entered by the user from <code>TextBox</code>es.</p>\n\n<p>As the user queries the database, pages of read-only rich text with colors and formatting is generated from scratch using the fields in the database, including the saved rich text fields above: these are converted from <code>FlowDocument</code>s to <code>Span</code>s and some replacement is done on them (<code>InlineUIContainer</code>s which host a class derived from <code>UIElement</code> which references a database entry, inlined in the text, like \"see [thisbook]\" where [thisbook] references some database entry's ID). MSDN says all that is far too much text for a <code>TextBlock</code>.</p>\n\n<p>That text rendering is the really slow part but there is no way around it, I need that formatting and it's just how the WPF <code>RichTextBox</code>es are: even when entering a little simple text in the <code>RichTextBox</code>es, there is a delay between typing and the character appearing on the screen...</p>\n\n<p>For now I still use <code>RichTextBox</code>es but I keep lots of rendered layouts in memory (the <code>Paragraph</code>/<code>Section</code>/<code>Span</code> objects) and I am careful to rerender only the least amount of formatted text possible when changes/queries are made or different views of the database data are requested by the user.</p>\n\n<p>It's still not fast but it's OK, changing the whole structure (AvalonEdit or <code>FormattedText</code> or <code>GlyphRun</code>) doesn't seem worth it right now, too much work, the whole serialization API with <code>XamlWriter.Save</code> and <code>XamlReader.Parse</code> simplifies much (for <code>FormattedText</code> and <code>GlyphRun</code>, I'd have to come up with a file format myself to save the formatted text to the database).</p>\n\n<p>There is also the possibility of using the OpenXML SDK to create Microsoft Word .docx documents but google says rendering performance isn't great either, and I don't know if embedding an <code>UIElement</code> in the text within an <code>InlineUIContainer</code> and serializing that to be saved in the database would be possible (same problem with AvalonEdit).</p>\n"},{"tags":[".net","sql","sql-server","performance","normalization"],"answer_count":5,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":50,"score":2,"question_id":12943776,"title":"more performant to have normalized or denormalized tables","body":"<p>I am currently developing an mvc application to read from an existing sql server database. The database is denormalized - and I was looking at modifying some tables to normalize it to a degree.</p>\n\n<p>This led to a discussion with a fellow developer as the most preformant way to read the data, or if the structure should change or not. The data will be read via ado.net with a stored procedure. Question I have is, is it more performant to have numerous fields in a table (denormalized) OR have several tables with inner joins (normalized) to retrieve the data?</p>\n\n<p>I should have mentioned, the actions on the tables will be 95% read, 5% write.</p>\n"},{"tags":["c","performance","fortran"],"answer_count":17,"favorite_count":29,"up_vote_count":90,"down_vote_count":3,"view_count":28514,"score":87,"question_id":146159,"title":"Is Fortran faster than C?","body":"<p>From time to time I read that Fortran is or can be faster then C for heavy calculations. Is that really true? I must admit that I hardly know Fortran, but the Fortran code I have seen so far did not show that the language has features that C doesn't have.</p>\n\n<p>If it is true, please tell me why. Please don't tell me what languages or libs are good for number crunching, I don't intend to write an app or lib to do that, I'm just curious.</p>\n"},{"tags":["sql-server","performance","blob","insert-into","poeaa"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":30,"score":1,"question_id":12938247,"title":"Should I Replace Multiple Float Columns with a BLOB?","body":"<p>How would a single BLOB column in SQL Server compare (performance wise), to ~20 REAL columns (20 x 32-bit floats)?</p>\n\n<p>I remember Martin Fowler recommending using BLOBs for persisting large object graphs (in Patterns of Enterprise Application Architecture) to remove multiple joins in queries, but does it make sense to do something like this for a table with 20 fixed columns (which are never used in queries)?</p>\n\n<p>This table is updated really often, around 100 times per second, and <code>INSERT</code> statements get rather large with all the columns specified in the query.</p>\n\n<p>I presume the first answer is going to be \"profile it yourself\", but I'd like to know if someone already has experience with this stuff.</p>\n"},{"tags":["asp.net","html","performance","image","optimization"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":25,"score":0,"question_id":12941712,"title":"Find Images in the Website Which are used Never","body":"<p>How can I detect image that is never used in the website, to improve loading speed of the whole content?.. Thanks in advance...</p>\n"},{"tags":["performance","r","matrix","sparse-matrix"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":39,"score":1,"question_id":12937043,"title":"Efficiency problems building a \"signature matrix\" using data from two (sparse) matrices in R","body":"<p>I don't know if the \"signature matrix\" I am trying to build has a proper pre-existing name or definition in any fields, but the following code appears to generate the correct result on some toy matrices. I have trouble explaining what exactly I am trying to do without causing confusion, but if the code I have provided isn't sufficient to deduce what I am trying to do, I'd be happy to give it a shot.</p>\n\n<p>When I run this code with my actual data (two integer matrices that are both approximately 300 by 20,000 elements in size) it appears to be working, but after hours and hours it still doesn't finish.</p>\n\n<p>I understand that the iteration might be the biggest problem here, but I haven't been able to work out how to remove it.</p>\n\n<p>The code:</p>\n\n<pre><code># Load required library\nlibrary(Matrix)\n\n# Load in the test data\nmut &lt;- matrix(data=c(1,1,1,0,0,0,0,1,0,1,1,0,0,1,1,0,0,0,1,0),\n              nrow=5,ncol=4,\n              dimnames=list(c(\"p1\",\"p2\",\"p3\",\"p4\",\"p5\"),c(\"GA\",\"GB\",\"GC\",\"GD\")))\n\noute &lt;- matrix(data=c(1,1,0,1,0,1,0,0,1,1,1,1,1,0,0,1,1,0,0,1),\n              nrow=5,ncol=4,\n              dimnames=list(c(\"p1\",\"p2\",\"p3\",\"p4\",\"p5\"),c(\"GQ\",\"GW\",\"GE\",\"GR\")))\n\npatOutMatrix &lt;- Matrix(data=oute,sparse=TRUE)\npatMutMatrix &lt;- Matrix(data=mut,sparse=TRUE)\n\ntransposePatMutMatrix &lt;- t(patMutMatrix)\n\n# Build the empty matrix (with row and col names)\nsigMatrix &lt;- Matrix(0,nrow=ncol(patMutMatrix), ncol=ncol(patOutMatrix),sparse=TRUE)\nrownames(sigMatrix) &lt;- colnames(patMutMatrix)\ncolnames(sigMatrix) &lt;- colnames(patOutMatrix)\n\n# Populate sigMatrix\nfor (mgene in rownames(transposePatMutMatrix))\n{\n  a &lt;- patOutMatrix[which(transposePatMutMatrix[mgene, ] == 1, arr.ind = T), ]\n\n  # Using an IF here to get around a problem with colSums() not working on single rows\n  sigMatrix[mgene,] &lt;- if (dim(as.matrix(a))[2] == 1) {\n    a\n  } else {\n    colSums(patOutMatrix[which(transposePatMutMatrix[mgene, ] == 1, arr.ind = T), ])\n  }\n}\n</code></pre>\n\n<p>Does anyone know how I could change anything here to make this perform faster?</p>\n"},{"tags":["java","performance","time","classloader"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":38,"score":1,"question_id":12368559,"title":"Measure how much time the ClassLoader takes to load class or jar","body":"<p>How to know how much time a specific .class or .jar takes to be loaded by the ClassLoader?</p>\n\n<p>The JVM option <code>-verbose:class</code> allows to know in which order the ClassLoader loads .class and jar. But it doesn't tell me if a specific .jar take a lot of time or not to be loaded.</p>\n"},{"tags":["iphone","performance","camera","iso","shutter"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":2233,"score":0,"question_id":10347515,"title":"It is possible to change the iso or shutter speed fot iphone camera?","body":"<p>I am tring to control the iso and the shutter speed for the iphone, but it is no API in AVFoundation. It can only change the exposure, wb for iphone. \nHow can i control the iso or shutter speed?</p>\n"},{"tags":["javascript","jquery","performance","jquery-ui"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":34,"score":0,"question_id":12939538,"title":"Is it a good idea to have two versions of jquery-ui library : One with core and other with all components?","body":"<p>We are trying to adapt to using widgets from jquery-ui for most of our functionality. It gives lot of OOP like features and ease of extension. The downside is the jquery-ui library comes with other baggage and in-built widgets that we don't need in some pages. No doubt our project uses  widgets from jquery-ui but in some cases we just need Core and Widget components to write our custom widgets. So my question : Is it worthwhile to have two versions of jquery-ui library one just with Core and other will Core + components we use across the project ? The idea is to just use a lighter jquery-ui-core in pages that have our custom widgets.</p>\n"},{"tags":["performance","apache","caching","http-headers","mod-expires"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":38,"score":0,"question_id":12939273,"title":"Changes to Expires Headers not respected by Pagespeed and YSlow","body":"<p>I added the following code to an htaccess file:</p>\n\n<pre><code>&lt;IfModule mod_expires.c&gt;\n\n# Enable expirations\nExpiresActive On\n\n# Default directive\nExpiresDefault \"access plus 1 month\"\n\n# Images\nExpiresByType image/gif \"access plus 1 month\"\nExpiresByType image/png \"access plus 1 month\"\nExpiresByType image/jpg \"access plus 1 month\"\n\n# CSS\nExpiresByType text/css \"access 1 month”\n\n# Javascript\nExpiresByType application/javascript \"access plus 1 year\"\n\n&lt;/IfModule&gt;\n</code></pre>\n\n<p>but these changes are not reflected by Google's PageSpeed and the YSlow Addon for Chrome. </p>\n\n<p>Based on the above code, can someone explain why I continue to receive an F grade for expires headers for PageSpeed and YSlow? More importantly, why does Google's PageSpeed indicate that the defined filetypes expire in 4 hours instead of the 1 month as defined in the htaccess file?</p>\n\n<p>Here are my response headers:</p>\n\n<pre><code>Date: Wed, 17 Oct 2012 15:29:36 GMT\nContent-Type: text/html\nServer: Nginx / Varnish\nX-Powered-By: PHP/5.2.17\nCache-Control: max-age=2592000\nExpires: Fri, 16 Nov 2012 15:29:36 GMT\nVary: Accept-Encoding\nContent-Encoding: gzip\nAge: 0\n\n200 OK\n</code></pre>\n"},{"tags":["performance","magento-admin","slowdown","slow-load"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":86,"score":0,"question_id":12763020,"title":"Magento admin backend is very slow even with fresh installation","body":"<p>The Magento admin backend is very slow with fresh installation of the same version 1.7.0.2.</p>\n\n<p>With my previous installation, it was working perfectly fine with reasonably good speed on the same server with same hosting company and no additional tweaks at all.</p>\n\n<p>But suddenly I messed up with it due to another custom theme that I installed. So I reinstalled it after removing it. Then I found more problems in even accessing it.</p>\n\n<p>Hence, I created new <code>public_html</code> folder and rename the previous one to <code>public_html.old</code>.</p>\n\n<p>Then I was able to reinstall the Magento successfully on the root folder. But this time it is opening very very slow, in fact, every step is slow.</p>\n\n<p>Can anybody help me to trace the actual reason. What could be the possibilities. It was working fine earlier but why not this time.</p>\n"},{"tags":["ruby-on-rails","performance","haml","benchmarking","slim-lang"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":259,"score":1,"question_id":12088818,"title":"How much faster is SLIM compared to HAML?","body":"<p>I'm trying to convince my team members to use SLIM instead of HAML because I like SLIM's syntax a lot more. I was promised that we would change to SLIM if it was really a lot faster than HAML as I read <a href=\"https://github.com/stonean/slim#benchmarks\" rel=\"nofollow\">here</a> and <a href=\"https://gist.github.com/626215\" rel=\"nofollow\">here</a>, but I don't know how recent these benchmarks are, so I wanted to know whether anybody has some real life experience with this topic?</p>\n\n<p>I am also not really sure which of the benchmarks (compiled, tilt compiled, cached, uncached) does mean the most, as I don't know what exactly will be used when a Rails application is in production mode (where the speed is the most needed).</p>\n\n<p>Thanks a lot for more information about this topic!</p>\n"},{"tags":["javascript","performance","firefox","dom-manipulation"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":66,"score":1,"question_id":12937905,"title":"Most efficient way to add remove classes with JavaScript","body":"<p>I am curious if anyone knows which of these is more efficient, I am only concerned with Firefox as a browser and do not need to know that this code doesn't work in IE, etc...</p>\n\n<p>Basically I am showing and hiding DOM elements based on an input field's value, an instant search if you will. I need to show or hide a \"nothing found\" element if no search results are shown. I am curious if it is cheaper (more efficient) to check if the \"nothing found\" element is in the proper state before modifying its class attribute, or to just modify the class attribute. </p>\n\n<p>Question: should I remove/add the hidden class every time the function is run, even if there is no change in the element's class attribute? </p>\n\n<pre><code>if (shown_count &gt; 0) {\n    element.classList.add('hidden');\n}\nelse {\n    element.classList.remove('hidden');\n}\n</code></pre>\n\n<p>or should I check to see if the element needs its class attribute updated before actually updating it?</p>\n\n<pre><code>if (shown_count &gt; 0) {\n    if (element.classList.contains('hidden') == false) {\n         element.classList.add('hidden');\n    }\n}\nelse {\n    if (element.classList.contains('hidden')) {\n         element.classList.remove('hidden');\n    }\n}\n</code></pre>\n"},{"tags":["java","performance","jvm","garbage-collection"],"answer_count":6,"favorite_count":3,"up_vote_count":0,"down_vote_count":0,"view_count":3784,"score":0,"question_id":2743106,"title":"Tuning JVM (GC) for high responsive server application","body":"<p>I am running an application server on Linux 64bit with 8 core CPUs and 6 GB memory.</p>\n\n<p>The server must be highly responsive.</p>\n\n<p>After some inspection I found that the application running on the server creates rather a huge amount of short-lived objects, and has only about 200~400 MB long-lived objects(as long as there is no memory leak)</p>\n\n<p>After reading <a href=\"http://java.sun.com/javase/technologies/hotspot/gc/gc_tuning_6.html\" rel=\"nofollow\">http://java.sun.com/javase/technologies/hotspot/gc/gc_tuning_6.html</a>\nI use these JVM options</p>\n\n<pre><code>-server -Xms2g -Xmx2g -XX:MaxPermSize=256m -XX:NewRatio=1 -XX:+UseConcMarkSweepGC\n</code></pre>\n\n<p>Result: the minor GC takes 0.01 ~ 0.02 sec, the major GC takes 1 ~ 3 sec\nthe minor GC happens constantly. </p>\n\n<p>How can I further improve or tune the JVM?</p>\n\n<p>larger heap size? but will it take more time for GC?</p>\n\n<p>larger NewSize and MaxNewSize (for young generation)?</p>\n\n<p>other collector? parallel GC? </p>\n\n<p>is it a good idea to let major GC take place more often? and how?</p>\n"},{"tags":["c++","iphone","objective-c","performance","optimization"],"answer_count":7,"favorite_count":30,"up_vote_count":51,"down_vote_count":0,"view_count":35229,"score":51,"question_id":926728,"title":"Will my iPhone app take a performance hit if I use Objective-C for low level code?","body":"<p>When programming a CPU intensive or GPU intensive application on the iPhone or other portable hardware, you have to make wise algorithmic decisions to make your code fast.</p>\n\n<p>But even great algorithm choices can be slow if the language you're using performs more poorly than another.</p>\n\n<p>Is there any hard data comparing Objective-C to C++, specifically on the iPhone but maybe just on the Mac desktop, for performance of various similar language aspects?  I am very familiar with <a href=\"http://stackoverflow.com/questions/826281/better-performance-with-libxml2-or-nsxmlparser-on-the-iphone\">this article comparing C and Objective-C</a>, but this is a larger question of comparing two object oriented languages to each other.</p>\n\n<p>For example, is a C++ vtable lookup really faster than an Obj-C message?  How much faster?  Threading, polymorphism, sorting, etc.  Before I go on a quest to build a project with duplicate object models and various test code, I want to know if anybody has already done this and what the results where.  This type of testing and comparison is a project in and of itself and can take a considerable amount of time.  Maybe this isn't one project, but two and only the outputs can be compared.</p>\n\n<p>I'm looking for <strong>hard data</strong>, not evangelism.  Like many of you I love and hate both languages for various reasons.  Furthermore, if there is someone out there actively pursuing this same thing I'd be interesting in pitching in some code to see the end results, and I'm sure others would help out too.  My guess is that they both have strengths and weaknesses, my goal is to find out precisely what they are so that they can be avoided/exploited in real-world scenarios.</p>\n"},{"tags":["performance","cdn","browser-cache"],"answer_count":5,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":91,"score":3,"question_id":12916430,"title":"Is better use Cache or CDN?","body":"<p>I was studying about the browser performance when loading static files and this doubt has come.</p>\n\n<blockquote>\n  <p>Some people say that use CDN static files (i.e. Google Code, jQuery\n  latest, AJAX CDN,...) is better for performance, because it requests\n  from another domain than the whole web page.</p>\n  \n  <p>Other manner to improve the performance is to set the <code>Expires</code> header\n  equal to some months later, forcing the browser to cache the static\n  files and cutting down the requests.</p>\n  \n  <p>I'm wondering which manner is the best, thinking about performance and\n  if I may combine both.</p>\n</blockquote>\n\n<p>Thank you in advance guys, I love StackOverflow.</p>\n"},{"tags":["javascript","jquery","performance","jstree"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":120,"score":1,"question_id":12692741,"title":"jstree performance issues","body":"<p>i am using a jstree with around 1500 nodes, nested a max of 4 levels deep (most are only 1 level deep), and i'm getting internet explorer's \"this script is running slowly\" error.  i began with just a straight html_data &lt;li&gt; structure, generated by ASP.NET.  The tree wouldn't finish loading at all.  then i tried xml_data and json_data, which was a little better but eventually errored out.  my last-stitch effort was async loading.  clearly, this fixed the initial load problem, but now i get IE's error when i expand one of the larger branches.</p>\n\n<p>some more details:  i'm using the checkbox plugin, and i will also need the ability to search.  unfortunately, when searching, the user could potentially enter as little as one character so i'm looking at some large search results.</p>\n\n<p>has anybody done something similar with such a large dataset?  any suggestions on speeding up jstree?  or, am i better off exploring other options for my gui?</p>\n\n<p>i realize i haven't posted any code, but any general techniques/gotcha's are welcome.</p>\n\n<p>thanks,</p>\n\n<p>mike</p>\n"},{"tags":["java","performance","eclipse-plugin","eclipse-rcp"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":35,"score":0,"question_id":12936587,"title":"Performance of TreeViewer.expandAll method","body":"<p>In my application, an action is to expand all elements of a certain tree.</p>\n\n<p>each time I execute this action, it takes longer time than before.</p>\n\n<pre><code>long startTime = System.currentTimeMillis();\ngetTreeViewer().expandAll();\nlong endTime = System.currentTimeMillis()-startTime;\nSystem.out.println(\"expandAll time :  \" + endTime );\n</code></pre>\n\n<p>and this is a sample from the output.</p>\n\n<p>expandAll time : 200\nexpandAll time : 800\nexpandAll time : 1800\nexpandAll time : 3200</p>\n\n<p>-- This slowdown my plugin so much. Is this an issue with the method ?</p>\n"},{"tags":["mysql","performance"],"answer_count":4,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":3309,"score":3,"question_id":5060366,"title":"MySQL: Fastest way to count number of rows","body":"<p>Which way to count a number of rows should be faster in MySQL?</p>\n\n<p>This:</p>\n\n<pre><code>SELECT COUNT(*) FROM ... WHERE ...\n</code></pre>\n\n<p>Or, the alternative:</p>\n\n<pre><code>SELECT 1 FROM ... WHERE ...\n\n// and then count the results with a built-in function, e.g. in PHP mysql_num_rows()\n</code></pre>\n\n<p>One would think that the first method should be faster, as this is clearly database territory and the database engine should be faster than anybody else when determining things like this internally.</p>\n"},{"tags":["java","json","performance","struts","tomcat7"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":120,"score":2,"question_id":12728860,"title":"Java Performance Issue - Tomcat WebappClassLoader locked","body":"<p>I have been facing this strange issue from past few days while doing the performance test of my product, i am using java 6,struts 3 framework and tomcat 7 server</p>\n\n<p>During performance test we start a load of thousands of UI requests hitting the server initialy it runs fine but after couple of hours the requests start getting blocked followed by a spike in CPU usage to 100% and even the UI becomes inaccessible.when i took the thread dumps to analyse the issue below is what i am getting consistently.</p>\n\n<pre><code>\"\"http-nio-8443\"-exec-1305\" daemon prio=10 tid=0x00007f0458538000 nid=0x56dd waiting for monitor entry [0x00007f04345c9000]\n   java.lang.Thread.State: BLOCKED (on object monitor)\n    at java.lang.Class.forName0(Native Method)\n    at java.lang.Class.forName(Class.java:247)\n    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1663)\n    - locked &lt;0x00000000a162f0c0&gt; (a org.apache.catalina.loader.WebappClassLoader)\n    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1521)\n    at java.beans.Introspector.instantiate(Introspector.java:1448)\n    at java.beans.Introspector.findExplicitBeanInfo(Introspector.java:431)\n    at java.beans.Introspector. (Introspector.java:380)\n    at java.beans.Introspector.getBeanInfo(Introspector.java:232)\n    at java.beans.Introspector.getBeanInfo(Introspector.java:218)\n    at com.googlecode.jsonplugin.JSONWriter.bean(JSONWriter.java:169)\n    at com.googlecode.jsonplugin.JSONWriter.process(JSONWriter.java:152)\n    at com.googlecode.jsonplugin.JSONWriter.value(JSONWriter.java:120)\n    at com.googlecode.jsonplugin.JSONWriter.write(JSONWriter.java:88)\n    at com.googlecode.jsonplugin.JSONUtil.serialize(JSONUtil.java:90)\n    at com.googlecode.jsonplugin.JSONResult.execute(JSONResult.java:119)\n    at com.opensymphony.xwork2.DefaultActionInvocation.executeResult(DefaultActionInvocation.java:348)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:253)\n    at com.facetime.imauditor.sreach.action.CustomRequestInterceptorJson.intercept(CustomRequestInterceptorJson.java:69)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.DefaultWorkflowInterceptor.doIntercept(DefaultWorkflowInterceptor.java:221)\n    at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.validator.ValidationInterceptor.doIntercept(ValidationInterceptor.java:150)\n    at org.apache.struts2.interceptor.validation.AnnotationValidationInterceptor.doIntercept(AnnotationValidationInterceptor.java:48)\n    at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.ConversionErrorInterceptor.intercept(ConversionErrorInterceptor.java:123)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.ParametersInterceptor.doIntercept(ParametersInterceptor.java:167)\n    at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.StaticParametersInterceptor.intercept(StaticParametersInterceptor.java:105)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at org.apache.struts2.interceptor.CheckboxInterceptor.intercept(CheckboxInterceptor.java:83)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at org.apache.struts2.interceptor.FileUploadInterceptor.intercept(FileUploadInterceptor.java:207)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.ModelDrivenInterceptor.intercept(ModelDrivenInterceptor.java:74)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.ScopedModelDrivenInterceptor.intercept(ScopedModelDrivenInterceptor.java:127)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at org.apache.struts2.interceptor.ProfilingActivationInterceptor.intercept(ProfilingActivationInterceptor.java:107)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at org.apache.struts2.interceptor.debugging.DebuggingInterceptor.intercept(DebuggingInterceptor.java:206)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.ChainingInterceptor.intercept(ChainingInterceptor.java:115)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.I18nInterceptor.intercept(I18nInterceptor.java:143)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.PrepareInterceptor.doIntercept(PrepareInterceptor.java:121)\n    at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:86)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at org.apache.struts2.interceptor.ServletConfigInterceptor.intercept(ServletConfigInterceptor.java:170)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.AliasInterceptor.intercept(AliasInterceptor.java:123)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at com.opensymphony.xwork2.interceptor.ExceptionMappingInterceptor.intercept(ExceptionMappingInterceptor.java:176)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:224)\n    at com.opensymphony.xwork2.DefaultActionInvocation$2.doProfiling(DefaultActionInvocation.java:223)\n    at com.opensymphony.xwork2.util.profiling.UtilTimerStack.profile(UtilTimerStack.java:455)\n    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:221)\n    at org.apache.struts2.impl.StrutsActionProxy.execute(StrutsActionProxy.java:50)\n    at org.apache.struts2.dispatcher.Dispatcher.serviceAction(Dispatcher.java:504)\n    at org.apache.struts2.dispatcher.FilterDispatcher.doFilter(FilterDispatcher.java:419)\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n    at com.facetime.imcoreserver.registration.AbsoluteSendRedirectFilter.doFilter(AbsoluteSendRedirectFilter.java:48)\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:240)\n    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:164)\n    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:462)\n    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:164)\n    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:100)\n    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)\n    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:403)\n    at org.apache.coyote.http11.Http11NioProcessor.process(Http11NioProcessor.java:369)\n    at org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process(Http11NioProtocol.java:317)\n    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1532)\n    - locked &lt;0x00000000abb53778&gt; (a org.apache.tomcat.util.net.SecureNioChannel)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n    at java.lang.Thread.run(Thread.java:662)\n</code></pre>\n\n<p>And below is the statistics of threads.</p>\n\n<pre><code>Threads locking monitor=1\nThreads sleeping on monitor=0\nThreads waiting to lock monitor=1097\n</code></pre>\n\n<p>Any help in this regards would really be helpful, thanks in advance.</p>\n\n<p>-Vinay</p>\n"},{"tags":["html","css","performance","wordpress","pagespeed"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":63,"score":0,"question_id":12935331,"title":"What is best practice for images in HTML and CSS","body":"<p>I creating a WordPress theme, and I have implement a function that allowing the images to be served embeded into the HTML document and the CSS files.</p>\n\n<p>What I mean is that instead of adding images in my web site as:</p>\n\n<pre><code>&lt;!-- In HMTL --&gt;\n&lt;img src=\"http://www.some-url.ext/img/my_image.jpg\" /&gt;\n\n/* In CSS */\nselector\n{\n    background-image: url(http://www.some-url.ext/img/my_image.jpg);\n}\n</code></pre>\n\n<p>to add the image in my site in the following form:</p>\n\n<pre><code>&lt;!-- In HMTL --&gt;\n&lt;img src=\"data:image/gif;base64,R0lG....\" /&gt;\n\n/* In CSS */\nselector\n{\n    background-image: url(data:image/gif;base64,R0lG....);\n}\n</code></pre>\n\n<p>The processed images are stored in cache files for better performance.</p>\n\n<p>My current theme also has a full width slider, that contains images that are large.</p>\n\n<p>The issue is that the processed document has the size of 1.83MB because of the embeded images.</p>\n\n<p>Also the document while is loading very fast, anything bellow the slideshow is getting slower to be displayed :(</p>\n\n<p>So, is it better to embed the images into the document or is it better to use the normal way with URLs ?</p>\n"},{"tags":["performance","google-chrome","network-protocols"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":139,"score":1,"question_id":12097425,"title":"Chrome Developer Tools: How to Read the Network Panel?","body":"<p>I am trying to make sense of the Chrome Developer Tools when I run performence tests on my websites. If you select Network on the tools meny it will look like this:</p>\n\n<p><img src=\"http://i.stack.imgur.com/ejvhD.png\" alt=\"enter image description here\"></p>\n\n<p>Then if I select the performance file I will have this information:</p>\n\n<p><img src=\"http://i.stack.imgur.com/ppZjE.png\" alt=\"enter image description here\"></p>\n\n<p>My question is this:</p>\n\n<ol>\n<li><em>What is the meaning of DNS Lookup, Connecting, Sending, Waiting and Receving? What is happening between the server, network and browser at each stage?</em> </li>\n<li><em>On the first image, the red line reads \"Load event fired\" and the blue one reads \"DOMContent event fired\". What is the meaning of this and why is it the DOMContent event is fired after all the content has been loaded?</em></li>\n</ol>\n"},{"tags":["c#","sql","performance","linq","linq-to-sql"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":75,"score":2,"question_id":12933794,"title":"Linq to SQL one to many relationships","body":"<p>Last couple of days i was struggling with a linq querys performance: </p>\n\n<pre><code>LinqConnectionDataContext context = new LinqConnectionDataContext();\nSystem.Data.Linq.DataLoadOptions options = new System.Data.Linq.DataLoadOptions();\noptions.LoadWith&lt;Question&gt;(x =&gt; x.Answers);\noptions.LoadWith&lt;Question&gt;(x =&gt; x.QuestionVotes);\noptions.LoadWith&lt;Answer&gt;(x =&gt; x.AnswerVotes);\ncontext.LoadOptions = options;\nvar query =( from c in context.Questions\n            where c.UidUser == userGuid\n            &amp;&amp; c.Answers.Any() == true\n            select new\n            {\n                c.Uid,\n                c.Content,\n                c.UidUser,\n                QuestionVote = from qv in c.QuestionVotes where qv.UidQuestion == c.Uid &amp;&amp; qv.UidUser == userGuid select new {qv.UidQuestion, qv.UidUser },\n                Answer = from d in c.Answers\n                         where d.UidQuestion == c.Uid\n                         select new\n                         {\n                             d.Uid,\n                             d.UidUser,\n                             d.Conetent,\n                             AnswerVote = from av in d.AnswerVotes where av.UidAnswer == d.Uid &amp;&amp; av.UidUser == userGuid select new { av.UidAnswer, av.UidUser }\n                         }\n            }).ToList();\n</code></pre>\n\n<p>Query have to run through 5000 rows, and it takes up to 1 minute. How can i improve performance of this query? </p>\n\n<p><strong>Update:</strong></p>\n\n<p><img src=\"http://i.stack.imgur.com/O61M5.png\" alt=\"enter image description here\"></p>\n"},{"tags":["python","regex","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":112,"score":3,"question_id":12726961,"title":"What is the fastest way to compare a text with a large number of regexp with python?","body":"<p>I have a list of regular expressions and I would like to match with tweets that as they arive so I can associate them with a specific account. With a small number of rules as above it goes really fast, but as soon as you increase the amount of rules, it becomes slower and slower.</p>\n\n<pre><code>import string, re2, datetime, time, array\n\nrules = [\n    [[1],[\"(?!.*ipiranga).*((?=.*posto)(?=.*petrobras).*|(?=.*petrobras)).*\"]],\n    [[2],[\"(?!.*brasil).*((?=.*posto)(?=.*petrobras).*|(?=.*petrobras)).*\"]],\n]\n\n#cache compile\ncompilled_rules = []\nfor rule in rules:\n    compilled_scopes.append([[rule[0][0]],[re2.compile(rule[1][0])]])\n\ndef get_rules(text):\n    new_tweet = string.lower(tweet)\n    for rule in compilled_rules:\n        ok = 1\n        if not re2.search(rule[1][0], new_tweet): ok=0\n        print ok\n\ndef test():\n    t0=datetime.datetime.now()\n    i=0\n    time.sleep(1)\n    while i&lt;1000000:\n        get_rules(\"Acabei de ir no posto petrobras. Moro pertinho do posto brasil\")\n        i+=1\n        t1=datetime.datetime.now()-t0\n        print \"test\"\n        print i\n        print t1\n        print i/t1.seconds\n</code></pre>\n\n<p>When I have tested with 550 rules, I couldn't do more then 50 reqs/s. Is there a better way for doing this? I need at least 200 reqs/s</p>\n\n<p>EDIT:\nafter tips from Jonathan I could improve about speed 5 times just but nesting a bit my rules. See the code below:</p>\n\n<pre><code>scope_rules = {\n    \"1\": {\n        \"termo 1\" : \"^(?!.*brasil)(?=.*petrobras).*\",\n        \"termo 2\" : \"^(?!.*petrobras)(?=.*ipiranga).*\",\n        \"termo 3\" : \"^(?!.*petrobras)(?=.*ipiranga).*\",\n        \"termo 4\" : \"^(?!.*petrobras)(?=.*ipiranga).*\",\n        },\n    \"2\": {\n        \"termo 1\" : \"^(?!.*ipiranga)(?=.*petrobras).*\",\n        \"termo 2\" : \"^(?!.*petrobras)(?=.*ipiranga).*\",\n        \"termo 3\" : \"^(?!.*brasil)(?=.*ipiranga).*\",\n        \"termo 4\" : \"^(?!.*petrobras)(?=.*ipiranga).*\",\n        }\n    }\ncompilled_rules = {}\nfor scope,rules in scope_rules.iteritems():\n    compilled_rules[scope]={}\n    for term,rule in rules.iteritems():\n        compilled_rules[scope][term] = re.compile(rule)\n\n\ndef get_rules(text):\n    new_tweet = string.lower(text)\n    for scope,rules in compilled_rules.iteritems():\n        ok = 1\n        for term,rule in rules.iteritems():\n            if ok==1:\n                if re.search(rule, new_tweet):\n                    ok=0\n                    print \"found in scope\" + scope + \" term:\"+ term\n\n\ndef test():\n    t0=datetime.datetime.now()\n    i=0\n    time.sleep(1)\n    while i&lt;1000000:\n        get_rules(\"Acabei de ir no posto petrobras. Moro pertinho do posto ipiranga da lagoa\")\n        i+=1\n        t1=datetime.datetime.now()-t0\n        print \"test\"\n        print i\n        print t1\n        print i/t1.seconds\n\ncProfile.run('test()', 'testproof')\n</code></pre>\n"},{"tags":["performance","oracle","postgresql"],"answer_count":4,"favorite_count":3,"up_vote_count":3,"down_vote_count":0,"view_count":1786,"score":3,"question_id":3316812,"title":"How big is the performance difference between Oracle and PostgreSQL?","body":"<p>I'm wondering about how to scale a database. Currently it uses PostgreSQL. Would switching to Oracle be worthwhile inspite of the coding pain and expense? Or is PostgreSQL + more boxes a better/cheaper approach?</p>\n"},{"tags":["android","performance","android-emulator","qemu"],"answer_count":39,"favorite_count":278,"up_vote_count":539,"down_vote_count":1,"view_count":189485,"score":538,"question_id":1554099,"title":"Slow Android emulator","body":"<p>I have a 2.67&nbsp;GHz Celeron processor, 1.21&nbsp;GB of RAM on a x86 Windows XP Professional machine. My understanding is that the Android emulator should start fairly quickly on such a machine, but for me it does not. I have followed all instructions in setting up the IDE, SDKs, JDKs and such and have had some success in staring the emulator quickly but is very particulary. How can I, if possible, fix this problem?</p>\n\n<p>Even if it starts and loads the home screen, it is very sluggish. I have tried the Eclipse IDE in Galileos, and Ganymede.</p>\n"},{"tags":["jquery","ajax","performance","click"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":46,"score":0,"question_id":12932237,"title":"Slowing process down by clicking multiple times in jquery","body":"<p>I'm working on my own CMS system. It works partly in Ajax and jQuery, but the problem is that I'm using a lot of click events. So, when I keep clicking on different items in my website, it slows the system down. Eventually it doesn't do anything anymore. Am I right about the click event and how do I need to use it in the good way? I used the .bind and .on event handlers. </p>\n\n<pre><code>$(document).ready(function(){\n\n\n//Standards\nvar windowWidth = $(window).width();\nvar windowHeight = $(window).height();\n$('#wrapper').css('width',windowWidth);\n$('#content').css('height',windowHeight);\n\n\n//Click related items\n$('.listItem').bind('click',function() {\n    var itemID = $(this).attr('rel');\n    $('#content').load('showitems.php',{newID:itemID});\n});\n\n\n//Click on tab\n$('.liBase a').on('click', function() {\n    $('.liBase a').parent().removeClass('activeList');\n    $('#imageShow').removeClass('activeList');\n    $(this).parent().addClass('activeList');\n});\n\n\n//Click pages\n\n$('.page').on('click', function() {\n    var pageID = $(this).attr('rel');\n    $('.liBase').parent().parent().removeClass('activeList');\n    $('#imageShow').removeClass('activeList');\n    $(this).parent().parent().addClass('activeList');\n    $('#content').load('showpages.php',{newID:pageID});\n});\n\n$('.item').on('click', function() {\n    var pageID = $(this).attr('rel');\n    $('.liBase').parent().parent().removeClass('activeList');\n    $('#imageShow').removeClass('activeList');\n    $(this).parent().parent().addClass('activeList');\n    $('#content').load('showitems.php',{newID:pageID});\n});\n\n$('.editItem').on('click', function() {\n    var newID = $(this).attr('rel');\n    $('.editPage').parent().parent().removeClass('activeList');\n    $('#imageShow').removeClass('activeList');\n    $(this).parent().parent().addClass('activeList');\n    $('#content').load('edititem.php',{itemID:newID});\n});\n\n$('.editPage').on('click',function() {\n    var newID = $(this).attr('rel');\n    $('.liBase').parent().parent().removeClass('activeList');\n    $('#imageShow').removeClass('activeList');\n    $(this).parent().parent().addClass('activeList');\n    $('#content').load('editpage.php',{pageID:newID});\n});\n\n$('.deleteItem').on('click', function() {\n    var newID = $(this).attr('rel');\n    $('.liBase').parent().parent().removeClass('activeList');\n    $('#imageShow').removeClass('activeList');\n    $(this).parent().parent().addClass('activeList');\n    $('#content').load('../control/deleteRecords.php',{postID:newID,tblName:'items',tblID:'itemID'});\n});\n\n$('.deletePage').on('click',function() {\n\n    var newID = $(this).attr('rel');\n    $('.liBase').parent().parent().removeClass('activeList');\n    $('#imageShow').removeClass('activeList');\n    $(this).parent().parent().addClass('activeList');\n    $('#content').load('../control/deleteRecords.php',{postID:newID,tblName:'pages',tblID:'pageID'});\n});\n\n$('#addPage').on('click', function() {\n    $('#content').load('addpage.php');\n});\n\n$('#addItem').on('click', function() {\n    $('#content').load('additem.php');\n});\n\n\n$('#imageShow a').on('click', function() {\n    var pageID = $(this).attr('rel');\n    $('.liBase').parent().parent().removeClass('activeList');\n    $(this).parent().addClass('activeList');\n    $('#content').load('showimages.php');\n\n});\n\n$('#imageAdd').on('click', function() {\n    $('#content').load('addimage.php');\n});\n\n\n\n});\n</code></pre>\n"},{"tags":["c++","performance","visual-studio","gcc","boost"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":115,"score":2,"question_id":12932348,"title":"Why std::make_shared<>() has much better performance than boost::make_shared()?","body":"<p>I have been doing some field performance test on </p>\n\n<pre><code>1&gt;std::shared_ptr, std::make_shared based on 'gcc 4.7.2' &amp; 'VC10 implementation' \n2&gt;boost::shared_ptr, boost::make_shared based on boost 1.47\n</code></pre>\n\n<p>The test result is somewhat interesting.</p>\n\n<p>1>In general <code>std</code> version performs better but especially <code>std::make_shared</code>. Why? Can I increase the <code>boost</code> version performance since C++ 11 is not available for some old project yet as they are using old version of Visual studio?</p>\n\n<p>Below is my code snippet used to test those. \nNB. you need to manually switch between boost &amp; std. \nNB. \"SimpleMSTimer.hpp\" is my timer wrapper for boost ptime, a bit too long to post here. But feel free to use your own timer.Any portable time would do.</p>\n\n<pre><code>#include \"stdafx.h\"\n#include &lt;vector&gt;\n#include &lt;iostream&gt;\n#include &lt;boost/shared_ptr.hpp&gt;\n#include &lt;boost\\make_shared.hpp&gt;\n\n#include \"SimpleMSTimer.hpp\"//my timer wrapper for boost ptime\n\nusing namespace std;\nusing namespace boost;\n\nclass Thing\n{\npublic:\n    Thing()\n    {\n    }\n\n    void method (void)\n    {\n        int i = 5;\n    }\n};\n\ntypedef boost::shared_ptr&lt;Thing&gt; ThingPtr;\n\nvoid processThing(Thing* thing)\n{\n    thing-&gt;method();\n}\n\n//loop1 and loop2 test shared_ptr in the vector container\nvoid loop1(long long num)\n{\n    cout &lt;&lt; \"native raw pointer: \";\n    vector&lt;Thing&gt; thingPtrs;\n    YiUtil::MSSegmentTimer segTimer(YiUtil::MSSegmentTimer::MLSEC, std::cout);\n    for(int i=0; i&lt; num; i++) {\n        Thing thing;\n        thingPtrs.push_back(thing);\n    }\n    thingPtrs.clear();\n}\n\nvoid loop2(long long num)\n{\n    cout &lt;&lt; \"native boost::shared_ptr: \";\n    vector&lt;ThingPtr&gt; thingPtrs;\n    YiUtil::MSSegmentTimer segTimer(YiUtil::MSSegmentTimer::MLSEC, std::cout);\n    for(int i=0; i&lt; num; i++) {\n        ThingPtr p1(new Thing);\n        thingPtrs.push_back(p1);\n    }\n}\n\nvoid loop3(long long num)\n{\n    cout &lt;&lt; \"optimized boost::shared_ptr: \";\n    vector&lt;ThingPtr&gt; thingPtrs;\n\n    YiUtil::MSSegmentTimer segTimer(YiUtil::MSSegmentTimer::MLSEC, std::cout);\n    for(int i=0; i&lt; num; i++) {\n        ThingPtr p1 = boost::make_shared&lt;Thing&gt;();\n        thingPtrs.push_back(p1);\n    }\n}\n\n\n//loop3 and loop4 test shared_ptr in loop\nvoid loop4(long long num)\n{\n    cout &lt;&lt; \"native raw pointer: \";\n    YiUtil::MSSegmentTimer segTimer(YiUtil::MSSegmentTimer::MLSEC, std::cout);\n    for(int i=0; i&lt; num; i++) {\n        Thing* p1 = new Thing();\n        processThing(p1);\n        delete p1;\n    }\n}\n\nvoid loop5(long long num)\n{\n    cout &lt;&lt; \"native boost::shared_ptr: \";\n    YiUtil::MSSegmentTimer segTimer(YiUtil::MSSegmentTimer::MLSEC, std::cout);\n    for(int i=0; i&lt; num; i++) {\n        ThingPtr p1(new Thing);\n        processThing(p1.get());\n    }\n}\n\nvoid loop6(long long num)\n{\n    cout &lt;&lt; \"optimized boost::shared_ptr: \";\n    YiUtil::MSSegmentTimer segTimer(YiUtil::MSSegmentTimer::MLSEC, std::cout);\n    for(int i=0; i&lt; num; i++) {\n        ThingPtr p1 = boost::make_shared&lt;Thing&gt;();\n        processThing(p1.get());\n    }\n}\n\nint main() {\n    long long num = 10000000;\n    cout &lt;&lt; \"test 1\" &lt;&lt; endl;\n    loop1(num);\n    loop2(num);\n    loop3(num);\n\n    cout &lt;&lt; \"test 2\"&lt;&lt; endl;\n    loop4(num);\n    loop5(num);\n    loop6(num);\n\n    return 0;\n}\n</code></pre>\n\n<p>VC10 compiler under release mode, gcc compiled with flag '-O3' for max optimization.\nTest result:</p>\n\n<pre><code>//VS2010 release mode\n//boost\ntest 1\nnative raw pointer: SegmentTimer: 15 milliseconds/n\nnative boost::shared_ptr: SegmentTimer: 3312 milliseconds/n\noptimized boost::shared_ptr: SegmentTimer: 3093 milliseconds/n\ntest 2\nnative raw pointer: SegmentTimer: 921 milliseconds/n\nnative boost::shared_ptr: SegmentTimer: 2359 milliseconds/n\noptimized boost::shared_ptr: SegmentTimer: 2203 milliseconds/n\n\n//std\ntest 1\nnative raw pointer: SegmentTimer: 15 milliseconds/n\nnative std::shared_ptr: SegmentTimer: 3390 milliseconds/n\noptimized std::shared_ptr: SegmentTimer: 2203 milliseconds/n\ntest 2\nnative raw pointer: SegmentTimer: 937 milliseconds/n\nnative std::shared_ptr: SegmentTimer: 2359 milliseconds/n\noptimized std::shared_ptr: SegmentTimer: 1343 milliseconds/n\n==============================================================================\ngcc 4.72 release mode\n//boost\ntest 1\nnative raw pointer: SegmentTimer: 15 milliseconds/n\nnative boost::shared_ptr: SegmentTimer: 4874 milliseconds/n\noptimized boost::shared_ptr: SegmentTimer: 3687 milliseconds/n\ntest 2\nnative raw pointer: SegmentTimer: 1109 milliseconds/n\nnative boost::shared_ptr: SegmentTimer: 2546 milliseconds/n\noptimized boost::shared_ptr: SegmentTimer: 1578 milliseconds/n\n\n//std\ntest 1\nnative raw pointer: SegmentTimer: 15 milliseconds/n\nnative std::shared_ptr: SegmentTimer: 3374 milliseconds/n\noptimized std::shared_ptr: SegmentTimer: 2296 milliseconds/n\ntest 2\nnative raw pointer: SegmentTimer: 1124 milliseconds/n\nnative std::shared_ptr: SegmentTimer: 2531 milliseconds/n\noptimized std::shared_ptr: SegmentTimer: 1468 milliseconds/n\n</code></pre>\n"},{"tags":["performance","excel","aggregate-functions","lookup"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":24,"score":0,"question_id":12930500,"title":"excel performance: Lookup Vs Getpivotdata","body":"<p>I build an Excel 2007 spreadsheet which contains a larger table with source data (about 500,000 rows and 10 columns). I need to extract data from this large table for my analysis. To extract and aggregate data I usually use sumif, vlookup/hlookup and index+match functions. </p>\n\n<p>I recently learned about the existence of the function getpivotdata, which makes it possible to extract data from a pivot table. To be able to use it, I first need to convert my large source table to a pivot table and after that I can extract data using the function getpivotdata.</p>\n\n<p>Would you expect a performance improvement if I would use getpivotdata to extract and aggregate data instead? I would expect that within the underlying Pivot object aggregated values are pre-calculated and therefore performance would be better. </p>\n\n<p>If performance would be better, are there any reasons not to follow this approach? To be clear, there is no need to refresh the pivot table because it contains source data (which is located in the beginning of the calculation chain).</p>\n"},{"tags":["mysql","performance","query-optimization"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":89,"score":4,"question_id":12930331,"title":"Why is MAX() 100 times slower than ORDER BY ... LIMIT 1?","body":"<p>I have a table <code>foo</code> with (among 20 others) columns <code>bar</code>, <code>baz</code> and <code>quux</code> with indexes on <code>baz</code> and <code>quux</code>. The table has ~500k rows.</p>\n\n<p>Why do the following to queries differ so much in speed? Query A takes 0.3s, while query B takes 28s.</p>\n\n<p><strong>Query A</strong></p>\n\n<pre><code>select baz from foo\n    where bar = :bar\n    and quux = (select quux from foo where bar = :bar order by quux desc limit 1)\n</code></pre>\n\n<p><strong>Explain</strong></p>\n\n<pre><code>id  select_type table   type    possible_keys   key     key_len ref     rows    Extra\n1   PRIMARY     foo     ref     quuxIdx         quuxIdx 9       const   2       \"Using where\"\n2   SUBQUERY    foo     index   NULL            quuxIdx 9       NULL    1       \"Using where\"\n</code></pre>\n\n<p><strong>Query B</strong></p>\n\n<pre><code>select baz from foo\n    where bar = :bar\n    and quux = (select MAX(quux) from foo where bar = :bar)\n</code></pre>\n\n<p><strong>Explain</strong></p>\n\n<pre><code>id  select_type table   type    possible_keys   key     key_len ref     rows    Extra\n1   PRIMARY     foo     ref     quuxIdx         quuxIdx 9       const   2       \"Using where\"\n2   SUBQUERY    foo     ALL     NULL            NULL    NULL    NULL    448060  \"Using where\"\n</code></pre>\n\n<p>I use MySQL 5.1.34.</p>\n"},{"tags":["c++","performance","memory","pointers"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":205,"score":0,"question_id":12904621,"title":"Is there any tricks for fast-memory access?","body":"<p>I am very new to the world of C++ programming, so sorry for my amatuerish question:</p>\n\n<p>I get a large block of data stored in the main memory (1-D array), and I need to access some of the data there frequently, my way of doing this is:</p>\n\n<pre><code>float *x=new float[20];//array to store x;\nint *indlistforx=new int[20];//array to store the index of x;\nfloat *databank=new float[100000000];//a huge array to store data\n\n/... fill data to databank.../\n\n\nfor (int i=0;i&lt;N;i++)//where N is a very large number;\n {\n  /... write index to indlistforx.../\n  getdatafromdatabank(x, indlistforx, databank);\n  //Based on the index provided by indlistforx, read data from databank then pass them to x\n\n  /...do something with x.../\n  };\n</code></pre>\n\n<p>Is there any efficient/fast way to access these data(the index for x are not aligned, and it is impossible to be aligned)?</p>\n\n<p>Many thanks in advance!</p>\n"},{"tags":["performance","apache","dns","varnish"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":2082,"score":3,"question_id":3728066,"title":"Varnish: cache only specific domain","body":"<p>I have been Googling aggressively, but without luck.</p>\n\n<p>I'm using Varnish with great results, but I would like to host multiple websites on a single server (Apache), without Varnish caching all of them.</p>\n\n<p>Can I specify what websites by URL to cache?</p>\n\n<p>Thanks</p>\n"},{"tags":["iphone","ios","performance","core-image"],"answer_count":3,"favorite_count":5,"up_vote_count":27,"down_vote_count":5,"view_count":9576,"score":22,"question_id":6625888,"title":"Are the Core Image filters in iOS 5.0 fast enough for realtime video processing?","body":"<p>Now that Apple has ported the Core Image framework over to iOS 5.0, I'm wondering: is Core Image is fast enough to apply live filters and effects to camera video?</p>\n\n<p>Also, what would be a good starting point to learn the Core Image framework for iOS 5.0?</p>\n"},{"tags":["xcode","performance","xcode4"],"answer_count":13,"favorite_count":64,"up_vote_count":74,"down_vote_count":0,"view_count":19827,"score":74,"question_id":6355667,"title":"Xcode 4 - slow performance","body":"<p>I have an issue with Xcode 4 really responding very slowly to user interactions, e.g. editing code, scrolling areas etc. This particularly happens with larger scale projects with many controllers/view files etc.</p>\n\n<p>I completely wiped the hard disk and re-installed Snow Leopard and Xcode the other week but steadily it ground to a frustrating response time again (over a number of days) disrupting workflow considerably.</p>\n\n<p>I have also on occasion removed the project's \"derived data\" via the Organiser -> Projects and this has had little effect.</p>\n\n<p>I'm wondering if there is anything I can do to improve performance other than get a higher specced machine in the first instance.</p>\n\n<p>FYI I'm running MacBook with Intel Core 2 Duo processors at 2GHz and 4GB of RAM.</p>\n\n<p>In case we need to upgrade I'd also like to know if people are experiencing this poor performance from Xcode 4 on well specced machines (which would make our hardware upgrade rather pointless as it's only Xcode that has any performance issue on the MacBook).</p>\n\n<p>If anybody has any suggestions or recommendations or could even let us know how improved hardware effects Xcode's performance on larger project trees then that would be extremely helpful and also a valuable resource for other devs in a similar position.</p>\n"},{"tags":["xcode","performance","cpu","xcode4.3"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":1347,"score":2,"question_id":10522936,"title":"Xcode 4.3.2 and 100% CPU constantly in the idle time","body":"<p>My Xcode started to behave very heavily from yesterday when working on medium size project (around 200 source files). Project compiles correctly and runs in both simulator and device. I do not use any 3rd party libraries, except few widely used includes (like JSON or facebook ios sdk).</p>\n\n<p>It constantly uses CPU(s) at full speed, even if it is in idle state (no indexing, no compiling, no editing). The usage of RAM is relatively normal (300-50MB).</p>\n\n<p>My machine uses: Core 2 Duo 3.04Ghz CPU, 8GB of RAM and Vertex OCZ 3 SSD drive.</p>\n\n<p>I have tried every suggested solution found at stackoverflow:</p>\n\n<ol>\n<li>Cleaned project</li>\n<li>Cleaned Derived Data in Organizer</li>\n<li>Cleaned repositories in Organizer</li>\n<li>Cleaned xcodeproject bundle from workspace and userdata files as suggested here: <a href=\"http://stackoverflow.com/a/8165886/229229\">http://stackoverflow.com/a/8165886/229229</a> (it is helping just for a moment and starts again after minute or so).</li>\n<li>Restarted Xcode many times (with the same effect as in 4).</li>\n<li>Disabled \"Live issues\"</li>\n<li>even Reinstalled Xcode</li>\n</ol>\n\n<p>Nothing helps. In most cases, Xcode indexes the project for a moment, then comes back to the normal performance, but after a while becomes unusable again. CPU jumps back to 95-100% for both cores, intelligence hangs, etc... </p>\n\n<p>I am attaching screenshots of how the Xcode processes are seen by the Instruments:</p>\n\n<p><img src=\"http://i.stack.imgur.com/nuEqj.png\" alt=\"enter image description here\">\n<img src=\"http://i.stack.imgur.com/ktR3m.png\" alt=\"enter image description here\">\n<img src=\"http://i.stack.imgur.com/bIhVX.png\" alt=\"enter image description here\">\n<img src=\"http://i.stack.imgur.com/NUdPx.jpg\" alt=\"enter image description here\">\n<img src=\"http://i.stack.imgur.com/AbsTj.png\" alt=\"enter image description here\"></p>\n\n<p><strong>UPDATE:</strong>\nAfter a moment of hope that I solved the problem by moving around few</p>\n\n<p><code>#import \"header.h\"</code> </p>\n\n<p>statements from headers to the implementation files and exchanging them with forward declarations ... the problem came back again after a while.\nI am adding the console log. \nThe strange thing is that the logs related to Xcode show up after I quit it, not during the run itsef.</p>\n\n<p>Console logs:</p>\n\n<pre><code>5/11/12 9:27:03.777 AM [0x0-0x45045].com.apple.dt.Xcode: com.apple.dt.instruments.backgroundinstruments: Already loaded\n5/11/12 9:27:05.571 AM Xcode: Performance: Please update this scripting addition to supply a value for ThreadSafe for each event handler: \"/Library/ScriptingAdditions/SIMBL.osax\"\n5/11/12 9:27:58.168 AM Xcode: ERROR: Failed to create an alert for ID \"enabled\" based on defaults: 1\n</code></pre>\n"},{"tags":["mysql","performance","partitioning"],"answer_count":1,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":39,"score":3,"question_id":12929624,"title":"Partitions and UPDATE","body":"<p>I'm diving deeper and deeper into MySQL Features, and the next one I'm trying out is table partitions</p>\n\n<p>There's basically only one question about them, where I couldnt find a clear answer yet:</p>\n\n<p>If you UPDATE a row, will the row be moved to another partition automatically, if the partition conditions of another partition is met? (if for example, the partitions are split up by region, and the region changes from region A to region B)</p>\n\n<p>And if that doesnt happen automatically, what do I need to do in order to move the row from partition A to partition B? (and will there be a performance hit by doing so?)</p>\n\n<p>What I would like to do, is to move 'deleted' (a flag) informations into a separate partition of the table, since those will rarely be called. Would that usually be a good idea or would it be better to just leave everything in the same (probably someday huge - multiple million rows) table?</p>\n"},{"tags":["mysql","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":33,"score":1,"question_id":12929964,"title":"MySQL column comments and performance","body":"<p>I have to add new column to database table but its name is relatively ambiguous and I thought a comment on the column would be perfectly utilised and would give good insight for the developers down the line.</p>\n\n<p>But the questions is, does the column comment have any impact on the SQL queries and their performance generally?</p>\n"},{"tags":["asp.net-mvc-3","performance","debugging","entity-framework-4.1"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":39,"score":0,"question_id":12914089,"title":"Count maximum and opened EF connections","body":"<p>I have ASP.NET MVC app which suffer from performance bottleneck. I would like to profile EF connections and find out maximum allowed EF connections and current opened connections. Any suggestions how to do that? </p>\n\n<p>My Repository code</p>\n\n<pre><code>    using EntityFramework.Patterns;\n\n\n     public class ServiceRepository : IServiceRepository\n        {\n            private readonly IRepository&lt;User&gt; _userRepository;\n            private readonly IUnitOfWork _unitOfWork;\n\n\n            public ServiceRepository(DbContext dbContext)\n            {\n                var dbContextAdapter = new DbContextAdapter(dbContext);\n                _unitOfWork  = new UnitOfWork(dbContextAdapter);            \n                _userRepository = new Repository&lt;User&gt;(dbContextAdapter);\n            }\n\n            public IQueryable&lt;User&gt; GetUsersByProduct(int productId)\n            {\n                return _userRepository.AsQueryable().Where(p =&gt; p.Orders.Any(o =&gt; o.ProductId == productId));\n            }\n//Skip code\n\n        public void Commit()\n        {\n            _unitOfWork.Commit();\n        }\n\n    }\n</code></pre>\n\n<p>Injection with Ninject</p>\n\n<pre><code>private static void RegisterServices(IKernel kernel)\n{\n       var connectionString = ConfigurationManager.ConnectionStrings[\"Entities\"].ConnectionString;\n       kernel.Bind(typeof(DbContext)).ToMethod(context =&gt; new DbContext(connectionString)).InRequestScope();           \n       kernel.Bind&lt;IServiceRepository&gt;().To&lt;ServiceRepository&gt;().InRequestScope();\n}\n</code></pre>\n"},{"tags":["javascript","performance","algorithm"],"answer_count":1,"favorite_count":2,"up_vote_count":7,"down_vote_count":1,"view_count":90,"score":6,"question_id":12926975,"title":"Closest Pair Algorithm in JavaScript","body":"<p>I'm trying to implement a divide and conquer algorithm to find the closest pair of points in a randomly-generated set of points using JavaScript.  This algorithm should be running in O(n log n) time, but it is taking considerably longer to run than a simple brute force algorithm, which should be O(n^2).</p>\n\n<p>I've created two jsfiddles that time the algorithms for an array of 16000 points:</p>\n\n<ul>\n<li><a href=\"http://jsfiddle.net/kaangs10/rfSz5/2/\">Divide and Conquer</a></li>\n<li><a href=\"http://jsfiddle.net/kaangs10/eFgSA/2/\">Brute Force</a></li>\n</ul>\n\n<p>My hypothesis is that the divide and conquer is so slow because JavaScript arrays are actually hash tables.  Is it possible to significantly speed up the algorithm in JavaScript?  If so, what would be the best way to go about doing this?</p>\n"},{"tags":["performance","algorithm","r"],"answer_count":2,"favorite_count":0,"up_vote_count":10,"down_vote_count":0,"view_count":104,"score":10,"question_id":12913446,"title":"Efficiently create dataframe from strings containing key-value pairs","body":"<p>I would like to ask you for efficiency suggestions for a specific coding problem in R. I have a string vector in the following style:</p>\n\n<pre><code>[1] \"HGVSc=ENST00000495576.1:n.820-1G&gt;A;INTRON=1/1;CANONICAL=YES\"\n[2] \"DISTANCE=2179\"                                              \n[3] \"HGVSc=ENST00000466430.1:n.911C&gt;T;EXON=4/4;CANONICAL=YES\"    \n[4] \"DISTANCE=27;CANONICAL=YES;common\"\n</code></pre>\n\n<p>In each element of the vector, the single entries are separated with a <code>;</code> and MOST of the single entries have the format <code>KEY=VALUE</code>. However, there are also some entries, which only have the format <code>KEY</code> (see \"common\" in [4]). In this example, there are 15 different keys and not every key appears in each element of the vector. The 15 different keys are:</p>\n\n<pre><code>names &lt;- c('ENSP','HGVS','DOMAINS','EXON','INTRON', 'HGVSp', 'HGVSc','CANONICAL','GMAF','DISTANCE', 'HGNC', 'CCDS', 'SIFT', 'PolyPhen', 'common')\n</code></pre>\n\n<p>From this vector I would like to create a dataframe that looks like this:</p>\n\n<pre><code>ENSP HGVS DOMAINS EXON INTRON HGVSp                        HGVSc CANONICAL\n1    -    -       -    -    1/1     - ENST00000495576.1:n.820-1G&gt;A       YES\n2    -    -       -    -      -     -                            -         -\n3    -    -       -  4/4      -     -   ENST00000466430.1:n.911C&gt;T       YES\n4    -    -       -    -      -     -                            -       YES\nGMAF DISTANCE HGNC CCDS SIFT PolyPhen common\n1    -        -    -    -    -        -      -\n2    -     2179    -    -    -        -      -\n3    -        -    -    -    -        -      -\n4    -       27    -    -    -        -    YES\n</code></pre>\n\n<p>I wrote this function to solve the problem:</p>\n\n<pre><code>unlist.info &lt;- function(names, column){\n  info.mat &lt;- matrix(rep('-', length(column)*length(names)), nrow=length(column), ncol=length(names), dimnames=list(c(), names))\n  info.mat &lt;- as.data.frame(info.mat, stringsAsFactors=F)\n\n  for (i in 1:length(column)){\n    info &lt;- unlist(strsplit(column[i], \"\\\\;\"))\n    for (e in info){\n      e &lt;- unlist(strsplit(e, \"\\\\=\"))\n      j &lt;- which(names == e[1])\n      if (length(e) &gt; 1){\n        # KEY=VALUE. The value might contain a = as well\n        value &lt;- paste(e[2:length(e)], collapse='=')\n        info.mat[i,j] &lt;- value\n      }else{\n        # only KEY\n        info.mat[i,j] &lt;- 'YES'\n      }\n    }\n  }\n  return(info.mat)\n}\n</code></pre>\n\n<p>And then I call:</p>\n\n<pre><code>mat &lt;- unlist.info(names, vector)\n</code></pre>\n\n<p>Even though this works, it is really slow. Also I am handling vectors with over 100.000 entries. Now I realize that looping is inelegant and inefficient in R and I am familiar with the concept of applying functions to data frames. However, since every entry of the vector contains a different subset of <code>KEY=VALUE</code> or <code>KEY</code> entries I could not come up with a more efficient function.</p>\n"},{"tags":["performance","postgresql","database-design","postgresql-performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":77,"score":1,"question_id":12915209,"title":"How to understand an EXPLAIN ANALYZE","body":"<p>I am not very familiar with looking at EXPLAIN ANALYZE results, I have a huge problem with  my queries being too slow. I have tried to read up on how to interpret results from an explain queries, but I still don't know what I should be looking for, and what might be wrong. I have a feeling that there is some big red light flashing somewhere, I just don't see it.</p>\n\n<p>So the query is pretty simple, it looks like this:</p>\n\n<pre><code>EXPLAIN ANALYZE SELECT \"cars\".* FROM \"cars\" WHERE \"cars\".\"sales_state\" = 'onsale' AND \"cars\".\"brand\" = 'BMW' AND \"cars\".\"model_name\" = '318i' AND \"cars\".\"has_auto_gear\" = TRUE  LIMIT 25 OFFSET 0\n</code></pre>\n\n<p>And the result like this:</p>\n\n<pre><code>Limit  (cost=0.00..161.07 rows=25 width=1245) (actual time=35.232..38.694 rows=25 loops=1)\n  -&gt;  Index Scan using index_cars_onsale_on_brand_and_model_name on cars  (cost=0.00..1179.06 rows=183 width=1245) (actual time=35.228..38.652 rows=25 loops=1)\n        Index Cond: (((brand)::text = 'BMW'::text) AND ((model_name)::text = '318i'::text))\n        Filter: has_auto_gear\"\nTotal runtime: 38.845 ms\n</code></pre>\n\n<p>A little background:\nI'm on Postgresql 9.1.6, running on Herokus dedicated databases. My db has aprox 7,5Gb RAM, the table cars contains 3,1M rows and an aprox 2,0M of the rows has sales_state = 'onsale'. The table has 170 columns. The index that it uses looks something like this:</p>\n\n<pre><code>CREATE INDEX index_cars_onsale_on_brand_and_model_name\n  ON cars\n  USING btree\n  (brand COLLATE pg_catalog.\"default\" , model_name COLLATE pg_catalog.\"default\" )\n  WHERE sales_state::text = 'onsale'::text;\n</code></pre>\n\n<p>Anyone seeing some big obvious issue?</p>\n\n<p>EDIT:</p>\n\n<pre><code>SELECT pg_relation_size('cars'), pg_total_relation_size('cars');\n</code></pre>\n\n<p>pg_relation_size: 2058444800\npg_total_relation_size: 4900126720</p>\n\n<pre><code>SELECT pg_relation_size('index_cars_onsale_on_brand_and_model_name');\n</code></pre>\n\n<p>pg_relation_size: 46301184</p>\n\n<pre><code>SELECT avg(pg_column_size(cars)) FROM cars limit 5000;\n</code></pre>\n\n<p>avg: 636.9732567210792995</p>\n\n<p>WITHOUT THE LIMIT:</p>\n\n<pre><code>EXPLAIN ANALYZE SELECT \"cars\".* FROM \"cars\" WHERE \"cars\".\"sales_state\" = 'onsale' AND \"cars\".\"brand\" = 'BMW' AND \"cars\".\"model_name\" = '318i' AND \"cars\".\"has_auto_gear\" = TRUE\n\nBitmap Heap Scan on cars  (cost=12.54..1156.95 rows=183 width=4) (actual time=17.067..55.198 rows=2096 loops=1)\n  Recheck Cond: (((brand)::text = 'BMW'::text) AND ((model_name)::text = '318i'::text) AND ((sales_state)::text = 'onsale'::text))\n  Filter: has_auto_gear\n  -&gt;  Bitmap Index Scan on index_cars_onsale_on_brand_and_model_name  (cost=0.00..12.54 rows=585 width=0) (actual time=15.211..15.211 rows=7411 loops=1)\"\n        Index Cond: (((brand)::text = 'BMW'::text) AND ((model_name)::text = '318i'::text))\nTotal runtime: 56.851 ms\n</code></pre>\n"},{"tags":["jquery","performance","caching","jquery-selectors","chaining"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":43,"score":1,"question_id":12926467,"title":"Cached vs chained selectors in jquery?","body":"<p>I am wondering if there is a performance difference between using a cached selector, and using chained selectors?</p>\n\n<p>If I understand it correctly the chaining works because each function returns the jquery object, which is exactly the same as what is contained in the cached selector. So there would be no difference performance wise in the two examples below is there?\n<hr />\n<strong>Cached Selector</strong></p>\n\n<pre><code>$(function(){\n\n    $.on('click', '.disabled', function(){\n        $toggle = $(this);\n        $toggle.attr('title', 'Object Enabled');\n        $toggle.toggleClass('disabled enabled');\n        $toggle.html('Enabled');\n    });\n});\n</code></pre>\n\n<p><hr />\n<strong>Chained Selector</strong></p>\n\n<pre><code>$(function(){\n\n    $.on('click', '.disabled', function(){\n        $(this)\n            .attr('title', 'Object Enabled')\n            .toggleClass('disabled enabled')\n            .html('Enabled');\n    });\n});\n</code></pre>\n"},{"tags":["python","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":140,"score":2,"question_id":12842717,"title":"Why is processing a sorted array not faster than an unsorted array in Python?","body":"<p>In this post <a href=\"http://stackoverflow.com/questions/11227809/why-is-processing-a-sorted-array-faster-than-an-unsorted-array\">Why is processing a sorted array faster than random array</a>, it says that branch predicton is the reason of the performance boost in sorted arrays.</p>\n\n<p>But I just tried the example using Python; and I think there is no difference between sorted and random arrays (I tried both bytearray and array; and use line_profile to profile the computation).</p>\n\n<p>Am I missing something?</p>\n\n<p>Here is my code:</p>\n\n<pre><code>from array import array\nimport random\narray_size = 1024\nloop_cnt = 1000\n# I also tried 'array', and it's almost the same\na = bytearray(array_size)\nfor i in xrange(array_size):\n    a.append(random.randint(0, 255))\n#sorted                                                                         \na = sorted(a)\n@profile\ndef computation():\n    sum = 0\n    for i in xrange(loop_cnt):\n        for j in xrange(size):\n            if a[j] &gt;= 128:\n                sum += a[j]\n\ncomputation()\nprint 'done'\n</code></pre>\n"},{"tags":["mysql","performance","primary-key","innodb","myisam"],"answer_count":9,"favorite_count":8,"up_vote_count":27,"down_vote_count":0,"view_count":13244,"score":27,"question_id":332300,"title":"Is there a REAL performance difference between INT and VARCHAR primary keys?","body":"<p>Is there a measurable performance difference between using INT vs. VARCHAR as a primary key in MySQL? I'd like to use VARCHAR as the primary key for reference lists (think US States, Country Codes) and a coworker won't budge on the INT AUTO_INCREMENT as a primary key for all tables. </p>\n\n<p>My argument, as detailed <a href=\"http://database-programmer.blogspot.com/2008/01/database-skills-sane-approach-to.html#rule1\" rel=\"nofollow\">here</a>, is that the performance difference between INT and VARCHAR is negligible, since every INT foreign key reference will require a JOIN to make sense of the reference, a VARCHAR key will directly present the information.</p>\n\n<p>So, does anyone have experience with this particular use-case and the performance concerns associated with it?</p>\n"},{"tags":[".net","regex","performance","multiple","backreference"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":39,"score":0,"question_id":12925890,"title":"Optimization of multiple expressions to one","body":"<p><strong>BACKGROUND</strong></p>\n\n<p>I have a scenario where I must repeatedly find certain words in text, over and over.\nI have currently used a series of regular Expressions in a format like this...</p>\n\n<pre><code>\"((^)|(\\W))(?&lt;c&gt;Word1)((\\W)|($))\"\n\n\"((^)|(\\W))(?&lt;c&gt;NextWord)((\\W)|($))\"\n\n\"((^)|(\\W))(?&lt;c&gt;AnotherWord)((\\W)|($))\"\n</code></pre>\n\n<p>...</p>\n\n<p>This list of Regex objects is them looped through with a chunk of data and the matches are pulled out (one loop for one regex.matches(data) call)</p>\n\n<p>I have done everything I can to optimize them, such as Compiling them before hand.</p>\n\n<p>However the list is growing longer and I decided to start making larger compiled regular expressions to optimize the process. such as...</p>\n\n<pre><code>\"((^)|(\\W))(?&lt;c&gt;((Word1)|(NextWord)|(AnotherWord)))((\\W)|($))\"\n</code></pre>\n\n<p>This provides a HUGE speed imporvement, however there is a side effect I cannot figure out how to correct.</p>\n\n<p>When the words are in the data side by side (such as space delimited. eg. \"Word1 NextWord AnotherWord\") the second word is missed in the capture because the regex for \"Word1\" also includes the trailing space. The match that could occur for \"NextWord\" no longer has the Leading space because it's part of the previous match.</p>\n\n<p><strong>QUESTION</strong></p>\n\n<p>Can anyone alter this Regular expression (.net format)</p>\n\n<pre><code>Pattern = \"((^)|(\\W))(?&lt;c&gt;((Word1)|(NextWord)|(AnotherWord)))((\\W)|($))\"\n</code></pre>\n\n<p>to work to capture all the words in this list below with a single call to \".matches(data)\"\nWhere </p>\n\n<pre><code>data = \"Word1 NextWord AnotherWord\" \n</code></pre>\n\n<p>? (without sacrificing the efficiency gain)</p>\n\n<p><strong>RESULTS</strong></p>\n\n<p>Just thought I would mention this. After applying the suggested answer/correction with the look ahead and look behind, which I now know how to use :) , the code I just modified has improved in speed by 347x (0.00347% of old testing speed). Which is definitly something to remember when you get into multiple expressions. Very happy.</p>\n"},{"tags":["javascript","html","performance"],"answer_count":4,"favorite_count":3,"up_vote_count":8,"down_vote_count":1,"view_count":216,"score":7,"question_id":2661770,"title":"Downloading javascript Without Blocking","body":"<p>The context: My question relates to improving web-page loading performance, and in particular the effect that javascript has on page-loading (resources/elements below the script are blocked from downloading/rendering).</p>\n\n<p>This problem is usually avoided/mitigated by placing the scripts at the bottom (eg, just before the  tag).  </p>\n\n<p>The code i am looking at is for web analytics. Placing it at the bottom reduces its accuracy; and because this script has no effect on the page's content, ie, it's not re-writing any part of the page--i want to move it inside the head. Just how to do that without ruining page-loading performance is the crux.</p>\n\n<p>From my research, i've found six techniques (w/ support among all or most of the major browsers) for downloading scripts so that they don't block down-page content from loading/rendering:</p>\n\n<p><strong>(i)</strong> XHR + <em>eval()</em>;</p>\n\n<p><strong>(ii)</strong> XHR + <em>inject</em>;</p>\n\n<p><strong>(iii)</strong> download the HTML-wrapped script as in iFrame;</p>\n\n<p><strong>(iv)</strong> setting the script tag's <em>async</em> flag to TRUE (HTML 5 only); </p>\n\n<p><strong>(v)</strong> setting the script tag's <em>defer</em> attribute; and </p>\n\n<p><strong>(vi)</strong> 'Script DOM Element'.</p>\n\n<p>It's the last of these i don't understand. The javascript to implement the pattern (vi) is: </p>\n\n<pre><code>(function() {\n  var q1 = document.createElement('script');\n  q1.src = 'http://www.my_site.com/q1.js'\n  document.documentElement.firstChild.appendChild(q1)\n})();\n</code></pre>\n\n<p>Seems simple enough: and anonymous function is created then executed in the same block. Inside this anonymous function:</p>\n\n<ul>\n<li><p>a script element is created</p></li>\n<li><p>its <em>src</em> element is set to it's location, then</p></li>\n<li><p>the script element is added to the DOM</p></li>\n</ul>\n\n<p>But while each line is clear, it's still not clear to me <em>how exactly this pattern allows script loading without blocking down-page elements/resources from rendering/loading</em>?</p>\n"},{"tags":["sql","performance","postgresql","index","postgresql-performance"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":72,"score":2,"question_id":12923558,"title":"Index for a WHERE clause with datetime, and more","body":"<p>I'm using Postgres 9.1 and have a horribly slow performing query.</p>\n\n<h3>The Query:</h3>\n\n<pre><code>Explain Analyze SELECT COUNT(DISTINCT email) FROM \"invites\" WHERE (\n created_at &lt; '2012-10-10 21:08:05.259200'\n AND invite_method = 'email' \n AND accept_count = 0 \n AND reminded_count &lt; 3 \n AND (last_reminded_at IS NULL OR last_reminded_at &lt; '2012-10-10 21:08:05.261483'))\n</code></pre>\n\n<h3>Results:</h3>\n\n<pre><code>Aggregate  (cost=19828.24..19828.25 rows=1 width=21) (actual time=11395.903..11395.903 rows=1 loops=1)\n  -&gt;  Seq Scan on invites  (cost=0.00..18970.57 rows=343068 width=21) (actual time=0.036..353.121 rows=337143 loops=1)\n        Filter: ((created_at &lt; '2012-10-10 21:08:05.2592'::timestamp without time zone) AND (reminded_count &lt; 3) AND ((last_reminded_at IS NULL) OR (last_reminded_at &lt; '2012-10-10 21:08:05.261483'::timestamp without time zone)) AND ((invite_method)::text = 'email'::text) AND (accept_count = 0))\nTotal runtime: 11395.970 ms\n</code></pre>\n\n<p>As you can see this is taking about 11 seconds. How would I go about adding an index to optimize this queries performance?</p>\n"},{"tags":["c#","performance","entity-framework","sql-server-2008"],"answer_count":4,"favorite_count":1,"up_vote_count":8,"down_vote_count":0,"view_count":1283,"score":8,"question_id":4355474,"title":"How do I speed up DbSet.Add()?","body":"<p>I have to import about 30k rows from a CSV file to my SQL database, this sadly takes 20 minutes.</p>\n\n<p>Troubleshooting with a profiler shows me that <strong>DbSet.Add is taking the most time, but why?</strong></p>\n\n<p>I have these Entity Framework Code-First classes:</p>\n\n<pre><code>public class Article\n{\n    // About 20 properties, each property doesn't store excessive amounts of data\n}\n\npublic class Database : DbContext\n{\n    public DbSet&lt;Article&gt; Articles { get; set; }\n}\n</code></pre>\n\n<p>For each item in my for loop I do:</p>\n\n<pre><code>db.Articles.Add(article);\n</code></pre>\n\n<p>Outside the for loop I do:</p>\n\n<pre><code>db.SaveChanges();\n</code></pre>\n\n<p>It's connected with my local SQLExpress server,   but I guess there isn't anything written till SaveChanges is being called so I guess the server won't be the problem....</p>\n"},{"tags":["performance","hibernate","commit","flush"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":3204,"score":1,"question_id":5523101,"title":"Hibernate Performance flush v commit","body":"<p>Im creating a hibernate component to interact with large incoming data to persist, both save(create) and update data with volumes in the million of rows.</p>\n\n<p>I am aware of the main differences around flush v commit, for example flush syncing the \"dirty\" data into the persistable underlying data, and that flush allows you to sync with the underlying persistable data without actually committing so that the transaction can be rolled back if required. Commit essentially commits all persistable data to the database.</p>\n\n<p>Im creating a hibernate component to interact with large incoming data to persist, both save(create) and update data with volumes in the million of rows.</p>\n\n<p>I am aware of the main differences around flush v commit, for example flush syncing the \"dirty\" data into the persistable underlying data, and that flush allows you to sync with the underlying persistable data without actually committing so that the transaction can be rolled back if required. Commit essentially commits all persistable data to the database.</p>\n\n<p>Whats a reasonable size to do a batch insert? IS 50 the max amount for reasonable performance so something like:</p>\n\n<p>for (i &lt; 1000000)</p>\n\n<p>if(i % 50 ) {\nsession.flush()\n}</p>\n\n<p>I gather 50 should match the value in the hibernate.jdbc.batch_size 50</p>\n"},{"tags":["performance","internet-explorer","internet-explorer-8","profiling"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":580,"score":0,"question_id":2098870,"title":"The kinds of rendering in dynatrace~~","body":"<p>I use Dynatrace to find problems in my company site. I want to say,wow, this is a beautiful tool for page performance.</p>\n\n<p>But I found there are many kinds of rendering in dynatrace. For example:</p>\n\n<ol>\n<li>Calculating generic layout</li>\n<li>Calculating flow layout</li>\n<li>Scheduling layout task</li>\n</ol>\n\n<p>What's the difference between these?</p>\n"},{"tags":["performance","daemon","vala"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":66,"score":0,"question_id":12561695,"title":"Efficient daemon in Vala","body":"<p>i'd like to make a daemon in Vala which only executes a task every X seconds.\nI was wondering which would be the best way:</p>\n\n<ol>\n<li>Thread.usleep() or Posix.sleep()</li>\n<li>GLib.MainLoop + GLib.Timeout</li>\n<li>other?</li>\n</ol>\n\n<p>I don't want it to eat too many resources when it's doing nothing..</p>\n"},{"tags":["python","performance","memory-profiling"],"answer_count":4,"favorite_count":7,"up_vote_count":42,"down_vote_count":0,"view_count":9757,"score":42,"question_id":33978,"title":"Find out how much memory is being used by an object in Python","body":"<p>How would you go about finding out how much memory is being used by an object in python?</p>\n\n<p>I know it is possible to find out how much is used by a block of code, but not by an instantiated object anytime in its life, which is what I want.</p>\n\n<p>EDIT:\ndecided to go with the stats that task manager gives me. Based on the strong evidence that what I want to do is impossible without modifying python, both from fserb and from the heapy documentation.</p>\n"},{"tags":["memory","linux-kernel","driver","performance","kernel-programming"],"answer_count":4,"favorite_count":3,"up_vote_count":1,"down_vote_count":0,"view_count":2076,"score":1,"question_id":4452400,"title":"Memory access after ioremap very slow","body":"<p>I'm working on a Linux kernel driver that makes a chunk of physical memory available to user space. I have a working version of the driver, but it's currently very slow. So, I've gone back a few steps and tried making a small, simple driver to recreate the problem.</p>\n\n<p>I reserve the memory at boot time using the kernel parameter <code>memmap=2G$1G</code>. Then, in the driver's <code>__init</code> function, I <code>ioremap</code> some of this memory, and initialize it to a known value. I put in some code to measure the timing as well:</p>\n\n<pre><code>#define RESERVED_REGION_SIZE    (1 * 1024 * 1024 * 1024)   // 1GB\n#define RESERVED_REGION_OFFSET  (1 * 1024 * 1024 * 1024)   // 1GB\n\nstatic int __init memdrv_init(void)\n{\n    struct timeval t1, t2;\n    printk(KERN_INFO \"[memdriver] init\\n\");\n\n    // Remap reserved physical memory (that we grabbed at boot time)\n    do_gettimeofday( &amp;t1 );\n    reservedBlock = ioremap( RESERVED_REGION_OFFSET, RESERVED_REGION_SIZE );\n    do_gettimeofday( &amp;t2 );\n    printk( KERN_ERR \"[memdriver] ioremap() took %d usec\\n\", usec_diff( &amp;t2, &amp;t1 ) );\n\n    // Set the memory to a known value\n    do_gettimeofday( &amp;t1 );\n    memset( reservedBlock, 0xAB, RESERVED_REGION_SIZE );\n    do_gettimeofday( &amp;t2 );\n    printk( KERN_ERR \"[memdriver] memset() took %d usec\\n\", usec_diff( &amp;t2, &amp;t1 ) );\n\n    // Register the character device\n    ...\n\n    return 0;\n}\n</code></pre>\n\n<p>I load the driver, and check dmesg. It reports:</p>\n\n<pre><code>[memdriver] init\n[memdriver] ioremap() took 76268 usec\n[memdriver] memset() took 12622779 usec\n</code></pre>\n\n<p>That's 12.6 seconds for the memset. That means the memset is running at <em><strong>81 MB/sec</strong></em>. Why on earth is it so slow?</p>\n\n<p>This is kernel 2.6.34 on Fedora 13, and it's an x86_64 system.</p>\n\n<p>EDIT:</p>\n\n<p>The goal behind this scheme is to take a chunk of physical memory and make it available to both a PCI device (via the memory's bus/physical address) and a user space application (via a call to <code>mmap</code>, supported by the driver). The PCI device will then continually fill this memory with data, and the user-space app will read it out. If <code>ioremap</code> is a bad way to do this (as Ben suggested below), I'm open to other suggestions that'll allow me to get any large chunk of memory that can be directly accessed by both hardware and software. I can probably make do with a smaller buffer also.</p>\n\n<hr>\n\n<p>See my eventual solution below.</p>\n"},{"tags":["java","performance","jvm"],"answer_count":4,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":114,"score":6,"question_id":12864943,"title":"JAVA: reference defined within a loop","body":"<p>I'm just curious.<br>\nsuppose I define a reference inside a while/for loop.  </p>\n\n<p>does the JVM define this reference every iteration, or it's optimized to define it only once? </p>\n"},{"tags":["c++","c","performance","floating-point"],"answer_count":3,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":164,"score":3,"question_id":12920700,"title":"floating point conversions and performance","body":"<p>I am aware of the errors that can occur when doing conversions between floating point numbers and integers, but what about performance (please disregard the accuracy issues)?</p>\n\n<p>Does performance, in general, suffer if I do n-ary operations on operands of differing arithmetic types, that is, on differing floating point types (e.g. <code>float</code> and <code>double</code>) and floating point/integer type combinations (e.g. <code>float</code> and <code>int</code>)? Do there exist rules of thumb, such as, to keep all operands the same type?</p>\n\n<p>P.S.: I am asking because I'm writing an expression template library and would like to know whether to allow binary operations on vectors containing values of differing arithmetic types.</p>\n"},{"tags":["python","performance","memory","memory-management","writing"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":69,"score":0,"question_id":12923935,"title":"Python: improve memory efficiency of a script","body":"<p>I am using <a href=\"http://www.liblas.org/tutorial/python.html\" rel=\"nofollow\">Liblas</a> and <a href=\"http://code.google.com/p/pyshp/\" rel=\"nofollow\">shapefile</a> module to read *.las file and save a point shapefile.</p>\n\n<p>I have the following memory problem: when i arrive around 91% the memory is full and the script became really slow. I had try to figurate with a buffer or save in streaming way, but in the shapefile module i didn't find a solution to resolve this problem.</p>\n\n<p>thanks in advance for help and suggestions\nGianni </p>\n\n<pre><code>inFile =\"mypoints.las\"\noutFile =\"myshape.shp\"\n\ndef LAS2SHP(inFile,outFile):\n    w = shapefile.Writer(shapefile.POINT)\n    w.field('Z','C','10')\n    pbar = ProgressBar(len(lasfile.File(inFile,None,'r')))\n    i = 0\n    for p in lasfile.File(inFile,None,'r'):\n        i +=1\n        pbar.update(i)\n        w.point(p.x,p.y)\n        w.record(float(p.z))\n    w.save(outFile)\n</code></pre>\n"},{"tags":["java","performance","exception","checked","unchecked"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":90,"score":4,"question_id":12923302,"title":"Good practices for Java exceptions handling","body":"<p>I have some questions regarding handling exceptions in Java. I read a bit about it and got some contradicting guidelines.</p>\n\n<p><a href=\"http://onjava.com/pub/a/onjava/2003/11/19/exceptions.html?page=1\" rel=\"nofollow\">Best Practices for Exception Handling</a></p>\n\n<p>Let's go through the mentioned article:</p>\n\n<p>It states that one should generally avoid using checked exceptions <em>if \"Client code cannot do anything\"</em>. But what does it exactly mean? Is displaying error message in GUI sufficient reason for bubbling up checked exception? But it would force GUI programmer to remember to catch RuntimeExceptions and their descendants to display potential error info.</p>\n\n<p>Second view presented in this article is that one should evade inventing own exception classes unless I want to implement some customs field/methods in them. \nI generally disagree with this, my practice up today was just the opposite: I wrapped exceptions in my own exception structure to reflex goals realized by classes I write, even if they just extend Exception without adding any new methods. I think it helps to handle them more flexibly in the higher layers when thrown plus it's generally more clear and comprehensible for programmer who will use these classes.</p>\n\n<p>I implemented some code today 'new way' presented in the article throwing RuntimeException here and there, then I let <a href=\"http://www.sonarsource.org/\" rel=\"nofollow\">Sonar</a> analyze it. To confuse me even more Sonar marked my RuntimeExceptions as Major errors with a message like <em>\"Avoid throwing root type exceptions, wrap'em in your own types\".</em></p>\n\n<p>So it looks quite controversional, what do you think?</p>\n\n<p>I also heard from one of tech-leads today that just wrapping exceptions is bad, 'because it's a really costly operation for JVM'. For me, on the other side throwing SQLExceptions or IOExceptions everywhere looks like a bit of breaking encapsulation..</p>\n\n<p><strong>So what is your general attitude to questions I presented here?</strong> </p>\n\n<ol>\n<li><p><strong>When to wrap exceptions in my own types, when I shouldn't do this?</strong></p></li>\n<li><p><strong>Where is that point of <em>'client cannot do anything about this, throw\nruntime exception?</strong>'</em></p></li>\n<li><p><strong>What about performance issues?</strong></p></li>\n</ol>\n"},{"tags":["encryption","compression","performance","aes","zlib"],"answer_count":5,"favorite_count":3,"up_vote_count":13,"down_vote_count":0,"view_count":3333,"score":13,"question_id":4676095,"title":"When compressing and encrypting, should I compress first, or encrypt first?","body":"<p>If I were to AES-encrypt a file, and then ZLIB-compress it, would the compression be less efficient than if I first compressed and then encrypted?</p>\n\n<p>In other words, should I compress first or encrypt first, or does it matter?</p>\n"},{"tags":["ruby-on-rails-3","performance","passenger"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":24,"score":0,"question_id":12923373,"title":"Rails startup performance: how to trace and debug","body":"<p>Our Rails 3.2.8 app is taking around 30 seconds to load after restarting, with similar load time for <code>rails console</code> -- this is on an Amazon EC2 <code>m1.small</code> instance with Ubuntu 11, 64-bits.  The server seems perfectly suitable under load (we have several).  The startup time, which effectively occurs only after a deploy is a problem.</p>\n\n<p>Is there a way to get a timed trace of what is loading as the Rails instance loads?</p>\n"},{"tags":["c#","performance","reflection","expression-trees"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":73,"score":1,"question_id":12914990,"title":"GetCustomAttributes performance issue (expression trees is the solution??)","body":"<p>I have a performance problem because I use reflection and GetCustomAttributes for my data access. The performance profiler detected it. I have an extension method like this:</p>\n\n<pre><code>public static class DataRowExtensions\n{\n    /// &lt;summary&gt;\n    /// Maps DataRow objecto to entity T depending on the defined attributes. \n    /// &lt;/summary&gt;\n    /// &lt;typeparam name=\"T\"&gt;Entity to map.&lt;/typeparam&gt;\n    /// &lt;param name=\"rowInstance\"&gt;DataRow instance.&lt;/param&gt;\n    /// &lt;returns&gt;Instance to created entity.&lt;/returns&gt;\n    public static T MapRow&lt;T&gt;(this DataRow rowInstance) where T : class, new()\n    {\n        //Create T item\n        T instance = new T();\n\n        IEnumerable&lt;PropertyInfo&gt; properties = typeof(T).GetProperties();\n        MappingAttribute map;\n        DataColumn column;\n\n        foreach (PropertyInfo item in properties)\n        {\n            //check if custom attribute exist in this property\n            object[] definedAttributes = item.GetCustomAttributes(typeof(MappingAttribute), false);\n\n            // Tiene atributos\n            if (definedAttributes != null &amp;&amp; definedAttributes.Length == 1)\n            {\n                //recover first attribute\n                map = definedAttributes.First() as MappingAttribute;\n\n                column = rowInstance.Table.Columns.OfType&lt;DataColumn&gt;()\n                                          .Where(c =&gt; c.ColumnName == map.ColumnName)\n                                          .SingleOrDefault();\n\n                if (column != null)\n                {\n                    object dbValue = rowInstance[column.ColumnName];\n                    object valueToSet = null;\n\n                    if (dbValue == DBNull.Value)//if value is null\n                        valueToSet = map.DefaultValue;\n                    else\n                        valueToSet = dbValue;\n\n                    //Set value in property \n                    setValue&lt;T&gt;(instance, item, valueToSet);\n                }\n            }\n        }\n\n        return instance;\n    }\n\n    /// &lt;summary&gt;\n    /// Set \"item\" property.\n    /// &lt;/summary&gt;\n    /// &lt;typeparam name=\"T\"&gt;Return entity type&lt;/typeparam&gt;\n    /// &lt;param name=\"instance\"&gt;T type instance&lt;/param&gt;\n    /// &lt;param name=\"item\"&gt;Property name to return value&lt;/param&gt;\n    /// &lt;param name=\"valueToSet\"&gt;Value to set to the property&lt;/param&gt;\n    private static void setValue&lt;T&gt;(T instance, PropertyInfo item, object valueToSet) where T : class, new()\n    {\n        if (valueToSet == null)\n        {\n            CultureInfo ci = CultureInfo.InvariantCulture;\n\n            if (item.PropertyType.IsSubclassOf(typeof(System.ValueType)))\n            {\n                //if is a value type and is nullable\n                if (item.PropertyType.FullName.Contains(\"System.Nullable\"))\n                {\n                    item.SetValue(instance, null, BindingFlags.Public, null, null, ci);\n                }\n                else\n                {\n                    item.SetValue(instance, Activator.CreateInstance(item.PropertyType, null), BindingFlags.Public, null, null, ci);\n                }\n            }\n            else //property type is reference type\n            {\n                item.SetValue(instance, null, BindingFlags.Public, null, null, ci);\n            }\n        }\n        else // set not null value\n        {\n            //if is a value type and is nullable\n            if (item.PropertyType.FullName.Contains(\"System.Nullable\"))\n            {\n                item.SetValue(instance, Convert.ChangeType(valueToSet, Nullable.GetUnderlyingType(item.PropertyType)), null);\n            }\n            else\n            {\n                item.SetValue(instance, Convert.ChangeType(valueToSet, item.PropertyType), null);\n            }\n        }\n    }\n}\n</code></pre>\n\n<p>What I do here, in essence, is to map the domain entities with the database fields, and a data helper attacks the tables automatically. An example of one of these entities is:</p>\n\n<pre><code>public class ComboBox\n    {\n    /// &lt;summary&gt;\n    /// Represents a ComboBox item.\n    /// &lt;/summary&gt;\n    [Mapping(\"CODE\", DefaultValue = 0, DBType = DbParametersTypes.Varchar2, IsKey = true, IdentifierFK = \"\")]\n    public string Code { get; set; }\n\n    /// &lt;summary&gt;\n    /// Represents Text.\n    /// &lt;/summary&gt;\n    [Mapping(\"DESCRIPTION\", DefaultValue = \"\", DBType = DbParametersTypes.Varchar2, IsKey = false, IdentifierFK = \"\")]\n    public string Description { get; set; }\n\n    }\n</code></pre>\n\n<p>And the attribute class I use:</p>\n\n<pre><code>public sealed class MappingAttribute : Attribute\n    {\n        public string ColumnName { get; set; }\n\n        public object DefaultValue { get; set; }\n\n        public DbParametersTypes DBType { get; set; }\n\n        public bool IsKey { get; set; }\n\n        public string IdentifierFK { get; set; }\n\n        public bool IsParameter { get; set; } \n\n        public MappingAttribute(string columnName)\n        {\n            if (String.IsNullOrEmpty(columnName))\n                throw new ArgumentNullException(\"columnName\");\n\n            ColumnName = columnName;\n        }               \n    }\n</code></pre>\n\n<p>I read <a href=\"http://blogs.microsoft.co.il/blogs/alon_nativ/archive/2011/01/15/using-expression-trees-to-optimize-your-code.aspx\" rel=\"nofollow\">here</a> that a possible improvement could be an expression tree, but, first, I'm not an expression tress expert, and second, I have to solve this with .NET 3.5...(in the sample .NET 4 or 4.5 is used...)</p>\n\n<p>¿Suggestions?</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["c","performance","pointers","struct"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":105,"score":3,"question_id":12921758,"title":"Pointer to struct or struct itself?","body":"<p>Consider this code:</p>\n\n<pre><code>struct s { /* ... */ };\n\nvoid f(struct s x) { /* ... */) /* (1) */\n/* or */\nvoid f(const struct s *x) { /* ... */ } /* (2) */\n</code></pre>\n\n<p>When <code>struct s</code> has a decent size, in which case should we prefer the first form?</p>\n"},{"tags":["c++","c","performance","floating-point","arbitrary-precision"],"answer_count":4,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":287,"score":7,"question_id":11798640,"title":"Floats vs rationals in arbitrary precision fractional arithmetic (C/C++)","body":"<p>Since there are two ways of implementing an AP fractional number, one is to emulate the storage and behavior of the <code>double</code> data type, only with more bytes, and the other is to use an existing integer APA implementation for representing a fractional number as a rational i.e. as a pair of integers, numerator and denominator, which of the two ways are more likely to deliver efficient arithmetic in terms of performance? (Memory usage is really of minor concern.)</p>\n\n<p>I'm aware of the existing C/C++ libraries, some of which offer fractional APA with \"floats\" and other with rationals (none of them features fixed-point APA, however) and of course I could benchmark a library that relies on \"float\" implementation against one that makes use of rational implementation, but the results would largely depend on implementation details of those particular libraries I would have to choose randomly from the nearly ten available ones. So it's more <em>theoretical</em> pros and cons of the two approaches that I'm interested in (or three if take into consideration fixed-point APA).</p>\n"},{"tags":["java","eclipse","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":63,"score":0,"question_id":12920887,"title":"Eclipse Indigo Speed Issues","body":"<p>After using Eclipse Indigo for around a year, suddenly the speed of an application ran in Eclipse has decreased intensely. For example, my Java game was getting 64 FPS, then suddenly 1 FPS.</p>\n\n<p>Creating a new Workspace and adding all the files from the old project seems to fix it, but how would I fix this without making a new Workspace? (I've already tried deleting the Workspace's .metadata, made no difference.)</p>\n\n<p>I've also tried running Eclipse with the -clean argument, no help at all.\nNote that the performance issues are only when running a Java Application from Eclipse, not within Eclipse itself.</p>\n"},{"tags":["javascript","performance","application","documentation","drive"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":43,"score":0,"question_id":12920616,"title":"The execution speed is very slow Javascript for app google","body":"<p>I have a Javascript app development Drive, I use a spreadsheet to manage information with an interface, but the execution speed is very slow, is there way to improve performance?</p>\n"},{"tags":["sql","sql-server","performance","analytics"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":43,"score":2,"question_id":12920454,"title":"Select distinct group of measurements in a specified time point","body":"<p>I have a SQL table with a following structure:</p>\n\n<pre><code>CREATE TABLE dataLog (\ntstamp datetime NOT NULL, \nidentifier smallint NOT NULL, \npayload binary(46) NOT NULL, \nPRIMARY KEY (tstamp, identifier));\n</code></pre>\n\n<p>In this table logging infrastructure logs data for many devices. The device identifier ranges from eg. 1 to 125. That mean column \"identifier\" has values from 1-125. The payload column holds binary data from each device (logging information, temperatures etc.) The tstamp column holds current time information.</p>\n\n<p>How can one build a query to get a 'snapshot' information for each device in a given time point. E.g. I want to know what was the payload column value for each identifier (125) in eg. 2012-06-12 12:00:00. The data in the table were written when the device sends it and that's why the data are not with the exact timestamp given above but eg. 2 devices sent data on 2012-06-12 11:59:59, 10 devices on 2012-06-12 11:15:30 etc. The data should be detected backwards in time.</p>\n\n<p>The expected result: 125 rows with each identifier, timestamp of each measurement and payload value for each identifier.</p>\n\n<p>The data is needed to draw eg. a plot of the temperature across all of the devices from 1 to 125 at a given timestamp and next iterate through the data in eg. 5 minutes steps.</p>\n"},{"tags":["c#","performance","memory"],"answer_count":5,"favorite_count":0,"up_vote_count":10,"down_vote_count":0,"view_count":1885,"score":10,"question_id":1152573,"title":"Does \"readonly\" (C#) reduce memory usage?","body":"<p>In C#, does setting a field as readonly reduce memory usage?</p>\n\n<p>i.e.</p>\n\n<pre><code>DBRepository _db = new DBRepository();\n</code></pre>\n\n<p>vs</p>\n\n<pre><code>readonly DBRepository _db = new DBRepository();\n</code></pre>\n\n<p>Just curious. Thanks.</p>\n"},{"tags":["mysql","performance","innodb","sequence","uuid"],"answer_count":7,"favorite_count":7,"up_vote_count":19,"down_vote_count":0,"view_count":6684,"score":19,"question_id":2365132,"title":"UUID performance in MySQL?","body":"<p>We're considering using UUID values as primary keys for our MySQL database. The data being inserted is generated from dozens, hundreds, or even thousands of remote computers and being inserted at a rate of 100-40,000 inserts per second, and we'll never do any updates.</p>\n\n<p>The database itself will typically get to around 50M records before we start to cull data, so not a massive database, but not tiny either. We're also planing to run on InnoDB, though we are open to changing that if there is a better engine for what we're doing.</p>\n\n<p>We were ready to go with Java's Type 4 UUID, but in testing have been seeing some strange behavior. For one, we're storing as varchar(36) and I now realize we'd be better off using binary(16) - though how much better off I'm not sure.</p>\n\n<p>The bigger question is: how badly does this random data screw up the index when we have 50M records? Would we be better off if we used, for example, a type-1 UUID where the leftmost bits were timestamped? Or maybe we should ditch UUIDs entirely and consider auto_increment primary keys?</p>\n\n<p>I'm looking for general thoughts/tips on the performance of different types of UUIDs when they are stored as an index/primary key in MySQL. Thanks!</p>\n"},{"tags":["performance","sqlite"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":65,"score":1,"question_id":12677761,"title":"Sqlite appending data performance linear degradation, is this solvable?","body":"<p>I have a test set up to write rows to a database.\nEach transaction inserts 10,000 rows, no updates.\nEach step takes a linear time longer then the last.\nThe first ten steps took the following amount of time in ms to perform a commit</p>\n\n<p>568, 772, 942, 1247, 1717, 1906, 2268, 2797, 2922, 3816, 3945</p>\n\n<p>By the time it reaches adding 10,00 rows to a table of 500,000 rows, it takes 37149 ms to commit!</p>\n\n<ul>\n<li><p>I have no foreign key constraints.</p></li>\n<li><p>I have found using WAL, improves performance (gives figures above), but still linear degradation</p></li>\n<li><p>PRAGMA Synchronous=OFF has no effect</p></li>\n<li><p>PRAGMA locking_mode=EXCLUSIVE has no effect</p></li>\n<li><p>Ran with no additional indexes and additional indexes.  Made a roughly constant time difference, so was still a linear degradation.</p></li>\n</ul>\n\n<p>Some other settings I have</p>\n\n<ul>\n<li>setAutocommit(false)</li>\n<li>PRAGMA page_size = 4096</li>\n<li>PRAGMA journal_size_limit = 104857600</li>\n<li>PRAGMA count_changes = OFF</li>\n<li>PRAGMA cache_size = 10000</li>\n<li>Schema has Id INTEGER PRIMARY KEY ASC, insertion of which is incremental and generated by Sqlite</li>\n</ul>\n\n<p>Full Schema as follows (I have run both with and without indexes, but have included)</p>\n\n<pre><code>create table if not exists [EventLog] (\nId INTEGER PRIMARY KEY ASC, \nDocumentId TEXT NOT NULL, \nEvent TEXT NOT NULL, \nContent TEXT NOT NULL, \nTransactionId TEXT NOT NULL, \nDate INTEGER NOT NULL, \nUser TEXT NOT NULL)\n\ncreate index if not exists DocumentId ON EventLog (DocumentId)\n\ncreate index if not exists TransactionId ON EventLog (TransactionId)\n\ncreate index if not exists Date ON EventLog (Date)\n</code></pre>\n\n<p>This is using sqlite-jdbc-3.7.2 running in a windows environment</p>\n"},{"tags":["c#","performance","azure"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":95,"score":0,"question_id":12915585,"title":"azure queue performance","body":"<p>For the windows azure queues the scalability target per storage is supposed to be around 500 messages / second (<a href=\"http://msdn.microsoft.com/en-us/library/windowsazure/hh697709.aspx\" rel=\"nofollow\">http://msdn.microsoft.com/en-us/library/windowsazure/hh697709.aspx</a>). I have the following simple program that just writes a few messages to a queue. The program takes 10 seconds to complete (4 messages / second). I am running the program from inside a virtual machine (on west-europe) and my storage account also is located in west-europe. I don't have setup geo replication for my storage. My connection string is setup to use the http protocol.</p>\n\n<pre><code>       // http://blogs.msdn.com/b/windowsazurestorage/archive/2010/06/25/nagle-s-algorithm-is-not-friendly-towards-small-requests.aspx\n        ServicePointManager.UseNagleAlgorithm = false;\n\n        CloudStorageAccount storageAccount=CloudStorageAccount.Parse(ConfigurationManager.AppSettings[\"DataConnectionString\"]);\n\n        var cloudQueueClient = storageAccount.CreateCloudQueueClient();\n\n        var queue = cloudQueueClient.GetQueueReference(Guid.NewGuid().ToString());\n\n        queue.CreateIfNotExist();\n        var w = new Stopwatch();\n        w.Start();\n        for (int i = 0; i &lt; 50;i++ )\n        {\n            Console.WriteLine(\"nr {0}\",i);\n            queue.AddMessage(new CloudQueueMessage(\"hello \"+i));    \n        }\n\n        w.Stop();\n        Console.WriteLine(\"elapsed: {0}\", w.ElapsedMilliseconds);\n        queue.Delete();\n</code></pre>\n\n<p>Any idea how I can get better performance?</p>\n\n<p>EDIT:</p>\n\n<p>Based on Sandrino Di Mattia's answer I re-analyzed the code I've originally posted and found out that it was not complete enough to reproduce the error. In fact I had created a queue just before the call to ServicePointManager.UseNagleAlgorithm = false; The code to reproduce my problem looks more like this:</p>\n\n<pre><code>        CloudStorageAccount storageAccount=CloudStorageAccount.Parse(ConfigurationManager.AppSettings[\"DataConnectionString\"]);\n\n        var cloudQueueClient = storageAccount.CreateCloudQueueClient();\n\n        var queue = cloudQueueClient.GetQueueReference(Guid.NewGuid().ToString());\n\n        //ServicePointManager.UseNagleAlgorithm = false; // If you change the nagle algorithm here, the performance will be okay.\n        queue.CreateIfNotExist();\n        ServicePointManager.UseNagleAlgorithm = false; // TOO LATE, the queue is already created without 'nagle'\n        var w = new Stopwatch();\n        w.Start();\n        for (int i = 0; i &lt; 50;i++ )\n        {\n            Console.WriteLine(\"nr {0}\",i);\n            queue.AddMessage(new CloudQueueMessage(\"hello \"+i));    \n        }\n\n        w.Stop();\n        Console.WriteLine(\"elapsed: {0}\", w.ElapsedMilliseconds);\n        queue.Delete();\n</code></pre>\n\n<p>The suggested solution from Sandrino to configure the ServicePointManager using the app.config file has the advantage that the ServicePointManager is initialized when the application starts up, so you don't have to worry about time dependencies.</p>\n"},{"tags":["performance","oracle","materialized-views"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":4,"view_count":43,"score":-4,"question_id":12911392,"title":"Fast refresh taking longer than complete refresh","body":"<p>Which one is better complete or fast?</p>\n\n<pre><code> create materialized view mv_sales_at_point\n    build immediate\n    refresh fast on demand\n    enable query rewrite\n    as\n    select  year ,month ,week,t.company_id,t.branch_id,t.customer_id,\n            sum(total_sales_txn),sum(total_spend),sum(total_discount)\n    from txn_agg1 t \n    group by year,month,week,t.company_id,t.branch_id,t.customer_id;\n</code></pre>\n\n<p>My Base Table is TXN_AGG1 Table having 285 Rows.\nComplete Refresh Takes  0.219 Seconds   and Fast refresh take 0.531 seconds.\nWhich one is better to use, since this this table (TXN_AGG1) will grow eventually?</p>\n"},{"tags":["javascript","html","performance","social-networking","webpage"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":54,"score":1,"question_id":12916045,"title":"The purpose of loading External Javascript of social media into webpages","body":"<p>When adding social media resources in webpage, the traditional method results in loading much external JS from other domains, just to load an iframe or an anchor with a brand image background. Probably the below are the most transferred files over the web   (after the <a href=\"http://www.google-analytics.com/ga.js\" rel=\"nofollow\">http://www.google-analytics.com/ga.js</a> 36.35KB, which is somehow inevitable for many)</p>\n\n<p><a href=\"http://connect.facebook.net/en_US/all.js\" rel=\"nofollow\">http://connect.facebook.net/en_US/all.js</a> 181.30KB (59.06KB gzipped)<br>\n<a href=\"https://platform.twitter.com/widgets.js\" rel=\"nofollow\">https://platform.twitter.com/widgets.js</a> 75.19KB (24.42KB gzipped)<br>\n<a href=\"https://apis.google.com/js/plusone.js\" rel=\"nofollow\">https://apis.google.com/js/plusone.js</a> 16.71KB<br>\n<a href=\"http://assets.pinterest.com/js/pinit.js\" rel=\"nofollow\">http://assets.pinterest.com/js/pinit.js</a> (well this is small, but still unneeded connection)  </p>\n\n<blockquote>\n  <p>For example, <a href=\"http://connect.facebook.net/en_US/all.js\" rel=\"nofollow\">http://connect.facebook.net/en_US/all.js</a> does only one\n  thing: adding an iFrame <code>&lt;iframe src=\"//www.facebook.com/plugins/likebox.php?href=http%3A%2F%2Fwww.facebook.com%2Flavishdream&amp;amp;width=292&amp;amp;height=180&amp;amp;colorscheme=light&amp;amp;show_faces=true&amp;amp;border_color&amp;amp;stream=false&amp;amp;header=false\" scrolling=\"no\" frameborder=\"0\" style=\"border:none; overflow:hidden; width:292px; height:180px;\" allowtransparency=\"true\"&gt;&lt;/iframe&gt;</code></p>\n  \n  <p>Twitter and Google Plus scripts does very similar tasks, only adding\n  small HTML chunks into the page.</p>\n</blockquote>\n\n<p>Why not only writing those iFrames, images and anchors HTML ?</p>\n"},{"tags":["performance","media","player"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":12916451,"title":"Fast Start Play WMP","body":"<p>I am developing an application in C# that has a form with Windows Media Player. </p>\n\n<p>There is an event called \"Buffering\" in AXWindowsMediaPlayer class that signals when media player finishes buffering the content. But I could not achieve this with a single \nAXWindowsMediaPlayer object. Whatever I did, I could not continue to play the \nfirst content while buffering the second.</p>\n\n<p>I want to, Fast Start and buffering disabled for local videos.</p>\n\n<pre><code>private void button1_Click(object sender, EventArgs e)\n{\ntimer1.Interval = 100;\ntimer1.Enabled = true;\ntimer1.Start();\naxWindowsMediaPlayer1.URL=(@\"d:\\\\adobes\\\\3.avi\");\naxWindowsMediaPlayer1.Ctlcontrols.play();\n}\n\nprivate void timer1_Tick(object sender, EventArgs e)\n{\n// nearest whole number.\ndouble t = Math.Round(axWindowsMediaPlayer1.Ctlcontrols.curre ntPosition,2);\n\n\nlabel1.Text = (t.ToString());\nif (t &gt; 5 )\n{\n\naxWindowsMediaPlayer1.URL = (@\"d:\\\\adobes\\\\2.avi\");\n\n}\n\n\n\n}\n</code></pre>\n"},{"tags":[".net","winforms","performance","events","treeview"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":56,"score":3,"question_id":12915525,"title":".NET Treeview: How to prevent unchanged child nodes from being redrawn?","body":"<p>I'm working with WinForms in .NET 2010. I've created a user control that inherits from Treeview and I'm using owner-drawing for the text part of my treenodes.</p>\n\n<p>Now I had to solve a strange performance problem:</p>\n\n<p>When the text of a treenode (I call it \"parentnode\") gets changed, the treeview control fires the DrawNode-event for each of \"parentnodes\"'s child nodes, whether they are visible or not!!!</p>\n\n<p>This is causing a big performance problem for my application. How can I prevent the treeview control from firing the DrawNode event for each child node?</p>\n\n<p>Thx a lot in advance for you help!</p>\n"},{"tags":["xml","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":3,"view_count":33,"score":-3,"question_id":12915365,"title":"Performance for an XML","body":"<p>I have an xml that I will need to very frequently update. What is teh best way to achieve this such that performance of the system does not go for a toss? My xml size would not be too big, maybe 5-10 KB. Only thing is that at every statement in the xml, I will need to edit something and persist it back. Any suggestions how this can be done?</p>\n"},{"tags":["jquery","performance","code-efficiency"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":46,"score":0,"question_id":12911543,"title":"What is the most efficient way to select a fallback element if original element is not present in DOM with jQuery?","body":"<p>In a jQuery Plugin I am writing, I want to check for specific elements inside the DOM object the plugin is called on. The code looks kind of like this: </p>\n\n<pre><code>//the object that you call the plugin on, stored in the variable \"o\"\no = this;\n\n//Store children of a child element inside \"o\" in the variable \"elInsideO\"\nelInsideO = o.find('selector').children('childrenSelector');\n\n/* check if elInsideO is empty in case 'selector' is not present and then look for \nalternative element 'selector2' */\nif (elInsideO.length == 0) {\n    elInsideO = o.find('selector2').children('childrenSelector');\n}\n</code></pre>\n\n<p>Is there a more efficent way to make this selection? One more possibility I can think of is this: </p>\n\n<pre><code>if (o.find('selector').length != 0) {\n    elInsideO = o.find('selector').children('childrenSelector');\n} else {\n    elInsideO = o.find('selector2').children('childrenSelector');\n}\n</code></pre>\n\n<p>Which one of these solutions is more efficient (meaning performs better)? Is there another way that is even better?</p>\n\n<p>Thx for any help!</p>\n"},{"tags":["ruby-on-rails","performance","activerecord","asynchronous","future"],"answer_count":1,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":124,"score":5,"question_id":12900121,"title":"Is it possible to transparently implement the Future pattern for ActiveRecord queries in Ruby 1.9?","body":"<p>I'm working on an existing Rails 2 site with a large codebase that recently updated to Ruby 1.9.2 and the mysql2 gem.  I've noticed that this setup allows for non-blocking database queries; you can do <code>client.query(sql, :async =&gt; true)</code> and then later call <code>client.async_result</code>, which blocks until the query completes.</p>\n\n<p>It seems to me that we could get a performance boost by having all <code>ActiveRecord</code> queries that return a collection decline to block until a method is called on the collection.  e.g.</p>\n\n<pre><code>@widgets = Widget.find(:all, :conditions=&gt; conditions) #sends the query\ndo_some_stuff_that_doesn't_require_widgets\n@widgets.each do #if the query hasn't completed yet, wait until it does, then populate @widgets with the result. Iterate through @widgets\n...\n</code></pre>\n\n<p>This could be done by monkey-patching <code>Base::find</code> and its related methods to create a new database client, send the query asynchronously, and then immediately return a Delegator or other proxy object that will, when any method is called on it, call <code>client.async_result</code>, instantiate the result using <code>ActiveRecord</code>, and delegate the method to that.  <code>ActiveRecord</code> association proxy objects already work similarly to implement ORM.</p>\n\n<p>I can't find anybody who's done this, though, and it doesn't seem to be an option in any version of Rails.  I've tried implementing it myself and it works in console (as long as I append <code>; 1</code> to the line calling everything so that <code>to_s</code> doesn't get called on the result). But it seems to be colliding with all sorts of other magic and creating various problems.</p>\n\n<p>So, is this a bad idea for some reason I haven't thought of? If not, why isn't it the way <code>ActiveRecord</code> already works? Is there a clean way to make it happen?</p>\n"},{"tags":["java","performance","java-3d","netbeans-platform"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":71,"score":0,"question_id":12693924,"title":"How to display Java3D image faster in NetBeans platform app?","body":"<p>I made a desktop app with the NetBeans platform using Java Swing technology. In that app I do image processing on a 3D image using PointArray[] objects.</p>\n\n<p>When my app runs on a PC which has windows 7, a graphics card, and good RAM capacity (4GB RAM), my 3D image is created and displayed within  5 to 6 sec after clicking on the 3DImage button. But when my app runs on a Windows XP, low graphics configuration PC, my 3D image takes up to three minutes to render or some time it doesn't display image. So how can I solve that problem?</p>\n\n<p>My code is below.</p>\n\n<pre><code>import com.sun.j3d.utils.behaviors.mouse.MouseRotate;\nimport com.sun.j3d.utils.behaviors.mouse.MouseWheelZoom;\nimport com.sun.j3d.utils.universe.SimpleUniverse;\nimport java.awt.BorderLayout;\nimport java.awt.GraphicsConfiguration;\nimport java.awt.image.BufferedImage;\nimport javax.media.j3d.Appearance;\nimport javax.media.j3d.BoundingSphere;\nimport javax.media.j3d.BranchGroup;\nimport javax.media.j3d.Canvas3D;\nimport javax.media.j3d.ColoringAttributes;\nimport javax.media.j3d.GeometryArray;\nimport javax.media.j3d.PointArray;\nimport javax.media.j3d.PointAttributes;\nimport javax.media.j3d.Shape3D;\nimport javax.media.j3d.TransformGroup;\nimport javax.media.jai.JAI;\nimport javax.media.jai.PlanarImage;\nimport javax.swing.JButton;\nimport javax.swing.JFrame;\nimport javax.swing.JPanel;\nimport javax.swing.JScrollPane;\nimport javax.vecmath.Color3f;\nimport javax.vecmath.Point3d;\nimport javax.vecmath.Point3f;\n\n\npublic final class FinalDImage extends JPanel {\n\n    private static int s = 0, count = 0, r = 0, g = 0, b = 0, medHeight = 0, medWidth = 0;\n    private static float divisior1 = 300000.0f;\n    private static Shape3D plShape;\n    private static TransformGroup objRotate;\n    private static BranchGroup scene;\n    private static JButton btn;\n    private static Canvas3D canvas3D;\n    private static SimpleUniverse simpleU;\n    private static GraphicsConfiguration gc;\n    private static BufferedImage histImage;\n    private static JPanel jPanel1 = new JPanel();\n\n    public FinalDImage(BufferedImage histImage) {\n        FinalDImage.histImage = histImage;\n        medWidth = (int) (histImage.getWidth() / 2.0f);\n        medHeight = (int) (histImage.getHeight() / 2.0f);\n        this.add(jPanel1);\n        initComponents();\n    }\n\n    public void initComponents() {\n        setLayout(new BorderLayout());\n        btn = new JButton(\"Intensity\");\n        gc = SimpleUniverse.getPreferredConfiguration();\n        canvas3D = new Canvas3D(gc);//See the added gc? this is a preferred config\n        add(\"Center\", canvas3D);\n        add(btn, BorderLayout.SOUTH);\n        scene = createSceneGraph(FinalDImage.histImage);\n        scene.setCapability(BranchGroup.ALLOW_DETACH);\n        scene.compile();\n        simpleU = new SimpleUniverse(canvas3D);\n        simpleU.getViewingPlatform().setNominalViewingTransform();\n        simpleU.addBranchGraph(scene);\n    }\n\n    public BranchGroup createSceneGraph(BufferedImage histImage) {\n        count = 0;\n        BranchGroup lineGroup = new BranchGroup();\n        Appearance app = new Appearance();\n        ColoringAttributes ca = new ColoringAttributes(new Color3f(204.0f, 204.0f, 204.0f), ColoringAttributes.SHADE_FLAT);\n        app.setColoringAttributes(ca);\n\n        Point3f plaPts;\n        Color3f color;\n        PointArray pla = new PointArray(histImage.getWidth() * histImage.getHeight(), GeometryArray.COLOR_3 | GeometryArray.COORDINATES);\n\n        if (histImage.getType() == 11) {\n            for (int i = histImage.getWidth() - 3; i &gt;= 3; i--) {\n                for (int j = histImage.getHeight() - 3; j &gt;= 3; j--) {\n                    s = histImage.getRaster().getSample(i, j, 0);\n                    plaPts = new Point3f((medWidth - i) / 1500.0f,\n                                         (medHeight - j) / 1500.0f,\n                                         s / divisior1);\n                    color = new Color3f(s / 60000.0f, s / 60000.0f, s / 60000.0f);\n                    pla.setCoordinate(count, plaPts);\n                    pla.setColor(count, color);\n                    count++;\n                }\n            }\n        } else {\n            for (int i = 2; i &lt; histImage.getWidth() - 2; i++) {\n                for (int j = 2; j &lt; histImage.getHeight() - 2; j++) {\n                    r = histImage.getRaster().getSample(i, j, 0);\n                    g = histImage.getRaster().getSample(i, j, 1);\n                    b = histImage.getRaster().getSample(i, j, 2);\n                    s = (r + g + b) / 3;\n                    plaPts = new Point3f((medWidth - i) / 1200.0f,\n                                         (medHeight - j) / 1200.0f,\n                                         s / (divisior1/600.0f));\n                    color = new Color3f(r / 300.0f, g / 300.0f, b / 300.0f);\n                    pla.setCoordinate(count, plaPts);\n                    pla.setColor(count, color);\n                    count++;\n                }\n            }\n        }\n\n\n        PointAttributes a_point_just_bigger = new PointAttributes();\n        a_point_just_bigger.setPointSize(4.0f);//10 pixel-wide point\n        a_point_just_bigger.setPointAntialiasingEnable(true); //now points are sphere-like (not a cube)\n        app.setPointAttributes(a_point_just_bigger);\n        plShape = new Shape3D(pla,app);\n        objRotate = new TransformGroup();\n        objRotate.setCapability(TransformGroup.ALLOW_TRANSFORM_WRITE);\n        objRotate.addChild(plShape);\n        lineGroup.addChild(objRotate);\n        MouseRotate myMouseRotate = new MouseRotate();\n        myMouseRotate.setFactor(0.015, 0.015);\n        myMouseRotate.setTransformGroup(objRotate);\n        myMouseRotate.setSchedulingBounds(new BoundingSphere());\n        myMouseRotate.setSchedulingBounds(new BoundingSphere(\n                new Point3d(0.0, 0.0, 0.0), 100));\n\n        lineGroup.addChild(myMouseRotate);\n        MouseWheelZoom mz = new MouseWheelZoom();\n        mz.setFactor(0.01);\n        mz.setTransformGroup(objRotate);\n        mz.setSchedulingBounds(new BoundingSphere());\n        lineGroup.addChild(mz);\n        return lineGroup;\n    }\n\n    public static void main(String[] args) {\n        PlanarImage plImg3 = JAI.create(\"fileload\", \"E:\\\\Data\\\\office\\\\teeth3.tiff\");\n        BufferedImage histImage = plImg3.getAsBufferedImage();\n        JFrame frame = new JFrame();\n        frame.add(new JScrollPane(new FinalDImage(histImage)));\n        frame.setSize(800, 700);\n        frame.setVisible(true);\n        frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\n    }\n}\n</code></pre>\n"},{"tags":["performance","amazon-s3"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":38,"score":0,"question_id":12914307,"title":"Number of objects in Amazon S3 (most efficient way)","body":"<p>Right now I am migrating to Amazon S3 from standard server. In current server, I have to set structure of directories in order to stay more efficient. For example, current structure of directory is: category_id/article_id/year/month/day/article_image.jpg and so on. </p>\n\n<p>So, do I need to create such structure in the bucket of Amazon S3. Does it affect speed of requests? </p>\n"},{"tags":["ajax","performance","magento"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":46,"score":0,"question_id":12914084,"title":"How to improve Magento Ajax Performance?","body":"<p>I'm currently developing an ajax plugin for Magento and wonder how I can improve it's performance. Let's start with an example. I want to show the number of items in the shopping cart on an external page. Or a page that has been rendered via full page caching and needs a to update the cart info via a JavaScript Ajax request. </p>\n\n<p><em>For that I see two ways of doing it.</em> </p>\n\n<p><strong>Making use of a Magento Controller stripped down to the minimum</strong></p>\n\n<pre><code>class AFCustom_CartInfo_AjaxController extends Mage_Core_Controller_Front_Action {\n\n    public function indexAction() {\n      header('Cache-Control: no-cache, must-revalidate');\n      header('Expires: Mon, 26 Jul 1997 05:00:00 GMT');\n      header('Content-Type: text/html; charset=utf-8');\n\n      $out = $this-&gt;__('My Cart');\n      if (Mage::helper('core')-&gt;isModuleOutputEnabled('Mage_Checkout')) {\n        $count = Mage::getSingleton('checkout/cart')-&gt;getSummaryQty() ? Mage::getSingleton('checkout/cart')-&gt;getSummaryQty()\n            : MAGE::helper('checkout/cart')-&gt;getSummaryCount();\n        if ($count == 1) {\n          $out = $this-&gt;__('My Cart (%s item)', $count);\n        } elseif ($count &gt; 0) {\n          $out = $this-&gt;__('My Cart (%s items)', $count);\n        } else {\n          $out = $this-&gt;__('My Cart');\n        }\n      }\n      echo $out;\n      exit;\n    }\n}\n</code></pre>\n\n<p>As you can see I exit the code in the controller and don't user render layout. However the requests are still taking quite long. I assume because it has to load the whole Framework. Are there ways to just load the minimum requirements of Magento to execute the controller? Would it be able to disable the layout engine as it is not needed? How would I do that? What other modules could I switch off?</p>\n\n<p><strong>Observer to Update Session</strong>\nA second solution would be to have an observer to listen for changes in the shopping cart (checkout_cart_save_after) and update an PHP session variable. Which could be read out in a small custom PHP script. However, I'm not sure how I could easily patch into the same session mechanism as Magento uses? I figure that it might not be advisable to directly use $_SESSION as an Magento session might be handled elsewhere. </p>\n\n<p>What would you do? Any pointers are appreciated? </p>\n\n<p>Many thanks!</p>\n"},{"tags":["c++","perl","optimization","performance"],"answer_count":20,"favorite_count":17,"up_vote_count":58,"down_vote_count":5,"view_count":10646,"score":53,"question_id":885908,"title":"while (1) Vs. for (;;) Is there a speed difference?","body":"<p>Long version...</p>\n\n<p>A co-worker asserted today after seeing my use of <code>while (1)</code> in a Perl script that <code>for (;;)</code> is faster.  I argued that they should be the same hoping that the interpreter would optimize out any differences. I set up a script that would run 1,000,000,000 for loop iterations and the same number of while loops and record the time between. I could find no appreciable difference. My co-worker said that a professor had told him that the <code>while (1)</code> was doing a comparison <code>1 == 1</code> and the <code>for (;;)</code> was not.  We repeated the same test with the 100x the number of iterations with C++ and the difference was negligible. It was however a graphic example of how much faster compiled code can be vs. a scripting language.</p>\n\n<p>Short version...</p>\n\n<p>Is there any reason to prefer a <code>while (1)</code> over a <code>for (;;)</code> if you need an infinite loop to break out of?</p>\n\n<p><strong>Note:</strong> If it's not clear from the question.  This was purely a fun academic discussion between a couple of friends.  I am aware this is not a super important concept that all programmers should agonize over.  Thanks for all the great answers I (and I'm sure others) have learned a few things from this discussion.</p>\n\n<p><strong>Update:</strong> The aforementioned co-worker weighed in with a response below.</p>\n\n<p>Quoted here in case it gets buried.</p>\n\n<blockquote>\n  <p>It came from an AMD assembly programmer. He stated that C programmers\n  (the poeple) don't realize that their code has inefficiencies. He said\n  today though, gcc compilers are very good, and put people like him out\n  of business. He said for example, and told me about the <code>while 1</code> vs\n  <code>for(;;)</code>. I use it now out of habit but gcc and especially interpreters\n  will do the same operation (a processor jump) for both these days,\n  since they are optimized.</p>\n</blockquote>\n"},{"tags":["asp.net","asp.net-mvc","performance","asp.net-webforms"],"answer_count":13,"favorite_count":27,"up_vote_count":73,"down_vote_count":1,"view_count":18829,"score":72,"question_id":43743,"title":"ASP.NET MVC Performance","body":"<p>I found some wild remarks that ASP.NET MVC is 30x faster than ASP.NET WebForms. What real performance difference is there, has this been measured and what are the performance benefits.</p>\n\n<p>This is to help me consider moving from ASP.NET WebForms to ASP.NET MVC.</p>\n"},{"tags":["mysql","performance","query"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":48,"score":2,"question_id":12912890,"title":"Simple MySQL query with performance issues","body":"<p>I have the following simple MySQL query:</p>\n\n<pre><code>SELECT SQL_NO_CACHE mainID\nFROM tableName \nWHERE otherID3=19\nAND dateStartCol &gt;= '2012-08-01' \nAND dateStartCol &lt;= '2012-08-31';\n</code></pre>\n\n<p>When I run this it takes 0.29 seconds to bring back 36074 results. When I increase my date period to bring back more results (65703) it runs in 0.56. When I run other similar SQL queries on the same server but on different tables (some tables are larger) the results come back in approximately 0.01 seconds.</p>\n\n<p>Although 0.29 isn't slow - this is a basic part for a complex query and this timing means that it is not scalable.</p>\n\n<p>See below for the table definition and indexes.  </p>\n\n<p>I know it's not server load as I have the same issue on a development server which has very little usage.</p>\n\n<pre><code>+---------------------------+--------------+------+-----+---------+----------------+\n| Field                     | Type         | Null | Key | Default | Extra          |\n+---------------------------+--------------+------+-----+---------+----------------+\n| mainID                    | int(11)      | NO   | PRI | NULL    | auto_increment |\n| otherID1                  | int(11)      | NO   | MUL | NULL    |                |\n| otherID2                  | int(11)      | NO   | MUL | NULL    |                |\n| otherID3                  | int(11)      | NO   | MUL | NULL    |                |\n| keyword                   | varchar(200) | NO   | MUL | NULL    |                |\n| dateStartCol              | date         | NO   | MUL | NULL    |                |\n| timeStartCol              | time         | NO   | MUL | NULL    |                |\n| dateEndCol                | date         | NO   | MUL | NULL    |                |\n| timeEndCol                | time         | NO   | MUL | NULL    |                |\n| statusCode                | int(1)       | NO   | MUL | NULL    |                |\n| uRL                       | text         | NO   |     | NULL    |                |\n| hostname                  | varchar(200) | YES  | MUL | NULL    |                |\n| IPAddress                 | varchar(25)  | YES  |     | NULL    |                |\n| cookieVal                 | varchar(100) | NO   |     | NULL    |                |\n| keywordVal                | varchar(60)  | NO   |     | NULL    |                |\n| dateTimeCol               | datetime     | NO   | MUL | NULL    |                |\n+---------------------------+--------------+------+-----+---------+----------------+\n\n\n+--------------------+------------+-------------------------------+--------------+---------------------------+-----------+-------------+----------+--------+------+------------+---------+\n| Table              | Non_unique | Key_name                      | Seq_in_index | Column_name               | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment |\n+--------------------+------------+-------------------------------+--------------+---------------------------+-----------+-------------+----------+--------+------+------------+---------+\n| tableName          |          0 | PRIMARY                       |            1 | mainID                    | A         |      661990 |     NULL | NULL   |      | BTREE      |         |\n| tableName          |          1 | idx_otherID1                  |            1 | otherID1                   | A         |      330995 |     NULL | NULL   |      | BTREE      |         |\n| tableName          |          1 | idx_otherID2                  |            1 | otherID2                   | A         |          25 |     NULL | NULL   |      | BTREE      |         |\n| tableName          |          1 | idx_otherID3                  |            1 | otherID3                   | A         |          48 |     NULL | NULL   |      | BTREE      |         |\n| tableName          |          1 | idx_dateStartCol              |            1 | dateStartCol               | A         |         187 |     NULL | NULL   |      | BTREE      |         |\n| tableName          |          1 | idx_timeStartCol              |            1 | timeStartCol               | A         |       73554 |     NULL | NULL   |      | BTREE      |         |\n|tableName          |          1 | idx_dateEndCol                 |            1 | dateEndCol                 | A         |         188 |     NULL | NULL   |      | BTREE      |         |\n|tableName          |          1 | idx_timeEndCol                 |            1 | timeEndCol                 | A         |       73554 |     NULL | NULL   |      | BTREE      |         |\n| tableName          |          1 | idx_keyword                   |            1 | keyword                    | A         |       82748 |     NULL | NULL   |      | BTREE      |         |\n| tableName           |          1 | idx_hostname                 |            1 | hostname                   | A         |        2955 |     NULL | NULL   | YES  | BTREE      |         |\n| tableName           |          1 | idx_dateTimeCol              |            1 | dateTimeCol                | A         |      220663 |     NULL | NULL   |      | BTREE      |         |\n| tableName           |          1 | idx_statusCode               |            1 | statusCode                 | A         |           2 |     NULL | NULL   |      | BTREE      |         |\n+--------------------+------------+-------------------------------+--------------+---------------------------+-----------+-------------+----------+--------+------+------------+---------+\n</code></pre>\n\n<p>Explain Output:</p>\n\n<pre><code>+----+-------------+-----------+-------+----------------------------------+-------------------+---------+------+-------+----------+-------------+\n| id | select_type | table     | type  | possible_keys                    | key               | key_len | ref  | rows  | filtered | Extra       |\n+----+-------------+-----------+-------+----------------------------------+-------------------+---------+------+-------+----------+-------------+\n|  1 | SIMPLE      | tableName | range | idx_otherID3,idx_dateStartCol | idx_dateStartCol | 3       | NULL | 66875 |    75.00 | Using where |\n+----+-------------+-----------+-------+----------------------------------+-------------------+---------+------+-------+----------+-------------+\n</code></pre>\n"},{"tags":["asp.net-mvc","database","performance","design-patterns","architecture"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":45,"score":0,"question_id":12913249,"title":"Some approach to work with big data cases","body":"<p>At the company that I work we have some problems with performance loading the data of database.\nOur ERP works with a big database and we need to make some complex queries.\nWe are using C# following the DDD pattern and in the front-end we are using ASP.NET MVC.</p>\n\n<p>The example that I'm talking is that:\nWe have alot of lists (grids) in the system where the data is loaded by ajax. But everytime that a user enter in some page like that (an html page), we need to make a query in the database.</p>\n\n<p>Its important to say that this data aren't only for \"query\". This data is constantly changed by the users.</p>\n\n<p>My doubt is if you have any sugestion to how we can minimize this problem. I already have searching some solutions of cache in the server side, but I don't have any experience working with cache in high complex scenarios.</p>\n\n<p>Very thanks,\nRenan Cunha.</p>\n"},{"tags":["c++","performance","huffman"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":993,"score":2,"question_id":807979,"title":"Efficient Huffman tree search while remembering path taken","body":"<p>As a follow up question related to my <a href=\"http://stackoverflow.com/questions/759707/efficient-way-of-storing-huffman-tree\">question regarding efficient way of storing huffman tree's</a> I was wondering what would be the fastest and most efficient way of searching a binary tree (based on the Huffman coding output) and storing the path taken to a particular node.</p>\n\n<p>This is what I currently have:</p>\n\n<ul>\n<li>Add root node to queue</li>\n<li>while queue is not empty, pop item off queue\n<ul>\n<li>check if it is what we are looking\n<ul>\n<li>yes:\nFollow a head pointer back to the root node, while on each node we visit checking whether it is the left or right and making a note of it.</li>\n<li>break out of the search</li>\n</ul></li>\n<li>enqueue left, and right node</li>\n</ul></li>\n</ul>\n\n<p>Since this is a Huffman tree, all of the entries that I am looking for will exist. The above is a breadth first search, which is considered the best for Huffman trees since items that are in the source more often are higher up in the tree to get better compression, however I can't figure out a good way to keep track of how we got to a particular node without backtracking using the head pointer I put in the node.</p>\n\n<p>In this case, I am also getting all of the right/left paths in reverse order, for example, if we follow the head to the root, and we find out that from the root it is right, left, left, we get left, left, right. or 001 in binary, when what I am looking for is to get 100 in an efficient way.</p>\n\n<p>Storing the path from root to the node as a separate value inside the node was also suggested, however this would break down if we ever had a tree that was larger than however many bits the variable we created for that purpose could hold, and at that point storing the data would also take up huge amounts of memory.</p>\n"},{"tags":["java","performance","methods"],"answer_count":3,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":75,"score":1,"question_id":12912543,"title":"How to know if method is N or N^2","body":"<p>I often see you guys talking about N methods and N^2 methods, which, correct me if I'm wrong, indicate how fast a method is. My question is: how do you guys know which methods are N and which are N^2? And also: are there other speed indications of methods then just N and N^2?</p>\n"},{"tags":["performance","glassfish","monitoring"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":19,"score":0,"question_id":12912495,"title":"What is the performance impact for GlassFish 2.1 of asadmin get --monitor command?","body":"<p>We are monitoring GlassFish by setting the monitoring levels of monitored elements to LOW or HIGH:</p>\n\n<pre><code>asadmin set server.monitoring-service.module-monitoring-levels.jvm=LOW\nasadmin set server.monitoring-service.module-monitoring-levels.http-service=HIGH\n</code></pre>\n\n<p>And then we use <code>asadmin</code> command to gather the metrics during one hour with a 10-seconds precision:</p>\n\n<pre><code>asadmin get --monitor --interval 10 --iterations 360 \\\n        server.http-service.server.http-listener-2.requestcount-count \\\n        server.jvm.heapsize-current \\\n        server.jvm.memory.usedheapsize-count \\\n        server.jvm.memory.usednonheapsize-count\n</code></pre>\n\n<p>It outputs the metrics every ten seconds during one hour. Each batch of metrics are separated by two empty line. We analyze this output to push these metrics in a statsd/graphite server (nice tool by the way).</p>\n\n<p>What is the performance impact of using the <code>asadmin get --monitor</code> command during a 1-hour period? I assume that it has been designed to be efficient but I do not find any note about this. Is it the good way to gather GlassFish metrics or should we be using JMX or something else?</p>\n\n<p>We are using GlassFish 2.1.1</p>\n"},{"tags":["java","performance","atomic"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":65,"score":3,"question_id":12910981,"title":"AtomicLong vs Long performance","body":"<p>Context:\nI am using netty and have defined a handler in order to count and categorize incoming/outgoing traffic. For this I have used an enumMap that looks like this:</p>\n\n<pre><code>EnumMap&lt;MyEnum, AtomicLong&gt;\n</code></pre>\n\n<p>However now I have realized that there is only one thread that is manipulating the values (previously I thought it was more than one, netty seems to guarantee that one thread per channel). This means that AtomicLong is not necessary. <strong>However, as AtomicLong is a wrapper for a primitive long meanwhile Long is an immutable type, I have a reason to think that just swapping AtomicLong to Long will be less performant.</strong></p>\n\n<p><strong>Any ideas on this?</strong></p>\n\n<p>What I probably should do is to move to int and remove the whole enumMap thing..</p>\n\n<p>BR\nSebastian</p>\n"},{"tags":["performance","optimization","opencl"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":87,"score":1,"question_id":12900835,"title":"How can I optimize this OpenCL Sobel Filter Kernel?","body":"<p>I wrote an OpenCL kernel for a 3x3 Sobel filter, and currently it's running in about 17 millis on a 2k x 2k image. This isn't as a fast as I had hoped; does anyone have any suggestions for how to improve the speed? I've followed most of the suggestions on the <a href=\"http://software.intel.com/en-us/articles/tips-and-tricks-for-kernel-development/\" rel=\"nofollow\">checklist for optimizing kernels</a>. My processor is an Intel i5-3450. The workgroup size is 8x8, and the number of workitems is height x width / 16, which is 2048 x 128 on the images I'm running on.</p>\n\n<pre><code>__kernel void localCacheSobelFilter(\n  const __global char16* src, \n  __write_only __global float16* angle, \n  __write_only __global float16* mag, \n  const int width, \n  const int height)\n{\n  // Cache the data we're looking at in __local space\n  const int row = get_global_id(0);\n  const int col = get_global_id(1);\n\n  const int cacheRow = get_local_id(0) + 1;\n  const int cacheCol = get_local_id(1) + 1;\n\n  __local char16 cache[BLOCK_SIZE + 2][BLOCK_SIZE + 2];\n\n\n  cache[cacheRow][cacheCol] = src[ indexOf(row, col) ];\n\n  // --- Deal with the boundary conditions\n  // This adds in the rows above and below the local block,\n  // ignoring the corners.\n  const bool atTopRow = (cacheRow == 1);\n  const bool atBottomRow = (cacheRow == BLOCK_SIZE);\n\n  if(atTopRow) {\n    cache[0][cacheCol] = src[ indexOf(row - 1, col) ];\n\n  } else if (atBottomRow) {\n    cache[BLOCK_SIZE + 1][cacheCol] = src[ indexOf(row + 1, col) ];\n  }\n\n\n  // This adds in the columns to the left and right of the local block,\n  // ignoring the corners.\n  const bool atLeftCol = (cacheCol == 1);\n  const bool atRightCol = (cacheCol == BLOCK_SIZE);\n\n  if(atLeftCol) { \n    cache[cacheRow][0].sf = src[ indexOf(row, col - 1) ].sf;\n\n  } else if (atRightCol) {\n    cache[cacheRow][BLOCK_SIZE + 1].s0 = src[ indexOf(row, col + 1) ].s0;\n  }\n\n\n  // Now finally check the corners\n  const bool atTLCorner = atTopRow &amp;&amp; atLeftCol;\n  const bool atTRCorner = atTopRow &amp;&amp; atRightCol;\n  const bool atBLCorner = atBottomRow &amp;&amp; atLeftCol;\n  const bool atBRCorner = atBottomRow &amp;&amp; atRightCol;\n\n\n  if(atTLCorner) {\n    cache[0][0].sf = src[ indexOf(row - 1, col - 1) ].sf;\n\n  } else if (atTRCorner) { \n    cache[0][BLOCK_SIZE + 1].s0 = src[ indexOf(row - 1, col + 1) ].s0;\n\n  } else if (atBLCorner) {\n    cache[BLOCK_SIZE + 1][0].sf = src[ indexOf(row + 1, col - 1) ].sf;\n\n  } else if (atBRCorner) {\n    cache[BLOCK_SIZE + 1][BLOCK_SIZE + 1].s0 = src[ indexOf(row + 1, col + 1) ].s0;\n  }\n\n  barrier(CLK_LOCAL_MEM_FENCE); \n\n  //===========================================================================\n  // Do the calculation\n\n  //  [..., pix00]  upperRow  [pix02, ...]\n  //  [..., pix10]  centerRow [pix12, ...]\n  //  [..., pix20]  lowerRow  [pix22, ...]\n  const char pix00 = cache[cacheRow - 1][cacheCol - 1].sf;\n  const char pix10 = cache[cacheRow    ][cacheCol - 1].sf;\n  const char pix20 = cache[cacheRow + 1][cacheCol - 1].sf;\n\n  const char16 upperRow  = cache[cacheRow - 1][cacheCol];    \n  const char16 centerRow = cache[cacheRow    ][cacheCol];\n  const char16 lowerRow  = cache[cacheRow + 1][cacheCol];\n\n  const char pix02 = cache[cacheRow - 1][cacheCol + 1].s0;\n  const char pix12 = cache[cacheRow    ][cacheCol + 1].s0;              \n  const char pix22 = cache[cacheRow + 1][cacheCol + 1].s0;\n\n\n\n  // Do the calculations for Gy\n  const char16 upperRowShiftLeft = (char16)(upperRow.s123456789abcdef, pix02);\n  const char16 upperRowShiftRight = (char16)(pix00, upperRow.s0123456789abcde);\n\n  const char16 lowerRowShiftLeft = (char16)(lowerRow.s123456789abcdef, pix22);\n  const char16 lowerRowShiftRight = (char16)(pix20, lowerRow.s0123456789abcde);\n\n  const float16 Gy = convert_float16(\n    (upperRowShiftLeft + 2 * upperRow + upperRowShiftRight)\n    - (lowerRowShiftLeft + 2 * lowerRow + lowerRowShiftRight));\n\n\n  // Do the calculations for Gx\n  const char16 centerRowShiftLeft = (char16)(centerRow.s123456789abcdef, pix12);\n  const char16 centerRowShiftRight = (char16)(pix10, centerRow.s0123456789abcde);\n\n  const float16 Gx = convert_float16(\n    (upperRowShiftRight + 2 * centerRowShiftRight + lowerRowShiftRight)\n    - (upperRowShiftLeft + 2 * centerRowShiftLeft + lowerRowShiftLeft));\n\n\n  // Find the angle and magnitude\n  angle[ indexOf(row, col) ] = 0.0; //atan2(Gy, Gx);\n  mag[ indexOf(row, col) ] = ALPHA * max(Gx, Gy) + BETA * min(Gx, Gy);\n}\n</code></pre>\n\n<p>Any help would be greatly appreciated. Thanks!</p>\n"},{"tags":[".net","performance","sqlite","non-admin"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":45,"score":2,"question_id":12908960,"title":"System.Data.SQLite slow connect for non-admin users","body":"<p>I have a .NET 4 application (in mixed mode) with System.Data.Sqlite (1.0.82) for database access to an encrypted database. </p>\n\n<p>When I install the application to \"c:\\program files\\myfolder\" the connect to the sqlite database file is slow. Log files show that it's the sqlite connect statement that is delayed by a few seconds. </p>\n\n<p>The problem does not occur when I do the following:</p>\n\n<ul>\n <li>Run the application with admin privileges</li>\n <li>Install any other place than c:\\program files\\</li>\n <li>Install the application to c:\\program files\\, but move the database to another folder.</li>\n</ul>\n\n<p>I have no clue what can be the cause of this...</p>\n"},{"tags":["performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":3,"view_count":21,"score":-3,"question_id":12909476,"title":"Youtube streaming status test [KT research center]","body":"<p>Dear Youtube developer</p>\n\n<p>I'm working in the KT research center in korea.\nAsk something for the youtube function. \nI'd like to check the ssid and streaming speed (mbps) everytime, when youtube is continually running.</p>\n\n<p>if you can could you please send me a api or guide to me ?</p>\n\n<p>Thanks and bestRegards</p>\n\n<p>-Woong-</p>\n"},{"tags":["linux","performance","file-io","rsync","mv"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":25,"score":0,"question_id":12909053,"title":"What is the fastest way to move a million images from one directory to another?","body":"<p>I have a million images totally 30gb of disk space that need to move from one local directory to another local directory.</p>\n\n<p>What will be the most efficient way?  mv?  cp?  rsync?  Something else?  Tips?</p>\n\n<pre><code>/path/to/old-img-dir/*\n                     00000000.jpg\n                     --------.jpg  ## nearly 1M of them! ##\n                     ZZZZZZZZ.jpg\n</code></pre>\n\n<p>Move them to here:</p>\n\n<pre><code>/path/to/new/img/dir/\n</code></pre>\n"},{"tags":["c#","performance","entity-framework","sql-server-2008"],"answer_count":7,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":4387,"score":3,"question_id":3890974,"title":"Entity Framework 4 : Bad performance with SQL Server 2008","body":"<p>I'am developing a software based on Entity Framework to handle data in a MS SQL Server 2008 database.</p>\n\n<p><strong>[Trouble 1]</strong></p>\n\n<p>I've just tried to insert some small data (<strong>about 2 Mb</strong>) from my progam to the database : the performance are very bad ! It takes <strong>more than 1 minute</strong> to insert these datas !</p>\n\n<p>I've try to generate pre-compiled views, I've got the same results :-(</p>\n\n<p>All my code use a business layer (automatically generated from .edmx file with T4 template) to manage data in a service layer. It is very pratical to navigate in the relations of objects.</p>\n\n<p><strong>How can I improve the performance of these inserts with Entity Framework ?</strong></p>\n\n<p><strong>[Trouble 2]</strong></p>\n\n<p>Also, before inserting data in database with SaveChanges() method, I fill my object context with AddObject() method. I add about 100 000 small objects (about 2 Mb) to my object context with AddObject() : it takes a very long time (more than 10 minutes) !</p>\n\n<p><strong>How can I decrease this time ?</strong></p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>My program must save more than 50 Mb in database in less than 2-3 minutes ? Do you think it will be possible with EF ?</p>\n"},{"tags":["php","performance","optimization","if-statement"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":22,"score":-1,"question_id":12908780,"title":"php ideal way to check function returns true","body":"<p>Which of the following way of checking whether a  function returns True or FALSE is best, in terms of efficiency and code readability </p>\n\n<p>I was told by a friend that Method B is a good practice, but I believe Method A is better since it checks whether function returns TRUE then Assigns</p>\n\n<p><strong>Method A:</strong>  (Checks whether function is TRUE and <strong>then</strong> assigns </p>\n\n<pre><code>  if( $result = $db-&gt;getResult($id)){\n      echo 'pass';\n   }else{\n      echo 'fail';\n   }\n</code></pre>\n\n<p><strong>Method B</strong>  (first <strong>assigns</strong> the value, then <strong>checks</strong> , '<strong>TWO OPERATIONS'</strong></p>\n\n<pre><code>  $result = $db-&gt;getResults(90);\n if($result){\n      echo 'pass';\n  }else{\n      echo 'fail';\n   }\n</code></pre>\n\n<hr>\n\n<pre><code>   public function getResults($no){\n     if($no&gt;85){\n       return TRUE;}\n      else{\n       return FALSE;}\n</code></pre>\n"},{"tags":["css","performance","internet-explorer","profiler"],"answer_count":4,"favorite_count":17,"up_vote_count":32,"down_vote_count":0,"view_count":2080,"score":32,"question_id":5173122,"title":"CSS Performance Profiler?","body":"<p>I'm currently working on a site, and somewhere in my mass of stylesheets, something is killing performance in IE.  Are there any good CSS profilers out there?  I'd like a tool that can pinpoint rules that are killing performance.</p>\n\n<p>Before you ask, I've disabled JavaScript, opacity, and box-shadow/text-shadow rules.  The page is still jumpy.  :/  If I disable all CSS, it runs great.  </p>\n\n<p>I need a tool that can profile the page and report where the CSS bottlenecks are.</p>\n"},{"tags":["performance","tinymce","loading","moodle"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":70,"score":0,"question_id":11982538,"title":"Moodle 2.2.3 TinyMCE loads slowly","body":"<p>In Moodle 2.2.3, after 10-12 seconds TinyMCE (TinyMCE HTML editor; editor_tinymce; Standard;    2012030300) buttons will show up (loads very slowly). Where problem lies? Can't figure out how to speed up TinyMCE HTML editor loading time.</p>\n\n<p>I have Moodle 2.2.3+ (Build: 20120519).</p>\n"},{"tags":["ruby-on-rails","database","performance"],"answer_count":1,"favorite_count":2,"up_vote_count":0,"down_vote_count":0,"view_count":56,"score":0,"question_id":12907264,"title":"What are the common and useful techniques to optimize database performance?","body":"<p>I was wondering how we could usually optimize the database performance for a data intensive web application. Are there any common requirements like what queries to use, how to deal with relationships fast, whether to avoid writing to database as much as possible, etc. Anything related works.</p>\n\n<p>Or more specifically, any techniques to make the Ruby on Rails database more efficient?</p>\n\n<p>Thanks a lot!</p>\n"}]}
{"total":25593,"page":11,"pagesize":100,"questions":[{"tags":["performance","tinymce","loading","moodle"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":70,"score":0,"question_id":11982538,"title":"Moodle 2.2.3 TinyMCE loads slowly","body":"<p>In Moodle 2.2.3, after 10-12 seconds TinyMCE (TinyMCE HTML editor; editor_tinymce; Standard;    2012030300) buttons will show up (loads very slowly). Where problem lies? Can't figure out how to speed up TinyMCE HTML editor loading time.</p>\n\n<p>I have Moodle 2.2.3+ (Build: 20120519).</p>\n"},{"tags":["ruby-on-rails","database","performance"],"answer_count":1,"favorite_count":2,"up_vote_count":0,"down_vote_count":0,"view_count":56,"score":0,"question_id":12907264,"title":"What are the common and useful techniques to optimize database performance?","body":"<p>I was wondering how we could usually optimize the database performance for a data intensive web application. Are there any common requirements like what queries to use, how to deal with relationships fast, whether to avoid writing to database as much as possible, etc. Anything related works.</p>\n\n<p>Or more specifically, any techniques to make the Ruby on Rails database more efficient?</p>\n\n<p>Thanks a lot!</p>\n"},{"tags":["performance","magento"],"answer_count":5,"favorite_count":5,"up_vote_count":4,"down_vote_count":0,"view_count":742,"score":4,"question_id":9216743,"title":"Tweaking magento for performance","body":"<p>i'm looking on performance (server load time) of magento site and i'm trying to tune search result pages. I realized that when I disabled all heavy things like top navigation, lev layered navigation and product listing and I cleared all cache then after this magento core does like 60 SQL queries agains a database. Does anyone have any procedure how to rid of them or how to reduce them to some acceptable amount?</p>\n\n<p>Also can I somehow reduce a time spent during creating of blocks?</p>\n\n<p>Thank you very much,\nJaro.</p>\n"},{"tags":["mysql","performance","cpu"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":354,"score":0,"question_id":6710563,"title":"MYSQL suddenly uses CPU even when no one user is connected to Mysql","body":"<p>Site is an ecommerce site: 5 Million records in table\nTwo servers: one for webserver and other contains mysql\nSearch is happenning through Sphinx server. So search queries do not come to MySQL</p>\n\n<p>Mysql configuration: Dual Quad Core Zeoo 2.0, 146 GB, 16 GB RAM.\nWebserver configuration: Dual Quad Core Zeoo 2.0, 146 GB, 16 GB RAM.</p>\n\n<p>For past four days I find MySQL is using CPU continuously for at least 6-7 hours in a day. It becomes normal after that. Even if I restart, it doesnt stop. It again uses CPU in 2 to 3 mins. I even tried stopping Apache and made sure no one is connecting to Mysql.</p>\n"},{"tags":["performance","sql-server-2008","stored-procedures","jdbc","clr"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":74,"score":0,"question_id":12869062,"title":"SQL Server 2008 x64 Express Batch Insert Throughput Performance to CLR 'Instead Of Insert' Trigger","body":"<p>I have a simple NO-OP CLR Trigger being fired on a six field table with schema:</p>\n\n<pre><code> datetime (PK)\n char (PK)\n int (PK, FK)\n varchar(1)\n tinyint\n real\n</code></pre>\n\n<p>which should be a total of 19 bytes / row inserted.</p>\n\n<p>The FK in the above points to a table with schema:</p>\n\n<pre><code> int (PK)\n nchar(10)\n ncahr(10)\n nchar(10)\n nchar(10)\n</code></pre>\n\n<p>I am performing batch inserts using a prepared statement every 1000 rows. I am getting for 150K rows inserted an execution time of about 11s. This corresponds to a throughput of ~.24MB/s (150,000*19B / 11s). Why so slow? I am not even talking to the disk?</p>\n\n<p>I know that my client side code doing the sending is much faster. If I comment out the:</p>\n\n<pre><code> executeBatch()\n</code></pre>\n\n<p>method in the client I get upwards of 5MB/s. What else is going on with the JDBC/ODBC connection and processing in SQL Server to make things so slow?</p>\n\n<p>Is this really the throughput one can expect from SQL Server and an JDBC/ODBC connection in this case?</p>\n\n<p>---EDIT---</p>\n\n<p>The actual CLR code is very simple, literally an empty method:</p>\n\n<pre><code> public class Triggers\n {\n     [SqlTrigger(Name = \"InsertHook\", Target = ConfigConstants.TABLE_NAME, Event =   \"INSTEAD OF INSERT\")]\n     public static void InsertHook()\n     {\n       //empty\n     }\n }\n</code></pre>\n\n<blockquote>\n  <p>No-op in T-SQL Stored Procedure Total Measured Time was:3475297036ns\n  or 3.475297036sec INSERT COUNT:150000\n  0.783276657MB/s\n  43165.47 Rows/s</p>\n  \n  <p>INSERTING NORMAL into SQL Server Total Measured Time was:4884190118ns\n  or 4.884190118sec INSERT COUNT:150000 MB Sent:2.7179718017578125\n  0.556MB/s\n  30712.53 Rows/s</p>\n</blockquote>\n\n<p>Sadly, I do not have control over the schemas I am working with here. I guess I am still shocked that even with one FK constraint that this is the best SQL Server can do?</p>\n\n<p>In summary, if I had to rank the performance. It would be:</p>\n\n<blockquote>\n  <p>1) T-SQL NO-OP Instead of Trigger (Fastest) - 0.78MB/s  </p>\n  \n  <p>2) Inserting Normal - 0.56MB/s </p>\n  \n  <p>3) CLR NO-OP Stored Procedure Instead of Trigger - .24MB/s</p>\n</blockquote>\n"},{"tags":["java","jvm","performance","jvm-arguments"],"answer_count":6,"favorite_count":13,"up_vote_count":15,"down_vote_count":0,"view_count":13876,"score":15,"question_id":564039,"title":"JVM performance tuning for large applications","body":"<p>The default JVM parameters are not optimal for running large applications. Any insights from people who have tuned it on a real application would be helpful. We are running the application on a 32-bit windows machine, where the client JVM is used <a href=\"http://java.sun.com/docs/hotspot/gc5.0/ergo5.html#0.0.%20Garbage%20collector,%20heap,%20and%20runtime%20compiler|outline\" rel=\"nofollow\">by default</a>. We have added -server and changed the NewRatio to 1:3 (A larger young generation).</p>\n\n<p>Any other parameters/tuning which you have tried and found useful?</p>\n\n<p>[Update] The specific type of application I'm talking about is a server application that are rarely shutdown, taking at least -Xmx1024m. Also assume that the application is profiled already. I'm looking for general guidelines in terms of <strong>JVM performance</strong> only.</p>\n"},{"tags":["sql","sql-server","performance","database-design","foreign-keys"],"answer_count":9,"favorite_count":3,"up_vote_count":5,"down_vote_count":0,"view_count":7657,"score":5,"question_id":599159,"title":"Should I use foreign keys?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/83147/whats-wrong-with-foreign-keys\">What’s wrong with foreign keys?</a>  </p>\n</blockquote>\n\n\n\n<p>I use MS Sql Server with a large database about 4 GB data.</p>\n\n<p>I search around the web why I should use foreign keys. \nby now I only indexed the keys used to join tables. \nPerformance is all fine, dataintegrety is no problem.</p>\n\n<p>Should I use foreign keys? Will I get even more performance with foreign keys?</p>\n"},{"tags":["linux","performance","x86","x86-64","linux-x32-abi"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":84,"score":4,"question_id":12903049,"title":"What <4GB workloads would have worse performance in the Linux x32 ABI than x64?","body":"<p>There is a relatively <a href=\"http://en.wikipedia.org/wiki/X32_ABI\" rel=\"nofollow\">new Linux ABI referred to as x32</a>, where the x86-64 processor is run in 32-bit mode, so pointers are still only 32-bits, but the 64-bit architecture specific registers are still used. So you're still limited to 4GB max memory use as in normal 32-bit, but your pointers use up less cache space than they do in 64-bit, you can do 64-bit arithmetic efficiently, and you get access to more registers (16) than you would in vanilla 32-bit (8).</p>\n\n<p>Assuming you have a workload the fits nicely within 4GB, is there any way the performance of x32 could be worse than on x86-64?</p>\n\n<p>It seems to me that if you don't need the extra memory space nothing is lost -- you should always get the same perf (when you already fit in cache) or better (when the pointer space savings lets you fit more in cache). But it wouldn't surprise me if there are paging/TLB/etc. details that I don't know about.</p>\n"},{"tags":[".net","vb.net","performance","events"],"answer_count":4,"favorite_count":0,"up_vote_count":6,"down_vote_count":0,"view_count":1868,"score":6,"question_id":250494,"title":"VB.NET: Are events raised even if there are no event handlers?","body":"<p>I have a class that downloads, examines and saves some large XML files. Sometimes I want the UI to tell me what's going on, but sometimes I will use the class and ignore the events. So I have placed lines of code like this in a dozen places:</p>\n\n<pre><code>RaiseEvent Report(\"Sending request: \" &amp; queryString)\n\nRaiseEvent Report(\"Saving file: \" &amp; fileName)\n\nRaiseEvent Report(\"Finished\")\n</code></pre>\n\n<p>My question is this - will these events slow down my code if nothing is listening for them? Will they even fire?</p>\n"},{"tags":["performance","algorithm","time","complexity"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":120,"score":0,"question_id":12877674,"title":"How to find time complexity of an algorithm when given number of iterations and total time?","body":"<p>I understand how to find the time complexity of an algorithm when I've been presented with an algorithm, but I can't seem to get my head around how to work it out when I've been given the number of times the algorithm is executed, and the time taken.</p>\n\n<p>I can sometimes get it, when it's obvious things like O(n), O(n) or O(n^2) but take this question for example:</p>\n\n<p>An algorithm runs a given input of size n.\nIf n is 4096 the run time is 512 milliseconds.\nIf n is 16384 the run time is 1024 milliseconds.\nIf n is 36864 the run time is 1536 milliseconds.</p>\n\n<p>What is the time complexity?</p>\n\n<p>I see that as n * 2, t * 1.5, but I'm not quite sure how to work it out.</p>\n\n<p>Thank you for your help :)</p>\n"},{"tags":["javascript","jquery","performance","image","firebug"],"answer_count":6,"favorite_count":3,"up_vote_count":3,"down_vote_count":0,"view_count":107,"score":3,"question_id":12825845,"title":"How to trace slow JS or JQuery code","body":"<p>I created a web page for viewing images. This page has some other code that gets included that I did not write. The page loads 40 small images upon load. Then the user will scroll down and additional pages of 40 images can be loaded via ajax. Once I get to  15-20 pages, I notice the page begins to slow significantly. I check app counters and it can go up to 100% cpu and memory can go over 3GB. Then I will inevitably get the modal that JQuery is taking too long to execute, asking me if I want to stop executing the script. Now I realize that a page with up to 800 images is a big load, but the issue with JQuery suggests to me that some code may also be iterating over this larger and larger group of dom objects. It almost appears to get exponentially slower as I pass 15 pages or so. Once I get to 20 pages it becomes almost unusable.</p>\n\n<p>First of all, is it even possible to run a page efficiently, even with minimal JS, when you have this many images? Secondly, is there a recommended way to \"trace\" JS and see what kinds of functions are getting executed to help determine what is the most likely culprit? This is most important to me - is there a good way to do in Firebug?</p>\n\n<p>Thanks :)</p>\n\n<p>EDIT - I found my answer. I had some older code which was being used to replace images that failed to load with a generic image. This code was using Jquery's .each operator and thus was iterating over the entire page and each new ajax addition every time the page loaded. I am going to set a class for the images that need to be checked in CSS so that the ajax-loaded images are unaffected.</p>\n"},{"tags":["javascript","jquery","performance","css3"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":80,"score":1,"question_id":12902323,"title":"How do I optimize this CSS3/many images for performance?","body":"<p>Here is the <a href=\"http://pineapple.io/\" rel=\"nofollow\">link to site</a></p>\n\n<p>There's a few things going on here I'd love to tweak, I'm not quite sure what else I can do though.</p>\n\n<p>Thumbnails and favicons are pulled via external services, and I'd rather not setup reverse proxy to cache them. Normally, the images are supposed to fade in smoothly when they load (in a sort of 'random' order, it gives it a nice effect).</p>\n\n<p>Youll notice when the page loads, it seems to be a bit choppy though, I think because the twitter script loading at the same  time as all the thumbnails fadein.</p>\n\n<p>I have tried looking at several performance tools but I'm at a bit of a loss as to how I can improve this.</p>\n\n<p>The only thing I can think of would be to maybe have twitter offset by a second or two.</p>\n"},{"tags":["java","performance","garbage-collection","jvm","tuning"],"answer_count":5,"favorite_count":4,"up_vote_count":4,"down_vote_count":0,"view_count":1670,"score":4,"question_id":9792590,"title":"GC Tuning - preventing a Full GC","body":"<p>I'm trying to avoid the Full GC (from gc.log sample below)\nrunning a Grails application in Tomcat in production.\nAny suggestions on how to better configure the GC?</p>\n\n<p><strong>14359.317: [Full GC 14359.317: [CMS: 3453285K->3099828K(4194304K), 13.1778420 secs] 4506618K->3099828K(6081792K), [CMS Perm : 261951K->181304K(264372K)] icms_dc=0 , 13.1786310 secs] [Times: user=13.15 sys=0.04, real=13.18 secs]</strong> </p>\n\n<p><strong>My VM params are as follow:</strong><br>\n-Xms=6G<br>\n-Xmx=6G<br>\n-XX:MaxPermSize=1G <br>\n-XX:NewSize=2G <br>\n-XX:MaxTenuringThreshold=8 <br>\n-XX:SurvivorRatio=7<br>\n-XX:+UseConcMarkSweepGC <br>\n-XX:+CMSClassUnloadingEnabled <br>\n-XX:+CMSPermGenSweepingEnabled <br>\n-XX:+CMSIncrementalMode<br> \n-XX:CMSInitiatingOccupancyFraction=60 <br>\n-XX:+UseCMSInitiatingOccupancyOnly <br>\n-XX:+HeapDumpOnOutOfMemoryError <br>\n-XX:+PrintGCDetails <br>\n-XX:+PrintGCTimeStamps <br>\n-XX:+PrintTenuringDistribution <br>\n-Dsun.reflect.inflationThreshold=0 <br></p>\n\n<pre>\n    14169.764: [GC 14169.764: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 8 (max 8)\n    - age   1:   15584312 bytes,   15584312 total\n    - age   2:   20053704 bytes,   35638016 total\n    - age   3:   13624872 bytes,   49262888 total\n    - age   4:   14469608 bytes,   63732496 total\n    - age   5:   10553288 bytes,   74285784 total\n    - age   6:   11797648 bytes,   86083432 total\n    - age   7:   12591328 bytes,   98674760 total\n    : 1826161K->130133K(1887488K), 0.1726640 secs] 5216326K->3537160K(6081792K) icms_dc=0 , 0.1733010 secs] [Times: user=0.66 sys=0.03, real=0.17 secs] \n    14218.712: [GC 14218.712: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 8 (max 8)\n    - age   1:   25898512 bytes,   25898512 total\n    - age   2:   10308160 bytes,   36206672 total\n    - age   3:   16927792 bytes,   53134464 total\n    - age   4:   13493608 bytes,   66628072 total\n    - age   5:   14301832 bytes,   80929904 total\n    - age   6:   10448408 bytes,   91378312 total\n    - age   7:   11724056 bytes,  103102368 total\n    - age   8:   12299528 bytes,  115401896 total\n    : 1807957K->147911K(1887488K), 0.1664510 secs] 5214984K->3554938K(6081792K) icms_dc=0 , 0.1671290 secs] [Times: user=0.61 sys=0.00, real=0.17 secs] \n    14251.429: [GC 14251.430: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 7 (max 8)\n    - age   1:   25749296 bytes,   25749296 total\n    - age   2:   20111888 bytes,   45861184 total\n    - age   3:    7580776 bytes,   53441960 total\n    - age   4:   16819072 bytes,   70261032 total\n    - age   5:   13209968 bytes,   83471000 total\n    - age   6:   14088856 bytes,   97559856 total\n    - age   7:   10371160 bytes,  107931016 total\n    - age   8:   11426712 bytes,  119357728 total\n    : 1825735K->155304K(1887488K), 0.1888880 secs] 5232762K->3574222K(6081792K) icms_dc=0 , 0.1895340 secs] [Times: user=0.74 sys=0.06, real=0.19 secs] \n    14291.342: [GC 14291.343: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 7 (max 8)\n    - age   1:   25786480 bytes,   25786480 total\n    - age   2:   21991848 bytes,   47778328 total\n    - age   3:   16650000 bytes,   64428328 total\n    - age   4:    7387368 bytes,   71815696 total\n    - age   5:   16777584 bytes,   88593280 total\n    - age   6:   13098856 bytes,  101692136 total\n    - age   7:   14029704 bytes,  115721840 total\n    : 1833128K->151603K(1887488K), 0.1941170 secs] 5252046K->3591384K(6081792K) icms_dc=0 , 0.1947390 secs] [Times: user=0.82 sys=0.04, real=0.20 secs] \n    14334.142: [GC 14334.143: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 6 (max 8)\n    - age   1:   31541800 bytes,   31541800 total\n    - age   2:   20826888 bytes,   52368688 total\n    - age   3:   19155264 bytes,   71523952 total\n    - age   4:   16422240 bytes,   87946192 total\n    - age   5:    7235616 bytes,   95181808 total\n    - age   6:   16549000 bytes,  111730808 total\n    - age   7:   13026064 bytes,  124756872 total\n    : 1829427K->167467K(1887488K), 0.1890190 secs] 5269208K->3620753K(6081792K) icms_dc=0 , 0.1896630 secs] [Times: user=0.80 sys=0.03, real=0.19 secs] \n    14359.317: [Full GC 14359.317: [CMS: 3453285K->3099828K(4194304K), 13.1778420 secs] 4506618K->3099828K(6081792K), [CMS Perm : 261951K->181304K(264372K)] icms_dc=0 , 13.1786310 secs] [Times: user=13.15 sys=0.04, real=13.18 secs]\n    14373.287: [GC [1 CMS-initial-mark: 3099828K(4194304K)] 3100094K(6081792K), 0.0107380 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \n    14373.298: [CMS-concurrent-mark-start]\n    14472.579: [GC 14472.579: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 8 (max 8)\n    - age   1:   42849392 bytes,   42849392 total\n    : 1677824K->86719K(1887488K), 0.1056680 secs] 4777652K->3186547K(6081792K) icms_dc=0 , 0.1063280 secs] [Times: user=0.61 sys=0.00, real=0.11 secs] \n    14506.980: [GC 14506.980: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 8 (max 8)\n    - age   1:   42002904 bytes,   42002904 total\n    - age   2:   35733928 bytes,   77736832 total\n    : 1764543K->96136K(1887488K), 0.0982790 secs] 4864371K->3195964K(6081792K) icms_dc=0 , 0.0988960 secs] [Times: user=0.53 sys=0.01, real=0.10 secs] \n    14544.285: [GC 14544.286: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 8 (max 8)\n    - age   1:   26159736 bytes,   26159736 total\n    - age   2:   37842840 bytes,   64002576 total\n    - age   3:   33192784 bytes,   97195360 total\n    : 1773960K->130799K(1887488K), 0.1208590 secs] 4873788K->3230628K(6081792K) icms_dc=0 , 0.1215900 secs] [Times: user=0.59 sys=0.02, real=0.13 secs] \n    14589.266: [GC 14589.266: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 4 (max 8)\n    - age   1:   28010360 bytes,   28010360 total\n    - age   2:   21136704 bytes,   49147064 total\n    - age   3:   35081376 bytes,   84228440 total\n    - age   4:   32468056 bytes,  116696496 total\n    : 1808623K->148284K(1887488K), 0.1423150 secs] 4908452K->3248112K(6081792K) icms_dc=0 , 0.1429440 secs] [Times: user=0.70 sys=0.02, real=0.14 secs] \n    14630.947: [GC 14630.947: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 8 (max 8)\n    - age   1:   28248240 bytes,   28248240 total\n    - age   2:   20712320 bytes,   48960560 total\n    - age   3:   18217168 bytes,   67177728 total\n    - age   4:   34834832 bytes,  102012560 total\n    : 1826108K->140347K(1887488K), 0.1784680 secs] 4925936K->3275469K(6081792K) icms_dc=0 , 0.1790920 secs] [Times: user=0.98 sys=0.03, real=0.18 secs] \n    14664.779: [GC 14664.779: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 5 (max 8)\n    - age   1:   25841000 bytes,   25841000 total\n    - age   2:   22264960 bytes,   48105960 total\n    - age   3:   17730104 bytes,   65836064 total\n    - age   4:   17988048 bytes,   83824112 total\n    - age   5:   34739384 bytes,  118563496 total\n    : 1818171K->147603K(1887488K), 0.1714160 secs] 4953293K->3282725K(6081792K) icms_dc=0 , 0.1720530 secs] [Times: user=0.82 sys=0.11, real=0.17 secs] \n    14702.488: [GC 14702.489: [ParNew\n    Desired survivor size 107347968 bytes, new threshold 8 (max 8)\n    - age   1:   26887368 bytes,   26887368 total\n    - age   2:   21403352 bytes,   48290720 total\n    - age   3:   18732224 bytes,   67022944 total\n    - age   4:   17640576 bytes,   84663520 total\n    - age   5:   17942952 bytes,  102606472 total\n    : 1825427K->142695K(1887488K), 0.2118320 secs] 4960549K->3312168K(6081792K) icms_dc=0 , 0.2124630 secs] [Times: user=1.13 sys=0.14, real=0.21 secs] \n</pre>\n\n<p>The strategy I was aiming at:\nI want to limit to the minimum what gets Tenured, I'm serving requests and expect that beyond a certain amount of shared objects, every other objects are useful only to the request at hand. Therefore by using a big NewSize and an increased TenuringThreshold and was hoping to have none of these single serving objects stick around.</p>\n\n<p>The following are there to support my strategy:<br>\n-Xms=6G<br>\n-Xmx=6G<br>\n-XX:NewSize=2G // big space so that ParNew doesn't occur to often and let time for objects to expire<br>\n-XX:MaxTenuringThreshold=8 // to limit the tenuring some more<br>\n-XX:SurvivorRatio=7 // based on examples\n-XX:CMSInitiatingOccupancyFraction=60<br> // to prevent a Full GC caused by promotion allocation failed<br>\n-XX:+UseCMSInitiatingOccupancyOnly<br> // to go with the one above based on example</p>\n\n<p>MaxPermSize=1G and \"-Dsun.reflect.inflationThreshold=0\" are related to another issue I'd rather keep separated.</p>\n\n<p>\"-XX:+CMSClassUnloadingEnabled\" and \"-XX:+CMSPermGenSweepingEnabled\" are there because of grails which rely heavily and extra classes for closures and reflexion</p>\n\n<p>-XX:+CMSIncrementalMode is an experiment which hasn't yield much success</p>\n"},{"tags":["java","performance","garbage-collection","jvm","permanent-generation"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":436,"score":2,"question_id":6260771,"title":"JVM performance tuning: young copy vs old generation gc","body":"<p>Hi: I have a multi thread Java application. There are many temporary objects.<br>\n<code>-XX:MaxTenuringThreshold=1</code>, we put above parameter when starting JVM. This means all the objects would be survive once during gc, then it would be promoted to old generation. Could we put this <code>-XX:MaxTenuringThreshold=10</code> for example, so that object would be promoted to old JVM old generation after 10 times gc. But will that cause unnecessary copy operation during young gc (since objects are copied 'from 'eden' to 'from', from 'from' to 'to', 'from','to' are two survivor buffer)?</p>\n\n<p>The questions might also mean if a) there are multiple times copy in young generation,less old generation gc, b) long old generation garbage collection but few young generation copy, which one is better for good performance?</p>\n"},{"tags":["jvm","performance"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":211,"score":1,"question_id":2391135,"title":"Resources for JVM Tuning","body":"<p>Anybody knows a good book or two (or resources) for JVM Tuning?</p>\n\n<p>I am struggling to find any. I stumbled upon Apress Java EE 5 Performance Management and Optimization, but there was not much in there.</p>\n"},{"tags":["performance","multithreading","file","file-io","parallel-processing"],"answer_count":6,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":438,"score":1,"question_id":993038,"title":"Directory walker on modern operating systems slower when it's multi-threaded?","body":"<p>Once I had the theory that on modern operating systems multithreaded \nread access on the HDD should perform better.</p>\n\n<p>I thought that:<br />\n<em>the operating system queues all read requests,\nand rearranges them in such a way, that it could read from the HDD more\nsequentially. The more requests it would get, the better it could rearrange them\nto optimize the read sequence.</em><br />\nI was very sure that I read it somewhere few times.</p>\n\n<p>But I did some benchmarking, and had to find out, that multithreaded\nread access mostly perform much worst, and never performs better.</p>\n\n<p>I had the experience under Windows and Linux. I benchmarked pure\nsearching of files using the operating system's tools, and also\nhad written own little benchmarks.</p>\n\n<p>Am I missing something?<br />\nCan someone explain to me the secrets of this topic?<br />\nThank you!</p>\n"},{"tags":["java","performance","delay","xuggler"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":80,"score":1,"question_id":12785995,"title":"Xuggler live streaming delay and high cpu usage","body":"<p>I'm currently using Xuggler to receive the video stream of an AR.Drone. The stream format is H.264 720p. I can decode and display the video using the following code, but the processor usage is very high (100% on dual-core 2ghz) and there is a huge delay in the stream that keeps increasing.</p>\n\n<pre><code>        final IMediaReader reader = ToolFactory.makeReader(\"http://192.168.1.1:5555\");\n        reader.setBufferedImageTypeToGenerate(BufferedImage.TYPE_3BYTE_BGR);\n\n        MediaListenerAdapter adapter = new MediaListenerAdapter()\n        {\n            public void onVideoPicture(IVideoPictureEvent e)\n            {\n                currentframe = e.getImage();\n                //Draw frame\n            }\n\n            public void onOpenCoder(IOpenCoderEvent e) {\n                videostreamopened = true;\n            }\n        };\n\n        reader.addListener(adapter);\n\n        while (!stop) {\n            try {\n                reader.readPacket();\n            } catch(RuntimeException re) {\n                // Errors happen relatively often\n            }\n        }\n</code></pre>\n\n<p>Using the Xuggler sample application resolves none of the problems, so I think my approach is correct. Also, when I decrease the resolution to 360p the stream is real-time and everything works OK. Does anybody know if this performance issues are normal or what I have to do to avoid this? I am <em>very</em> new to this, and I have not been able to find information, so does anybody have suggestions? </p>\n\n<p>By the way, I tried changing the bitrate without success. Calling <code>reader.getContainer().getStream(0).getStreamCoder().setBitRate(bitrate);</code> seems to be ignored...</p>\n\n<p>Thanks in advance!</p>\n\n<p><strong>UPDATE:</strong>\nI get many of these errors:</p>\n\n<pre><code>9593 [Thread-7] ERROR org.ffmpeg - [h264 @ 0x7f12d40e53c0] mmco: unref short failure\n39593 [Thread-7] ERROR org.ffmpeg - [h264 @ 0x7f12d40e53c0] number of reference frames (0+2) exceeds max (1; probably corrupt input), discarding one\n39593 [Thread-15] ERROR org.ffmpeg - [h264 @ 0x7f12d40e53c0] reference overflow\n39593 [Thread-15] ERROR org.ffmpeg - [h264 @ 0x7f12d40e53c0] decode_slice_header error\n</code></pre>\n\n<p><strong>UPDATE 2:</strong> Changing the codec solves the above errors, but performance is still poor.</p>\n"},{"tags":["ruby-on-rails","ruby","performance","testing"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":39,"score":0,"question_id":12902064,"title":"How to measure file upload performance in Rails 2?","body":"<p>We have a rails 2 application with an uploader which currently has not bee performing to expectations in terms of speed. We would like to try a few things to speed it up.</p>\n\n<p>The first task is to define what \"slow\" means and try to measure that it terms of time it takes for an upload. I looked into new relic and now looking into munin, but from what I have seen I am not sure these are the right tools for the job.</p>\n\n<p>How would you recommend we should approach this problem?</p>\n"},{"tags":["performance","algorithm","math","interview"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":100,"score":1,"question_id":12867219,"title":"Given a point inside a rectangle, determine the side that's closest to the point","body":"<p>I was curious if there was an elegant way to do this, aside from just calculating the distance from the point to each side and finding the minimum.</p>\n\n<p>Some things I've thought about:\nIf it's a square, we can just draw the diagonals and figure out which of the 4 regions the point falls on. Each of these region corresponds to a closest side.</p>\n\n<p>Perhaps we can divide up the rectangle into squares and go somewhere from there?</p>\n\n<p>It seems an alternative solution would be too complicated and not worth looking for.</p>\n"},{"tags":["java","performance","jasper-reports","itext"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":78,"score":0,"question_id":12900968,"title":"What is the expected performance of JasperReports 4.7/iText 2.7.1","body":"<p>I know after reading the title many will think \"well it depends...\" but that's exactly my question: on what does it depend?</p>\n\n<p>In my particular case, i'm generating a ~84Mb file which contains about 1000 pages of a single table(band). That is ~24,000 rows and 18 columns, and a summary element. Using a compiled (.jasper) file.</p>\n\n<p>When i run it on a dev pc (2.5 Ghz CPU, 3 Gb RAM) it takes about 90 seconds to generate.\nI profiled it using JVisualVM and the output was (cumulative times):</p>\n\n<pre><code>* JRPdfExporter.exportReport() 90,389 ms\n* JRPdfExporter.exportElements() 85,559 ms\n* JRPdfExporter.exportText() 50,338 ms\n* SimplePdfTextRenderer.render() 43,520 ms\n* SimplePdfTextRenderer.getPhrase() 22,278 ms\n* JRDataUtils.getLocale() 14,094 ms\n</code></pre>\n\n<p>I think this getLocale() method is taking way too long (15% of total time) considering i set it to a constant when passing it to the JasperExportManager.exportReportToPdfFile() method like this:</p>\n\n<pre><code>params.put(JRParameter.REPORT_LOCALE, MX_LOCALE);//MX_LOCALE is a Locale constant\n</code></pre>\n\n<p>Am i doing this wrong?</p>\n\n<p>I'm using a custom JRDataSource implementation, which esentially iterates over a List. There's nothing terribly complicated to it, but I can post it if it helps.</p>\n\n<p>I hope i'm not asking too many questions into one, or including way too much information, but id like to know if this is a reasonable time for this kind of report, or there´s anything i can do to make it faster.  Thanks in advance</p>\n"},{"tags":["android","performance","sqlite","ipc"],"answer_count":1,"favorite_count":6,"up_vote_count":12,"down_vote_count":0,"view_count":836,"score":12,"question_id":4426616,"title":"Android ContentProvider Performance","body":"<p>I'm curious if anyone has done any performance testing on querying a <code>ContentProvider</code> via <code>ContentResolver</code> vs querying a <code>SQLiteDatabase</code> object in the same process. I'm guessing that a <code>ContentResolver</code> query passes back a Cursor that communicates with the database through a Binder (Android IPC). This means that if I read the contents of 100 records through the <code>Cursor</code> that would result 100 Binder method calls. Are my guesses correct and if so would that be significantly slower than accessing the database in the same process?</p>\n"},{"tags":["java","regex","performance","algorithm","string"],"answer_count":5,"favorite_count":0,"up_vote_count":11,"down_vote_count":2,"view_count":1034,"score":9,"question_id":2667015,"title":"Is regex too slow? Real life examples where simple non-regex alternative is better","body":"<p>I've seen people here made comments like \"regex is too slow!\", or \"why would you do something so simple using regex!\" (and then present a 10+ lines alternative instead), etc.</p>\n\n<p>I haven't really used regex in industrial setting, so I'm curious if there are applications where regex is demonstratably just too slow, <strong>AND</strong> where a <em>simple</em> non-regex alternative exists that performs significantly (maybe even asymptotically!) better.</p>\n\n<p>Obviously many highly-specialized string manipulations with sophisticated string algorithms will outperform regex easily, but I'm talking about cases where a simple solution exists and <em>significantly</em> outperforms regex.</p>\n\n<p>What counts as simple is subjective, of course, but I think a reasonable standard is that if it uses only <code>String</code>, <code>StringBuilder</code>, etc, then it's probably simple.</p>\n\n<hr>\n\n<p><em>Note</em>: I would very much appreciate answers that demonstrate the following:</p>\n\n<ol>\n<li>a beginner-level regex solution to a non-toy real-life problem that performs horribly</li>\n<li>the simple non-regex solution</li>\n<li>the expert-level regex rewrite that performs comparably</li>\n</ol>\n"},{"tags":["performance","magento","indexing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":67,"score":0,"question_id":12899701,"title":"Magento - slow url rewrite reindex","body":"<p>I'm thinking about improving reindexing of url rewrites in magento, we have 130k simple and configurable products. And reindexing consumes a lot of memory, basicaly I'm not able to finish this job in comammnd line because of memory limit.</p>\n\n<p>So I started with looking for some solution which could speedup whole process. Whole reindex is happening in <code>Mage_Catalog_Model_Url::_refreshProductRewrite</code> because system iterate over all products and categories and stores. And for each iteration it possibly does 1 or 2 <code>insertOnDuplicate</code> actions.</p>\n\n<p>I'm thinking about storing such queries somewhere and merging them into one or more bigger queries which would be much more faster (I guess).</p>\n\n<p>Slow bit for one comibation (product, category, store) looks like</p>\n\n<pre><code>    $this-&gt;getResource()-&gt;saveRewrite($rewriteData, $this-&gt;_rewrite);\n\n    if ($this-&gt;getShouldSaveRewritesHistory($category-&gt;getStoreId())) {\n        $this-&gt;_saveRewriteHistory($rewriteData, $this-&gt;_rewrite);\n    }\n</code></pre>\n\n<p>Have anyone better idea how to improve this?</p>\n"},{"tags":["c","performance","visual-c++","c99"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":97,"score":0,"question_id":12899048,"title":"Efficiency issues when using C99 and C11.","body":"<p>The other day I was converting a program written with C99 standard into C11. Basically the motive was to use the code with MSVC but It was written in Linux and was mostly compiled with default GCC behaviour. During the code conversion, I found out that you can not decalre variables of a function after any statement i.e. you must declare them at the top of the function. </p>\n\n<p>But my question is that wouldn't it be against the efficient programming rule that variables should be declared near their use so that it maximizes the cache hits? For example, In a large function of say 200 LOC, I want to use some big static look up array at nearly the end of the function. Wouldn't declaring and initializing it just before the usage cause more cache hits?  or am I simple missing some basic point of C11 C language standard?</p>\n"},{"tags":["jquery","performance","jquery-deferred"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":73,"score":0,"question_id":12899442,"title":"jQuery performance: $(document).ready(...), $(window).load(...) or during page load?","body":"<p>I'm referring to <a href=\"http://www.artzstudio.com/2009/04/jquery-performance-rules/#defer-to-window-load\" rel=\"nofollow\">this article discussing performance solutions for jQuery</a></p>\n\n<p>It mentions:</p>\n\n<blockquote>\n  <p>There is a temptation among jQuery developers to hook everything into\n  the $(document).ready pseudo event. After all, it is used in most\n  examples you will find. Although $(document).ready is incredibly\n  useful, it occurs during page render while objects are still\n  downloading. If you notice your page stalling while loading, all those\n  $(document).ready functions could be the reason why. You can reduce\n  CPU utilization during the page load by binding your jQuery functions\n  to the $(window).load event, which occurs after all objects called by\n  the HTML (including  content) have downloaded.</p>\n</blockquote>\n\n<pre><code> $(window).load(function(){     // jQuery functions to initialize after\n the page has loaded. });\n</code></pre>\n\n<blockquote>\n  <p>Superfluous functionality such as drag and drop, binding visual\n  effects and animations, pre-fetching hidden images, etc., are all good\n  candidates for this technique.</p>\n</blockquote>\n\n<p>Some things have to be inline imho - otherwise the flicker of unstyled content is too strong. For everything else I'm trying to use lazy loading:\nfor example I'm using deferred event listeners on: auto select on focus for inputs, jquery tooltips, some drop down buttons, datepickers. I'm adding as well default buttons the user actually clicks when he presses enter.</p>\n\n<p>First of all: is this a good idea or do everything inline?\nSecondly: if this is a good idea - is using window.load() the way to go? When would you use which method?</p>\n"},{"tags":["ios","performance","uiscrollview"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":68,"score":0,"question_id":12898259,"title":"UIScrollView Performance Tips?","body":"<p>I have a UIScrollView that is animating a lot of UIViews (probably too many).  Also, these UIViews represent \"pages\", and sometimes multiple pages are stacked on top of each other, resulting in a \"pile\" of pages (setup by adding subviews to a given view).</p>\n\n<p>I know that scrolling an excessive number of UIViews can have poor performance, but I was wondering if anybody had some general tips for me to improve performance?  </p>\n\n<p>For now, doing drawing manually in drawRect is not something I would like to consider because it would mess up various \"page pile\" animations.  I will keep it in mind for a last resort, but I'd definitely like to avoid it if possible.</p>\n\n<p>Update:\nThe cause of the performance hit has been determined and is two fold: I'm using antialiasing and shadows on all my UIViews.  When I toggle them both off, the performance issues are resolved!  However, I obviously don't want to just toggle them off :)</p>\n\n<p>I'm creating my shadows like so:</p>\n\n<pre><code>self.imageView.layer.opaque = YES;\n        self.imageView.layer.masksToBounds = NO;\n        self.imageView.layer.shadowOffset = CGSizeMake(-4, 0);\n        self.imageView.layer.shadowRadius = 2.5;\n        self.imageView.layer.shadowOpacity = 0.15;\n        self.imageView.layer.shadowPath = [UIBezierPath bezierPathWithRect:CGRectMake(\n                                                                                      self.bounds.origin.x,\n                                                                                      self.bounds.origin.y,\n                                                                                      self.bounds.size.width + 8,\n                                                                                      self.bounds.size.height + 2)].CGPath;\n</code></pre>\n\n<p>Any tips to improve the performance?</p>\n\n<p>As far as antialiasing, it is almost a necessity.  The problem is that those \"offset pages\" are slightly rotated AND my pages have a 1 pixel border.  Slightly rotated with a 1 pixel border without antialiasing looks awful.  I am simply enabling antialiasing in the .plist by setting \"Renders with Edge Antialiasing\" to YES.</p>\n\n<p>Any suggestions on how to improve my shadow/antialiasing performance would be appreciated.</p>\n"},{"tags":["java","android","performance","class"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":90,"score":2,"question_id":12897241,"title":"Android: Is Java less effective with many classes","body":"<p>I am writing a game for android, and I am worry about performance and memory.\nIs Java less effective with many classes?</p>\n"},{"tags":["php","mysql","performance","search","tags"],"answer_count":2,"favorite_count":2,"up_vote_count":5,"down_vote_count":0,"view_count":98,"score":5,"question_id":12897817,"title":"PHP, MySQL, Efficient tag-driven search algorithm","body":"<p>I'm currenlty building a webshop. This shop allows users to filter products by <code>category</code>, and a couple optional, additional filters such as <code>brand</code>, <code>color</code>, etc.</p>\n\n<p>At the moment, various properties are stored in different places, but I'd like to switch to a tag-based system. Ideally, my database should store tags with the following data:</p>\n\n<ul>\n<li><code>product_id</code> </li>\n<li><code>tag_url_alias</code> (unique)</li>\n<li><code>tag_type</code> (unique) (category, product_brand, product_color, etc.)</li>\n<li><code>tag_value</code> (not unique)</li>\n</ul>\n\n<h1>First objective</h1>\n\n<p>I would like to search for <code>product_id</code>'s that are associated with anywhere <strong><em>between 1-5 particular tags</em></strong>. The tags are extracted from a SEO-friendly url. So I will be retrieving a unique strings (the <code>tag_url_alias</code>) for each tag, but I won't know the <code>tag_type</code>.\nThe search will be an <strong><em>intersection</em></strong>, so my search should return the <code>product_id</code>'s that match <strong><em>all</em></strong> of the provided <code>tags</code>.</p>\n\n<h1>Second objective</h1>\n\n<p>Besides displaying the products that match the current filter, I would also like to display the product-count for other categories and filters which the user might supply.</p>\n\n<p>For instance, my current search is for products that match the tags: </p>\n\n<pre><code>Shoe + Black + Adidas\n</code></pre>\n\n<p>Now, a visitor of the shop might be looking at the resulting products and wonder which black shoes other brands have to offer. So they might go to the \"brand\" filter, and choose any of the other listed brands. Lets say they have 2 different options (in practice, this will probably have many more), resulting in the following searches:</p>\n\n<pre><code>Shoe + Black + Nike &gt; 103 results\nShoe + Black + K-swiss &gt; 0 results\n</code></pre>\n\n<p>In this case, if they see the brand \"K-swiss\" listed as an available choise in their filter, their search will return 0 results.</p>\n\n<p>This is obviously rather disappointing to the user... I'd much rather know that switching the \"brand\" from \"adidas\" to \"k-swiss\" will 0 results, and simply remove the entire option from the filter.</p>\n\n<p>Same thing goes for categories, colors, etc.</p>\n\n<p>In practice this would mean a single page view would not only return the filtered product list described in my primary objective, but potentially hundreds of similar yet different lists. One for each filter value that could replace another filter value, or be added to the existing filter values.</p>\n\n<h1>Capacity</h1>\n\n<p>I suspect my database will eventually contain:</p>\n\n<blockquote>\n  <p>between 250 and 1.000 unique tags</p>\n</blockquote>\n\n<p>And it will contain:</p>\n\n<blockquote>\n  <p>between 10.000 and 100.000 unique products </p>\n</blockquote>\n\n<h1>Current Ideas</h1>\n\n<p>I did some Google searches and found the following article: <a href=\"http://www.pui.ch/phred/archives/2005/06/tagsystems-performance-tests.html\">http://www.pui.ch/phred/archives/2005/06/tagsystems-performance-tests.html</a></p>\n\n<p>Judging by that article, running hundreds of queries to achieve the 2nd objective, is going to be a painfully slow route. The \"toxy\" example might work for my needs and it might be acceptable for my First objective, but it would be unacceptably slow for the Second objective.</p>\n\n<p>I was thinking I might run individual queries that match 1 <code>tag</code> to it's associated <code>product_id</code>'s, cache those queries, and then calculate intersections on the results. But, do I calculate these intersections in MySQL? or in PHP? If I use MySQL, is there a particular way I should cache these individual queries, or is supplying the right indexes all I need?</p>\n\n<p>I would imagine it's also quite possible to maybe even cache the intersections between two of these <code>tag</code>/<code>product_id</code> sets. The amount of intersections would be limited by the fact that a <code>tag_type</code> can have only one particular value, but I'm not sure how to efficiently manage this type of caching. Again, I don't know if I should do this in MySQL or in PHP. And if I do this in MySQL, what would be the best way to store and combine this type of cached results?</p>\n"},{"tags":["performance","matlab","optimization","if-statement"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":1,"view_count":103,"score":2,"question_id":12895609,"title":"Optimization a recurring matlab code","body":"<p>I'm optimizing a model which takes some weather data and then converts the clouds into polygons, so that they can be further utilized. <br />\nThe code is working, but its kinds slow. By running the profiler I was able to found out the following lines are being called <code>106360430</code> times and takes about 50 secs to process. <br />\nIs there a way I can make these lines more efficient?</p>\n\n<pre><code>function [oddNodes] = pointInPolygon (point,thePolygon)\n% determine if a point is in the polygon (faster than matlab \"inpolygon\"command\n\npolyPoints=size(thePolygon,1);    % number of polygon points\noddNodes = false;\n\nj=polyPoints;\nx=point(1); y=point(2);\n\nfor i=1:polyPoints\nif (thePolygon(i,2)&lt;y &amp;&amp; thePolygon(j,2)&gt;=y ||  thePolygon(j,2)&lt;y &amp;&amp; thePolygon(i,2)&gt;=y)\nif (thePolygon(i,1)+(y-thePolygon(i,2))/(thePolygon(j,2)-thePolygon(i,2))*(thePolygon(j,1)-thePolygon(i,1))&lt;x)\noddNodes=~oddNodes;\nend\nend\nj=i; \nend\n</code></pre>\n"},{"tags":["asp.net-mvc-3","performance","debugging","profiling"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":42,"score":2,"question_id":12897336,"title":"ASP.NET MVC lost in finding botleneck","body":"<p>I have <code>ASP.NET MVC</code> app which accept file uploads and has result pooling using SignalR. The app hosted on Prod server with IIS7, 4 Gb Ram and two cores CPU. </p>\n\n<p>The app on Dev server works perfectly but when I host it on Prod server with about <code>50 000 users per day</code> the app become unrresponsible after five minutes of running. The web page request time increase dramatically and it takes about 30 seconds to load one page. I have tried to record all <code>MvcApplication.Application_BeginRequest</code> event call and got <code>9000</code> hits in <code>5 minutes</code>. Not sure is this acceptable number of hits or not for app like this.</p>\n\n<p>I have used ANTS Performance Profiler(not useful in Prod app profiling, slow and eats all memory) to profile code but profiler do not show any time delay issues in my code/MSSQL queries. </p>\n\n<p>Also I have tried to monitored CPU and RAM spike problems but I didn't find any. CPU percentage sometimes goes to 15% but never up and memory usage is normal.</p>\n\n<p>I suspect that there is something wrong with request or threads limits in ASP.NET/IIS7 but don't know how to profile it.  </p>\n\n<p>Could someone suggest any profiling solutions which could help in this situation? Tried to hunt the problem for two week already without any result :(</p>\n"},{"tags":["performance","oracle","jdbc"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":58,"score":1,"question_id":12896752,"title":"Oracle Perfomance - JDBC Oracle Thin driver: Use parameters or not?","body":"<p>I have two decades SQL experience, but not specifically with Oracle. An 'Oracle expert' assures me that building a SQL query without parameters (like this): </p>\n\n<pre><code>SELECT t.ID, t.Name, t.Address ... FROM Table1 t WHERE t.ID = 'someID' AND t.Name = 'someName'...\n</code></pre>\n\n<p>is at least as fast as using parameters (like this)</p>\n\n<pre><code>SELECT t.ID, t.Name, t.Address ... FROM Table1 t WHERE t.ID = ? AND t.Name = ?\n</code></pre>\n\n<p>The code is executed in a loop.</p>\n\n<p>In most other databases I have experience with, using parameters increases speed. It allows the database to cache the compiled plan that matches the SQL statement. Since the SQL does not change per invocation (although the parameters do) this improves performance. The database simply binds the parameters and continues.</p>\n\n<p>The 'Oracle expert' states that this is not necessary. But obviously, Oracle needs to 'parse out' the parameters, match the remaining string to a cached execution plan, then rebind the parameters as if they were passed along as parameters in the first place.  </p>\n\n<p>Do I have the correct mental picture here? Is there something 'magical' about Oracle that it really does not make a difference how we approach our parameter passing/SQL building strategy?</p>\n\n<p>Are there thoughts about Java / JDBC / Oracle thin driver that I am not aware of that I should be aware of here?</p>\n\n<p>I am looking to either reinforce my understanding or to expand my knowledge.</p>\n\n<p>(Security concerns aside please, I understand that building SQL strings allows for SQL injection attacks, I am looking for more direct ammunition to use against the experts opinion - if it exists).</p>\n\n<p>Other details: <code>Oracle 11gR2, Java 1.6</code></p>\n"},{"tags":[".net","vb.net","performance","assembly"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":122,"score":-1,"question_id":11195759,"title":"Use asm in VB.NET","body":"<p>I wanna call some ASM functions in VB.NET. How can I do it?\nIt's for performance purpose.</p>\n\n<p>I did not find anything on the web.</p>\n\n<p>Thx</p>\n"},{"tags":["asp.net","firefox","localhost","performance","loading"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":3642,"score":1,"question_id":2386299,"title":"Running sites on \"localhost\" is extremely slow","body":"<p>Having real trouble using my localhost to test sites. It runs extremely slowly! Sometimes it takes up to a minute to load a page. I'm using Firefox and the sites I'm testing run fine on other developers in my office's local machines / on the production server.</p>\n\n<p>I've gone through the normal things :-</p>\n\n<ol>\n<li>Disabled IPv6</li>\n<li>Not running in debug mode</li>\n<li>Put the site in the highest app pool (High Isolated) on IIS 6.</li>\n<li>Taking of firewalls etc.</li>\n</ol>\n\n<p>The problem only seems to occur when hitting pages which contain some form of .net code in the code-behind.</p>\n\n<p>Appreciate that this a little bit of a vague topic / stab in the dark but would appreciate any sort of advice - it's horrible waiting a minute each refresh to try out a change!</p>\n\n<p>Cheers, Sean.</p>\n"},{"tags":["python","performance","image","scroll","zoom"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":60,"score":0,"question_id":12895309,"title":"Python - Efficient way to display, scroll and zoom image / array with thousands of columns","body":"<p>I am looking for an efficient way to display, scroll and zoom into an image. I have an array with 256xn values that are calculated elsewhere (height x width). n is in the neighborhod of 15k to 50k. For starters lets assume that the window of the application is 256x1000 (height x with).</p>\n\n<p>The question now is: which library, algorithm or any other trick could I use to achieve the image to be displayable, scrollable and zoomable without the user noticing any lags in Python?</p>\n\n<p>I did some research but could not find anything that looked to promising (e.g. <a href=\"http://stackoverflow.com/questions/602557/image-viewer-standard-gui-controls-bottom-up-or-what/604916#604916\">this</a>). I'm sure I could pull off something, but was wondering if there might be an elegant way of doing so.</p>\n"},{"tags":["performance","redis","lru"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":52,"score":1,"question_id":12892396,"title":"Redis maxmemory-policy: performances of volatile-lru vs allkeys-lru","body":"<p>assuming all keys in a redis instance have an expire set, volatile-lru and allkeys-lru are similar. But is there a significative performance difference between the 2 when a key is removed?</p>\n\n<p>Bonus question:</p>\n\n<p>between 2 distinct instances configured with the allkeys-lru policy, having the same content and same configuration, except:</p>\n\n<ul>\n<li>Instance A has <em>all</em> its keys with an expire set (different values of expire)</li>\n<li>Instance B has <em>none</em> key with an expire set</li>\n</ul>\n\n<p>Aside the overhead of memory in instance A due to the expires bits, is there a performance difference between the 2 when a key is removed by the allkeys-lru algorithm?</p>\n\n<p>In both cases, I'm talking about instances of redis 2.4.x on linux 64 bits with maxmemory = 3Gb with 4-5000 keys when the maxmemory is reached (most of the keys are hashes).</p>\n\n<p>Thanks</p>\n"},{"tags":["c++","performance","optimization","gcc","compiler"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":106,"score":1,"question_id":12849170,"title":"Any areas where the GCC compiler is better than Intel's?","body":"<p>Are there any \"areas\" where the GCC C++ compiler is better than the Intel C++ compiler? </p>\n\n<p>I presumed optimization-wise Intel would win hands down, but wanted to double-check before assuming?</p>\n\n<p>It would also be interesting if there are any areas where both compilers are not particularly good/problems optimizing C++ code.</p>\n"},{"tags":["performance","algorithm"],"answer_count":17,"favorite_count":14,"up_vote_count":27,"down_vote_count":0,"view_count":16431,"score":27,"question_id":3947867,"title":"Find the least number of coins required that can make any change from 1 to 99 cents","body":"<p>Recently I challenged my co-worker to write an algorithm to solve this problem:</p>\n\n<blockquote>\n  <p>Find the least number of coins required that can make any change from 1 to 99 cents. The coins can only be pennies (1), nickels (5), dimes (10), and quarters (25), and you must be able to make every value from 1 to 99 (in 1-cent increments) using those coins.</p>\n</blockquote>\n\n<p>However, I realized that I don't actually know how to do this myself without examining every possible combination of coins. There has to be a better way of solving this problem, but I don't know what the generic name for this type of algorithm would be called, and I can't figure out a way to simplify it beyond looking at every solution.</p>\n\n<p>I was wondering if anybody could point me in the right direction, or offer up an algorithm that's more efficient.</p>\n"},{"tags":["multithreading","performance","optimization"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":75,"score":2,"question_id":12892941,"title":"CPU usage vs Number of threads","body":"<p>In general what is the relation between CPU usage and number of threads in a program.\nAssumptions:</p>\n\n<ul>\n<li>Multi-core CPU</li>\n<li>Threads do the exact same job (assume they fetch identical work items from a queue and process them)</li>\n</ul>\n"},{"tags":["javascript","performance","design-patterns","object","properties"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":38,"score":1,"question_id":12892849,"title":"Method that returns property VS property direct access in javascript","body":"<p>I have found that many javascript developers create methods that simply return a property like this :</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>function Obj (prop) {\n    this.prop = prop; // public\n}\nObj.prototype.getProp = function () {\n    return this.prop;\n};\n</code></pre>\n\n<p>While prop is public and can be accessed like this :</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>var a = obj.prop;\n</code></pre>\n\n<p>Moreover, I found that accessing an object property with a method is 121 times slower than accessing it directly (in Firefox)</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>var a, b,\n    obj = new Obj(1);\n\na = obj.prop;\n// ~6ns on Chrome\n// ~5ns on Firefox\n\nb = obj.getProp();\n// ~6ns on Chrome (no difference)\n// ~730ns on Firefox (122x slower...)\n</code></pre>\n\n<p>So my question is: should we always create methods that return properties or can we access properties directly? Is that antipattern?</p>\n"},{"tags":["android","performance","image","load"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":12686752,"title":"Android read file optimize performance","body":"<p>I have function to read image file az byte array.\nPerformance analisys gives to me interesting facts.\nI have check if file exists and then read as byte array.\nFile.exists spent 69.7% of time in function ?????\nOpen read and close spent only 30,3% of time.</p>\n\n<p>I cant't explaint to mysefl why?</p>\n\n<p>Is this depend on something or it is default behaviour?</p>\n\n<p>If this check is always slow - may be better approach is to open file without check for existing. And of cource use catch to solve case with missing file.</p>\n\n<p>UPDATE:\nFiles are stored in internal SD card. More than 20000 files.\nWhen test with 30 files percent is reduced to 23%.</p>\n"},{"tags":["performance","oracle","pagination"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":67,"score":1,"question_id":12881519,"title":"Oracle data paging optimization","body":"<pre><code>select * from (\n select t_tmp_a.*, rownum t_tmp_id from (\n select  t.*, i.counts  \n from table1 t, (select id, count(id) counts from table2 group by id) i\n where t.id=i.id and t.kindid in (0,1,3) order by t.id desc\n) t_tmp_a where rownum &lt;= 20) t_tmp_b where t_tmp_id &gt;= 11;\n</code></pre>\n\n<p>table1 and table2 have more then 2 million data per table, when execute this query need 18s , before this query execute we should calculation total count need about 7s, so it spends more than 25s， any idea to optimiza it?</p>\n"},{"tags":["performance","cross-platform","scientific-computing","generic-programming"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":45,"score":0,"question_id":12545460,"title":"Cross-platform development solutions for (mobile) performance computing","body":"<p>I am looking for a programming language/platform which is suited for handling around 1 to 2 GB of data, and can run signal processing algorithms fast enough across platforms (including modern mobile platforms). Did someone come across suitable solutions?. Please <strong>share</strong> your <strong>research</strong> and <strong>experience</strong> on cross-platform development.</p>\n\n<p>My current research effort is complicated, owing to</p>\n\n<ul>\n<li>the increased deployment of powerful [mobile] platforms, </li>\n<li>the backing of Java through the popularity of Android (currently the #1 tag on stackoverflow) </li>\n</ul>\n\n<p>I had mixed experience with C# and <a href=\"http://www.mono-project.com/Main_Page\" rel=\"nofollow\">Mono</a> (It is fast, but cross-platform compatibility can take some additional effort).</p>\n\n<p>I am <strong>not</strong> looking for <strong>High performance computing</strong>(HPC), but into <strong>developing code</strong> with a data and computing-intensive background <strong>that performs well</strong> with as little effort as possible. A case study scenario for this would be Python code which is adapted to the Cython compiler.</p>\n\n<p>I am <strong>not</strong> looking for <strong>UI-centric</strong> projects with solutions such as <a href=\"https://build.phonegap.com/\" rel=\"nofollow\">Phonegap</a> and Adobe Air, but I know little about its performance.</p>\n\n<p><strong>Links</strong>:</p>\n\n<ul>\n<li><a href=\"http://www.cs.colostate.edu/saxs/researchexam/GenericProgramming.pdf\" rel=\"nofollow\">Cross-Platform Development of High Performance Applications\nUsing Generic Programming</a></li>\n<li><a href=\"http://research.google.com/pubs/archive/34913.pdf\" rel=\"nofollow\">Native Client: A Sandbox for Portable, Untrusted x86 Native Code</a></li>\n</ul>\n"},{"tags":["java","performance"],"answer_count":6,"favorite_count":4,"up_vote_count":16,"down_vote_count":0,"view_count":3572,"score":16,"question_id":2589741,"title":"How to effectively copy an array in java?","body":"<p>The toArray method in ArrayList , Bloch uses both System.arraycopy and Arrays.copyOf to copy an array .</p>\n\n<pre><code>public &lt;T&gt; T[] toArray(T[] a) {\n  if (a.length &lt; size)\n        // Make a new array of a's runtime type, but my contents:\n        return (T[]) Arrays.copyOf(elementData, size, a.getClass());\n    System.arraycopy(elementData, 0, a, 0, size);\n    if (a.length &gt; size)\n        a[size] = null;\n    return a;\n}\n</code></pre>\n\n<p>How to compare these two copy methods , when to use which ? Thanks.</p>\n"},{"tags":["mvc","iis7","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":584,"score":0,"question_id":2320636,"title":"How can I optimize MVC and IIS pipeline to obtain higher speed?","body":"<p>I am doing performance tweaking of a simple app that uses MVC on IIS 7.5.\nI have a StopWatch starting up in Application_BeginRequest and I take a snapshot at Controller.OnActionExecuting.</p>\n\n<p>So I measure the time spend in the entire IIS pipeline: from request receipt to the moment execution finally gets to my controller.</p>\n\n<p>I obtain 700 microseconds on my 3GHz quad-core (project compiled Release x64), and I wonder where the bottleneck is, especially hearing some people say that one can get up to <a href=\"http://stackoverflow.com/questions/43743/asp-net-mvc-performance/530946\">8000 page loads per second</a> with MVC.</p>\n\n<p>How can I optimize MVC and IIS pipeline to obtain higher speed?</p>\n"},{"tags":["performance","nosql","cassandra","hector"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":51,"score":0,"question_id":12888954,"title":"Cassandra performance slow down with counter column","body":"<p>I have a cluster (4 node ) and a node have 16 core and 24 gb ram: </p>\n\n<pre><code>192.168.23.114  datacenter1 rack1       Up     Normal  44.48 GB        25.00%             \n192.168.23.115  datacenter1 rack1       Up     Normal  44.51 GB        25.00%\n192.168.23.116  datacenter1 rack1       Up     Normal  44.51 GB        25.00%\n192.168.23.117  datacenter1 rack1       Up     Normal  44.51 GB        25.00%\n</code></pre>\n\n<p>We use about 10 column family (counter column) to make some system statistic report.</p>\n\n<p>Problem on here is that When i set replication_factor of this keyspace from 1 to 2 (contain 10 counter column family ), all cpu of node increase from 10% ( when use replication factor=1) to ---> 90%. :( :(</p>\n\n<p>who can help me work around that :( . why counter column consume too much cpu time :(.</p>\n\n<p>thanks all</p>\n"},{"tags":["performance","comparison","integer","range"],"answer_count":7,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":798,"score":5,"question_id":3269434,"title":"What's the most efficient way to test two integer ranges for overlap?","body":"<p>Given two inclusive integer ranges [x1:x2] and [y1:y2], where x1 &lt;= x2 and y1 &lt;= y2, what is the most efficient way to test whether there is any overlap of the two ranges?</p>\n\n<p>A simple implementation is as follows:</p>\n\n<pre><code>bool testOverlap(int x1, int x2, int y1, int y2) {\n  return (x1 &gt;= y1 &amp;&amp; x1 &lt;= y2) ||\n         (x2 &gt;= y1 &amp;&amp; x2 &lt;= y2) ||\n         (y1 &gt;= x1 &amp;&amp; y1 &lt;= x2) ||\n         (y2 &gt;= x1 &amp;&amp; y2 &lt;= x2);\n}\n</code></pre>\n\n<p>But I expect there are more efficient ways to compute this.</p>\n\n<p>What method would be the most efficient in terms of fewest operations.</p>\n"},{"tags":["sql","performance","postgresql","file-io","csv"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":68,"score":0,"question_id":12756413,"title":"Check if records exists in a Postgres table","body":"<p>I have to read a CSV every 20 seconds. Each CSV contains min. of 500 to max. 60000 lines. I have to insert the data in a Postgres table, but before that I need to check if the items have already been inserted, because there is a high probability of getting duplicate item. The field to check for uniqueness is also indexed.</p>\n\n<p>So, I read the file in chunks and use the IN clause to get the items already in the database.</p>\n\n<p>Is there a better way of doing it? </p>\n"},{"tags":["performance","postgresql","strict","sql-function"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":85,"score":2,"question_id":8455177,"title":"Function executes faster without STRICT modifier?","body":"<p>I wonder about a slump in performance when a simple SQL function is declared <code>STRICT</code>. I stumbled upon this phenomenon while <a href=\"http://stackoverflow.com/a/8455172/939860\">answering a question here</a>.</p>\n\n<p>To demonstrate the effect I create two variants of a simple SQL function that orders two elements of an array in ascending order.</p>\n\n<h3>Test setup</h3>\n\n<pre><code>-- temporary table with 10000 random pairs of integer\nCREATE TEMP TABLE arr (i int[]);\n\nINSERT INTO arr \nSELECT ARRAY[(random() * 1000)::int, (random() * 1000)::int]\nFROM   generate_series(1,10000);\n</code></pre>\n\n<p>Function with <code>STRICT</code> modifier:</p>\n\n<pre><code>CREATE OR REPLACE FUNCTION f_sort_array1(int[])  RETURNS int[] AS\n$$\nSELECT CASE WHEN $1[1] &gt; $1[2] THEN ARRAY[$1[2], $1[1]] ELSE $1 END;\n$$ LANGUAGE sql STRICT IMMUTABLE;\n</code></pre>\n\n<p>Function without <code>STRICT</code> modifier (otherwise identical):</p>\n\n<pre><code>CREATE OR REPLACE FUNCTION f_sort_array2(int[])  RETURNS int[] AS\n$$\nSELECT CASE WHEN $1[1] &gt; $1[2] THEN ARRAY[$1[2], $1[1]] ELSE $1 END;\n$$ LANGUAGE sql IMMUTABLE;\n</code></pre>\n\n<h3>Results</h3>\n\n<p>I executed each around 20 times and took the best result from <code>EXPLAIN ANALYZE</code>.</p>\n\n<pre><code>SELECT f_sort_array1(i) FROM arr  -- Total runtime: 103 ms\nSELECT f_sort_array2(i) FROM arr  -- Total runtime:  43 ms (!!!)\n</code></pre>\n\n<p>These are the results from a v9.0.5 server on Debian Squeeze. Similar results on v8.4. Did not test on 9.1, have no cluster at my disposal right now. (Can someone supply additional results for v9.1?)</p>\n\n<p>Edit:\nIn a test with 10000 NULL values both functions perform the same in the same test environment: ~37 ms.</p>\n\n<p>I did some research and found an interesting gotcha. Declaring an SQL function <strong>STRICT disables function-inlining</strong> in most cases. More about that in the <a href=\"http://www.postgresonline.com/journal/archives/163-STRICT-on-SQL-Function-Breaks-In-lining-Gotcha.html\" rel=\"nofollow\">PostgreSQL Online Journal</a> or in the <a href=\"http://archives.postgresql.org/pgsql-performance/2011-11/msg00119.php\" rel=\"nofollow\">pgsql-performance mailing list</a>.</p>\n\n<p>But I am not quite sure how this could be the explanation. How can not inlining the function cause the performance slump in this simple scenario? No index, no disc read, no sorting. Maybe an overhead from the repeated function call that is streamlined away by inlining the function? Can you explain it? Or am I missing something?</p>\n\n<hr>\n\n<h3>Retest with Postgres 9.1</h3>\n\n<p>Ran the same test on the same hardware with PostgreSQL 9.1 an found even bigger differences:</p>\n\n<pre><code>SELECT f_sort_array1(i) FROM arr  -- Total runtime: 107 ms\nSELECT f_sort_array2(i) FROM arr  -- Total runtime:  27 ms (!!!)\n</code></pre>\n"},{"tags":["java","ruby-on-rails","ruby","performance","playframework-2.0"],"answer_count":2,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":210,"score":4,"question_id":12886667,"title":"Play! framework vs Ruby on Rails","body":"<p>It's not going to be a request for general comparisson: </p>\n\n<p><strong>Play! framework is Java based</strong> which means the code is interpreted to bytecode and then compiled by the JVM in runtime. On the other hand, <strong>Ruby is a dynamic language</strong> which means the code is interpreted with every request. This is certainly obvious for every programmer.</p>\n\n<p>Another aspect is the development process and the ease of the language (strong typing vs weak typing).</p>\n\n<p>Currently I'm developing a new website using Play!<br>\nSo, for the questions:  </p>\n\n<ol>\n<li><strong>Performance for an HTTP server</strong> (Play! runs on the JVM, Ruby is dynamic) - does it really matter for a website? would you see a significant differences? </li>\n<li>I feel RoR has much <strong>larger community, sources, tutorials etc</strong>, and it's a little batter me. Or should it?</li>\n</ol>\n"},{"tags":["python","performance","memory","memory-management"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":134,"score":3,"question_id":12872244,"title":"Python: memory problems in a script","body":"<p>I have wrote a script where i read around 4 million of points and 800.000 plots. The script clip the points inside each plot and save a new text file for each plot.</p>\n\n<p>After a certain period of time my PC memory is full. I had tried to dig inside my script but in each loop <code>for i in xrange(len(sr)):</code> the each object is replaced and the points clipped saved in a new txt file.</p>\n\n<p>are there some strategy to use in this case in order to improve memory usage without reduce the performance(the script is already slow)? I am a beginner in python and sorry if the question is simple.</p>\n\n<p>Thanks in advance\nGianni</p>\n\n<pre><code>inFile =\"C://04-las_clip_inside_area//prova//Ku_115_class_Notground_normalize.las\"\npoly =\"C://04-las_clip_inside_area//prova//ku_115_plot_clip.shp\"\nchunkSize = None\nMinPoints = 1\n\nsf = shapefile.Reader(poly) #open shpfile\nsr = sf.shapeRecords()\npoly_filename, ext = path.splitext(poly)\ninFile_filename = os.path.splitext(os.path.basename(inFile))[0]\npbar = ProgressBar(len(sr)) # set progressbar\nif chunkSize == None:\n    points = [(p.x,p.y) for p in lasfile.File(inFile,None,'r')]\n    for i in xrange(len(sr)):\n        pbar.update(i+1) # progressbar\n        verts = np.array(sr[i].shape.points,float)\n        record = sr[i].record[0]\n        index = nonzero(points_inside_poly(points, verts))[0]\n        if len(index) &gt;= MinPoints:\n            file_out = open(\"{0}_{1}_{2}.txt\".format(poly_filename, inFile_filename, record), \"w\")\n            inside_points = [lasfile.File(inFile,None,'r')[l] for l in index]\n            for p in inside_points:\n                file_out.write(\"%s %s %s %s %s %s %s %s %s %s %s\" % (p.x, p.y, p.z, p.intensity,p.return_number,p.number_of_returns,p.scan_direction,p.flightline_edge,p.classification,p.scan_angle,record)+ \"\\n\")\n            file_out.close()\n</code></pre>\n\n<p>this is the origial function</p>\n\n<pre><code>def LAS2TXTClipSplitbyChunk(inFile,poly,chunkSize=1,MinPoints=1):\n    sf = shapefile.Reader(poly) #open shpfile\n    sr = sf.shapeRecords()\n    poly_filename, ext = path.splitext(poly)\n    inFile_filename = os.path.splitext(os.path.basename(inFile))[0]\n    pbar = ProgressBar(len(sr)) # set progressbar\n    if chunkSize == None:\n        points = [(p.x,p.y) for p in lasfile.File(inFile,None,'r')]\n        for i in xrange(len(sr)):\n            pbar.update(i+1) # progressbar\n            verts = np.array(sr[i].shape.points,float)\n            record = sr[i].record[0]\n            index = nonzero(points_inside_poly(points, verts))[0]\n            if len(index) &gt;= MinPoints:\n                file_out = open(\"{0}_{1}_{2}.txt\".format(poly_filename, inFile_filename, record), \"w\")\n                inside_points = [lasfile.File(inFile,None,'r')[l] for l in index]\n                for p in inside_points:\n                    file_out.write(\"%s %s %s %s %s %s %s %s %s %s %s\" % (p.x, p.y, p.z, p.intensity,p.return_number,p.number_of_returns,p.scan_direction,p.flightline_edge,p.classification,p.scan_angle,record)+ \"\\n\")\n                file_out.close()\n    else:\n        for i in xrange(len(sr)):\n            pbar.update(i+1) # progressbar\n            verts = np.array(sr[i].shape.points,float)\n            record = sr[i].record[0]\n            f = lasfile.File(inFile,None,'r')\n            file_out = open(\"{0}_{1}_{2}.txt\".format(poly_filename, inFile_filename, record), \"w\")\n            TotPoints = 0\n            while True:\n                chunk = list(islice(f,chunkSize))\n                if not chunk:\n                    break\n                points = [(p.x,p.y) for p in chunk]\n                index = nonzero(points_inside_poly(points, verts))[0]\n                TotPoints += len(index) #add points to count inside th plot\n                chunk = [chunk[l] for l in index]\n                for p in chunk:\n                    file_out.write(\"%s %s %s %s %s %s %s %s %s %s %s\" % (p.x, p.y, p.z, p.intensity,p.return_number,p.number_of_returns,p.scan_direction,p.flightline_edge,p.classification,p.scan_angle,record)+ \"\\n\")\n            if TotPoints &gt;= MinPoints:\n                file_out.close()\n            else:\n                file_out.close()\n                os.remove(\"{0}_{1}_{2}.txt\".format(poly_filename, inFile_filename, record))\n            f.close()\n</code></pre>\n\n<p>the script by the suggestion of unutbu is:</p>\n\n<pre><code>import shapefile\nimport os\nimport glob\nfrom os import path\nimport numpy as np\nfrom numpy import nonzero\nfrom matplotlib.nxutils import points_inside_poly\nfrom itertools import islice\nfrom liblas import file as lasfile\nfrom shapely.geometry import Polygon\nfrom progressbar import ProgressBar\nimport multiprocessing as mp\n\n\ninFile =\"C://04-las_clip_inside_area//prova//Ku_115_class_Notground_normalize.las\"\npoly =\"C://04-las_clip_inside_area//prova//ku_115_plot_clip.shp\"\nchunkSize = None\nMinPoints = 1\n\ndef pointinside(record):\n    verts = np.array(record.shape.points, float)\n    record = record.record[0]\n    index = nonzero(points_inside_poly(points, verts))[0]\n    if len(index) &gt;= MinPoints:\n        outfile = \"{0}_{1}_{2}.txt\".format(poly_filename, inFile_filename, record)\n        with open(outfile, \"w\") as file_out:\n            inside_points = [lasfile.File(inFile, None, 'r')[l] for l in index]\n            for p in inside_points:\n                fields = (p.x, p.y, p.z, p.intensity, p.return_number,\n                          p.number_of_returns, p.scan_direction, p.flightline_edge,\n                          p.classification, p.scan_angle, record)\n                file_out.write(' '.join(map(str, fields)) + \"\\n\")\n\nsf = shapefile.Reader(poly) #open shpfile\nsr = sf.shapeRecords()\npoly_filename, ext = path.splitext(poly)\ninFile_filename = os.path.splitext(os.path.basename(inFile))[0]\npbar = ProgressBar(len(sr)) # set progressbar\nif chunkSize == None:\n    points = [(p.x,p.y) for p in lasfile.File(inFile,None,'r')]\n    for i in xrange(len(sr)):\n        pbar.update(i+1) # progressbar\n        proc = mp.Process(target = pointinside, args = (sr[i], ))\n        proc.start()\n        proc.join()\n</code></pre>\n"},{"tags":["java","performance","jvm"],"answer_count":4,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":1383,"score":2,"question_id":4687757,"title":"Tools to monitor java thread execution","body":"<p>I've a java web application running on an Tomcat server(Linux). In the production environment I'm facing some performance issue. At random intervals the jsvc process on which tomcat is running starts to run at 90-100% CPU. I'm unable to find out the trigger for this event. The server is a quad core system. Memory conception does not indicate any abnormalities.</p>\n\n<p>How can I monitor which thread(application stack trace) in the application is causing the problem?</p>\n\n<p>I'm checking with <a href=\"http://java.sun.com/developer/technicalArticles/J2SE/jconsole.html\" rel=\"nofollow\">jconsole</a> and <a href=\"http://code.google.com/p/psi-probe/\" rel=\"nofollow\">PSI Probe</a>, but both are not giving any detailed information about what thread inside the application is causing the CPU usage abnormality.</p>\n"},{"tags":["php","performance","content-management-system"],"answer_count":7,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":364,"score":1,"question_id":1159413,"title":"Make PHP code as small as possible while not reducing performance?","body":"<p>I'm writing a Content Management System in PHP, and I want it to be the smallest one in the world. I'm planning to make it available to everyone, just like Drupal and Joomla. But to make it so ultra-tiny, I change code to smaller code.</p>\n\n<p>For example, I change:</p>\n\n<pre><code>$info = parse_ini_file(\"info.scm\"); /* to */ $i=parse_ini_file(\"info.scm\");\n</code></pre>\n\n<p>just to make it smaller. But, I use some functions very often, like preg_replace();. I use it over 30 times. Should I make a function like:</p>\n\n<pre><code>function p($p,$r,$s){preg_replace($p,$r,$s);}\n//and than just use:\np($my_regex, $my_replacement, $my_string);\n</code></pre>\n\n<p>or does this make it all work slower?</p>\n\n<p>Notice that my goal is to make it so tiny as possible.</p>\n"},{"tags":["c#","performance","linked-list","binary-search","linear-search"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":68,"score":0,"question_id":12887872,"title":"Will a binary search on LinkedList to insert a value into the middle of a sorted value list increase performance?","body":"<p>I need to create a sorted list adding one item at a time. So I decided to go with <strong>LinkedList</strong>, Since it is efficient in <strong>insert</strong> operations. But when finding the proper location, it seems to take much longer time. I am using linear search to get the location. If I use binary search to get the proper location using <strong>elementAt()</strong> method, will it increase the performance.  According to <a href=\"http://stackoverflow.com/questions/10164355/how-do-i-get-the-n-th-element-in-a-linkedlistt\">this</a>, still it is a O(n) operation. What do you think? If it is so, is there any other better data structure for the work? Because if I use a different datatype, when inserting a new value to the middle, I will have to shift all the data after that location by one location, which is obviously not a .</p>\n"},{"tags":["python","performance","data-structures","iterate"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":79,"score":3,"question_id":12886611,"title":"High-performance way to eliminate semi-duplicate items from a list","body":"<p>I have a series of puzzles: Strings of morse code with no spaces between the letters or words.  My plan is to do a dictionary attack to find the best solution candidates. My weapon is Python.</p>\n\n<p>I have a list of 17000 English words. I also have a much smaller list of words that are pertinent to the puzzle's theme, and if those words show up they should score higher.</p>\n\n<p>So at the very beginning of my script when I generate the list of words, I use a list of tuples of the form (word, scoremultiplier). Here's a small subset:</p>\n\n<pre><code>[('zoned', 1.0), \n ('zonely', 1.0), \n ('zoner', 1.0), \n ('zones', 1.0), \n ('zoning', 1.0), \n ('zoo', 1.0), \n ('zoom', 1.0), \n ('zoomed', 1.0), \n ('zooming', 1.0), \n ('zooms', 1.0), \n ('zoos', 1.0), \n ('ten', 1.0), \n ('tens', 1.0), \n ('gnash', 1.0), \n ('shag', 1.0), \n ('75th', 2.0), \n ('seventy', 2.0), \n ('fifth', 2.0)]\n</code></pre>\n\n<p>In the file that I parse all that out of, I want to just stick the high-value words at the end, without manually getting rid of any duplicates in the main part of the file. So I need to write something to get rid of the early tuples whose first value is equal to that of a later tuple.</p>\n\n<p>I can do this with brute force:</p>\n\n<pre><code>for firstkey, (firstword, firstfactor) in enumerate(wordlist):\n    for laterkey, (laterword, laterfactor) in enumerate(wordlist[firstkey+1:]):\n        if firstword == laterword:\n            del wordlist[firstkey]\n            break\n</code></pre>\n\n<p>But that part of the script alone takes almost 45 seconds, and my 17000 words isn't even a full dictionary. (That code is also untested other than the time it takes to finish, so it may not even work.) It also seems very un-Pythony, though I'm just now learning Python (and doing some of my first programming at all) with this very project.</p>\n\n<p>Is there a better way to do this? I can't use <code>set()</code> because the duplicate words are part of nonequal tuples. Do I need to restructure my data somehow? Or should I just be prepared to wait a full minute every time I run this?</p>\n"},{"tags":["c++","performance","variables"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":92,"score":2,"question_id":12887342,"title":"Is it better to check a variable before setting its value in C++?","body":"<p>If I have a boolean and some code which maybe changes it, and then I want to set it to <code>true</code>, should I check if it's <code>false</code>?</p>\n\n<p>For example:</p>\n\n<pre><code>bool b = false;\n// Some code\n// Here \"b\" can be true or false\nif (cond) {\n    b = true;\n}\n</code></pre>\n\n<p>vs</p>\n\n<pre><code>bool b = false;\n// Some code\n// Here `b` can be `true` or `false`\nif (cond &amp;&amp; !b){\n    b = true;\n}\n</code></pre>\n\n<p>Which is faster?</p>\n\n<p><strong>Note</strong>:</p>\n\n<p>I ask that because of the following implementation of <a href=\"http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes\" rel=\"nofollow\">Sieve of Eratosthenes</a>: <a href=\"http://bloc.gerardfarras.com/wp-content/uploads/2011/12/erastotenes.txt\" rel=\"nofollow\">http://bloc.gerardfarras.com/wp-content/uploads/2011/12/erastotenes.txt</a></p>\n\n<pre><code>if (( i % divisor == 0 ) &amp;&amp; ( numsprimers[i] == 0 )) {\n    numsprimers[i] = 1;\n}\n</code></pre>\n\n<p>(If <code>numsprimers[i]==1</code> it means that <code>i</code> isn't a prime number. And if it's 0 it can be prime or not)</p>\n"},{"tags":["mysql","performance","innodb","sqlperformance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":33,"score":0,"question_id":12887646,"title":"What should i configure to able insert huge data to my INNODB tables?","body":"<p>I insert big data to my tables.\nSome of the tables not insert the data because to much rows.</p>\n\n<p>How should be number of rows enlarged in INNODB?</p>\n\n<p>(I try ALTER TABLE table MAX_ROWS=1000000000, after i create the empty tables, but after i insert to much rows the table not insert.)</p>\n\n<p>Edit:</p>\n\n<p>Simplify my Case:</p>\n\n<p>I have 10 text files that i insert to INNDOB database + tables(new installation of mysql 5.5 windows server 2008 with):</p>\n\n<pre><code>LOAD DATA LOCAL INFILE\n</code></pre>\n\n<p>Its works for 8 files, but 2 huge text file not success no insert to tables(when i cut them and make them small they are inserted soo it limit table problem).</p>\n\n<p>Thanks</p>\n"},{"tags":["performance","opencl","gpgpu"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":111,"score":0,"question_id":12846809,"title":"Why is this simple OpenCL kernel running so slowly?","body":"<p>I'm looking into OpenCL, and I'm a little confused why this kernel is running so slowly, compared to how I would expect it to run. Here's the kernel: </p>\n\n<pre><code>__kernel void copy(\n  const __global char* pSrc, \n  __global __write_only char* pDst, \n  int length)\n{\n  const int tid = get_global_id(0);\n\n  if(tid &lt; length) {\n    pDst[tid] = pSrc[tid];\n  }\n}\n</code></pre>\n\n<p>I've created the buffers in the following way:</p>\n\n<pre><code>char* out = new char[2048*2048];\ncl::Buffer(\n  context,\n  CL_MEM_USE_HOST_PTR | CL_MEM_WRITE_ONLY,\n  length,\n  out);\n</code></pre>\n\n<p>Ditto for the input buffer, except that I've initialized the in pointer to random values. Finally, I run the kernel this way: </p>\n\n<pre><code>cl::Event event;\nqueue.enqueueNDRangeKernel(\n  kernel, \n  cl::NullRange,\n  cl::NDRange(length),\n  cl::NDRange(1), \n  NULL, \n  &amp;event);\n\nevent.wait();\n</code></pre>\n\n<p>On average, the time is around 75 milliseconds, as calculated by: </p>\n\n<pre><code>cl_ulong startTime = event.getProfilingInfo&lt;CL_PROFILING_COMMAND_START&gt;();\ncl_ulong endTime = event.getProfilingInfo&lt;CL_PROFILING_COMMAND_END&gt;();\nstd::cout &lt;&lt; (endTime - startTime) * SECONDS_PER_NANO / SECONDS_PER_MILLI &lt;&lt; \"\\n\";\n</code></pre>\n\n<p>I'm running Windows 7, with an Intel i5-3450 chip (Sandy Bridge architecture). For comparison, the \"direct\" way of doing the copy takes less than 5 milliseconds. I don't think the event.getProfilingInfo includes the communication time between the host and device. Thoughts? </p>\n\n<p>EDIT: </p>\n\n<p>At the suggestion of ananthonline, I changed the kernel to use float4s instead of chars, and that dropped the average run time to about 50 millis. Still not as fast as I would have hoped, but an improvement. Thanks ananthonline!</p>\n"},{"tags":["java","c++","performance","logic"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":4,"view_count":88,"score":-4,"question_id":12886799,"title":"Is a for loop that ends up not even running once just as fast as calling an if statement to check the size first?","body":"<p>I am trying to optimize a chunk of code where speed is very important and wondered if checking  the <code>int</code> that holds the number of times a for loop is about to loop and not doing the for loop if it is equal to zero was any faster or slower than just letting the for loop execute 0 times.</p>\n\n<p>I realize that any speed improvement would be tiny; it just started to become more of a curiosity. Also would this be different from Java to say C++ or C?</p>\n\n<p>Example:</p>\n\n<pre><code>size=0;\nfor (int i = 0;i&lt;size;i++)\n{\n}\n</code></pre>\n\n<p>or</p>\n\n<pre><code>size=0;\nif (size!=0)\n{ \n    for (int i = 0;i&lt;size;i++)\n    {\n    }\n}\n</code></pre>\n\n<p>Of course, in the real code the size is often not zero, but when it is which would be faster if either?</p>\n"},{"tags":["c#","asp.net","performance","web","web-performance-test"],"answer_count":4,"favorite_count":0,"up_vote_count":3,"down_vote_count":1,"view_count":56,"score":2,"question_id":12637844,"title":"Does space matters on a page for web application","body":"<p>My client complained me that my web page contains so many spaces so it is not good for the performance.</p>\n\n<p>Does the space matters in web pages?</p>\n\n<p><strong>UPDATE :</strong></p>\n\n<p>I mean whitespace for performance and cost issue.</p>\n"},{"tags":["sql","sql-server","performance","client","native"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":119,"score":3,"question_id":12877300,"title":"SQL Native Client 10 Performance miserable (due to server-side cursors)","body":"<p>we have an application that uses ODBC via CDatabase/CRecordset in MFC (VS2010).\nWe have two backends implemented. MSSQL and MySQL.</p>\n\n<p>Now, when we use MSSQL (with the Native Client 10.0), retrieving records with SELECT is dramatically slow via slow links (VPN, for example). The MySQL ODBC driver does not exhibit this nasty behavior.</p>\n\n<p>For example:</p>\n\n<pre><code>CRecordset r(&amp;m_db);\nr.Open(CRecordset::snapshot, L\"SELECT a.something, b.sthelse FROM TableA AS a LEFT JOIN TableB AS b ON a.ID=b.Ref\");\nr.MoveFirst();\nwhile(!r.IsEOF())\n{\n    // Retrieve\n    CString strData;\n    crs.GetFieldValue(L\"a.something\", strData);\n    crs.MoveNext();\n}\n</code></pre>\n\n<p>Now, with the MySQL driver, everything runs as it should. The query is returned, and everything is lightning fast.\nHowever, with the MSSQL Native Client, things slow down, because on every MoveNext(), the driver communicates with the server.</p>\n\n<p>I think it is due to server-side cursors, but I didn't find a way to disable them. I have tried using:</p>\n\n<pre><code>::SQLSetConnectAttr(m_db.m_hdbc, SQL_ATTR_ODBC_CURSORS, SQL_CUR_USE_ODBC, SQL_IS_INTEGER);\n</code></pre>\n\n<p>But this didn't help either. There are still long-running exec's to sp_cursorfetch() et al in SQL Profiler.\nI have also tried a small reference project with SQLAPI and bulk fetch, but that hangs in FetchNext() for a long time, too (even if there is only one record in the resultset).\nThis however only happens on queries with LEFT JOINS, table-valued functions, etc.\n<em>Note that the query doesn't take that long</em> - executing the same SQL via SQL Studio over the same connection returns in a reasonable time.</p>\n\n<p><strong>Question1: Is is possible to somehow get the native client to <del>\"cache\" all results locally </del> use local cursors in a similar fashion as the MySQL driver seems to do it?</strong></p>\n\n<p>Maybe this is the wrong approach altogether, but I'm not sure how else to do this.</p>\n\n<p>All we want is to retrieve all data at once from a SELECT, then never talk the server again until the next query.\nWe don't care about recordset updates, deletes, etc or any of that nonsense. We only want to retrieve data.\nWe take that recordset, get all the data, and delete it.</p>\n\n<p><strong>Question2: Is there a more efficient way to just retrieve data in MFC with ODBC?</strong></p>\n"},{"tags":["javascript","jquery","ajax","performance","post"],"answer_count":5,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":82,"score":1,"question_id":12880539,"title":"Native or jQuery for a large web app","body":"<p>I know that coding in native JavaScript means that your code will execute faster than if you were to code it in jQuery, but how much faster?</p>\n\n<p>In particular, I want to know if the speed increase would make it worth while spending the longer time coding in native JavaScript than jQuery if it was for a very large webapp?</p>\n\n<p>Or is the difference in speed not that great at all?</p>\n\n<p>For instance, to setup an AJAX request in jQuery all you have to do is call <code>$.ajax</code> or <code>$.post</code> and pass a few parameters, but with native JavaScript you have to create <code>XMLHttpRequest</code> or <code>ActiveXObject</code> objects depending on the users browser etc etc.</p>\n"},{"tags":["python","performance","iterator"],"answer_count":4,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":171,"score":6,"question_id":12775449,"title":"Group an iterable by a predicate in Python","body":"<p>I'm parsing a file like this:</p>\n\n<pre>\n--header--\ndata1\ndata2\n--header--\ndata3\ndata4\ndata5\n--header--\n--header--\n...\n</pre>\n\n<p>And I want groups like this:</p>\n\n<pre><code>[ [header, data1, data2], [header, data3, data4, data5], [header], [header], ... ]\n</code></pre>\n\n<p>so I can iterate over them like this:</p>\n\n<pre><code>for grp in group(open('file.txt'), lambda line: 'header' in line):\n    for item in grp:\n        process(item)\n</code></pre>\n\n<p>and keep the detect-a-group logic separate from the process-a-group logic.</p>\n\n<p>But I need an iterable of iterables, as the groups can be arbitrarily large and I don't want to store them.  That is, I want to split an iterable into subgroups every time I encounter a \"sentinel\" or \"header\" item, as indicated by a predicate.  Seems like this would be a common task, but I can't find an efficient Pythonic implementation.</p>\n\n<p>Here's the dumb append-to-a-list implementation:</p>\n\n<pre><code>def group(iterable, isstart=lambda x: x):\n    \"\"\"Group `iterable` into groups starting with items where `isstart(item)` is true.\n\n    Start items are included in the group.  The first group may or may not have a \n    start item.  An empty `iterable` results in an empty result (zero groups).\"\"\"\n    items = []\n    for item in iterable:\n        if isstart(item) and items:\n            yield iter(items)\n            items = []\n        items.append(item)\n    if items:\n        yield iter(items) \n</code></pre>\n\n<p>It feels like there's got to be a nice <code>itertools</code> version, but it eludes me.  The 'obvious' (?!) <code>groupby</code> solution doesn't seem to work because there can be adjacent headers, and they need to go in separate groups.  The best I can come up with is (ab)using <code>groupby</code> with a key function that keeps a counter:</p>\n\n<pre><code>def igroup(iterable, isstart=lambda x: x):\n    def keyfunc(item):\n        if isstart(item):\n            keyfunc.groupnum += 1       # Python 2's closures leave something to be desired\n        return keyfunc.groupnum\n    keyfunc.groupnum = 0\n    return (group for _, group in itertools.groupby(iterable, keyfunc))\n</code></pre>\n\n<p>But I feel like Python can do better -- and sadly, this is even slower than the dumb list version:</p>\n\n<pre>\n# ipython\n%time deque(group(xrange(10 ** 7), lambda x: x % 1000 == 0), maxlen=0)\nCPU times: user 4.20 s, sys: 0.03 s, total: 4.23 s\n\n%time deque(igroup(xrange(10 ** 7), lambda x: x % 1000 == 0), maxlen=0)\nCPU times: user 5.45 s, sys: 0.01 s, total: 5.46 s\n</pre>\n\n<p>To make it easy on you, here's some unit test code:</p>\n\n<pre><code>class Test(unittest.TestCase):\n    def test_group(self):\n        MAXINT, MAXLEN, NUMTRIALS = 100, 100000, 21\n        isstart = lambda x: x == 0\n        self.assertEqual(next(igroup([], isstart), None), None)\n        self.assertEqual([list(grp) for grp in igroup([0] * 3, isstart)], [[0]] * 3)\n        self.assertEqual([list(grp) for grp in igroup([1] * 3, isstart)], [[1] * 3])\n        self.assertEqual(len(list(igroup([0,1,2] * 3, isstart))), 3)        # Catch hangs when groups are not consumed\n        for _ in xrange(NUMTRIALS):\n            expected, items = itertools.tee(itertools.starmap(random.randint, itertools.repeat((0, MAXINT), random.randint(0, MAXLEN))))\n            for grpnum, grp in enumerate(igroup(items, isstart)):\n                start = next(grp)\n                self.assertTrue(isstart(start) or grpnum == 0)\n                self.assertEqual(start, next(expected))\n                for item in grp:\n                    self.assertFalse(isstart(item))\n                    self.assertEqual(item, next(expected))\n</code></pre>\n\n<p>So: how can I subgroup an iterable by a predicate elegantly and efficiently in Python?</p>\n"},{"tags":["performance","interpreter","brainfuck"],"answer_count":4,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":987,"score":5,"question_id":5543360,"title":"Fastest brainfuck interpreter?","body":"<p>Simple question: <strong>What is the fastest brainfuck interpreter available?</strong></p>\n\n<p>I am asking this because I am about to write my own optimizing bf interpreter and I need something to compare it with.</p>\n"},{"tags":["php","performance","mvc","cakephp"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":61,"score":0,"question_id":12885270,"title":"Would there be benefit in CakePHP performance if it dropped support for JS/AJAX engines?","body":"<p>My main Question:</p>\n\n<p><strong>Is there any practical effect on performance of CakePHP (i.e. faster view rendering), if support for JS helpers was dropped from the core completely?</strong></p>\n\n<p>My reasoning: </p>\n\n<p>Right now CakePHP has a few classes that allow a programmer to create basic client-side code using PHP, both for things like effects and AJAX requests.</p>\n\n<p>There is some coupling with the View object, which could be degrading to performance.</p>\n\n<p>Considering that a lot of frameworks are moving to a RESTful model and in general it is hard to keep up with the changes of the client-side frameworks, while coupling them with the server-side framework, like CakePHP.</p>\n\n<p>I am wondering if it's worthwhile to drop support for JS/AJAX and focus on PHP framework patterns. Of course we lose the ability to write some JS code through the helpers, but in my opinion it is still best left to a JS framework. </p>\n\n<p>The benefits are reduced coupling, lighter weight, and possibly improved performance. </p>\n"},{"tags":["performance","plc"],"answer_count":6,"favorite_count":5,"up_vote_count":10,"down_vote_count":0,"view_count":3200,"score":10,"question_id":1361396,"title":"Being a better / more efficient PLC Programmer","body":"<p>The company I am doing my intership/appretinceship in, does mainly PLC programming with Siemens modules.\nComes from the fact that most of the people were electric guys and switched over to engineering.</p>\n\n<p>My problem as newbie there is, that I can't be really efficient and fast when I code PLC software.</p>\n\n<p>Even though I am very efficient when I am coding C# or Java in VS/Eclipse</p>\n\n<p>It really bothers that I can't be really productive with PLC as opposed to the \"real\" programming languages.</p>\n\n<ul>\n<li>Is it the lack of code completion?</li>\n<li>Is it the lack of overall knowledge on the automation side?</li>\n<li>Is it the lack of innovation in PLC as opposed to VS (LINQ, Dynamics, Lambda)</li>\n</ul>\n\n<p>Have you guys any good experience with PLC?\nAnd how did you get productive with it?</p>\n\n<p>Notice: It is my last year at the company, that's also why I want to be very productive.</p>\n\n<p>Looking forward to many great answers!</p>\n"},{"tags":["python","performance","multiprocessing","cpu"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":64,"score":0,"question_id":12883237,"title":"Python: improve the efficency of my script using multiprocessing module (tips and suggestions)","body":"<p>I am a beginner of Python (few weeks) and recently i had read some post in Stackoverflow about <strong>multiprocessing module</strong>. Normally i work with million of points format data (*.las file. This <a href=\"http://vimeo.com/16282938\" rel=\"nofollow\">video</a> to understand the source of my data) and I have interest to understand better how use multiprocessing module.  </p>\n\n<p><strong>I use Python 2.7 on windows 7, intel core i7-3770CPU</strong> </p>\n\n<p>Normally I use this <strong>def</strong> wrote from me as a Benchmark to understand:</p>\n\n<pre><code># load line-by-line the las file, check if the points are inside the polygon\n# if yes save a new *.las file\n\nimport shapefile\nimport numpy\nimport numpy as np\nfrom numpy import nonzero\nfrom matplotlib.mlab import griddata\nfrom matplotlib.nxutils import pnpoly\nfrom liblas import file as lasfile\n\ndef LAS2LASClip(inFile,poly,outFile):\n    f = lasfile.File(inFile,None,'r') # open LAS\n    h = f.header\n    # change the software id to libLAS\n    h.software_id = \"Python 2.7\"\n    file_out = lasfile.File(outFile,mode='w',header= h)\n    f.close()\n    sf = shapefile.Reader(poly) #open shpfile\n    shapes = sf.shapes()\n    for i in xrange(len(shapes)):\n        verts = np.array(shapes[i].points,float)\n        inside_points = [p for p in lasfile.File(inFile,None,'r') if pnpoly(p.x, p.y, verts)]\n        for p in inside_points:\n            file_out.write(p)\n    file_out.close()\n</code></pre>\n\n<p>Thanks in advance\nGianni</p>\n"},{"tags":["mysql","sql","performance","table","entity-attribute-value"],"answer_count":2,"favorite_count":2,"up_vote_count":1,"down_vote_count":0,"view_count":109,"score":1,"question_id":12882531,"title":"Entity attribute value model - Performance alternative?","body":"<p>I work with PHP and mySQL.</p>\n\n<p>I have a page table and a meta table. It looks a little bit like this.</p>\n\n<p><strong>Page table</strong></p>\n\n<pre><code>page_id | headline    | content\n--------------------------\n1       | My headline | My content\n2       | Another one | Another text\n</code></pre>\n\n<p><strong>Meta table</strong></p>\n\n<pre><code>id | page_id | meta_key  | meta_value\n------------------------------------\n1  | 2       | seo_title | Hello world\n2  | 2       | price     | 299\n</code></pre>\n\n<p>I've read that this type of model is called <a href=\"http://en.wikipedia.org/wiki/Entity%E2%80%93attribute%E2%80%93value_model\" rel=\"nofollow\">EAV</a>. I also read that it is <a href=\"http://karwin.blogspot.se/2009/05/eav-fail.html\" rel=\"nofollow\">bad for performance</a>.</p>\n\n<p>My meta table is made for any kind of value connected to a page. I can not created a table with \"static\" columns this time.</p>\n\n<p><strong>Question</strong></p>\n\n<ul>\n<li>How bad is this for 300 pages with 30 meta values on each page? 9000\nrows in the meta table that is.</li>\n<li>Is there a better model for \"dynamic\" data?</li>\n</ul>\n"},{"tags":["directx","performance","fullscreen","directx-10"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":2639,"score":1,"question_id":1006039,"title":"D3D10 (DirectX10) fullscreen performance issue","body":"<p>I have a bit of a problem setting up my DirectX10 (Win32/c++) application for fullscreen mode. The problem is that I want to have my app running in fullscreen right from the start. This can be done by taking the DXGISwapChain::SetFullScreenState function. This works, but i get a small notice in my Visualc++ 2008 debugger which states: </p>\n\n<p><strong>\"DXGI Warning: IDXGISwapChain::Present: Fullscreen presentation inefficiencies incurred due to application not using IDXGISwapChain::ResizeBuffers appropriately, specifying a DXGI_MODE_DESC not available in IDXGIOutput::GetDisplayModeList, or not using DXGI_SWAP_CHAIN_FLAG_ALLOW_MODE_SWITCH.\"</strong></p>\n\n<p>What this means is that DirectX will not take full ownership of the graphicscard and flip the images from front to backbuffer but instead blit them which is much slower.</p>\n\n<p>Now, i do have the DXGI_SWAP_CHAIN_FLAG_ALLOW_MODE_SWITCH enabled and i did try to resize my buffers but i have absolutely no idea what would be the best way to go into fullscreen mode. I have looked on MSDN but there they mostly assume you will only go into Fullscreen by pressing Alt+Enter which lest DXGI do all the work. If someone please could post a bit of code which takes DirectX10 into fullscreen mode and takes full advantage of the \"flipping\" it would be greatly appriciated! </p>\n\n<p>For anybody interested in the code used on resize:</p>\n\n<pre><code>ReleaseCOM(m_pD3DRenderTargetView);\nReleaseCOM(m_pD3DDepthStencilView);\nReleaseCOM(m_pD3DDepthStencilBuffer);\n\nDXGI_MODE_DESC* mod = new DXGI_MODE_DESC;\nmod-&gt;Format = DXGI_FORMAT_R8G8B8A8_UNORM;\nmod-&gt;Height = m_ScreenHeight;\nmod-&gt;Width = m_ScreenWidth;\nmod-&gt;RefreshRate.Denominator = 0;\nmod-&gt;RefreshRate.Numerator = 0;\nmod-&gt;ScanlineOrdering = DXGI_MODE_SCANLINE_ORDER_UNSPECIFIED;\nmod-&gt;Scaling = DXGI_MODE_SCALING_UNSPECIFIED;\ndelete mod; mod = 0;\n\nm_pSwapChain-&gt;ResizeTarget(mod);\n\nHR(m_pSwapChain-&gt;ResizeBuffers(1, m_ScreenWidth, m_ScreenHeight, DXGI_FORMAT_R8G8B8A8_UNORM, DXGI_SWAP_CHAIN_FLAG_ALLOW_MODE_SWITCH))\n\tthrow(Exception(GET_BUFFER_FAIL, AT));\n\n//problem area\nm_pSwapChain-&gt;SetFullscreenState(TRUE, NULL);\n\nID3D10Texture2D* pBackBuffer;\nHR( m_pSwapChain-&gt;GetBuffer(0, __uuidof(ID3D10Texture2D), (LPVOID*)&amp;pBackBuffer))\n\tthrow(Exception(GET_BUFFER_FAIL, AT)); //continues as usual\n</code></pre>\n"},{"tags":["php","performance","sqlite"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":54,"score":1,"question_id":12881778,"title":"SQLite - How to make more smaller databases in order to get better performance?","body":"<p>Is there some class or method to limit the size or the number of records in a SQLite database and automatically create a new DB once first one is full (up to limit)? </p>\n\n<p>My SQLite DB can go up to 500mb in size and on shared hosting it won't work.\nSo, I want to divide it to 10 databases of 50 MB or so.</p>\n\n<p>Will this also allow to easily read from the right database if several of them are created?</p>\n"},{"tags":["c#","asp.net","ajax","performance","drop-down-menu"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":122,"score":0,"question_id":12881803,"title":"how to fill dropdown list from extra huge datatable?","body":"<p>i have table from database this table has 400000 row in my asp page my dropdown list(ddlPlaintiffName) fill from\nthis method </p>\n\n<pre>\n\n private void FillPlaintiff()\n    {\n\n        //declare connection by pass connection string from web.config\n        SqlConnection sqlcon = new SqlConnection\n            (ConfigurationManager.ConnectionStrings[\"SystemConn\"].ConnectionString);\n        //declare sql statment  as astring variable\n\n        SqlCommand sqlcom = new SqlCommand();\n        sqlcom.Connection = sqlcon;\n        sqlcom.CommandType = CommandType.StoredProcedure;\n        sqlcom.CommandText = \"proc_SelectPlaintiff\";\n\n\n\n        DataTable ds = new DataTable();\n        //fill data set with data adabter that contain data from database\n     //   sad.Fill(ds);\n        sqlcon.Open();\n         SqlDataAdapter sad = new SqlDataAdapter(sqlcom);\n\n         sad.Fill(ds);\n\n        ddlPlaintiffName.DataSource = ds;\n        ddlPlaintiffName.DataBind();\n        ddlPlaintiffName.Items.Insert(0, \"--select  --\");\n        sqlcon.Close();\n\n    }\n\n\n</pre> \n\n<p>but every postback my load is very very slow how can i avoid this</p>\n"},{"tags":["database","performance","debugging","interface"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":2,"view_count":38,"score":-2,"question_id":12881707,"title":"Database & Interface speed","body":"<p>I have a project which requires extreme processing speeds. I have budgeted for a multi CPU solution with tons of RAM and SSD drives. My question is now what to code in and what database to use.</p>\n\n<p>The options I have and am comfortable using are:\nOperating System Options\n1) Windows Server 2008 \n2) A Linux distro</p>\n\n<p>Databases:\n1) Oracle\n2) MySQL/Postgres\n3) MYSQL (obviously tied into windows platform)</p>\n\n<p>Coding/Language:\n1) C# (.Net interface)\n2) C++ (.Net again)\n3) C++ (low level compiler like a GNU verison)\n4) Java</p>\n\n<p>Now I need the best combination to give me maximum speed. Now I am not sure if on new systems these even make a difference but I need speed increases in terms of milliseconds. (ie: if turnaround time is 5-20ms faster then it's still a clear winner).</p>\n\n<p>Now this is what I am thinking:</p>\n\n<p>Windows vs Linux: while windows generally takes more resources, once the applications are running the OS shouldnt make a difference.</p>\n\n<p>Database: My experience is as such - \nOracle, huge amounts of spatial data; \nMySQL/Postgres, large websites (forums, etc)\nMYSQL - middle tier for oracle backend for fin trnsactions.</p>\n\n<p>Now my gut feels tells me Oracle as it's the most stable and handles lare amounts of data, but is it fastest?\nThe data processing will be mostly reads (speed very imortant) and writes are not as mission critical, those will happen every 0.5 to 5 seconds. Data size is usually 100-1000 records of 4-8 fields most containing real number values (float/double in C terms).</p>\n\n<p>Language:\n- Java I would throw out right away since the fact that it's a proces srunning in a virtual machine adds another layer and could sslow things down.\n- C#/C++ .NET versions, this is a bit faster then Java and the easiest/nices for me to work in but the .NET framework could slow things down and I'm worried about the speed of the.NETR database interfaces.\n- C++ (GNU flavor), as this is the closest to hardware without going to assembler, this would be my best option. Only drawback is I don't know if there are any interfaces to the databases above and how much of a pain they are to use.</p>\n\n<p>So my own conclusion would be:\nOracle with a GNU C++ language interface. Unsure about windows or linux as yet, don't mind using either.  </p>\n\n<p>My biggest problem with using GNU C++ and linux is the debugging process, in windows C#/C++ I just use VStudio and am golden, are there any decent graphical debuggers/tools for the gnu compilers?</p>\n\n<p>I apologise for the length o the question but any comments and changes to my thinking process and conclusion would be appreciated.</p>\n"},{"tags":["c++","python","performance","code-generation","converter"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":9,"view_count":182,"score":-9,"question_id":12344414,"title":"Should I use a code converter (Python to C++)?","body":"<p>Let me just say right off the bat that i'm not a programmer. I'm just a guy with an idea taking his first steps to make it a reality. I'm no stranger to programming, mind you, but some of the concepts and terminology here are way over my head; so i apologize in advance if this question was answered before (i.e. <a href=\"http://stackoverflow.com/questions/4650243/convert-python-program-to-c-c-code\">Convert Python program to C/C++ code?</a>).</p>\n\n<p>I have an idea to create a simple A.I. network to analyze music data sent from a phone via cloud computing (I got a guy for the cloud stuff). It will require a lot of memory and need to be fast for the hard number-crunching. I had planned on doing it in python, but have since learned that might not be such a good idea (<a href=\"http://stackoverflow.com/questions/801657/is-python-faster-and-lighter-than-c\">Is Python faster and lighter than C++?</a>).</p>\n\n<p>Since python is really the only gun i have in my holster, i was thinking of using a python-to-C++-converter.  But nothing comes without a price:</p>\n\n<ol>\n<li>Is this an advantageous way to keep my code fast?</li>\n<li>What's the give-and-take for using a converter?</li>\n<li>Am i missing anything? I'm still new to this so i'm not even sure what questions to ask.</li>\n</ol>\n\n<p>Thanks in advance.</p>\n"},{"tags":["java","performance","solr"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":652,"score":0,"question_id":6124657,"title":"Calling solr inside solr","body":"<p>I am having multiple solr with seprate schema...</p>\n\n<p>i need to get results each solr and then i will append the results to final query and then i will call solr - to get final result..</p>\n\n<p>How to do this?\nI need to write seperate requesthandler ..?? </p>\n\n<p>Any other way is there???</p>\n\n<p>Ex:\nquery1 AND query2 AND query3 OR query4</p>\n\n<p>All the results i m passing in the next</p>\n\n<p>Query 5 : solr\\select?q=res.query1 AND res.query2 AND res.query3 OR res.query4 </p>\n\n<p>why i am separating the index?\n because i have 100,000 million of datas . so only i spliting the index</p>\n\n<p>Thanks in advance</p>\n"},{"tags":["mysql","sql","performance","query","sql-update"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":60,"score":0,"question_id":12812331,"title":"How to efficiently UPDATE a column when it requires joining 2 large tables in MySQL?","body":"<p>I've got 2 tables, named: <code>csv (a csv dump)</code>, and <code>items (primary data table)</code> with 7M (csv dump) and 15M rows respectively. I need to update a column in <code>items</code> that exists in table <code>csv</code>.</p>\n\n<p>Both tables have a commonly indexed join ID (a <code>VARCHAR(255)</code>).</p>\n\n<p>An UPDATE query with a join on the mutual ID column (indexed) still takes multiple days to run. After researching it  I believe the inefficiency is in MySQL scanning the <code>csv</code> table and making per-row random-access queries against the <code>items</code> table.</p>\n\n<p>Even though there are indexes, those indexes don't fit in memory, so the required 7M random access queries are nose diving performance.</p>\n\n<p>Are there \"typical\" ways of addressing this kind of issue?</p>\n\n<hr>\n\n<p><strong>Update:</strong></p>\n\n<blockquote>\n  <p>We're basically taking multiple catalogs of \"items\" and storing them\n  in our <code>items</code> table (this is bit of a simplification for discussion).\n  Each of say 10 catalogs will have 7M items (some duplicates across catalogs that we\n  normalize to 1 row in our item table). We need to compare and\n  verify changes to those 10 catalogs daily (<code>UPDATES</code> w/ joins between two big\n  tables, or other such mechanism).</p>\n  \n  <p>In reality we have an <code>items</code> table and an <code>items_map</code> table, but no\n  need to discuss that additional level of abstraction here. I'd be\n  happy to find a way to perform an update between the <code>csv</code> dump table\n  and an <code>items</code> table (given that they both have a common ID that's\n  indexed in both tables). But  given that the <code>items</code> table might have\n  20M rows, and the <code>csv</code> table might have 7M rows.</p>\n  \n  <p>In this case indexes don't fit in memory and we're hammering the drive with random seeks I believe</p>\n</blockquote>\n"},{"tags":["performance","actionscript-3","flash"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":59,"score":0,"question_id":12880015,"title":"AS3 Bottlenecks","body":"<p>What are the common bottlenecks or inherently slow actions/functions that I should look out for while developing an app/game/anything in ActioScript3 and flash?</p>\n"},{"tags":["android","performance","cursor"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":33,"score":0,"question_id":12880095,"title":"update single row in Android cursor","body":"<p>In my app I use ListView with CursorAdapter. When some data in some row of database is changed, I recreate cursor for CursorAdapter.</p>\n\n<p>This process is expensive. Is there any way to update just needed rows in Cursor?</p>\n\n<p>Thanks.</p>\n"},{"tags":["android","database","performance","sqlite"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":58,"score":1,"question_id":12831504,"title":"Quick readonly sqlite database","body":"<p>I have a huge database and I want my application to work with it as soon as possible. I'm using android so resources are more restricted. I know that its not a good idea to storage huge  data in the sqlite database, but I need this.</p>\n\n<p>Each database contain only ONE table and I use it READ only.</p>\n\n<p>What advice can you give me to optimize databases as much as possible. I've already read <a href=\"http://stackoverflow.com/questions/784173/what-are-the-performance-characteristics-of-sqlite-with-very-large-database-file\">this</a> post, and except the PRAGMA commands what else can I use?</p>\n\n<p>Maybe there are some special <strong>types</strong> of the tables which are restricted for read only queries, but principally faster then ordinary table types?</p>\n"},{"tags":["javascript","jquery","performance","delay","domready"],"answer_count":3,"favorite_count":3,"up_vote_count":9,"down_vote_count":0,"view_count":127,"score":9,"question_id":12850622,"title":"$.ready() before closing body","body":"<p>This is not a real coding question, more of a real-world statement.</p>\n\n<p>I have previously <a href=\"http://stackoverflow.com/questions/9557846/why-is-jquery-ready-recommended-when-its-so-slow\">noted</a> that <code>DOMReady</code> events are slow, very slow. So, I noticed while browsing the jQuery source that the jQuery domeready event can be trigger using <code>$.ready()</code>. Then I thought, placing this simple execution script just before closing the body should trigger all the \"onDomReady\" listeners that where previoulsy attached. And yes, it works as expected:</p>\n\n<pre><code>     &lt;script&gt;$.ready()&lt;/script&gt;\n&lt;/body&gt;\n</code></pre>\n\n<p>Here are two examples, this one measures the ms spent while waiting for DOMReady:</p>\n\n<p><a href=\"http://jsbin.com/aqifon/10\">http://jsbin.com/aqifon/10</a></p>\n\n<p>As you can see, the DOMReady trigger is very natively slow, the user has to wait for a whole 200-300 milliseconds before the domready script kick in.</p>\n\n<p>Anyway, if we place <code>$.ready()</code> just before closing the <code>BODY</code> tag we get this:</p>\n\n<p><a href=\"http://jsbin.com/aqifon/16\">http://jsbin.com/aqifon/16</a></p>\n\n<p>See the difference? By triggering domready manually, we can cut off 100-300 ms of execution delay. This is a major deal, because we can rely on jQuery to take care of DOM manipulations before we see them.</p>\n\n<p>Now, to a question, I have never seen this being recommended or discussed before, but still it seems like a major performance issue. Everything is about optimizing the code itself, which is good of course, but it is in vain if the execution is delayed for such a long time that the user sees a \"flash of \"unjQueryedContent\".</p>\n\n<p>Any ideas why this is not discussed/recommended more frequently?</p>\n"},{"tags":["iphone","xcode","build","compilation","performance"],"answer_count":8,"favorite_count":7,"up_vote_count":14,"down_vote_count":0,"view_count":4718,"score":14,"question_id":1479085,"title":"How to decrease build times / speed up compile time in XCode?","body":"<p><strong>What strategies can be used in general to decrease build times for any XCode project? I'm mostly interested in XCode specific strategies.</strong> </p>\n\n<p>I'm doing iPhone development using XCode, and my project is slowly getting bigger and bigger. I find the compile / link phases are starting to take more time than I'd like.</p>\n\n<p>Currently, I'm:</p>\n\n<ul>\n<li><p>Using Static Libraries to make it so\nmost of my code doesn't need to be\ncompiled everytime I clean and build\nmy main project</p></li>\n<li><p>Have removed most resources from my\napplication, and test with a hard\ncoded file system path in the iPhone\nsimulator whenever possible so my\nresources don't have to constantly be\npackaged as I make changes to them.</p></li>\n</ul>\n\n<p>I've noticed that the \"Checking Dependencies\" phase seems to take longer than I'd like. Any tips to decrease that as well would be appreciated!</p>\n"},{"tags":["java","performance","collections","guava"],"answer_count":1,"favorite_count":0,"up_vote_count":7,"down_vote_count":5,"view_count":169,"score":2,"question_id":12853375,"title":"ArrayListMultimap Vs ArrayList , which have the high performance?","body":"<p>In my application, I'm using <code>ArrayList(java.util)</code> for storing bulk of customized data and processing. But it causes the process delay when use continuously in a scheduled manner. So I would like to switch to some other.</p>\n\n<p>Does <code>ArrayListMultimap(com.google.common.collect.ArrayListMultimap)</code> have higher performance than <code>ArrayList</code>? Or any other open source collection frameworks that have better performance than <code>ArrayList</code>?</p>\n"},{"tags":["performance","spring","hibernate","unit-testing","dbunit"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":144,"score":2,"question_id":12876278,"title":"Improving performance of database tests using Spring 3.1, Hibernate 4.1, Dbunit, etc","body":"<p>I'm currently starting a new project, and I've got around 190 repository tests. One thing I've noticed - and I am not entirely sure why this happening - is that the integration tests against HSQLDB (2.2.8) are running a lot slower than I think they should be.</p>\n\n<p>I think I've tracked the bottleneck to the insertion of data before each test. For most tests, it ranges from .15 to .38 seconds just to setup the database. This is unacceptable. I would have imagined that an in-memory database would be much faster :(</p>\n\n<p>Here is the database test class that all of my repository tests extend from:</p>\n\n<pre><code>@ContextConfiguration(locations = {\"classpath:applicationContext.xml\"})\n@RunWith(SpringJUnit4ClassRunner.class)\n@TransactionConfiguration(defaultRollback=true)\n@Transactional\npublic abstract class DatabaseTest {\n\n    public static final String TEST_RESOURCES = \"src/test/resources/\";\n\n    @Autowired\n    protected SessionFactory sessionFactory;\n\n    @Autowired\n    protected UserRepository userRepository;\n\n    @Autowired\n    protected DataSource dataSource;\n\n    protected IDatabaseTester databaseTester;\n\n    protected Map&lt;String, Object&gt; jdbcMap;\n    protected JdbcTemplate jdbcTemplate;\n\n    @PostConstruct\n    public void initialize() throws SQLException, IOException, DataSetException {\n        jdbcTemplate = new JdbcTemplate(dataSource);\n\n        setupHsqlDb();\n\n        databaseTester = new DataSourceDatabaseTester(dataSource);\n        databaseTester.setSetUpOperation(DatabaseOperation.CLEAN_INSERT);\n        databaseTester.setTearDownOperation(DatabaseOperation.NONE);\n        databaseTester.setDataSet(getDataSet());\n    }\n\n    @Before\n    public void insertDbUnitData() throws Exception {\n        long time = System.currentTimeMillis();\n\n        databaseTester.onSetup();\n\n        long elapsed = System.currentTimeMillis() - time;\n        System.out.println(getClass() + \" Insert DB Unit Data took: \" + elapsed);\n    }\n\n    @After\n    public void cleanDbUnitData() throws Exception {\n        databaseTester.onTearDown();\n    }\n\n    public IDataSet getDataSet() throws IOException, DataSetException {\n        Set&lt;String&gt; filenames = getDataSets().getFilenames();\n\n        IDataSet[] dataSets = new IDataSet[filenames.size()];\n        Iterator&lt;String&gt; iterator = filenames.iterator();\n        for(int i = 0; iterator.hasNext(); i++) {\n            dataSets[i] = new FlatXmlDataSet(\n                new FlatXmlProducer(\n                    new InputSource(TEST_RESOURCES + iterator.next()), false, true\n                )\n            );\n        }\n\n        return new CompositeDataSet(dataSets);\n    }\n\n    public void setupHsqlDb() throws SQLException {\n        Connection sqlConnection = DataSourceUtils.getConnection(dataSource);\n        String databaseName = sqlConnection.getMetaData().getDatabaseProductName();\n        sqlConnection.close();\n\n        if(\"HSQL Database Engine\".equals(databaseName)) {\n            jdbcTemplate.update(\"SET DATABASE REFERENTIAL INTEGRITY FALSE;\");\n\n            // MD5\n            jdbcTemplate.update(\"DROP FUNCTION MD5 IF EXISTS;\");\n            jdbcTemplate.update(\n                \"CREATE FUNCTION MD5(VARCHAR(226)) \" +\n                    \"RETURNS VARCHAR(226) \" +\n                    \"LANGUAGE JAVA \" +\n                    \"DETERMINISTIC \" +\n                    \"NO SQL \" +\n                    \"EXTERNAL NAME 'CLASSPATH:org.apache.commons.codec.digest.DigestUtils.md5Hex';\"\n            );\n        } else {\n            jdbcTemplate.update(\"SET foreign_key_checks = 0;\");\n        }\n    }\n\n    protected abstract DataSet getDataSets();\n\n    protected void flush() {\n        sessionFactory.getCurrentSession().flush();\n    }\n\n    protected void clear() {\n        sessionFactory.getCurrentSession().clear();\n    }\n\n    protected void setCurrentUser(User user) {\n        if(user != null) {\n            Authentication authentication = new UsernamePasswordAuthenticationToken(user,\n                user, user.getAuthorities());\n\n            SecurityContextHolder.getContext().setAuthentication(authentication);\n        }\n    }\n\n    protected void setNoCurrentUser() {\n        SecurityContextHolder.getContext().setAuthentication(null);\n    }\n\n    protected User setCurrentUser(long userId) {\n        User user = userRepository.find(userId);\n\n        if(user.getId() != userId) {\n            throw new IllegalArgumentException(\"There is no user with id: \" + userId);\n        }\n\n        setCurrentUser(user);\n\n        return user;\n    }\n\n    protected User getCurrentUser() {\n        return (User) SecurityContextHolder.getContext().getAuthentication().getPrincipal();\n    }\n\n}\n</code></pre>\n\n<p>Here is the relevant beans on my application context:</p>\n\n<pre><code>&lt;bean class=\"org.springframework.beans.factory.config.PropertyPlaceholderConfigurer\"&gt;\n    &lt;property name=\"locations\" value=\"classpath:applicationContext.properties\"/&gt;\n&lt;/bean&gt;\n\n&lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"\n      destroy-method=\"close\"&gt;\n    &lt;property name=\"driverClass\" value=\"${database.driver}\"/&gt;\n    &lt;property name=\"jdbcUrl\" value=\"${database.url}\"/&gt;\n    &lt;property name=\"user\" value=\"${database.username}\"/&gt;\n    &lt;property name=\"password\" value=\"${database.password}\"/&gt;\n    &lt;property name=\"initialPoolSize\" value=\"10\"/&gt;\n    &lt;property name=\"minPoolSize\" value=\"10\"/&gt;\n    &lt;property name=\"maxPoolSize\" value=\"50\"/&gt;\n    &lt;property name=\"idleConnectionTestPeriod\" value=\"100\"/&gt;\n    &lt;property name=\"acquireIncrement\" value=\"2\"/&gt;\n    &lt;property name=\"maxStatements\" value=\"0\"/&gt;\n    &lt;property name=\"maxIdleTime\" value=\"1800\"/&gt;\n    &lt;property name=\"numHelperThreads\" value=\"3\"/&gt;\n    &lt;property name=\"acquireRetryAttempts\" value=\"2\"/&gt;\n    &lt;property name=\"acquireRetryDelay\" value=\"1000\"/&gt;\n    &lt;property name=\"checkoutTimeout\" value=\"5000\"/&gt;\n&lt;/bean&gt;\n\n&lt;bean id=\"sessionFactory\"\n      class=\"org.springframework.orm.hibernate4.LocalSessionFactoryBean\"&gt;\n    &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt;\n    &lt;property name=\"mappingResources\"&gt;\n        &lt;list&gt;\n            &lt;value&gt;...&lt;/value&gt;\n        &lt;/list&gt;\n    &lt;/property&gt;\n    &lt;property name=\"namingStrategy\"&gt;\n        &lt;bean class=\"org.hibernate.cfg.ImprovedNamingStrategy\"/&gt;\n    &lt;/property&gt;\n    &lt;property name=\"hibernateProperties\"&gt;\n        &lt;props&gt;\n            &lt;prop key=\"javax.persistence.validation.mode\"&gt;none&lt;/prop&gt;\n\n            &lt;prop key=\"hibernate.dialect\"&gt;${hibernate.dialect}&lt;/prop&gt;\n            &lt;prop key=\"hibernate.hbm2ddl.auto\"&gt;${hibernate.hbm2ddl.auto}\n            &lt;/prop&gt;\n            &lt;prop key=\"hibernate.generate_statistics\"&gt;false&lt;/prop&gt;\n\n            &lt;prop key=\"hibernate.show_sql\"&gt;false&lt;/prop&gt;\n            &lt;prop key=\"hibernate.format_sql\"&gt;true&lt;/prop&gt;\n\n            &lt;prop key=\"hibernate.cache.use_second_level_cache\"&gt;false&lt;/prop&gt;\n            &lt;prop key=\"hibernate.cache.provider_class\"&gt;\n\n            &lt;/prop&gt;\n        &lt;/props&gt;\n    &lt;/property&gt;\n&lt;/bean&gt;\n\n&lt;bean class=\"org.springframework.orm.hibernate4.HibernateExceptionTranslator\"/&gt;\n\n&lt;bean id=\"transactionManager\"\n      class=\"org.springframework.orm.hibernate4.HibernateTransactionManager\"&gt;\n    &lt;property name=\"sessionFactory\" ref=\"sessionFactory\"/&gt;\n&lt;/bean&gt;\n</code></pre>\n\n<p>In order to try and insert less data, I allow each test class to pick a DataSet enum that only loads the data it needs. It's specified like this:</p>\n\n<pre><code>public enum DataSet {\n    NONE(create()),\n    CORE(create(\"core.xml\")),\n    USERS(combine(create(\"users.xml\"), CORE)),\n    TAGS(combine(create(\"tags.xml\"), USERS)),\n</code></pre>\n\n<p>Could this be causing it to run slower rather than faster? The idea is that if I only want the core xml (languages, provinces, etc.), I only have to load those records. I thought this would make the test suite faster, but it's still too slow.</p>\n\n<p><strong>I can save some time by creating a separate xml dataset specifically designed for each test class. This chops out some of the insert statements. But even when I have 20 insert statements in a single xml dataset (thus, the minimum I/O loss other than in-lining the dataset right into java code directly), each test still takes .1 to .15 seconds during the initialization of the database data!</strong> I am in disbelief that it takes .15 seconds to insert 20 records into memory.</p>\n\n<p>In my other project using Spring 3.0 and Hibernate 3.x, it takes 30 milliseconds to insert everything before each test, but it's actually inserting 100 or more rows per test. For the tests that only have 20 inserts, they are flying as if there was no delay at all. This is what I expected. I'm starting to think the problem is with Spring's annotations - or the way I have them setup in my <code>DatabaseTest</code> class. This is basically the only thing different now.</p>\n\n<p>Also, my repositories are using the sessionFactory.getCurrentSession() instead of the HibernateTemplate. This is the first time I started using the annotation-based unit test stuff from Spring, since the Spring test classes are deprecated. Could that be the reason they are going slow?</p>\n\n<p>If there's anything else you need to know to help figure it out, please let me know. I am sort of stumped.</p>\n\n<p>EDIT: I put in the answer. The problem was hsqldb 2.2.x. Reverting to 2.0.0 fixes the problem.</p>\n"},{"tags":["performance","optimization","encoding","bitmap","bitvector"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":56,"score":3,"question_id":12877557,"title":"Efficient way to encode bit-vectors?","body":"<p>Currently using the run length encoding for encoding bit-vectors, and the current run time is \n2log(i), where is the size of the run. Is there another way of doing it to bring it down to log(i)?\nThanks.</p>\n"},{"tags":["sql","performance","standards","aggregate-functions"],"answer_count":1,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":63,"score":5,"question_id":12876873,"title":"Is there a standard for SQL aggregate function calculation?","body":"<p>Is there a standard on SQL implementaton for multiple calls to the same aggregate function in the same query?</p>\n\n<p>For example, consider the following example, based on a popular example schema:</p>\n\n<pre><code>SELECT Customer,SUM(OrderPrice) FROM Orders\nGROUP BY Customer\nHAVING SUM(OrderPrice)&gt;1000\n</code></pre>\n\n<p>Presumably, it takes computation time to calculate the value of SUM(OrderPrice).  Is this cost incurred for each reference to the aggregate function, or is the result stored for a particular query?</p>\n\n<p>Or, is there no standard for SQL engine implementation for this case?</p>\n"},{"tags":["python","performance","redis","generator"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":166,"score":4,"question_id":12868222,"title":"Performance of Redis vs Disk in caching application","body":"<p>I wanted to create a redis cache in python, and as any self respecting scientist I made a bench mark to test the performance.</p>\n\n<p>Interestingly, redis did not fare so well. Either Python is doing something magic (storing the file) or my version of redis is stupendously slow.</p>\n\n<p>I don't know if this is because of the way my code is structured, or what, but I was expecting redis to do better than it did.</p>\n\n<p>To make a redis cache, I set my binary data (in this case, an HTML page) to a key derived from the filename with an expiration of 5 minutes. </p>\n\n<p>In all cases, file handling is done with f.read() (this is ~3x faster than f.readlines(), and I need the binary blob). </p>\n\n<p>Is there something I'm missing in my comparison, or is Redis really no match for a disk? Is Python caching the file somewhere, and reaccessing it every time? Why is this so much faster than access to redis? </p>\n\n<p>I'm using redis 2.8, python 2.7, and redis-py, all on a 64 bit Ubuntu system.</p>\n\n<p>I do not think Python is doing anything particularly magical, as I made a function that stored the file data in a python object and yielded it forever. </p>\n\n<p>I have four function calls that I grouped:</p>\n\n<p>Reading the file X times</p>\n\n<p>A function that is called to see if redis object is still in memory, load it, or cache new file (single and multiple redis instances).</p>\n\n<p>A function that creates a generator that yields the result from the redis database (with single and multi instances of redis).</p>\n\n<p>and finally, storing the file in memory and yielding it forever.</p>\n\n<pre><code>import redis\nimport time\n\ndef load_file(fp, fpKey, r, expiry):\n    with open(fp, \"rb\") as f:\n        data = f.read()\n    p = r.pipeline()\n    p.set(fpKey, data)\n    p.expire(fpKey, expiry)\n    p.execute()\n    return data\n\ndef cache_or_get_gen(fp, expiry=300, r=redis.Redis(db=5)):\n    fpKey = \"cached:\"+fp\n\n    while True:\n        yield load_file(fp, fpKey, r, expiry)\n        t = time.time()\n        while time.time() - t - expiry &lt; 0:\n            yield r.get(fpKey)\n\n\ndef cache_or_get(fp, expiry=300, r=redis.Redis(db=5)):\n\n    fpKey = \"cached:\"+fp\n\n    if r.exists(fpKey):\n        return r.get(fpKey)\n\n    else:\n        with open(fp, \"rb\") as f:\n            data = f.read()\n        p = r.pipeline()\n        p.set(fpKey, data)\n        p.expire(fpKey, expiry)\n        p.execute()\n        return data\n\ndef mem_cache(fp):\n    with open(fp, \"rb\") as f:\n        data = f.readlines()\n    while True:\n        yield data\n\ndef stressTest(fp, trials = 10000):\n\n    # Read the file x number of times\n    a = time.time()\n    for x in range(trials):\n        with open(fp, \"rb\") as f:\n            data = f.read()\n    b = time.time()\n    readAvg = trials/(b-a)\n\n\n    # Generator version\n\n    # Read the file, cache it, read it with a new instance each time\n    a = time.time()\n    gen = cache_or_get_gen(fp)\n    for x in range(trials):\n        data = next(gen)\n    b = time.time()\n    cachedAvgGen = trials/(b-a)\n\n    # Read file, cache it, pass in redis instance each time\n    a = time.time()\n    r = redis.Redis(db=6)\n    gen = cache_or_get_gen(fp, r=r)\n    for x in range(trials):\n        data = next(gen)\n    b = time.time()\n    inCachedAvgGen = trials/(b-a)\n\n\n    # Non generator version    \n\n    # Read the file, cache it, read it with a new instance each time\n    a = time.time()\n    for x in range(trials):\n        data = cache_or_get(fp)\n    b = time.time()\n    cachedAvg = trials/(b-a)\n\n    # Read file, cache it, pass in redis instance each time\n    a = time.time()\n    r = redis.Redis(db=6)\n    for x in range(trials):\n        data = cache_or_get(fp, r=r)\n    b = time.time()\n    inCachedAvg = trials/(b-a)\n\n    # Read file, cache it in python object\n    a = time.time()\n    for x in range(trials):\n        data = mem_cache(fp)\n    b = time.time()\n    memCachedAvg = trials/(b-a)\n\n\n    print \"\\n%s file reads: %.2f reads/second\\n\" %(trials, readAvg)\n    print \"Yielding from generators for data:\"\n    print \"multi redis instance: %.2f reads/second (%.2f percent)\" %(cachedAvgGen, (100*(cachedAvgGen-readAvg)/(readAvg)))\n    print \"single redis instance: %.2f reads/second (%.2f percent)\" %(inCachedAvgGen, (100*(inCachedAvgGen-readAvg)/(readAvg)))\n    print \"Function calls to get data:\"\n    print \"multi redis instance: %.2f reads/second (%.2f percent)\" %(cachedAvg, (100*(cachedAvg-readAvg)/(readAvg)))\n    print \"single redis instance: %.2f reads/second (%.2f percent)\" %(inCachedAvg, (100*(inCachedAvg-readAvg)/(readAvg)))\n    print \"python cached object: %.2f reads/second (%.2f percent)\" %(memCachedAvg, (100*(memCachedAvg-readAvg)/(readAvg)))\n\nif __name__ == \"__main__\":\n    fileToRead = \"templates/index.html\"\n\n    stressTest(fileToRead)\n</code></pre>\n\n<p>And now the results:</p>\n\n<pre><code>10000 file reads: 30971.94 reads/second\n\nYielding from generators for data:\nmulti redis instance: 8489.28 reads/second (-72.59 percent)\nsingle redis instance: 8801.73 reads/second (-71.58 percent)\nFunction calls to get data:\nmulti redis instance: 5396.81 reads/second (-82.58 percent)\nsingle redis instance: 5419.19 reads/second (-82.50 percent)\npython cached object: 1522765.03 reads/second (4816.60 percent)\n</code></pre>\n\n<p>The results are interesting in that a) generators are faster than calling functions each time, b) redis is slower than reading from the disk, and c) reading from python objects is ridiculously fast.</p>\n\n<p>Why would reading from a disk be so much faster than reading from an in-memory file from redis?</p>\n\n<p>EDIT:\nSome more information and tests.</p>\n\n<p>I replaced the function to </p>\n\n<pre><code>data = r.get(fpKey)\nif data:\n    return r.get(fpKey)\n</code></pre>\n\n<p>The results do not differ much from </p>\n\n<pre><code>if r.exists(fpKey):\n    data = r.get(fpKey)\n\n\nFunction calls to get data using r.exists as test\nmulti redis instance: 5320.51 reads/second (-82.34 percent)\nsingle redis instance: 5308.33 reads/second (-82.38 percent)\npython cached object: 1494123.68 reads/second (5348.17 percent)\n\n\nFunction calls to get data using if data as test\nmulti redis instance: 8540.91 reads/second (-71.25 percent)\nsingle redis instance: 7888.24 reads/second (-73.45 percent)\npython cached object: 1520226.17 reads/second (5132.01 percent)\n</code></pre>\n\n<p>Creating a new redis instance on each function call actually does not have a noticable affect on read speed, the variability from test to test is larger than the gain.</p>\n\n<p>Sripathi Krishnan suggested implementing random file reads. This is where caching starts to really help, as we can see from these results.</p>\n\n<pre><code>Total number of files: 700\n\n10000 file reads: 274.28 reads/second\n\nYielding from generators for data:\nmulti redis instance: 15393.30 reads/second (5512.32 percent)\nsingle redis instance: 13228.62 reads/second (4723.09 percent)\nFunction calls to get data:\nmulti redis instance: 11213.54 reads/second (3988.40 percent)\nsingle redis instance: 14420.15 reads/second (5157.52 percent)\npython cached object: 607649.98 reads/second (221446.26 percent)\n</code></pre>\n\n<p>There is a HUGE amount of variability in file reads so the percent difference is not a good indicator of speedup.</p>\n\n<pre><code>Total number of files: 700\n\n40000 file reads: 1168.23 reads/second\n\nYielding from generators for data:\nmulti redis instance: 14900.80 reads/second (1175.50 percent)\nsingle redis instance: 14318.28 reads/second (1125.64 percent)\nFunction calls to get data:\nmulti redis instance: 13563.36 reads/second (1061.02 percent)\nsingle redis instance: 13486.05 reads/second (1054.40 percent)\npython cached object: 587785.35 reads/second (50214.25 percent)\n</code></pre>\n\n<p>I used random.choice(fileList) to randomly select a new file on each pass through the functions. </p>\n\n<p>The full gist is here if anyone would like to try it out - <a href=\"https://gist.github.com/3885957\" rel=\"nofollow\">https://gist.github.com/3885957</a></p>\n\n<p>Edit edit:\nDid not realize that I was calling one single file for the generators (although the performance of the function call and generator was very similar). Here is the result of different files from the generator as well.</p>\n\n<pre><code>Total number of files: 700\n10000 file reads: 284.48 reads/second\n\nYielding from generators for data:\nsingle redis instance: 11627.56 reads/second (3987.36 percent)\n\nFunction calls to get data:\nsingle redis instance: 14615.83 reads/second (5037.81 percent)\n\npython cached object: 580285.56 reads/second (203884.21 percent)\n</code></pre>\n"},{"tags":["ios","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":54,"score":0,"question_id":12876566,"title":"experiencing heavy performance hit in my iOS app","body":"<p>It is my first iOS app, and I am trying to figure out what I am doing wrong here. My app would go up and would hang for a few seconds until it is responsive. \nThe app would go over the web and bring images from there when it starts up. It then builds views from the images with added text. The function that builds up the views is quite long, but basically it is fetching the images data from the web for every single object.\nI use these methods:</p>\n\n<pre><code>NSURL *url = [NSURL URLWithString: \n   @\"http://mysite.com/images/best_trip_ever.png\"];\nUIImage *image = [UIImage imageWithData: [NSData dataWithContentsOfURL:url]];\n</code></pre>\n\n<p>Since it is happening tens of times when the app starts, I figured this might be one reason for the performance hit.</p>\n\n<p>The views I am creating are made by adding subviews to a view I create. It is happening for every object with the images I fetched from the web.</p>\n\n<p>I also have a <code>table view</code>, for each row I am using the same methods to display a nice row in the table view again from images brought from the web. Something VERY noticeable in the table view is that when I scroll down the cells would get stuck and not move smoothly.</p>\n\n<p>For each object I store the data in an <code>NSData</code> object with <code>encode/decode</code> methods to fetch data and write it back down to the object.</p>\n\n<p>I don't know if it is the bringing of images from the network makes things so slow (which in my opinion shouldn't be THAT slow. It might takes like 7-8 seconds!)\nOr is it the act of building the views from the images.</p>\n\n<p>I don't mind showing the spinner rotating symbol for each image until it is available, providing the app would wake up as fast as possible.</p>\n\n<p>Any ideas?</p>\n"},{"tags":["performance","optimization","dns"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":50,"score":1,"question_id":12874799,"title":"DNS prefetching and page optimization","body":"<p>Today I saw this snippet in the HTML source of a webpage:</p>\n\n<pre><code>&lt;!-- prefetch dns --&gt;\n&lt;link rel=\"dns-prefetch\" href=\"//s3.amazonaws.com\"&gt;\n&lt;link rel=\"dns-prefetch\" href=\"//cdn.api.twitter.com\"&gt;\n&lt;link rel=\"dns-prefetch\" href=\"//graph.facebook.com\"&gt;\n&lt;link rel=\"dns-prefetch\" href=\"//connect.facebook.net\"&gt;\n&lt;link rel=\"dns-prefetch\" href=\"//api.pinterest.com\"&gt;\n&lt;link rel=\"dns-prefetch\" href=\"//google-analytics.com\"&gt;\n</code></pre>\n\n<p>How much can you gain by doing this? I haven't seen this before, nor in the <a href=\"http://developer.yahoo.com/performance/rules.html\" rel=\"nofollow\">Yahoo! Developer Networks guidelines for optimization</a>. The only thing that seems related is \"Reduce DNS Lookups\". </p>\n\n<p>In a similar fashion, why doesn't these services expose an IP address to their services and avoid the DNS look-up altogether? </p>\n"},{"tags":["c++","performance","vector","comparison"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":83,"score":1,"question_id":12874517,"title":"Compare vector of vectors","body":"<p>I have a vector of vectors that store pointers. Currently I iterate over them and compare each pointer and if I find ones that are not equal then vectors also do not equal, but I wonder if it is the right way to do such a thing. </p>\n\n<p>UPD: <code>std::vector&lt;std::vector&lt;Combination*&gt; &gt; combinations;</code> </p>\n"},{"tags":["php","jquery","ajax","performance","firebug"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":77,"score":0,"question_id":12873931,"title":"Ajax request waiting time","body":"<p>I've been working on a project that required me to use Ajax. In the past I have had no problems creating a project with it, but this time every request takes at least 1 second, which is ofcourse way too long.</p>\n\n<p>I can't give you the entire code, but I'll share as much as possible. The requests are as follows:</p>\n\n<pre><code>/* *\n * Update the navigation screen\n */\n    function UpdateNavigation() {\n         $.ajax({\n            type: \"POST\",\n            url: \"application/controllers/LocationController.php\",\n            dataType: \"json\",\n            data: \"action=GetSurroundings\",\n            success: function(data){\n                $(\"#direction-north\").html((typeof data.north != 'undefined' ? data.north : \"\") + '&lt;/br&gt; North');\n                $(\"#direction-west\").html((typeof data.west != 'undefined' ? data.west : \"\") + '&lt;/br&gt; West');\n                $(\"#direction-center\").html((typeof data.center != 'undefined' ? data.center : \"\") + '&lt;/br&gt; Center');\n                $(\"#direction-east\").html((typeof data.east != 'undefined' ? data.east : \"\") + '&lt;/br&gt; East');\n                $(\"#direction-south\").html((typeof data.south != 'undefined' ? data.south : \"\") + '&lt;/br&gt; South');\n            }\n        });\n    }\n\n/* *\n * Update the current location\n */\n    $('#navigation-list :button').click(function(event) {\n        if (event.target.id == \"direction-center\")\n            return;\n\n         $.ajax({\n            type: \"POST\",\n            url: \"application/controllers/LocationController.php\",\n            data: \"action=SetLocation&amp;value=\" + event.target.id,\n            success: function() {\n                UpdateNavigation();\n            }\n        });\n    });\n</code></pre>\n\n<p>As far as I know this could is fine. It might be a good idea to change it a little but it shouldn't cause the delay as far as I know. The LocationController file is pretty big, but it uses a switch for different cases so the actual executed code isn't that big.</p>\n\n<p>Using FireBug I found out that it's waiting for 1.01 seconds for the first request, and 1.00 second for the second request. I did some research and people told me that this might be because the server is too busy to handle your request properly so that's why it's taking so long. But that seems unlikely because the code and database are hosted locally. There shouldn't be a conflict in requests either since they are executed one at a time.</p>\n\n<p>I'm at a loss here. I have no idea how to start debugging this problem. Deleting parts of the code didn't help because it would either stop executing alltogether or just take 1+ second. This leads me to believe the code is not the problem, although I could be wrong.</p>\n\n<p>Any help would be greatly apreciated! If you need more information, please don't hesitate to ask.</p>\n\n<p>Edit: Some more digging around leads me to believe the queries inside the code may be at fault? If so, I'm using the following (pretty ugly) queries:</p>\n\n<pre><code>SELECT \n    character_location.block,\n    character_location.location\nFROM\n    character_location\nWHERE\n    character_location.id = 1\nLIMIT\n    1\n\n\n\nSELECT \n    zones.name,\n    zones.location,\n    zones.block\nFROM\n    zones\nWHERE\n    (\n            `zones`.`location` = (\".$getBlock['location'].\" - 1)\n        AND\n            zones.block = '\".$playerBlock.\"'\n    )\nOR\n    (\n            `zones`.`location` = \".$getBlock['location'].\"\n        AND\n            zones.block = '\".$playerBlock.\"'\n    )\nOR\n    (\n            `zones`.`location` = (\".$getBlock['location'].\" + 1)\n        AND\n            zones.block = '\".$playerBlock.\"'\n    )\nOR\n    (\n            `zones`.`location` = \".$getBlock['location'].\"\n        AND\n            zones.block = '\".$playerBlockDown.\"'\n    )\nOR\n    (\n            `zones`.`location` = \".$getBlock['location'].\"\n        AND\n            zones.block = '\".$playerBlockUp.\"'\n    )\nLIMIT\n    5\n\n\nUPDATE \n    character_location\nSET\n    character_location.block = '\" . $targetBlock . \"',\n    `character_location`.`location` = \" . $targetLocation . \"\nWHERE\n    character_location.id = 1\n</code></pre>\n\n<p>Guess which one I think might be causing the problem?</p>\n"},{"tags":["c++","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":4,"view_count":110,"score":-4,"question_id":12873719,"title":"Which is more optimized code?","body":"<pre><code>// (1)\nfor (int iter = 1; iter &lt;= VERTEX_SIZE; iter ++) {\n    if (visit[iter]) continue;\n    dfs(iter);\n}\n\n// (2)\nfor (int iter = 1; iter &lt;= VERTEX_SIZE; iter ++) {\n    if (!visit[iter]) {\n        dfs(iter);\n    }\n}\n</code></pre>\n\n<p>Which code is more optimized? I'm just curious about it.</p>\n"},{"tags":["c#","performance","dblinq"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":12873685,"title":"New Dblinq DataContext instantion gets slower as the number of DLLs referenced in the project increases","body":"<p>I have been using Dblinq(v0.20.0.0) for SQLite database. But initialization(instantiation) of my datacontext(It has only 15 tables/entities) is becoming slower and slower as i reference new DLLs in my project.(I have reached 5 seconds after referencing 30 DLLs)</p>\n\n<p>I have used visual studio profiler to get the following results :</p>\n\n<p><img src=\"http://i.stack.imgur.com/IbumT.png\" alt=\"Test result summary\">\n<img src=\"http://i.stack.imgur.com/tvXNd.png\" alt=\"Test result hot lines\"></p>\n\n<p>I guess dblinq is using too much reflection, I read from <a href=\"http://www.abhisheksur.com/2010/11/reflection-slow-or-faster-demonstration.html\" rel=\"nofollow\">this</a> post that a call to GetCustomAttributes method is very costly. But why the performance constantly deteriorates when i add reference to new DLLs. Does it mean DBLinq is iterating through all available DLLs to get a certain type. </p>\n\n<p>Is there something i can do about it? I am to give up on DBlinq because of this.<br>\nThanks,</p>\n"},{"tags":["java","performance","optimization"],"answer_count":6,"favorite_count":14,"up_vote_count":28,"down_vote_count":0,"view_count":3206,"score":28,"question_id":4019180,"title":"Obsolete Java Optimization Tips","body":"<p>There are number of performance tips made obsolete by Java compiler and especially <a href=\"http://en.wikipedia.org/wiki/Profile-guided_optimization\" rel=\"nofollow\">Profile-guided optimization</a>. For example, these platform-provided optimizations can drastically (according to sources) reduces the cost of virtual function calls. VM is also capable of method inlining, loop unrolling etc.</p>\n\n<p>What are other performance optimization techniques you came around still being applied but are actually made obsolete by optimization mechanisms found in more modern JVMs?</p>\n"},{"tags":["asp.net",".net","asp.net-mvc","performance","profiling"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":86,"score":2,"question_id":12872041,"title":"Monitoring ASP.NET application memory and disk usage","body":"<p>What is the best way to monitor memory/cpu/disk (reads/sec or total reads) utilisation for an ASP.NET (MVC) application (or and app pool). Are there any perf counters that can do that?</p>\n"},{"tags":["android","performance","webview","prefetch"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":51,"score":0,"question_id":12772262,"title":"Android prefetch Webview","body":"<p>I'm creating a \"Daily ....\" application for Android. (Let's say \"Daily flowers\") The idea is: you get a notification each day and when you click it, it will show you a picture of a flower + some text. I'd like to distribute this from a website.</p>\n\n<p>I can just create a notification that and an Intent to an activity with a WebView that does a loadUrl. However, this is far too slow! (takes between 1 and 5 secs depending on the network.)</p>\n\n<p>So I would like to prefetch the \"entire site\" (1 HTML file, 1 image) and then create the notification so the application can show the daily flower in a snappy way.</p>\n\n<p>Can this be done easily? I could do this the hard way with HttpClient etc, but I'd rather use something like WebView.saveState</p>\n"},{"tags":["iphone","objective-c","performance","audio"],"answer_count":4,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":1351,"score":5,"question_id":2761388,"title":"Is Objective C fast enough for DSP/audio programming","body":"<p>I've been making some progress with audio programming for iPhone. Now I'm doing some performance tuning, trying to see if I can squeeze more out of this little machine. Running Shark, I see that a significant part of my cpu power (16%) is getting eaten up by objc_msgSend. I understand I can speed this up somewhat by storing pointers to functions (IMP) rather than calling them using [object message] notation. But if I'm going to go through all this trouble, I wonder if I might just be better off using C++.</p>\n\n<p>Any thoughts on this? </p>\n"},{"tags":["php","mysql","performance","drupal"],"answer_count":5,"favorite_count":9,"up_vote_count":12,"down_vote_count":0,"view_count":1112,"score":12,"question_id":573276,"title":"Scaling Drupal","body":"<p>I am working on a Drupal based site and notice there are a lot of seperate CSS and js files. Wading though some of the code I can also see quite a few cases where many queries are used too.</p>\n\n<p>What techniques have you tried to improve the performance of Drupal and what modules (if any) do you use to improve the performance of Drupal 'out of the box'?</p>\n"},{"tags":["java","performance","pattern-matching"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":51,"score":0,"question_id":12870372,"title":"Efficiency formula code in Pattern matching","body":"<p>I need a formula that will determine the efficiency of the system in pattern matching\nusing time and the number of comparison factors.</p>\n\n<p>Is there any formula that would produce numeric output using these factors?</p>\n"},{"tags":["iphone","objective-c","ios","performance","audio"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":57,"score":0,"question_id":12869960,"title":"Quicker alternative to AVAudioPlayer?","body":"<p>I have an app in which the frame rate slows down dramatically when a sound is played. I am using <code>AVAudioPlayer</code> to play these sounds, and there are many sounds being played within short spaces of time. These sounds are only a matter of kilobytes. Is there an alternative way to play these sounds with much lower performance costs?</p>\n"},{"tags":["c++","performance","compilation","linker"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":57,"score":-1,"question_id":12869894,"title":"Does network drive space affect C++ linking time on linux?","body":"<p>I recently noticed an interesting phenomenon - my linking time for a project linking with several libraries recently had its link time drop by nearly an order of magnitude. It was not a small change, but literally an order of magnitude. This occurred (and is continuing) on a machine which is a lab machine.</p>\n\n<p>This was not caused by any chances to source code (that I am aware of) nor the build method changing.</p>\n\n<p>I am speculating my particular drop was caused by a network drive the libraries are linked on having increased free space but regardless this has caused me to wonder if other things outside code/build systems can significantly affect linking time. I would like to know what factors -- assuming source code/build/computer remains constant -- affect how long linking takes on Linux? </p>\n\n<p>In other words:</p>\n\n<ul>\n<li>Assuming my project source code and build system remains constant on the same machine, what factors can cause large linking time differences, particularly on Linux?</li>\n</ul>\n"},{"tags":["java","performance","java-ee","distributed-transactions","xa"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":63,"score":0,"question_id":12305900,"title":"Performance Overhead of XA Data Sources - Best Practices","body":"<p>I am trying to understand the Impact of XA Datasources on Performance. </p>\n\n<p>In many applications, it happens that not all the transactions need to participate in Distributed Transactions (meaning only a few transactions require to be distributed/participating with other resources). </p>\n\n<p>Is the trade-off of the performance high enough to have two data sources configured (one each for XA and non-XA)? Again, the answer is, it depends on the scenario, but I am looking for \"Best Practices\".</p>\n"},{"tags":["java","performance","tomcat","solr","config"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":794,"score":2,"question_id":5725329,"title":"\"Connection Reset\" occurs in Solr client side","body":"<p>We are encountering a \"connection reset\" error when we call the SOLR server. And our concurrent load is rather small.</p>\n\n<p>Here is the Tomcat connector config for SOLR:</p>\n\n<pre><code>&lt;Connector port=\"8983\" protocol=\"HTTP/1.1\" \n           connectionTimeout=\"20000\" maxThreads=\"40000\" minSpareThreads=\"400\" maxSpareThreads=\"5000\" maxKeepAliveRequests=\"100\" URIEncoding=\"UTF-8\"\n           redirectPort=\"8943\" /&gt;\n</code></pre>\n\n<p>And  here is we got from our SOLR client:</p>\n\n<pre><code>Caused by: org.apache.solr.client.solrj.SolrServerException: java.net.SocketException: Connection reset\nat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:472)\nat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:243)\nat org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:89)\nat org.apache.solr.client.solrj.SolrServer.query(SolrServer.java:122)\n\n... 36 more\nCaused by: java.net.SocketException: Connection reset\nat java.net.SocketInputStream.read(SocketInputStream.java:168)\nat java.io.BufferedInputStream.fill(BufferedInputStream.java:218)\nat java.io.BufferedInputStream.read(BufferedInputStream.java:237)\nat org.apache.commons.httpclient.HttpParser.readRawLine(HttpParser.java:78)\nat org.apache.commons.httpclient.HttpParser.readLine(HttpParser.java:106)\nat org.apache.commons.httpclient.HttpConnection.readLine(HttpConnection.java:1116)\nat org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnectionAdapter.readLine(MultiThreadedHttpConnectionManager.java:1413)\nat org.apache.commons.httpclient.HttpMethodBase.readStatusLine(HttpMethodBase.java:1973)\nat org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1735)\nat org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:1098)\nat org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:398)\nat org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)\nat org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)\nat org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)\n</code></pre>\n\n<p>After trouble shooting by reading through the SOLR client code, we found this may be due to an improper connection timeout setting in SOLR's Tomcat config. We decide to change it to default (infinite timeout). So, my question is, will it bring　out other performance issues when setting this value to infinite?</p>\n"}]}
{"total":25593,"page":12,"pagesize":100,"questions":[{"tags":["performance","aix"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":38,"score":0,"question_id":11028725,"title":"Performance Improvement of AIX application.","body":"<p>I want improve the performance of application running of aix operating system.</p>\n\n<p>please find the details about the application below.</p>\n\n<ol>\n<li>Application coded in c/c++. </li>\n<li>The code is compiled by the gcc  version 4.2.4 compiler.</li>\n<li>The operation system version is AIX 6.1</li>\n</ol>\n\n<p>please suggest some sources for performance tweaks in code  and operating system settings.</p>\n\n<p>note: The cpu usage of the application is very low mostly it is around 30-40.</p>\n\n<p>Thanks in advance.   </p>\n"},{"tags":["sql-server","performance","java-ee","content-management-system"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":55,"score":1,"question_id":12843345,"title":"How to cache a query which is often used?","body":"<p>When there is a web app will query some information frequently, how to improve the performance by cache the query result?\n(The information is like top news in a website and my database is SQL Server 2008, the application is on tomcat.)</p>\n"},{"tags":["java","performance","swing","jtable","java-web-start"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":139,"score":3,"question_id":12864998,"title":"JTable Calls Custom Cell Renderer Method... Continuously","body":"<p>Compilable source can be found at: <a href=\"http://www.splashcd.com/jtable.tar\" rel=\"nofollow\">http://www.splashcd.com/jtable.tar</a></p>\n\n<p>I'm new to the language, so I'm not sure if this is acceptable behavior or not.</p>\n\n<p>I created a JTable to display a row for each message received (it receives about\none every 20 seconds). One of the table columns can contain a large amount of\ntext, so I created a custom cell renderer which word wraps and sets the row\nheight accordingly.</p>\n\n<p>All that works as expected, except that once the table displays its first row,\nit calls the cell renderer about ten times a second... until the user closes the\ntable.</p>\n\n<p>Once I get approx 20 rows in there, the table gets fairly sluggish, taking 2-8\nseconds to resize a column, scoll up or down, or render a selected row with the\nselected background color.</p>\n\n<p>I inserted a print statement inside the renderer, so I can see how many times\nthe getTableCellRendererComponent method is being called.</p>\n\n<p>I disabled tool tips, and disabled all cell editing. I do have a listener that\nscrolls the view to the last row when either a new row is added or the table is\nresized.</p>\n\n<p>Should the getTableCellRendererComponent method be called several times a second\nwhen I'm just viewing the screen (not touching mouse or keyboard)?</p>\n\n<p>TIA</p>\n"},{"tags":["performance","optimization","language-agnostic"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":52,"score":0,"question_id":12868499,"title":"When does it makes sense to move a length getter out of the loop?","body":"<p>Consider the following (not in any particular language):</p>\n\n<pre><code>for (i=0; i&lt;list.length(); i++) { ... }\n</code></pre>\n\n<p>Some people prefer to rewrite it as:</p>\n\n<pre><code>int len = list.length()\nfor (i=0; i&lt;len; i++) { ... }\n</code></pre>\n\n<p>This would make sense if getting the length via <code>list.length()</code> was anything other than O(1). But I don't see any reason why this would be the case. Regardless of the data type, it should be trivial to add a length field somewhere and update it whenever the size changes.</p>\n\n<p>Is there a common data type where getting or updating the length is not O(1)? Or is there another reason why someone would want to do that? </p>\n"},{"tags":["performance","optimization","lua","tips-and-tricks"],"answer_count":4,"favorite_count":4,"up_vote_count":5,"down_vote_count":0,"view_count":2563,"score":5,"question_id":154672,"title":"What can I do to increase the performance of a Lua program?","body":"<p>I asked a question about Lua perfromance, and on of the <a href=\"http://stackoverflow.com/questions/124455/how-do-you-pre-size-an-array-in-lua#152894\">responses</a> asked:</p>\n\n<blockquote>\n  <p>Have you studied general tips for keeping Lua performance high? i.e. know table creation and rather reuse a table than create a new one, use of 'local print=print' and such to avoid global accesses.</p>\n</blockquote>\n\n<p>This is a slightly different question from <a href=\"http://stackoverflow.com/questions/89523/lua-patternstips-and-tricks\">Lua Patterns,Tips and Tricks</a> because I'd like answers that specifically impact performance and (if possible) an explanation of why performance is impacted.</p>\n\n<p>One tip per answer would be ideal.</p>\n"},{"tags":["performance","performancecounter","perfmon","performance-monitor"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":33,"score":1,"question_id":12868345,"title":"Capture All Windows Performance Monitor Counter","body":"<p>I need to monitor the performance of some servers and thus need to create perfmon counters for that.</p>\n\n<p>At the moment I am having to add the counters manually to create the data collector and this is taking a long time to do as the servers have different customised counters.</p>\n\n<p>Is there an easy way to select all of the counters and collect them instead of selecting and adding one by one?</p>\n\n<p>Thanks\nRikesh</p>\n"},{"tags":["mysql","sql","performance","database-design","optimization"],"answer_count":3,"favorite_count":8,"up_vote_count":10,"down_vote_count":0,"view_count":526,"score":10,"question_id":10731333,"title":"Steps to design a well organized and normalized Relational Database","body":"<p>I just started making a database for my website so I am re-reading <code>Database Systems - Design, Implementation and Management (9th Edition)</code>but i notice there is no single step by step process described in the book to create a well organized and normalized database. The book seems to be a little all over the place and although the normalization process is all in one place the steps leading up to it are not. </p>\n\n<p>I thought it be very usefull to have all the steps in one list but i cannot find anything like that online or anywhere else. I realize the answerer explaining all of the steps would be quite an extensive one but anything i can get on this subject will be greatly appreciated; including the order of instructions before normalization and links with suggestions.</p>\n\n<p>Although i am semi familiar with the process i took a long break (about 1 year) from designing any databases so i would like everything described in detail. </p>\n\n<p>I am especially interested in:</p>\n\n<ul>\n<li>Whats a good approach to begin modeling a database (or how to list business rules so its not confusing) </li>\n</ul>\n\n<p>I would like to use ER or EER (extended entity relationship model) and I would like to know</p>\n\n<ul>\n<li>how to model subtypes and supertypes correctly using EER(disjoint and overlapping) (as well as writing down the business rules for it so you know that its a subtype if there is any common way of doing that) </li>\n</ul>\n\n<p>(I allready am familiar with the normalization process but an answer can include tips about it as well)</p>\n\n<p><strong>Still need help with:</strong></p>\n\n<ul>\n<li>Writing down business rules (including business rules for subtypes and super types in EER)</li>\n<li>How to use subtypes and super-types in EER correctly (how to model them) </li>\n</ul>\n\n<p>Any other suggestions will be appreciated. </p>\n"},{"tags":["performance","query","innodb","longtext"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":64,"score":0,"question_id":12863549,"title":"query very slow longtext field innodb table","body":"<p>Well, firts of all, sorry by my english. I try to do a query in a table that the users can include some text, like a blog page. The users can design the content in a html format. In my table it is stored like this:</p>\n\n<pre><code>Estad&amp;amp;iacute;sticas&amp;lt;br /&amp;gt;\n&amp;lt;table border=&amp;quot;0&amp;quot;&amp;gt;\n&amp;lt;tbody&amp;gt;\n&amp;lt;tr&amp;gt;\n&amp;lt;td&amp;gt;Columna 1&amp;lt;/td&amp;gt;\n&amp;lt;td&amp;gt;Columna 2&amp;lt;/td&amp;gt;\n&amp;lt;/tr&amp;gt;\n&amp;lt;tr&amp;gt;\n&amp;lt;td&amp;gt;Columna 3&amp;lt;br /&amp;gt;&amp;lt;/td&amp;gt;\n&amp;lt;td&amp;gt;Columna 4&amp;lt;br /&amp;gt;&amp;lt;/td&amp;gt;\n&amp;lt;/tr&amp;gt;\n&amp;lt;/tbody&amp;gt;\n&amp;lt;/table&amp;gt;\n</code></pre>\n\n<p>I must serch in that content all that user's want. The field 'texto' (that I'm using for it) is a longtext field and the table is innodb. I can't use full text search, 'cause it is only for myisam tables. I made the query as:</p>\n\n<pre><code>\"SELECT * FROM texto WHERE texto like '%$variable%'\"\n</code></pre>\n\n<p>but the query is very, very slow, an it take an eternity. The table has a 849 records, that's isn't big. If I write the same query in a phpmyadmin also take a very, very long time. But there are big records in this field, some records have the video html, tables, images, but it's just that, text like the above.</p>\n\n<p>What I can do??? How can improve the performance of the query??? I appreciate all your help. Thanks a lot. And again, sorry for my english.</p>\n"},{"tags":["mysql","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":47,"score":0,"question_id":12866906,"title":"Need performance improvement suggestions for MySQL 5.5 procedures","body":"<p>I am running MySQL 5.5 - WAMP stack. I am having performance issues. I am new to MySQL hence any guidance will be helpful. I am not too much worried about PHP performance at this time, I am mainly worried about MySQL performance.</p>\n\n<p>I have about 20 procedures(~5000 lines of code) being used in calculating an answer to user question. These procedures mainly select data from 4 main tables(each having around 500 rows) and performs CRUD operations on another 7-8 tables(each having around 100 records). These 7-8 tables' data is deleted after responding user with answer. Due to complex nature of the application, these 20 procedures are called several times recursively (depending upon the type of user input), and CRUD operations are performed on these 7-8 tables for about 1000-2000 times. It is taking around 4 seconds to respond with an answer if run from MySQL Workbench, and around 6 seconds if run from PHP.</p>\n\n<p>Is there a better way to manage this as the procedures are called recursively and same 7-8 tables are updated/selected few thousand times?</p>\n\n<p>I tried tweaking several innodb and other parameters but nothing has shown any improvement yet except setting innodb_flush_log_at_trx_commit to 0.</p>\n"},{"tags":["windows-7","ruby-on-rails","git","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":244,"score":1,"question_id":8169902,"title":"windows 7 very slow console actions","body":"<p>Standard console functions work fast, but when I try to use git  or Ruby on Rails console - actions take too long (like git pull origin master - slow on showing \"Enter passphrase\", RoR - run server and all rake db commands). In average RoR commands take more than 1 minute.</p>\n"},{"tags":["wpf","wcf","performance","startup","datacontractserializer"],"answer_count":3,"favorite_count":1,"up_vote_count":8,"down_vote_count":0,"view_count":933,"score":8,"question_id":12835949,"title":"Using WCF from WPF very slow on first use","body":"<p>I have been struggling for a few days with an issue with our WPF applications and I wonder if someone has come across this before and can help?\nThe problem seems to boil down to the client generating \"on-the-fly\" a serializer to handle the types in that web method call. When that method is called for the first time (the web service itself has been running already), it may take e.g. 8 seconds, subsequent calls may take e.g. 20ms. The CPU on the client WPF process is v. high during this delay.</p>\n\n<p>When using the XmlSerializer, there is a way of pre-generating these serializer assemblies, using svcutil. When (as we are) using the normal WCF DataContractSerializer, this option does not seem to be present.</p>\n\n<p>What I would like is to be able to pre-generate this assembly for all types in all my data contracts (a lot) or, alternatively, to replace this process with a custom one that I can code and passes the data in binary (we own both ends of this webservice/client and they are both .NET 4). I have already used BinaryForamtter and GZip compression and while this speeds up the transfer of data, it always gets restored to XML to be de-serialized by the framework, hence this problem remains.</p>\n\n<p>Any ideas?</p>\n"},{"tags":["sql","performance","postgresql","plpgsql","temporary-tables"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":2,"view_count":64,"score":0,"question_id":12856629,"title":"Copying Rows of Table0 to Table2 Where Same Rows Do Not Exist in Table1 (PostgreSQL)","body":"<p>Could anyone please tell me which one of the following is more efficient? I have tens of millions of rows to process, and performance is critical.</p>\n\n<p>In the second example, <code>table0</code> is a temporary table, which seems to be much faster to create than table0 in the first example. (Why?) I couldn't use a temporary table in the first example because the variable row could not be declared before the table is created. (<code>table0</code> holds all distinct rows of the original table, which is not shown in the code below.)</p>\n\n<p>I guess it'd be a good idea to create hash indices for <code>blah2</code>, <code>blah3</code>, <code>blah4</code> and <code>blah5</code> of <code>table1</code> in the second example, though it would then take longer to write onto the table.</p>\n\n<hr>\n\n<pre><code>FOR row IN SELECT * FROM table0\nLOOP\n  IF NOT EXISTS (SELECT 1 FROM table1\n                   WHERE blah2 = row.blah2 AND blah3 = row.blah3\n                     AND blah4 = row.blah4 AND blah5 = row.blah5) THEN\n    INSERT INTO table2\n      (blah0, blah1, blah2, blah3, blah4, blah5)\n      VALUES (row.blah0, row.blah1, row.blah2, row.blah3, row.blah4, row.blah5);\n  END IF;\nEND LOOP;\n</code></pre>\n\n<hr>\n\n<pre><code>INSERT INTO table2\n  (blah0, blah1, blah2, blah3, blah4, blah5)\n  SELECT blah0, blah1, blah2, blah3, blah4, blah5 FROM table0\n    WHERE NOT EXISTS\n      (SELECT 1 FROM table1\n         WHERE table1.blah2 = table0.blah2\n           AND table1.blah3 = table0.blah3\n           AND table1.blah4 = table0.blah4\n           AND talbe1.blah5 = table0.blah5);\n</code></pre>\n"},{"tags":["mysql","performance","query","group-concat"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":58,"score":0,"question_id":12864693,"title":"Mysql, group_concat performance: split huge table","body":"<p>I've a huge table (231,451,584 rows, 14 columns, 15.1 GiB size). </p>\n\n<p>The last 3 columns have data that will be \"group_concated\" into 3 different tables (resuming the number of rows to ≈2,500 by eliminate the redundancy of the first 11 columns and merging the values of columns #12 or #13 or #14 into a big comma separated, csv formatted text).</p>\n\n<p>This operation (the group_concat insert on each new table) takes a huge time (≈18.000 seconds per new table).</p>\n\n<p>It should be fast if I split my first table into 3 (same 11 columns first, and just a last one with the different values I want to concat, for each new table)?</p>\n\n<p>I'm asking this here because it takes to long to get a benchmark for my case... </p>\n\n<p>Thanks!</p>\n"},{"tags":["javascript","performance","dom","z-index","reflow"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":54,"score":1,"question_id":12828850,"title":"Avoiding zIndex by appending dom elements","body":"<p>So I'm trying to control the stacking of equally-sized  elements via z-index. \nNow an idea that I came across recently to avoid going through z-indices and improve performance times by hopefully to avoiding browser reflows is instead order layers via the order I append things to the parent.</p>\n\n<p>So if I have a container div that holds all the stacking divs, and linked list that mirrors the order, referencing the stacking divs, then I reorder the divs based on user input. Then instead of updating the z-indices, I would recreate the div element and just append everything in order. So something like this:</p>\n\n<pre><code>var from = nodeBeforeFrom; // Input\nvar target = nodeBeforeTarget; // Input\nvar linkedlist = input; // var linkedlist contains all the stacking divs\nlinkedlist.moveElement(div1, div2); //Move div1 to after div2\nvar container = document.createElement('div');\n\nlinkedlist.reorder; // \n\nvar cur = linkedlist.first;\nwhile (cur.next) {\n  container.appendChild(cur)\n  cur = cur.next;\n}\ndocument.removeChild(oldContainer);\ndocument.appendChild(container);\n// This is meant as pseudocode so forgive an errors in regards to the specifics\n</code></pre>\n\n<p>So my questions are the following:</p>\n\n<ol>\n<li>Would this reduce browser reflows from n reflows to just 1 or 2 (where n is the number of divs)? If I understand it right, changing the z-index of a single element should cause either a browser repaint or a reflow.</li>\n<li>Will the second approach work and stack elements in the order you append them? </li>\n<li>Is there a way to move childs around using the DOM's child node structure already so I don't have to create a separate linked list? I only see removeChild and appendChild functions that I can use at the moment.</li>\n</ol>\n\n<p>And yes performance is an issue since I'm planning on using this for graphics and html5 stuff. So where I can save I would like to save.</p>\n"},{"tags":["javascript","performance","benchmarking"],"answer_count":1,"favorite_count":3,"up_vote_count":3,"down_vote_count":0,"view_count":90,"score":3,"question_id":12863861,"title":"JavaScript - string versus integer key","body":"<p>I was running performance benchmarks for jQuery (don't ask) and discovered something interesting. For some reason it seems that <code>this[0] = element</code> is quite slow compared to <code>this.foo = element</code>. Here is the obligatory <a href=\"http://jsperf.com/string-integer-property\" rel=\"nofollow\">jsPerf case</a>.</p>\n\n<p>Can anybody explain why there is such a performance hit? Is there any way to improve the performance apart from the obvious \"use a string key\"?</p>\n"},{"tags":["javascript","performance","knockout.js"],"answer_count":7,"favorite_count":12,"up_vote_count":16,"down_vote_count":0,"view_count":3556,"score":16,"question_id":9709374,"title":"Knockout.js incredibly slow under semi-large datasets","body":"<p>I'm just getting started with Knockout.js (always wanted to try it out, but now I finally have an excuse!) - However, I'm running into some really bad performance problems when binding a table to a relatively small set of data (around 400 rows or so).</p>\n\n<p>In my model, I have the following code:</p>\n\n<pre><code>this.projects = ko.observableArray( [] ); //Bind to empty array at startup\n\nthis.loadData = function (data) //Called when AJAX method returns\n{\n   for(var i = 0; i &lt; data.length; i++)\n   {\n      this.projects.push(new ResultRow(data[i])); //&lt;-- Bottleneck!\n   }\n};\n</code></pre>\n\n<p>The issue is the <code>for</code> loop above takes about 30 seconds to so with around 400 rows.  However, if I change the code to:</p>\n\n<pre><code>this.loadData = function (data)\n{\n   var testArray = []; //&lt;-- Plain ol' Javascript array\n   for(var i = 0; i &lt; data.length; i++)\n   {\n      testArray.push(new ResultRow(data[i]));\n   }\n};\n</code></pre>\n\n<p>Then the <code>for</code> loop completes in the blink of an eye.  In other words, the <code>push</code> method of Knockout's <code>observableArray</code> object is incredibly slow.</p>\n\n<p>Here is my template:</p>\n\n<pre><code>&lt;tbody data-bind=\"foreach: projects\"&gt;\n    &lt;tr&gt;\n       &lt;td data-bind=\"text: code\"&gt;&lt;/td&gt;\n       &lt;td&gt;&lt;a data-bind=\"projlink: key, text: projname\"&gt;&lt;/td&gt;\n       &lt;td data-bind=\"text: request\"&gt;&lt;/td&gt;\n       &lt;td data-bind=\"text: stage\"&gt;&lt;/td&gt;\n       &lt;td data-bind=\"text: type\"&gt;&lt;/td&gt;\n       &lt;td data-bind=\"text: launch\"&gt;&lt;/td&gt;\n       &lt;td&gt;&lt;a data-bind=\"mailto: ownerEmail, text: owner\"&gt;&lt;/a&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n&lt;/tbody&gt;\n</code></pre>\n\n<p><strong>My Questions:</strong></p>\n\n<ol>\n<li>Is this the right way to bind my data (which comes from an AJAX method) to an observable collection?</li>\n<li>I expect <code>push</code> is doing some heavy re-calc every time I call it, such as maybe rebuilding bound DOM objects.  Is there a way to either delay this recalc, or perhaps push in all my items at once?</li>\n</ol>\n\n<p>I can add more code if needed, but I'm pretty sure this is what's relevant.  For the most part I was just following Knockout tutorials from the site.</p>\n\n<p><strong>UPDATE:</strong></p>\n\n<p>Per the advice below, I've updated my code:</p>\n\n<pre><code>this.loadData = function (data)\n{\n   var mappedData = $.map(data, function (item) { return new ResultRow(item) });\n   this.projects(mappedData);\n};\n</code></pre>\n\n<p>However, <code>this.projects()</code> still takes about 10 seconds for 400 rows.  I do admit I'm not sure how fast this would be <em>without</em> Knockout (just adding rows through the DOM), but I have a feeling it would be much faster than 10 seconds.</p>\n\n<p><strong>UPDATE 2:</strong></p>\n\n<p>Per other advice below, I gave <strong>jQuery.tmpl</strong> a shot (which is natively supported by KnockOut), and this templating engine will draw around 400 rows in just over 3 seconds.  This seems like the best approach, short of a solution that would dynamically load in more data as you scroll.</p>\n"},{"tags":["performance","ruby-on-rails-3","windows-7","cygwin","mingw"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":880,"score":2,"question_id":7276993,"title":"Improve Ruby on Rails Performance Windows 7","body":"<p>I'm pursuing Ruby on Rails development but using windows to perform rake and rails tasks is PAINFULLY slow, but I heard it's quite the opposite on Linux.</p>\n\n<p>I'm using a Netbook (Acer Aspire One 722) for development and using VirtualBox to run Ubuntu is out of the question. Doing the whole dual boot thing is also not an option because I run into severe processor load balancing and heating issues that I really do not have the luxury of time to troubleshoot right now.</p>\n\n<p>What I would like to know is: is there anything I can install or any settings I can change that will give me linux-like speed when performing these rake and rails tasks on windows 7?</p>\n\n<p>I've heard that Cygwin and Mingw are \"linux emulators\", is there any way I could leverage them?</p>\n\n<p>Thanks.</p>\n"},{"tags":["performance","optimization","measurement"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":1749,"score":2,"question_id":1346308,"title":"How do you measure site load time in IE6?","body":"<p>I'm looking for something similar to <a href=\"http://stevesouders.com/hammerhead/\" rel=\"nofollow\">Hammerhead</a>. Currently, I write javascript code to test, and I'd rather just use a tool that I can easily share and has a GUI. </p>\n\n<p><strong>Edit: I'm hoping for something that tracks load events if possible and can easily do repeat tests.</strong></p>\n"},{"tags":["mysql","sql","performance","database-performance","sqlperformance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":72,"score":1,"question_id":12856783,"title":"Best practice with mysql innodb to rename huge table when table with same name already exist","body":"<p>I Use Mysql 5.5..  + INNODB and windows server.</p>\n\n<p>The case(make it simple then real case):</p>\n\n<p>I have 2 tables 1GB with name <code>new_car</code> and <code>car</code> table 1GB.</p>\n\n<p>I to replace <code>car</code> table with <code>new_car</code> table every 10 hours not manually(auto by code) - important to do it fast(real website data).</p>\n\n<p>I read(say that drop its problem in innodb) :<a href=\"http://www.mysqlperformanceblog.com/2011/02/03/performance-problem-with-innodb-and-drop-table/\" rel=\"nofollow\">http://www.mysqlperformanceblog.com/2011/02/03/performance-problem-with-innodb-and-drop-table/</a></p>\n\n<p>solution1:</p>\n\n<pre><code>DROP TABLE car;\nRENAME TABLE new_car TO car;\n</code></pre>\n\n<p>Solution2(making drop in the end -maybe it not block the table to access that happen during drop):</p>\n\n<pre><code>RENAME TABLE car TO temp_car;\nRENAME TABLE new_car TO car;\nDROP TABLE temp_car;\n</code></pre>\n\n<p>Solution3(Truncate delete fast the table and create empty table then maybe drop action after should be very fast):</p>\n\n<pre><code>TRUNCATE TABLE car;\nDROP TABLE car;\nRENAME TABLE new_car TO car; \n</code></pre>\n\n<p>Solution4:</p>\n\n<pre><code>RENAME TABLE car TO temp_car;\nRENAME TABLE new_car TO car;\nTRUNCATE TABLE temp_car;\nDROP TABLE temp_car;\n</code></pre>\n\n<p>Which solution is the best and why or please write other better solution?</p>\n\n<p>Thanks</p>\n"},{"tags":["java","android","performance","bitmap"],"answer_count":1,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":4598,"score":2,"question_id":4715840,"title":"improving speed of getpixel() and setpixel() on Android Bitmap","body":"<p>All,</p>\n\n<p>after I noticed how slow getPixel and setPixel are (not sure which one, guess both are not turbocharged) I quickly coded a container for Bitmap that uses int[] array to handle bitmap operations. </p>\n\n<p>Already - its noticeably faster, but this is not enough. Please could you advice how to speed it further? </p>\n\n<p>My idea is to keep track of what is made \"dirty\" by the setPixel functions and update only this part of Bitmap when getBitmap() is called ... not clear how to set the setPixels parameters though (something with offset and stride I guess). </p>\n\n<p>Also - any faster recipe? </p>\n\n<p>Thanks for all help in advance!</p>\n\n<pre><code>import android.graphics.Bitmap;\n\npublic class DrawableBitmapContainer {\nprivate Bitmap image;\nprivate int width, height;\nprivate int[]  pixels;\npublic DrawableBitmapContainer(Bitmap _source ){\n    image = _source;\n    width = image.getWidth();\n    height = image.getHeight();\n    pixels = new int[width*height];\n    image.getPixels(pixels,0,width,0,0,width,height);\n}\npublic int getPixel(int x,int y){\n    return pixels[x+y*width];\n}\npublic void setPixel(int x,int y, int color){\n    pixels[x+y*width]=color;\n}\npublic Bitmap getBimap(){\n    image.setPixels(pixels,0,width,0,0,width,height);\n    return image;\n}\npublic int getWidth(){\n    return image.getWidth();\n}\npublic int getHeight(){\n    return image.getHeight();\n}\n}\n</code></pre>\n"},{"tags":["iphone","objective-c","ios","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":69,"score":3,"question_id":12467009,"title":"How can I speed up the loading of images from a web service?","body":"<p>I'm new to iPhone development. In my application, I had kept a scrollview and I have loaded the images from web service using JSON parsing. Unfortunately, it is taking too much time to load them. Do you have any suggestions on how to speed up the (down)loading?</p>\n"},{"tags":["python","performance","nlp","text-processing","nltk"],"answer_count":3,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":385,"score":1,"question_id":6621666,"title":"Real time text processing using Python","body":"<p>Real time text processing using Python. For e.g. consider this sentance</p>\n\n<pre>\nI am going to schol today\n</pre>\n\n<p>I want to do the following (real time):</p>\n\n<pre>\n1) tokenize \n2) check spellings\n3) stem(nltk.PorterStemmer()) \n4) lemmatize (nltk.WordNetLemmatizer())\n</pre> \n\n<p>Currently I am using <a href=\"http://www.nltk.org/\" rel=\"nofollow\">NLTK</a> library to do these operations, but its not real time (meaning its taking few seconds to complete these operations). I am processing 1 sentence at a time, Is it possible to make it efficient</p>\n\n<p>Update:\nProfiling: </p>\n\n<pre>\nFri Jul  8 17:59:32 2011    srj.profile\n\n         105503 function calls (101919 primitive calls) in 1.743 CPU seconds\n\n   Ordered by: internal time\n   List reduced from 1797 to 10 due to restriction \n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n     7450    0.136    0.000    0.208    0.000 sre_parse.py:182(__next)\n  602/179    0.130    0.000    0.583    0.003 sre_parse.py:379(_parse)\n23467/22658    0.122    0.000    0.130    0.000 {len}\n 1158/142    0.092    0.000    0.313    0.002 sre_compile.py:32(_compile)\n    16152    0.081    0.000    0.081    0.000 {method 'append' of 'list' objects}\n     6365    0.070    0.000    0.249    0.000 sre_parse.py:201(get)\n     4947    0.058    0.000    0.086    0.000 sre_parse.py:130(__getitem__)\n 1641/639    0.039    0.000    0.055    0.000 sre_parse.py:140(getwidth)\n      457    0.035    0.000    0.103    0.000 sre_compile.py:207(_optimize_charset)\n     6512    0.034    0.000    0.034    0.000 {isinstance}\n\n</pre>\n\n<p>timit:</p>\n\n<pre>\nt = timeit.Timer(main)\nprint t.timeit(1000)\n\n=> 3.7256231308\n</pre>\n"},{"tags":["c++","string","performance","if-statement","string-length"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":108,"score":1,"question_id":12856288,"title":"Code performance between if else statements and length of string","body":"<p>I'm writing code that takes a number from a user and prints in back in letters as string. I want to know, which is better performance-wise, to have if statements, like </p>\n\n<pre><code>if (n &lt; 100) {\n    // code for 2-digit numbers\n} else if (n &lt; 1000) {\n    // code for 3-digit numbers\n} // etc..\n</code></pre>\n\n<p>or to put the number in a string and get its length, then work on it as a string.</p>\n\n<p>The code is written in C++.</p>\n"},{"tags":["performance","index","sql-server-2008-r2"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":56,"score":0,"question_id":12859636,"title":"Column index not updating in SQL Server 2008","body":"<p>I had asked a question on stackoverflow:\n<a href=\"http://stackoverflow.com/questions/12460602/large-mssql-database-timing-out-php-web-application\">Large SQL Server database timing out PHP web application</a> </p>\n\n<p>The problem was that the query was fast for when a previous date was chosen and slow for a more current date. We fixed it by recreating the index and hence it worked. For a while I thought the index might have gotten corrupted. </p>\n\n<p>Today the issue occurred again (slow) and by recreating it again it ran fast. This was done on the date column which is of type datetime.</p>\n\n<p>Is there a specific reason for this or is the SQL Server 2008 R2 corrupted?</p>\n"},{"tags":["sql-server","performance","query","large-data"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":60,"score":0,"question_id":12460602,"title":"Large SQL Server database timing out PHP web application","body":"<p>We are running a hospital system which is web based created in PHP. The system was initially fast due to small size of the database but now it has become slow.</p>\n\n<p>The following is an example query</p>\n\n<pre><code>select pa.id, pa.date as date, pa.visitno, pa.receiptno, pa.debitnoteno, pad.id as padid,\n pad.serviceid as serviceid, pad.waitno, pa.paytype, s.id as doctorid, s.fullname as\n doctorname, p.id as patientid, p.name as patient, p.regno, p.age, p.gender, p.doc,\n p.department, p.telno, p.address, pa.ins_prov_id, ip.name as provider,\n pa.sicksheet_billcode as billcode, ds.id as serviceid, ds.name as servicename, ds.departid\n as departid, ds.servicetype as servicetype, pad.charge, pad.status as status, ts.id as\n timeslotid, ts.name as timeslot, pad.treatment, sd.anesthesiologist, sd.hospitalcharge,\n sd.anesthcharge from patientappointments as pa\nINNER JOIN patientappdetails as pad ON pa.id = pad.patappid \nINNER JOIN patients as p ON pa.patid = p.id \nINNER JOIN staffs as s ON pad.doctorid = s.id \nLEFT JOIN departmentalservices as ds ON pad.serviceid = ds.id \nLEFT JOIN insproviders as ip ON pa.ins_prov_id = ip.id \nLEFT JOIN timeslots as ts ON pad.timeslotid = ts.id \nLEFT JOIN surgerydetails as sd ON sd.appdetid = pad.id \nwhere 1 = 1 and pa.date &gt;= '01.Jul.2012' and ds.departgroupid = 16 and pad.charge != 0\n</code></pre>\n\n<p>As you can see the size of our queries (call them un-optimized) which shows the patient, doctor, service taken, what time and which ins company he came from. So now we created indexes that did help for a while but now again the speed has become slow. Running the system on localhost results in around 15 secs for the result to appear while on the live system, <strong>it times out</strong>.</p>\n\n<p>Can you suggest any method to improve the speed and exactly how to implement them. </p>\n\n<p>Just FYI, rows in each table are as follows:</p>\n\n<ul>\n<li>patientappdetails - 195k</li>\n<li>patients - 34k</li>\n<li>staffs - 200</li>\n<li>departmentalservices - 700</li>\n<li>insproviders - 2800</li>\n</ul>\n\n<p>Thank you</p>\n"},{"tags":["performance","query","mongodb","limit"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":25,"score":0,"question_id":12859857,"title":"Find the next \"older\" document (performance)","body":"<p>I have a collection of posts (see the pseudo-json :-) ) : </p>\n\n<pre><code>[\n  {\"title\":\"First\",\"created\":\"2012-10-04 01:00:00\"},\n  {\"title\":\"Second\",\"created\":\"2012-10-04 03:30:00\"},\n  {\"title\":\"Third\",\"created\":\"2012-10-02 02:00:00\"}\n]\n</code></pre>\n\n<p>I am in the \"2012-10-04\" posts page and I want to know if there are older posts, just for displaying a \"next page\" button.</p>\n\n<p>I can think about a query like this:</p>\n\n<pre><code>db.posts.find({\"created\":{$lt:ISODate(\"2012-10-04\")} }).limit(1);\n</code></pre>\n\n<p>I still have to try it, but I think it will work... actually my question is:</p>\n\n<p><em>What about its performance? Will it run at the same speed with billions of documents in the collection?</em></p>\n"},{"tags":["python","performance","coding-style","progress-bar","easy-install"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":57,"score":1,"question_id":12859253,"title":"Python: how to use a progressbar inside my function","body":"<p>I am using the following function: </p>\n\n<pre><code>def LAS2TXTGridClip(inFile,poly,MinPoints=1):\n        sf = shapefile.Reader(poly) #open shpfile\n        sr = sf.shapeRecords()\n        poly_filename, ext = path.splitext(poly)\n        inFile_filename = os.path.splitext(os.path.basename(inFile))[0]\n        for i in xrange(len(sr)):\n            verts = np.array(sr[i].shape.points,float)\n            record = sr[i].record[0]\n            inside_points = [p for p in lasfile.File(inFile,None,'r') if pnpoly(p.x, p.y, verts)]\n            if len(inside_points) &gt;= MinPoints:\n                file_out = open(\"{0}_{1}_{2}.txt\".format(poly_filename, inFile_filename, record), \"w\")\n                for p in inside_points:\n                    file_out.write(\"%s %s %s %s %s %s %s %s %s %s %s\" % (p.x, p.y, p.z, p.intensity,p.return_number,p.number_of_returns,p.scan_direction,p.flightline_edge,p.classification,p.scan_angle,record)+ \"\\n\")\n                file_out.close()\n</code></pre>\n\n<p>where <code>for i in xrange(len(sr)):</code> the function will be process several times. The <code>len(sr)</code> is around half million and I wish a insert a progress bar in order to have an idea of the time I need to wait (it's friday). I have the following question:</p>\n\n<ol>\n<li>Which is the \"best and easy\" progressbar for python 27 on windows OS\n64bit?</li>\n<li>I found <a href=\"http://pypi.python.org/pypi/progressbar/2.2\" rel=\"nofollow\">progressbar module</a> but I have problem to use\neasy_install progressbar after this step.</li>\n<li>where is the best position to insert the progressbar?</li>\n</ol>\n"},{"tags":["performance","drupal","memory","memory-limit","ini-set"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":31,"score":-1,"question_id":12859159,"title":"Why Drupal doesn't use special Memory Limit (ini_set) in Cache Clearing Script?","body":"<p>In Drupal 7, i have found that <code>CLEAR CACHE</code> (on the Web Panel) is using extreme high Memory resource. Giant projects like mine, needs over 350MB to that process.</p>\n\n<ul>\n<li>Then i set <code>400MB</code> inside <code>php.ini</code></li>\n</ul>\n\n<p>So as far as i have investigated, i have found that having high memory limit (Globally) is totally the <code>damage</code> to the overall Performance because every single script is using that limit in memory.</p>\n\n<ul>\n<li>Then i found, we can use separate/runtime setting as <code>ini_set('memory_limit',____)</code> only for the specific scripts.</li>\n</ul>\n\n<p>So my question here is:</p>\n\n<ul>\n<li>Why Drupal by default is <code>NOT HAVING</code> this <code>ini_set('memory_limit',____)</code> separately in Cache Clearing Script (hardcoded or Panel Setting, etc) ?</li>\n</ul>\n"},{"tags":["php","performance","memory","memory-limit"],"answer_count":3,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":86,"score":4,"question_id":12858457,"title":"Is the more the better for PHP memory_limit?","body":"<p>In PHP, i am oftenly facing memory limit problem. Especially with highly resource integrated systems like Drupal, etc.</p>\n\n<p>What i want to know here is:</p>\n\n<ul>\n<li>Is it good to have very high php memory limit like 2GB?</li>\n<li>Is there any major drawback?</li>\n</ul>\n\n<p><strong>Edited:</strong></p>\n\n<blockquote>\n  <p>As a scenario, for example in Drupal, it NEEDS so much memory while we\n  CLEAR the CACHE via Web Panel (Not by Drush or any script). So even for\n  this case only, i am definitely needing high-limit around 512MB currently.</p>\n</blockquote>\n"},{"tags":["performance","azure","disk","infrastructure"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":130,"score":2,"question_id":11844900,"title":"Is there any data on how fast Azure VM local drives are?","body":"<p>I'm experimenting with <code>OnStart()</code> in my Azure role using \"small\" instances. Turns out it takes about two minutes to unpack a 400 megabytes ZIP file that is located in \"local storage\" on drive D into a folder on drive E.</p>\n\n<p>I though maybe I should do it some other way around but I can't find any data about how fast the local disks on Azure VMs typically are.</p>\n\n<p>Are there any test results for how fast Azure VM local disks are?</p>\n"},{"tags":["mysql","sql","performance","query"],"answer_count":2,"favorite_count":2,"up_vote_count":0,"down_vote_count":0,"view_count":72,"score":0,"question_id":12858224,"title":"Why doesn't this query run?","body":"<p>I have this query that isn't finishing (I think the server runs out of memory)</p>\n\n<pre><code>SELECT fOpen.*, fClose.*\nFROM (\n    SELECT of.*\n    FROM fixtures of\n        JOIN (\n            SELECT MIN(id) id\n            FROM fixtures\n            GROUP BY matchId, period, type\n        ) ofi ON ofi.id = of.id\n) fOpen\nJOIN (\n    SELECT cf.*\n    FROM fixtures cf\n        JOIN (\n            SELECT MAX(id) id\n            FROM fixtures\n            GROUP BY matchId, period, type\n        ) cfi ON cfi.id = cf.id\n) fClose ON fClose.matchId = fOpen.matchId AND fClose.period = fOpen.period AND fClose.type = fOpen.type\n</code></pre>\n\n<p>This is the EXPLAIN of it:</p>\n\n<p><img src=\"http://i.stack.imgur.com/EP83o.png\" alt=\"\"></p>\n\n<p>Those 2 subqueries 'of' and 'cf' take about 1.5s to run, if I run them separately.</p>\n\n<p>'id' is a PRIMARY INDEX and there is a BTREE INDEX named 'matchPeriodType' that has those 3 columns in that order.</p>\n\n<p>More info: MySQL 5.5, 512MB of server memory, and the table has about 400k records.</p>\n"},{"tags":["performance","design","sorting","filter","mapreduce"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":12846878,"title":"How to filter and sort large resultsets in-memory","body":"<p>I have two REST services:</p>\n\n<ol>\n<li>Service-one provides a list of all ids that should not be included in the final result set</li>\n<li>Service-two maintains a list of ids that need to sorted</li>\n</ol>\n\n<p>I need to capture the result (all excluded ids) from service-one and then remove from the list obtained by invoking service 2 and then sort the resultant set.</p>\n\n<p>I need perform this in-memory, the result sets could be huge (20k to 100 k rows, but only ids and a few other columns). I need performance that should be good.</p>\n\n<p>I was wondering if I could employ mapreduce to perform this task, but I am not sure if it can provide the necessary performance. (entire operation has to complete quickly)</p>\n\n<p>Any suggestions/clues about how to approach this problem?</p>\n\n<p>I could regularly invoke the services, store the results and then run a SQL query, but wanted to check if there ways of doing this in memory and dynamically without storing the results </p>\n\n<p>Environment: Windows, ASP.Net Web API for REST Services. Can use No-SQLs such as Mongo, but is there a simpler solution?</p>\n"},{"tags":["c#",".net","performance","datetime","stopwatch"],"answer_count":7,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":718,"score":0,"question_id":1566852,"title":"DateTime.AddDays or new DateTime","body":"<p>I'm creating a list of a month's worth of dates.  I'm wondering what will be more efficient</p>\n\n<pre><code>List&lt;DateTime&gt; GetDates(DateTime StartDay) {\n  List&lt;DateTime&gt; dates = new List&lt;DateTime&gt;();\n  int TotalDays=StartDay.AddMonths(1).AddDays(-1).Day;\n  for (int i=1; i&lt;TotalDays; i++) {\n    dates.Add(new DateTime(StartDay.Year, StartDay.Month, i));\n  }\n  return dates;\n}\n</code></pre>\n\n<p>or</p>\n\n<pre><code>List&lt;DateTime&gt; GetDates(DateTime StartDay) {\n  List&lt;DateTime&gt; dates = new List&lt;DateTime&gt;();\n  DateTime NextMonth = StartDay.AddMonths(1);\n  for (DateTime curr=StartDay; !curr.Equals(NextMonth); curr=curr.AddDays(1)) {\n    dates.Add(curr);\n  }\n  return dates;\n}\n</code></pre>\n\n<p>basically, is new DateTime() or DateTime.addDays more efficient.  </p>\n\n<p>UPDATE:</p>\n\n<pre><code>static void Main(string[] args) {\n  System.Diagnostics.Stopwatch sw=new System.Diagnostics.Stopwatch();\n  long t1, t2, total;\n  List&lt;DateTime&gt; l;\n  DateTime begin = DateTime.Now;\n  total = 0L;\n  for (int i=0; i&lt;10; i++) {\n    sw.Start();\n    l = GetDates(begin);\n    sw.Stop();\n\n\n    sw.Stop();\n    t1 = sw.ElapsedTicks;\n    sw.Reset();\n    sw.Start();\n\n    l = GetDates2(begin);\n    sw.Stop();\n    t2=sw.ElapsedTicks;\n    total +=  t1- t2;\n\n    Console.WriteLine(\"Test {0} : {1} {2} : {3}\", i,t1,t2, t1- t2);\n  }\n  Console.WriteLine(\"Total: {0}\", total);\n\n  Console.WriteLine(\"\\n\\nDone\");\n  Console.ReadLine();\n}\n\nstatic List&lt;DateTime&gt; GetDates(DateTime StartDay) {\n  List&lt;DateTime&gt; dates = new List&lt;DateTime&gt;();\n  int TotalDays=StartDay.AddMonths(10000).AddDays(-1).Day;\n  for (int i=1; i&lt;TotalDays; i++) {\n    dates.Add(new DateTime(StartDay.Year, StartDay.Month, i));\n  }\n  return dates;\n}\n\n\nstatic List&lt;DateTime&gt; GetDates2(DateTime StartDay) {\n  List&lt;DateTime&gt; dates = new List&lt;DateTime&gt;();\n  DateTime NextMonth = StartDay.AddMonths(10000);\n  for (DateTime curr=StartDay; !curr.Equals(NextMonth); curr=curr.AddDays(1)) {\n    dates.Add(curr);\n  }\n  return dates;\n}\n</code></pre>\n\n<pre>\nTest 0 : 2203229 63086205 : -60882976\nTest 1 : 63126483 102969090 : -39842607\nTest 2 : 102991588 93487982 : 9503606\nTest 3 : 93510942 69439034 : 24071908\nTest 4 : 69465137 70660555 : -1195418\nTest 5 : 70695702 68224849 : 2470853\nTest 6 : 68248593 63555492 : 4693101\nTest 7 : 63578536 65086357 : -1507821\nTest 8 : 65108190 64035573 : 1072617\nTest 9 : 64066128 64933449 : -867321\nTotal: -62484058\n\nDone\n</pre>\n\n<p>results are consistently negative... way negative, so, looks like the constructor and integer test is the more efficient method.</p>\n"},{"tags":["python","performance","multiprocessing","multicore"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":56,"score":0,"question_id":12855914,"title":"Python: how to run several scripts (or functions) at the same time under windows 7 multicore processor 64bit","body":"<p>sorry for this question because there are several examples in Stackoverflow. I am writing in order to clarify some of my doubts because I am quite new in Python language. </p>\n\n<p>i wrote a function:</p>\n\n<pre><code>def clipmyfile(inFile,poly,outFile):\n... # doing something with inFile and poly and return outFile\n</code></pre>\n\n<p>Normally I do this:</p>\n\n<pre><code>clipmyfile(inFile=\"File1.txt\",poly=\"poly1.shp\",outFile=\"res1.txt\")\nclipmyfile(inFile=\"File2.txt\",poly=\"poly2.shp\",outFile=\"res2.txt\")\nclipmyfile(inFile=\"File3.txt\",poly=\"poly3.shp\",outFile=\"res3.txt\")\n......\nclipmyfile(inFile=\"File21.txt\",poly=\"poly21.shp\",outFile=\"res21.txt\")\n</code></pre>\n\n<p>I had read in this example <a href=\"http://stackoverflow.com/questions/12126655/run-several-python-programs-at-the-same-time\">Run several python programs at the same time</a> and i can use (but probably i wrong)</p>\n\n<pre><code>from multiprocessing import Pool\np = Pool(21)  # like in your example, running 21 separate processes\n</code></pre>\n\n<p>to run the function in the same time and speed my analysis</p>\n\n<p>I am really honest to say that I didn't understand the next step.</p>\n\n<p>Thanks in advance for help and suggestion\nGianni</p>\n"},{"tags":["php","python","performance","testing","load"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":44,"score":0,"question_id":12856487,"title":"Need load testing tool where we configure the URLs","body":"<p>I require the tool for testing the load of server, I need load testing tool where I can configure the URL's used for testing the load.</p>\n"},{"tags":["jquery","performance","user-interface","position","bounce"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":36,"score":0,"question_id":12856861,"title":"Unexpected behavior of changing position with hover and jQuery UI bounce effect","body":"<p>I've got this jQuery code:</p>\n\n<pre><code>$('.cennik-circle').hover(function() {                  \n    $(this).effect('bounce', { times:1, distance:10  }, 200); \n}, \n\n    function() { }\n);\n</code></pre>\n\n<p>What it does, it gives a bounce effect while hover on .cennik-circle element.\nThis is an example: <a href=\"http://goo.gl/z585f\" rel=\"nofollow\">http://goo.gl/z585f</a></p>\n\n<p>When you hover, for example at one of the white circles you'll see an effect of bounce. But if you try to move mouse cursor fast (over and out) you will see that the circle somehow change its position to left: 0; Why does it happen? What is the reason for this issue? How to avoid it?</p>\n\n<p>Regards, \nDave </p>\n"},{"tags":["php","performance"],"answer_count":7,"favorite_count":18,"up_vote_count":41,"down_vote_count":1,"view_count":9549,"score":40,"question_id":482202,"title":"Is there a performance benefit single quote vs double quote in php?","body":"<p>Are there any performance benefits to using single quotes instead of double quotes in php?</p>\n\n<p>In other words, would there be a performance benefit of:</p>\n\n<pre><code>$foo = 'Test';\n</code></pre>\n\n<p>versus</p>\n\n<pre><code>$foo = \"Test\";\n</code></pre>\n"},{"tags":["iphone","asp.net","performance","jquery-ui","mobile-safari"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":47,"score":1,"question_id":12856535,"title":"Performance issues with asp.net &/or jQuery - Safari iPhone 5","body":"<p>I've built an asp.net (VB) web app at work that uses SQL Server backend for data with stored procedures. The front end is built in VS2008 and also uses bits of jQuery, Dundas charts, JavaScript, CSS etc.</p>\n\n<p>It's largely accessed on desktop machines but also mobile. On the iPad 2 &amp; 3, iPhone 4/4s performance is great and pages load close to instantly. I've just received an iPhone 5, and accessing the site in Safari and Chrome takes between 2x to 5x longer to load, and some image buttons/JavaScript is slow/unresponsive?</p>\n\n<p>Has anybody else run into any problems, or does anyone know a good way I can go about testing what is the cause of the problem, or the best way to develop a workaround?</p>\n\n<p>Thanks in advance (first post but have used the fantastic answers from this site constantly in the last 2 years).</p>\n"},{"tags":["performance","oracle","entity-framework","repository-pattern"],"answer_count":3,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":352,"score":4,"question_id":12177322,"title":"Entity Framework 5 and Oracle: Expression with closure affecting performance when querying against a non-unique indexed field","body":"<p>I have a performance issue with Entity Framework 5 and Oracle DB.</p>\n\n<p>I have a simple SQL select: <code>SELECT * FROM NOTE WHERE NOTENUMBER = '1A23456'</code></p>\n\n<p><code>NOTENUMBER</code> is included in an index on a table called NOTE, but the field is NOT primary key / unique.</p>\n\n<ul>\n<li><p>When I execute the statement with Oracle SQL Developer, results return\nquickly and query plan shows that RANGE SCAN is being used like it\nshould.</p></li>\n<li><p>When I use Entity Framework, the generated SQL takes a lot\nlonger (5 seconds vs. 30ms).</p></li>\n<li><p>When I use Entity Framework and query\nwith a <strong>primary key field</strong> (NOTE_KEY), results return as quickly as with SQL \nDeveloper.</p></li>\n</ul>\n\n<p>I suspect 2 things:</p>\n\n<ul>\n<li><p>There's some problem with EF and Oracle.DataAccess-provider not using the non-unique-index that is available. It would help if I had debug symbols for Entity Framework 5, but I can't find them anywhere.</p></li>\n<li><p>The performance problem is somewhere in EF, regarding closures and/or the way I use generic repository pattern with EF:</p>\n\n<p>If I call my repository like this:<br/>\n<code>var notenumber = \"1A23456\";</code><br />\n<code>var notes = repository.All(n =&gt; n.NOTENUMBER == notenumber).ToList();</code><br>\nThe predicate comes in at the method <code>All</code> as:<br />\n<code>{n =&gt; (n.NOTE == value(Tester.Program+&lt;&gt;c__DisplayClass0).notenumber)}</code><br/>\nAnd EfProf-profiler traces the resulting SQL as:</p>\n\n<p><code>SELECT \"Extent1\".\"NOTE_KEY\"      AS \"NOTE_KEY\",</code><br/>\n<code>\"Extent1\".\"NOTENUMBER\"    AS \"NOTENUMBER\",</code><br/>\n<code>\"Extent1\".\"NOTETEXT\"      AS \"NOTETEXT\",</code><br/>\n<code>FROM   \"NOTE_DBA\".\"NOTE\" \"Extent1\"</code><br/>\n<code>WHERE  (\"Extent1\".\"NOTENUMBER\" = '1PSA0500237500' /* @p__linq__0 */)</code><br/></p>\n\n<p>And the query takes takes <strong>~5500ms</strong>.</p>\n\n<p><br/>\nOn the other hand, if I call my repository like this:<br/>\n<code>var notes = repository.All(n =&gt; n.NOTENUMBER == \"1A23456\").ToList();</code><br/>\nThen the predicate comes in as:<br/>\n<code>{n =&gt; (n.NOTENUMBER == \"1A23456\")}</code><br />\nAnd EfProf-profiler traces the resulting SQL as:</p>\n\n<p><code>SELECT \"Extent1\".\"NOTE_KEY\"      AS \"NOTE_KEY\",</code><br/>\n<code>\"Extent1\".\"NOTENUMBER\"    AS \"NOTENUMBER\",</code><br/>\n<code>\"Extent1\".\"NOTETEXT\"      AS \"NOTETEXT\",</code><br/>\n<code>FROM   \"NOTE_DBA\".\"NOTE\" \"Extent1\"</code><br/>\n<code>WHERE  ('1PSA0500237500' = \"Extent1\".\"NOTENUMBER\")</code><br/></p>\n\n<p>And the query takes <strong>~30ms</strong>.</p>\n\n<p>So the only difference is the order of the condition in the WHERE-clause, and the fact that in the latter there seems to be no parameter replaced by EF</p></li>\n</ul>\n\n<p><br/>\nI use VS2010 and .NET4, and reference EF5 (v4.4.0.0).\nThe repository's All-method is:<br/></p>\n\n<pre><code>public IQueryable&lt;NOTE&gt; All(Expression&lt;Func&lt;NOTE, bool&gt;&gt; predicate = null)\n{\n    var setOfNotes = GetDbSet&lt;NOTE&gt;();\n    var notesQuery = from note in setOfNotes select note;\n    if (predicate != null)\n    {\n        notesQuery = notesQuery.Where(predicate);\n    }\n    return notesQuery;\n}\n</code></pre>\n\n<p>I tried to create a CompiledQuery, I tried using <code>setOfNotes.AsNoTracking()</code> and I tried to target .NET 4.5 - with no difference in performance.<br/></p>\n\n<p>One way I was able to get this particular query fast, was to use Oracle's basic Data Provider for .NET (ODB.NET) and construct the query manually, but I'd rather not stick with that solution. Again, if I use a primary field in the where clause, the query is fast even with EF and the same All-method.</p>\n\n<p>So the problem seems to be somewhere in EF. I feel could find out a lot more if I only had the symbols for EntityFramework.dll.</p>\n\n<p>Could there be a problem with the way EF invokes predicates? How does the '@p_<em>linq</em>_0'-parameter get replaced inside EF?</p>\n"},{"tags":["php","performance","image","curl"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":404,"score":0,"question_id":8146317,"title":"curl_exec vs curl_multi_getcontent","body":"<p>so I'm trying to get the contents of an image (the first 700 bytes of it) from a remote site: </p>\n\n<pre><code>$headers = array(\n\"Range: bytes=0-700\"\n);\n$curl = curl_init($url);\ncurl_setopt($curl, CURLOPT_HTTPHEADER, $headers);\ncurl_setopt($curl, CURLOPT_RETURNTRANSFER, 1);\n$raw = curl_exec($curl);\ncurl_close($curl);\n@$im = imagecreatefromstring($raw);\n</code></pre>\n\n<p>where $url is some remote image....and it works just fine</p>\n\n<p>but then when I use curl_multi_getcontent,</p>\n\n<pre><code>$h = curl_init();\n$headers = array(\n\"Range: bytes=0-700\"\n);\ncurl_setopt($h, CURLOPT_URL, $url);\ncurl_setopt($h, CURLOPT_HEADER, $headers); \ncurl_setopt($h, CURLOPT_RETURNTRANSFER, 1); //return the image value\n\n$mh = curl_multi_init();\ncurl_multi_add_handle($mh, $h);\n\n$running = null;\ndo {\n   curl_multi_exec($mh, $running);\n} while ($running &gt; 0);\n\n$raw = curl_multi_getcontent($h);\n@$im = imagecreatefromstring($raw);\n</code></pre>\n\n<p>PHP will complain at the <code>@$im = imagecreatefromstring($raw);</code> line that Data is not in a recognized format</p>\n\n<p>what did I do wrong? I would like to use the multi exec option since I can then parallelize it....</p>\n\n<p>I also tried changing this line: <code>curl_setopt($h, CURLOPT_HEADER, $headers);</code>  in the multi segment into CURLOPT_HTTPHEADER just like the first one, but instead the connection was reset and once again it was due to imagecreatefromstring since after commenting that line, it didn't get reset</p>\n"},{"tags":["php","arrays","performance","memory","quickhash"],"answer_count":2,"favorite_count":0,"up_vote_count":8,"down_vote_count":0,"view_count":130,"score":8,"question_id":12173669,"title":"Experience with PHP QuickHash for large arrays","body":"<p>Does anyone have any experience with PHP QuickHash (<a href=\"http://php.net/manual/en/book.quickhash.php\" rel=\"nofollow\">http://php.net/manual/en/book.quickhash.php</a>)?</p>\n\n<p>Some early testing shows massive improvements in memory usage for large arrays. An array with 1M items takes 226mb with a normal array and only 41mb with QuickHash and speed is about the same.</p>\n\n<p>However I can hardly find stories from people actually using it in production environment so I'm curious to see if there are any reasons for not using it in production.</p>\n"},{"tags":["performance","silverlight","internet-explorer","google-chrome","browser"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":93,"score":1,"question_id":12838828,"title":"Google Chrome Silverlight Performance Is Significantly Slow","body":"<p>Chrome Silverlight rendering is very slow (estimation: 10 times slower) comparing to firefox and internet explorer.\nYou can test this simply using following sample code. Sample code demonstrates 15000 random data rendering inside a data grid.</p>\n\n<p>Do you have any opinion why this is happening? \nThere are 2 speculations came to my mind.</p>\n\n<ol>\n<li>Google does not want Microsoft's Silverlight gather wide usage on their platform.</li>\n<li>Chrome's infrastructure somehow causes plugin rendering slow.</li>\n</ol>\n\n<p>XAML part:</p>\n\n<pre><code>&lt;UserControl x:Class=\"sample.view.MemberView\"\n    xmlns=\"http://schemas.microsoft.com/winfx/2006/xaml/presentation\"\n    xmlns:x=\"http://schemas.microsoft.com/winfx/2006/xaml\"\n    xmlns:d=\"http://schemas.microsoft.com/expression/blend/2008\"\n    xmlns:mc=\"http://schemas.openxmlformats.org/markup-compatibility/2006\"\n    xmlns:data=\"clr-namespace:System.Windows.Controls;assembly=System.Windows.Controls.Data\"    \n    xmlns:viewmodel=\"clr-namespace:sample.ViewModel\"\n    mc:Ignorable=\"d\"\n    &gt;\n    &lt;UserControl.Resources&gt;\n        &lt;viewmodel:MemberViewModel x:Key=\"viewModel\"/&gt;\n    &lt;/UserControl.Resources&gt;\n    &lt;Grid x:Name=\"LayoutRoot\" VerticalAlignment=\"Stretch\" DataContext=\"{Binding Source={StaticResource viewModel}}\"&gt;\n        &lt;Grid.RowDefinitions&gt;\n            &lt;RowDefinition&gt;&lt;/RowDefinition&gt;\n            &lt;RowDefinition Height=\"35\"&gt;&lt;/RowDefinition&gt;\n        &lt;/Grid.RowDefinitions&gt;\n        &lt;data:DataGrid BorderThickness=\"1\" ItemsSource=\"{Binding Members}\" VirtualizingStackPanel.VirtualizationMode=\"Recycling\"&gt;\n            &lt;data:DataGrid.Columns&gt;\n                &lt;data:DataGridTextColumn Header=\"Name\" Width=\"3*\" Binding=\"{Binding FirstName}\"/&gt;\n                &lt;data:DataGridTextColumn Header=\"Surname\" Width=\"3*\" Binding=\"{Binding LastName}\"/&gt;\n                &lt;data:DataGridTextColumn Header=\"GSM\" Width=\"3*\" Binding=\"{Binding MobilePhone}\"/&gt;\n                &lt;data:DataGridTextColumn Header=\"Email\" Width=\"3*\" Binding=\"{Binding Email}\"/&gt;\n                &lt;data:DataGridTextColumn Header=\"BirthDate\" Width=\"2*\" Binding=\"{Binding BirthDate}}\"/&gt;\n            &lt;/data:DataGrid.Columns&gt;\n        &lt;/data:DataGrid&gt;\n        &lt;StackPanel Orientation=\"Horizontal\" HorizontalAlignment=\"Right\" Grid.Row=\"1\"&gt;\n            &lt;Button Content=\"Get Random Data\" Width=\"120\" Height=\"25\" Margin=\"5\" Command=\"{Binding GetRandomDataCommand}\"/&gt;\n        &lt;/StackPanel&gt;\n    &lt;/Grid&gt;\n&lt;/UserControl&gt;\n</code></pre>\n\n<p>C# Part:</p>\n\n<pre><code>using System;\nusing System.Collections.Generic;\nusing System.Collections.ObjectModel;\nusing System.ComponentModel;\nusing System.Net;\nusing System.Text;\nusing System.Windows;\nusing System.Windows.Browser;\nusing System.Windows.Controls;\nusing System.Windows.Documents;\nusing System.Windows.Ink;\nusing System.Windows.Input;\nusing System.Windows.Media;\nusing System.Windows.Media.Animation;\nusing System.Windows.Shapes;\n\nnamespace sample.ViewModel\n{\n    public class MemberModel\n    {\n        public string FirstName { get; set; }\n\n        public string LastName { get; set; }\n\n        public string MobilePhone { get; set; }\n\n        public string Email { get; set; }\n\n        public DateTime BirthDate { get; set; }\n    }\n\n    public class MemberViewModel\n    {\n        private List&lt;MemberModel&gt; _members;\n        public List&lt;MemberModel&gt; Members\n        {\n            get\n            {\n                return _members;\n            }\n            set\n            {\n                _members = value;\n                RaisePropertyChanged(new PropertyChangedEventArgs(\"Members\"));\n            }\n        }\n        public MyCommand GetRandomDataCommand { get; set; }\n\n        public MemberViewModel()\n        {\n            GetRandomDataCommand = new MyCommand();\n        }\n    }\n\n    public class MyCommand : ICommand\n    {\n        public bool CanExecute(object parameter)\n        {\n            return true;\n        }\n\n        public event EventHandler CanExecuteChanged;\n\n        private static Random random = new Random();\n        public void Execute(object parameter)\n        {\n            List&lt;MemberModel&gt; members = new List&lt;MemberModel&gt;();\n            for (int i = 0; i &lt; 15000; ++i)\n            {\n                MemberModel m = new MemberModel();\n                m.FirstName = GetRandomString(random.Next(5) + 4);\n                m.LastName = GetRandomString(random.Next(5) + 4);\n                m.Email = GetRandomString(random.Next(10) + 4);\n                m.MobilePhone = GetRandomString(random.Next(12));\n                m.BirthDate = DateTime.Now.AddYears(-random.Next(50) - 18).AddMonths(random.Next(12)).AddDays(random.Next(30));\n                members.Add(m);\n            }\n        }\n\n        private string GetRandomString(int length)\n        {\n            StringBuilder srb = new StringBuilder();\n            for (int i = 0; i &lt; length; ++i)\n            {\n                srb.Append((char)(65 + random.Next(29));\n            }\n            return srb.ToString();\n        }\n    }\n\n}\n</code></pre>\n"},{"tags":["java","performance","exception","try-catch"],"answer_count":4,"favorite_count":3,"up_vote_count":4,"down_vote_count":2,"view_count":157,"score":2,"question_id":12853391,"title":"Why adding a try block makes the program faster?","body":"<p>I am using the follow code to test how slow a try block is. To my surprise, the try block makes code faster. Why?</p>\n\n<pre><code>public class Test {\n    int value;\n\n    public int getValue() {\n        return value;\n    }\n\n    public void reset() {\n        value = 0;\n    }\n\n    // Calculates without exception\n    public void method1(int i) {\n        value = ((value + i) / i) &lt;&lt; 1;\n        // Will never be true\n        if ((i &amp; 0xFFFFFFF) == 1000000000) {\n            System.out.println(\"You'll never see this!\");\n        }\n    }\n\n    public static void main(String[] args) {\n        int i;\n        long l;\n        Test t = new Test();\n\n        l = System.currentTimeMillis();\n        t.reset();\n        for (i = 1; i &lt; 100000000; i++) {\n            t.method1(i);\n        }\n        l = System.currentTimeMillis() - l;\n        System.out.println(\"method1 took \" + l + \" ms, result was \"\n                + t.getValue());\n\n        // using a try block\n        l = System.currentTimeMillis();\n        t.reset();\n        for (i = 1; i &lt; 100000000; i++) {\n            try {\n                t.method1(i);\n            } catch (Exception e) {\n\n            }\n        }\n\n        l = System.currentTimeMillis() - l;\n        System.out.println(\"method1 with try block took \" + l + \" ms, result was \"\n                + t.getValue());\n    }\n}\n</code></pre>\n\n<p>My machine is running 64-bit Windows 7 and 64-bit JDK7. I got the following result:</p>\n\n<pre><code>method1 took 914 ms, result was 2\nmethod1 with try block took 789 ms, result was 2\n</code></pre>\n\n<p>And I have run the code many times and every time I got almost the same result.</p>\n"},{"tags":[".net","performance","memory","memory-leaks","profiling"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":58,"score":1,"question_id":12854576,"title":"Memory profiling a .NET 3.5 running process","body":"<p>Is there a tool for attaching to a <em>running</em> process which is in <strong>.NET 3.5</strong> and take a snapshot? </p>\n\n<p>I have looked at <a href=\"http://memprofiler.com/download.aspx\" rel=\"nofollow\">.NET Memory profiler</a>, <a href=\"http://www.jetbrains.com/profiler/download/download_thanks.jsp?os=64\" rel=\"nofollow\">dotTrace memory</a> and <a href=\"http://www.red-gate.com/products/dotnet-development/ants-memory-profiler/?utm_source=google&amp;utm_medium=cpc&amp;utm_content=unmet_need&amp;utm_campaign=antsmemoryprofiler&amp;gclid=CJu10LyD-bICFYTMtAodq0UA7Q\" rel=\"nofollow\">RedGate ANTS profiler</a> and none of them seems to be able to do it.</p>\n\n<p>.NET Memory profiler and ANTS can attach to only to .NET 4.0 processes and dotTrace memory does not seem to be able to attach to running processes.</p>\n\n<hr>\n\n<h1>UPDATE</h1>\n\n<p>OS is Windows Server 2003 (R2).</p>\n"},{"tags":["selenium","performance"],"answer_count":8,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":1274,"score":3,"question_id":1309347,"title":"Performance testing for existing web app - Useful tools?","body":"<p>I'm maintaining a web app that has performance problems.</p>\n\n<p>I want to record a series of actions, then play back those actions once I've made changes and compare page load times so that I can quantify the performance improvement.</p>\n\n<p>The Selenium IDE does what I need for recording and playing back the actions, but I haven't found an easy way to record timings.</p>\n\n<p>Is there a good way to record and compare page load timings using Selenium? Is there a better tool to use instead?</p>\n"},{"tags":["mysql","sql","database","performance","sql-update"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":80,"score":2,"question_id":12838579,"title":"MySQL: How many UPDATES per second can an average box support?","body":"<p>We've got a constant stream of simple updates to a single MySQL table (storing user activity information). Let's say we group these into batch updates each second.</p>\n\n<p>I want a ballpark idea of when mysql on a typical 4-core 8GB box will start having an issue keeping up with the updates coming in each second. E.g. how many rows of updates can I make @ 1 per second?</p>\n\n<p>This is a thought exercise to decide if I should get going with MySQL in the early days of our applications release (simplify development), or if MySQL's likely to bomb so soon as to make it not worth even venturing down that path.</p>\n"},{"tags":["android","performance","listview","background-image"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":70,"score":0,"question_id":12760445,"title":"Android listView really slow with background Image","body":"<p>I've the following problem: I am using a regular ListView with a themed background image which gives me really big performance headaches. When I remove the window (see themes.xml) background image, everything is butter smooth. With the background image it is noticeable slower.</p>\n\n<p>I already tried:</p>\n\n<ul>\n<li>android:cacheColorHint=\"#00000000\"</li>\n<li>listview.setScrollingCacheEnabled(false);</li>\n</ul>\n\n<p>But nothing worked. I don't undretand why this is so slow, because the ListView is just plain white - shouldn't be a problem ...</p>\n\n<p>theme.xml:</p>\n\n<pre><code> &lt;item name=\"android:windowBackground\"&gt;@drawable/window_bg&lt;/item&gt;\n</code></pre>\n\n<p>list.xml</p>\n\n<pre><code>&lt;RelativeLayout \n  xmlns:android=\"http://schemas.android.com/apk/res/android\"\n  android:layout_width=\"fill_parent\"\n  android:layout_height=\"fill_parent\"\n  android:orientation=\"vertical\"\n  android:padding=\"10dp\"&gt;\n  &lt;ListView\n        android:id=\"@+id/my_list\"\n        android:layout_width=\"fill_parent\"\n        android:layout_height=\"fill_parent\"\n        android:background=\"@color/white\"/&gt;\n&lt;/RelativeLayout&gt;\n</code></pre>\n"},{"tags":["performance","select","sqlite3"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":24,"score":0,"question_id":12852489,"title":"Efficient way to find lots of \"most recent\" events in sqlite3","body":"<p>I've got an sqlite3 database that contains events.  Each event is either \"on\" or \"off\" of something happening, and contains the time of the event as well as what the event is and some miscellaneous data which varies by event.</p>\n\n<p>I want to query to find that last event of each type.  So far this is the query I have come up with:</p>\n\n<pre><code>SELECT * from event where name='event1on' or name='event1off' ORDER BY t DESC LIMIT 1\n</code></pre>\n\n<p>This works, but it is slow when I have a lot of events I want to find the latest one of.  I suspect this is because for each SELECT a full scan of the database must be made (several million rows), but I am at a loss to find a more efficient way to do this.</p>\n"},{"tags":["android","performance","image-processing","cursor"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":63,"score":0,"question_id":12853162,"title":"Speeding up a slow app with lots of cursor action and image loading","body":"<p>My app is running a bit too slow in one place.  The rest of the app`s 4 different activities are reasonably fast responding, however one of the activities is too slow for comfort.  From the main activity you press a button and it starts the next activity with a gridView which fills with images and text in each view.   After the button press it takes too long, like 3 to 5 seconds before that gridView loaded page appears. </p>\n\n<p>Testing on an vary slow 600mhz single core processor android phone made by ZTE. One of the low cost phones used by peoples like the Kalahari Bushmen in the South African Desert.  For those that don`t know, it would be an understatement to say that it is not in the same class as a Samsung Galaxy S3 or iPhone 5.</p>\n\n<p>I wonder if I can do something to the code in the slow running portion of the app to give it a kick in the ass for speed.  Any ideas?  The only thing I can see is that the extensive use of multiple cursors and query of databases that may be slowing it down.  I imagine that loading up the gridView takes up some serious jigaboo.</p>\n\n<p>My code is messy, but I wonder if there is something that I can do.  How about doing all the content provider query stuff in a separate thread?  Any ideas?  This code needs an enima.</p>\n\n<p>Here below is sample of the parts of the code that are slowing down the app.</p>\n\n<pre><code>private void init_phone_image_grid() {\n        String[] img = { MediaStore.Images.Media._ID };\n        imagecursor = managedQuery(\n                MediaStore.Images.Media.EXTERNAL_CONTENT_URI, img,  MediaStore.Images.Media.IS_PRIVATE + \"='\" + 1 +\"'\",null, MediaStore.Images.Media._ID);\n        image_column_index = imagecursor.getColumnIndexOrThrow(MediaStore.Images.Media._ID);\n       count = imagecursor.getCount();\n       imagegrid = (GridView) findViewById(R.id.PhoneImageGrid);\n        imagegrid.setAdapter(new ImageAdapter(getApplicationContext()));\n        imagegrid.setOnItemClickListener(new OnItemClickListener() {\n\n            public void onItemClick(AdapterView parent, View v,int position, long id) {\n\n                    System.gc();\n\n                    String[] img = { MediaStore.Images.Media._ID, MediaStore.Images.Media.DATA, MediaStore.Images.Media.TITLE };\n                    imagecursor = managedQuery(\n                            MediaStore.Images.Media.EXTERNAL_CONTENT_URI, img, MediaStore.Images.Media.IS_PRIVATE + \"='\" + 1 +\"'\",null, MediaStore.Images.Media._ID);\n                    image_column_index = imagecursor.getColumnIndexOrThrow(MediaStore.Images.Media.DATA);\n                    count = imagecursor.getCount();\n                    imagecursor.moveToPosition(position);\n\n\n\n                   // String[] proj = {  };\n                   // imagecursor = managedQuery(\n                   //       MediaStore.Images.Media.EXTERNAL_CONTENT_URI, proj,null, null, null);\n                  //  image_column_index = imagecursor.getColumnIndexOrThrow(MediaStore.Images.Media.DATA);\n\n\n                    String i = imagecursor.getString(image_column_index);\n                   // terra = i;\n\n\n\n          // opens larger pic    Intent intent = new Intent(getApplicationContext(), ViewImage.class);\n               /*   intent.putExtra(\"filename\", i);\n                    startActivity(intent); */\n\n                    // passing bitmap image to another activiy and starting that activity\n                //  textone.setText(i); (i == /mnt/sdcard/DCIM/pic06.png)\n                    Intent intentBitmapStart = new Intent(ImageThumbnailsActivity.this, Editor.class);\n                    intentBitmapStart.putExtra(\"filename\", i);\n\n                    startActivity(intentBitmapStart);\n\n                   // Intent myIntent = new Intent(ImageThumbnailsActivity.this, Editor.class);\n                   // ImageThumbnailsActivity.this.startActivity(myIntent);\n              }\n        });\n  }\n</code></pre>\n\n<p>here is a section from the getview of the adapter method.</p>\n\n<pre><code>    public View getView(int position,View convertView,ViewGroup parent) {\n              System.gc();\n\n\n              if (convertView == null) {\n                  // Make up a new view\n                  LayoutInflater inflater = (LayoutInflater) mContext\n                          .getSystemService(Context.LAYOUT_INFLATER_SERVICE);\n                  view = inflater.inflate(R.layout.image_item, null);\n              } else {\n                  // Use convertView if it is available\n                  view = convertView;\n              }\n              PanoramioItem s = mImageManager.get(position);\n\n              ImageView i = (ImageView) view.findViewById(R.id.image);\n              i.setImageBitmap(s.getBitmap());\n              i.setBackgroundResource(R.drawable.picture_frame);\n\n              TextView t = (TextView) view.findViewById(R.id.title);\n              t.setText(s.getTitle());\n\n              t = (TextView) view.findViewById(R.id.owner);\n              t.setText(s.getOwner());\n              return view;\n          }\n\n\n              View view;\n              ImageView iv = new ImageView(mContext.getApplicationContext());\n              if (convertView == null) {\n\n                    LayoutInflater inflater = (LayoutInflater) mContext\n                            .getSystemService(Context.LAYOUT_INFLATER_SERVICE);\n                    view = inflater.inflate(R.layout.activity_adapterview, null);\n                } else {\n                    // Use convertView if it is available\n                view = convertView;\n                }\n\n                  //  Toast.makeText(ImageThumbnailsActivity.this, \"image_column_index2: \" + image_column_index, Toast.LENGTH_SHORT).show();\n\n                   imagecursor.moveToPosition(position);\n                   int id = imagecursor.getInt(image_column_index);\n\n                   Uri uri = Uri.withAppendedPath(MediaStore.Images.Media.EXTERNAL_CONTENT_URI, \"\"+ id);\n\n                    String imagePath = getRealPathFromURI(uri);\n\n                    String[] titleText = { MediaStore.Images.Media.DATA, MediaStore.Images.Media.TITLE };\n\n                    textcursor = managedQuery(\n                    MediaStore.Images.Media.EXTERNAL_CONTENT_URI, titleText, MediaStore.Images.Media.DATA + \"='\" + imagePath +\"'\",null, null);\n                   if(textcursor.moveToFirst()){\n\n                       titleString = (String) textcursor.getString(textcursor.getColumnIndex(MediaStore.Images.Media.TITLE));\n                   textcursor.close();\n                   }\n                   else{\n                       titleString = \"\";\n                   }\n\n                    BitmapFactory.Options bfo = new BitmapFactory.Options();\n                    bfo.inJustDecodeBounds = true;\n\n                   Bitmap bm = BitmapFactory.decodeFile(imagePath, bfo);\n\n                 int  imageHeight = bfo.outHeight;\n                 int  imageWidth = bfo.outWidth;\n                 String  imageType = bfo.outMimeType;\n\n               BitmapFactory.Options bfo2 = new BitmapFactory.Options();\n\n                 int reqHeight = 50;\n                 int reqWidth = 50;\n\n                 if (imageHeight &gt; reqHeight || imageWidth &gt; reqWidth) {\n                     if (imageWidth &gt; imageHeight) {\n                         bfo2.inSampleSize = Math.round((float)imageHeight / (float)reqHeight);\n                     } else {\n                         bfo2.inSampleSize = Math.round((float)imageWidth / (float)reqWidth);\n                     }\n               }  \n\n                 Bitmap bm2 = BitmapFactory.decodeFile(imagePath, bfo2);\n\n                   // iv.setImageBitmap(bm2);\n\n                 ImageView i = (ImageView) view.findViewById(R.id.adapterimageview);\n\n                 i.setImageBitmap(bm2);\n\n\n\n                 TextView t = (TextView) view.findViewById(R.id.adaptertextview);\n                 t.setText(titleString);\n\n\n                 //   iv.setImageURI(Uri.withAppendedPath(MediaStore.Images.Media.EXTERNAL_CONTENT_URI, \"\"+ id));\n                    view.setLayoutParams(new GridView.LayoutParams(103, 103));\n\n\n                    return view;\n\n        }\n</code></pre>\n"},{"tags":["mysql","performance","io","innodb"],"answer_count":0,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":65,"score":1,"question_id":12851720,"title":"mysql IO read rate is very slow","body":"<p>I have a very simple query </p>\n\n<pre><code>select * from ap_statistic\n</code></pre>\n\n<p>running in my servers. The servers have the same hardware and software configuration (CPU 8 core, mem :32G, OS: redhat 5.5, mysql version: 5.1 ) and run the same applications. </p>\n\n<p>In server A, the row number of the table <code>ap_statistic</code> is about 22,512,379, in server B, the row number of the table is 41,438,751. Of course the query running on server A is faster than server B, but what is strange is the query on server B is extreme slow, it takes more than 1 hours where in server A it just takes 10 minutes.</p>\n\n<p>I use some tool to monitor system status and find that when the query is running in server A, system IO read speed is about 20~30M/s, but in server B it's 2~3M/s. I've tried to clean linux cache and restart mysql server, all is the same result. And I tried to restored DB from server B to server A, so the query in Server A is very very slow and io read speed is very slow. I want to know why this happens?</p>\n"},{"tags":["javascript","objective-c","performance","loops","combinations"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":82,"score":0,"question_id":12851723,"title":"Objective-C vs JavaScript loop performance","body":"<p>I have a PhoneGap mobile application that I need to generate an array of match combinations. In JavaScript side, the code hanged pretty soon when the array of which the combinations are generated from got a bit bigger. So, I thought I'll make a plugin to generate the combinations, passing the array of javascript objects to native side and loop it there.</p>\n\n<p>To my surprise the following codes executes in 150 ms (JavaScript) whereas in native side (Objective-C) it takes ~1000 ms.</p>\n\n<p>Does anyone know any tips for speeding up those executing times? When players exceeds 10, i.e. the length of the array of teams equals 252 it really gets slow. Those execution times mentioned above are for 10 players / 252 teams.</p>\n\n<p>Here's the JavaScript code:  </p>\n\n<pre><code>for (i = 0; i &lt; GAME.teams.length; i += 1) {\n            for (j = i + 1; j &lt; GAME.teams.length; j += 1) {\n                t1                  = GAME.teams[i];\n                t2                  = GAME.teams[j];\n\n                if ((t1.mask &amp; t2.mask) === 0) {\n\n                    GAME.matches.push({\n                        Team1: t1,\n                        Team2: t2\n                    });\n                }\n            }\n}\n</code></pre>\n\n<p>... and here's the native code:  </p>\n\n<pre><code>NSArray *teams = [[NSArray alloc] initWithArray: [options objectForKey:@\"teams\"]];\nNSMutableArray *t = [[NSMutableArray alloc] init];\nint mask_t1;\nint mask_t2;\n\nfor (NSInteger i = 0; i &lt; [teams count]; i++) {\n        for (NSInteger j = i + 1; j &lt; [teams count]; j++) {\n\n            mask_t1     = [[[teams objectAtIndex:i] objectForKey:@\"mask\"] intValue];\n            mask_t2     = [[[teams objectAtIndex:j] objectForKey:@\"mask\"] intValue];\n\n            if ((mask_t1 &amp; mask_t2) == 0) {\n                [t insertObject:[teams objectAtIndex:i] atIndex:0];\n                [t insertObject:[teams objectAtIndex:j] atIndex:1];\n                /*\n                NSArray *newCombination = [[NSArray alloc] initWithObjects:\n                                           [teams objectAtIndex:i],\n                                           [teams objectAtIndex:j],\n                                           nil];\n                */\n                [combinations addObject:t];\n            }\n        }\n}\n</code></pre>\n\n<p>... the array in question (GAME.teams) looks like this:  </p>\n\n<pre><code>{\n    count = 2;\n    full = 1;\n    list =         (\n                    {\n            index = 0;\n            mask = 1;\n            name = A;\n            score = 0;\n        },\n                    {\n            index = 1;\n            mask = 2;\n            name = B;\n            score = 0;\n        }\n    );\n    mask = 3;\n    name = A;\n},\n    {\n    count = 2;\n    full = 1;\n    list =         (\n                    {\n            index = 0;\n            mask = 1;\n            name = A;\n            score = 0;\n        },\n                    {\n            index = 2;\n            mask = 4;\n            name = C;\n            score = 0;\n        }\n    );\n    mask = 5;\n    name = A;\n},\n</code></pre>\n"},{"tags":["php","performance","loops","nested-loops"],"answer_count":5,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":79,"score":0,"question_id":12847502,"title":"for loop vs while loop vs foreach loop PHP","body":"<p>1st off I'm new to PHP. I have been using for loop,while loop,foreach loop in scripts. I wonder </p>\n\n<ul>\n<li>which one is better for performance?</li>\n<li>what's the criteria to select a loop?</li>\n<li>which should be used when we loop inside another loop?</li>\n</ul>\n\n<p>the code which I'm stuck with wondering which loop to be used.</p>\n\n<pre><code>for($i=0;$i&lt;count($all);$i++)\n{\n //do some tasks here\n for($j=0;$j&lt;count($rows);$j++)\n {\n  //do some other tasks here \n }\n}\n</code></pre>\n\n<p>It's pretty obvious that I can write the above code using while. Hope someone will help me out to figure out which loop should be better to be used.</p>\n"},{"tags":["c#",".net","winforms","performance","listview"],"answer_count":4,"favorite_count":0,"up_vote_count":4,"down_vote_count":1,"view_count":109,"score":3,"question_id":12831979,"title":"How to speed up Winforms ListView item removal?","body":"<p>I already use <code>listView.BeginUpdate()</code> and <code>listView.EndUpdate()</code>, but it still takes like 10 seconds when I delete for example 100 items out of 25k.</p>\n\n<p>Any ideas, tricks to make it faster?</p>\n\n<p>EDIT:</p>\n\n<pre><code>this.listView.BeginUpdate();\nfor (int i = this.listView.CheckedItems.Count - 1; i &gt; -1; --i)\n{\n    this.listView.CheckedItems[i].Remove();\n}\nthis.listView.EndUpdate();\n</code></pre>\n"},{"tags":["python","c","performance","sockets","scalability"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":75,"score":1,"question_id":12521751,"title":"C or Python for performance and scalability in creating socket connections?","body":"<p>I did see this post but it does not answer my question: <a href=\"http://stackoverflow.com/questions/1909471/c-python-socket-performance\">C/Python Socket Performance?</a></p>\n\n<p>I have been tasked with creating an application that can create thousands of connections based on sockets. I can do this in Python but I want to have room for performance improvements. I know it's possible in Python because of my past projects, but I'm curious how much of a performance improvement this would be if I was to do this project in C (not C++)?</p>\n"},{"tags":["c#","performance","backpropagation"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":51,"score":0,"question_id":12831575,"title":"backpropagation running slow","body":"<p>I've made a Backpropagation RNA using C#, it receives a 50x50 grayscale bitmap and should output 4 bits representing some alphabet characters.\nIt's working too slow, VS2010 says that 40% of the time is taken by feedforward and 50% by error propagation, With 2500 inputs, 2000 hidden units and 4 outputs Im geting about 1 to 3 iterations per second, is it normal?</p>\n\n<p>here is my feedforward:</p>\n\n<pre><code>public void relajacion()\n    {\n        yo[0] = 1;\n        for (int i = 1; i &lt; yo.Length; i++) ocultas\n        {\n            double total = 0;\n            for (int j = 0; j &lt; entrada.Length; j++)\n                total += entrada[j] * wo[j, i];\n            yo[i] = 1.0 / (1.0 + Math.Exp(-total));\n        }\n\n        for (int i = 0; i &lt; ys.Length; i++)\n        {\n            double total = 0;\n            for (int j = 0; j &lt; yo.Length; j++)\n                total += yo[j] * ws[j, i];\n            ys[i] = 1.0 / (1.0 + Math.Exp(-total));\n        }\n    }\n</code></pre>\n"},{"tags":["java","regex","performance","matcher"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":52,"score":1,"question_id":12846866,"title":"java regex to clear mediawiki markup","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/2863272/wikipedia-java-library-to-remove-wikipedia-text-markup-removal\">Wikipedia : Java library to remove wikipedia text markup removal</a>  </p>\n</blockquote>\n\n\n\n<p>I have to clean some content that comes from Confluence, that content is almost clean, however, there are some things like: </p>\n\n<ol>\n<li>[link|]: A link without the url part</li>\n<li>*[link|]*: A link (without the url part) in bold</li>\n<li>*text*: Bolded text</li>\n<li>_*text*_: italic bolded text </li>\n</ol>\n\n<p>And so on.\nI need to write a regex that clean all that, so, i did something like: </p>\n\n<p><code>String wikiCleanMarkupRegex = \"\\\\\\\\[(.*?)[\\\\\\\\|.*?]?\\\\\\\\]|\\\\\\\\*(.*?)\\\\\\\\*|_(.*?)_\";</code></p>\n\n<p>But that doesn't clean everything, I mean, if I give it the link in #2, I will get: </p>\n\n<p>[link|]</p>\n\n<p>Which is not what I want, I want to get \"link\" ... so, I need to reparse the string again and again until no other match is found.</p>\n\n<p>This is really slow because there are millions of records to clean, so, is there any way of doing a regex that does all at once?</p>\n\n<p>Thanks a lot. </p>\n"},{"tags":["performance","iis","asp-classic","parallel-processing"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":62,"score":1,"question_id":12845478,"title":"why classic asp scripts or requests run sequentially for each client","body":"<p>I have noticed that so many times, and i don't know why it works like this\n<img src=\"http://i.stack.imgur.com/klr4x.png\" alt=\"firebug capture\"></p>\n\n<p>for this example :\nit is all written in asp, buffering on, IIS 7, carrier class server, full of RAM and CPU</p>\n\n<p>As you can see on capture, first the main page is executed, then 3 ajax scripts are called, with Jquery.</p>\n\n<p>I had to call the first one before document.ready event to make it answers before the 2 others.</p>\n\n<p>Otherwise, if the 3 are called at the same time,  they won't get answer until the slowest one has finished executing.\nThe 2 lasts are  called at document.ready. We clearly see that the 2 start to respond when execution of the slowest one is over. But the lightest one would be very fast alone, some ms.</p>\n\n<p>This happens too when i download some files from the app. The app sends files using bynarystream method. When a file is downloaded from a browser client, app wont answer to any other browser request(I mean in another tab for example) until download is finished.</p>\n\n<p>The strange thing is that you can open another browser (IE, Chrome...) and the server will answer any request immediately, but will act the same for this new browser. No requests from this browser can be run in parallel.</p>\n\n<p>I know ASP is a old stuff, but can somebody tell me why it works like this ? </p>\n\n<p>Is it due to browser limitations or technology? due to session mechanism? due to IIS ? due to database accesses (I believe no because downloads don't involve DB access when streaming content)  ?\nWould it work in another way for others languages and environment ?</p>\n\n<p>Can i change this behavior ? How ?</p>\n\n<p>Cheers</p>\n\n<p>Fred</p>\n"},{"tags":["c++","c","performance","caching","computer-architecture"],"answer_count":7,"favorite_count":7,"up_vote_count":17,"down_vote_count":1,"view_count":708,"score":16,"question_id":12675092,"title":"Writing a program to get (L1) cache line size","body":"<p>As a school assignment, I need to find a way to get the L1 data cache line size, without reading config files or using api calls. Supposed to use memory accesses read/write timings to analyze &amp; get this info. So how might I do that? </p>\n\n<p>In an incomplete try for another part of the assignment, to find the levels &amp; size of cache, I have: </p>\n\n<pre><code>for (i = 0; i &lt; steps; i++) {\n    arr[(i * 4) &amp; lengthMod]++;\n}\n</code></pre>\n\n<p>I was thinking maybe I just need vary line 2, <code>(i * 4)</code> part? So once I exceed the cache line size, I might need to replace it, which takes sometime? But is it so straightforward? The required block might already be in memory somewhere? Or perpahs I can still count on the fact that if I have a large enough <code>steps</code>, it will still work out quite accurately? </p>\n\n<p><strong>UPDATE</strong></p>\n\n<p><strong><a href=\"https://github.com/jiewmeng/cs3210-assign1/blob/master/cache-l1-line.cpp\">Heres an attempt on GitHub</a></strong>  ... main part below</p>\n\n<pre><code>// repeatedly access/modify data, varying the STRIDE\nfor (int s = 4; s &lt;= MAX_STRIDE/sizeof(int); s*=2) {\n    start = wall_clock_time();\n    for (unsigned int k = 0; k &lt; REPS; k++) {\n        data[(k * s) &amp; lengthMod]++;\n    }\n    end = wall_clock_time();\n    timeTaken = ((float)(end - start))/1000000000;\n    printf(\"%d, %1.2f \\n\", s * sizeof(int), timeTaken);\n}\n</code></pre>\n\n<p>Problem is there dont seem to be much differences between the timing. FYI. since its for L1 cache. I have SIZE = 32 K (size of array)</p>\n"},{"tags":["c","performance","optimization","macros","inline"],"answer_count":9,"favorite_count":3,"up_vote_count":6,"down_vote_count":0,"view_count":3713,"score":6,"question_id":5226803,"title":"Inline function v. Macro in C -- What's the Overhead (Memory/Speed)?","body":"<p>I searched <a href=\"http://stackoverflow.com/\">Stack Overflow</a> for the pros/cons of function-like macros v. inline functions.</p>\n\n<p>I found the following discussion:\n<a href=\"http://stackoverflow.com/questions/1571392/pros-and-cons-of-different-macro-function-inline-methods-in-c\">Pros and Cons of Different macro function / inline methods in C</a></p>\n\n<p>...but it didn't answer my primary burning question.</p>\n\n<p>Namely, what is the overhead in c of using a macro function (with variables, possibly other function calls) v. an inline function, in terms of memory usage and execution speed?</p>\n\n<p>Are there any compiler-dependent differences in overhead?  I have both icc and gcc at my disposal.</p>\n\n<p>My code snippet I'm modularizing is:</p>\n\n<pre><code>double AttractiveTerm = pow(SigmaSquared/RadialDistanceSquared,3);\ndouble RepulsiveTerm = AttractiveTerm * AttractiveTerm;\nEnergyContribution += \n   4 * Epsilon * (RepulsiveTerm - AttractiveTerm);\n</code></pre>\n\n<p>My reason for turning it into an inline function/macro is so I can drop it into a c file and then conditionally compile other similar, but slightly different functions/macros.</p>\n\n<p>e.g.:</p>\n\n<pre><code>double AttractiveTerm = pow(SigmaSquared/RadialDistanceSquared,3);\ndouble RepulsiveTerm = pow(SigmaSquared/RadialDistanceSquared,9);\nEnergyContribution += \n   4 * Epsilon * (RepulsiveTerm - AttractiveTerm);\n</code></pre>\n\n<p>(note the difference in the second line...)</p>\n\n<p>This function is a central one to my code and gets called thousands of times per step in my program and my program performs millions of steps.  Thus I want to have the LEAST overhead possible, hence why I'm wasting time worrying about the overhead of inlining v. transforming the code into a macro.</p>\n\n<p>Based on the prior discussion I already realize other pros/cons (type independence and resulting errors from that) of macros... but what I want to know most, and don't currently know is the PERFORMANCE.</p>\n\n<p>I know some of you C veterans will have some great insight for me!!</p>\n"},{"tags":["performance","visual-studio-2010","msbuild"],"answer_count":4,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":1037,"score":1,"question_id":5298491,"title":"MSBUILD (VS2010) very slow on some machines","body":"<p>I've got a very weird one.</p>\n\n<p>I have about a dozen PC's in our development department that all suffer the same issue:\ncommandline builds using msbuild4.0 (VS2010) are way slower than should be. 4x to 5x slower as expected.</p>\n\n<p>All machines are HP z400 Workstations (quad-core Xeon+hyperthreading, 2.4 GHz, 6GB RAM)\n running Windows 7 Pro 64bit, or HP Elitebook notebooks (Core-i7 4GB RAM) also on Win7 X64.\nIf I take one of those with the vanilla factory pre-installed Win7, install VS2010 and do a build they are as fast as expected. If I install them with our company standard software image, they became 4x to 5x slower on the same VS project.</p>\n\n<p>The same company software image on Lenovo laptops (Core-i5) or desktops (Core-i7) shows no measurable difference between the company image and the factory pre-installed Win7.\nIt's even stranger: If I install VirtualBox with a Win7 image on a HP system with the problem the virtual machine DOESN'T have the problem.</p>\n\n<p>Every benchmark tool I have tried shows no measurable difference between the company image and the pre-installed Win7. Only msbuild is affected and only on the HP machines running Win7 on the company image.</p>\n\n<p>Before you ask: I have disabled all software/background processes in the company image, but that doesn't make any difference. It's obviously not something running in the background that somehow interacts with msbuild. My best guess is that on the HP hardware some settings gets changed that affects msbuild. This doesn't happen on other hardware.\n(And the VS2010 GUI is not used/executed at all. I am aware this can interact with the msbuild if both try top access the same solution/files. Antivirus doesn't make any difference either.) </p>\n\n<p>Anybody have any idea what could possible influence msbuild to slow down?\nAny suggestion, no matter how far-fetched, is welcome.</p>\n"},{"tags":["c++","performance","visual-studio-2008","windows-7","cpu-usage"],"answer_count":4,"favorite_count":2,"up_vote_count":2,"down_vote_count":1,"view_count":223,"score":1,"question_id":12606033,"title":"Computing CPU time in C++ on Windows","body":"<p>Is there any way in C++ to calculate how long does it take to run a given program or routine <strong>in CPU time</strong>?</p>\n\n<p>I work with Visual Studio 2008 running on Windows 7.</p>\n"},{"tags":["java","performance","tomcat","cluster-computing"],"answer_count":5,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":1518,"score":4,"question_id":4182716,"title":"Running Tomcat in production environments","body":"<p>I am currently using Tomcat6 as a Web-Container on development and production. </p>\n\n<p>I have heard that Tomcat is not the best performing Web-Container for production environments. Is this true?</p>\n\n<p><strong>Is Tomcat sufficient in terms of performance and memory management to use on production environments?</strong></p>\n\n<p>Our system/s have around 100 to 400 users.</p>\n\n<p>For me its more about how the Web-App was written and the type of operations inside the Web-App.</p>\n\n<p>Even if performance is a problem on one Tomcat instance, <strong>is it possible to cluster Tomcat?</strong></p>\n\n<p>Thanks.</p>\n"},{"tags":["asp.net","performance","tracing"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":49,"score":3,"question_id":12467349,"title":"ASP.Net MVC Tracing Shows 75 seconds I can't account for","body":"<p>I noticed long render times for a page with a user's account on one of my company's websites. I turned tracing on locally and accessed his account from my development machine, and it as 75 seconds of rendering that I can't find. I added tracing through both the master and child <code>aspx</code> files, but even after the last bit of code (a foreach) in the child page is called, there is another 75 seconds of something happening. </p>\n\n<p>Trace results.</p>\n\n<p>Half a second makes sense (cleaning up something, I don't know what), but 75 makes NO sense.</p>\n\n<p><img src=\"http://i.stack.imgur.com/tuMms.png\" alt=\"Trace Results\"></p>\n"},{"tags":["python","performance","path","split"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":86,"score":1,"question_id":12844235,"title":"Python: elegant way to split a string in order to pick the last element when the len of string is unknown","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/8384737/python-extract-file-name-from-path-no-matter-what-the-os-path-format\">Python, extract file name from path, no matter what the os/path format</a>  </p>\n</blockquote>\n\n\n\n<p>I have a string as:</p>\n\n<pre><code>filename = \"C:\\\\mydata\\\\yourdata\\\\Finaldata.txt\"\n&gt;&gt;&gt; filename\n'C:\\\\mydata\\\\yourdata\\\\Finaldata.txt'\n</code></pre>\n\n<p>i wish to split and pick the last element also when i don't know where is the path. I wrote these lines code</p>\n\n<pre><code>from os import path\npath.splitext(filename)[0].split(\"\\\\\")[len(path.splitext(filename)[0].split(\"\\\\\"))-1]\n'Finaldata'\n</code></pre>\n\n<p>but i am looking if there is an elegant way to do this.\nthanks in advance for any help\nGianni</p>\n"},{"tags":["vb.net","performance","virus","signatures"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":808,"score":1,"question_id":5304657,"title":"Antivirus scan speed optimization","body":"<p>I have been developing an antivirus using vb.net. The virus scanner works fine but I was thinking of ways to optimize the scanning speed (because large files take forever). </p>\n\n<p>The algorithm I'm using to detect the viruses is via binary (converted to hex) signatures. I think I don't have to look around the whole file just to find if it's a virus or not, I think there's a specific place and a specific number of bytes that I should scan instead of scanning the whole file. Anyway, if anyone can provide any help in this subject please do so.</p>\n\n<p>Thanks in advance.</p>\n\n<p>BTW the virus signatures come from the hex collection from the clamAv antivirus...</p>\n"},{"tags":["php","performance","login","amqp"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":37,"score":1,"question_id":12843389,"title":"Slow performance for rabbitmq amqp_login (PHP client)","body":"<p>I'm using php-amqp to read and write from a local rabbitmq server.  This is for a high traffic website.  Following the example at <a href=\"http://code.google.com/p/php-amqp/\" rel=\"nofollow\">http://code.google.com/p/php-amqp/</a>, I haven't found a way to avoid calling amqp_login with every web request.  The call to amqp_login is, by far, the slowest in the sequence.  Is there an easy way to bypass the need to call this with every web request?  We're using Apache on SuSE linux.</p>\n\n<pre><code>$time = microtime(true);\n$connection = amqp_connection_popen('localhost', 5672);\nprint \"connect: \".(microtime(true) - $time) . \"\\n\"; \n\n$time = microtime(true);\namqp_login($connection, 'guest', 'guest', '/');\nprint \"login: \".(microtime(true) - $time) . \"\\n\"; \n\n$time = microtime(true);\n$channel = 1;\namqp_channel_open($connection, $channel);\nprint \"channel open: \".(microtime(true) - $time) . \"\\n\"; \n\n$time = microtime(true);\n$exchange = 'amq.fanout';\namqp_basic_publish($connection, $channel, $exchange, $routing_key, 'junk', $mandatory = false, $immediate = false, array());\nprint \"publish: \".(microtime(true) - $time) . \"\\n\";\n</code></pre>\n\n<p>Example Results:</p>\n\n<pre><code>connect: 0.00019311904907227\nlogin: 0.041217088699341\nchannel open: 0.00034213066101074\npublish: 5.6028366088867E-5\n</code></pre>\n"},{"tags":["android","performance","variables"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":51,"score":1,"question_id":12842331,"title":"Clean declaring and using variables in Android","body":"<p>Every once in a while, entering in a new Activity requires updating values in let's say several <code>TextView</code>s. So let's say I got n <code>String</code>s to write in n <code>TextView</code>s of the starting Activity.</p>\n\n<p>Which one is the best approach to guarantee good performance and providing \"clean code\"?</p>\n\n<p><strong>Variant 1</strong> (The way I actually apply):\nI declare a single <code>TextView</code> variable \"tempText\" as global variable and assign the TextView to update to this variable (alternatively in a extra method).\nAlternatively, a) is doing the whole procedere in the <code>onCreate()</code>, while b) is handle everything in a method called e.g. <code>updateTextViews()</code></p>\n\n<pre><code>(...)\n\npublic class MyActivity extends Activity{\n\n    private TextView tempText;\n\n    public onCreate(Bundle icicle){\n        (...)\n\n        tempText = (TextView) findViewById(R.id.tv_1);\n        tempText.setText(string_1);\n\n        tempText = (TextView) findViewById(R.id.tv_2);\n        tempText.setText(string_2);\n\n        (...)\n\n        tempText = (TextView) findViewById(R.id.tv_n);\n        tempText.setText(string_n);\n    }\n}\n</code></pre>\n\n<p><strong>Variant 2</strong> :\nI declare a single <code>TextView</code> variable \"tempText\" as variable in the <code>onCreate()</code> or the respective method and assign the TextView to update to this variable.\nThe rest is in analogy to Variant 1.</p>\n\n<pre><code>(...)\n\npublic class MyActivity extends Activity{\n\n    public onCreate(Bundle icicle){\n        (...)\n\n        private TextView tempText;\n\n        tempText = (TextView) findViewById(R.id.tv_1);\n        tempText.setText(string_1);\n\n        tempText = (TextView) findViewById(R.id.tv_2);\n        tempText.setText(string_2);\n\n        (...)\n\n        tempText = (TextView) findViewById(R.id.tv_n);\n        tempText.setText(string_n);\n    }\n}\n</code></pre>\n\n<p><strong>Variant 3:</strong>\nI declare a global <code>TextView</code> variable for every <code>TextView</code> to update. This, as far as I know, needs more space in the RAM, but I don't know about the impact to velocity. Also here, are there differences between handling it in the <code>onCreate()</code> (a)) or in a seperate method (b))?</p>\n\n<pre><code>(...)\n\npublic class MyActivity extends Activity{\n\n    private TextView tempText_1;\n    private TextView tempText_2;\n    (...)\n    private TextView tempText_n;\n\n    public onCreate(Bundle icicle){\n        (...)\n\n        tempText_1 = (TextView) findViewById(R.id.tv_1);\n        tempText_1.setText(string_1);\n\n        tempText_2 = (TextView) findViewById(R.id.tv_2);\n        tempText_2.setText(string_2);\n\n        (...)\n\n        tempText_n = (TextView) findViewById(R.id.tv_n);\n        tempText_n.setText(string_n);\n    }\n}\n</code></pre>\n\n<p><strong>Variant 4:</strong>\nI declare a <code>TextView</code> variable for every <code>TextView</code> to update in the <code>onCreate()</code> or the respective method which handles this. The rest is in analogy to Variant 3?</p>\n\n<pre><code>(...)\n\npublic class MyActivity extends Activity{\n\n\n    public onCreate(Bundle icicle){\n        (...)\n\n        private TextView tempText_1;\n        private TextView tempText_2;\n        (...)\n        private TextView tempText_n;\n\n        tempText_1 = (TextView) findViewById(R.id.tv_1);\n        tempText_1.setText(string_1);\n\n        tempText_2 = (TextView) findViewById(R.id.tv_2);\n        tempText_2.setText(string_2);\n\n        (...)\n\n        tempText_n = (TextView) findViewById(R.id.tv_n);\n        tempText_n.setText(string_n);\n    }\n}\n</code></pre>\n\n<p>Which one is the \"best\" method? Variants 1 and 2 provide reserving only one memory address in the RAM and use this, while according to Robert C. Martins \"Clean Code\" the variables are really ambiguous. Option 3 and 4 would be the exact opposite. But for the rest I'm not very conscious of other effects.</p>\n"},{"tags":["windows","performance","powershell","com"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":50,"score":1,"question_id":12841161,"title":"Difficulty creating file system shortcuts in Powershell","body":"<p>I've got a script that is currently using a WScript.Shell COM object to create a shortcut.</p>\n\n<pre><code>$shortcut = (New-Object -ComObject WScript.Shell).Createshortcut(\"$shortcutFolder\\target.lnk\")\n</code></pre>\n\n<p>Unfortunately, setting the target path of the shortcut...:</p>\n\n<pre><code>$shortcut.TargetPath = $targetPath\n</code></pre>\n\n<p>...is taking a very long time (30 to 75 seconds) if it has not been done previously in the current Powershell session. If, however, the command is run again, its time to execute is on the order of milliseconds as it should be.</p>\n\n<p>Is there another, maybe native, way to accomplish this that will work better? Alternatively, is there anything I can do to speed this process up? I've got a Process Monitor trace, but I haven't been able to glean much from it.</p>\n"},{"tags":["sql","sql-server","performance","index"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":70,"score":0,"question_id":12839779,"title":"Execution time after adding indexes","body":"<p>Indexes increase data access performance. I tried creating indexes but there was no difference in the time cosumption. Am i missing something here?</p>\n\n<p>My schema looks like this. (49 Columns). Id - PrimaryKey (Clustered Index) </p>\n\n<pre><code>Id | Name | Age | CountryId | CourseId | ....... | EnrolledOn\n</code></pre>\n\n<p>There are around 425,000 records in the table. Considering country and course are the columns used to filter the records, I created a composite index comprising these two columns (These two columns are foreign keys).</p>\n\n<p>I tried the following query.</p>\n\n<pre><code>Select * From Students Where CountryId = 1 And CourseId = 1\n</code></pre>\n\n<p>Without indexes and with indexes the query took 11 seconds. </p>\n\n<blockquote>\n  <p>Note : Around 415,000 records match the above condition. Will this be a reason for no difference in the time consumed.</p>\n</blockquote>\n\n<p>Can anyone help me on this.</p>\n\n<pre><code>CREATE NONCLUSTERED INDEX IX_Country_Course ON Students(CountryId,CourseId)\n</code></pre>\n"},{"tags":["java","arrays","performance","arraylist"],"answer_count":6,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":1122,"score":3,"question_id":5081682,"title":"Array and ArrayList positional access performance","body":"<p>i just read this code example:\n<a href=\"http://robaustin.wikidot.com/how-does-the-performance-of-arraylist-compare-to-array\" rel=\"nofollow\">http://robaustin.wikidot.com/how-does-the-performance-of-arraylist-compare-to-array</a></p>\n\n<p>what causing\nj = INT_ARRAY[i]; \nto be three times faster than \nj = ARRAY_LIST.get(i)</p>\n\n<p>I know that ArrayList internally uses an array. So i would like to know\nin details what are the extra operations that add this time ( calling methods, casting, other JVM considerations etc..)</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["mysql","performance","query-optimization","fulltext","sql-calc-found-rows"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":45,"score":0,"question_id":12793301,"title":"Mysql fulltext search performance with SQL_CALC_FOUND_ROWS","body":"<p>I'm trying to optimize the performance of some queries in my application.</p>\n\n<p>In one query with multiple joins and a fulltext search I use <code>SQL_CALC_FOUND_ROWS</code> in a first query for pagination.</p>\n\n<p>Unfortunately the performance of the query is very slow, Without the <code>SQL_CALC_FOUND_ROWS</code> the query is about 100 times faster.</p>\n\n<p>I there a possibility to get a better performance in this case? </p>\n\n<p>I tried a single count-query without the <code>SQL_CALC_FOUND_ROWS</code>, but this query is an additional second slower than the <code>SQL_CALC_FOUND_ROWS</code>-query.</p>\n"},{"tags":["mysql","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":42,"score":1,"question_id":12824639,"title":"mysql int vs boolean index","body":"<p>I have a task:</p>\n\n<p>records in database can be in statuses:</p>\n\n<pre><code>NEW = 1\nCONFIRMED = 2\nFINISHED = 3\nDELETED = 4\n</code></pre>\n\n<p>1.</p>\n\n<p>I can store them in int field, with index for it, and then select like this: </p>\n\n<pre><code>... WHERE status = FINISHED ...\n... WHERE status = DELETED ...\n</code></pre>\n\n<p>2.</p>\n\n<p>also i can store them as 4 separated boolean fields</p>\n\n<p>and then select: </p>\n\n<pre><code>... WHERE finished_field = True ...\n... WHERE deleted_field = True ...\n</code></pre>\n\n<p>which case is better in performance (for select) 1 or 2, and does it nessesary create index for fields in case 2?</p>\n"},{"tags":["mysql","ruby-on-rails","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":31,"score":0,"question_id":12836135,"title":"Rails AR: Most efficient way of writing a scope which requires several joins","body":"<p>Consider the following</p>\n\n<pre><code>class Room &lt; ActiveRecord::Base\n  belongs_to :user\n  has_many :authorisations, dependent: :destroy\n  has_many :authorised_users, through: :authorisations, source: :user\n\n  scope :without_privacy_restriction, where(public: true)\nend\n\nclass User &lt; ActiveRecord::Base\n  has_many :rooms, dependent: :destroy\n  has_many :accessible_rooms, through: :authorisations, source: :room\n  has_many :authorisations\nend\n</code></pre>\n\n<p>User can be a owner of a room, he can also be an authorised user in another user's room. Above all this, user can be defined as an admin (another column in the users table which represents this) and have access to all rooms no matter what.</p>\n\n<p>I want to be able to write an efficient scope which returns back all accessible rooms to a given user, but I can't determine what's the most efficient way of achieving this.</p>\n"},{"tags":["java","performance","netbeans","java-ee","tips-and-tricks"],"answer_count":12,"favorite_count":4,"up_vote_count":21,"down_vote_count":1,"view_count":26639,"score":20,"question_id":229763,"title":"How to improve Netbeans performance?","body":"<p>Is there a real way to get Netbeans to load and work faster?</p>\n\n<p>It is too slow and gets worse when you have been coding for some time. It eats all my RAM.</p>\n\n<hr>\n\n<p>I am on a Windows machine, specifically Windows Server 2008 Datacenter Edition x64,\n4Gb of RAM, 3Ghz Core 2 Duo processor, etc. I am using the x64 JDK. I use the NOD32 Antivirus since for me it is the best in machine performance.</p>\n\n<p>In Task Manager netbeans.exe only shows no more than 20 Mb, java.exe more than 600Mb.</p>\n\n<p>My project is a J2EE web application, more than 500 classes, only the project libraries not included (externals). And when I said slow, I mean 3, 4, 5 minutes or more Netbeans is frozen.</p>\n\n<p>Is my project just too large for Netbeans, if it has to read all files to get the state of files like error warnings, svn status and more? Can I disable all this? Is it possible to set it to scan only when I open a file?</p>\n\n<p>My CPU use is normally at 30 percent with all my tools opened, I mean Netbeans, MS SQL Manager, Notepad, XMLSpy, Task Manager, Delphi, VirtualBox. Netbeans eats more RAM than my virtualized systems.</p>\n\n<p>In Linux it is as slow as in Windows in the same machine (Ubuntu 8.04 x64).</p>\n\n<p>It is true that the Netbeans team improved startup speed but when it opens it begins to cache ALL. </p>\n\n<p>I have used some JVM parameters to set high memory usage and others:\n<code>\"C:\\Program Files\\NetBeans Dev\\bin\\netbeans.exe\" -J-Xms32m -J-Xmx512m -J-Xverify:none  -J-XX:+CMSClassUnloadingEnabled</code></p>\n\n<p>But it is still slow.</p>\n"},{"tags":["php","performance","logging"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":127,"score":1,"question_id":10148085,"title":"Efficiency of logging in PHP?","body":"<p>I'm working on a PHP/CodeIgniter web app that will be the backend for a non-realtime game. We want the ability to record game activity for later analysis. In my performance tests using either codeigniter's own logging system or log4php, file logging seems slow, reducing the number of requests per second the server can handle by 50%. I've tried it on both a WAMP machine and an Apache/Ubuntu server. If I change logging to use MongoDB, the performance only drops by a few percent, even if I'm logging the same amount of information.</p>\n\n<p>Is file logging going to be inherently slow for php scripts because they are all waiting on locks on the same file or is it likely a configuration issue?</p>\n"},{"tags":["mysql","performance","io","innodb","iostat"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":103,"score":-1,"question_id":12838162,"title":"Select * from statement execute very slowly, innodb io read speed is low","body":"<p>I have a very simple  query \" select * from ap_statistic \" running in my servers. the servers have the same hardware and software configuration (CPU 8 core, mem :32G, OS: redhat 5.5, mysql version: 5.1 ) and run the same applications. In  server A, the row number of the table ap_statistic is about 22512379, in  server B, the row number of the table is 41438751. Of course the query running on server A is faster than server B, but what is strange is the query on server B is extreme slow, it takes more than 3 hours where in server A it just takes 10 minutes. </p>\n    I use some tool to monitor system status and find that when the query is running in server A, system IO read speed is about 20~30M/s, but in server B it's 2~3M/s. I've tried to clean linux cache  and restart mysql server, all is the same result. And  I tried to restored DB from server B to server A, so the query in Server A is very very slow and io read speed is very slow. I want to know why this happen? </p>\n    the ap_statistic table data in server A is generated by normally running and table data in server B is generated by a stored procedure. the table scheme is:</p>\n\n<pre><code>CREATE TABLE `ap_statistic` (\n  `ap_statisticId` BIGINT(20) UNSIGNED NOT NULL,\n  `deviceId` INT(11) UNSIGNED NOT NULL DEFAULT '0',\n  `macaddress` VARCHAR(100) DEFAULT NULL,\n  `check_time` TIMESTAMP NOT NULL DEFAULT '0000-00-00 00:00:00',\n  `ap_count` INT(10) UNSIGNED NOT NULL DEFAULT '0',\n  `wlan` VARCHAR(64) DEFAULT NULL,\n  `radio` VARCHAR(50) DEFAULT NULL,\n  `bssid` VARCHAR(32) DEFAULT NULL,\n  `zd_ap_name` VARCHAR(64) DEFAULT NULL,\n  `channel` INT(2) DEFAULT NULL,\n  `uplinkRSSI` INT(3) DEFAULT '0',\n  `downlinkRSSI` INT(3) DEFAULT '0',\n  `txBytes` BIGINT(20) DEFAULT '0',\n  `rxBytes` BIGINT(20) DEFAULT '0',\n  `txPkts` BIGINT(20) DEFAULT '0',\n  `rxPkts` BIGINT(20) DEFAULT '0',\n  `hops` INT(1) DEFAULT '0',\n  `numDownlink` INT(3) DEFAULT '0',\n  `distance` INT(4) DEFAULT '0',\n  `phyerr` INT(11) DEFAULT '0',\n  `max_num_clients` INT(3) DEFAULT '0',\n  `max_mesh_downlinks` INT(1) DEFAULT '0',\n  `airtime` INT(3) DEFAULT '0',\n  `uptimePercentage` INT(3) DEFAULT '0',\n  `total_num_clients` INT(3) DEFAULT '0',\n  `tx_actual_throughput` BIGINT(20) DEFAULT '0',\n  `rx_actual_throughput` BIGINT(20) DEFAULT '0',\n  `tunnelMode` VARCHAR(32) DEFAULT NULL,\n  `externalIp` VARCHAR(64) DEFAULT NULL,\n  `externalPort` VARCHAR(32) DEFAULT NULL,\n  `level` INT(1) DEFAULT '1' \n  `essid` VARCHAR(64) DEFAULT NULL,\n  `total_client_join` INT(11) DEFAULT '0',\n  PRIMARY KEY (`ap_statisticId`),\n  KEY `check_time` (`check_time`),\n  KEY `macaddress` (`macaddress`),\n  KEY `deviceId` (`deviceId`)\n) ENGINE=INNODB DEFAULT CHARSET=utf8\n</code></pre>\n\n<p>the follows are the table file info and some outputs of the monitor tools</p>\n\n<p><strong>Server B</strong></p>\n\n<pre><code>  -rw-rw---- 1 mysql mysql 18568183808 Oct 11 14:52 ap_statistic.ibd\n\n  [root@localhost itms]# filefrag ./ap_statistic.ibd\n  ./ap_statistic.ibd: 164 extents found, perfection would be 159 extents\n\n\n    TABLE         Non_unique  Key_name    Seq_in_index  Column_name     COLLATION  Cardinality  Sub_part  Packed  NULL    Index_type  COMMENT\n    ------------  ----------  ----------  ------------  --------------  ---------  -----------  --------  ------  ------  ----------  -------\n    ap_statistic           0  PRIMARY                1  ap_statisticId  A             41438751    (NULL)  (NULL)          BTREE              \n    ap_statistic           1  check_time             1  check_time      A                10320    (NULL)  (NULL)          BTREE              \n    ap_statistic           1  macaddress             1  macaddress      A                   16    (NULL)  (NULL)  YES     BTREE              \n    ap_statistic           1  deviceId               1  deviceId        A                   16    (NULL)  (NULL)          BTREE    \n\n\n  mysql&gt;show status;\n\n        Variable_name   Value\n        Aborted_clients 0\n        Aborted_connects    0\n        Binlog_cache_disk_use   0\n        Binlog_cache_use    0\n        Bytes_received  1256\n        Bytes_sent  8844\n        Com_admin_commands  0\n        Com_assign_to_keycache  0\n        Com_alter_db    0\n        Com_alter_db_upgrade    0\n        Com_alter_event 0\n        Com_alter_function  0\n        Com_alter_procedure 0\n        Com_alter_server    0\n        Com_alter_table 0\n        Com_alter_tablespace    0\n        Com_analyze 0\n        Com_backup_table    0\n        Com_begin   0\n        Com_binlog  0\n        Com_call_procedure  0\n        Com_change_db   1\n        Com_change_master   0\n        Com_check   0\n        Com_checksum    0\n        Com_commit  0\n        Com_create_db   0\n        Com_create_event    0\n        Com_create_function 0\n        Com_create_index    0\n        Com_create_procedure    0\n        Com_create_server   0\n        Com_create_table    0\n        Com_create_trigger  0\n        Com_create_udf  0\n        Com_create_user 0\n        Com_create_view 0\n        Com_dealloc_sql 0\n        Com_delete  0\n        Com_delete_multi    0\n        Com_do  0\n        Com_drop_db 0\n        Com_drop_event  0\n        Com_drop_function   0\n        Com_drop_index  0\n        Com_drop_procedure  0\n        Com_drop_server 0\n        Com_drop_table  0\n        Com_drop_trigger    0\n        Com_drop_user   0\n        Com_drop_view   0\n        Com_empty_query 0\n        Com_execute_sql 0\n        Com_flush   0\n        Com_grant   0\n        Com_ha_close    0\n        Com_ha_open 0\n        Com_ha_read 0\n        Com_help    0\n        Com_insert  0\n        Com_insert_select   0\n        Com_install_plugin  0\n        Com_kill    0\n        Com_load    0\n        Com_load_master_data    0\n        Com_load_master_table   0\n        Com_lock_tables 0\n        Com_optimize    0\n        Com_preload_keys    0\n        Com_prepare_sql 0\n        Com_purge   0\n        Com_purge_before_date   0\n        Com_release_savepoint   0\n        Com_rename_table    0\n        Com_rename_user 0\n        Com_repair  0\n        Com_replace 0\n        Com_replace_select  0\n        Com_reset   0\n        Com_restore_table   0\n        Com_revoke  0\n        Com_revoke_all  0\n        Com_rollback    0\n        Com_rollback_to_savepoint   0\n        Com_savepoint   0\n        Com_select  1\n        Com_set_option  3\n        Com_show_authors    0\n        Com_show_binlog_events  0\n        Com_show_binlogs    0\n        Com_show_charsets   0\n        Com_show_collations 0\n        Com_show_column_types   0\n        Com_show_contributors   0\n        Com_show_create_db  0\n        Com_show_create_event   0\n        Com_show_create_func    0\n        Com_show_create_proc    0\n        Com_show_create_table   1\n        Com_show_create_trigger 0\n        Com_show_databases  0\n        Com_show_engine_logs    0\n        Com_show_engine_mutex   0\n        Com_show_engine_status  0\n        Com_show_events 0\n        Com_show_errors 0\n        Com_show_fields 1\n        Com_show_function_status    0\n        Com_show_grants 0\n        Com_show_keys   1\n        Com_show_master_status  0\n        Com_show_new_master 0\n        Com_show_open_tables    0\n        Com_show_plugins    0\n        Com_show_privileges 0\n        Com_show_procedure_status   0\n        Com_show_processlist    0\n        Com_show_profile    0\n        Com_show_profiles   0\n        Com_show_slave_hosts    0\n        Com_show_slave_status   0\n        Com_show_status 21\n        Com_show_storage_engines    0\n        Com_show_table_status   0\n        Com_show_tables 0\n        Com_show_triggers   0\n        Com_show_variables  0\n        Com_show_warnings   0\n        Com_slave_start 0\n        Com_slave_stop  0\n        Com_stmt_close  0\n        Com_stmt_execute    0\n        Com_stmt_fetch  0\n        Com_stmt_prepare    0\n        Com_stmt_reprepare  0\n        Com_stmt_reset  0\n        Com_stmt_send_long_data 0\n        Com_truncate    0\n        Com_uninstall_plugin    0\n        Com_unlock_tables   0\n        Com_update  0\n        Com_update_multi    0\n        Com_xa_commit   0\n        Com_xa_end  0\n        Com_xa_prepare  0\n        Com_xa_recover  0\n        Com_xa_rollback 0\n        Com_xa_start    0\n        Compression ON\n        Connections 323\n        Created_tmp_disk_tables 1\n        Created_tmp_files   5\n        Created_tmp_tables  2\n        Delayed_errors  0\n        Delayed_insert_threads  0\n        Delayed_writes  0\n        Flush_commands  1\n        Handler_commit  1\n        Handler_delete  0\n        Handler_discover    0\n        Handler_prepare 0\n        Handler_read_first  0\n        Handler_read_key    0\n        Handler_read_next   0\n        Handler_read_prev   0\n        Handler_read_rnd    0\n        Handler_read_rnd_next   39\n        Handler_rollback    0\n        Handler_savepoint   0\n        Handler_savepoint_rollback  0\n        Handler_update  0\n        Handler_write   37\n        Innodb_buffer_pool_pages_data   43392\n        Innodb_buffer_pool_pages_dirty  0\n        Innodb_buffer_pool_pages_flushed    43822\n        Innodb_buffer_pool_pages_free   637198\n        Innodb_buffer_pool_pages_misc   562\n        Innodb_buffer_pool_pages_total  681152\n        Innodb_buffer_pool_read_ahead_rnd   9\n        Innodb_buffer_pool_read_ahead_seq   27\n        Innodb_buffer_pool_read_requests    36489397\n        Innodb_buffer_pool_reads    27421\n        Innodb_buffer_pool_wait_free    0\n        Innodb_buffer_pool_write_requests   4165371\n        Innodb_data_fsyncs  5228\n        Innodb_data_pending_fsyncs  0\n        Innodb_data_pending_reads   1\n        Innodb_data_pending_writes  0\n        Innodb_data_read    626216960\n        Innodb_data_reads   36565\n        Innodb_data_writes  293947\n        Innodb_data_written 1792826880\n        Innodb_dblwr_pages_written  43822\n        Innodb_dblwr_writes 830\n        Innodb_log_waits    0\n        Innodb_log_write_requests   492588\n        Innodb_log_writes   268248\n        Innodb_os_log_fsyncs    2130\n        Innodb_os_log_pending_fsyncs    0\n        Innodb_os_log_pending_writes    0\n        Innodb_os_log_written   356559872\n        Innodb_page_size    16384\n        Innodb_pages_created    5304\n        Innodb_pages_read   38087\n        Innodb_pages_written    43822\n        Innodb_row_lock_current_waits   0\n        Innodb_row_lock_time    0\n        Innodb_row_lock_time_avg    0\n        Innodb_row_lock_time_max    0\n        Innodb_row_lock_waits   0\n        Innodb_rows_deleted 28637\n        Innodb_rows_inserted    306449\n        Innodb_rows_read    16579740\n        Innodb_rows_updated 887251\n        Key_blocks_not_flushed  0\n        Key_blocks_unused   212928\n        Key_blocks_used 1415\n        Key_read_requests   393323\n        Key_reads   16\n        Key_write_requests  102461\n        Key_writes  102439\n        Last_query_cost 9142769.199000\n        Max_used_connections    19\n        Not_flushed_delayed_rows    0\n        Open_files  24\n        Open_streams    0\n        Open_table_definitions  142\n        Open_tables 146\n        Opened_files    592\n        Opened_table_definitions    0\n        Opened_tables   0\n        Prepared_stmt_count 0\n        Qcache_free_blocks  0\n        Qcache_free_memory  0\n        Qcache_hits 0\n        Qcache_inserts  0\n        Qcache_lowmem_prunes    0\n        Qcache_not_cached   0\n        Qcache_queries_in_cache 0\n        Qcache_total_blocks 0\n        Queries 1578897\n        Questions   30\n        Rpl_status  NULL\n        Select_full_join    0\n        Select_full_range_join  0\n        Select_range    0\n        Select_range_check  0\n        Select_scan 2\n        Slave_open_temp_tables  0\n        Slave_retried_transactions  0\n        Slave_running   OFF\n        Slow_launch_threads 0\n        Slow_queries    0\n        Sort_merge_passes   0\n        Sort_range  0\n        Sort_rows   0\n        Sort_scan   0\n        Ssl_accept_renegotiates 0\n        Ssl_accepts 0\n        Ssl_callback_cache_hits 0\n        Ssl_cipher  \n        Ssl_cipher_list \n        Ssl_client_connects 0\n        Ssl_connect_renegotiates    0\n        Ssl_ctx_verify_depth    0\n        Ssl_ctx_verify_mode 0\n        Ssl_default_timeout 0\n        Ssl_finished_accepts    0\n        Ssl_finished_connects   0\n        Ssl_session_cache_hits  0\n        Ssl_session_cache_misses    0\n        Ssl_session_cache_mode  NONE\n        Ssl_session_cache_overflows 0\n        Ssl_session_cache_size  0\n        Ssl_session_cache_timeouts  0\n        Ssl_sessions_reused 0\n        Ssl_used_session_cache_entries  0\n        Ssl_verify_depth    0\n        Ssl_verify_mode 0\n        Ssl_version \n        Table_locks_immediate   1549525\n        Table_locks_waited  0\n        Tc_log_max_pages_used   0\n        Tc_log_page_size    0\n        Tc_log_page_waits   0\n        Threads_cached  0\n        Threads_connected   17\n        Threads_created 322\n        Threads_running 2\n        Uptime  8093\n        Uptime_since_flush_status   8093\n\n        mysql&gt;show variables;\n\n        Variable_name   Value\n        auto_increment_increment    1\n        auto_increment_offset   1\n        autocommit  ON\n        automatic_sp_privileges ON\n        back_log    50\n        big_tables  OFF\n        binlog_cache_size   32768\n        binlog_direct_non_transactional_updates OFF\n        binlog_format   STATEMENT\n        bulk_insert_buffer_size 8388608\n        character_set_client    utf8\n        character_set_connection    utf8\n        character_set_database  utf8\n        character_set_filesystem    binary\n        character_set_results   utf8\n        character_set_server    utf8\n        character_set_system    utf8\n        collation_connection    utf8_general_ci\n        collation_database  utf8_general_ci\n        collation_server    utf8_general_ci\n        completion_type 0\n        concurrent_insert   1\n        connect_timeout 10\n        date_format %Y-%m-%d\n        datetime_format %Y-%m-%d %H:%i:%s\n        default_week_format 0\n        delay_key_write ON\n        delayed_insert_limit    100\n        delayed_insert_timeout  300\n        delayed_queue_size  1000\n        div_precision_increment 4\n        engine_condition_pushdown   ON\n        error_count 0\n        event_scheduler OFF\n        expire_logs_days    0\n        flush   OFF\n        flush_time  0\n        foreign_key_checks  ON\n        ft_boolean_syntax   + -&gt;&lt;()~*:\"\"&amp;|\n        ft_max_word_len 84\n        ft_min_word_len 4\n        ft_query_expansion_limit    20\n        ft_stopword_file    (built-in)\n        general_log OFF\n        group_concat_max_len    1024\n        have_community_features YES\n        have_compress   YES\n        have_crypt  YES\n        have_csv    YES\n        have_dynamic_loading    YES\n        have_geometry   YES\n        have_innodb YES\n        have_ndbcluster NO\n        have_openssl    DISABLED\n        have_partitioning   NO\n        have_query_cache    YES\n        have_rtree_keys YES\n        have_ssl    DISABLED\n        have_symlink    YES\n        hostname    localhost.localdomain\n        identity    0\n        ignore_builtin_innodb   OFF\n        init_connect    \n        init_file   \n        init_slave  \n        innodb_adaptive_hash_index  ON\n        innodb_additional_mem_pool_size 67108864\n        innodb_autoextend_increment 8\n        innodb_autoinc_lock_mode    1\n        innodb_buffer_pool_size 11159994368\n        innodb_checksums    ON\n        innodb_commit_concurrency   0\n        innodb_concurrency_tickets  500\n        innodb_data_file_path   ibdata1:10M:autoextend\n        innodb_data_home_dir    \n        innodb_doublewrite  ON\n        innodb_fast_shutdown    1\n        innodb_file_io_threads  4\n        innodb_file_per_table   ON\n        innodb_flush_log_at_trx_commit  2\n        innodb_flush_method O_DIRECT\n        innodb_force_recovery   0\n        innodb_lock_wait_timeout    120\n        innodb_locks_unsafe_for_binlog  ON\n        innodb_log_buffer_size  134217728\n        innodb_log_file_size    5242880\n        innodb_log_files_in_group   2\n        innodb_log_group_home_dir   ./\n        innodb_max_dirty_pages_pct  90\n        innodb_max_purge_lag    0\n        innodb_mirrored_log_groups  1\n        innodb_open_files   300\n        innodb_rollback_on_timeout  OFF\n        innodb_stats_on_metadata    ON\n        innodb_support_xa   ON\n        innodb_sync_spin_loops  20\n        innodb_table_locks  ON\n        innodb_thread_concurrency   8\n        innodb_thread_sleep_delay   10000\n        innodb_use_legacy_cardinality_algorithm ON\n        insert_id   0\n        interactive_timeout 28800\n        join_buffer_size    268435456\n        keep_files_on_create    OFF\n        key_buffer_size 268435456\n        key_cache_age_threshold 300\n        key_cache_block_size    1024\n        key_cache_division_limit    100         \n        large_files_support ON\n        large_page_size 0\n        large_pages OFF\n        last_insert_id  0\n        lc_time_names   en_US\n        license Commercial\n        local_infile    ON\n        locked_in_memory    OFF\n        log OFF\n        log_bin OFF\n        log_bin_trust_function_creators OFF\n        log_bin_trust_routine_creators  OFF         \n        log_output  FILE\n        log_queries_not_using_indexes   OFF\n        log_slave_updates   OFF\n        log_slow_queries    OFF\n        log_warnings    1\n        long_query_time 10.000000\n        low_priority_updates    OFF\n        lower_case_file_system  OFF\n        lower_case_table_names  1\n        max_allowed_packet  134217728\n        max_binlog_cache_size   18446744073709547520\n        max_binlog_size 1073741824\n        max_connect_errors  10\n        max_connections 300\n        max_delayed_threads 20\n        max_error_count 64\n        max_heap_table_size 268435456\n        max_insert_delayed_threads  20\n        max_join_size   18446744073709551615\n        max_length_for_sort_data    1024\n        max_prepared_stmt_count 16382\n        max_relay_log_size  0\n        max_seeks_for_key   18446744073709551615\n        max_sort_length 1024\n        max_sp_recursion_depth  0\n        max_tmp_tables  32\n        max_user_connections    0\n        max_write_lock_count    18446744073709551615\n        min_examined_row_limit  0\n        multi_range_count   256\n        myisam_data_pointer_size    6\n        myisam_max_sort_file_size   9223372036853727232\n        myisam_mmap_size    18446744073709551615\n        myisam_recover_options  OFF\n        myisam_repair_threads   1\n        myisam_sort_buffer_size 8388608\n        myisam_stats_method nulls_unequal\n        myisam_use_mmap OFF\n        net_buffer_length   16384\n        net_read_timeout    30\n        net_retry_count 10\n        net_write_timeout   60\n        new OFF\n        old OFF\n        old_alter_table OFF\n        old_passwords   OFF\n        open_files_limit    10240\n        optimizer_prune_level   1\n        optimizer_search_depth  62\n        optimizer_switch    index_merge=on,index_merge_union=on,index_merge_sort_union=on,index_merge_intersection=on\n        port    3306\n        preload_buffer_size 32768\n        profiling   OFF\n        profiling_history_size  15\n        protocol_version    10\n        pseudo_thread_id    18\n        query_alloc_block_size  8192\n        query_cache_limit   1048576\n        query_cache_min_res_unit    4096\n        query_cache_size    0\n        query_cache_type    ON\n        query_cache_wlock_invalidate    OFF\n        query_prealloc_size 8192\n        rand_seed1  \n        rand_seed2  \n        range_alloc_block_size  4096\n        read_buffer_size    67108864\n        read_only   OFF\n        read_rnd_buffer_size    67108864\n        relay_log   \n        relay_log_index \n        relay_log_info_file relay-log.info\n        relay_log_purge ON\n        relay_log_space_limit   0\n        report_host \n        report_password \n        report_port 3306\n        report_user \n        rpl_recovery_rank   0\n        secure_auth OFF\n        secure_file_priv    \n        server_id   0\n        skip_external_locking   ON\n        skip_name_resolve   ON\n        skip_networking OFF\n        skip_show_database  OFF\n        slave_compressed_protocol   OFF\n        slave_exec_mode STRICT\n        slave_net_timeout   3600\n        slave_skip_errors   OFF\n        slave_transaction_retries   10\n        slow_launch_time    2\n        slow_query_log  OFF\n        sort_buffer_size    16777216\n        sql_auto_is_null    ON\n        sql_big_selects ON\n        sql_big_tables  OFF\n        sql_buffer_result   OFF\n        sql_log_bin ON\n        sql_log_off OFF\n        sql_log_update  ON\n        sql_low_priority_updates    OFF\n        sql_max_join_size   18446744073709551615\n        sql_mode    \n        sql_notes   ON\n        sql_quote_show_create   ON\n        sql_safe_updates    OFF\n        sql_select_limit    18446744073709551615\n        sql_slave_skip_counter  \n        sql_warnings    OFF\n        ssl_ca  \n        ssl_capath  \n        ssl_cert    \n        ssl_cipher  \n        ssl_key \n        storage_engine  MyISAM\n        sync_binlog 0\n        sync_frm    ON\n        system_time_zone    UTC\n        table_definition_cache  256\n        table_lock_wait_timeout 50\n        table_open_cache    512\n        table_type  MyISAM\n        thread_cache_size   0\n        thread_handling one-thread-per-connection\n        thread_stack    262144\n        time_format %H:%i:%s\n        time_zone   +08:00\n        timed_mutexes   OFF\n        timestamp   1349946061\n        tmp_table_size  1073741824\n        transaction_alloc_block_size    8192\n        transaction_prealloc_size   4096\n        tx_isolation    REPEATABLE-READ\n        unique_checks   ON\n        updatable_views_with_limit  YES\n        version 5.1.53-enterprise-commercial-pro\n        version_comment MySQL Enterprise Server - Pro Edition (Commercial)\n        version_compile_machine x86_64\n        version_compile_os  unknown-linux-gnu\n        wait_timeout    28800\n        warning_count   0\n\n\n        mysql&gt; show innodb status\\G;\n        *************************** 1. row ***************************\n          Type: InnoDB\n          Name:\n        Status:\n        =====================================\n        121011 10:22:13 INNODB MONITOR OUTPUT\n        =====================================\n        Per second averages calculated from the last 39 seconds\n        ----------\n        SEMAPHORES\n        ----------\n        OS WAIT ARRAY INFO: reservation count 3806, signal count 3778\n        Mutex spin waits 0, rounds 282892, OS waits 2075\n        RW-shared spins 1969, OS waits 864; RW-excl spins 2336, OS waits 749\n        ------------\n        TRANSACTIONS\n        ------------\n        Trx id counter 0 5303968\n        Purge done for trx's n:o &lt; 0 5303951 undo n:o &lt; 0 0\n        History list length 1\n        LIST OF TRANSACTIONS FOR EACH SESSION:\n        ---TRANSACTION 0 0, not started, process no 30336, OS thread id 1189509440\n        MySQL thread id 520, query id 1861594 localhost root\n        show innodb status\n        ---TRANSACTION 0 5303967, not started, process no 30336, OS thread id 1188710720\n        MySQL thread id 526, query id 1861593 127.0.0.1 root\n        ---TRANSACTION 0 5303962, not started, process no 30336, OS thread id 1186314560\n        MySQL thread id 519, query id 1861555 127.0.0.1 root\n        ---TRANSACTION 0 5303952, not started, process no 30336, OS thread id 1188444480\n        MySQL thread id 515, query id 1861567 127.0.0.1 root\n        ---TRANSACTION 0 5303948, not started, process no 30336, OS thread id 1187912000\n        MySQL thread id 516, query id 1861566 127.0.0.1 root\n        ---TRANSACTION 0 5303937, not started, process no 30336, OS thread id 1190308160\n        MySQL thread id 511, query id 1861568 127.0.0.1 root\n        ---TRANSACTION 0 0, not started, process no 30336, OS thread id 1090791744\n        MySQL thread id 18, query id 1596073 172.18.112.84 root\n        ---TRANSACTION 0 5303959, ACTIVE 63 sec, process no 30336, OS thread id 1090525504 fetching rows, thread declared inside InnoDB 500\n        mysql tables in use 1, locked 0\n        MySQL thread id 17, query id 1861400 localhost root Sending data\n        select * from ap_statistic\n        Trx read view will not see trx with id &gt;= 0 5303960, sees &lt; 0 5303960\n        --------\n        FILE I/O\n        --------\n        I/O thread 0 state: waiting for i/o request (insert buffer thread)\n        I/O thread 1 state: waiting for i/o request (log thread)\n        I/O thread 2 state: waiting for i/o request (read thread)\n        I/O thread 3 state: waiting for i/o request (write thread)\n        Pending normal aio reads: 0, aio writes: 0,\n         ibuf aio reads: 0, log i/o's: 0, sync i/o's: 0\n        Pending flushes (fsync) log: 0; buffer pool: 0\n        63521 OS file reads, 294656 OS file writes, 5641 OS fsyncs\n        1 pending preads, 0 pending pwrites\n        149.38 reads/s, 16384 avg bytes/read, 0.00 writes/s, 0.00 fsyncs/s\n        -------------------------------------\n        INSERT BUFFER AND ADAPTIVE HASH INDEX\n        -------------------------------------\n        Ibuf: size 1, free list len 318, seg size 320,\n        63593 inserts, 63593 merged recs, 9674 merges\n        Hash table size 22086161, node heap has 607 buffer(s)\n        0.08 hash searches/s, 0.26 non-hash searches/s\n        ---\n        LOG\n        ---\n        Log sequence number 15 2873617336\n        Log flushed up to   15 2873617336\n        Last checkpoint at  15 2873617336\n        0 pending log writes, 0 pending chkp writes\n        269102 log i/o's done, 0.00 log i/o's/second\n        ----------------------\n        BUFFER POOL AND MEMORY\n        ----------------------\n        Total memory allocated 12452785320; in additional pool allocated 15261440\n        Dictionary memory allocated 789024\n        Buffer pool size   681152\n        Free buffers       610013\n        Database pages     70532\n        Modified db pages  0\n        Pending reads 1\n        Pending writes: LRU 0, flush list 0, single page 0\n        Pages read 65043, created 5488, written 45924\n        149.38 reads/s, 0.00 creates/s, 0.00 writes/s\n        Buffer pool hit rate 888 / 1000\n        --------------\n        ROW OPERATIONS\n        --------------\n        1 queries inside InnoDB, 0 queries in queue\n        2 read views open inside InnoDB\n        Main thread process no. 30336, id 1185782080, state: waiting for server activity\n        Number of rows inserted 336555, updated 1112311, deleted 28681, read 29200669\n        0.00 inserts/s, 0.00 updates/s, 0.00 deletes/s, 8258.58 reads/s\n        ----------------------------\n        END OF INNODB MONITOR OUTPUT\n        ============================\n\n        1 row in set, 1 warning (0.00 sec)\n\n        ERROR:\n        No query specified\n\n\n\n        iostat -dx 2\n\n        Device:    rrqm/s wrqm/s   r/s   w/s  rsec/s  wsec/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await  svctm  %util\n        sda          0.00   2.50 141.50 11.50 4516.00  112.00  2258.00    56.00    30.25     0.95    6.23   5.70  87.25\n        sda1         0.00   0.00  0.00  0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00   0.00   0.00\n        sda2         0.00   2.50 141.50 11.50 4516.00  112.00  2258.00    56.00    30.25     0.95    6.23   5.70  87.25\n        dm-0         0.00   0.00 141.50 14.00 4516.00  112.00  2258.00    56.00    29.76     0.97    6.24   5.62  87.35\n        dm-1         0.00   0.00  0.00  0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00   0.00   0.00\n\n        Device:    rrqm/s wrqm/s   r/s   w/s  rsec/s  wsec/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await  svctm  %util\n        sda          3.00   0.00 154.50  0.00 4932.00    0.00  2466.00     0.00    31.92     0.93    6.04   6.04  93.25\n        sda1         0.00   0.00  0.00  0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00   0.00   0.00\n        sda2         3.00   0.00 154.50  0.00 4932.00    0.00  2466.00     0.00    31.92     0.93    6.04   6.04  93.25\n        dm-0         0.00   0.00 157.50  0.00 4932.00    0.00  2466.00     0.00    31.31     0.95    6.04   5.93  93.40\n        dm-1         0.00   0.00  0.00  0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00   0.00   0.00\n\n        Device:    rrqm/s wrqm/s   r/s   w/s  rsec/s  wsec/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await  svctm  %util\n        sda          3.00   1.50 150.50  1.50 4804.00   24.00  2402.00    12.00    31.76     0.94    6.15   6.14  93.40\n        sda1         0.00   0.00  0.00  0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00   0.00   0.00\n        sda2         3.00   1.50 150.50  1.50 4804.00   24.00  2402.00    12.00    31.76     0.94    6.15   6.14  93.40\n        dm-0         0.00   0.00 153.50  3.00 4804.00   24.00  2402.00    12.00    30.85     0.95    6.08   5.97  93.50\n        dm-1         0.00   0.00  0.00  0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00   0.00   0.00\n\n\n        vmstat 2\n\n        procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------\n         r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n         1  1     16 27358488  18152 115500    0    0  2558     0 1193 8634 14  1 73 12  0\n         1  1     16 27346840  18168 115500    0    0  2356    12 1174 9129 14  2 73 12  0\n         2  1     16 27334320  18172 115504    0    0  2522     0 1184 8918 14  1 73 12  0\n         1  1     16 27321812  18180 115496    0    0  2456    12 1183 7357 13  1 74 12  0\n         1  1     16 27310132  18180 115504    0    0  2344    12 1174 6990 12  2 74 12  0\n         1  1     16 27297608  18184 115508    0    0  2506     0 1182 6203 12  2 74 11  0\n         1  1     16 27285444  18200 115504    0    0  2420    90 1187 9667 13  2 73 12  0\n         1  1     16 27277640  18200 115508    0    0  2248     0 1165 8103 19  2 69 11  0\n         2  1     16 27265380  18204 115504    0    0  2498     0 1179 5690 13  1 74 12  0\n         4  1     16 27252972  18216 115508    0    0  2434    12 1178 6096 14  1 74 12  0\n         1  1     16 27241032  18232 115496    0    0  2520     0 1181 9252 12  1 75 11  0\n         2  1     16 27229136  18240 115508    0    0  2468    10 1178 7116 13  1 74 12  0\n         1  0     16 27630612  18248 115508    0    0  1536    20 1121 4082 13  1 79  7  0\n\n\n        mpstat -P ALL 2\n\n\n        02:48:57 PM  CPU   %user   %nice %system %iowait    %irq   %soft   %idle    intr/s\n        02:48:59 PM  all   13.69    0.00    1.31   11.56    0.00    0.62   72.81   1190.95\n        02:48:59 PM    0   33.67    0.00    0.50    0.00    0.00    0.00   65.83   1006.03\n        02:48:59 PM    1    6.53    0.00    0.50   92.96    0.50    0.50    0.00    160.80\n        02:48:59 PM    2    1.01    0.00    0.50    0.00    0.00    0.00   98.49      0.00\n        02:48:59 PM    3    0.00    0.00    0.00    0.00    0.00    0.00  100.50      3.52\n        02:48:59 PM    4   35.68    0.00    1.01    0.00    0.00    1.01   62.81     13.57\n        02:48:59 PM    5    4.52    0.00    0.00    0.00    0.00    0.00   96.48      0.50\n        02:48:59 PM    6    3.52    0.00    0.00    0.00    0.00    0.00   96.98      0.50\n        02:48:59 PM    7   25.13    0.00    7.54    0.00    0.00    4.02   63.82      6.03\n\n        02:48:59 PM  CPU   %user   %nice %system %iowait    %irq   %soft   %idle    intr/s\n        02:49:01 PM  all   12.50    0.00    1.19   11.69    0.00    0.56   74.06   1177.11\n        02:49:01 PM    0   22.89    0.00    1.49    0.00    0.00    1.49   74.13    995.52\n        02:49:01 PM    1    5.97    0.00    0.50   92.54    0.00    0.50    0.00    159.70\n        02:49:01 PM    2    0.50    0.00    0.50    0.50    0.00    0.00   98.01      1.99\n        02:49:01 PM    3    0.00    0.00    0.00    0.00    0.00    0.00   99.50      2.49\n        02:49:01 PM    4   45.77    0.00    1.49    0.00    0.00    0.50   51.24     11.94\n        02:49:01 PM    5    0.00    0.00    0.00    0.00    0.00    0.00   99.50      0.50\n        02:49:01 PM    6    0.50    0.00    0.00    0.00    0.00    0.00   99.00      0.50\n        02:49:01 PM    7   23.38    0.00    5.47    0.00    0.00    1.99   68.16      4.48\n\n        02:49:01 PM  CPU   %user   %nice %system %iowait    %irq   %soft   %idle    intr/s\n        02:49:03 PM  all   13.05    0.00    1.12   11.62    0.00    0.50   73.70   1179.00\n        02:49:03 PM    0   43.50    0.00    0.50    0.00    0.00    0.00   56.00   1000.50\n        02:49:03 PM    1    6.50    0.00    1.00   93.00    0.00    0.50    0.00    157.00\n        02:49:03 PM    2    1.50    0.00    0.50    0.00    0.00    0.00   98.50      0.00\n        02:49:03 PM    3    0.00    0.00    0.00    0.00    0.00    0.00  100.00      2.50\n        02:49:03 PM    4   32.50    0.00    1.50    0.00    0.00    1.00   65.50     13.00\n        02:49:03 PM    5   11.00    0.00    4.00    0.00    0.00    1.50   83.50      0.50\n        02:49:03 PM    6    0.00    0.00    0.00    0.00    0.00    0.00  100.00      0.00\n        02:49:03 PM    7   10.50    0.00    2.00    0.00    0.00    1.00   87.00      5.50\n</code></pre>\n"},{"tags":["mysql","performance","view","where"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":34,"score":1,"question_id":12836791,"title":"Functionality of VIEW's with WHERE","body":"<p>I'm trying to find out more about MySQL VIEW's, and I'm still quite confused (especially, about the details)</p>\n\n<p>If I get it right, a view is nothng more than a stored SELECT query that gets used. But my problem starts, when I want to use certain values for the WHERE clause inside of the VIEW.</p>\n\n<p>Lets suppose, we have this VIEW:</p>\n\n<pre><code>SELECT `user`.`id`, `userrole`.`key` AS `role`, `userstatus`.`key` AS `status`\nFROM `user`\nLEFT JOIN `userrole`\nON `userrole`.`id` = `user`.`userrole_id`\nLEFT JOIN `userstatus`\nON `userstatus`.`id` = `user`.`userstatus_id`\n</code></pre>\n\n<p>Now, this would deliver a List of ALL users, with their current role / status</p>\n\n<pre><code>+----+-------+--------+\n| id |  role | status |\n+----+-------+--------+\n|  1 | admin | active |\n|  2 |  user | active |\n|  3 |  user | active |\n|  4 | admin | active |\n| .. |   ... |    ... |\n+----+-------+--------+\n</code></pre>\n\n<p>Now, I just want to know the role / status of the user with the <code>ID</code> 3 - so I would have to do an additional <code>SELECT</code> on the <code>VIEW</code>, with <code>WHERE id = 3</code>, right? But if I do that, what will happen exactly?</p>\n\n<p>Will the <code>VIEW</code> still <code>SELECT</code> all the users, and then the whole data will be dropped, and only the <code>id = 3</code> will be displayed? Or will it be optimized so that the <code>VIEW</code> only selects <code>user.id = 3</code> ?</p>\n\n<p>I'm confused about the performance aspect of this - since if I would have 1 million users, using a <code>VIEW</code> for this would be a rather bad idea. But on the other hand, I would save myself to use the whole <code>SELECT</code> with all the <code>JOIN</code>'s every time, if I have the <code>VIEW</code> for this.</p>\n\n<p>So how exactly does the <code>VIEW</code> act? As simple SUBQUERY, or is it more optimized if you add a <code>WHERE</code> when selecting a <code>VIEW</code>?</p>\n\n<p>I guess if it acts like a SUBQUERY, a <code>STORED PROCEDURE</code> with parameters will always be better than a <code>VIEW</code>, then (if you have <code>WHERE</code> parameters that vary)?</p>\n\n<p>Thanks for the informations</p>\n"},{"tags":["android","performance","upload","emulator","apk"],"answer_count":12,"favorite_count":2,"up_vote_count":13,"down_vote_count":0,"view_count":2701,"score":13,"question_id":7597309,"title":"Slow uploads to running Android emulator","body":"<p>I have searched but not found any queries or answers for my specific circumstance. I have a fast new machine with plenty of memory running Windows 7. I'm using the latest Eclipse and Android SDK.</p>\n\n<p>When I run an app the emulator takes about 45 seconds to start (blazingly fast for the emulator!) from run initiation to running my app.</p>\n\n<p>If I then change the app and re-run it on the still-running emulator, the time increases to 70+ seconds! As opposed to all other complaints people have, in my case restarting the emulator is faster than using the existing instance. I don't like that.</p>\n\n<p>Here are the times:</p>\n\n<pre><code>2011-09-29 13:07:13 - hello Uploading hello.apk onto device 'emulator-5554'\n2011-09-29 13:07:18 - hello Installing hello.apk...\n2011-09-29 13:07:37 - hello Success!\n</code></pre>\n\n<p>on rerunning after changing the app to get it to reload:</p>\n\n<pre><code>2011-09-29 13:08:18 - hello Uploading hello.apk onto device 'emulator-5554'\n2011-09-29 13:09:16 - hello Installing hello.apk...\n2011-09-29 13:09:24 - hello Success!\n</code></pre>\n\n<p>As you can see, the upload to the emulator takes a mere 5 seconds when the emulator is freshly started. It takes nearly a minute with a running emulator! This is the cause of the extended re-running time. This doesn't change even when I uninstall the app on the emulator before re-running it.</p>\n\n<p>Any ideas on what I could try to solve this? It appears to be some kind of communication problem, possibly with adb.</p>\n"},{"tags":["java","performance","spring","struts2"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":48,"score":0,"question_id":12835281,"title":"Regarding profilers for web based application","body":"<p>I have an web application , it runs quite slow in other words Hitting the request from one page to other takes a long time , I have to measure it performance ,Please advise me what tools are there to measure the performance of the web based application , like profilers..!!\nApllicastion is mainly build in struts spring and ibatis.</p>\n\n<p>Please tell the profiler for web which are free..!!</p>\n"},{"tags":["performance","maven","maven-war-plugin"],"answer_count":2,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":77,"score":3,"question_id":12818262,"title":"Speed up maven war plugin","body":"<p>The maven plugin works very slow for me. In my project the webapp folder has nearly 15000 small files (images, jsp, css, etc). When I assemble it with the maven, it first copies all files to the <code>target/myProject-1.0.0</code> directory, then builds <code>myProject-1.0.0.war</code> file from it. The copy process takes 10 minutes, building the <code>.war</code> takes 2 minutes.</p>\n\n<p>As I see the build could be much faster if the <code>.war</code> file will be assembled straight from the webapp folder. Is it possible to do?</p>\n"},{"tags":["java","performance","monitoring","metrics"],"answer_count":18,"favorite_count":29,"up_vote_count":27,"down_vote_count":0,"view_count":16265,"score":27,"question_id":130067,"title":"Best server Performance Monitoring Tool for Java Servers","body":"<p>At work, we are trying to combat the monstrosity that our application servers have become - we are now running out of server resources after many months of resource-creep. </p>\n\n<p>I would like to begin the process of finding out what to investigate by seeing what people think the best server monitoring tools are. </p>\n\n<p>Specifically, I'm interested in tools that work with Java and provide method hit counts and runtime, can run stress testing, report on Exception throws, uptime, load, etc. They don't necessarily all need to be a single tool, although that would be great. </p>\n\n<p>What are the best tools out there for this sort of thing? How has your experience been with them? What are the costs involved?</p>\n\n<p>Thanks!</p>\n"},{"tags":["jquery","performance","dom"],"answer_count":2,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":75,"score":4,"question_id":12835275,"title":"Difference between creation of DOM elements in jQuery","body":"<p>I'm wondering if there is a difference in performance (or what is the best practice) in creating DOM elements with jQuery.</p>\n\n<p>By my knowlegde there are 3 ways to do this:</p>\n\n<ol>\n<li><p>By string:</p>\n\n<pre><code>$('&lt;a href=\"http://www.example.com\" class=\"footerLink\" rel=\"external\"&gt;example&lt;/a&gt;');`\n</code></pre></li>\n<li><p>Create element first, add attributes later:</p>\n\n<pre><code>$('&lt;a&gt;&lt;/a&gt;')\n  .addClass('footerLink')\n  .attr({ \n     rel: 'external, \n     href: 'http://www.example.com' \n  })\n  .text('example');\n</code></pre></li>\n<li><p>Create element and pass attributes object with it:</p>\n\n<pre><code>$('&lt;a&gt;&lt;/a&gt;', {\n  'class': 'footerLink', \n  href: 'http://www.example.com',\n  rel: 'external'\n})\n.text('example');\n</code></pre></li>\n</ol>\n\n<p><strong>EDIT:</strong>\nWhat if you are appending a lot of items to an element? Should you make a very long <code>string</code> first and append that after the loop?</p>\n"},{"tags":["java","performance","data-structures","linkedhashset"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":46,"score":0,"question_id":12826916,"title":"Ordered insertion in linkedHashSet, any performant way ?","body":"<p>So I have a LinkedHashSet , with values  say   a1, a2, , b, c1, c2</p>\n\n<p>I want to replace,   b  with  x , such that the order of x should be same as order of b.</p>\n\n<p>One obvious way would be </p>\n\n<pre><code> private LinkedHashSet&lt;String&gt; orderedSubstitution(final Set&lt;String&gt; originalOrderedSet, final String oldItem,\n            final String newItem) {\n        final LinkedHashSet&lt;String&gt; newOrderedSet = new LinkedHashSet&lt;String&gt;();\n        // Things we do to maintain order in a linkedHashSet\n        for (final String stringItem : originalOrderedSet) {\n            if (stringItem.equals(oldItem)) {\n                newOrderedSet.add(newItem);\n            } else {\n                newOrderedSet.add(stringItem);\n            }\n        }\n        return newOrderedSet;\n    }\n</code></pre>\n\n<p>not only this is O(n) i also <em>feel</em> this is not the fastest way.  Any better solution ?\nNOTE : I HAVE TO use linkedHashMap.</p>\n"},{"tags":["performance","memory","webserver","hardware","cpu"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":33,"score":0,"question_id":12835235,"title":"How to speed up this development machine more with minimum $ and time?","body":"<p>What could I do to speed up apps open time on this development box ?\nThe overall context is I have a box that being used for daily development work. The apps open too slow (I think!), sometime they seem to be hang. </p>\n\n<p><strong>Its daily jobs:</strong></p>\n\n<ul>\n<li>Handle image processing/pattern recognition of 10K+ small jpeg/gif/png (each file &lt;=100Kb).</li>\n<li>Handle DB transaction (100K+ transaction) and file I/O (copy/move 150GB of data).</li>\n<li>Serve as test server (for few users only, but need good response time, page load should &lt;=100ms).</li>\n</ul>\n\n<p><strong>Current hardware specs:</strong></p>\n\n<ul>\n<li><p>CPU: i7 2.2G (CPU usage is medium, ~20%)</p></li>\n<li><p>Mem: 16GB DDR3 (mem usage is low, ~4GB only).</p></li>\n<li><p>1x Kingston SSD 128G SATA3.</p></li>\n<li><p>2x1TB HDD as storage (running on RAID1)</p></li>\n</ul>\n\n<p><strong>Software specs:</strong></p>\n\n<ul>\n<li><p>MS-platform: Win 7 Ultimate 64bit, Visual Studio 2010/2012, MSSQL 2008R2.</p></li>\n<li><p>Linux platform: CentOS, Eclipse/MySQL, Apache, memcached, nodejs.</p></li>\n</ul>\n\n<p><strong>Others thoughts:</strong>\nSometimes the box also need to serve as file server (for movie/torrent ^^).</p>\n\n<p>Is there anyway I can speed up the apps open time (VS2010/2012, MySQL,Eclipse), with minimum amount of time and $ ? Help is greatly appreciated :)</p>\n"},{"tags":["optimization","testing","vba","profiling","performance"],"answer_count":5,"favorite_count":4,"up_vote_count":12,"down_vote_count":0,"view_count":21325,"score":12,"question_id":198409,"title":"How do you test running time of VBA code?","body":"<p>Is there code in VBA I can wrap a function with that will let me know the time it took to run, so that I can compare the different running times of functions?</p>\n"},{"tags":["php","performance","boolean","boolean-logic"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":38,"score":1,"question_id":12834666,"title":"Convention of setting return type in php boolean functions","body":"<p>Is it good to explicitly 'return FALSE' in a Boolean function or just 'return TRUE'</p>\n\n<p>Example A:</p>\n\n<pre><code>  function check($link){\n  if(isset($link)){\n  return TRUE;\n  }else{\n  return FALSE;\n  }\n</code></pre>\n\n<p><strong>OR</strong>\nExample B:</p>\n\n<pre><code>  function check($link){\n  if(isset($link)){\n  return TRUE;\n  }\n</code></pre>\n\n<p>By default php functions return FALSE, so  is it good if I mention return FALSE or ignore it as Example B</p>\n\n<p>so if im checking whether its true or false</p>\n\n<pre><code> if(check('eqweqw')){\n  echo 'its set';\n }else{\n echo 'its not';\n }\n</code></pre>\n"},{"tags":["ios","performance","uitableview","scrolling"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":58,"score":2,"question_id":12822438,"title":"Different UITableView scrolling performance on iPad 2 & iPhone 4s","body":"<p><strong>Solved</strong></p>\n\n<p>There was a shadow on the self.view layer. After removed it, the tableview scrolls smoothly (56-59fps).</p>\n\n<p>The problem I got is so weird. </p>\n\n<p>I am building a UITableView based Twitter like App. After optimized the table view loading and scrolling, I tested the App (same code &amp; same UI) on both iPad 2 and iPhone 4S. On the iPad 2, I got 56-59fps smooth scrolling animation. But on the iPhone 4S, I got only 43-49fps scrolling animation, which is awful.</p>\n\n<p>This seems not caused by the Retina display. After I remove all images from cells, the problem is still there. The following is the code of cellForRowAtIndexPath</p>\n\n<pre><code>- (UITableViewCell *)tableView:(UITableView *)tableView cellForRowAtIndexPath:(NSIndexPath *)indexPath\n{\n    // fetch note from the fetched results controller\n    Note *note = [self.fetchedResultsController objectAtIndexPath:indexPath];\n\n    if ([note.photos count] &gt; 0) {\n\n        static NSString *CellIdentifier = @\"Photo Cell\";\n        PhotoCell *cell = (PhotoCell *)[tableView dequeueReusableCellWithIdentifier:CellIdentifier];\n\n        if (cell == nil){\n            cell = [[PhotoCell alloc] initWithStyle:UITableViewCellStyleDefault reuseIdentifier:CellIdentifier];\n        } else {\n            [cell clearContents];\n        }\n\n        NSNumber *currentRowNumber = [NSNumber numberWithInt:indexPath.row];\n        // check if the photos of this cell has been pre loaded\n        NSDictionary *coverImages = [self.coverImagesForNotes objectForKey:currentRowNumber];\n\n        if (coverImages == nil) {\n            if (self.tableView.dragging == NO &amp;&amp; self.tableView.decelerating == NO) {\n                coverImages = [self loadCoverImagesForCell:indexPath photos:note.photos];\n                [self.coverImagesForNotes setObject:coverImages forKey:currentRowNumber];\n            } \n        }\n\n        // assign contents to the cell\n        [self compositeContentView:cell forNote:note rowNumber:indexPath.row];\n\n        // refine\n        unsigned textContentHeight = [[self.cellTextHeightDict objectForKey:currentRowNumber] integerValue];\n        unsigned currentPageNumber = [self currentPageNumberAtRow:indexPath.row];\n        [cell initPhotoVideoView:textContentHeight photos:note.photos coverImages:coverImages showPage:currentPageNumber rowNumber:indexPath.row];\n\n        cell.delegate = self;\n\n        return cell;\n\n    } else {\n        static NSString *CellIdentifier = @\"Simple Cell\";\n        BaseCell *cell = (BaseCell *)[tableView dequeueReusableCellWithIdentifier:CellIdentifier];\n\n        if (cell == nil){\n            cell = [[OneViewBaseCell alloc] initWithStyle:UITableViewCellStyleDefault reuseIdentifier:CellIdentifier];\n        }\n\n        // assign contents to the cell\n        [self compositeContentView:cell forNote:note rowNumber:indexPath.row];\n\n        return cell;\n    }\n}\n</code></pre>\n\n<p>Thanks for answers.</p>\n"},{"tags":["performance","apache","wamp"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":63,"score":1,"question_id":12834207,"title":"Apache server running very slow","body":"<p>I've installed latest wamp server on </p>\n\n<p>Windows Server 2008 R2 x64. <br>\nIntel Xeon x5570 2.93GHz<br>\n8 GB</p>\n\n<p>When I request a php page locally using localhost/index.php it loads very fast.</p>\n\n<p>Ex :</p>\n\n<ul>\n<li>(Page generated in 0.0396 seconds) </li>\n<li>(Page generated in 0.0406 seconds)</li>\n<li>(Page generated in 0.0334 seconds)</li>\n</ul>\n\n<p>When I request the same page (192.168.1.3/index.php) from another machine on my network it's very slow.<br>\nBut the page generation time is still the same!. Php does all queries and process.<br>\nIt just loads to other machines very slowly.<br></p>\n\n<p>When I try to open shares on 192.168.1.3 using \\192.168.1.3\\ it works fast.<br>\nNo slowness when copying files.<br></p>\n\n<p>What may be the problem ??<br>\nFirewall is turned off<br>\nHow to fix it ?</p>\n"},{"tags":["performance","java-ee","websphere","health-monitoring"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":45,"score":0,"question_id":12825602,"title":"How to exploit \"Soft Reference Statistic\" and \"Weak Reference Statistics to improve a Java EE application","body":"<p>I'm focused on </p>\n\n<ul>\n<li>Derive architectural understanding of our Java application through footprint analysis</li>\n<li>Improve application performance by tuning memory footprint and optimizing Java collections and Java cache usage</li>\n</ul>\n\n<p>For this purpose I ran a load test and analyzed a Java heap dump snapshots by using the Java heap analysis tool <a href=\"http://www.ibm.com/developerworks/java/jdk/tools/memoryanalyzer/\" rel=\"nofollow\">memoryanalyzer</a> on our Java EE application which uses WebSphere application server v7.</p>\n\n<p>I'm asking if the \"Soft Reference Statistic\" and \"Weak Reference Statistics\" can help for my goal, i.e: \nto understand if we have a problem or there is something to fix ( or just improve ) in the application java code or in the configurazione of the application server.</p>\n\n<p>advice to figure out how to figure out how to exploit these data would be greatly appreciated</p>\n\n<blockquote>\n  <p>Soft Reference Statistics</p>\n  \n  <p>A total of 11.416 java.lang.ref.SoftReference objects have been found,\n  which softly reference 393 objects.\n  2.414 objects totalling 122,9 KB are retained (kept alive) only via soft references. No objects totalling 0 B are softly referenced and\n  also strongly retained (kept alive) via soft references.</p>\n  \n  <p>Weak Reference Statistics</p>\n  \n  <p>A total of 28.849 java.lang.ref.WeakReference objects have been found,\n  which weakly reference 11.663 objects.\n  132.437 objects totalling 7,4 MB are retained (kept alive) only via weak references. No objects totalling 0 B are weakly referenced and\n  also strongly retained (kept alive) via weak references.</p>\n</blockquote>\n"},{"tags":["sql","sql-server","performance","query"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":72,"score":1,"question_id":12832988,"title":"SQL Server query response time changes greatly with very little code change","body":"<p>I have a query that when modified ever so slightly will reduce its runtime from approx 37 seconds down to 4 seconds. There is no change to the joins, or the columns being returned.</p>\n\n<p><strong>SLOW query (37 seconds):</strong></p>\n\n<pre><code>declare @PeriodFrom DateTime\ndeclare @PeriodTo DateTime\nSet @PeriodFrom = '2012-06-01'\nSet @PeriodTo = '2012-06-30'\n\nSelect\n  0 as PrimaryAccount,\n  0 as PrintOrder,\n  Cast(Null as integer) as ID,\n  Sum(IsNull(MT.Amount, 0)) as Amount,\n  Cast(0 as Money) as NetAmount,\n  Cast(0 As Money) as TaxAmount,\n  Cast(0 as Money) as AmountOutstanding,\n  Cast(0 as Money) as AmountPaid,\n  'Balance brought forward' as Description\nFrom\n  db_site4.dbo.AccountReceivable P\nJoin\n  db_site4.dbo.ARType ART on ART.ARTypeID = P.ARTypeID and ART.ARTypeID = 24\nleft Join\n  db_site4.dbo.vw_MemberTransactions MT\non\n  P.AccountReceivableID = MT.AccountReceivableID\nwhere \n  (MT.AccountingDate &lt;= @PeriodFrom or MT.AccountingDate is null)\nand\n  (Authorised = 1 or Authorised is Null)\nand\n  IsHidden = 0\nand\n  P.MemberID = 123\n</code></pre>\n\n<p><strong>SQL I/O Statistics for the above:</strong></p>\n\n<pre><code>Table 'Payment'. Scan count 16, logical reads 23558, physical reads 19, read-ahead reads 5448.\nTable 'InvoiceItemPayment'. Scan count 4, logical reads 22237, physical reads 51, read-ahead reads 13432.\nTable 'UnallocatedPayment'. Scan count 12, logical reads 431, physical reads 1, read-ahead reads 80.\nTable 'AccountReceivable'. Scan count 1, logical reads 2, physical reads 0, read-ahead reads 0.\nTable 'ARType'. Scan count 1, logical reads 2, physical reads 0, read-ahead reads 0.\nTable 'Invoice'. Scan count 11116, logical reads 116984, physical reads 190, read-ahead reads 30910.\nTable 'InvoiceItem'. Scan count 5122, logical reads 99786, physical reads 316, read-ahead reads 46236.\n</code></pre>\n\n<p><strong>Now for the query that returns in 4 seconds:</strong></p>\n\n<pre><code>declare @PeriodFrom DateTime\ndeclare @PeriodTo DateTime\nSet @PeriodFrom = '2012-06-01'\nSet @PeriodTo = '2012-06-30'\n\nSelect\n  0 as PrimaryAccount,\n  0 as PrintOrder,\n  Cast(Null as integer) as ID,\n  Sum(IsNull(MT.Amount, 0)) as Amount,\n  Cast(0 as Money) as NetAmount,\n  Cast(0 As Money) as TaxAmount,\n  Cast(0 as Money) as AmountOutstanding,\n  Cast(0 as Money) as AmountPaid,\n  'Balance brought forward' as Description\nFrom\n  db_site4.dbo.AccountReceivable P\nJoin\n  db_site4.dbo.ARType ART on ART.ARTypeID = P.ARTypeID and ART.ARTypeID = 24\nleft Join\n  db_site4.dbo.vw_MemberTransactions MT\non\n  P.AccountReceivableID = MT.AccountReceivableID\nwhere \n  (MT.AccountingDate &lt;= @PeriodFrom or MT.AccountingDate is null)\nand\n  (Authorised = 1 or Authorised is Null)\nand\n  (IsHidden = 0 or IsHidden is null)\nand\n  P.MemberID = 123\n</code></pre>\n\n<p><strong>SQL I/O statistics for the above:</strong></p>\n\n<pre><code>Table 'Payment'. Scan count 6271, logical reads 19857, physical reads 0, read-ahead reads 0.\nTable 'UnallocatedPayment'. Scan count 2, logical reads 4, physical reads 0, read-ahead reads 0.\nTable 'InvoiceItemPayment'. Scan count 4399, logical reads 33400, physical reads 0, read-ahead reads 0.\nTable 'InvoiceItem'. Scan count 10581, logical reads 60682, physical reads 4, read-ahead reads 0.\nTable 'Invoice'. Scan count 3, logical reads 22102, physical reads 3, read-ahead reads 0.\nTable 'AccountReceivable'. Scan count 1, logical reads 2, physical reads 0, read-ahead reads 0.\nTable 'ARType'. Scan count 1, logical reads 2, physical reads 0, read-ahead reads 0.\n</code></pre>\n\n<p><strong>My question is:</strong> how can there be such a difference in execution time between the two, when the only change that is made is replacing <code>IsHidden = 0</code> with <code>(IsHidden = 0 or IsHidden IS NULL)</code> ? (3 lines from the bottom)</p>\n"},{"tags":["java","performance","benchmarking","caliper"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":41,"score":0,"question_id":12830895,"title":"Get logarithmic benchmark runtime from Caliper","body":"<p>I've recently discovered the <a href=\"http://code.google.com/p/caliper/\" rel=\"nofollow\">Caliper benchmark framework for Java</a> which seems like a very useful tool for microbenchmarks. I'm using it to run microbenchmarks for my vector maths library (<a href=\"https://github.com/mikera/vectorz\" rel=\"nofollow\">vectorz</a>)</p>\n\n<p>However the standard \"SimpleBenchmark\" output gives a <strong>linear</strong> runtime chart which is not very useful if you have very different execution times:</p>\n\n<pre><code>        benchmark     ns linear runtime\n  Vector3Addition   1.04 =\n  Matrix3Rotation   4.92 =\nVectorAddMultiple   6.29 =\n    MatrixInverse 955.27 ==============================\n</code></pre>\n\n<p>How do you configure Caliper to output <strong>logarithmic</strong> runtime in the chart?</p>\n\n<p>I'm running Caliper via code rather than from the command line: <a href=\"https://github.com/mikera/vectorz/blob/master/src/test/java/mikera/vectorz/performance/PerformanceTest.java\" rel=\"nofollow\">source code here</a></p>\n"},{"tags":["c","performance","algorithm","sorting","linked-list"],"answer_count":9,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":895,"score":3,"question_id":9302726,"title":"Sorting doubly linked list in c","body":"<p>I want to keep a linked list in sorted order when inserting elements (about 200000 elements in the list), which algorithm can you recommend? I made a simple implementation using insertion sort, but its performance is very very bad (a lot of CPU usage).</p>\n\n<p>Thanks for your help.</p>\n\n<p>I did some comparison between merge sort and insertion sort but it seems that insertion sort has better performance, I am a bit confused by this result. Can you tell me what's wrong and if there is a better algorithm?</p>\n\n<p>My code (for simplicity, I omitted the prev node in the node struct):</p>\n\n<pre><code>struct node {\n    int number;\n    struct node *next;\n};\n</code></pre>\n\n<p>Insertion sort :</p>\n\n<pre><code>void insert_node(int value) {\n    struct node *new_node = NULL;\n    struct node *cur_node = NULL;\n    struct node *last_node = NULL;\n    int found; /* 1 means found a place to insert the new node in, 0 means not*/\n\n\n    new_node = (struct node *)malloc(sizeof(struct node *));\n    if(new_node == NULL) {\n        printf(\"memory problem\\n\");\n    }\n    new_node-&gt;number = value;\n    /* If the first element */\n    if (head == NULL) {\n        new_node-&gt;next = NULL;\n        head = new_node;\n    } \n\n    else if (new_node-&gt;number &lt; head-&gt;number) {\n        new_node-&gt;next = head;\n        head = new_node;    \n    } \n\n    else {\n        cur_node = head;\n        found = 0;\n        while (( cur_node != NULL ) &amp;&amp; ( found == 0 )) {\n            if( new_node-&gt;number &lt; cur_node-&gt;number )\n            {\n                found = 1;\n            }\n            else\n            {\n                last_node = cur_node;\n                cur_node = cur_node-&gt;next;\n            }\n        }\n    /* We got the right place to insert our node */\n    if( found == 1 )\n    {\n        new_node-&gt;next = cur_node; \n    }\n    /* Insert at the tail of the list */\n    else\n    {\n        last_node-&gt;next = new_node;\n        new_node-&gt;next = NULL;\n    }           \n}\n</code></pre>\n\n<p>Merge Sort :</p>\n\n<pre><code>/* add a node to the linked list */\nstruct node *addnode(int number, struct node *next) {\n    struct node *tnode;\n\n    tnode = (struct node*)malloc(sizeof(*tnode));\n\n    if(tnode != NULL) {\n        tnode-&gt;number = number;\n        tnode-&gt;next = next;\n    }\n\n    return tnode;\n}\n\n/* perform merge sort on the linked list */\nstruct node *merge_sort(struct node *head) {\n    struct node *head_one;\n    struct node *head_two;\n\n    if((head == NULL) || (head-&gt;next == NULL))\n        return head;\n\n    head_one = head;\n    head_two = head-&gt;next;\n    while((head_two != NULL) &amp;&amp; (head_two-&gt;next != NULL)) {\n        head = head-&gt;next;\n        head_two = head-&gt;next-&gt;next;\n    }\n    head_two = head-&gt;next;\n    head-&gt;next = NULL;\n\n    return merge(merge_sort(head_one), merge_sort(head_two));\n}\n\n/* merge the lists.. */\nstruct node *merge(struct node *head_one, struct node *head_two) {\n    struct node *head_three;\n\n    if(head_one == NULL)\n        return head_two;\n\n    if(head_two == NULL)\n        return head_one;\n\n    if(head_one-&gt;number &lt; head_two-&gt;number) {\n        head_three = head_one;\n        head_three-&gt;next = merge(head_one-&gt;next, head_two);\n    } else {\n        head_three = head_two;\n        head_three-&gt;next = merge(head_one, head_two-&gt;next);\n    }\n\n    return head_three;\n}\n</code></pre>\n"},{"tags":["java","multithreading","performance","swing"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":65,"score":0,"question_id":12733297,"title":"Running two expensive database calls in parallel","body":"<p>I have (say) 2 functions which does a db-hit to fetch a lot of data. Since the two functions are executed one after the other (by the same thread), the time taken is T(f(1)) + T(f(2)). How can I execute the two functions in parallel (by means of creating 2 threads) so that the total time taken is: T(max(T(f1), T(f2)) </p>\n\n<p>I am done writing my complete java swing application and want to optimize it for performance now. Appreciate any insight, and excuse if the question is too naive.</p>\n\n<p>Thank you!</p>\n"},{"tags":["jquery","performance","variables","jquery-selectors"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":12831995,"title":"Should I combine jQuery selectors into variables for better performance?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/5724400/does-using-this-instead-of-this-provide-a-performance-enhancement\">Does using $this instead of $(this) provide a performance enhancement?</a>  </p>\n</blockquote>\n\n\n\n<p>So I am creating a theater like photo gallery that takes up the entire screen, I change the sizes of the divs and Images depending on the window size. So to do this I have been using:</p>\n\n<pre><code>function setSizes()\n{\nvar theaterHeight = $(\".theater-wrapper\").height();\nvar theaterWidth = $(\".theater-wrapper\").width();\n}\n\n$(window).on(\"resize\", window, function() {\n        setSizes();\n    }); \n</code></pre>\n\n<p>That's basically what I am doing to select the elements in order to get the dimensions to set the heights and widths and such, however window resize does get a bit laggy, and I do end up having quite a few selectors and changing quite a few divs and images all in this one function, and also sometimes the initial call doesn't work properly, or vice versa. So my question is would comibining the selectors result in better performance and such?</p>\n\n<p>like:</p>\n\n<pre><code>var $theaterWrapper = $(\".theater-wrapper\");\n\nvar theaterHeight = $theaterWrapper.height();\nvar theaterWidth = $theaterWrapper.width();\n</code></pre>\n"},{"tags":["java","performance","mouse","velocity","awtrobot"],"answer_count":3,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":1077,"score":2,"question_id":5339325,"title":"Java Robot mouse move: setting speed?","body":"<p>The Java Robot class allows one to move the mouse as if the actual physical mouse was moved.</p>\n\n<p>However, how does one move the mouse from Point1 to Point2 in a humane (and thus not instant) manner? Aka, how does one set the speed of movement?</p>\n\n<p>If no such speed is possible with the Robot class, thus if the mouse can only be moved instantenously, what kind of \"algorithm\" should be used to mimic a human's mouse movement? Should it move the mouse pixel by pixel with a certain incrementing speed?</p>\n"},{"tags":["java","performance"],"answer_count":3,"favorite_count":1,"up_vote_count":0,"down_vote_count":7,"view_count":73,"score":-7,"question_id":12831719,"title":"Fastest way to check a string is alphanumeric in Java","body":"<p>What is the fastest way to check that a String contains only alphanumeric characters.</p>\n\n<p>I've got some code that is going to chew up a lot of CPU and I wonder if there is going to be a quicker way than using pre-compiled regular expressions.</p>\n\n<p><strong>UPDATE:</strong> So many down votes, care to explain? Stackoverflow should not be used to discuss which algorithm to use to achieve a task might be faster?</p>\n"},{"tags":["algorithm","performance","math","combinatorics"],"answer_count":9,"favorite_count":5,"up_vote_count":4,"down_vote_count":0,"view_count":1311,"score":4,"question_id":5307222,"title":"How to calculate the index (lexicographical order) when the combination is given","body":"<p>I know that there is an algorithm that permits, given a combination of number (no repetitions, no order), calculates the index of the lexicographic order.<br>\nIt would be very useful for my application to speedup things...</p>\n\n<p>For example:  </p>\n\n<pre><code>combination(10, 5)  \n1 - 1 2 3 4 5  \n2 - 1 2 3 4 6  \n3 - 1 2 3 4 7  \n....  \n251 - 5 7 8 9 10  \n252 - 6 7 8 9 10  \n</code></pre>\n\n<p>I need that the algorithm returns the index of the given combination.<br>\nes: <code>index( 2, 5, 7, 8, 10 )</code> --> index  </p>\n\n<p>EDIT: actually I'm using a java application that generates all combinations C(53, 5) and inserts them into a TreeMap.\nMy idea is to create an array that contains all combinations (and related data) that I can index with this algorithm.<br>\nEverything is to speedup combination searching.\nHowever I tried some (not all) of your solutions and the algorithms that you proposed are slower that a get() from TreeMap.<br>\nIf it helps: my needs are for a combination of 5 from 53 starting from 0 to 52.</p>\n\n<p>Thank you again to all :-)</p>\n"},{"tags":["c++","performance","assembly"],"answer_count":14,"favorite_count":112,"up_vote_count":458,"down_vote_count":18,"view_count":33786,"score":440,"question_id":12135518,"title":"Is < faster than <=?","body":"<p>I'm reading a book where the author says that <code>if( a &lt; 901 )</code> is faster than <code>if( a &lt;= 900 )</code>. Not exactly as this simple example, but slightly performance changes on loop complex code. I suppose this has to do something with ASM in case it's even true.</p>\n"},{"tags":[".net","asp.net-mvc","performance","iis"],"answer_count":9,"favorite_count":106,"up_vote_count":73,"down_vote_count":0,"view_count":6473,"score":73,"question_id":2246251,"title":"How To improve ASP.NET MVC Application Performance","body":"<p><strong>How do you improve your ASP.NET MVC application performance?</strong></p>\n"},{"tags":["facebook","performance","google","indexing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":47,"score":0,"question_id":12829016,"title":"How do Databases of Facebook and Google etc work?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/3479720/what-database-does-facebook-use\">What database does Facebook use?</a>  </p>\n</blockquote>\n\n\n\n<p>These sites hold data of millions of users and despite that their performance is blazingly fast. So how do manage this kind of data? The best thing we can do is index the data but after certain limit even that thing will fail to work. So what techniques are used by these website?</p>\n"}]}
{"total":25593,"page":13,"pagesize":100,"questions":[{"tags":["javascript","html","load","performance","html-rendering"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":177,"score":2,"question_id":5377577,"title":"How to Load Javascript Simultaneously with HTML","body":"<p><strong>Is there any technique to render a javascript file in the same time as the HTML is rendering?</strong></p>\n\n<p>My first idea was to load it into the head in a <code>&lt;script&gt;</code> tag, but as I see this doesn't affects the loading order, or I am false?</p>\n\n<p>The problem is that in some times I need to use javascript to set an element's width when the page loads, and it's really annoying the little vibration what is because the javascript code which sets the elements width after the element was rendered in HTML.</p>\n"},{"tags":["c++","performance","boost","ublas","boost-ublas"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":1199,"score":2,"question_id":7798285,"title":"BOOST uBLAS matrix product extremely slow","body":"<p>Is there a way to improve the boost ublas product performance?</p>\n\n<p>I have two matrices A,B which i want to mulitply/add/sub/...</p>\n\n<p>In MATLAB vs. C++ i get the following times [s] for a 2000x2000 matrix Operations</p>\n\n<pre><code>OPERATION | MATLAB | C++ (MSVC10)\nA + B     |  0.04  |  0.04\nA - B     |  0.04  |  0.04\nAB        |  1.0   | 62.66\nA'B'      |  1.0   | 54.35\n</code></pre>\n\n<p>Why there is such a huge performance loss here?</p>\n\n<p>The matrices are only real doubles.\nBut i also need positive definites,symmetric,rectangular products.</p>\n\n<p>EDIT:\nThe code is trivial</p>\n\n<pre><code>matrix&lt;double&gt; A( 2000 , 2000 );\n// Fill Matrix A\nmatrix&lt;double&gt; B = A;\n\nC = A + B;\nD = A - B;\nE = prod(A,B);\nF = prod(trans(A),trans(B));\n</code></pre>\n\n<p>EDIT 2:\nThe results are mean values of 10 trys. The stddev was less than 0.005</p>\n\n<p>I would expect an factor 2-3 maybe to but not 50 (!)</p>\n\n<p>EDIT 3:\nEverything was benched in Release ( NDEBUG/MOVE_SEMANTICS/.. ) mode.</p>\n\n<p>EDIT 4:\nPreallocated Matrices for the product results did not affect the runtime.</p>\n"},{"tags":["performance","fortran","32bit-64bit","numerical","f2py"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":1616,"score":1,"question_id":1668899,"title":"Fortran: 32 bit / 64 bit performance portability","body":"<p>I've been starting to use Fortran (95) for some numerical code (generating python modules). Here is a simple example:</p>\n\n<pre><code>subroutine bincount (x,c,n,m)\n  implicit none\n  integer, intent(in) :: n,m\n  integer, dimension(0:n-1), intent(in) :: x\n  integer, dimension(0:m-1), intent(out) :: c\n  integer :: i\n\n  c = 0\n  do i = 0, n-1\n    c(x(i)) = c(x(i)) + 1 \n  end do\nend\n</code></pre>\n\n<p>I've found that this performs very well in 32 bit, but when compiled as x86_64 it is about 5x slower (macbook pro core2duo, snow leopard, gfortran 4.2.3 from r.research.att.com). I finally realised this might be due to using 32bit integer type instead of the native type, and indeed when I replace with integer*8, the 64 bit performance is only 25% worse than the 32bit one. </p>\n\n<p>Why is using a 32 bit integer so much slower on a 64 bit machine? Are there any implicit casts going on with the indexing that I might not be aware of?</p>\n\n<p>Is it always the case that 64 bit will be slower than 32 bit for this type of code (I was surprised at this) - or is there a chance I could get the 64 bit compiled version running the same speed or faster?</p>\n\n<p>(<strong>main question</strong>) Is there any way to declare a (integer) variable to be the 'native' type... ie 32 bit when compiled 32 bit, 64 bit when compiled 64 bit in modern fortran. Without this it seems like it is impossible to write portable fortran code that won't be much slower depending on how its compiled - and I think this means I will have to stop using fortran for my project. I have looked at kind and selected_kind but not been able to find anything that does this.</p>\n\n<p>[Edit: the large performance hit was from the f2py wrapper copying the array to cast it from 64 bit int to 32 bit int, so nothing inherent to the fortran.]</p>\n"},{"tags":["ios","performance","uiscrollview","uiscrollviewdelegate"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":105,"score":0,"question_id":12828169,"title":"Lazy Loading UIScrollView - how to make scrolling smoother","body":"<p>I have accomplished lazy loading in a scrollView like this:</p>\n\n<pre><code>-(void)scrollViewDidScroll:(UIScrollView *)myScrollView {\n\n    int currentPage = (1 + myScrollView.contentOffset.x / kXItemSpacingIphone);\n    for (ItemView* itemView in [self.itemRow subviews]){\n        if (itemView.tag &gt;= currentPage-2 &amp;&amp; itemView.tag &lt;= currentPage+2)\n        {\n            //keep it visible\n            if (!itemView.isLoaded) {\n                [itemView layoutWithData:[self.items objectAtIndex:itemView.tag-1]];\n            }\n        }\n        else\n        {\n            //hide it\n            if (itemView.isLoaded) {\n                [itemView unloadData];\n            }\n\n        }\n    }\n}\n</code></pre>\n\n<p>Basically loading the view if it's +/- 2 \"pages\" from being on screen. This greatly reduces the amount of memory I'm using (instead of loading 20+ ItemViews at once), which is good. However, all the loading/unloading does make the scrolling a bit choppy especially on slower devices. Here is what is actually happening on the ItemView loading:</p>\n\n<pre><code>- (void)layoutWithData:(Item*)_data {\n    self.data = _data;\n\n//grab the image from the bundle\n    UIImage *img;\n    NSString *filePath = [[NSBundle mainBundle] pathForResource:_data.image ofType:@\"jpg\"];\n        if(filePath.length &gt; 0 &amp;&amp; filePath != (id)[NSNull null]) {\n            img = [UIImage imageWithContentsOfFile:filePath];\n        }\n\n    UIButton *btn = [UIButton buttonWithType:UIButtonTypeCustom];\n    [btn setImage:img forState:UIControlStateNormal];\n    [btn addTarget:self action:@selector(tapDetected:) forControlEvents:UIControlEventTouchUpInside];\n    btn.frame = CGRectMake(0, 0, kItemPosterWidthIphone, kItemPosterHeightIphone);\n    [self addSubview:btn];\n\n    self.isLoaded = YES;\n\n}\n</code></pre>\n\n<p>And the ItemView unloading:</p>\n\n<pre><code>- (void)unloadData{\n    for(UIView *subview in [self subviews]) {\n        [subview removeFromSuperview];\n    }\n    self.data = nil;\n    self.isLoaded = NO;\n}\n</code></pre>\n\n<p>Again, what can I do to make the loading/unloading faster and therefore the UIScrollView more smooth?</p>\n\n<hr>\n\n<p>Attempting async:</p>\n\n<pre><code>- (void)layoutWithData:(Item*)_data {\n    self.data = _data;\n    self.isLoaded = YES;\n\ndispatch_async(dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_LOW, 0), ^{\n        UIImage *img;\n        NSString *filePath = [[NSBundle mainBundle] pathForResource:_data.image ofType:@\"jpg\"];\n            if(filePath.length &gt; 0 &amp;&amp; filePath != (id)[NSNull null]) {\n                img = [UIImage imageWithContentsOfFile:filePath];\n            }\n\n        dispatch_async(dispatch_get_main_queue(), ^{\n            UIButton *btn = [UIButton buttonWithType:UIButtonTypeCustom];\n            [btn setImage:img forState:UIControlStateNormal];\n            [btn addTarget:self action:@selector(tapDetected:) forControlEvents:UIControlEventTouchUpInside];\n            btn.frame = CGRectMake(0, 0, kItemPosterWidthIphone, kItemPosterHeightIphone);\n            self.imageView = btn;\n            [self addSubview:btn];\n\n\n        });\n\n    });\n}\n</code></pre>\n"},{"tags":["c++","performance","function-call"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":108,"score":0,"question_id":12818665,"title":"have function calls a perceptible overhead in C++?","body":"<p>In my project I have a class where execution time is the first goal. For it I don’t care much about maintenance, order and so on. At least I didn’t care till yesterday... Now I’m in situation where I have to be a little worried about it, too.  </p>\n\n<p>I have a class, say A, which performs multiple scans on images coming from a camera, i.e. a variable width window scans them in real time.</p>\n\n<pre><code>\n\n    class A{  \n    // methods and attributes of A:  \n    ...  \n    void runiterator(){  \n      ...  \n    for{    // change window’s dimension  \n      for{  // rows  \n       for{ // columns  \n          // many lines of code of operations to be executed for each window at each position  \n          ...      \n       }  \n      }   \n     }  \n    }  \n    }; \n\n</code></pre>\n\n<p>Performance shows already a little delay, but I could solve it skipping a limited area of the image. Furthermore I have a second function, say B, which has exactly the same scheme as A, and executes different operations on each scan (and luckily is much faster than A).</p>\n\n<p>Well, now it is time to join all the operations to benefit significantly the overall result. Only that the code would really become messed-up, huge and mixing things that are really different. I thought to define a class X that does the iterations and at each scan executes function calls to one function in A_new and one in B_new. But I’m worried that about 200000x2 function calls per image would result in loss in performance.</p>\n\n<p>What is your advice?  </p>\n\n<p>EDIT<br>\nWith class X that only calls Anew (so it could only be compared to what now is A), I get on the average, out of many repetitions:  </p>\n\n<p>Time for executing X on a series of 56 images       = 6.15 s<br>\nTime for executing A on thesame series of 56 images = 5.98 s  </p>\n\n<p>It seems that my suspects were not so naive.<br>\nThe difference is about 3%, not so much, but still sorry for the loss.  </p>\n\n<p>With __forceinline time is 5.98 s for X as well, but I would prefer not relying on it.</p>\n\n<p>I think that code is optimized and margins for further improvements are very little.<br>\nIndeed it does a lot of stuff on images in a relatively short time.<br>\nProcessing data sequentially is not possible in class A because it is based on the values coming from the images which are impredictable. This is the reason why class B (that manages to do it) is much faster.</p>\n"},{"tags":["php","performance","sorting","file-io","file-search"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":51,"score":0,"question_id":12829624,"title":"Fastest way to search and remove/add line from/to file","body":"<p>I'm wondering what the fastest way is to remove a specific line from a potentially large file, if it exists.</p>\n\n<p>So for example I want the following line</p>\n\n<pre><code>abc\n</code></pre>\n\n<p>to be removed from the file</p>\n\n<pre><code>yxz\nsrtabc\nabc\nefg\n</code></pre>\n\n<p>becoming</p>\n\n<pre><code>yxz\nsrtabc\nefg\n</code></pre>\n\n<p>Also, I want to add a line to the file, if it doesn't already exist.</p>\n\n<p>The order of the file doesn't matter, and can be sorted if it provide a performance boost to the search.</p>\n\n<p>Performance has never been my strong point, so I'm having a little trouble determining the best path to go down here.</p>\n"},{"tags":["c++","performance","debugging","visual-studio-2005","profiling"],"answer_count":4,"favorite_count":2,"up_vote_count":1,"down_vote_count":0,"view_count":100,"score":1,"question_id":12119849,"title":"Huge performance impact between VS2005 debug mode and release mode","body":"<p>I had an issue that a Win32 application has huge performance difference between debug and release build. It takes 20 sec for release, while 6 min for debug build to initialize the application. This is painful since when debugging, it always takes 6 min to proceed the initialization before starting doing anything. So I am looking for a way to tune the performance in debug build.</p>\n\n<p>After running profiler, I found below code is the hot-spot.</p>\n\n<pre><code>class CellList {\n    std::vector&lt;CellPtr&gt;* _cells;\n    iterator begin() { return (*_cells).begin(); }\n    iterator end()   { return (*_cells).end(); }\n    reverse_iterator rbegin() { return (*_cells).rbegin(); }\n    reverse_iterator rend()   { return (*_cells).rend(); }\n    ...\n}\n\nCellList _cellList = ...;\n\nfor (CellList::iterator itr = _cellList.begin(), end = _cellList.end(); itr &lt; end; ++itr) {\n  Cell* cell = *itr;\n  if (cell-&gt;getFoo()) cell-&gt;setBar(true);\n  else cell-&gt;setBar(false);\n}\n\nfor (CellList::iterator itr = _cellList.rbegin(), end = _cellList.rend(); itr &lt; end; ++itr) {\n  Cell* cell = *itr;\n  if (cell-&gt;getFoo2()) cell-&gt;setBar2(true);\n  else cell-&gt;setBar2(false);\n}\n</code></pre>\n\n<p>And these are the hot-spot in the time-base profile result.</p>\n\n<pre><code>std::operator&lt; &lt;std::_Vector_iterator&lt;Cell *,std::allocator&lt;Cell *&gt; &gt;,std::_Vector_iterator&lt;Cell *,std::allocator&lt;Cell *&gt; &gt; &gt;\nstd::_Vector_const_iterator&lt;Cell *,std::allocator&lt;Cell *&gt; &gt;::operator&lt;\nstd::reverse_iterator&lt;std::_Vector_iterator&lt;Cell *,std::allocator&lt;Cell *&gt; &gt; &gt;::operator*\nstd::reverse_iterator&lt;std::_Vector_const_iterator&lt;Cell *,std::allocator&lt;Cell *&gt; &gt; &gt;::reverse_iterator&lt;std::_Vector_const_iterator&lt;Cell *,std::allocator&lt;Cell *&gt; &gt; &gt;&lt;std::_Vector_iterator&lt;Cell *,std::allocator&lt;Cell *&gt; &gt; &gt;\n</code></pre>\n\n<p>I guess it's the iterator operation not being inlined and causes this huge difference. Is there any way to improve this? I can debug in release mode as long as it's still possible to step line by line in the source code and check all the variable values.</p>\n"},{"tags":["performance",".net-4.0","perfview"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":21,"score":1,"question_id":12828491,"title":"How can I see me expensive methods in PerfView","body":"<p>I have created a simple console app and execute it from PerfView via Run Command -> PerfMonTest.exe</p>\n\n<p>I get the log file and see the process of the app. It is expensive as expected (99% CPU ), but when I want to drill down into the expensive methods they are not shown in the list of expensive methods.</p>\n\n<p>Is there something I can do to make them visible?</p>\n\n<p>Here is the view when I selected the process. I would expect CallExpensive and CallCheap in the list:</p>\n\n<p><img src=\"http://i.stack.imgur.com/koAX8.png\" alt=\"enter image description here\"></p>\n\n<p>Selecting the Main Methods doesnt give me the chace to drill further into the called methods</p>\n\n<p><img src=\"http://i.stack.imgur.com/ZA3f4.png\" alt=\"enter image description here\"></p>\n\n<p>Here is the app:</p>\n\n<pre><code>using System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\n\nnamespace PerfMonTest\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            for (int i = 0; i &lt;= 2000; i++)\n            {\n                CallExpensive(1000);\n                CallCheap(1000);\n                CallCheap(400);\n            }\n\n        }\n\n        public static void CallExpensive(int expense)\n        {\n            for (int i = 0; i &lt;= expense; i++)\n            {\n                DateTime checkTime = DateTime.Now;\n                string val = \"10\" + i.ToString();\n            }\n        }\n\n        public static void CallCheap(int expense)\n        {\n            for (int i = 0; i &lt;= expense; i++)\n            {\n                int j = 2;\n            }\n        }\n    }\n}\n</code></pre>\n"},{"tags":["java","performance","websphere","monitoring","websphere-mq"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":1049,"score":2,"question_id":5459908,"title":"IBM Websphere MQ Monitoring","body":"<p>I've been trying to get some performance statistics out of IBM WebSphere MQ to monitor with Spring Source's Hyperic HQ.</p>\n\n<p>I'm after the message enqueue and dequeue rate along with queue depth of a few local queues to ensure messages are being delivered to and consumed by our local application.</p>\n\n<p>Initially attempted to retrieve the data using WMI and windows performance counters however it seems that on one of our servers the counters aren't available for any of the local queues (just a load of temporary queues) and on the other the counters are available but don't always return a value correctly though WMI.</p>\n\n<p>I've tried PCF (using <code>MQIA_MSG_DEQ_COUNT</code>) which wouldn't provide the counter requested. MQSC (Using <code>DISPLAY QUEUE</code> &amp; <code>DISPLAY QSTATUS</code>) which didn't seem to support the queuing rates - only providing last message get / put date and time.</p>\n\n<p>Anyone got an idea how to either get WMI and performance counters working correctly or an alternative to WMI that would provide the statistics I need?</p>\n"},{"tags":["performance","zend-framework","doctrine2"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":76,"score":0,"question_id":12825745,"title":"Zend Framework Application Runtime Benchmarking","body":"<p>Been trying to understand the overall performance of our application by comparing the benchmark we get from our dev environment and our prod environment.</p>\n\n<p>Interestingly, in our dev environment, which is our local machine, we get application run time as fast as 98ms.</p>\n\n<p>The same application runs on avg at 400ms in our production server, which is a VPS with CentOS  5.8 running.</p>\n\n<p>I'm assuming that this increase must be because of network connection lag between web server and database server, since we didn't have this gap in the dev environment, everything is local. </p>\n\n<p>We're using Doctrine 2.0 as an ORM for our application, we haven't really gotten into optimizing it by caching.</p>\n\n<p>Is there a way to optimize this lag time? Or am I completely wrong about the case?</p>\n"},{"tags":["objective-c","performance","nsdictionary"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":233,"score":0,"question_id":10088194,"title":"How to improve NSDictionary's performance","body":"<p>I've got a lot of objects, every one has some tags, which are stored in NSDictionary. I've also got a loop, in which I'm iterating through all objects and do some stuff depending on object's tags. Method objectForKey: worsens performance a lot, if there are too many calls of it. How can I improve the performance? I've tried, for example, set \"strong\" instead of \"copy\" into @property, but that did not solve problem. Thanks!</p>\n\n<p>EDITED:</p>\n\n<p>Here is some kind of code, it is much more simple than the one in my project, of course. The Profiler says that the hot spots are in objectForKey:, not in doSomeStuff:. Can It be because of amount of its calls? Tags have got few objects, about 10 for each object, but the amount of objects is rather large</p>\n\n<p>.h:</p>\n\n<pre><code>@interface MyObject : NSObject\n\n@property (nonatomic, readwrite, strong/*copy*/  ) NSDictionary *tags;\n\n-(void)doSomeStuff;\n\n@end\n</code></pre>\n\n<p>.m:</p>\n\n<pre><code>@implementation MyObject\n{\n    NSDictionary *tags; // about 10 values for each object\n}\n\n\n-(NSDictionary *)tags\n{\n    return tags;\n}\n\n-(void)setTags:(NSDictionary *)newTags\n{\n    tags = [newTags copy];\n}\n\n-(void)doSomeStuff;\n\n@end\n</code></pre>\n\n<p>using:</p>\n\n<pre><code>NSArray *objects = ... // a lot of items, &gt; 1000\nNSArray *rules = ... // NSArray of objects with property NSArray* Strings, about 50 strings\n\nfor (MyObject *object in objects)\n{\n    for (NSArray *rule in rules)\n    {\n        for (NSString *ruleString in rule.Strings)\n        {\n            // there is a more complicated check in my project, so you may imagine that there are 10 calls of objectForKey:, with different keys\n            if ([object objectForKey:ruleString] isEqualToString:@\"yes\"])\n            {\n                [object doSomeStuff];\n            }\n        }\n    }\n}\n</code></pre>\n"},{"tags":["database","performance","oracle","oracle10g","union"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":72,"score":2,"question_id":12827525,"title":"Any Quicker Option Than Oracle Database Union","body":"<p>I have a table that has multiple columns which store a text value. For example:</p>\n\n<pre><code>ID    FATHER_NAME    MOTHER_NAME\n--------------------------------\n1     Henry          Sarah\n2     Martin         Rebecca\n3     Martin         Nancy\n</code></pre>\n\n<p>I want to get all of the names in the table. I know I can do a union to do this:</p>\n\n<pre><code>(SELECT FATHER_NAME FROM MY_TABLE)\nUNION\n(SELECT MOTHER_NAME FROM MY_TABLE)\n</code></pre>\n\n<p>However, in my real table there are 15 columns I need to union and the query is obviously taking awhile (approximately 12 seconds). And I still need to do joins on these names, etc. Is there any other alternative to doing unions?</p>\n\n<p>FYI: I am using Oracle. </p>\n"},{"tags":["performance","algorithm","data-structures","time-complexity","in-place"],"answer_count":1,"favorite_count":4,"up_vote_count":5,"down_vote_count":2,"view_count":259,"score":3,"question_id":12338654,"title":"Move all odd positioned element to left half and even positioned to right half in-place","body":"<p>Given an array with positive and negative integers, move all the odd indexed elements to the left and even indexed elements to the right.</p>\n\n<p>The difficult part of the problem is to do it in-place while maintaining the order.</p>\n\n<p>e.g.</p>\n\n<pre><code>7, 5, 6, 3, 8, 4, 2, 1\n</code></pre>\n\n<p>The output should be: </p>\n\n<pre><code>5, 3, 4, 1, 7, 6, 8, 2\n</code></pre>\n\n<p>If the order didn't matter, we could have been used partition() algorithm of quick sort.</p>\n\n<p>How to do it in O( N )?</p>\n"},{"tags":["html","performance","webbrowser","rendering"],"answer_count":6,"favorite_count":3,"up_vote_count":5,"down_vote_count":0,"view_count":3510,"score":5,"question_id":1323449,"title":"Tool to measure Render time","body":"<p>Is there a tool out there to measure the actual Render time of an element(s) on a page?  I don't mean download time of the resources, but the actual time the browser took to render something.  I know that this time would vary based on factors on the client machine, but would still be very handy in knowing what the rendering engine takes a while to load.  I would imagine this should be a useful utility since web apps are becoming pretty client heavy now.  Any thoughts?</p>\n"},{"tags":["python","performance","matplotlib","plot","cstring"],"answer_count":2,"favorite_count":3,"up_vote_count":6,"down_vote_count":0,"view_count":1771,"score":6,"question_id":5391026,"title":"Matplotlib, alternatives to savefig() to improve performance when saving into a CString object?","body":"<p>I am trying to speed up the process of saving my charts to images. Right now I am creating a cString Object where I save the chart to by using savefig; but I would really, really appreciate any help to improve this method of saving the image. I have to do this operation dozens of times, and the savefig command is very very slow; there must be a better way of doing it. I read something about saving it as uncompressed raw image, but I have no clue of how to do it. I don't really care about agg if I can switch to another faster backend too.</p>\n\n<p>ie:</p>\n\n<pre><code>RAM = cStringIO.StringIO()\n\nCHART = plt.figure(.... \n**code for creating my chart**\n\nCHART.savefig(RAM, format='png')\n</code></pre>\n\n<p>I have been using matplotlib with FigureCanvasAgg backend. </p>\n\n<p><strong>Thanks!</strong></p>\n"},{"tags":["c#","sql","performance","linq","query-optimization"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":84,"score":0,"question_id":12826725,"title":"Query optimization challenge - Is it possible to tweak this linq query","body":"<p>I am in to a perf excercise and I am wondering if there is any chance to improve this, just by the way I query for this info rather than attempting to do any DB schema or data changes.</p>\n\n<p>The below <strong>query takes around 200 ms to execute</strong>. I know I am ambitious but the challenge is is there a way to bring down. I am ready to employ ADO.NET or use a SP or use sql query but cannot do any data change. Also, <strong>the EmpTable has around 20 million rows</strong>.\nIt selects 1 record matching the paramid.</p>\n\n<p>Any one think this is still possible to optimize further ?</p>\n\n<pre><code>Log.Time()\n            using (MyDataContext db = MyDataContext.GetContext())\n            {\n                db.ObjectTrackingEnabled = false;\n                var _Query = from t in db.EmpTable\n                             where t.id == paramId\n                             select t;\n\n                 if (!_Query.Any())\n                    return null;\n            }\n            Log.Time()  -- 200 ms approx\n</code></pre>\n"},{"tags":["c++","performance","stdmap","stdset"],"answer_count":5,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":93,"score":0,"question_id":12826364,"title":"C++ std::map or std::set - efficiently insert duplicates","body":"<p>I have a bunch of data full of duplicates and I want to eliminate the duplicates. You know, e.g. [1, 1, 3, 5, 5, 5, 7] becomes [1, 3, 5, 7].</p>\n\n<p>It looks like I can use either std::map or std::set to handle this. However I'm not sure whether it's faster to (a) simply insert all the values into the container, or (b) check whether they already exist in the container and only insert if they don't - are inserts very efficient? Even if there's a better way... can you suggest a fast way to do this?</p>\n\n<p>Another question - if the data I'm storing in them isn't as trivial as integers, and instead is a custom class, how does the std::map manage to properly store (hash?) the data for fast access via operator[]?</p>\n"},{"tags":["python","performance","coding-style","matplotlib","chunked-encoding"],"answer_count":1,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":157,"score":3,"question_id":12769353,"title":"Python: suggestions to improve a chunk-by-chunk code to read several millions of points","body":"<p>I wrote a code to read <code>*.las</code> file in Python. <code>*las</code> file are special ascii file where each line is <code>x,y,z</code> value of points.</p>\n\n<p>My function read <code>N</code>. number of points and check if they are inside a polygon with <code>points_inside_poly</code>.</p>\n\n<p>I have the following questions:</p>\n\n<ol>\n<li>When I arrive at the end of the file I get this message: <code>LASException: LASError in \"LASReader_GetPointAt\": point subscript out of range</code> because the number of points is under the chunk dimension. I cannot figure how to resolve this problem.</li>\n<li><code>a = [file_out.write(c[m]) for m in xrange(len(c))]</code> I use <code>a =</code> in order to avoid video print. Is it correct?</li>\n<li>In <code>c = [chunk[l] for l in index]</code> I create a new list <code>c</code> because I am not sure that replacing a new chunk is the smart solution (ex: <code>chunk = [chunk[l] for l in index]</code>). </li>\n<li>In a statement <code>if else...else</code> I use <code>pass</code>. Is this the right choice?</li>\n</ol>\n\n<p>Really thank for help. It's important to improve listen suggestions from expertise!!!!</p>\n\n<pre><code>import shapefile\nimport numpy\nimport numpy as np\nfrom numpy import nonzero\nfrom liblas import file as lasfile\nfrom shapely.geometry import Polygon\nfrom matplotlib.nxutils import points_inside_poly  \n\n\n# open shapefile (polygon)\nsf = shapefile.Reader(poly)\nshapes = sf.shapes()\n# extract vertices\nverts = np.array(shapes[0].points,float)\n\n# open las file\nf = lasfile.File(inFile,None,'r') # open LAS\n# read \"header\"\nh = f.header\n\n# create a file where store the points\nfile_out = lasfile.File(outFile,mode='w',header= h)\n\n\nchunkSize = 100000\nfor i in xrange(0,len(f), chunkSize):\n    chunk = f[i:i+chunkSize]\n\n    x,y = [],[]\n\n    # extraxt x and y value for each points\n    for p in xrange(len(chunk)):\n        x.append(chunk[p].x)\n        y.append(chunk[p].y)\n\n    # zip all points \n    points = np.array(zip(x,y))\n    # create an index where are present the points inside the polygon\n    index = nonzero(points_inside_poly(points, verts))[0]\n\n    # if index is not empty do this otherwise \"pass\"\n    if len(index) != 0:\n        c = [chunk[l] for l in index] #Is It correct to create a new list or i can replace chunck?\n        # save points\n        a = [file_out.write(c[m]) for m in xrange(len(c))] #use a = in order to avoid video print. Is it correct?\n    else:\n        pass #Is It correct to use pass?\n\nf.close()\nfile_out.close()\n</code></pre>\n\n<p>code proposed by @Roland Smith and changed by Gianni</p>\n\n<pre><code>f = lasfile.File(inFile,None,'r') # open LAS\nh = f.header\n# change the software id to libLAS\nh.software_id = \"Gianni\"\nfile_out = lasfile.File(outFile,mode='w',header= h)\nf.close()\nsf = shapefile.Reader(poly) #open shpfile\nshapes = sf.shapes()\nfor i in xrange(len(shapes)):\n    verts = np.array(shapes[i].points,float)\n    inside_points = [p for p in lasfile.File(inFile,None,'r') if pnpoly(p.x, p.y, verts)]\n    for p in inside_points:\n        file_out.write(p)\nf.close()\nfile_out.close()\n</code></pre>\n\n<p>i used these solution:\n1) reading <strong>f = lasfile.File(inFile,None,'r')</strong> and after the read <strong>head</strong> because i need in the *.las output file\n2) close the file \n3) i used <strong>inside_points = [p for p in lasfile.File(inFile,None,'r') if pnpoly(p.x, p.y, verts)]</strong> instead of </p>\n\n<pre><code>with lasfile.File(inFile, None, 'r') as f:\n...     inside_points = [p for p in f if pnpoly(p.x, p.y, verts)]\n...     \n</code></pre>\n\n<p>because i always get this error message</p>\n\n<p><strong>Traceback (most recent call last):\nFile \"\", line 1, in \nAttributeError: _<em>exit</em>_</strong></p>\n"},{"tags":["asp.net","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":24,"score":0,"question_id":12825944,"title":"Which is faster (in browser rendering): Include() or Server.Execute() of an .aspx script?","body":"<p>I've got a small cookie-leaving aspx script.</p>\n\n<p>I'm wondering if it's faster to just do it as an \n</p>\n\n<p>or\nServer.Execute (same script)</p>\n\n<p>or does it not matter?</p>\n"},{"tags":["sql","sql-server","performance","tsql","triggers"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":71,"score":2,"question_id":12825727,"title":"Modify SQL trigger to work when inserted table contains more than one row","body":"<p>I've got a SQL trigger written for a table in SQL Server 2008. It works well when there is only one row in the 'inserted' table. How can I modify this trigger to work correctly when there are multiple rows? Performance is key here, so I'd like to stay away from cursors, temp tables, etc. (if possible).</p>\n\n<p>Essentially the trigger checks to see if either the 'ClientID' or 'TemplateID' fields were changed. If they were, and the OriginalClientID or OriginalTemplateID fields are null, it populates them (thus setting the OriginalXXX fields once and only once so I can always see what the first values were).</p>\n\n<pre><code>CREATE TRIGGER [dbo].[trigSetOriginalValues]\n   ON  [dbo].[Review]\n   FOR INSERT, UPDATE\nAS \nBEGIN\n    IF (NOT UPDATE(TemplateID) AND NOT UPDATE(ClientID)) return\n\n    DECLARE @TemplateID int\n    DECLARE @OriginalTemplateID int\n    DECLARE @ClientID int\n    DECLARE @OriginalClientID int   \n    DECLARE @ReviewID int\n\n    SET @ReviewID = (SELECT ReviewID FROM inserted)\n    SET @ClientID = (SELECT ClientID FROM inserted)\n    SET @TemplateID = (SELECT TemplateID FROM inserted)\n    SET @OriginalTemplateID = (SELECT OriginalTemplateID FROM inserted);\n    SET @OriginalClientID = (SELECT OriginalClientID FROM inserted);\n\n    IF (@OriginalTemplateID IS NULL AND @TemplateID IS NOT NULL) \n    BEGIN\n        UPDATE [dbo].[Review] SET OriginalTemplateID = @TemplateID WHERE ReviewID=@ReviewID\n    END\n\n    IF (@OriginalClientID IS NULL AND @ClientID IS NOT NULL) \n    BEGIN\n        UPDATE [dbo].[Review] SET OriginalClientID = @ClientID WHERE ReviewID=@ReviewID\n    END \nEND\n</code></pre>\n"},{"tags":["javascript","performance","optimization","requirejs","js-amd"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":69,"score":0,"question_id":12825103,"title":"Load js scripts/libraries the right way","body":"<p>I'm in charge of building my band's website and tumblr. I'm in the paper draft stage. Currently thinking how to handle scripts to optimize performance.</p>\n\n<p>Basically, I'd need:</p>\n\n<ol>\n<li>jQuery: to manipulate DOM once it's ready as well as some interactivity.</li>\n<li>jPlayer with jPlayer's playlist plugin for an HTML5 music player with Flash fallback.</li>\n<li>A custom script to load data from the Songkick API (tour dates). It'd add tour show to the page. Should happen after the DOM is ready (Ajax request).</li>\n<li>A custom script to load songs from SoundCloud using their API. That should occur when a user is clicking in the play button from the music player (those are long tracks, loading them during page load is a bad practice).</li>\n</ol>\n\n<p>SO, I'm thinking about how to structure all those scripts so that things occurs at the good time in the good order. I've been off the dev scene for some years but read a bit before posting. Saw that now the design pattern of JS tends to use a modular approaches. I've read a bit on the RequireJS website and might give it a try.</p>\n\n<p>Well, my main question is, how should I structure my scripts so that it loads at the correct time with the best performance possible? Is RequireJS an option to solve such design patterns issues?</p>\n\n<p>Sorry, it's not a syntax problem but more a thinking pre-coding problem. I'm just trying to think right before getting my hands dirty.</p>\n\n<p>Regards,\nO.</p>\n"},{"tags":["python","performance","graphics"],"answer_count":5,"favorite_count":2,"up_vote_count":5,"down_vote_count":0,"view_count":426,"score":5,"question_id":7117143,"title":"Setting Pixels Quickly","body":"<p>I have searched and searched for a good answer and I am about to cry from frustration. I am a hobbyist programmer, I don't do things because they make sense, or they are the right way to do them; I do them to learn how, and right now I am stumped.</p>\n\n<p>I want to set individual pixels on the screen. This may sound easy, but it's my other conditions that makes it hard. I need to do this quickly, CPU only, 20 fps or better (with other program elements running of course), on a 400 by 300 screen or better (full screen?).</p>\n\n<p>I have been rendering some cool images using programs I wrote in Python that uses Pygame, but it takes 50 milliseconds to fill a 100px by 100px screen with just random pixels (that's my 20 fps right there, and other program bits slow it down more). Ideally I would LOVE to make my own (crappy) 3D game that just uses the CPU only, setting pixels on the screen (perhaps a voxel octree sort of graphics).</p>\n\n<p>Is there any way (with any language, but preferably Python) I could like, make a 2D array of pixel values (more like 3D array with RGB) (is this called a bitmap?) in the RAM and dump it on to the display or something? Wouldn't that be fast??? How DO you interface directly with the pixels on a window. Argh! I am so clueless. I am / am not a programming noob. Give me whatever you can throw at me, I can digest it. I just need some pointers (haha) in the right direction.</p>\n"},{"tags":["c#","performance","concurrency","multithreading","threadpool"],"answer_count":4,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":1217,"score":4,"question_id":1958524,"title":"Limiting concurrent threads equal to number of processors?","body":"<p>Are there any benefits to limiting the number of concurrent threads doing a given task to equal the number of processors on the host system?  Or better to simply trust libraries such as .NET's ThreadPool to do the right thing ... even if there are 25 different concurrent threads happening at any one given moment?</p>\n"},{"tags":["performance","graph","graph-databases"],"answer_count":3,"favorite_count":7,"up_vote_count":7,"down_vote_count":0,"view_count":923,"score":7,"question_id":2153963,"title":"Storing very large graphs on disk/streaming graph partitioning algorithms?","body":"<p>Suppose that I have a very large undirected, unweighted graph (starting at hundreds of millions of vertices, ~10 edges per vertex), non-distributed and processed by single thread only and that I want to do breadth-first searches on it. I expect them to be I/O-bound, thus I need a good-for-BFS disk page layout, disk space is not an issue. The searches can start on every vertex with equal probability. Intuitively that means minimizing the number of edges between vertices on different disk pages, which is a graph partitioning problem.</p>\n\n<p>The graph itself looks like a spaghetti, think of random set of points randomly interconnected, with some bias towards shorter edges.</p>\n\n<p>The problem is, how does one partition graph this large? The available graph partitioners I have found work with graphs that fit into memory only.  I could not find any descriptions nor implementations of any streaming graph partitioning algorithms.</p>\n\n<p>OR, maybe there is an alternative to partitioning graph for getting a disk layout that works well with BFS?</p>\n\n<p>Right now as an approximation I use the fact that the vertices have spatial coordinates attached to them and put the vertices on disk in Hilbert sort order. This way spatially close vertices land on the same page, but the presence or absence of edge between them is completely ignored. Can I do better?</p>\n\n<p>As an alternative, I can split graph into pieces using the Hilbert sort order for vertices, partition the subgraphs, stitch them back and accept poor partitioning on the seams.</p>\n\n<p>Some things I have looked into already:</p>\n\n<ol>\n<li><a href=\"http://stackoverflow.com/questions/1526479/how-to-store-a-large-directed-unweighted-graph-with-billions-of-nodes-and-vertice\">http://stackoverflow.com/questions/1526479/how-to-store-a-large-directed-unweighted-graph-with-billions-of-nodes-and-vertice</a></li>\n<li><a href=\"http://neo4j.org/\">http://neo4j.org/</a> - I found zero information on how does it do graph layout on disk</li>\n</ol>\n\n<p>Partitioning implementations (unless I'm mistaken, all of them need to fit graph into memory):</p>\n\n<ol>\n<li><a href=\"http://glaros.dtc.umn.edu/gkhome/views/metis\">http://glaros.dtc.umn.edu/gkhome/views/metis</a></li>\n<li><a href=\"http://www.sandia.gov/~bahendr/chaco.html\">http://www.sandia.gov/~bahendr/chaco.html</a></li>\n<li><a href=\"http://staffweb.cms.gre.ac.uk/~c.walshaw/jostle/\">http://staffweb.cms.gre.ac.uk/~c.walshaw/jostle/</a></li>\n<li><a href=\"http://www.cerfacs.fr/algor/Softs/MESHPART/\">http://www.cerfacs.fr/algor/Softs/MESHPART/</a></li>\n</ol>\n\n<p>EDIT: info on how the graphs looks like and that BFS can start everywhere.\nEDIT: idea on partitioning subgraphs</p>\n"},{"tags":["sql","performance"],"answer_count":5,"favorite_count":1,"up_vote_count":11,"down_vote_count":0,"view_count":10139,"score":11,"question_id":1503959,"title":"How to count occurrences of a column value efficiently in SQL?","body":"<p>I have a table of students:</p>\n\n<pre><code>id | age\n--------\n0  | 25\n1  | 25\n2  | 23\n</code></pre>\n\n<p>I want to query for all students, and an additional column that counts how many students are of the same age:</p>\n\n<pre><code>id | age | count\n----------------\n0  | 25  | 2\n1  | 25  | 2\n2  | 23  | 1\n</code></pre>\n\n<p>What's the most efficient way of doing this? <strong>I fear that a sub-query will be slow, and I'm wondering if there's a better way</strong>. Is there?</p>\n"},{"tags":["mysql","performance","algorithm","query","index"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":42,"score":1,"question_id":12821936,"title":"Mysql implementation and efficiency querying on multiple fields","body":"<p>I want to search a table by two fields in MySQL:</p>\n\n<pre><code>select * from table where  \n90 &lt; x and x &lt; 100 and \n50 &lt; y and y &lt; 60\n</code></pre>\n\n<p>What is the efficiency of this search if optimised? O(log(n))? </p>\n\n<p>And what type of index and algorithm would it be implementing? (Am I correct in saying it would be O(n.log(n)) if using standard B-tree or hash map?)</p>\n\n<p>Thanks</p>\n"},{"tags":["performance","sql-server-2008","query","table"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":52,"score":0,"question_id":12822518,"title":"How to improve Query performance on large table in MS SQL Server 2008 R2","body":"<p>I have a table with 20 million records. What is the best option to improve query performance on this table. Is partitioning the table into filegroups  is a best option or splitting the table into multiple smaller tables? </p>\n"},{"tags":["iphone","objective-c","performance","uitableview","scrolling"],"answer_count":2,"favorite_count":5,"up_vote_count":2,"down_vote_count":0,"view_count":761,"score":2,"question_id":5459708,"title":"Measuring the UITableView Scrolling Performance - iphone","body":"<p>I am trying to measure the scrolling performance for my UITableView, between using subview and drawing the view myself. As we may know about scrolling performance, there are a couple of famous articles (<a href=\"http://blog.atebits.com/2008/12/fast-scrolling-in-tweetie-with-uitableview/\" rel=\"nofollow\">Tweetie</a>, <a href=\"http://developer.apple.com/library/ios/#samplecode/TableViewSuite/Introduction/Intro.html\" rel=\"nofollow\">TableViewSuite</a>, <a href=\"http://www.fieryrobot.com/blog/2008/10/01/glassy-scrolling-with-uitableview/\" rel=\"nofollow\">Glassy</a> and <a href=\"http://www.fieryrobot.com/blog/2008/10/08/more-glassy-scrolling-with-uitableview/\" rel=\"nofollow\">Glassy2</a> that help us with the technique and all will point to the same point: when we have lots of subviews, we should go with drawRect. </p>\n\n<p>The problem is that I do not know how to benchmark the performance in either case: using subview or drawing. And drawing is actually harder to do than subview, so it is hard to convince everybody to go with drawing directly. I am trying to write 2 small samples and using 2 techniques and benchmark the performance result. I am currently trying with this, but it generates the same results for both techniques:</p>\n\n<pre><code>NSDate *date = [NSDate date];\n    static NSString *CellIdentifier = @\"CellIdentifier\";\n\n    CustomDrawingTableViewCell *cell = (CustomDrawingTableViewCell *) [self.tableView dequeueReusableCellWithIdentifier:CellIdentifier];\n    if (cell == nil) {\n         cell = [[[CustomDrawingTableViewCell alloc] initWithStyle:UITableViewCellStyleDefault \n                                                                             reuseIdentifier:CellIdentifier] autorelease];\n    }\n\n    // Configure the cell...\n    // Main Code is HERE\n\n\n    NSDate *date2 = [NSDate date];\n    NSLog(@\"%f\", [date2 timeIntervalSinceDate:date]);\n    return cell;\n</code></pre>\n\n<p>My Cell has around 4 images, 1 text</p>\n"},{"tags":["python","performance","numpy","vectorization"],"answer_count":2,"favorite_count":0,"up_vote_count":6,"down_vote_count":0,"view_count":106,"score":6,"question_id":12816293,"title":"Vectorize this convolution type loop more efficiently in numpy","body":"<p>I have to do many loops of the following type</p>\n\n<pre><code>for i in range(len(a)):\n    for j in range(i+1):\n        c[i] += a[j]*b[i-j]\n</code></pre>\n\n<p>where a and b are short arrays (of the same size, which is between about 10 and 50). This can be done efficiently using a convolution:</p>\n\n<pre><code>import numpy as np\nnp.convolve(a, b) \n</code></pre>\n\n<p>However, this gives me the full convolution (i.e. the vector is too long, compared to the for loop above). If I use the 'same' option in convolve, I get the central part, but what I want is the first part. Of course, I can chop off what I don't need from the full vector, but I would like to get rid of the unnecessary computation time if possible.\nCan someone suggest a better vectorization of the loops?</p>\n"},{"tags":["c++","performance","image-processing","opencv","computer-vision"],"answer_count":3,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":130,"score":2,"question_id":12804708,"title":"C++: OpenCV performance issues scanning images","body":"<p>I have about 20 images which have been colour coded. I want to scan each image and match the pixel to the label associated with that colour. I have written the code below, however it takes about 30 mins to run to do this seemingly simple task. The images have resolution of 960 x 720.</p>\n\n<p>My Code:</p>\n\n<pre><code>void go_through_pixels(path &amp;image_dir, string&amp; ground_truth_suffix, string image_format, unordered_map&lt;RGB, string&gt; colors_for_labels){\n\n    if(!exists(image_dir)){\n        cerr &lt;&lt; image_dir &lt;&lt; \" does not exist, prematurely returning\" &lt;&lt; endl;\n        exit(-1);\n    }\n\n    unordered_map&lt;string, set&lt;path&gt; &gt; label_to_files_map;\n\n    //initialise label_to_files_map\n    for(unordered_map&lt;RGB, string&gt;::iterator it = colors_for_labels.begin(); it != colors_for_labels.end(); it++){\n        label_to_files_map[it-&gt;second] = set&lt;path&gt;();\n    }\n\n    directory_iterator end_itr; //default construction provides an end reference\n\n    for(directory_iterator itr(image_dir); itr != end_itr; itr++){\n\n        path file = itr-&gt;path();\n        string filename = file.filename().string();\n        RGB rgb(0,0,0); //default rgb struct, values will be changed in the loop\n\n        if(extension(file) == image_format &amp;&amp; filename.find(ground_truth_suffix) != string::npos){\n            //ground truth file\n            Mat img = imread(file.string(), CV_LOAD_IMAGE_COLOR);\n\n            for(int y = 0; y &lt; img.rows; y++){\n                for(int x = 0; x &lt; img.cols; x++){\n                    //gives data as bgr instead of rgb\n                    Point3_&lt;uchar&gt;* pixel = img.ptr&lt;Point3_&lt;uchar&gt; &gt;(y,x);\n                    rgb.red = (int)pixel-&gt;z;\n                    rgb.green = (int)pixel-&gt;y;\n                    rgb.blue =(int)pixel-&gt;x;\n                    string label = colors_for_labels[rgb];\n                    label_to_files_map[label].insert(file);\n                    cout &lt;&lt; label &lt;&lt; endl;\n                }\n            }\n        }\n    }\n}\n</code></pre>\n\n<p>I will be doing more with this data afterwards, but have simplified my code down to this just to try and find the performance issue. </p>\n\n<p>I have found that the <code>label_to_files_map[label].insert(file)</code> is causing most of the delay, as when removed it takes about 3 mins to just scan the images. I still think this is too long, but may be wrong?</p>\n\n<p>Also, as the set <code>insert</code> is taking a long time (as it has to check for duplicate before insertion) can anyone suggest a better data structure to use here?</p>\n\n<p>Essentially a picture can have lets say 100 pixels corresponding to a building, 100 corresponding to a car and so on so I just want to record in the map <code>label_to_files_map</code> that this file (the current image being scanned) has a building in it (which in this case is denoted by a particular rgb value).</p>\n"},{"tags":["linux","performance","usb","kernel","driver"],"answer_count":1,"favorite_count":4,"up_vote_count":4,"down_vote_count":0,"view_count":381,"score":4,"question_id":7962182,"title":"very poor performance (~0.4MB/s) with Linux usb bulk transfer kernel driver and hardware loopback","body":"<p>I am writing a Linux kernel driver for a custom USB device which will use bulk endpoints, everything seems to work fine, however, I am getting very slow data rates. Specifically, it takes ~25 seconds to write and read 10MB worth of data. I tried this on an embedded system and a Linux VM running on a reasonable PC with similar results. </p>\n\n<p>I am using a EZ-USB FX2 development kit from Cypress as the target board. It is running the bulkloop firmware which sets up two in and two out endpoints. Each endpoint is double buffered and supports 512 byte windows. The firmware polls out endpoints via a while(1) loop in main(), no sleep, and copies data from out to in endpoints when those data are available using autopointers. I have been told that this can move data fairly on Windows using their specific application but have not had a chance to verify this. </p>\n\n<p>My code (relevant portions below) calls a function called bulk_io in the device probe routine. This function creates a number (URB_SETS) of out urbs which attempt to write 512 bytes to the device. Changing this number between 1 and 32 doesn't change performance. They are all copying from the same buffer. The callback handler for each write operation to an out endpoint is used to create a read urb on the corresponding in endpoint. The read callback creates another write urb until I have hit the total number of write/read requests that I want to run at a time (20,000). I am working now to push most of the operations in the callback functions into bottom halves in case they are blocking other interrupts. I am also thinking of rewriting the bulk-loop firmware for the Cypress FX2 to use interrupts instead of polling. Is there anything here that looks out of the ordinary to make the performance so low? Thank you in advance. Please let me know if you would like to see more code, this is just a bare-bone driver to test I/O to the Cypress FX2.</p>\n\n<p>This is the out endpoint write callback function:</p>\n\n<pre><code>static void bulk_io_out_callback0(struct urb *t_urb) {\n    // will need to make this work with bottom half\n    struct usb_dev_stat *uds = t_urb-&gt;context;\n    struct urb *urb0 = usb_alloc_urb(0,GFP_KERNEL);\n    if (urb0 == NULL) {\n            printk(\"bulk_io_out_callback0: out of memory!\");\n    }\n    usb_fill_bulk_urb(urb0, interface_to_usbdev(uds-&gt;intf), usb_rcvbulkpipe(uds-&gt;udev,uds-&gt;ep_in[0]), uds-&gt;buf_in, uds-&gt;max_packet, bulk_io_in_callback0, uds);\n    usb_submit_urb(urb0,GFP_KERNEL);\n    usb_free_urb(urb0);\n}\n</code></pre>\n\n<p>This is the in endpoint read callback function:</p>\n\n<pre><code>static void bulk_io_in_callback0(struct urb *t_urb) {\n    struct usb_dev_stat *uds = t_urb-&gt;context;\n\n    struct urb *urb0 = usb_alloc_urb(0,GFP_KERNEL);\n    if (urb0 == NULL) {\n            printk(\"bulk_io_out_callback0: out of memory!\");\n    }\n\n    if (uds-&gt;seq--) {\n            usb_fill_bulk_urb(urb0, interface_to_usbdev(uds-&gt;intf), usb_sndbulkpipe(uds-&gt;udev,uds-&gt;ep_out[0]), uds-&gt;buf_out, uds-&gt;max_packet, bulk_io_out_callback0, uds);\n            usb_submit_urb(urb0,GFP_KERNEL);\n    }\n    else {\n            uds-&gt;t1 = get_seconds();\n            uds-&gt;buf_in[9] = 0; // to ensure we only print the first 8 chars below\n            printk(\"bulk_io_in_callback0: completed, time=%lds, bytes=%d, data=%s\\n\", (uds-&gt;t1-uds-&gt;t0), uds-&gt;max_packet*SEQ, uds-&gt;buf_in);\n    }\n    usb_free_urb(urb0);\n}\n</code></pre>\n\n<p>This function gets called to set up the initial urbs:</p>\n\n<pre><code>static int bulk_io (struct usb_interface *interface, struct usb_dev_stat *uds) {\n    struct urb *urb0;\n    int i;\n\n    uds-&gt;t0 = get_seconds();\n\n    memcpy(uds-&gt;buf_out,\"abcd1234\",8);\n\n    uds-&gt;seq = SEQ; // how many times we will run this\n\n    printk(\"bulk_io: starting up the stream, seq=%ld\\n\", uds-&gt;seq);\n\n    for (i = 0; i &lt; URB_SETS; i++) {\n            urb0 = usb_alloc_urb(0,GFP_KERNEL);\n            if (urb0 == NULL) {\n                    printk(\"bulk_io: out of memory!\\n\");\n                    return(-1);\n            }\n\n            usb_fill_bulk_urb(urb0, interface_to_usbdev(uds-&gt;intf), usb_sndbulkpipe(uds-&gt;udev,uds-&gt;ep_out[0]), uds-&gt;buf_out, uds-&gt;max_packet, bulk_io_out_callback0, uds);\n                            printk(\"bulk_io: submitted urb, status=%d\\n\", usb_submit_urb(urb0,GFP_KERNEL));\n            usb_free_urb(urb0); // we don't need this anymore\n    }\n\n\n    return(0);\n}\n</code></pre>\n\n<p><strong>Edit 1</strong> I verified that udev->speed == 3, so USB_SPEED_HIGH, meaning this is not because Linux thinks this is a slow device....</p>\n\n<p><strong>Edit 2</strong> I moved everything in the callbacks related to urb creation (kmalloc, submit) and freeing into bottom halves, same performance. </p>\n"},{"tags":["c#",".net","performance","entity-framework"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":52,"score":0,"question_id":12821377,"title":"Can EF5 works fine in event based processing","body":"<p>I have this code for example for an event handler </p>\n\n<pre><code>public void ONDataArrived ( string data ) \n{\n//do some processing and save it to DB using EF \nctx.Add ( x ) ; \nctx.SaveChanges ()  ; \n\n}\n</code></pre>\n\n<p>Is there any chance that EF may error if this event fired a couple of times in the same time ? </p>\n\n<p>thanks </p>\n"},{"tags":["python","performance","testing","load"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":3,"view_count":49,"score":-3,"question_id":12819068,"title":"python framework for load testing","body":"<p>I'm want find framework for load testing in python, but not for web. From framework I need possibility for create my test scenario. I will test rabbitmq and redis.</p>\n"},{"tags":["performance","entity-framework-4","contains"],"answer_count":6,"favorite_count":10,"up_vote_count":20,"down_vote_count":1,"view_count":3987,"score":19,"question_id":7897630,"title":"Why does the Contains() operator degrade Entity Framework's performance so dramatically?","body":"<p>UPDATE: I've created a suggestion to fix this problem. To vote for it, <a href=\"http://data.uservoice.com/forums/72025-ado-net-entity-framework-ef-feature-suggestions/suggestions/2598644-improve-the-performance-of-the-contains-operator\" rel=\"nofollow\">go here</a>.</p>\n\n<p>Consider a SQL database with one very simple table.</p>\n\n<pre><code>CREATE TABLE Main (Id INT PRIMARY KEY)\n</code></pre>\n\n<p>I populate the table with 10,000 records.</p>\n\n<pre><code>WITH Numbers AS\n(\n  SELECT 1 AS Id\n  UNION ALL\n  SELECT Id + 1 AS Id FROM Numbers WHERE Id &lt; 10000\n)\nINSERT Main (Id)\nSELECT Id FROM Numbers\nOPTION (MAXRECURSION 0)\n</code></pre>\n\n<p>I build an EF model for the table and run the following query in LINQPad (I am using \"C# Statements\" mode so LINQPad doesn't create a dump automatically).</p>\n\n<pre><code>var rows = \n  Main\n  .ToArray();\n</code></pre>\n\n<p>Execution time is ~0.07 seconds. Now I add the Contains operator and re-run the query.</p>\n\n<pre><code>var ids = Main.Select(a =&gt; a.Id).ToArray();\nvar rows = \n  Main\n  .Where (a =&gt; ids.Contains(a.Id))\n  .ToArray();\n</code></pre>\n\n<p>Execution time for this case is <strong>20.14 seconds</strong> (288 times slower)!</p>\n\n<p>At first I suspected that the T-SQL emitted for the query was taking longer to execute, so I tried cutting and pasting it from LINQPad's SQL pane into SQL Server Management Studio.</p>\n\n<pre><code>SET NOCOUNT ON\nSET STATISTICS TIME ON\nSELECT \n[Extent1].[Id] AS [Id]\nFROM [dbo].[Primary] AS [Extent1]\nWHERE [Extent1].[Id] IN (1,2,3,4,5,6,7,8,...\n</code></pre>\n\n<p>And the result was</p>\n\n<pre><code>SQL Server Execution Times:\n  CPU time = 0 ms,  elapsed time = 88 ms.\n</code></pre>\n\n<p>Next I suspected LINQPad was causing the problem, but performance is the same whether I run it in LINQPad or in a console application.</p>\n\n<p>So, it appears that the problem is somewhere within Entity Framework.</p>\n\n<p>Am I doing something wrong here? This is a time-critical part of my code, so is there something I can do to speed up performance?</p>\n\n<p>I am using Entity Framework 4.1 and Sql Server 2008 R2.</p>\n\n<p><em>Update</em></p>\n\n<p>In the discussion below there were some questions about whether the delay occurred while EF was building the initial query or while it was parsing the data it received back. To test this I ran the following code,</p>\n\n<pre><code>var ids = Main.Select(a =&gt; a.Id).ToArray();\nvar rows = \n  (ObjectQuery&lt;MainRow&gt;)\n  Main\n  .Where (a =&gt; ids.Contains(a.Id));\nvar sql = rows.ToTraceString();\n</code></pre>\n\n<p>which forces EF to generate the query without executing it against the database. The result was that this code required ~20 secords to run, so it appears that almost all of the time is taken in building the initial query.</p>\n\n<p>CompiledQuery to the rescue then? Not so fast ... CompiledQuery requires the parameters passed into the query to be fundamental types (int, string, float, and so on). It won't accept arrays or IEnumerable, so I can't use it for a list of Ids.</p>\n"},{"tags":["javascript","performance","browser","garbage-collection"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":48,"score":1,"question_id":11641778,"title":"How to improve the browser garbage collection phase of web app?","body":"<p>I have optimized my php/javascript web app to be fairly fast, but an analysis with the google speed tracer shows a 230ms garbage collection phase. What is it, as pertains to the speed tracer results, and what are some guidelines in improving it?</p>\n"},{"tags":["php","performance","node.js","beanstalkd"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":52,"score":0,"question_id":12810622,"title":"NodeJS setInterval bad for performance?","body":"<p>I have a beanstalkapp worker that is made with nodejs what it does is there is a PHP application which does all the site stuff and if there are errors or issues or notifications or what ever it adds it to the beanstalkapp. The nodejs then needs to run pretty much constantly checking the beanstalkapp for any messages and do something with them (email someone, add it to the log, post somewhere).</p>\n\n<p>My question is, is this bad performance wise or is there a better way to do this? I would assume that setInterval doesn't let the process end and would therefore be bad?</p>\n"},{"tags":["java","performance","distributed"],"answer_count":6,"favorite_count":2,"up_vote_count":9,"down_vote_count":0,"view_count":584,"score":9,"question_id":2028568,"title":"Java framework for distributed system","body":"<p>I am looking for a library (or a combination of libraries) to build a java distributed system, made of several applications exchanging data through several pairwise connections (no mapreduce). For the moment I did an expolration of existing libraries and I could only discard what I'v found. Here are my requirements:</p>\n\n<ul>\n<li>Easy discovery of systems at runtime (possibly through a central server/directory)</li>\n<li>Lightweight and low latency messages (no CORBA, RMI, SOAP,. etc.)</li>\n<li>Decentralized communications (no LINDA like)</li>\n<li>Easy enough to use and learn (no JXTA)</li>\n<li>Compatible with GPL license (so GPL, BSD, etc.)</li>\n</ul>\n\n<p>Do you have any suggestion ? Thanks in advance</p>\n"},{"tags":["android","database","performance","memory"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":12817577,"title":"passing custom objects or just ids","body":"<p>I've a sqllite database where I store some data with a picture. </p>\n\n<p>In my MainActivity, I load this data to a List&lt; CustomObject > and using a ArrayAdapter with ListView to display this informations. On Item click, I'm passing the item id to the DetailActivity and there I'm requesting the item data again from the sqllite database to display it.</p>\n\n<p>I'm worried, because I don't know whats the best way on android. </p>\n\n<p>Should I use less informations in the MainActivity (only id, title and the picture) and pass the ID to the DetailActivity or should I pass the complete CustomObject through my DetailActivity (so I don't have to access the database again)</p>\n\n<p>I want the right combination of memory-usage and performance.</p>\n"},{"tags":["performance"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":367,"score":0,"question_id":4642653,"title":"How do you achieve acceptable performance metrics for your web application?","body":"<p>What analysis do you currently perform to achieve performance metrics that are acceptable? Metrics such as page weight, response time, etc. What are the acceptable metrics that are  currently recommended?</p>\n"},{"tags":["performance"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":197,"score":1,"question_id":1348742,"title":"Measuring page processing time (serverside) of a web application","body":"<p>When a user performs any action on a page, the request is sent to the server. The server processes the request and sends the files necessary to render the page to the client.</p>\n\n<p>I want to track the time taken by the server for every page load, and log it to database for further analysis.</p>\n\n<p>I calculated the time that each action takes on the serverside, but it doesn't truly correlate with the page. There can be multiple redirects before a page is rendered, and hence multiple actions, and it might change based on workflow and exceptions .</p>\n\n<p>Basically, I want the total serverside execution time for the page just like YSlow  (without the queueing, latency and client side processing). </p>\n\n<p>How do you track your server side speed? Anyone know of a better way to accomplish the same thing?</p>\n"},{"tags":["performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":129,"score":0,"question_id":5606032,"title":"Load-testing a web application","body":"<p>How does one go about testing the server-side performance of a web application?</p>\n\n<p>I'm working on a small web application (specifically, it will solely be responding to AJAX requests with database rows). I want to see how it performs under load. However, I cannot upload it to an internet host right away. The development environment is, however, part of the local intranet, so I can use as many machines as I want to hammer the development server, presumably using Python in conjunction with urllib2.</p>\n\n<p>My question is, is such an approach really accurate for determining the high-load performance of a server-side script? Is there a better way to do this? Am I missing something here?</p>\n"},{"tags":["performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":120,"score":1,"question_id":5999564,"title":"Web Performance","body":"<p>Im working on a large site, trying to decrease the load times, and I have bumped into a rather strange issue. Im using google chromes built in developer tools, and I am finding that certain images are getting hung up, and the browser is continuing to look for them. Has anyone encountered this issue before? How do I isolate what is causing this problem? </p>\n\n<p>The site runs a couple of ads, is it possible this error is occurring because of ad networks?</p>\n\n<p>Here is a link to the actual problem: <a href=\"http://i.stack.imgur.com/IEtLA.png\" rel=\"nofollow\">http://i.stack.imgur.com/IEtLA.png</a> (updated)</p>\n"},{"tags":["javascript","performance","node.js","loops","for-loop"],"answer_count":5,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":90,"score":2,"question_id":12796887,"title":"FOR loop performance in Javascript","body":"<p>As my research leads me to believe that for loops are the fastest iteration construct in javascript language. I was thinking that also declaring a conditional length value for the for loop would be faster... to make it clearer, which of the following do you think would be faster?</p>\n\n<p><strong>Example ONE</strong></p>\n\n<pre><code>for(var i = 0; i &lt; myLargeArray.length; i++ ) {\n    console.log(myLargeArray[i]);\n} \n</code></pre>\n\n<p><strong>Example TWO</strong></p>\n\n<pre><code>var count = myLargeArray.length;\nfor(var i = 0; i &lt; count; i++ ) {\n    console.log(myLargeArray[i]);\n} \n</code></pre>\n\n<p>my logic follows that on each iteration in example one accessing the length of myLargeArray on each iteration is more computationally expensive then accessing a simple integer value as in example two? </p>\n"},{"tags":["performance","production-code"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":20,"score":0,"question_id":12819293,"title":"checkRep() methods should be turned off in production code to ensure maximum performance?","body":"<p>I'm wondering if the following statement is true?</p>\n\n<p><code>checkRep()</code> methods should be turned off in production code to ensure \nmaximum performance.</p>\n\n<p>Thanks!</p>\n"},{"tags":["performance","web-applications","bandwidth","latency"],"answer_count":1,"favorite_count":3,"up_vote_count":5,"down_vote_count":0,"view_count":1771,"score":5,"question_id":465314,"title":"Simple bandwidth / latency test to estimate a users experience","body":"<p>I write web based applications. Performance is obviously a key factor. Whilst database load and page rendering time are things I have control of, the users internet connection is not.</p>\n\n<p>What I'm looking for is a way to indicate what sort of a connection a user has. Something along the lines of a traffic light in the corner of a website that shows the user what sort of a connection they have to the site and therefore indicating what sort of perceived performance they should expect. e.g. Maybe the app just seems slow because everyone else in your company is browsing facebook on their lunch hour.</p>\n\n<p>My initial thoughts are that this could be achieved by some javascript that runs on each page load.</p>\n\n<p>Ideally the code is very \"dropable\" and does not require major code or infrastructure changes to implement.</p>\n"},{"tags":["c++","performance","architecture","latency"],"answer_count":3,"favorite_count":1,"up_vote_count":1,"down_vote_count":1,"view_count":90,"score":0,"question_id":12818460,"title":"Must-read C++ documents for \"under the hood\" performance understanding?","body":"<p>In order to understand exactly what happens \"under the hood\" when writing C++ what documents would you recommend?</p>\n\n<p>So far I have:</p>\n\n<ul>\n<li>GNU C++ compiler documentation (I thought this would be good to see what actually happens)</li>\n<li>C++ ABI(?) specification</li>\n</ul>\n\n<p>Books:</p>\n\n<ul>\n<li>Inside the C++ Object Model </li>\n<li>Meyers' Effective Series</li>\n</ul>\n"},{"tags":["c#","performance","linq","linq-to-sql"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":81,"score":2,"question_id":12802221,"title":"Improving performance of condtitional query inside LINQ enumerator","body":"<p>I've got some code that looks like this, using .NET 3.5 (customer requirement):</p>\n\n<pre><code>void Process (ActionState state)\n{\n    var orderItemQuery = from OrderItem item in order.OrderItems\n                           orderby item.OrderLineNumber ascending\n                           select item;\n\n    foreach (OrderItem item in orderItemQuery)\n    {\n        ActionData actionData;\n        switch (state)\n        {\n            case ActionState.Prepare:\n                actionData = (\n                    from ActionData ad in db.ActionDataTable\n                      where ad.ObjectId == item.ProductId\n                      select ad\n                ).First();\n\n            case ActionState.QualityCheck:\n                actionData = (\n                    from ActionData ad in db.ActionDataTable\n                      where ad.ObjectId == item.OrderItemId\n                      select ad\n                ).First();\n\n            default:\n                throw new InvalidOperationException();\n        }\n\n        // ...\n    }\n}\n</code></pre>\n\n<p>Essentially the results of the first query are iterated through, and a particular foreign key is used to fetch an <code>ActionData</code> record from the database, based upon the current <code>ActionState</code>. In reality there's a bit more nesting and a few checks here and there, but it's essentially the same.</p>\n\n<p>This initially worked great for test databases, but a customer just sent me a copy of their live data and it's hideously slow. It takes around 15 minutes to run the entire batch, including some other processing. After running the code through a performance tester, I discovered that the slowest part of the entire process was the <code>.First()</code> call on each of the cases.</p>\n\n<p>If this were plain SQL, I'd compile a stored procedure outside of the loop based on the value of <code>state</code>, and use that. Since I can't do that, what's the alternative? How can I speed this up?</p>\n"},{"tags":["javascript","xml","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":734,"score":1,"question_id":5138086,"title":"Fastest Way to Parse this XML in JS","body":"<p>Say I have this XML with about 1000+ bookinfo nodes. </p>\n\n<pre><code>&lt;results&gt;\n  &lt;books&gt;\n   &lt;bookinfo&gt;\n        &lt;name&gt;1&lt;/dbname&gt;\n   &lt;/bookinfo&gt;\n   &lt;bookinfo&gt;\n     &lt;name&gt;2&lt;/dbname&gt;\n   &lt;/bookinfo&gt;\n   &lt;bookinfo&gt;\n     &lt;name&gt;3&lt;/dbname&gt;\n   &lt;/bookinfo&gt;\n &lt;/books&gt;\n&lt;/results&gt;\n</code></pre>\n\n<p>I'm currently using this to get the name of each book:</p>\n\n<pre><code>var books = this.req.responseXML.getElementsByTagName(\"books\")[0].getElementsByTagName(\"bookinfo\")\n</code></pre>\n\n<p>Then use a for loop to do something with each book name:</p>\n\n<pre><code>var bookName = books[i].getElementsByTagName(\"name\")[0].firstChild.nodeValue;\n</code></pre>\n\n<p>I'm finding this really slow when books is really big. Unfortunately, there's no way to limit the result set nor specify a different return type.</p>\n\n<p>Is there a faster way?</p>\n"},{"tags":["sql","performance","oracle"],"answer_count":2,"favorite_count":2,"up_vote_count":2,"down_vote_count":1,"view_count":82,"score":1,"question_id":12782269,"title":"Best Oracle SQL query analyze tool","body":"<p>I'm looking for a tool (either free or paid) capable of visualization oracle SQL query performance. </p>\n\n<p>The ultimate goal is to have a tool, that can be easily read (eg. not EXPLAIN PLAN result) and that I can use to detect slow parts of query (eg. what join with what condition is actually adding the most to the whole query cost). I do <strong>not</strong> need automatic query optimization, I just want to see why does a query has too high cost. Also, it is best if the analyzer can run without special privileges.</p>\n\n<p>I've tried Quest SQL Optimizer for Oracle, which looks quite ok, but I'm missing the option to see join conditions (much like SQL Developer). Also, I'm not able to easily find what join takes up most of the query cost.</p>\n\n<p>What are your experiences? Is there a tool for this?</p>\n"},{"tags":["performance","debugging","vba"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":12815563,"title":"Performance tuning VBA code in large procedure","body":"<p>I've been asked to tune the performance of a specific function which loads every time a worksheet is opened (so it's important that it doesn't make things slow). One of the things that seems to make this function slow is that it does a long call to the database (which is remote), but there are a bunch of other possibilities too. So far, I've been stepping through the code, and when something seems to take a long time making a note of it as a candidate for tuning.</p>\n\n<p>I'd like a more objective way to tell which calls are slowing me down. Searching for timing and VBA yields a lot of results which basically amount to \"Write a counter, and start and stop it either side of the critical section\" (often with the macro explicitly called). I was wondering whether there was a way to (in the debugger) do something like \"Step to next line, <strong>and tell me the time elapsed</strong>\".</p>\n\n<p>If not, can someone suggest a reasonable macro that I could use in the Immediate window to get what I'm after? Specifically, I would like to be able to time an arbitrary line of code within a larger procedure (rather than a whole procedure at once, which is what I found through Google).</p>\n"},{"tags":["sql","oracle","performance"],"answer_count":3,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":6170,"score":0,"question_id":744058,"title":"How do I improve performance of a SQL UPDATE statement whose SET involves an expensive aggregate subquery?","body":"<p>I have the following UPDATE scenario:</p>\n\n<pre><code>UPDATE destTable d\nSET d.test_count = ( SELECT COUNT( employee_id )\n                     FROM sourceTable s\n                     WHERE d.matchCode1 = s.matchCode1 AND\n                           d.matchCode2 = s.matchCode2 AND\n                           d.matchCode3 = s.matchCode3 \n                     GROUP BY matchCode1, matchCode2, matchCode3, employee_id )\n</code></pre>\n\n<p>I have to execute this in a loop changing out the match codes for each iteration.</p>\n\n<p>Between two large tables (~500k records each), this query takes an unacceptably long time to execute.  If I just had to execute it once, I wouldn't care too much.  Given it is being executed about 20 times, it takes way too long for my needs.</p>\n\n<p>It requires two full table scans (one for the destTable and another for the subquery).  </p>\n\n<p>Questions:</p>\n\n<ol>\n<li><p>What techniques do you recommend to speed this up?</p></li>\n<li><p>Does the SQL-optimizer run the subquery for each row I'm updating in the destTable to satisfy the where-clause of the subquery or does it have some super intelligence to do this all at once?</p></li>\n</ol>\n"},{"tags":["asp.net","performance","migration"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":12815984,"title":"Performance issues after upgrading from asp.net 1.1 to 3.5","body":"<p>We have an application which currently runs on Asp.Net 1.1 When we upgraded it to Asp.Net 3.5 we got complaints that the web site was not performing. \nWhen we looked at the CPU history of the webservers then we see when the site was running on Asp.Net 1.1 the load was max 20%, and with 3.5 the load was around the 40 - 60%</p>\n\n<p>The only thing we did change was the Asp.Net upgrade, and the settings in IIS 6 and the web.config change. In the code we did not change anything. </p>\n\n<p>So what is performing better in 1.1 and not in 3.5? \n(the server is an 2003 webserver)</p>\n"},{"tags":["php","database","performance","caching","asynchronous"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":35,"score":0,"question_id":12815546,"title":"Design patterns for asynchronous rebuilding of cache entries","body":"<p>I have an application which routes requests depending on <code>predefined URLs</code> in the following manners:</p>\n\n<p><strong>1st step: Cache lookup</strong><br>\nAll the <code>URLS</code> are aggregated into a single <code>array</code> (which is serialized and stored in cache) which structure is as follows:  </p>\n\n<ul>\n<li>Each <code>array key</code> represents a <code>URL</code>  </li>\n<li>The <code>URL</code> information (i.e. what to do with the <code>URL</code>) is defined as the corresponding <code>array value</code>.</li>\n</ul>\n\n<p>In practice, this gives me (FYI <code>PHP 5.4</code> array syntax):  </p>\n\n<pre><code>&lt;?php\n\n// structure of the cached URLS array\n$cached_urls = [\n    '/pageA' =&gt; [\n        'controller' =&gt; 'ProductController',\n        'content_id' =&gt; 1234\n    ],\n    '/serviceA' =&gt; [\n        'controller' =&gt; 'ServiceController',\n        'content_id' =&gt; 45678\n    ]\n];\n\n// working with the array (retrieve $cached_urls from cache, then...)\nif (!isset($cached_urls[$request['url']])) {\n    // 404\n} else {\n    $url = $cached_urls[$request['url']];\n    // further actions based on $url\n}\n</code></pre>\n\n<p><strong>2nd step: DB lookup &amp; cache rebuilding</strong><br>\nif the <code>$cached_urls</code> array could not be retrieved from cache I do 3 things:  </p>\n\n<ul>\n<li>retrieve the <code>URL</code> information from a <code>url</code> table (where each <code>row</code> represents <code>1 URL</code> and the <code>URL</code> itself is the field used to filter the query) and other related tables.</li>\n<li>Process the request the same way I would to it in the cache scenario once I have <code>$url</code></li>\n<li>Rebuild the cache so that next time a request comes in, I don't have to do a DB lookup,</li>\n</ul>\n\n<p>In terms of execution speed, here is what we have (from faster to slower):  </p>\n\n<ol>\n<li>Cache Lookup  ( ~10ms)  </li>\n<li>DB lookup     ( ~100ms) </li>\n<li>Rebuilding the cache ( ~2000ms)</li>\n</ol>\n\n<p>When the cache is available, page are served very fast, however every time the cache gets rebuilt (which is about every minute), pages take a couple of seconds to be served, which is a problem. Therefore I was wondering:</p>\n\n<p><strong>What are the design patterns available in PHP to perform asynchronous processing (which in my case would be used to rebuild the cache) while avoiding the same tasks to be executed several times concurrently (as I only need the cache to be rebuilt once at a time until it is rebuilt, not for every request that hits my application in the meantime)?</strong></p>\n"},{"tags":["c#",".net","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":110,"score":1,"question_id":1461174,"title":"Fast function to return a string based on a key, and visa versa","body":"<p>I need a fast lookup function that will return a string based on an integer key, and also perform the oppose lookup (pass in string, return int).</p>\n\n<p>Should I create 2 hashtables for this?</p>\n"},{"tags":["android","performance","memory-management","memory-leaks"],"answer_count":1,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":66,"score":2,"question_id":12707572,"title":"Android: what is the differences between shallow and retained heap","body":"<p>I trying to find out memory leak of app by <code>MAT</code>, Here In <code>list_object</code>  I found some numeric value \n     int <code>Shallow heap</code> and <code>retained heap</code> column. What is these value, and how to know where is memory leak.</p>\n\n<p>Thanks In advance.</p>\n"},{"tags":["objective-c","ios","xcode","performance","game-physics"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":81,"score":0,"question_id":12812927,"title":"Can someone help me improve my code so that it can work faster?","body":"<p>In my attempt of making a game without a physics engine my iPod 4g runs the code really slowly. The code works, but its really slow. I was wondering if someone can help me make it run faster. </p>\n\n<pre><code>-(void)Loop {\nNSMutableArray *_remove = [NSMutableArray array];\nfor (UIImageView *b in _blocks) {\n    int i = 0;\n    for (UIImageView *b2 in _blocks) {\n        if (!CGRectContainsPoint(b2.frame, CGPointMake(b.center.x, b.center.y + b.frame.size.height/2)) &amp;&amp; b != b2) {\n            i++;\n        }\n    }\n    if (i == [_blocks count] - 1 &amp;&amp; b.image != wall) {\n        b.center = CGPointMake(b.center.x, b.center.y + 1);\n    } else if (b.image != wall) {\n        for (UIImageView *b2 in _blocks) {\n            for (UIImageView *b3 in _blocks) {\n            //Check for collitions\n            if (b !=b2 &amp;&amp; ((CGRectContainsPoint(b2.frame, CGPointMake(b.center.x + b.frame.size.width/2, b.center.y)) &amp;&amp; b.image == b2.image &amp;&amp; b.image !=wall &amp;&amp; b2.image != wall &amp;&amp; CGRectContainsPoint(b3.frame, CGPointMake(b2.center.x, b2.center.y + b2.frame.size.height/2))) || (CGRectContainsPoint(b2.frame, CGPointMake(b.center.x, b.center.y + b.frame.size.height/2)) &amp;&amp; b.image == b2.image &amp;&amp; b.image !=wall &amp;&amp; b2.image != wall) )) {\n                    [_remove addObject:b];\n                    [_remove addObject:b2];\n            }\n            }}\n    }\n}\nfor (UIImageView *b in _remove) {\n    [b removeFromSuperview];\n    [_blocks removeObject:b];\n}\n}\n</code></pre>\n\n<p>What the code does is it removes the UIImageViews that have a \"wall\" or \"block\" and are also side by side or on top of a \"block\" that has the same image. In other words deletes the images that if they are touching, have the same image, and if they are still (have a block below)</p>\n"},{"tags":["sql","performance","oracle","merge"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":3,"view_count":59,"score":-3,"question_id":12814955,"title":"Performance Issue in Merge Statement","body":"<pre><code>MERGE INTO XXXCG.CCO620_N2K_PURCHASE cst\nUSING (\n  select DISTINCT gl_period_name,    --10.0.0.39\n        org_id,\n        project_code,\n        task_code,\n        exp_type,\n        exp_category,\n        rev_category,\n        gl_unit_revenu,\n        pa_date,\n        line_detail,\n        vendor_name,\n        vendor_code,\n        vendor_type,\n        anly_acc_rev,\n        inter_soc_code,\n        invoice_id,\n        ap_invoice_number,\n        itesoft,\n        revenu,\n        revenu_quantity\n  from  XXXCG.CCO620_N2K_REV_PURCHASE\n where write_flag='N'\n)\nrev\nON(\n      cst.GL_PERIOD_NAME = rev.GL_PERIOD_NAME\n      and cst.org_id=rev.org_id\n      and cst.project_code=rev.project_code\n      and cst.task_code =rev.task_code\n      and cst.exp_category = rev.exp_category\n      and (cst.gl_unit_cost = rev.gl_unit_revenu or  cst.gl_unit_cost &lt;&gt; rev.gl_unit_revenu) --10.0.0.39\n      and cst.exp_type = rev.exp_type\n      and cst.pa_date = rev.pa_date\n      and nvl(cst.invoice_id,'ZZ') = nvl(rev.invoice_id,'ZZ')\n      and nvl(cst.itesoft,'ZZ') = nvl(rev.itesoft,'ZZ')\n      and nvl(cst.ap_invoice_number,'ZZ') = nvl(rev.ap_invoice_number,'ZZ')\n      and nvl(cst.line_detail,'ZZ') = nvl(rev.line_detail,'ZZ'  )\n      and nvl(cst.vendor_type,'ZZ') = nvl(rev.vendor_type,'ZZ')\n      and nvl(cst.vendor_name,'ZZ') = nvl(rev.vendor_name,'ZZ')\n      and cst.rev_category = rev.rev_category\n)\nWHEN MATCHED THEN\nUPDATE SET\n     cst.revenu = rev.revenu,\n     cst.revenu_quantity = rev.revenu,--decode(rev.revenu,0,rev.revenue_quantity,cst.cost_qty_hours *(rev.revenu / abs(rev.revenu))),\n     cst.anly_acc_rev = rev.anly_acc_rev,\n     --cst.rev_category = rev.revenue_category,\n     cst.gl_unit_revenu = rev.gl_unit_revenu,\n     cst.cost_qty_hours = cst.cost_qty_hours *(rev.revenu / abs(rev.revenu ))\nWHERE (cst.gl_unit_cost = rev.gl_unit_revenu or  cst.gl_unit_cost &lt;&gt; rev.gl_unit_revenu)\nWHEN NOT MATCHED THEN\nINSERT (\n      GL_PERIOD_NAME ,\n      ORG_ID        ,\n      PROJECT_CODE  ,\n      TASK_CODE     ,\n      EXP_TYPE      ,\n      EXP_CATEGORY  ,\n      REV_CATEGORY  ,\n      GL_UNIT_COST  ,\n      GL_UNIT_REVENU,\n      PA_DATE       ,\n      LINE_DETAIL   ,\n      VENDOR_NAME   ,\n      VENDOR_CODE   ,\n      VENDOR_TYPE   ,\n      ANLY_ACC_REV  ,\n      ANLY_ACC_COST ,\n      INTER_SOC_CODE,\n      INVOICE_ID  ,\n      AP_INVOICE_NUMBER ,\n      ITESOFT ,\n      REVENU,\n      REVENU_QUANTITY,\n      COST_QTY_HOURS,\n      COST_AMOUNT )\nVALUES (\n     rev.gl_period_name,\n     rev.org_id,\n     rev.project_code,\n     rev.task_code,\n     rev.exp_type,\n     rev.exp_category,\n     rev.rev_category,\n     null,\n     rev.gl_unit_revenu,\n     rev.pa_date,\n     rev.line_detail,\n     rev.vendor_name,\n     rev.vendor_code,\n     rev.vendor_type,\n     rev.anly_acc_rev,\n     null,\n     rev.inter_soc_code,\n     rev.invoice_id,\n     rev.ap_invoice_number,\n     rev.itesoft,\n     rev.revenu,\n     rev.revenu_quantity,\n     null,\n     null\n )\nMERGE STATEMENT;  Optimizer=All_rows;  Cost=5,131,774;  Cardinality=5,865;  Bytes=6,979,350;  Cpu_cost=221,543,483,081;  Io_cost=5,114,435;  Time=157\n</code></pre>\n\n<p>Also I have optimized my merge:</p>\n\n<pre><code>MERGE INTO XXXCG.CCO620_N2K_PURCHASE cst\nUSING (\n  select  gl_period_name,    --10.0.0.39\n        org_id,\n        project_code,\n        task_code,\n        exp_type,\n        exp_category,\n        rev_category,\n        gl_unit_revenu,\n        pa_date,\n        line_detail,\n        vendor_name,\n        vendor_code,\n        vendor_type,\n        anly_acc_rev,\n        inter_soc_code,\n        invoice_id,\n        ap_invoice_number,\n        itesoft,\n        revenu,\n        revenu_quantity\n  from  XXXCG.CCO620_N2K_REV_PURCHASE\n where write_flag='N'\n)\nrev\nON(\n      cst.GL_PERIOD_NAME = rev.GL_PERIOD_NAME\n      and cst.org_id=rev.org_id\n      and cst.project_code=rev.project_code\n      and cst.task_code =rev.task_code\n      and cst.exp_category = rev.exp_category\n--      and (cst.gl_unit_cost = rev.gl_unit_revenu or  cst.gl_unit_cost &lt;&gt; rev.gl_unit_revenu) --10.0.0.39\n      and cst.exp_type = rev.exp_type\n      and cst.pa_date = rev.pa_date\n      and nvl(cst.invoice_id,'ZZ') = nvl(rev.invoice_id,'ZZ')\n      and nvl(cst.itesoft,'ZZ') = nvl(rev.itesoft,'ZZ')\n      and nvl(cst.ap_invoice_number,'ZZ') = nvl(rev.ap_invoice_number,'ZZ')\n      and nvl(cst.line_detail,'ZZ') = nvl(rev.line_detail,'ZZ'  )\n      and nvl(cst.vendor_type,'ZZ') = nvl(rev.vendor_type,'ZZ')\n      and nvl(cst.vendor_name,'ZZ') = nvl(rev.vendor_name,'ZZ')\n      and cst.rev_category = rev.rev_category\n)\nWHEN MATCHED THEN\nUPDATE SET\n     cst.revenu = rev.revenu,\n     cst.revenu_quantity = rev.revenu,--decode(rev.revenu,0,rev.revenue_quantity,cst.cost_qty_hours *(rev.revenu / abs(rev.revenu))),\n     cst.anly_acc_rev = rev.anly_acc_rev,\n     --cst.rev_category = rev.revenue_category,\n     cst.gl_unit_revenu = rev.gl_unit_revenu,\n     cst.cost_qty_hours = cst.cost_qty_hours *(rev.revenu / abs(rev.revenu ))\nWHERE (cst.gl_unit_cost = rev.gl_unit_revenu or  cst.gl_unit_cost &lt;&gt; rev.gl_unit_revenu)\nWHEN NOT MATCHED THEN\nINSERT (\n      GL_PERIOD_NAME ,\n      ORG_ID        ,\n      PROJECT_CODE  ,\n      TASK_CODE     ,\n      EXP_TYPE      ,\n      EXP_CATEGORY  ,\n      REV_CATEGORY  ,\n      GL_UNIT_COST  ,\n      GL_UNIT_REVENU,\n      PA_DATE       ,\n      LINE_DETAIL   ,\n      VENDOR_NAME   ,\n      VENDOR_CODE   ,\n      VENDOR_TYPE   ,\n      ANLY_ACC_REV  ,\n      ANLY_ACC_COST ,\n      INTER_SOC_CODE,\n      INVOICE_ID  ,\n      AP_INVOICE_NUMBER ,\n      ITESOFT ,\n      REVENU,\n      REVENU_QUANTITY,\n      COST_QTY_HOURS,\n      COST_AMOUNT )\nVALUES (\n     rev.gl_period_name,\n     rev.org_id,\n     rev.project_code,\n     rev.task_code,\n     rev.exp_type,\n     rev.exp_category,\n     rev.rev_category,\n     null,\n     rev.gl_unit_revenu,\n     rev.pa_date,\n     rev.line_detail,\n     rev.vendor_name,\n     rev.vendor_code,\n     rev.vendor_type,\n     rev.anly_acc_rev,\n     null,\n     rev.inter_soc_code,\n     rev.invoice_id,\n     rev.ap_invoice_number,\n     rev.itesoft,\n     rev.revenu,\n     rev.revenu_quantity,\n     null,\n     null\n )\nMERGE STATEMENT;  Optimizer=All_rows;  Cost=1,031;  Cardinality=5,865;  Bytes=7,032,135;  Cpu_cost=55,910,709;  Io_cost=1,027;  Time=1\n</code></pre>\n\n<p>But the latter (Optimized merge) takes long time to execute, can anyone help me optimize the query?\nWhat does COST, CARDINALITY and BYTES mean?</p>\n\n<p>Thanks in advance</p>\n"},{"tags":["performance","memcached","benchmarking","tuning","amazon-elasticache"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":33,"score":0,"question_id":12815001,"title":"Benchmarking Elasticache with memslap","body":"<p>I am benchmarking Elasticache m2.4xlarge instance running in Amazon AWS cloud using memslap.</p>\n\n<p>memslap is run from a m2.4xlarge Amazon EC2 linux instance. Memslap successfully works with concurrency levels 256, 512 but fails when the concurrency level is  1024.</p>\n\n<p>When I run memslap with concurrency = 1024, I get the following error\n<em>getaddrinfo(): Name or service not known\nInitialization failed</em></p>\n\n<p><strong>How do I tune the EC2 instance to get memslap to work with a higher concurrency level (>1024) ?</strong></p>\n\n<p><em>ulimit output is unlimited and sysctl -p shows 'fs.file-max = 300000'</em></p>\n\n<p>-Santhosh</p>\n"},{"tags":["performance","osx","firefox","contenteditable","aloha-editor"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":34,"score":0,"question_id":12814872,"title":"The typing goes slow inside contenteditable with Firefox on Mac OS","body":"<p>The typing goes slow inside contenteditable only with Firefox on Mac OS.\nIn this case doesn't happen with another browsers.\nI try reproduce this case with Firefox on Windows OS. It works!!</p>\n\n<p>I found this problem on : <a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=458051\" rel=\"nofollow\">https://bugzilla.mozilla.org/show_bug.cgi?id=458051</a></p>\n\n<p>And you can test on this linl : <a href=\"https://bug458051.bugzilla.mozilla.org/attachment.cgi?id=341294\" rel=\"nofollow\">https://bug458051.bugzilla.mozilla.org/attachment.cgi?id=341294</a></p>\n\n<p>But it haven't had any solution to solve it yet.</p>\n"},{"tags":["asp.net","performance","session","object","pagespeed"],"answer_count":4,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":51,"score":2,"question_id":12813171,"title":"what's better : create new object every time OR store it in session?","body":"<p>Lets take an example of a table adapter (typed data-sets)</p>\n\n<p>I am storing data-set in session. but I don't know what is better for table adapters. </p>\n\n<p>In general, will creating object hit performance badly when compared to storing and getting from session ?</p>\n"},{"tags":["sql","sql-server","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":49,"score":1,"question_id":12812926,"title":"Difference in performance between scope identity() and scope identity() from table?","body":"<p>I have a table say scope_test as follows:</p>\n\n<pre><code>create table scope_test\n(\nid int identity(1,1),\nval varchar(100)\n)\n</code></pre>\n\n<p>Now when i insert a row in this table and select the scope identity through 2 different sql statements, i get to see a difference in performance of the 2 statements:</p>\n\n<pre><code>insert into scope_test values('abcd')\nselect scope_identity() -- statement 1\nselect scope_identity() from scope_test -- statement 2\n</code></pre>\n\n<p>statement 1 is faster than statement 2 according to the execution plan:</p>\n\n<p><img src=\"http://i.stack.imgur.com/aenG1.jpg\" alt=\"enter image description here\"></p>\n\n<p>I am curios to know :\n1. why is this difference in performance , and\n2. Is it safe to use scope identity() as used in statement 1 i.e. without the table name ?</p>\n"},{"tags":["sql","performance","sql-server-2008-r2"],"answer_count":2,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":77,"score":4,"question_id":12812288,"title":"Difference between different ways of writing IF Exists?","body":"<p>I am using SQL Server 2008 R2</p>\n\n<p>I just want to test if something exists in a table</p>\n\n<pre><code>IF EXISTS (SELECT * FROM ta WHERE ca = 'abc') PRINT 'YES'\nIF EXISTS (SELECT ca FROM ta WHERE ca = 'abc') PRINT 'YES'\nIF EXISTS (SELECT 1 FROM ta WHERE ca = 'abc') PRINT 'YES'\nIF EXISTS (SELECT (1) FROM ta WHERE ca = 'abc') PRINT 'YES'\nIF EXISTS (SELECT TOP 1 1 FROM ta WHERE ca = 'abc') PRINT 'YES'\n</code></pre>\n\n<p>Do they have any differences in result/side effect/performance (no matter how tiny)?</p>\n\n<p>Thank you</p>\n"},{"tags":["performance","memory","cpu","capture"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":51,"score":0,"question_id":12795880,"title":"How to capture a process's memory and current CPU usage at specified intervals","body":"<p>requirement is to capture the following information in a single log file every 5 minutes for 4 processes (7005.exe, 7006.exe, 7007.exe, 7008.exe).</p>\n\n<p>filename, memory used (kb), Cpu%, timestamp\n7005.exe, 10240, 75, 10:30 AM\n7006.exe, 10240, 75, 10:30 AM\n7005.exe, 10242, 75, 10:35 AM\n7006.exe, 10000, 75, 10:35 AM</p>\n\n<p>I tried using task list but I am no good at command file scripting.</p>\n\n<p>Please advise,</p>\n\n<p>Thanks.</p>\n"},{"tags":["c#","performance","class"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":49,"score":1,"question_id":12811713,"title":"Efficient temporary dataset","body":"<p>I have a class in C# that is storing information on a stack to be used by other pieces of the application later. The information is currently stored in a class (without any methods) and consists of some ints, shorts, floats and a couple of boolean values.\nI can be processing 10-40 of these packets every second - potentially more - and the information on the stack will be removed when it's needed by another part of the program; however this isn't guaranteed to occur at any definite interval. The information also has a chance of being incomplete (I'm processing packets from a network connection).\nCurrently I have this represented as such:</p>\n\n<pre><code>public class PackInfo\n{\n    public boolean active;\n    public float f1;\n    public float f2;\n    public int i1;\n    public int i2;\n    public int i3;\n    public int i4;\n    public short s1;\n    public short s2;\n}\n</code></pre>\n\n<p>Is there a better way that this information can be represented? There's no chance of the stack getting too large (most of the information will be cleared if it starts getting too big) but I'm worried that there will be a needless amount of memory overhead involved in creating so many instances of the class to act as little more than a container for this information. Even though this is neither a computationally complex or memory-consuming task, I don't see it scaling well should it become either.</p>\n"},{"tags":["c++","performance","boost","hashmap","unordered-map"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":55,"score":2,"question_id":12807659,"title":"What storage policy is being used in the bucket of boost::intrusive::unordered_set?","body":"<p>I understand that boost buckets internally are implemented as liked lists, right?  At least according to <a href=\"http://www.boost.org/doc/libs/1_50_0/doc/html/unordered/buckets.html\" rel=\"nofollow\">http://www.boost.org/doc/libs/1_50_0/doc/html/unordered/buckets.html</a> it seems like it.</p>\n\n<p>My question is, what is the order of elements in these buckets? If they are unordered, is there any way to enforce MRU (most recently used) or some other move-to-front heuristic onto items order in these buckets?</p>\n\n<p>Edit: I understand arguments against enforcing MRU inside buckets. But in my specific case I know, that enforcing MRU [or even having last-in-first-served] would outperform having smaller load factor. Question is, what is the order? Is there a simple way to enforce at least last-inserted-first-out&amp;served.</p>\n"},{"tags":["asp.net","performance","request.form"],"answer_count":2,"favorite_count":5,"up_vote_count":7,"down_vote_count":0,"view_count":648,"score":7,"question_id":5880168,"title":"ASP.NET Request.Form Performance","body":"<p>I used a <code>HttpHandler</code> to implement a light-weight web service targeted for high performance.  It requires a <code>POST</code> with content-type <code>application/x-www-form-urlencoded</code>. The web service does many tasks including decryption, database work, business logic and so on.  During load testing, the performance monitor (ANTS and Visual Studio) point to a single line of code that is taking the majority of time, in fact 67%.</p>\n\n<pre><code>string value = context.Request.Form[MY_FORM_KEY];\n</code></pre>\n\n<p>At the bottom of the call stack for this line of code, the performance monitor, says this call:</p>\n\n<pre><code>System.Web.Hosting.UnsafeIISMethods.MgdSyncReadRequest();\n</code></pre>\n\n<p>is the culprit.</p>\n\n<p>Can anyone help please explain?! The application is in .Net 4, published as release, IIS 7, on Windows Server 2008.</p>\n\n<p>Thank you,\nJoey J. Barrett</p>\n"},{"tags":["django","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":150,"score":3,"question_id":10817352,"title":"django performance bottleneck","body":"<p>My django webpage gets all db queries with 45,41 ms (according django debug toolbar), but server returns initial html page with 300-350 ms (i turn off django debug toolbar when i check out this number). Where is the bottleneck could be? </p>\n\n<p>I suppose problem could be with django templates. Are they that slow?</p>\n\n<p>P.S. Must notice it's not a real webserver, but development one (just running app with \"python manage.py runserver\")</p>\n"},{"tags":["wpf","performance","3d","models"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":237,"score":2,"question_id":9910613,"title":"wpf 3d with lots of models","body":"<p>I'm making a model of human anatomy . there is about 2100 mesh and model.\nI used Helix 3d toolkit to import all files into a scene but that takes a long time to import (about 20 second) and scroll and rotations are slow. and that takes 800 meg of RAM.</p>\n\n<p>objects are optimized and the sum of vertics must be less that 2 million. </p>\n\n<p>i don't need shadows or any special effect. but i need scroll , zoom , hit testing and ..</p>\n\n<p>what format should i use for saving models? i think .obj files are slow. should i use 3ds? or xaml?</p>\n\n<p>should i use 2000 modelvisual3d or one with 2000 Geometrymodel3d ?</p>\n\n<p>or should i use XNA?</p>\n\n<p>can i disable some features to speed things up ?</p>\n\n<p>tnx</p>\n"},{"tags":["performance","profiling","jvisualvm"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":47,"score":1,"question_id":10782234,"title":"Getting breakdown of method execution time in jvisualvm","body":"<p>I am profiling a web application with jvisualvm. I can see how long various methods takes for example methodA takes 5 seconds... However, I can't see to double click this method to see where the 5 seconds is going.  I can \"drill down\" so to speak.</p>\n\n<p>How do I achieve this in jvisualvm?</p>\n\n<p>Thanks.</p>\n"},{"tags":["java","performance","web-services","memory","operating-system"],"answer_count":6,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":104,"score":3,"question_id":12807797,"title":"Java get available memory","body":"<p>Is there any good way to get the remaining memory available to the JVM at run time? The use case of this would be to have web services which fail gracefully when they are nearing their memory limits by refusing new connections with a nice error message \"too many people using this, try again later\", rather than dying abruptly with an OutOfMemory error.</p>\n\n<p>Note this has nothing to do with calculating/estimating the cost of each object beforehand. In principle I could estimate how much memory my objects take and refuse new connections based on that estimate, but that seems kind of hacky/fragile.</p>\n"},{"tags":["ruby-on-rails","performance","query","many-to-many","associations"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":46,"score":3,"question_id":12806991,"title":"Rails: Find entity by many-to-many association","body":"<p>I have a model A, and a model B. A <code>has_and_belongs_to_many</code> Bs, and vice versa.</p>\n\n<p>Now, I want to find an object/entity within A that has_and_belongs_to certain objects within B (say B1 and B2). How can I do that <em>efficiently</em> within Rails? My current solution is something like this:</p>\n\n<pre><code>A.all.select {|a| a.bs.sort == [B1, B2]}.first\n</code></pre>\n\n<p>It basically iterates through <em>all</em> objects within A and checks if it <code>has_and_belongs_to</code> the correct Bs. That is very inefficient. Is there a better way to do this?</p>\n"},{"tags":["mysql","sql","performance","stored-procedures","string-matching"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":89,"score":0,"question_id":12720135,"title":"MySQL stored procedure to find the closest matching call rate for a phone call","body":"<h1>I've got two phone numbers, an a-party and b-party which I want to find the closest matching call rate for.</h1>\n\n<p>There is a table of call rates containing various a-party and b-party numbers and rates per min.</p>\n\n<p>When looking up a call rate, first the most specific match in the a-party is found. If a b-party match is found, the rate from the most specific b-party match is used.\nIf a b-party match is not found for the a-party, the next most specific a-party match is looked at to see if it has a b-party match. If no match is found, a separate message must be returned.</p>\n\n<pre><code>i.e \ncall_rates\na-party     b-party    rate_per_min\n6495631234  619234     0.10\n6495631     6192       0.12\n649         61923      0.09\n649         61         0.16\n</code></pre>\n\n<p>For the above call rates, a call from:</p>\n\n<p><code>a-party: 6495631234</code> to <code>b-party: 619234567</code> would return the <code>rate_per_min: 0.10</code></p>\n\n<p><code>a-party: 6495631111</code> to <code>b-party: 619234567</code> would return the <code>rate_per_min: 0.12</code></p>\n\n<p><code>a-party: 6495631111</code> to <code>b-party: 611112345</code> would return the <code>rate_per_min: 0.16</code></p>\n\n<p><code>a-party: 6495631111</code> to <code>b-party: 619234566</code> would return the <code>rate_per_min: 0.09</code></p>\n\n<p>This is how I've attempted to go about it so far. This is a very rough outline. I have limited experience in writing stored procedures.</p>\n\n<p>I'm just looking for some advice about whether there is a better way to go about this. The call rate table will be very large, so I can imagine having a double for loop will be very inefficient.</p>\n\n<p>Any feedback will be much appreciate.</p>\n\n<pre><code>DELIMITER //\nDROP PROCEDURE IF EXISTS `get_rate`;\nCREATE PROCEDURE `get_rate` (a VARCHAR(45), b VARCHAR(45), OUT rate VARCHAR(45))\nget_r:BEGIN\n    DECLARE i, j INT;\n    DECLARE match_string, result, temp_string VARCHAR(255);\n\n    SET j = LENGTH(b);\n    SET i = LENGTH(a);\n\n    WHILE i &gt; 0 DO\n        SET temp_string = SUBSTRING(a,0,i);\n        SET result = (SELECT * FROM call_rate_overrides WHERE a_party LIKE CONCAT(temp_string, '%'));\n        WHILE j &gt; 0 DO\n            SET temp_string = SUBSTRING(b,0,j);\n            SET match_string = (SELECT * FROM call_rate_overrides WHERE b_party LIKE CONCAT(temp_string,'%'));\n            IF ISNOTNULL(match_string) THEN\n                SET rate = match_string;\n                LEAVE get_r;\n            END IF;  \n            SET j = j - 1;\n        END WHILE;\n\n        SET i = i - 1;\n    END WHILE;\nEND //\n</code></pre>\n"},{"tags":["performance","pex"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":25,"score":0,"question_id":12767989,"title":"Pex run in loop after reporting?","body":"<p>i run pex by command line on a class of about 2200 methods. </p>\n\n<p>After 2 days of elaboration, I think to have the test suite and report html, pex finished this morning at 8.05 AM and the list file modified is at 08.40 AM.</p>\n\n<p>The last line of output are :</p>\n\n<pre><code>08:05:08.3&gt; [finished] execution time 2.08:05:08.3014942.\n        -- 0 critical errors, 0 errors, 0 warnings.\n        -- 83635 generated tests, 1 failing, 83635 new, 0 inconclusive.\n\n[coverage] generating coverage reports...\n[reports] generating reports...\n[reports] report path: reports\\121005.001302.3848.pex\n</code></pre>\n\n<p>The process pex.exe is running continuos, But what is happened ? After about 3.5 hours ( now for me is 12.15 pm ) its maybe is going to loop or what is the operation is it doing ?</p>\n\n<p>CPU is full and memory too ( process pex 1.5G of RAM)</p>\n\n<p>Do you think I have to stop it?\nThanks best regards.</p>\n"},{"tags":["java","performance","coding-style"],"answer_count":7,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":100,"score":3,"question_id":12802562,"title":"Java performace vs. code-style: Making multiple method calls from the same line of code","body":"<p>I am curious whether packing multiple and/or nested method calls within the same line of code is better for performance and that is why some develpers do it, at the cost of making their code less readable.</p>\n\n<p>E.g.</p>\n\n<pre><code>//like\nSet&lt;String&gt; jobParamKeySet = jobParams.keySet();\nIterator&lt;String&gt; jobParamItrtr = jobParamKeySet.iterator();\n</code></pre>\n\n<p>Could be also written as</p>\n\n<pre><code>//dislike    \nIterator&lt;String&gt; jobParamItrtr = jobParams.keySet().iterator();\n</code></pre>\n\n<p>Personally, I hate the latter because it does multiple evaluations in the same line and is hard for me to read the code.  That is why I try to <strong>avoid by all means</strong> to have more than one evaluation per line of code.  I also don't know that <code>jobParams.keySet()</code> returns a <code>Set</code> and that bugs me.<br>\nAnother example would be:</p>\n\n<pre><code>//dislike\nBar.processParameter(Foo.getParameter());\n</code></pre>\n\n<p>vs</p>\n\n<pre><code>//like\nParameter param = Foo.getParameter();\nBar.processParameter(param);\n</code></pre>\n\n<p>The former makes me noxious and dizzy as I like to consume simple and clean evaluations in every line of code and I just hate it when I see other people's code written like that.</p>\n\n<p><strong>But are there any (performance) benefits to packing multiple method calls in the same line?</strong></p>\n\n<p>EDIT:  Single liners are also more difficult to debug, thanks to @stemm for reminding</p>\n"},{"tags":["performance","file","file-io","folder"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":11,"score":0,"question_id":12746309,"title":"Opening a file or a folder deeper on the server","body":"<p>Generally speaking, when opening a folder or a file on a remote server, is it slower if it's deeper? </p>\n\n<p>In some cases, like programming in Outlook with VBA, we need to open each folders independently to look at the sub-directories (even if we already have the full path), which means it's a lot longer when it's deeper. But usually, when opening a file on any samba server, we just go to the direct path. </p>\n\n<p>So, does it make a difference in time/response when opening <code>\\\\server\\file.ext</code> versus opening <code>\\\\server\\folder1\\folder2\\folder3\\folder4\\folder5\\folder6\\folder7\\folder8\\file.ext</code> considering a slow to respond server?</p>\n"},{"tags":["javascript","performance","events","observers","mutation"],"answer_count":2,"favorite_count":2,"up_vote_count":1,"down_vote_count":1,"view_count":419,"score":0,"question_id":11425209,"title":"Are DOM Mutation Observers slower than DOM Mutation Events?","body":"<p>The following code utilize DOM Mutation Event <code>DOMNodeInserted</code> to detect the existence of the <code>body</code> element and wrap its <code>innerHTML</code> into a wrapper.</p>\n\n<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;script src=\"http://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js\"&gt;&lt;/script&gt;\n    &lt;script&gt;\n        function DOMmanipulation() {\n            if (document.body) {\n                document.removeEventListener('DOMNodeInserted', DOMmanipulation);\n                // DOM manipulation start\n                document.body.innerHTML = '&lt;div class=\"wrapper\"&gt;' + document.body.innerHTML + '&lt;/div&gt;';\n                // DOM manipulation end\n            }\n        }\n        document.addEventListener('DOMNodeInserted', DOMmanipulation);\n    &lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;p&gt;Lorem ipsum dolor sit amet.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n\n<p>And despite the success of the wrapping, there is an error shows that a node was not found. <a href=\"http://stackoverflow.com/a/11407389/1091014\">This answer</a> of a question explained that it is because when jQuery had been loaded, it added a <code>div</code> element into the body to do some tests, but it failed to remove that <code>div</code> element because that element has been wrapped into the wrapper so that it's not a child element of body anymore.</p>\n\n<p>The above experiment tells us that <code>DOMNodeInserted</code> event is faster than jQuery's tests because jQuery's test element (<code>div</code>) got wrapped before it can be removed by jQuery.</p>\n\n<p><br></p>\n\n<hr>\n\n<p><br>\nNow the following code can achieve the same manipulation, and it's using the newly introduced DOM Mutation Observers. As of this time (2012-07-11), it works only on Chrome 18 and higher.</p>\n\n<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;script src=\"http://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js\"&gt;&lt;/script&gt;\n    &lt;script&gt;\n        var observer = new WebKitMutationObserver(function() {  \n            if (document.body) {\n                observer.disconnect();\n                // DOM manipulation start\n                document.body.innerHTML = '&lt;div class=\"wrapper\"&gt;' + document.body.innerHTML + '&lt;/div&gt;';\n                // DOM manipulation end\n            }\n        });\n        observer.observe(document, { subtree: true, childList: true });\n    &lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;p&gt;Lorem ipsum dolor sit amet.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n\n<p>This codes didn't produce any error. That means jQuery is faster than DOM Mutation Observers, so it was able to remove its test element (<code>div</code>) before that element can be wrapped into the wrapper.</p>\n\n<p><br></p>\n\n<hr>\n\n<p><br></p>\n\n<p>From the above two experiments, we find that when it comes to execution speed:</p>\n\n<ul>\n<li>DOM Mutation Events &gt; jQuery's tests</li>\n<li>jQuery's tests &gt; DOM Mutation Observers</li>\n</ul>\n\n<p>Can this result appropriately prove that DOM Mutation Observers is slower than DOM Mutation Events?</p>\n"},{"tags":["java","performance","build"],"answer_count":3,"favorite_count":8,"up_vote_count":11,"down_vote_count":1,"view_count":370,"score":10,"question_id":12605377,"title":"Build times increase substantially when switching to Java 7","body":"<p>We use Java within our build process, as it is used to resolve/publish our dependencies via Ivy. </p>\n\n<p>No problem, nor have we had with it for 2 years, until we've tried to upgrade Java 6 Update 26 to Version 7 Update 7, whereas a build on a local developer PC (WinXP) now takes 2 hours to complete, instead of 10 minutes!!</p>\n\n<p>Nothing else has changed on the PC, making it the absolute target for our concerns.</p>\n\n<p>Does anyone know of any reason as to why version 7 of Java would make such a speed difference like this?</p>\n\n<p>UPDATE: The build process is NAnt-based, so Java.exe is called from a NAnt script, running in a Command (DOS) window.</p>\n"},{"tags":["jquery","ajax","performance","firebug","fiddler"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":92,"score":1,"question_id":12797737,"title":"jQuery ajax call, server respoding very late?","body":"<p>I have this ajax call executing from client side, with jQuery, but at times, (I mean generally at the very first call) it is executing very slow, perhaps this figure will explain..</p>\n\n<p><img src=\"http://i.stack.imgur.com/ALI8I.png\" alt=\"enter image description here\"></p>\n\n<p>As you can see, the last call is waiting <strong>609ms</strong> before anything, though it took <strong>210ms</strong> to receive data, that's bearable, but why the wait of <strong>609ms</strong>?</p>\n\n<p>Fiddler's statistics are</p>\n\n<pre><code>Request Count:   1\nBytes Sent:      552        (headers:543; body:9)\nBytes Received:  364        (headers:234; body:130)\n\nACTUAL PERFORMANCE\n--------------\nClientConnected:    15:16:42.799\nClientBeginRequest: 15:16:42.799\nGotRequestHeaders:  15:16:42.799\nClientDoneRequest:  15:16:42.799\nDetermine Gateway:  0ms\nDNS Lookup:         0ms\nTCP/IP Connect:     0ms\nHTTPS Handshake:    0ms\nServerConnected:    15:16:42.799\nFiddlerBeginRequest:15:16:42.799\nServerGotRequest:   15:16:43.408\nServerBeginResponse:15:16:43.408\nGotResponseHeaders: 15:16:43.408\nServerDoneResponse: 15:16:43.408\nClientBeginResponse:15:16:43.408\nClientDoneResponse: 15:16:43.705\n</code></pre>\n\n<p>The jQuery is nothing special, just a simple ajax call...</p>\n\n<pre><code>$.ajax({\n        url: '/AutoComplete.asmx/GetPriorityAndRemarks',\n        type: 'POST',\n        timeout: 20000,\n        datatype: 'xml',\n        cache: false,\n        data: 'arg=' + custCode,\n        success: function (response) {\n            var result = $(response).find(\"string\").text();\n            // the values is in form of name, address, mobile, priority and remark, and discount\n            var resultAry = result.split(':');\n            //alert(resultAry);\n            $('#txtCustomerName').val(resultAry[0].trim());\n            $('#lblAddress').text(resultAry[1]);\n            $('#lblMobileNo').text(resultAry[2]);\n            $('#lblPriority').text(resultAry[3]);\n            $('#lblRemarks').text(resultAry[4]);\n            $('#txtDiscount').val(resultAry[5]);\n            $('#txtQty').focus();\n            $('#txtQty').select();\n            $('#hdnCustCode').val(custCode);\n            return false;\n        },\n        error: function (response) {\n             alert('some error occured');\n        }\n    });\n</code></pre>\n\n<p>The code is implemented as</p>\n\n<pre><code>public string GetPriorityAndRemarks(string arg)\n{\n    try\n    {\n        SqlCommand cmd = new SqlCommand();\n\n        cmd.CommandText = \"sp_NewBooking\";\n        cmd.CommandType = CommandType.StoredProcedure;\n        cmd.Parameters.AddWithValue(\"@BranchId\", Globals.BranchID);\n        cmd.Parameters.AddWithValue(\"@CustCode\", arg);\n        cmd.Parameters.AddWithValue(\"@Flag\", 31);\n        return PrjClass.ExecuteScalar(cmd);\n    }\n    catch (Exception)\n    {\n        return \"\";\n    }\n}\n</code></pre>\n\n<p>So, what's causing it to taking so much time? Also, as a added measure, thinking that calling this code the first time might be slow, I called this code with dummy argument at page load just so that when code gets called with real data, it's not the <em>first time</em> and any thing regarding server being set up, caching or whatever is already done. But still no luck, the call regardless of weather I call this code with dummy data at page load or not, is always slow when making first call. Can anyone explain to me why is this happening this way?</p>\n"},{"tags":["ios","performance","uiscrollview","cap"],"answer_count":0,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":52,"score":2,"question_id":12805145,"title":"Slow down UIScrollView / Cap the max speed","body":"<p>I have a pagingEnabled scrollview which dynamnically adds more content as the user approaches the very end of it. Paging enabled allows me to force the user to be only able to scroll one item at a time.</p>\n\n<p>The problem is that adding additional content takes time. If the user scrolls through my content too quickly (you can still go pretty darn quickly even with pagingEnabled = TRUE), the scroll attempts will cause my scrollview to occasionally \"bounce\" as though it hit the end. If the user attempts to scroll again, immediately afterwards, they can continue scrolling. I wish to prevent this bounce from happening in a user-friendly way.</p>\n\n<p>I assume this bounce is happening because the iPod/iPhone cannot load the scrollview content in time. It is actually a fairly resource intensive process, I'd imagine (Each subView added to the scrollview is a UIImageView with a UITextView subview added to it).</p>\n\n<p>I need a way to cap the scrollview's speed. I tried playing with decelerate rate but I didn't notice any difference. What can I do?</p>\n"},{"tags":["python","performance","memoization","cprofile"],"answer_count":3,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":230,"score":2,"question_id":7077820,"title":"python how to memoize a method","body":"<p>Say I a method to create a dictionary from the given parameters:</p>\n\n<pre><code>def newDict(a,b,c,d): # in reality this method is a bit more complex, I've just shortened for the sake of simplicity\n    return d { \"x\": a,\n               \"y\": b,\n               \"z\": c,\n               \"t\": d }\n</code></pre>\n\n<p>And I have another method that calls newDict method each time it is executed. Therefore, at the end, when I look at my cProfiler I see something like this:</p>\n\n<pre><code>17874 calls (17868 primitive) 0.076 CPU seconds\n</code></pre>\n\n<p>and of course, my newDict method is called <code>1785</code> times. Now, my question is whether I can memorize the newDict method so that I reduce the call times? (Just to make sure, the variables change almost in every call, though I'm not sure if it has an effect on memorizing the function)</p>\n\n<p>Sub Question: I believe that 17k calls are too much, and the code is not efficient. But by looking at the stats can you also please state whether this is a normal result or I have too many calls and the code is slow?</p>\n"},{"tags":["c#",".net","performance","design","clr"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":87,"score":2,"question_id":12503732,"title":"What are the performance costs of introducing a CLR type (class or interface)?","body":"<p>Even though .Net allows dynamic invocation (e.g. with reflection, C# dynamic keyword), but when using a language such as C# we sometimes feel it is necessary to use static typing, in order to prove that our program is correct, and will not have typing issues at runtime.</p>\n\n<p>Sometimes this results in us introducing interfaces or base classes that fee like they are just for purpose of explaining to the compiler that 'Yes, I know all the objects I pass to this context are going to be understand invoke Method X with arg Y - here, I will prove it to you using an interface definition!' (For example - .net internally uses IReadChunkBytes interface to allow passing either SteamReadChunkBytes or BufferReadChunkBytes objects to some method or other.) </p>\n\n<p>Other times we create classes or types to serve other purposes which are do not feel very usefully type-y, such as being unique identifiers (a bit like enums) with small attached behavior, or to hold a set of constants, etc.</p>\n\n<p>I'm interested in better understanding what the compiletime, runtime, and other costs are going to be when I face such design decisions where I am asking 'should I define a new type or interface just in order to solve this problem?' Obviously there will be two sides to the cost and benefit in each such comparison, but in general we should hopefully see the same costs for 'define new type' in each such comparison/disucssion. How do we quantify these costs?</p>\n"},{"tags":["php","performance","fopen"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":96,"score":-1,"question_id":12804023,"title":"What is faster alternative to fOpen?","body":"<p>I am trying to do the following code <code>fOpen()</code> but much faster. </p>\n\n<pre><code>&lt;?php\n\n    $supercdn=\"http://super.xxxxxxxxxxc-cdn.com/\"; \n    $catwalkcdn=\"http://catwalk.xxxxxxxc-cdn.com/\";\n\n    $oldfile=\"\".$catwalkcdn.\"\".$pid.\".flv\";\n    $newfile=\"\".$supercdn.\"\".$pid.\"/v/\".$pid.\".swf\";\n    $combi=0;\n    if(@fopen($oldfile,\"r\")){\n        $combi=1;\n    } else {\n        if(@fopen($newfile,\"r\")){\n            $combi=2;\n        } else {\n            $combi=3;\n        }\n    }\n?&gt;\n</code></pre>\n\n<p><code>fOpen()</code> is being incredibly slow on my application. I want to know if there is an alternative to using it? I have broken my entire application down to this chunk and I am using shared hosting. Is there some method I can read from and return the value of <code>combi</code> back. Appreciate any tips.</p>\n"},{"tags":["iphone","ios","performance","avfoundation","alassetslibrary"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":134,"score":1,"question_id":11788633,"title":"iOS faster saving of photos to album","body":"<p>I'm using the ALAssets library to write photos I take to the album using the library's\n    writeImageDataToSavedPhotosAlbum:metadata:completionBlock:</p>\n\n<p>However, when I take photos to often the images take too much RAM so my app crashes. </p>\n\n<p>So.. what I did is I disabled the \"take photo\" button at the beginning of its IBAction and unblocked it in the completionBlock. </p>\n\n<p>And.. that makes it really slow. So my question is.. does anyone know a faster way of saving photos, or at least an easy way to keep a manageable number of them in RAM so that the first few can be made faster.. The thing is I haven't found an easy way to check if I'll have enough memory if I take a new photo.. Thanks!</p>\n"},{"tags":["c++","performance","stdstring","zero-copy"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":86,"score":2,"question_id":12785666,"title":"Can I do a zero-copy std::string allocation in C++ from a const char * array?","body":"<p>Profiling of my application reveals that it is spending nearly 5% of CPU time in string allocation. In many, many places I am making C++ std::string objects from a 64MB char buffer. The thing is, the buffer never changes during the running of the program. My analysis of <code>std::string(const char *buf,size_t buflen)</code> calls is that that the string is being copied because the buffer might change after the string is made. That isn't the problem here. Is there a way around this problem?</p>\n\n<p>EDIT: I am working with binary data, so I can't just pass around <code>char *s</code>. Besides, then I would have a substantial overhead from always scanning for the NULL, which the <code>std::string</code> avoids.</p>\n"},{"tags":["php","performance","mongodb","lithium"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":120,"score":2,"question_id":12772994,"title":"Extremely slow lithium query, fast in MongoDB","body":"<p>So, I've been trying out the php framework lithium now and it seems like a really good framework and all but I have a slight problem. A query I run on a collection with only 6k+ documents is amazingly slow from php but blazingly fast when I run it from the terminal.</p>\n\n<p>One document in the collection may look like this:</p>\n\n<pre><code>{\n    \"_id\" : ObjectId(\"504c9a3b6070d8b7ea61938e\"),\n    \"startDate\" : \"Jan 2011\",\n    \"episodes\" : [\n        {\n            \"title\" : \"Series 1, Episode 1\",\n            \"airdate\" : ISODate(\"2011-01-20T00:00:00Z\"),\n            \"epnum\" : \"1\",\n            \"prodnum\" : null,\n            \"seasonnum\" : \"01\",\n            \"link\" : \"http://www.tvrage.com/10_OClock_Live/episodes/1065007783\"\n        },\n        {and maybe 20 more},\n    ],\n    \"runTime\" : \"60 min\",\n    \"endDate\" : \"Apr 2012\",\n    \"network\" : \"Channel 4\",\n    \"numberOfEpisodes\" : \"25 eps\",\n    \"title\" : \"10 O'Clock Live\",\n    \"directory\" : \"10OClockLive\",\n    \"country\" : \"UK\",\n    \"tvrage\" : \"27363\"\n}\n</code></pre>\n\n<p>I want to get all episodes that exists for this current month. So in the terminal (I use fake values and more than a month) I use the following query:</p>\n\n<pre><code>db.series.find({'episodes.airdate': {$gt: ISODate('2012-09-07 00:00:00'), $lt: ISODate('2012-11-01')}})\n</code></pre>\n\n<p>And wham, it just goes very fast. Even if I do an explain() on the query it tells me that it's fast:</p>\n\n<pre><code>{\n    \"cursor\" : \"BtreeCursor episodes.airdate_1\",\n    \"isMultiKey\" : true,\n    \"n\" : 382,\n    \"nscannedObjects\" : 1620,\n    \"nscanned\" : 1620,\n    \"nscannedObjectsAllPlans\" : 1620,\n    \"nscannedAllPlans\" : 1620,\n    \"scanAndOrder\" : false,\n    \"indexOnly\" : false,\n    \"nYields\" : 0,\n    \"nChunkSkips\" : 0,\n    **\"millis\" : 181**,\n    \"indexBounds\" : {\n        \"episodes.airdate\" : [\n            [\n                ISODate(\"2012-09-07T00:00:00Z\"),\n                ISODate(\"292278995-01--2147483647T07:12:56.808Z\")\n            ]\n        ]\n    },\n    \"server\" : \"example:27017\"\n}\n</code></pre>\n\n<p>But when I use the query inside php and lithium, man, it take ages:</p>\n\n<pre><code>$series = Series::find('all', array(\n                'fields' =&gt; array('title', 'episodes.title', 'episodes.airdate'),\n                'conditions' =&gt; array('episodes.airdate' =&gt; array('$gt' =&gt; new MongoDate(strtotime(date('Y-m-01'))), '$lt' =&gt;  new MongoDate(strtotime(date('Y-m-t')))))\n            ));\n</code></pre>\n\n<p>And if I even try to loop through it, then it's even worse <strong>well past</strong> the 30 second execution time. All though, I think I have a memory leak since I had to add this <code>ini_set('memory_limit', '-1');</code> without getting a \"maxium usage\" or whatever.</p>\n\n<p>Could anyone provide me with a answer on why this is happening? Is there any way to improve the speed of the query? I have no idea why it is so slow and I would be super glad if anyone could point me in the right direction.</p>\n"},{"tags":["vb.net","performance","event-handling","windows-mobile"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":66,"score":1,"question_id":12749758,"title":"Slow Response After Clicking Button in VB.NET","body":"<p>I have created a windows form on a Windows Mobile 6.5 application. This screen has a username and password field and a login menu button. After clicking the button, it takes my click event for the login button 6 or 7 seconds to reach the first line of code (an alert). It takes that long to reach the breakpoint as well.</p>\n\n<p>I've tried:</p>\n\n<ol>\n<li>changing the menu button to a login button</li>\n<li>using AddHandler in my formLoad event</li>\n</ol>\n\n<p>...and have still not gotten a fast response. What else could be slowing this down?</p>\n"},{"tags":["java","performance","hibernate","java-ee","glassfish"],"answer_count":2,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":263,"score":4,"question_id":12661103,"title":"Very low performance with Glassfish 3.1.2 and Java EE + Hibernate","body":"<p>I faced with very very low performance of Java EE (EJB + JSF) application and Hibernate(3.6.8.Final and 4.1.7.Final) on Glassfish 3.1.2. Sending about 300 select queries takes about 20 seconds. This is unacceptable.</p>\n\n<p>I have exactly the same application deployed on JBoss and TomEE. There, the same 300 select queries takes about 1,5 second.</p>\n\n<p>I found in google some answers that maybe <code>hibernate.show_sql</code> is <code>true</code> or <code>hibernate.hbm2ddl</code> make application soo slow. But it is not true. I turned off <code>hibernate.show_sql</code> but is doesn't matter. Moreover, these options are true in the JBoss and TomEE versions and it works over 10 times faster!\nI thought that this is the issue between Glasfish and Hibernate. But I have the next application with the same business logic, the same DAO with EntityManager provided by Hibernate but configurated with Spring. And the performance is great. It is weird, isn't it?</p>\n\n<p><code>persistence.xml</code> from the defect version:</p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;persistence xmlns=\"http://java.sun.com/xml/ns/persistence\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" version=\"1.0\"\n  xsi:schemaLocation=\"http://java.sun.com/xml/ns/persistence\n  http://java.sun.com/xml/ns/persistence/persistence_1_0.xsd\"&gt;\n\n   &lt;persistence-unit name=\"jee_project\" transaction-type=\"JTA\"&gt;\n      &lt;provider&gt;org.hibernate.ejb.HibernatePersistence&lt;/provider&gt;\n      &lt;jta-data-source&gt;jdbc/PostgreSQL&lt;/jta-data-source&gt;\n      &lt;properties&gt;\n         &lt;property name=\"hibernate.dialect\" value=\"org.hibernate.dialect.PostgreSQLDialect\"/&gt;\n         &lt;property name=\"hibernate.hbm2ddl.auto\" value=\"update\"/&gt;\n         &lt;property name=\"hibernate.show_sql\" value=\"false\"/&gt;\n         &lt;property name=\"current_session_context_class\" value=\"thread\"/&gt;\n      &lt;/properties&gt;\n   &lt;/persistence-unit&gt;\n\n&lt;/persistence&gt;\n</code></pre>\n\n<p>Glassfish JDBC configuration</p>\n\n<pre><code>&lt;jdbc-connection-pool driver-classname=\"\" datasource-classname=\"org.postgresql.ds.PGConnectionPoolDataSource\" res-type=\"javax.sql.ConnectionPoolDataSource\" description=\"\" name=\"PostgreSQLPool\"&gt;\n      &lt;property name=\"User\" value=\"postgresql\"&gt;&lt;/property&gt;\n      &lt;property name=\"DatabaseName\" value=\"qazxsw\"&gt;&lt;/property&gt;\n      &lt;property name=\"LogLevel\" value=\"0\"&gt;&lt;/property&gt;\n      &lt;property name=\"Password\" value=\"1234\"&gt;&lt;/property&gt;\n      &lt;property name=\"ServerName\" value=\"localhost\"&gt;&lt;/property&gt;\n      &lt;property name=\"Ssl\" value=\"false\"&gt;&lt;/property&gt;\n      &lt;property name=\"ProtocolVersion\" value=\"0\"&gt;&lt;/property&gt;\n      &lt;property name=\"TcpKeepAlive\" value=\"false\"&gt;&lt;/property&gt;\n      &lt;property name=\"SocketTimeout\" value=\"0\"&gt;&lt;/property&gt;\n      &lt;property name=\"PortNumber\" value=\"5432\"&gt;&lt;/property&gt;\n      &lt;property name=\"LoginTimeout\" value=\"0\"&gt;&lt;/property&gt;\n      &lt;property name=\"UnknownLength\" value=\"2147483647\"&gt;&lt;/property&gt;\n      &lt;property name=\"PrepareThreshold\" value=\"5\"&gt;&lt;/property&gt;\n    &lt;/jdbc-connection-pool&gt;\n    &lt;jdbc-resource pool-name=\"PostgreSQLPool\" description=\"\" jndi-name=\"jdbc/PostgreSQL__pm\"&gt;&lt;/jdbc-resource&gt;\n    &lt;jdbc-resource pool-name=\"PostgreSQLPool\" description=\"\" jndi-name=\"jdbc/PostgreSQL__nontx\"&gt;&lt;/jdbc-resource&gt;\n</code></pre>\n"},{"tags":["magento","performance"],"answer_count":3,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":1313,"score":2,"question_id":3706138,"title":"Magento Performance Tuning","body":"<p>I have a server which is running more than 15 Magento stores, but they are not performing well, though I have a giant server for hosting them. My server configuration is - 8 CPU's Quad Core 24GB RAM and 2 TB HDD.</p>\n\n<p>My current page load is 1.6sec. I want it under 600ms. I have already installed APC, &amp; eAccelerator and tuned Apache's parameters. I am using the latest Magento version.</p>\n\n<p>Please suggest.</p>\n\n<p>-Ramesh</p>\n"},{"tags":["java","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":30,"score":0,"question_id":12800434,"title":"JASPER performance improvement?","body":"<p>Using jasper 4.6.0 version. Created repor templates (jrxml) using iReport. Using java objects data source to create the reports. JASPER operation is taken over 2 minutes to generate a XLS/CSV with 16,000 rows in the report! We use 2 jasper operations - compile and fillReport.</p>\n\n<p>Any tips to improve the performance, please? On googling found that one of the techniques is to have the .jasper file instead of .jrxml file i.e. avoid compilation as part of the report generation time...will try on this, is there any other additional performance tips??</p>\n\n<p>Below is the code used,</p>\n\n<p>JasperReport jasperReport = JasperCompileManager.compileReport(getTemplateFile());\njasperPrint = JasperFillManager.fillReport(jasperReport,                    parameters, new JRBeanCollectionDataSource(object));</p>\n\n<p>Note: Using JExcelApiExporter and JRCsvExporter to generate excel and csv files - using byte array output stream for JRCsvExporterParameter.OUTPUT_STREAM and JRXlsExporterParameter.OUTPUT_STREAM,</p>\n"},{"tags":["php","mysql","performance","insert"],"answer_count":1,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":102,"score":7,"question_id":12797368,"title":"Variation in speed of SQL inserts","body":"<p>The script I'm working on is designed to update a database table which records the country of use and the status of all IP addresses (or almost all of them). Currently I'm keeping it simple and am only fetching data from the 5 RIRs (Regional Internet Registries) and saving that to my database. </p>\n\n<p>Initially the speeds were impractical but they have been improved signficantly by reducing the amount of information in the log and grouping the SQL inserts into groups of 1000 and using a single query. However, when running the script now I get very large variations in the speed of the SQL inserts and I was wondering if anyone knew why. </p>\n\n<p>Here are the some of the speeds I've recorded. In the test I separated out the time taken to execute the iterations of the script in PHP and the time taken to apply the sql statement, I've not included the PHP times in the list below as the effect was negligible; no more than 1 second for even the largest blocks of data.</p>\n\n<p><strong>Test Speeds</strong> (number of data rows being inserted remains the same throughout)</p>\n\n<p><strong>Test 1</strong>\nTotal SQL executing time: 33 seconds</p>\n\n<p><strong>Test 2</strong>\nTotal SQL executing time: 72 seconds</p>\n\n<p><strong>Test 3</strong>\nTotal SQL executing time: 78 seconds</p>\n\n<p>Other tests continued to fluctuate between ~30 seconds and ~80 seconds.</p>\n\n<p>I have two questions:</p>\n\n<p>1) Should I accept these disparities as the way of the world, or is there a reason for them?</p>\n\n<p>2) I felt nervous about lumping the ~185000 row inserts into one query. Is there any reason I should avoid using one query for these inserts? I've not worked with this amount of data being saved at one time before.</p>\n\n<p>Thank you</p>\n\n<p>__</p>\n\n<p>The database table is as follows.</p>\n\n<p>Sorage Engine - InnoDB</p>\n\n<p><strong>Columns:</strong></p>\n\n<p>id         - int, primary key</p>\n\n<p>registry   - varchar(7)</p>\n\n<p>code       - varchar(2)</p>\n\n<p>type       - varchar(4)</p>\n\n<p>start      - varchar(15)</p>\n\n<p>value      - int</p>\n\n<p>date       - datetime</p>\n\n<p>status     - varchar(10)</p>\n"},{"tags":["performance","lucene","hibernate-search"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":12798300,"title":"What is a reasonable setting for Hibernate Search MassIndexer?","body":"<p>In my application I use <em>Hibernate Search</em> to manage a <em>Lucene</em> index of some of my mapped model classes (10 classes, partly associated to each other -- using <code>indexEmbedded</code> quite some time in the index definitions). There are approx. 1,500,000 documents to index </p>\n\n<p>For rebuilding the whole index, I use a mass indexer as proposed in the documentation \n<a href=\"http://docs.jboss.org/hibernate/search/3.3/reference/en-US/html/manual-index-changes.html\" rel=\"nofollow\">http://docs.jboss.org/hibernate/search/3.3/reference/en-US/html/manual-index-changes.html</a></p>\n\n<pre><code>fullTextSession\n    .createIndexer()\n    .batchSizeToLoadObjects(200)\n    .cacheMode(CacheMode.IGNORE)\n    .purgeAllOnStart(true)\n    .threadsToLoadObjects(10)\n    .threadsForIndexWriter(10)\n    .threadsForSubsequentFetching(5)\n    .startAndWait();\n</code></pre>\n\n<p>My database connection pool has a size of 50</p>\n\n<p>I observe that the indexing procedure starts promising fast until it reached about 25% of all documents. After that the performance declines drastically (the next 5% take twice as long as the first 25%) and I am wondering why this happens?</p>\n\n<ul>\n<li>Do I have a wrong ratio of object-loading threads and indexing threads?</li>\n<li>Or is it simply due to the growing size of the index? Does this justify this decline of performance?</li>\n<li>How to improve the performance? How to achieve a constant progress in time?</li>\n</ul>\n\n<p>Because I make use of projections rather than letting Hibernate Search fetch search results from DB, many of my indexed fields are stored in Index (<code>Store.YES</code>). Does this affect the performance significantly?</p>\n\n<p>-- Edit:</p>\n\n<p>My Hibernate search configuration:</p>\n\n<pre><code>properties.setProperty(\"hibernate.search.default.directory_provider\", \"filesystem\");\nproperties.setProperty(\"hibernate.search.default.indexBase\", searchIndexPath);\nproperties.setProperty(\"hibernate.search.indexing_strategy\", \"manual\");\nproperties.setProperty(\"hibernate.default_batch_fetch_size\", \"200\");\n</code></pre>\n"},{"tags":["performance","magento"],"answer_count":5,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":92,"score":0,"question_id":12758604,"title":"How do you test Magento performance","body":"<p>What tools do you use to test Magento hosting performance?\nI need a tool to check how many customers website can have at once, with reporting. Also it will be nice if tool can show me page load time (without images and with) and give some advices for performance increase.</p>\n"},{"tags":["performance","windows-phone-7","profiling"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":201,"score":1,"question_id":8930474,"title":"decent performance profiler windows phone 7","body":"<p>I am looking for a performance/cpu profiler for Windows Phone 7 silverlight apps. Something like dotTrace or Ants Profiler. Is there such a thing?</p>\n"},{"tags":["c++","performance","memory","aix"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":53,"score":0,"question_id":12799969,"title":"Performance Memory Analyzer for C++,C on AIX","body":"<p>I am looking to find a performance/memory analysis tool for C++,C on AIX machine..</p>\n\n<p>Anay best recommended tool ? help me out.</p>\n\n<p>Thanks</p>\n"},{"tags":["database","performance","serialization","filesystems"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":184,"score":1,"question_id":4793353,"title":"How Fast is Windows File System for Database Operations?","body":"<p>I came across a crazy thought and I wanted to share it with you and ask about its feasibility, especially performance wise:</p>\n\n<p>The idea is to manage object database operations by:</p>\n\n<ul>\n<li>creating a folder for each class named after class name</li>\n<li>creating a sub-folder for each sub-class named after sub-class name</li>\n<li>creating a file for each object named after its unique ID</li>\n<li>creating a sub-folder for each index named after names of indexed fields</li>\n<li>creating a shortcut file for each index entry referring to the original object file</li>\n<li>reading/writing binary objects by very fast serializer/deserializer</li>\n<li>inserting/updating/deleting objects and index entries by renaming object and shortcut files</li>\n<li>caching/paging by using memory-mapped files</li>\n<li>querying would utilize binary search on sorted file names</li>\n</ul>\n\n<p><strong>UPDATE:</strong> Thank you all for your replies. I was thinking this can be even improved by using some compression/encryption library such as 7z, instead of dealing with the OS file system. Otherwise, all of your stated concerns so far are valid. I'm wondering what kind of underlying file system does, for example, Oracle uses</p>\n"},{"tags":["c#","visual-studio-2010","performance","release","launch"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":80,"score":0,"question_id":12799303,"title":"Why is C# Release 3x slower when launched from VS2010 IDE?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/12397108/disassembly-view-of-c-sharp-64-bit-release-code-is-75-longer-than-32-bit-debug\">Disassembly view of C# 64-bit Release code is 75% longer than 32-bit Debug code?</a>  </p>\n</blockquote>\n\n\n\n<p>I have an extremely simple C# Console Application, hat does some sorting on a big number of elements (only a few lines of code with array operations).</p>\n\n<p>When I start the release code from Visual Studio IDE with F5 or Ctrl-F5 the program is about 3x slower than when started directly from Win-Explorer.</p>\n\n<pre><code>41.140 seconds when launched from VS 2010 IDE\n13.950 seconds when launched by double-clicking myprogram.exe\n</code></pre>\n\n<p>Why???</p>\n"},{"tags":["programming-languages","performance","screen-scraping"],"answer_count":5,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":335,"score":3,"question_id":5900844,"title":"Screen Scraping Efficiency","body":"<p>We are going to be scraping thousands of websites each night to update client data, and we are in the process of deciding which language we would like to use to do the scraping.</p>\n\n<p>We are not locked into any platform or language, and I am simply looking for efficiency.  If I have to learn a new language to make my servers perform well, that is fine.</p>\n\n<p>Which language/platform will provide the highest scraping efficiency per dollar for us?  Really I'm looking for real-world experience with high volume scraping.  It will be about maximizing CPU/Memory/Bandwidth.</p>\n"},{"tags":["sql-server","performance","locking"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":1480,"score":0,"question_id":2606226,"title":"SQL Server 2008 Running trigger after Insert, Update locks original table","body":"<p>I have a serious performance problem.</p>\n\n<p>I have a database with (related to this problem), 2 tables.</p>\n\n<p>1 Table contains strings with some global information. The second table contains the string stripped down to each individual word. So the string is like indexed in the second table, word by word.</p>\n\n<p>The validity of the data in the second table is of less important then the validity of the data in the first table. </p>\n\n<p>Since the first table can grow like towards 1*10^6 records and the second table having an average of like 10 words for 1 string can grow like 1*10^7 records, i use a nolock in order to read the second this leaves me free for inserting new records without locking it (Expect many reads on both tables). </p>\n\n<p>I have a script which keeps on adding and updating rows to the first table in a MERGE statement. On average, the data beeing merged are like 20 strings a time and the scripts runs like ones every 5 seconds.</p>\n\n<p>On the first table, i have a trigger which is beeing invoked on a Insert or Update, which takes the newly inserted or updated data and calls a stored procedure on it which makes sure the data is indexed in the second table. (This takes some significant time).</p>\n\n<p>The problem is that when having the trigger disbaled, Reading the first table happens in a few ms. However, when enabling the trigger and your in bad luck of trying to read the first table while this is beeing updated, Our webserver gives you a timeout after 10 seconds (which is way to long anyways).</p>\n\n<p>I can quess from this part that when running the trigger, the first table is kept (partially) in a lock untill the trigger is completed.</p>\n\n<p>What do you think, if i'm right, is there a easy way around this?</p>\n\n<p>Thanks in advance!</p>\n\n<p>As requested:</p>\n\n<pre><code>ALTER TRIGGER [dbo].[OnFeedItemsChanged] \n   ON  [dbo].[FeedItems] \n   AFTER INSERT,UPDATE\nAS \nBEGIN\n    -- SET NOCOUNT ON added to prevent extra result sets from\n    -- interfering with SELECT statements.\n    SET NOCOUNT ON;\n\n    DECLARE @id int;\n    SELECT @id = ID FROM INSERTED;\n    IF @id IS NOT NULL\n    BEGIN\n        DECLARE @title nvarchar(MAX);\n        SELECT @title = Title FROM INSERTED;\n        DECLARE @description nvarchar(MAX);\n        SELECT @description = [Description] FROM INSERTED;\n\n        SELECT @title = dbo.RemoveNonAlphaCharacters(@title)\n        SELECT @description = dbo.RemoveNonAlphaCharacters(@description)\n\n        -- Insert statements for trigger here\n        EXEC dbo.usp_index_itemstring @id, @title;\n        EXEC dbo.usp_index_itemstring @id, @description;\n    END\nEND\n</code></pre>\n\n<p>The FeedItems table is populated by this query: </p>\n\n<pre><code>MERGE INTO FeedItems i\nUSING @newitems d ON i.Service = d.Service AND i.GUID = d.GUID\nWHEN matched THEN UPDATE\n    SET i.Title = d.Title,\n        i.Description = d.Description,\n        i.Uri = d.Uri,\n        i.Readers = d.Readers\nWHEN NOT matched THEN INSERT\n    (Service, Title, Uri, GUID, Description, Readers)\n    VALUES\n    (d.Service, d.Title, d.Uri, d.GUID, d.Description, d.Readers);\n</code></pre>\n\n<p>The sproc: IndexItemStrings is populating the second table, executing this proc does indeed take his time. The problem is that while executing this trigger. Queries applied to the FeedItems table are mostly timing out (even those queries who dont uses the second table)</p>\n\n<p>First table:</p>\n\n<pre><code>USE [ICI]\nGO\n\n/****** Object:  Table [dbo].[FeedItems]    Script Date: 04/09/2010 15:03:31 ******/\nSET ANSI_NULLS ON\nGO\n\nSET QUOTED_IDENTIFIER ON\nGO\n\nCREATE TABLE [dbo].[FeedItems](\n    [ID] [int] IDENTITY(1,1) NOT NULL,\n    [Service] [int] NOT NULL,\n    [Title] [nvarchar](max) NULL,\n    [Uri] [nvarchar](max) NULL,\n    [Description] [nvarchar](max) NULL,\n    [GUID] [nvarchar](255) NULL,\n    [Inserted] [smalldatetime] NOT NULL,\n    [Readers] [int] NOT NULL,\n CONSTRAINT [PK_FeedItems] PRIMARY KEY CLUSTERED \n(\n    [ID] ASC\n)WITH (PAD_INDEX  = OFF, STATISTICS_NORECOMPUTE  = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS  = ON, ALLOW_PAGE_LOCKS  = ON) ON [PRIMARY]\n) ON [PRIMARY]\n\nGO\n\nALTER TABLE [dbo].[FeedItems]  WITH CHECK ADD  CONSTRAINT [FK_FeedItems_FeedServices] FOREIGN KEY([Service])\nREFERENCES [dbo].[FeedServices] ([ID])\nON DELETE CASCADE\nGO\n\nALTER TABLE [dbo].[FeedItems] CHECK CONSTRAINT [FK_FeedItems_FeedServices]\nGO\n\nALTER TABLE [dbo].[FeedItems] ADD  CONSTRAINT [DF_FeedItems_Inserted]  DEFAULT (getdate()) FOR [Inserted]\nGO\n</code></pre>\n\n<p>Second table:</p>\n\n<pre><code>USE [ICI]\nGO\n\n/****** Object:  Table [dbo].[FeedItemPhrases]    Script Date: 04/09/2010 15:04:47 ******/\nSET ANSI_NULLS ON\nGO\n\nSET QUOTED_IDENTIFIER ON\nGO\n\nCREATE TABLE [dbo].[FeedItemPhrases](\n    [FeedItem] [int] NOT NULL,\n    [Phrase] [int] NOT NULL,\n    [Count] [smallint] NOT NULL,\n CONSTRAINT [PK_FeedItemPhrases] PRIMARY KEY CLUSTERED \n(\n    [FeedItem] ASC,\n    [Phrase] ASC\n)WITH (PAD_INDEX  = OFF, STATISTICS_NORECOMPUTE  = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS  = ON, ALLOW_PAGE_LOCKS  = ON) ON [PRIMARY]\n) ON [PRIMARY]\n\nGO\n\nALTER TABLE [dbo].[FeedItemPhrases]  WITH CHECK ADD  CONSTRAINT [FK_FeedItemPhrases_FeedItems] FOREIGN KEY([FeedItem])\nREFERENCES [dbo].[FeedItems] ([ID])\nON UPDATE CASCADE\nON DELETE CASCADE\nGO\n\nALTER TABLE [dbo].[FeedItemPhrases] CHECK CONSTRAINT [FK_FeedItemPhrases_FeedItems]\nGO\n\nALTER TABLE [dbo].[FeedItemPhrases]  WITH CHECK ADD  CONSTRAINT [FK_FeedItemPhrases_Phrases] FOREIGN KEY([Phrase])\nREFERENCES [dbo].[Phrases] ([ID])\nON UPDATE CASCADE\nON DELETE CASCADE\nGO\n\nALTER TABLE [dbo].[FeedItemPhrases] CHECK CONSTRAINT [FK_FeedItemPhrases_Phrases]\nGO\n</code></pre>\n\n<p>And more: </p>\n\n<pre><code>ALTER PROCEDURE [dbo].[usp_index_itemstring] \n    -- Add the parameters for the stored procedure here\n    @item int, \n    @text nvarchar(MAX) \nAS\nBEGIN\n    -- SET NOCOUNT ON added to prevent extra result sets from\n    -- interfering with SELECT statements.\n    SET NOCOUNT ON;\n\n    -- DECLARE a table containing all words within the text\n    DECLARE @tempPhrases TABLE \n    ( \n        [Index] int,\n        [Phrase] NVARCHAR(256) \n    );\n\n    -- extract each word from text and store it in the temp table\n    WITH Pieces(pn, start, [stop]) AS \n    ( \n        SELECT 1, 1, CHARINDEX(' ', @text) \n        UNION ALL \n        SELECT pn + 1, CAST([stop] + 1 AS INT), CHARINDEX(' ', @text, [stop] + 1) \n        FROM Pieces \n        WHERE [stop] &gt; 0 \n    )   \n    INSERT INTO @tempPhrases\n    SELECT pn, SUBSTRING(@text, start, CASE WHEN [stop] &gt; 0 THEN [stop]-start ELSE LEN(@text) END) AS s \n    FROM Pieces\n    OPTION (MAXRECURSION 0);    \n\n    WITH CombinedPhrases ([Phrase]) AS \n    (\n        -- SELECT ALL 2-WORD COMBINATIONS\n        SELECT w1.[Phrase] + ' ' + w2.[Phrase] \n        FROM @tempPhrases w1\n        JOIN @tempPhrases w2 ON w1.[Index] + 1 = w2.[Index]\n        UNION ALL -- SELECT ALL 3-WORD COMBINATIONS\n        SELECT w1.[Phrase] + ' ' + w2.[Phrase] + ' ' + w3.[Phrase]\n        FROM @tempPhrases w1\n        JOIN @tempPhrases w2 ON w1.[Index] + 1 = w2.[Index]\n        JOIN @tempPhrases w3 ON w1.[Index] + 2 = w3.[Index]\n        UNION ALL  -- SELECT ALL 4-WORD COMBINATIONS\n        SELECT w1.[Phrase] + ' ' + w2.[Phrase] + ' ' + w3.[Phrase] + ' ' + w4.[Phrase]\n        FROM @tempPhrases w1\n        JOIN @tempPhrases w2 ON w1.[Index] + 1 = w2.[Index]\n        JOIN @tempPhrases w3 ON w1.[Index] + 2 = w3.[Index]\n        JOIN @tempPhrases w4 ON w1.[Index] + 3 = w4.[Index]\n    )\n\n    -- ONLY INSERT THE NEW PHRASES IN THE Phrase TABLE      \n    INSERT INTO @tempPhrases\n    SELECT 0, [Phrase] FROM CombinedPhrases\n\n    -- DELETE PHRASES WHICH ARE EXCLUDED\n    DELETE FROM @tempPhrases\n    WHERE [Phrase] IN\n    (\n        SELECT [Text] FROM Phrases p\n        JOIN ExcludedPhrases ex\n        ON ex.ID = p.ID\n    );\n\n    MERGE INTO Phrases p\n    USING \n    (\n        SELECT DISTINCT Phrase FROM @tempPhrases\n    ) t\n    ON p.[Text] = t.Phrase\n    WHEN NOT MATCHED THEN\n        INSERT VALUES (t.Phrase);\n\n\n    -- Finally create relations between the phrases and feeditem,   \n    MERGE INTO FeedItemPhrases p\n    USING \n    (\n        SELECT @item as [Item], MIN(p.[ID]) as Phrase, COUNT(t.[Phrase]) as [Count]\n        FROM Phrases p WITH (NOLOCK)\n        JOIN @tempPhrases t ON p.[Text] = t.[Phrase]\n        GROUP BY t.[Phrase]\n    ) t\n    ON p.FeedItem = t.Item\n    AND p.Phrase = t.Phrase\n    WHEN MATCHED THEN\n        UPDATE SET p.[Count] = t.[Count]\n    WHEN NOT MATCHED THEN\n        INSERT VALUES (t.[Item], t.Phrase, t.[Count]);\nEND\n</code></pre>\n\n<p>and more:</p>\n\n<pre><code>ALTER Function [dbo].[RemoveNonAlphaCharacters](@Temp NVarChar(max)) \nReturns NVarChar(max) \nAS \nBegin \n    SELECT @Temp = REPLACE (@Temp, '%20', ' ');\n\n    While PatIndex('%[^a-z ]%', @Temp) &gt; 0 \n        Set @Temp = Stuff(@Temp, PatIndex('%[^a-z ]%', @Temp), 1, '') \n    Return @TEmp \nEnd \n</code></pre>\n"},{"tags":["performance","postgresql","jsf","jpa","ejb"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":3,"view_count":40,"score":-2,"question_id":12794659,"title":"How to improve page load performance in JSF","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/12793598/want-tips-and-tricks-to-speed-up-jsf-page-loading\">Want tips and tricks to speed up JSF page loading</a>  </p>\n</blockquote>\n\n\n\n<p>Please any one can help me in, how to speed up JSF page loading. My all jsf pages contains huge number of components. I am not sure whether it is taking time to load all the components. I am using EJB, JPA, JSF framework. DB is postgresql. Please help me out in this.</p>\n\n<p>I cannot put all the code here. But I can tell u that, my page contains many command links. Each command link will opens a dialog box. When user clicks on command link button, it will calls an action listener routine then it will updates the dialog box form.</p>\n\n<p>and one more issue is when submitting the form from dialog box, then it takes time(~2-3 secs) to complete submit and updating other things.</p>\n"},{"tags":["performance","codeigniter","profiling"],"answer_count":5,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":3628,"score":3,"question_id":2789089,"title":"Codeigniter: Profiling and Performance","body":"<p>I'm new to app development and CI as a whole, so I've got an oodle of questions.</p>\n\n<p>What is profiling?\nHow is it used?\nHow does it work?\nWhat is considered a \"long\" time?</p>\n\n<p>More importantly,</p>\n\n<p><strong>How do I use it to improve performance?</strong></p>\n\n<p><em>The reason I'm asking is b/c my app is <strong>really</strong> sluggish right now.</em></p>\n"}]}
{"total":25593,"page":14,"pagesize":100,"questions":[{"tags":["performance","xslt","razor","umbraco"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":80,"score":1,"question_id":12786042,"title":"Umbraco XSLT Performance factors ","body":"<p>In Umbraco CMS. Is there a way to test and see how longs it takes to pull nodes of a certain DocType in XSLT?  Is there an XSLT timer or anything that can do this. </p>\n\n<p>We are trying to test what is faster to use. Razor or XSLT for our needs.  In Razor we can get the time because it is essentially C# but if XSLT is rendered faster, we would like to use that instead.</p>\n"},{"tags":["c#","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":81,"score":0,"question_id":12796843,"title":"c# ReadToEnd slow performance with large xml files","body":"<p>I get huge xml file from a service, and Im facing some performance issue with readtoend which is taking about 2 minutes to complete with 3 replace() and 1.3 minutes without using replace().</p>\n\n<pre><code> HttpWebResponse objResponse = (HttpWebResponse)objRequest.GetResponse();\n  using (StreamReader sr = new StreamReader(objResponse.GetResponseStream()))\n{\nresult = sr.ReadToEnd().Replace(\"\\n\", \"\").Replace(\"\\r\", \"\").Replace(\"\\t\", \"\");   \n         sr.Close();\n        //ReadToEnd it is taking about 2 minutes to complete \n}\n</code></pre>\n\n<p>What to use to overcome this performance issue.</p>\n\n<p><strong>Edited</strong> \nThe xml file size is 2.77 MB</p>\n"},{"tags":["java","performance","jsf","components"],"answer_count":2,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":2332,"score":2,"question_id":6015609,"title":"JSF ui:fragment rendered performance","body":"<p>I have a set of jsf components that are statically generated from a set of excel files (they are updated by business people). Each generated file represents a business object that has slightly different data, and all of them belong to a same class.</p>\n\n<p>In order to render this dynamically, the only solution I found was to set up a bunch of <code>ui:fragment</code> and dispatch to the right component at runtime:</p>\n\n<p>\n    \n        \n        \n    </p>\n\n<pre><code>&lt;!-- IMPLEMENTATION --&gt;          \n&lt;composite:implementation&gt; \n    &lt;ui:fragment rendered=\"#{cc.attrs.type eq 'cartcred'}\"&gt;\n        &lt;limites:limites-cartcred  limite=\"#{cc.attrs.limite}\"/&gt;\n    &lt;/ui:fragment&gt;\n    &lt;ui:fragment rendered=\"#{cc.attrs.type eq 'cdcp'}\"&gt;\n        &lt;limites:limites-cdcp limite=\"#{cc.attrs.limite}\"/&gt;\n    &lt;/ui:fragment&gt;\n    &lt;ui:fragment rendered=\"#{cc.attrs.type eq 'cheqpredatado'}\"&gt;\n        &lt;limites:limites-cheqpredatado limite=\"#{cc.attrs.limite}\"/&gt;\n    &lt;/ui:fragment&gt;\n    &lt;ui:fragment rendered=\"#{cc.attrs.type eq 'confirming'}\"&gt;\n        &lt;limites:limites-confirming limite=\"#{cc.attrs.limite}\"/&gt;\n    &lt;/ui:fragment&gt;\n   &lt;!-- many more lines --&gt;\n   &lt;!-- many more lines --&gt;\n   &lt;!-- many more lines --&gt;\n    &lt;ui:fragment rendered=\"#{cc.attrs.type eq 'contacorr'}\"&gt;\n        &lt;limites:limites-contacorr limite=\"#{cc.attrs.limite}\"/&gt;\n    &lt;/ui:fragment&gt;\n</code></pre>\n\n<p></p>\n\n<p>But I found out that the perfomance of this is terrible. I tought that JSF would only render a single component, but it seems that it is rendering <strong>all</strong> of them and \"hiding\" the others at runtime.</p>\n\n<p>Is there a more efficient way of achieving my goal? I want to render <strong>a single component</strong> based on runtime information about a business class (much like an if-then-else), but I can only determine what is the component to render at runtime.</p>\n\n<hr>\n\n<p><strong>Clarification:</strong>\nwhat happens is that each component referenced by <code>limites:limites*</code> is a huge complex page with lots of other components. At runtime, the parameter named <code>type' will decide what component to render. But my tests show that if I only render one component, but leave the other</code>ui:fragments` (even knowing that they will not be rendered), it will render <em>much</em> slower than if I remove the components.</p>\n\n<p>So if my page is exactly like this:</p>\n\n<pre><code>&lt;composite:interface&gt;\n    &lt;composite:attribute name=\"type\" required=\"true\" /&gt;\n    &lt;composite:attribute name=\"limite\" required=\"true\" /&gt;\n&lt;/composite:interface&gt;         \n&lt;composite:implementation&gt; \n    &lt;ui:fragment rendered=\"#{cc.attrs.type eq 'cartcred'}\"&gt;\n        &lt;limites:limites-cartcred  limite=\"#{cc.attrs.limite}\"/&gt;\n    &lt;/ui:fragment&gt;\n&lt;/composite:implementation&gt;\n</code></pre>\n\n<p>it will render <em>much</em> (around 10x) faster than the initial version, even though the parameters are the same. I suspect that JSF will create the entire component tree and only at runtime it will decide (depending on the supplied parameter) if it will render each other or not.</p>\n\n<hr>\n\n<p><strong>Edit</strong></p>\n\n<p>Almost there. I just need to include my <strong>composite component</strong> <em>dynamically</em>. I tried evaluating an ELExpression but that didn't work. What I need is a way of accessing the current scope within the component creation, and using that to generate the proper file name:</p>\n\n<pre><code>//obviously, ELExpressions don't work here\nResource resource = application.getResourceHandler().createResource(\"file-#{varStatus.loop}.xhtml\", \"components/dynamicfaces\");\n</code></pre>\n"},{"tags":["iphone","ios","performance","uilocalnotification","localnotification"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":49,"score":1,"question_id":12795252,"title":"Performance issue with scheduleLocalNotification(Large number of local notifications)","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/7721362/schedule-number-of-local-notifications\">Schedule number of Local Notifications</a>  </p>\n</blockquote>\n\n\n\n<p>I'm facing performance problem with scheduleLocalNotification.\nI'm trying to register large number of local notifications. \nIt's like birthday alarms for friends. For test, I tried to register about 300 notifications, but my iPhone4 took over 2 minutes. (iPad2 4 seconds, iPhone4S 8 seconds)</p>\n\n<p>Here's a code.</p>\n\n<pre><code>-(void)setAllBirthdaysSchedule\n{\n    Class cls = NSClassFromString(@\"UILocalNotification\");\n    if (cls == nil) {\n        return ;\n    }\n\n    NSAutoreleasePool *pool = [[NSAutoreleasePool alloc] init];\n\n    [[UIApplication sharedApplication] cancelAllLocalNotifications];\n\n    if (prototypeNotification == nil) {\n        prototypeNotification = [[UILocalNotification alloc] init];\n        prototypeNotification.repeatCalendar = [NSCalendar currentCalendar];\n        prototypeNotification.repeatInterval = NSYearCalendarUnit;\n\n        prototypeNotification.timeZone = [NSTimeZone defaultTimeZone];\n        prototypeNotification.fireDate = [NSDate dateWithTimeIntervalSinceNow:0];\n        prototypeNotification.applicationIconBadgeNumber = 0;\n        prototypeNotification.alertBody = NSLocalizedString(@\"Body\", nil);\n        prototypeNotification.alertAction = NSLocalizedString(@\"Action\", nil);\n    }\n\n    NSArray* arr = [self getAllBirthday];\n\n    for (User* user in arr) {                \n        UILocalNotification *notif = [prototypeNotification copy];\n        notif.fireDate = user.birthday;\n        [[UIApplication sharedApplication] scheduleLocalNotification:notif];\n        [notif release];\n\n    }\n\n    [pool release];\n}\n</code></pre>\n"},{"tags":["c++","performance","time-series"],"answer_count":8,"favorite_count":10,"up_vote_count":9,"down_vote_count":0,"view_count":1734,"score":9,"question_id":4426362,"title":"C++ Time Series Library (Analysis and Processing)","body":"<p>I'm looking to get SO'ers advice and suggestion on Time-Series libraries written in C++, some of the constraints and requirements for the library:</p>\n\n<ul>\n<li>Performance is very critical</li>\n<li>Capable of handling <strong><em>very large</em></strong> data sets (1MB - 100TB range)</li>\n<li>Various kind of discretization/grouping methodologies</li>\n<li>Basic functionality (n-avg, EMA, smoothing, Forecasting, normalization)</li>\n<li>Suitable for use in a multi-threaded environments</li>\n<li>Free or Open source preferred, however commercial libraries are welcome</li>\n<li>Libraries capable of delegating to GPU based calculations are welcome</li>\n</ul>\n"},{"tags":["windows","performance","xperf"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":113,"score":0,"question_id":10111876,"title":"Dumping select GPUView events","body":"<p>Is there a way to dump specific events, for example, only the NT events corresponding to a specific process + DX events, into a text file from GPUView or xperf?</p>\n"},{"tags":["asp.net","sql-server","database","performance","sql-server-2008"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":44,"score":3,"question_id":12795236,"title":"Find database calls from a Page","body":"<p>I am using <code>sql server 2008</code> and website is in <code>asp.net 1.1</code>.</p>\n\n<p>Is there a way to find out <strong><code>how many times database has been called from a page</code></strong>?</p>\n\n<pre><code>What are the stored procedure called from the page?\n\nWhat are the tables involved in a page load?\n</code></pre>\n\n<p>Please suggest how can i track these?</p>\n"},{"tags":["performance","optimization","webserver"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":20,"score":0,"question_id":12794717,"title":"How to keep a webpage on the screen after link click and refresh only after the new page loaded?","body":"<p>I've seen some websites do it, I'll just explain what happens.</p>\n\n<p>Normally when you click on a link inside any site, the browser screen goes white, waits a couple of seconds, then it starts rendering the page, loading images, etc.</p>\n\n<p>But some websites, after you click links, the current page stays put even though you see the browser's preloaders spinning and the server processes the request in the background. But it never goes white. It will only display their empty background of the next page, and only for a split-second. </p>\n\n<p>How do websites achieve this?</p>\n\n<p>Here's two example links for comparison:</p>\n\n<ol>\n<li><a href=\"http://www.blackhatworld.com/\" rel=\"nofollow\">http://www.blackhatworld.com/</a> (no white background)</li>\n<li><a href=\"http://www.moneymakerdiscussion.com/\" rel=\"nofollow\">http://www.moneymakerdiscussion.com/</a> (shows white background)</li>\n</ol>\n"},{"tags":["c++","performance","cuda","copy","device"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":124,"score":1,"question_id":12792693,"title":"CUDA device to host copy very slow","body":"<p>I'm running windows 7 64 bits, cuda 4.2, visual studio 2010.</p>\n\n<p>First, I run some code on cuda, then download the data back to host. Then do some processing and move back to the device.\nThen I did the following copy from device to host, it runs very fast, like 1ms.</p>\n\n<pre><code>clock_t start, end;\ncount=1000000;\nthrust::host_vector &lt;int&gt; h_a(count);\nthrust::device_vector &lt;int&gt; d_b(count,0);\nint *d_bPtr = thrust::raw_pointer_cast(&amp;d_b[0]);\nstart=clock();\nthrust::copy(d_b.begin(), d_b.end(), h_a.begin());\nend=clock();\ncout&lt;&lt;\"Time Spent:\"&lt;&lt;end-start&lt;&lt;endl;\n</code></pre>\n\n<p>It takes ~1ms to finish.</p>\n\n<p>Then I ran some other code on the cuda again, mainly atomic operations. Then I copy the data from device to host, it takes very long time， like ~9s.</p>\n\n<pre><code>__global__ void dosomething(int *d_bPtr)\n{\n....\natomicExch(d_bPtr,c)\n....\n}\n\nstart=clock();\nthrust::copy(d_b.begin(), d_b.end(), h_a.begin());\nend=clock();\ncout&lt;&lt;\"Time Spent:\"&lt;&lt;end-start&lt;&lt;endl;\n</code></pre>\n\n<p>~ 9s</p>\n\n<p>I ran the code multiple times, for example</p>\n\n<pre><code>int i=0;\nwhile (i&lt;10)\n{\nclock_t start, end;\ncount=1000000;\nthrust::host_vector &lt;int&gt; h_a(count);\nthrust::device_vector &lt;int&gt; d_b(count,0);\nint *d_bPtr = thrust::raw_pointer_cast(&amp;d_b[0]);\nstart=clock();\nthrust::copy(d_b.begin(), d_b.end(), h_a.begin());\nend=clock();\ncout&lt;&lt;\"Time Spent:\"&lt;&lt;end-start&lt;&lt;endl;\n\n__global__ void dosomething(int *d_bPtr)\n{\n....\natomicExch(d_bPtr,c)\n....\n}\n\nstart=clock();\nthrust::copy(d_b.begin(), d_b.end(), h_a.begin());\nend=clock();\ncout&lt;&lt;\"Time Spent:\"&lt;&lt;end-start&lt;&lt;endl;\ni++\n}\n</code></pre>\n\n<p>The results are pretty much the same.<br>\nWhat could be the problem?</p>\n\n<p>Thank you!</p>\n"},{"tags":["c#","asp.net","performance","data-binding","treeview"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":340,"score":2,"question_id":3959544,"title":"Better than TreeView","body":"<p>I'm binding a lot of data to a TreeView control as the data is a natural category hierarchy. The problem is that there is a lot of it. I have managed to remove a lot of the overhead by only binding those nodes which appear in the visible tree, but this still leaves a lot in the ViewState, et al.</p>\n\n<p>Does anyone have a method or alternative control for improving this kind of performance issue, please?</p>\n\n<p>I was thinking about trying to inherit the TreeView control and dump it's viewstate value into Sesssion and back - but it's quite a hack I don't really have time for, right now...</p>\n"},{"tags":["ios","performance","tableview","local","sdwebimage"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":60,"score":0,"question_id":12792704,"title":"Loading local images from documents directory to tableview using SDWebImage","body":"<p>I am using a tableview which loads images from the documents directory, creates a thumbnail and shows it in the tableview. However, I have a problem: it becomes slow and crashes as the pictures are large, taken using the camera.</p>\n\n<p>I have explored several solution including GCD to do the work in a background thread but the result is the same thing. So, I thought to look into SDWebImage but I don't know if it will also work for local files, not web images in this case. Can someone advise me please? If not, how is this problem solved? Is there an API that can help to resolve this issue?</p>\n"},{"tags":["python","performance","numpy"],"answer_count":4,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":128,"score":3,"question_id":12787650,"title":"Finding the Index of N biggest elements in Python Array / List Efficiently","body":"<p>I'm sorry in advance if this is a duplicated question, I looked for this information but still couldn't find it.</p>\n\n<p>Is it possible to arrange a numpy array (or python list) by using the indexes of the N biggest elements in decreasing order very efficiently?</p>\n\n<p>For instance, the array:</p>\n\n<pre><code>a = array([4, 1, 0, 8, 5, 2])\n</code></pre>\n\n<p>The indexes of the biggest elements in decreasing order would give (considering N = 6, all the elements are included):</p>\n\n<p>8 --> 3</p>\n\n<p>5 --> 4</p>\n\n<p>4 --> 0</p>\n\n<p>2 --> 5</p>\n\n<p>1 --> 1</p>\n\n<p>0 --> 2</p>\n\n<pre><code>result = [3, 4, 0, 5, 1, 2]\n</code></pre>\n\n<p>I know how to make it using a somewhat silly approach (like sorting the array and searching for each of the N numbers for their indexes), but I was wondering if is there any efficient library like bottleneck or heapq or maybe a pythonic approach to make this very fast. I have to apply it in several arrays with 300k elements each so that's why performance is an issue.</p>\n\n<p>Thanks in advance!</p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>I read the answers and decided to timeit them using a 300k of random integers, here are the results:</p>\n\n<p><strong>solution 1:</strong> sorted(range(len(a)), key=lambda i:a[i]) <strong>time:</strong> 230 ms</p>\n\n<p><strong>solution 2:</strong> heapq.nlargest(len(a), zip(a, itertools.count())) <strong>time:</strong> 396 ms</p>\n\n<p><strong>solution 3:</strong> heapq.nlargest(len(a), enumerate(a), key=operator.itemgetter(1)) <strong>time:</strong> 864 ms</p>\n\n<p><strong>solution 4:</strong> def f(a,N): return np.argsort(a)[::-1][:N] (N = len(a)) <strong>time: 104 ms</strong></p>\n\n<p>Thanks a lot for the fast and very good answers!</p>\n"},{"tags":["asp.net","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":45,"score":-1,"question_id":12792110,"title":"100% CPU Usage ASP.NET App","body":"<p>I am using an ASP.NET application. On click of any control in the page, it is found that the CPU usage goes to 100% which result in slow response. And to come to the OnInit/Page_load it took around 9-10 seconds of time. During check in the task manager, there is one mscorsvw.exe running in the background and it tooks most of the CPU usage.</p>\n\n<p>Can somebody provide any input/hint, what could be the issue.</p>\n"},{"tags":["performance","implementation","classifier","theorem"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":29,"score":-1,"question_id":12769845,"title":"Implementation of naive bayes theorem:","body":"<p>I am trying to implement naive bayes theorem in Java.\nThere is a small problem.\nI have all the documents required for training.\nBut I am not sure where to store the probabilities that are calculated for every word for every class.</p>\n\n<p>What is an efficient way to store these values in such a way that its easy to retrieve them? Database or files or any other approach?</p>\n"},{"tags":["mysql","sql","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":76,"score":1,"question_id":12774137,"title":"What's wrong with my query (very slow)?","body":"<p>The following query is taking more than 20 seconds to execute on a table with around half million rows:</p>\n\n<pre><code>SELECT images.id, images.user_id, images_locale.filename, extension, size, width, height, views, batch, source, status, images.created_at, images.category_id, title, short_description, long_description, alternate, slugs.name as slug, images_locale.slug_id, path_cache AS category_path, full_name, users.username\nFROM images\nJOIN images_locale ON images_locale.image_id = images.id JOIN slugs ON images_locale.slug_id = slugs.id JOIN categories_locale ON images.category_id = categories_locale.category_id JOIN users ON users.id = images.user_id\nWHERE slugs.name = 'THE_SLUG_HERE' AND images.status = '1' AND images_locale.locale_id = 1 AND categories_locale.locale_id = 1\nLIMIT 1\n</code></pre>\n\n<p>Now when I remove <code>slugs.name = 'THE_SLUG_HERE' AND</code> I get the result in a few milliseconds.</p>\n\n<p>This is my slug table:</p>\n\n<pre><code>CREATE TABLE `slugs` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `name` varchar(250) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '',\n  `type` tinyint(4) NOT NULL,\n  `locale_id` smallint(6) NOT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `name` (`name`)\n) ENGINE=InnoDB AUTO_INCREMENT=3611900 DEFAULT CHARSET=utf8;\n</code></pre>\n\n<p>I tried to <code>CREATE INDEX test_speed ON slugs(name)</code> but it didn't speed up things.</p>\n\n<p>Please help.</p>\n\n<p><strong>EDIT:</strong></p>\n\n<p>Here are the result of EXPLAIN:</p>\n\n<p><img src=\"http://i.stack.imgur.com/DnUzd.png\" alt=\"result of &lt;code&gt;EXPLAIN&lt;/code&gt;\"></p>\n"},{"tags":["performance","node.js"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":126,"score":1,"question_id":12788143,"title":"Node.js suddenly getting extremely slow","body":"<p>We have this architecture with 2 node processes. </p>\n\n<p>One polls a private API and pushes the changes to the second node if any.\nThe second node process the data and calls a bunch other API's and eventually emits a change event to the client, a HTML5 website, with socket.io</p>\n\n<p>This second node will always process the data and will always emit changes even if no clients are connected. So in my opinion the CPU or mem usage is not that greatly affected by the number of connected clients. Also note that this architecture is still running on a private staging environment.</p>\n\n<p>Everything runs fine and we're ready to go live until we noticed after couple of days, maybe a week, the second node suddenly gets extremely slow while the first node is still fine.</p>\n\n<p>It gets so bad that even the connection between the two nodes gets timed out and they are on the same network over localhost. It also takes more then 10 seconds to browse to the socket.io/socket.io.js file.</p>\n\n<p>I know its very hard to understand the problem without seeing any code but I'm kinda pulling my hair out because we have to go live in couple of days and my logs are not revealing anything and google isn't helping either.</p>\n\n<p><strong>Whats a good practice towards building Have you ever experienced anything like this? What was the problem and how did you fix it?</strong></p>\n\n<p><strong>Whats a good monitor and profiler for node.js? (preferably free)</strong></p>\n\n<p><strong>What are good practices towards building a node.js app with makes a lot of outgoing API calls?</strong></p>\n\n<p>Anything or anyone that could help me in the right direction of solving or even discovering the actual problem will be greatly appreciated!</p>\n\n<p>Thank you!</p>\n"},{"tags":["java","performance","string-concatenation"],"answer_count":3,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":108,"score":3,"question_id":12786902,"title":"Performance: Java's String.format","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/513600/should-i-use-javas-string-format-if-performance-is-important\">Should I use Java&#39;s String.format() if performance is important?</a>  </p>\n</blockquote>\n\n\n\n<p>I was wondering if is good to use <code>String.format</code> in Java apps instead of <code>StringBuilder</code>... so, I just write a simple test, like this:</p>\n\n<pre><code>public static void main(String[] args) {\n        int i = 0;\n        Long start = System.currentTimeMillis();\n        while (i &lt; 10000) {\n            String s = String.format(\"test %d\", i);\n            i++;\n        }\n        System.out.println(System.currentTimeMillis() - start);\n        i = 0;\n        start = System.currentTimeMillis();\n        while (i &lt; 10000) {\n            String s = new StringBuilder().append(\"test \").append(i).toString();\n            i++;\n        }\n        System.out.println(System.currentTimeMillis() - start);\n    }\n</code></pre>\n\n<p>And the results where:</p>\n\n<pre><code>238\n15\n</code></pre>\n\n<p>So, if my test is valid, <code>StringBuilder</code> is faster than <code>String.format</code>. OK.\nNow, I start thinking how <code>String.format</code> works. Is it a simple String concatenation, like <code>\"test \" + i</code>?</p>\n\n<p>What the differences between <code>StringBuilder</code> concatenation and <code>String.format</code>? Is there a way simple as <code>String.format</code> and fast like <code>StringBuilder</code>? </p>\n"},{"tags":["c#","c++","performance","math"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":46,"score":0,"question_id":12718942,"title":"Theorical Mpx/s value depending of processor speed and RAM","body":"<p>My teacher wants me to evaluate the theorical value of Mpixel/second that a specific CPU and RAM can handle. We must compare that theorical value to the real value we get with two distinct C# and C++ projects while displaying any loaded video.</p>\n\n<p>I actually have no idea of how to calculate this, I'm stuck there. Any one as an idea?</p>\n"},{"tags":["performance","xpath","xquery","marklogic"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":55,"score":0,"question_id":12781301,"title":"In XQuery, how do I avoid deteriorating performance in a paged simple query?","body":"<p>I have a ML database with a few tens of thousands of documents in it, and a query that returns some simple calculated values for either all or a subset of those documents. The document count has grown to the point that the \"all documents\" option no longer reliably runs without timing out, and is only going to get worse as the document count grows. The obvious solution is for the client application to use the other form and paginate the results. It's an offline batch process, so overall speed isn't an issue - we'd just like to keep individual requests sane.</p>\n\n<p>The paged version of the query is very simple:</p>\n\n<pre><code>declare namespace ns = \"http://some.namespace/here\"\n\ndeclare variable $fromCount external;\ndeclare variable $toCount external;\n\n&lt;response&gt; {\n  for $doc in fn:doc()/ns:entity[$fromCount to $toCount]\n  return\n  &lt;doc&gt; omitted for brevity &lt;/doc&gt;\n} &lt;/response&gt;\n</code></pre>\n\n<p>The problem is that the query is slower the further through the document set the requested page is; presumably because it's having to load every document in order, check whether it's the right type and iterate until its found <code>$fromCount</code> <code>ns:entity</code>s before it even begins building the response.</p>\n\n<p>One wrinkle is that there are other types of document in the database, so just using fn:doc isn't a realistic option (although, they are in different directories, so <code>xdmp:directory()</code> might be an option; something I'll look into.)</p>\n\n<p>There also isn't currently an index on the <code>ns:entity</code> element; would that help? It's always the root-node of a document, and the documents are quite large, so I'm concerned about the size of the index. Also, (the slow part of) this query isn't interested in the <em>value</em> of the element, just that it exists.</p>\n\n<p>I thought about using the <code>search:</code> api for it's built-in paging, but it seems overkill for a query that is intended to match all documents; surely it's possible to manually construct the query that <code>search:search()</code> would build internally.</p>\n\n<p>It seems like what I really need is an efficient list of all root-nodes of a certain type in the database. Does Marklogic maintain such a thing? If Not would an index solve the problem?</p>\n\n<hr>\n\n<p><strong>Edit:</strong> It turns out that the answer in my case is use the <code>xdmp:directory()</code> option, since ML apparently stores a fast, in-memory list of all documents. Still, if there <em>is</em> a more general solution to the problem, it's bound to be of interest, so I'll leave the question here.</p>\n"},{"tags":["performance","image","divide"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":16,"score":0,"question_id":12785813,"title":"Displaying a big image - Performance wise, one big or a bunch of small ones?","body":"<p>I'm developing a site where the size of the image is 5x the window in height and width. I have experienced stuttering when scrolling over the image with IE &amp; Chrome.</p>\n\n<p>Is there any difference in how the image is loaded? Is the perfomance boosted if you divide the image into lets say 100 images and put them next to each other?</p>\n"},{"tags":["php","performance","exception-handling","try-catch"],"answer_count":7,"favorite_count":7,"up_vote_count":25,"down_vote_count":0,"view_count":12647,"score":25,"question_id":104329,"title":"Performance of try-catch in php","body":"<p>What kind of performance implications are there to consider when using try-catch statements in php 5? </p>\n\n<p>I've read some old and seemingly conflicting information on this subject on the web before. A lot of the framework I currently have to work with was created on php 4 and lacks many of the niceties of php 5. So, I don't have much experience myself in using try-catchs with php.</p>\n"},{"tags":["python","performance","list","dictionary"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":97,"score":1,"question_id":12543837,"title":"Python: iterating over list vs over dict items efficiency","body":"<p>Is iterating over <code>some_dict.items()</code> as efficient as iterating over a list of the same items in CPython?</p>\n"},{"tags":["python","performance","programming-competitions"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":76,"score":0,"question_id":12784192,"title":"Is there a faster way to get input in python?","body":"<p>In coding competitions we encounter inputs like-</p>\n\n<pre><code>2 3\n\n4 5\n</code></pre>\n\n<p>so we do the following</p>\n\n<pre><code>m, n = [int(x) for x in raw_input().split(' ')]\n</code></pre>\n\n<p>Is there a faster way of doing the same thing?</p>\n"},{"tags":["c++","performance","bitwise-operators","unions"],"answer_count":5,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":906,"score":3,"question_id":3486968,"title":"C++: Union vs Bitwise operators","body":"<p>I have two <code>char</code>'s and I want to \"stitch\" them bitwise together.<br>\nFor example:</p>\n\n<pre><code>char c1 = 11; // 0000 1011\nchar c2 = 5;  // 0000 0101\nshort int si = stitch(c1, c2); // 0000 1011 0000 0101\n</code></pre>\n\n<p>So, what I first tried was with bitwise operators:</p>\n\n<pre><code>short int stitch(char c1, char c2)\n{\n    return (c1 &lt;&lt; 8) | c2;\n}\n</code></pre>\n\n<p>But this doesn't work: I'm getting a <code>short</code> equals to <code>c2</code>... <strong>(1) Why?</strong><br>\n<em>(But: <code>c1</code> and <code>c2</code> are negative numbers in my real app... maybe this is a part of the problem?)</em></p>\n\n<p>So, my second solution was to use a <code>union</code>:</p>\n\n<pre><code>union stUnion\n{\n    struct\n    {\n         char c1;\n         char c2;\n    }\n    short int si;\n}\n\nshort int stitch(char c1, char c2)\n{\n    stUnion u;\n    u.c1 = c1;\n    u.c2 = c2;\n    return u.si;\n}\n</code></pre>\n\n<p>This works like I want... <sup><sub>I think</sub></sup></p>\n\n<p><strong>(2) What is the best/fastest way?</strong></p>\n\n<p>Thanks!</p>\n"},{"tags":["python","performance","reading","chunks"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":53,"score":1,"question_id":12783478,"title":"Python: Problems with islice to read N number of lines at a time","body":"<p>I am trying to use \"from itertools import islice\" in order to read a number of lines at a time from a *.las file using the liblas module. (my goal is reading chunk-bychunk)</p>\n\n<p>following the question: <a href=\"http://stackoverflow.com/questions/6335839/python-how-to-read-n-number-of-lines-at-a-time\">Python how to read N number of lines at a time</a></p>\n\n<blockquote>\n  <p>islice() can be used to get the next n items of an iterator. Thus,\n  list(islice(f, n)) will return a list of the next n lines of the file\n  f. Using this inside a loop will give you the file in chunks of n\n  lines. At the end of the file, the list might be shorter, and finally\n  the call will return an empty list.</p>\n</blockquote>\n\n<p>I used the the following code:</p>\n\n<pre><code>from numpy import nonzero\nfrom liblas import file as lasfile\nfrom itertools import islice\n\n\nchunkSize = 1000000\n\nf = lasfile.File(inFile,None,'r') # open LAS\nwhile True:\n    chunk = list(islice(f,chunkSize))\n    if not chunk:\n        break\n    # do other stuff\n</code></pre>\n\n<p>but i have this problem:</p>\n\n<pre><code>len(f)\n2866390\n\nchunk = list(islice(f, 1000000))\nlen(chunk)\n**1000000**\nchunk = list(islice(f, 1000000))\nlen(chunk)\n**1000000**\nchunk = list(islice(f, 1000000))\nlen(chunk)\n**866390**\nchunk = list(islice(f, 1000000))\nlen(chunk)\n**1000000**\n</code></pre>\n\n<p>when the file f arrives in the end the islice restart to read the file.</p>\n\n<p>Thanks for any suggestions and help. It's very appreciate</p>\n"},{"tags":["performance","java-ee","openejb","transport"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":54,"score":1,"question_id":11888161,"title":"What is the most performant and lighter transport for OpenEJB?","body":"<p>There are a few transports available for OpenEJB 4.0.0:</p>\n\n<ol>\n<li>ejbd</li>\n<li>ejbds</li>\n<li>httpejbd</li>\n</ol>\n\n<p>Which one is lighter on the network?</p>\n\n<p>Which one is faster?</p>\n\n<p>Are there any pros and cons of choosing any one of then?</p>\n\n<p>Our application as around 450 clients talking to remote EJBs on a OpenEJB 4.0.0 container. All in a local LAN (but using cascading switches with some redundancy).</p>\n\n<p>Update:</p>\n\n<p>This question is not related to another one on timeouts. We don't have any timeouts or application problems that we could identify. The application works very well when we have a limited number of clients, but when we try it with hundreds we are facing what seems to be network errors: the server logs show recurring \"IoExpcetion unkown byte received\". Since CORBA ORBs has been reported to have broadcast problems, we were suspecting it might be a RMI over IIOP kind of problem. We are going to try other protocol options to compare against our current set up.</p>\n\n<p>Update (2012-oct-08):</p>\n\n<p>we have run hundreds of tests now, with 450+ clients in a LAN. There is no one size fits all answer. If we have very few clients, EJBD is faster. If we have hundreds of clients, EJBD stops working (it seems to cause switch saturation). With hundreds of clients, httpejbd still works (it seems related to the fact that each remote call creates a short duration http request).</p>\n"},{"tags":["c#","wcf","performance","wcf-client"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":70,"score":2,"question_id":12779844,"title":"How do I handle WCF Call lifecycles under load when timeouts are expected?","body":"<p>I have a nice fast task scheduling component (windows service as it happens but this is irrelevant), it subscribes to an in memory queue of things to do. </p>\n\n<p>The queue is populated really fast ... and when I say fast I mean fast ... so fast that I'm experiencing problems with some particular part. </p>\n\n<p>Each item in the queue gets a \"category\" attached to it and then is passed to a WCf endpoint to be processed then saved in a remote db.</p>\n\n<p>This is presenting a bit of a problem. </p>\n\n<p>The \"queue\" can be processed in the millions of items per minute whereas the WCF endpoint will only realistically handle about 1000 to 1200 items per second and many of those are \"stacked\" in order to wait for a slot to dump them to the db.</p>\n\n<p>My WCF client is configured so that the call is fire and forget (deliberate) my problem is that when the call is made occasionally a timeout occurs and thats when the headaches begin.</p>\n\n<p>The thread just seems to stop after timeout no dropping in to my catch block nothing ... just sits there, whats even more confusing is that this is an intermittent thing, this only happens when the queue is dealing with extreme loads and the WCF endpoint is over taxed, and even in that scenario it's only about once a fortnight this happens. </p>\n\n<p>This code is constantly running on the server, round the clock 24/7.</p>\n\n<p>So ... my question ...\nHow can I identify the edge case that is causing my problem so that I can resolve it?</p>\n\n<p>Some extra info:</p>\n\n<p>The client calling the WCF endpoint seems to automatically \"throttle itself\" by the fact that i'm limiting the number of threads making calls, and the code hangs about until a call is considered complete (i'm thinking this is a http level thing as im not asking the service for a result of my method call).</p>\n\n<p>The db is talked to with EF which seems to never open more than a fixed number of connections to the db (quite a low number too which is cool) and the WCF endpoint from the call reception back seems super reliable. </p>\n\n<p>The problem seems to be coming off the queue processor to the WCf endpoint. </p>\n\n<p>The queue processor has a single instance of my WCF endpoint client which it reuses for all calls ... (is it good practice to rebuild this endpoint per call? - bear in mind number of calls here).</p>\n\n<p>Final note:</p>\n\n<p>It's a peculiar \"module\" of functionality, under heavy load for hours at a time it's stable, but for some reason this odd thing happens resulting in the whole lot just stopping and not recovering. The call is wrapped in a try catch, but seemingly even if the catch is hit (which isn't guaranteed) the code doesn't recover / drop out as expected ... it just hangs.</p>\n\n<p>Any ideas? </p>\n\n<p>Please let me know if there's anything else I can add to help resolve this. </p>\n\n<p>Edit 1:</p>\n\n<p>binding - basicHttpBinding</p>\n\n<p>error handling - no code written other than wrapping the WCF call in a try catch.</p>\n"},{"tags":["php","arrays","performance","dump","deallocate"],"answer_count":5,"favorite_count":0,"up_vote_count":0,"down_vote_count":4,"view_count":79,"score":-4,"question_id":12781823,"title":"PHP: How to dump/delete an array completely?","body":"<p>In my code I create a temporary array which i process, then i take a value from it and want to delete this array so it´s not allocated in the memory anymore and nothing points or references to it.\nHow may I achieve that my array is completely deleted if this is possible?</p>\n\n<p>And can I do this with a string also?</p>\n\n<p><strike><b>EDIT 1:</b><br>\nunset is not what I need because only the global variable is unset, I need to remove it completely from the memory. It´s not my goal to clear the variable, I want to DELETE the VARIABLE, not only it´s content. Unset removes only the content and the type and memory adress is kept.</strike></p>\n\n<p><b>EDIT 2:</b><br>\nI was told that the memory is set free when the variable content is unset with unset() so I should use unset()</p>\n"},{"tags":["visual-studio-2010","performance","c#-4.0","optimization","compiler-optimization"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":81,"score":1,"question_id":12778067,"title":"Visual Studio performance optimization in branching","body":"<p>Consider the following</p>\n\n<pre><code>while(true)\n{\n    if(x&gt;5)\n     // Run function A\n    else\n     // Run function B\n}\n</code></pre>\n\n<p>if x is always less than 5, does visual studio compiler do any optimization? i.e. like never checks if x is larger than 5 and always run function B</p>\n"},{"tags":["performance","mongodb","schema","compare"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":59,"score":1,"question_id":12777515,"title":"Which schema would be good regarding performance","body":"<p>I am creating schema for MongoDB, I have two type of schema for Gender collection. I want to know which one would be better for Reading data (collection may contains millions record so I couldn't evaluate practically right now so please don't \"say try on sample data\")</p>\n\n<p>one is array type : </p>\n\n<pre><code>\"gender\": [{\n            \"GenderType\":M/F/U,\n            \"total:logins\": 12,\n            \"totaluser\": 20,\n            \"newuser\": 11\n     },\n     {\n           \"GenderType\":M/F/U,\n           \"total:logins\": 12,\n           \"totaluser\": 20,\n           \"newuser\": 11\n     },\n     {\n          \"GenderType\":M/F/U,\n          \"total:logins\": 12,\n          \"totaluser\": 20,\n          \"newuser\": 11\n     }\n]\n</code></pre>\n\n<p>second is object type.</p>\n\n<pre><code>\"gender\": {\n     \"male\": {\n          \"total:logins\": 12,\n          \"totaluser\": 20,\n          \"newuser\": 11\n     },\n     \"female\": {\n          \"total:logins\": 11,\n          \"totaluser\": 10,\n          \"newuser\": 10\n     }\n     \"unknown\": {\n          \"total:logins\": 11,\n          \"totaluser\": 10,\n          \"newuser\": 10\n     }\n}\n</code></pre>\n\n<p>Priorities </p>\n\n<ol>\n<li>Performance</li>\n<li>Storage</li>\n<li>Maintainability  </li>\n</ol>\n"},{"tags":["c#","performance","reflection"],"answer_count":7,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":353,"score":4,"question_id":8846948,"title":"Is Reflection really slow?","body":"<p>This is a common belief that reflection is slow and try to avoid it as much as possible. But is that belief true, in the current situation? There has been lot of changes in the current .net versions like, use of IL Weaving (i.e. IL Emit) etc, as opposed to traditional PropertyInfo and MethodInfo ways of performing reflection.</p>\n\n<p>Is that any convincing proof, that the new reflection is not that slow any more, and can be used. Is there better way to read attribute data?</p>\n\n<p>Thanks,\nBhaskar</p>\n"},{"tags":["java","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":4,"view_count":34,"score":-4,"question_id":12780834,"title":"What are the steps mainly followed in performance tuning using java for SCRATCH project?","body":"<p>We are developing the Scratch project. That project we use the java with the struts 1.2.9 framework. So we need some briefly about the performance tuning in java side not the database side.</p>\n\n<p>I know some performance tuning that is most of the time we use the string immutable object. So I replace that using the stringbuilder instead of string. It also improves our code quality and performances also. Can you anybody describes about that or any link is enough for me. I need the answers as per the architect level. Because I give these tips to our architect.Thanks.</p>\n"},{"tags":["ios","performance","ios6","uicollectionview"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":835,"score":2,"question_id":12563997,"title":"UICollectionView Performance Problems on performBatchUpdates","body":"<p>We are trying to set up a UICollectionView with a custom layout. The content of each CollectionViewCell will be an image. Over all there will be several thousand images and about 140-150 being visible at one certain time. On an action event potentially all cells will be reorganized in position and size. The goal is to animate all the moving events currently using the performBatchUpdates method. This causes a huge delay time before everything gets animated.</p>\n\n<p>This far we found out that internally the method layoutAttributesForItemAtIndexPath is called for every single cell (several thousand in total). Additionally, the method cellForItemAtIndexPath is called for more cells than can actually be displayed on the screen.</p>\n\n<p>Are there any possibilities to enhance the performance of the animation?</p>\n\n<hr>\n\n<p>The default UICollectionViewFlowLayout can't really offer the kind of design we want to realize in the app. Here's some of our code:</p>\n\n<pre><code>-(UICollectionViewCell *)collectionView:(UICollectionView *)collectionView cellForItemAtIndexPath:(NSIndexPath *)indexPath {\nRPDataModel *dm = [RPDataModel sharedInstance]; //Singleton holding some global information\nNSArray *plistArray = dm.plistArray; //Array containing the contents of the cells\nNSDictionary *dic = plistArray[[indexPath item]];\nRPCell *cell = [collectionView dequeueReusableCellWithReuseIdentifier:@\"CELL\" forIndexPath:indexPath];\ncell.label.text = [NSString stringWithFormat:@\"%@\",dic[@\"name\"]];\ncell.layer.borderColor = nil;\ncell.layer.borderWidth = 0.0f;\n[cell loadAndSetImageInBackgroundWithLocalFilePath:dic[@\"path\"]]; //custom method realizing asynchronous loading of the image inside of each cell\nreturn cell;\n}\n</code></pre>\n\n<p>The layoutAttributesForElementsInRect iterates over all elements setting layoutAttributes for allthe elements within the rect. The for-statement breaks on the first cell being past the borders defined by the bottom-right corner of the rect:</p>\n\n<pre><code>-(NSArray*)layoutAttributesForElementsInRect:(CGRect)rect {\nNSMutableArray* attributes = [NSMutableArray array];\nRPDataModel *dm = [RPDataModel sharedInstance];\nfor (int i = 0; i &lt; dm.cellCount; i++) {\n    CGRect cellRect = [self.rp getCell:i]; //self.rp = custom object offering methods to get information about cells; the getCell method returns the rect of a single cell\n    if (CGRectIntersectsRect(rect, cellRect)) {\n        NSIndexPath *indexPath = [NSIndexPath indexPathForItem:[dm.relevanceArray[i][@\"product\"] intValue] inSection:0];\n        UICollectionViewLayoutAttributes *attribute = [UICollectionViewLayoutAttributes layoutAttributesForCellWithIndexPath:indexPath];\n        attribute.size = cellRect.size;\n        attribute.center = CGPointMake(cellRect.origin.x + attribute.size.width / 2, cellRect.origin.y + attribute.size.height / 2);\n        [attributes addObject:attribute];\n    } else if (cellRect.origin.x &gt; rect.origin.x + rect.size.width &amp;&amp; cellRect.origin.y &gt; rect.origin.y + rect.size.height) {\n        break;\n    }\n}\nreturn attributes;\n}\n</code></pre>\n\n<p>On Layout changes the results are pretty much the same no matter if the number of cells being defined in the layoutAttributesForElementsInRect is limited or not .. Either the system gets the layout attributes for all cells in there if it isn't limited or it calls the layoutAttributesForElementAtIndexPath method for all the missing cells if it is limited. Overall the attributes for every single cell is being used somehow.</p>\n\n<pre><code>-(UICollectionViewLayoutAttributes*)layoutAttributesForItemAtIndexPath:(NSIndexPath *)indexPath {\nRPDataModel *dm = [RPDataModel sharedInstance];\nUICollectionViewLayoutAttributes *attribute = [UICollectionViewLayoutAttributes layoutAttributesForCellWithIndexPath:indexPath];\nCGRect cellRect = [self.rp getCell:[dm.indexDictionary[@(indexPath.item)] intValue]];\nattribute.size = cellRect.size;\nattribute.center = CGPointMake(cellRect.origin.x + attribute.size.width / 2, cellRect.origin.y + attribute.size.height / 2);\nreturn attribute;\n}    \n</code></pre>\n"},{"tags":["java","performance","swing","border","jtextpane"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":81,"score":1,"question_id":12778275,"title":"java swing setBorder speed issue","body":"<p>i have a problem with the \"speed\" of setting up a border. i have a display with multiple JTextPane´s (around 450, what is required) which are updated quite often(depending on user input). \nhere is the setting border function:</p>\n\n<pre><code>    private void setBorder(int top, int left, int bottom, int right, Color color)\n    {\n        Args.checkForNull(color);\n        this.setBorder(BorderFactory.createMatteBorder(top, left, bottom, right, color));\n    }\n</code></pre>\n\n<p>can you give me some tipps on, how to improve the speed of the border changing??\ni mean this part:</p>\n\n<pre><code>this.setBorder(BorderFactory.createMatteBorder(top, left, bottom, right, color));\n</code></pre>\n\n<p>something like:</p>\n\n<pre><code>tmp = this.getStyledDocument();\n        this.setDocument(blank);\n        if(onOff){\n            tmp.setParagraphAttributes(0, tmp.getLength(), underlinedAttr, false);\n        }\n        else{\n            tmp.setParagraphAttributes(0, tmp.getLength(), notUnderlinedAttr, false);\n        }\n\n        this.setDocument(tmp);\n</code></pre>\n\n<p>thanks!</p>\n"},{"tags":["performance","actionscript-3","compare","bitmapdata"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":76,"score":1,"question_id":12778639,"title":"AS3 - Comparing BitmapDatas, fastest method?","body":"<p>Right, so let's say I want to compare two BitmapDatas. One is an image of a background (not solid, it has varying pixels), and another is of something (like a sprite) on top of the exact same background. Now what I want to do is remove the background from the second image, by comparing the two images, and removing all the pixels from the background that are present in the second image.\nFor clarity, basically I want to do <a href=\"http://graphicdesign.stackexchange.com/questions/1511/remove-identical-pixels-in-two-layers-images-with-free-software\">this</a> in AS3.</p>\n\n<p>Now I came up with two ways to do this, and they both work perfectly. One compares pixels directly, while the other uses the <code>BitmapData.compare()</code> method first, then copies the appropriate pixels into the result. What I want to know is which way is faster.</p>\n\n<p>Here are my two ways of doing it:</p>\n\n<p><strong>Method 1</strong></p>\n\n<pre><code>for (var j:int = 0; j &lt; layer1.height; j++)\n{\n    for (var i:int = 0; i &lt; layer1.width; i++)\n    {\n        if (layer1.getPixel32(i, j) != layer2.getPixel32(i, j))\n        {\n            result.setPixel32(i, j, layer2.getPixel32(i, j));\n        }\n    }\n}\n</code></pre>\n\n<p><strong>Method 2</strong></p>\n\n<pre><code>result = layer1.compare(layer2) as BitmapData;\n\nfor (var j:int = 0; j &lt; layer1.height; j++)\n{\n    for (var i:int = 0; i &lt; layer1.width; i++)\n    {\n        if (result.getPixel32(i, j) != 0x00000000)\n        {\n            result.setPixel32(i, j, layer2.getPixel32(i, j));\n        }\n    }\n}\n</code></pre>\n\n<p><code>layer1</code> is the background, <code>layer2</code> is the image the background will be removed from, and <code>result</code> is just a <code>BitmapData</code> that the result will come out on.</p>\n\n<p>These are very similar, and personally I haven't noticed any difference in speed, but I was just wondering if anybody knows which would be faster. I'll probably use Method 1 either way, since <code>BitmapData.compare()</code> doesn't compare pixel alpha unless the colours are identical, but I still thought it wouldn't hurt to ask.</p>\n"},{"tags":["c++","c","performance","io"],"answer_count":10,"favorite_count":1,"up_vote_count":11,"down_vote_count":0,"view_count":4970,"score":11,"question_id":1042110,"title":"Using scanf() in C++ programs is faster than using cin?","body":"<p>I don't know if this is true, but when I was reading FAQ on one of the problem providing sites, I found something, that poke my attention:</p>\n\n<blockquote>\n  <p>Check your input/output methods. In C++, using cin and cout is too slow. Use these, and you will guarantee not being able to solve any problem with a decent amount of input or output. Use printf and scanf instead.</p>\n</blockquote>\n\n<p>Can someone please clarify this? Is really using <em>scanf()</em> in C++ programs faster than using <em>cin >> something</em> ? If yes, that is it a good practice to use it in C++ programs? I thought that it was C specific, though I am just learning C++...</p>\n"},{"tags":["java","math","matrix","performance"],"answer_count":20,"favorite_count":29,"up_vote_count":54,"down_vote_count":0,"view_count":22951,"score":54,"question_id":529457,"title":"Performance of Java matrix math libraries?","body":"<p>We are computing something whose runtime is bound by matrix operations.  (Some details below if interested.) This experience prompted the following question:</p>\n\n<p>Do folk have experience with the performance of Java libraries for matrix math (e.g., multiply, inverse, etc.)?  For example:</p>\n\n<ul>\n<li>JAMA: <a href=\"http://math.nist.gov/javanumerics/jama/\" rel=\"nofollow\">http://math.nist.gov/javanumerics/jama/</a></li>\n<li>COLT: <a href=\"http://acs.lbl.gov/~hoschek/colt/\" rel=\"nofollow\">http://acs.lbl.gov/~hoschek/colt/</a></li>\n<li>Apache commons math: <a href=\"http://commons.apache.org/math/\" rel=\"nofollow\">http://commons.apache.org/math/</a></li>\n</ul>\n\n<p>I searched and found nothing.</p>\n\n<p><hr /></p>\n\n<p>Details of our speed comparison:</p>\n\n<p>We are using Intel FORTRAN (ifort (IFORT) 10.1 20070913).  We have reimplemented it in Java (1.6) using Apache commons math 1.2 matrix ops, and it agrees to all of its digits of accuracy.  (We have reasons for wanting it in Java.)  (Java doubles, Fortran real*8).  Fortran: 6 minutes, Java 33 minutes, same machine.  jvisualm profiling shows much time spent in RealMatrixImpl.{getEntry,isValidCoordinate} (which appear to be gone in unreleased Apache commons math 2.0, but 2.0 is no faster).  Fortran is using Atlas BLAS routines (dpotrf, etc.).</p>\n\n<p>Obviously this could depend on our code in each language, but we believe most of the time is in equivalent matrix operations.</p>\n\n<p>In several other computations that do not involve libraries, Java has not been much slower, and sometimes much faster.</p>\n"},{"tags":["java","performance"],"answer_count":8,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":77,"score":2,"question_id":12776307,"title":"Multiple && Statements - Efficiency","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/6352139/does-java-evaluate-remaining-conditions-after-boolean-result-is-known\">Does Java evaluate remaining conditions after boolean result is known?</a>  </p>\n</blockquote>\n\n\n\n<p>When executing code with multiple <code>&amp;&amp;</code> statements in java, does the compiler continue to resolve additional boolean comparisons if the first one resolves to false?</p>\n\n<pre><code>if (1==2 &amp;&amp; 3==4 &amp;&amp; 4==5) { Do Something }\n</code></pre>\n\n<p>Once determining <code>1==2</code> is false, will the compiler immediately break out of the if statement or will it continue to resolve <code>3==4</code> and <code>4==5</code> after?</p>\n"},{"tags":["objective-c","ios","performance"],"answer_count":2,"favorite_count":3,"up_vote_count":4,"down_vote_count":0,"view_count":1275,"score":4,"question_id":6592073,"title":"Speed up saving image - iOS","body":"<p>I am working on more of mini-project that will be later included into a new project. It basically a test unit.</p>\n\n<p>What I am doing is creating an AVCaptureSession and then creating a method for OutputSampleBufferDelegate. In the method, I convert the sampleBuffer into a UIImage and save the UIImage. When I run the application on my iPhone 4 it can only save 2-3 images per second. There must be a more efficient way to save the image. </p>\n\n<p>Can someone help me speed it up?</p>\n\n<p>Thanks!</p>\n\n<p><a href=\"http://developer.apple.com/library/ios/#qa/qa1702/_index.html\" rel=\"nofollow\">lots of the code is from here</a></p>\n\n<pre><code>- (void)captureOutput:(AVCaptureOutput *)captureOutput didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection {  \n\nUIImage *resultUIImage = [self imageFromSampleBuffer:sampleBuffer];\n\nNSData *imageData = [NSData dataWithData:UIImagePNGRepresentation(resultUIImage)];\n\nNSArray *paths = NSSearchPathForDirectoriesInDomains(NSDocumentDirectory, NSUserDomainMask, YES);\nNSString *path = [paths objectAtIndex:0];\n\n\nCMTime frameTime = CMSampleBufferGetPresentationTimeStamp(sampleBuffer);\n\nNSString *filename =  [NSString stringWithFormat:@\"%f.png\", CMTimeGetSeconds(frameTime)];\n\nNSString *finalPath = [path stringByAppendingString:filename];\n\n[imageData writeToFile:finalPath atomically:YES];\n}\n\n// Create a UIImage from sample buffer data\n- (UIImage *) imageFromSampleBuffer:(CMSampleBufferRef) sampleBuffer \n{\n// Get a CMSampleBuffer's Core Video image buffer for the media data\nCVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer); \n// Lock the base address of the pixel buffer\nCVPixelBufferLockBaseAddress(imageBuffer, 0); \n\n// Get the number of bytes per row for the pixel buffer\nvoid *baseAddress = CVPixelBufferGetBaseAddress(imageBuffer); \n\n// Get the number of bytes per row for the pixel buffer\nsize_t bytesPerRow = CVPixelBufferGetBytesPerRow(imageBuffer); \n// Get the pixel buffer width and height\nsize_t width = CVPixelBufferGetWidth(imageBuffer); \nsize_t height = CVPixelBufferGetHeight(imageBuffer); \n\n// Create a device-dependent RGB color space\nCGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB(); \n\n// Create a bitmap graphics context with the sample buffer data\nCGContextRef context = CGBitmapContextCreate(baseAddress, width, height, 8, \n                                             bytesPerRow, colorSpace, kCGBitmapByteOrder32Little | kCGImageAlphaPremultipliedFirst); \n// Create a Quartz image from the pixel data in the bitmap graphics context\nCGImageRef quartzImage = CGBitmapContextCreateImage(context); \n// Unlock the pixel buffer\nCVPixelBufferUnlockBaseAddress(imageBuffer,0);\n\n// Free up the context and color space\nCGContextRelease(context); \nCGColorSpaceRelease(colorSpace);\n\n// Create an image object from the Quartz image\nUIImage *image = [UIImage imageWithCGImage:quartzImage];\n\n// Release the Quartz image\nCGImageRelease(quartzImage);\n\nreturn (image);\n}\n</code></pre>\n"},{"tags":["performance","network-programming","operating-system","udp","remote-desktop"],"answer_count":3,"favorite_count":6,"up_vote_count":15,"down_vote_count":0,"view_count":8259,"score":15,"question_id":9498877,"title":"How is TeamViewer so fast?","body":"<p>Sorry about the length, it's kinda necessary.</p>\n\n<p><strong>Introduction</strong></p>\n\n<p>I'm developing a remote desktop software (just for fun) in C# 4.0 for Windows Vista/7. I've gotten through basic obstacles: I have a robust UDP messaging system, relatively clean program design, I've got a mirror driver (the free DFMirage mirror driver from DemoForge) up and running, and I've implemented NAT traversal for all NAT types except Symmetric NATs (present in corporate firewall situations).</p>\n\n<p>Regarding screen transfer/sharing, thanks to the mirror driver, I'm automatically notified of changed screen regions and I can simply marshal the mirror driver's ever-changing screen bitmap to my own bitmap. Then I compress the screen region as a PNG and send it off from the server to my client. Things are looking pretty good, but it's not fast enough. It's just as slow as VNC (btw, I don't use the VNC protocol, just a custom amateur protocol).</p>\n\n<p>From the slowest remote desktop software to the fastest, the list usually begins at all VNC-like implementations, then climbs up to Microsoft Windows Remote Desktop...and then...TeamViewer. Not quite sure about CrossLoop, LogMeIn - I haven't used them, but TeamViewer is <em>insanely</em> fast. It's quite literally live. I ran a <code>tree</code> command on Command Prompt and it updated with 20 ms delay. I can browse the web just a few milliseconds slower than on my laptop. Scrolling code vertically in Visual Studio has 50 ms lag time. Think about how robust TeamViewer's screen-transfer solution must be to accomplish all this.</p>\n\n<p>VNCs use poll-based hooks for detecting screen change and brute force screen capturing/comparing at their worst. At their best, they use a mirror driver like DFMirage. I'm at this level. And they use something called the RFB protocol.</p>\n\n<p>Microsoft Windows Remote Desktop apparently goes one step higher than VNC. I heard, from somewhere on StackOverflow, that Windows Remote Desktop doesn't send screen bitmaps, but actual drawing commands. That's quite brilliant, because it can just send simple text (draw this rectangle at this coordinate and color it with this gradient)! Remote Desktop really is pretty fast - and it's the standard way of working from home. And it uses something called the RDP protocol.</p>\n\n<p>Now TeamViewer is a complete mystery to me. Apparently, they released their source code for Version 2 (TeamViewer is Version 7 as of February 2012). People have read it and said that Version 2 is useless - that it's just a few improvements over VNC with automatic NAT traversal. </p>\n\n<p>But Version 7...it's ridiculously fast now. I mean, it's actually faster than Windows Remote Desktop. I've streamed DirectX 3D games with TeamViewer (at 1 fps, but Windows Remote Desktop doesn't even allow DirectX to run).</p>\n\n<p>By the way, TeamViewer does all this <em>without</em> a mirror driver. There is an option to install one, and it gets just a bit faster.</p>\n\n<p><strong>The Question</strong></p>\n\n<p><strong>My question is, how is TeamViewer so fast?</strong> It must not be possible. If you've got 1920 by 1080 resolution at even 24 bit depth (16 bit depth would be noticeably ugly), thats still 6,220,800 bytes raw. Even using libjpeg-turbo (one of the fastest JPG compression libraries used by large corporations), compressing it down to 30KB (let's be extremely generous), would take time to route through TeamViewer's servers (TeamViewer bypasses corporate Symmetric NATs by simply proxying traffic through their servers). And that libjpeg-turbo compression would take time to compress. High-quality JPG compression takes 175 milliseconds for a full 1920 by 1080 screenshot for me. And that number goes up if the host's computer runs an Atom processor. I simply don't understand how TeamViewer has optimized their screen transfer so well. Again, small-size images might be highly compressed, but take at least tens of milliseconds to compress. Large-size images take no time to compress, but take a long time to get through. Somehow, TeamViewer completes this entire process to get roughly 20-25 frames per second. I've used a network monitor, and TeamViewer is still lagless at speeds of 500 Kbps and 1 Mbps (VNC software lag for a few seconds at that transfer rate). During my <code>tree</code> Command Prompt test, TeamViewer was receiving inbound data at a rate of 1 Mbps and still running 5-6 fps. VNC and remote desktop don't do that. So, how?</p>\n\n<p>The answers will be somewhat complicated and intricate, so <em>please don't post your $0.02 if you're only going to say it's because they use UDP instead of TCP</em> (would you believe they actually do use TCP just as successfully though).</p>\n\n<p>I'm hoping there's a TeamViewer developer somewhere here on StackOverflow.</p>\n\n<p><strong>Potential Answers</strong></p>\n\n<p>Will update this once people reply.</p>\n\n<ol>\n<li>My thoughts are, first of all, that TeamViewer has very fine network control. For example, they split large packets to just under the MTU size and never waste a trip. They probably have all sorts of fancy hooks to detect screen changes along with extremely fast XOR image comparisons.</li>\n</ol>\n"},{"tags":["c","performance","optimization"],"answer_count":5,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":94,"score":2,"question_id":12776189,"title":"How fast is strn*() compared to str*()?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/1258550/why-should-you-use-strncpy-instead-of-strcpy\">Why should you use strncpy instead of strcpy?</a>  </p>\n</blockquote>\n\n\n\n<p>I'm reading a book about computers/cryptographic etc. And often the writer use thing such as <code>strncpy(dest, src, strlen(src));</code> instead of <code>strcpy(dest, src);</code> it makes no much sense for me.. Well, I'm a professional programmer.</p>\n\n<p>The question is: It make really a real difference? real applications use something like this?</p>\n"},{"tags":["java","mysql","performance","algorithm","complexity"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":4,"view_count":74,"score":-3,"question_id":12776364,"title":"What is the best algorithm for identifying the best Influencer for Social media and Traditional Media?","body":"<p>Is there any best algorithm/method to find the Influenced person in Traditional and Social Media? I am having more than 10 million person in my database. How to calculate the influencers for all these people everyday?</p>\n"},{"tags":["java","performance","algorithm"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":163,"score":-1,"question_id":12774823,"title":"Determine if array contains two elements which equal a certain sum?","body":"<pre><code>// Checks whether the array contains two elements whose sum is s.\n// Input: A list of numbers and an integer s\n// Output: return True if the answer is yes, else return False\n\npublic static boolean calvalue (int[] numbers, int s){\nfor (int i=0; i&lt; numbers.length; i++){\n    for (int j=i+1; j&lt;numbers.length;j++){\n        if (numbers[i] &lt; s){\n            if (numbers[i]+numbers[j] == s){\n                return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n</code></pre>\n"},{"tags":["c#","performance","upload","webclient"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":156,"score":1,"question_id":11719804,"title":"C# WebClient upload speeds","body":"<p>I was wondering if it is possible to increase buffer size on WebClient Async data upload, because currently it pushes ~320kB/s maximum.</p>\n\n<p>My current code:</p>\n\n<pre><code>using (WebClient Client = new WebClient())\n{\n    byte[] Buffer = File.ReadAllBytes(this.WorkItem.FileLocation);\n\n    Client.UploadProgressChanged += new UploadProgressChangedEventHandler(Client_UploadProgressChanged);\n    Client.UploadDataCompleted += new UploadDataCompletedEventHandler(Client_UploadDataCompleted);\n    Client.UploadDataAsync(new Uri(\"-snip-\"), Buffer);\n}\n</code></pre>\n\n<p><strong>Edit</strong><br>\nConnection is not the limiting factor. ( its 300mbit connection, web-servers push content at ~30-40mB/s mark )</p>\n"},{"tags":["mysql","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":5,"view_count":33,"score":-5,"question_id":12775138,"title":"Good reference for my sql with practical scenarios, problems (especially performance) and solutions","body":"<p>There are many books out there for mysql learning. My concern/question is that is there any book out there which discuss real life scenarios, problems (especially performance related) and their solutions. Fake company names and db names are acceptable but scenarios, problems and solution should be practical.</p>\n"},{"tags":["performance","image","xaml","winrt","winrt-xaml"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":147,"score":1,"question_id":12698757,"title":"How to improve XAML WinRT image changing speed?","body":"<p>Suppose I have a variable number of static image resources that I would like to show rapidly.</p>\n\n<p>So I have an INotifyPropertyChanged class with </p>\n\n<pre><code>for (blah)\n    bitMaps.Add(new BitmapImage(new Uri(blah + i + \".png\")));\n</code></pre>\n\n<p>in a list and a property to get the images</p>\n\n<pre><code>public ImageSource Image\n{\n    get\n    {\n        return bitMaps[Index];\n    }\n}\n</code></pre>\n\n<p>along with</p>\n\n<pre><code>public int Index\n{\n    set\n    {\n        _Index = value;\n        OnPropertyChanged(\"Image\");\n    }\n}\n</code></pre>\n\n<p>so that I can change the displayed image by changing Index. </p>\n\n<p>I then have a bound Image tag <code>&lt;Image Source=\"{Binding Image}\" Stretch=\"None\" /&gt;</code> to display it. </p>\n\n<p>But despite seemingly loading the bitmap images initially, the transitions when changing Index still flickers on the first time leading me to thing that the BitmapImage are lazy loading. Is there a better/more performant way of doing this? Or how can I pre-load the images properly?</p>\n"},{"tags":["sql","sql-server","performance"],"answer_count":6,"favorite_count":2,"up_vote_count":7,"down_vote_count":0,"view_count":4339,"score":7,"question_id":921282,"title":"Compare performance difference of T-SQL Between and '<' '>' operator?","body":"<p>I've tried searching through search engines,MSDN,etc. but can't anything. Sorry if this has been asked before. Is there any performance difference between using the T-SQL \"Between\" keyword or using comparison operators?</p>\n"},{"tags":["objective-c","c","ios","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":3,"down_vote_count":2,"view_count":516,"score":1,"question_id":7370955,"title":"language speeds c vs objective c","body":"<p>Ive been wondering, how much faster does c run than objective c? From what I understand c does run faster. I recently implemented a maths function in my app (written in standard c) in the hope that it would increase speed but does it really have that much of an effect?</p>\n\n<p>cheers GC</p>\n"},{"tags":["mysql","performance","optimization","database-schema","tagging"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":56,"score":0,"question_id":12769077,"title":"How to create tagging system using PHP & MySQL (Performance)?","body":"<p>How to create many-to-many tagging system using PHP &amp; MySQL (With good perfomance in a big size DB)?\n(I mean to database schema...)</p>\n"},{"tags":["c","performance","algorithm","search","counting"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":61,"score":0,"question_id":12769224,"title":"traversing numbers in an interval wisely","body":"<p>I want to scan the numbers in a big interval wisely until I find the one I need.\nBut, I don't have any clue where this number might be and I will not have any clue during searching process.</p>\n\n<p>Let me give an example to make it easy to state my question</p>\n\n<p>Assume I am searching a number between 100000000000000 and 999999999999999</p>\n\n<p>Naive approach would be starting from 100000000000000 and counting to 99... one by one.\nbut this is not wise because number can be on the far end If I am not lucky.</p>\n\n<p>so, what is the best approach to this problem. I am not looking for mathematically best, I need a technique which is easy to implement in C programming Language.</p>\n\n<p>thanks in advance.</p>\n"},{"tags":["python","multithreading","performance","web","crawl"],"answer_count":0,"favorite_count":1,"up_vote_count":0,"down_vote_count":1,"view_count":82,"score":-1,"question_id":12772464,"title":"Error with Threading (Python)","body":"<p>I'm getting Threading errors, obviously because I reached the threading limit. (I thought it does well untill 1000 threads). Only whenever I build some kind of xrange (for i in xrange (0, 20000, 900: thread etc..) I get these errors as well. Does anyone know how to fix this? </p>\n\n<p>I love the speed it does right now, 900 in +- 1 sec I believe, only with that xrange thing it gets a lot slower. Anyone has tips?</p>\n\n<pre><code>import urllib\nimport datetime\nimport threading\nimport urllib2\n\nprint \"[SCAN] Started: \"+str(datetime.datetime.now())\n\nvar_WebsiteList = [\"http://nl.wikipedia.org/wiki/Website\", \"http://nl.wikipedia.org/wiki/Website\", \"http://nl.wikipedia.org/wiki/Website\"]\nclass k_Threads(threading.Thread):\n    def run(self):\n        var_HTML = urllib2.urlopen(var_WebsiteList[int(self.getName().split(\"-\")[1]))\n        var_HTML.close()\n\ndef k_Start():\n    for email in var_WebsiteList:\n        k_Thread = k_Threads()\n        k_Thread.start()\nk_Start()\nprint \"[SCAN] Stopped: \"+str(datetime.datetime.now())\n</code></pre>\n\n<p>Only in this example the website list is thousend times bigger... Anyone who could help me?</p>\n\n<p>Error: <strong>error: can't start new thread</strong></p>\n\n<p><strong>Edit: Code with the xrange - catches the error, but speed has decreased a lot!</strong></p>\n\n<pre><code>import urllib\nimport datetime\nimport threading\nimport urllib2\n\nprint \"[SCAN] Started: \"+str(datetime.datetime.now())\n\nvar_WebsiteList = [\"http://nl.wikipedia.org/wiki/Website\", \"http://nl.wikipedia.org/wiki/Website\", \"http://nl.wikipedia.org/wiki/Website\"]\nclass k_Threads(threading.Thread):\n    def run(self):\n        var_HTML = urllib2.urlopen(var_WebsiteList[int(self.getName().split(\"-\")[1]))\n        var_HTML.close()\n\ndef k_Threading():\n    try:\n        k_Thread = k_Threads()\n        k_Thread.start()\n    except:\n        time.sleep(1)\n        k_Threading()\n\ndef k_Start():\n    for Range in xrange(0, len(var_WebsiteList), 500):\n        for email in var_WebsiteList[Range:Range+500]:\n            k_Threading()\nk_Start()\nprint \"[SCAN] Stopped: \"+str(datetime.datetime.now())\n</code></pre>\n"},{"tags":["mysql","performance","phpmyadmin"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":127,"score":-1,"question_id":12770713,"title":"phpMyAdmin with large InnoDB tables is very slow on \"SHOW TABLE STATUS FROM 'mydb'\"","body":"<p>After logging into phpMyAdmin i see the list of databases on the server fairly quickly, however after clicking on the database in question it spins for over 30s before displaying the table list.</p>\n\n<p>Also, when running a query against one of these large tables similar things happen, but i believe on SHOW INDEXES.  If i run the same query in the mysql console window, the query is lightning fast.</p>\n\n<p>Side note, there are several partitioned tables in this database, not sure if that has anything to do with the slow down...</p>\n\n<p>Experiencing this behavior when running against MariaDB 5.1.x and phpMyAdmin 3.5.2</p>\n\n<p>Is there some specific phpMyAdmin or MySQL (MariaDB in my case) parameters that can be tweaked to improve this performance?</p>\n"},{"tags":["python","performance","rsa"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":65,"score":0,"question_id":12771104,"title":"Computation time for RSA?","body":"<p>I'm quite new to programming and had done a Cryptography course on Coursera before I stared learning Python. Recently, as a project, I wanted to write my own code for the RSA algorithm. I have just finished writing the encryption process which is as such:</p>\n\n<p><img src=\"http://i.stack.imgur.com/PrTkF.png\" alt=\"enter image description here\"></p>\n\n<p>However, the program is running now and is taking a long time. I did notice, it took a long time for the keys and modulos to compute because of the sheer size. Because I am new to all of this, I don't know enough and was wondering if there was any way to speed up the process? </p>\n\n<p>If my code is required to be posted, I can do it however I would prefer a more general answer on how to speed up code. </p>\n\n<p>Thanks </p>\n"},{"tags":["c++","floating-point","performance"],"answer_count":7,"favorite_count":0,"up_vote_count":10,"down_vote_count":0,"view_count":3308,"score":10,"question_id":2487653,"title":"Avoiding denormal values in C++","body":"<p>After searching a long time for a performance bug, I read about denormal floating point values.</p>\n\n<p>Apparently denormalized floating-point values can be a major performance concern as is illustrated in this question:\n<a href=\"http://stackoverflow.com/questions/9314534/why-does-changing-0-1f-to-0-slow-down-performance-by-10x\">Why does changing 0.1f to 0 slow down performance by 10x?</a></p>\n\n<p>I have an Intel Core 2 Duo and I am compiling with gcc, using <code>-O2</code>.</p>\n\n<p>So what do I do? Can I somehow instruct g++ to avoid denormal values?\nIf not, can I somehow test if a <code>float</code> is denormal?</p>\n"},{"tags":["css","performance","google-chrome","webkit"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":3,"view_count":45,"score":-3,"question_id":12770355,"title":"Chrome forcing sidebar down on slow website","body":"<p>I have developed a website: <a href=\"http://dannalab.com\" rel=\"nofollow\">http://dannalab.com</a> which <em>was</em> working fine. But now in Chrome the sidebar is pushed further down the page? </p>\n\n<p>Client is also claiming the website takes extremely long to load because of the Google Web Font being used - seems extremely odd to me, any ideas?</p>\n\n<p>Thanks</p>\n"},{"tags":["performance","node.js"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":1333,"score":3,"question_id":8782908,"title":"Fastest way to check for existence of a file in NodeJs","body":"<p>I'm building a super simple server in node and in my onRequest listener I'm trying to determine if I should serve a static file (off the disk) or some json (probably pulled from mongo) based on the path in <code>request.url</code>.</p>\n\n<p>Currently I'm trying to stat the file first (because I use mtime elsewhere) and if that doesn't fail then I read the contents from disk. Something like this:</p>\n\n<pre><code>fs.stat(request.url.pathname, function(err, stat) {\n    if (!err) {\n        fs.readFile(request.url.pathname, function( err, contents) {\n            //serve file\n        });\n    }else {\n        //either pull data from mongo or serve 404 error\n    }\n});\n</code></pre>\n\n<p>Other than cacheing the result of <code>fs.stat</code> for the <code>request.url.pathname</code>, is there something that could speed this check up? For example, would it be just as fast to see if <code>fs.readFile</code> errors out instead of the <code>stat</code>? Or using <code>fs.createReadStream</code> instead of <code>fs.readFile</code>? Or could I potentially check for the file using something in <code>child_process.spawn</code>? Basically I just want to make sure I'm not spending any extra time messing w/ fileio when the request should be sent to mongo for data...</p>\n\n<p>Thanks!</p>\n"},{"tags":["javascript","performance","mobile","web"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":49,"score":0,"question_id":12768327,"title":"Can mobile web applications be made as fast as native applications?","body":"<p>Can (client-side) mobile web applications be made as fast as native applications? AFAIK there's no JS -> machine code compiler. I am wondering if it is practical to do JS -> (some OOP language like C++) -> machine code and make the client side mobile web app perform in par with native applications? </p>\n\n<p>== EDIT ==</p>\n\n<p>I am aware of phonegap and other related frameworks. My interest is to know if local web applications can perform as fast as native code/applications.</p>\n"},{"tags":["c#","winforms","performance","httpwebrequest"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":436,"score":2,"question_id":8300060,"title":"Winform application first web request is slow","body":"<p>When I first make a HttpWebRequest in a Winform App, it takes 10~30 seconds to process the first request. Subsequent calls takes less than a second. I wrote a little POC app to exemplify this, it consists of two buttons and 2 textBoxes, an image of the app can be found here (I can't upload images here yet...):</p>\n\n<p><img src=\"http://i.stack.imgur.com/HUccI.jpg\" alt=\"enter image description here\"></p>\n\n<p>The app code is very simple:</p>\n\n<pre><code>public partial class Form1 : Form\n{\n    DateTime _start;\n    TimeSpan _span;\n    int _count = 0;\n    public Form1()\n    {\n        InitializeComponent();\n    }\n\n    private void Form1_Load(object sender, EventArgs e)\n    {\n\n    }\n\n    private void button1_Click(object sender, EventArgs e)\n    {\n        _count++;\n        _start = DateTime.Now;\n        HttpWebRequest _request = (HttpWebRequest)WebRequest.Create(textBox1.Text); ;\n        // Added after Conrad's response:\n        _request.CachePolicy = new HttpRequestCachePolicy(HttpRequestCacheLevel.NoCacheNoStore);\n        HttpWebResponse _response = (HttpWebResponse)_request.GetResponse(); \n        _response.Close();\n        _span = DateTime.Now - _start;\n        textBox2.Text += _count.ToString(\"000\") + \": \" + _span.ToString(@\"mm\\:ss\\,fff\") + \"\\r\\n\";\n    }\n\n    private void button2_Click(object sender, EventArgs e)\n    {\n        textBox2.Text = \"\";\n    }\n}\n</code></pre>\n\n<p>Why does it take so long in the 1st request? There is something I can do to speed this up?</p>\n"},{"tags":["python","performance","perl"],"answer_count":9,"favorite_count":2,"up_vote_count":12,"down_vote_count":2,"view_count":1332,"score":10,"question_id":1984871,"title":"Why is my Python version slower than my Perl version?","body":"<p>I've been a Perl guy for over 10 years but a friend convinced me to try Python and told me how much faster it is than Perl. So just for kicks I ported an app I wrote in Perl to Python and found that it runs about 3x slower. Initially my friend told me that I must have done it wrong, so I rewrote and refactored until I could rewrite and refactor no more and ... it's still a lot slower. So I did a simple test:</p>\n\n<pre><code>i = 0\nj = 0\n\nwhile (i &lt; 100000000):\n    i = i + 1\n    j = j + 1\n\nprint j\n</code></pre>\n\n<p>$ time python python.py<br>\n100000000</p>\n\n<p>real    0m48.100s<br>\nuser    0m45.633s<br>\nsys 0m0.043s</p>\n\n<pre><code>my $i = 0;\nmy $j = 0;\n\nwhile ($i &lt; 100000000) {\n    ++$i; # also tested $i = $i + 1 to be fair, same result\n    ++$j;\n}\n\nprint $j;\n</code></pre>\n\n<p>$ time perl perl.pl<br>\n100000000</p>\n\n<p>real    0m24.757s<br>\nuser    0m22.341s<br>\nsys 0m0.029s</p>\n\n<p>Just under twice as slow, which doesn't seem to reflect any of the benchmarks I've seen ... is this a problem with my installation or is Python really that much slower than Perl?</p>\n"},{"tags":["c#","wpf","windows","multithreading","performance"],"answer_count":6,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":803,"score":2,"question_id":6296234,"title":"Preload WPF Window (perhaps in other thread) to improve performance","body":"<p>I have a WPF Window which takes a lot of time to create and show. The user should be able to open one ore more of this windows.\nNow, I am searching a way to improve the performance. One Idea is to create two of this Windows at the startup of the Application (when It shows the splash-screen). After I have only to show and hide this windows (and change the attached viewmodel). This should not be a problem. But when the user is working with the application and he uses the two windows which are loaded at startup of the application, I should load a third instance of the window in background. Now, the application should no be blocked when It loads the third one (or the 4. oder 5. etc.). Now, I am searching a way to do this. Would it be possible to load the window in another thread and after transfer it to the main UI thread?\nOr are there other scenarios to reach the goal?</p>\n\n<p>Thanks for any help.</p>\n\n<p>Best Regards, Thomas</p>\n"},{"tags":["python","performance","list","index","slice"],"answer_count":1,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":80,"score":3,"question_id":12768504,"title":"Python: an efficient way to slice a list with a index list","body":"<p>I wish to know an efficient way and code saving to slice a list of thousand of elements</p>\n\n<p>example: </p>\n\n<pre><code>b = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\"] \nindex = [1,3,6,7] \n</code></pre>\n\n<p>I wish a result like as:</p>\n\n<pre><code>c = [\"b\",\"d\",\"g\",\"h\"] \n</code></pre>\n"},{"tags":["sql","performance","sqlite","composite-key","unique-constraint"],"answer_count":3,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":59,"score":1,"question_id":12765480,"title":"Composite key vs extra unique column","body":"<p>Are there any major overhead differences between using a <code>UNIQUE</code> constraint on two columns and simply having one column that contains the concatenation of the two column values/GUID and using an index on that? For example, instead of using <code>|John|Smith|</code> (two different columns) as the unique constraint, would it be more efficient to add an extra column that holds <code>JohnSmith</code> or a random GUID?</p>\n"},{"tags":["ios","performance","ipad","rotation"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":12765631,"title":"ipad app - orientation rotation performance is poor","body":"<p>My iPad app is master-detail styled. The detail view contains a self-defined view (let's call it board view), all contents in that view are drawn in drawRect(). When iPad rotates orientation, because detail view size is changed, it needs to redraw the whole board view. Everything works fine so far. But I noticed one problem that when iPad rotates orientation, my app always delay a couple of seconds before rotate the screen. I checked the Mail application, when iPad rotates, its screen rotates immediately.</p>\n\n<p>I wonder how to debug such performance problem. Is that because the board view's drawRect() performs too slow?</p>\n"},{"tags":["java","performance","drawing","javafx-2"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":65,"score":2,"question_id":12733051,"title":"JavaFX 2 path drawing performance","body":"<p>I have a JavaFX <code>Group</code> with a <code>Path</code> Node added to it, to which I add data approximately 30 times per second. This causes my whole GUI to become <em>very</em> laggy and unresponsive after about a minute.\nFirst, I add the path to the <code>Group</code> like this:</p>\n\n<pre><code>root.getChildren().add(path);\n</code></pre>\n\n<p>Data is added like this:</p>\n\n<pre><code>while(true) {\n    // Calculate x and y...\n\n    path.getElements().add(new LineTo(x, y));\n    path.getElements().add(new MoveTo(x, y));\n\n    // Sleep 33 milliseconds...\n}\n</code></pre>\n\n<p>If I do not add the path to the group, but still add data afterwards, the GUI remains responsive, so the performance issue seems to be when drawing the path's shape. Why? What can I do to improve the performance? Is this known to happen or am I doing something wrong?\nThanks!</p>\n"},{"tags":["android","performance","emulator","android-edittext"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":203,"score":1,"question_id":7998379,"title":"Is there a real solution for android emulator?","body":"<p>I'm trying to program a game for android phones. Anyway I have to check any of my minor changes on my code. But I have only 5MB of file and only using <strong>SurfaceView</strong> to draw not even <strong>GLSurfaceView</strong>(So I think because of <strong>I'm not using OpenGL:3D stuff</strong> and just <strong>drawing bitmaps</strong> it should be faster. I'm not sure because it wasn't fast in C/BorlandC 8D) and android emulator is that much slow It kills me of waiting. <strong>I tried to not use all of my bitmaps so it can upload and install faster.</strong> But <strong>how about FPS</strong> ????? I get 5-10 fps which I need 20 for my game. plus <strong>some times I can't get all the bitmap that should be drawn</strong> by emulator(Example: If I have 10 fps in 5 of that I can't see half of my bitmaps...) I can't check what's going on in my game! So if there is a better way please tell me. I read some peoples are using their <strong>android phone as emulator is that really faster?</strong> If so I'll get one. Some other says <strong>using windows XP is the best way, is there no FPS problem with it?</strong> In that case I have to use virtual machine. Thank you for any reply!</p>\n"},{"tags":["performance","algorithm","matrix"],"answer_count":3,"favorite_count":1,"up_vote_count":2,"down_vote_count":2,"view_count":357,"score":0,"question_id":12744424,"title":"Algorithm to find max cost path of length N in matrix, from [0,0] to last row","body":"<p>I have a <code>n*n</code> matrix, where each element represents an integer. Starting in <code>[0,0]</code> I have to find the path of exactly <code>m</code> elements down to the last row, returning the maximum cost. The path can end in any column on the last row and <code>n ≤ m ≤ n^2</code></p>\n\n<p>I thought of finding all paths of length <code>m</code> from <code>[0,0]</code> to <code>[n-1, 0], [n-1, 1] ... [n-1, n-1]</code>. But it does not feel optimal...</p>\n\n<p>Which algorithm would be the most efficient way of doing this? BFS or DFS?</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>Possible directions are down/right/left, but only visit each element at most once.</p>\n\n<p><strong>EDIT 2</strong></p>\n\n<p>So for example, if this matrix is given (n=4):</p>\n\n<pre><code>[   1   4   1  20 ]\n[   5   0   2   8 ]\n[   6   8   3   8 ]\n[   3   2   9   5 ]\n</code></pre>\n\n<p>And m=7, the path could be</p>\n\n<pre><code>[   →   →   →   ↓ ]\n[   5   0   2   ↓ ]\n[   6   8   3   ↓ ]\n[   3   2   9   x ] = Path cost = 47\n</code></pre>\n\n<p>or</p>\n\n<pre><code>[   ↓   4   1  20 ]\n[   ↓   0   2   8 ]\n[   →   →   ↓   8 ]\n[   3   2   →   x ] = Path cost = 32 \n</code></pre>\n\n<p>or if <code>m = n^2</code></p>\n\n<pre><code>[   →   →   →   ↓ ]\n[   ↓   ←   ←   ← ]\n[   →   →   →   ↓ ]\n[   x   ←   ←   ← ]\n</code></pre>\n\n<p><strong>EDIT 3 / SOLUTION</strong></p>\n\n<p>Thanks to Wanderley Guimarães,<br>\n<a href=\"http://ideone.com/0iLS2\" rel=\"nofollow\">http://ideone.com/0iLS2</a></p>\n"},{"tags":["performance","git","version-control"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":69,"score":0,"question_id":12756202,"title":"What are some alternatives to git rev-list as this is sometimes slow?","body":"<p>I've got a tool in production which calls git rev-list on a single, large repo 10-30 times per minute. I'm seeing git response times vary widely, from around 1 second to as much as 50 seconds (before timeout mechanism abandons the git request).</p>\n\n<pre><code>git rev-list --pretty=raw 2ef9fa0d0fa4c34d57103a0545b3cc96c2552e6f..f5daa48ebcd3cc95a0df683f8c3a3ad64def4a6e\n</code></pre>\n\n<p>The goal is to see if the two commits are ancestors/descendants and if so, which of the two is the ancestor. I make this call once, if I get output I have my answer, if no output, I swap the commit positions and run again. If no output this time then they are not ancestors/descendants of one another.</p>\n\n<p>Is there another, more efficient way, to find this information? If it comes to it, even suggestions on modeling the commit tree in some structure outside of git are appreciated.</p>\n\n<p>Thanks.</p>\n"},{"tags":["performance","osx","debugging","callstack","thread-dump"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":3,"view_count":48,"score":-3,"question_id":12764965,"title":"application hang, MS word, deadlock ? , blocked?","body":"<p>I need some debugging guidance.\nBackground:</p>\n\n<ul>\n<li>I am working on a word doc [MS Office2011, trial version] on Mac [OS\nX 10.7.3]. After a lot of work I tried to paste something form a pdf\nand it has hung :( </li>\n<li>One of those days, I dont want to loose my work</li>\n<li>I opened the terminal and tried to do a cp doc doc.bkup and seems like doc.bkup doesnt have any of my changes though I have saved it few times</li>\n</ul>\n\n<p>So this leads me to believe that if I force-quit I will have to start over. </p>\n\n<p>Luckily I noticed that mac provides a call stack of the process. </p>\n\n<p>I see a lot of STACK TOP at the end:</p>\n\n<pre><code>???  (in Microsoft Word)  load address 0xa000 + 0xcdc5c  [0xd7c5c] [STACK TOP]        5\n???  (in Microsoft Word)  load address 0xa000 + 0xdb7a3  [0xe57a3] [STACK TOP]        5\n???  (in Microsoft Word)  load address 0xa000 + 0xdd58f  [0xe758f] [STACK TOP]        5\n???  (in Microsoft Word)  load address 0xa000 + 0xdde0d  [0xe7e0d] [STACK TOP]        5\n???  (in Microsoft Word)  load address 0xa000 + 0xdf3a2  [0xe93a2] [STACK TOP]        5\nDYLD-STUB$$MBUThreadStorageGetValue  (in WLMKernel)        5\n</code></pre>\n\n<p>and also a lot of load address lines:</p>\n\n<ul>\n<li>! : | 1 ???  (in Microsoft Word)  load address 0xa000 + 0xcd900  [0xd7900]</li>\n<li>! : | 1 ???  (in Microsoft Word)  load address 0xa000 + 0xcd903  [0xd7903]</li>\n<li>! : | 1 ???  (in Microsoft Word)  load address 0xa000 + 0xcda74  [0xd7a74]</li>\n<li>! : | 1 ???  (in Microsoft Word)  load address 0xa000 + 0xcda97  [0xd7a97]</li>\n</ul>\n\n<p>I tried to close all other applications . \nThe CPU seems to be 100% for the Word process, though the monitor shows overall user CPU = 32% only [~2% System CPU]. Not sure how mac reports this, probably its using just one core thats 100%</p>\n\n<p>1GB of memory is free. </p>\n\n<p>I can hold on just some more time, before I give up, force kill and probably start over. But it would be nice to fix this thing to atleast kill current operation [copy from pdf] and be able to save the file</p>\n"},{"tags":["python","performance","web","crawl"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":88,"score":1,"question_id":12763735,"title":"Problems with Speed during web-crawling (Python)","body":"<p>I would love to have this programm improve a lot in speed. It reads +- 12000 pages in 10 minutes. I was wondering if there is something what would help a lot to the speed? I hope you guys know some tips. I am supposed to read +- millions of pages... so that would take way too long :( Here is my code:</p>\n\n<pre><code>from eventlet.green import urllib2                          \nimport httplib                                              \nimport time                                                 \nimport eventlet   \n\n# Create the URLS in groups of 400 (+- max for eventlet)                                    \ndef web_CreateURLS():\n    print str(str(time.asctime( time.localtime(time.time()) )).split(\" \")[3])\n    for var_indexURLS in xrange(0, 2000000, 400):\n        var_URLS = []\n        for var_indexCRAWL in xrange(var_indexURLS, var_indexURLS+400):\n            var_URLS.append(\"http://www.nu.nl\")\n        web_ScanURLS(var_URLS)    \n\n# Return the HTML Source per URL\ndef web_ReturnHTML(url):\n    try:\n        return [urllib2.urlopen(url[0]).read(), url[1]]\n    except urllib2.URLError:\n        time.sleep(10)\n        print \"UrlError\"\n        web_ReturnHTML(url)\n\n# Analyse the HTML Source\ndef web_ScanURLS(var_URLS):\n    pool = eventlet.GreenPool()\n    try:  \n        for var_HTML in pool.imap(web_ReturnHTML, var_URLS):\n               # do something etc..\n    except TypeError: pass\n\nweb_CreateURLS()\n</code></pre>\n"},{"tags":["performance","nginx"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":31,"score":-1,"question_id":12764389,"title":"Ngingx max worker_connections and access log","body":"<p>I'm troubleshoot an issue with my site. I'm seeing in the ngingx-error.log that the max worker_connection limit has been reached when the site went down.\nI'm not seeing an increase of requests during that time in the ngingx-access.log.</p>\n\n<p>Does that mean the mysql database had a bottleneck at that time that caused the requests to queue up? \nOr would it not log any requests that where made after the max worker_connection limit has been reached?</p>\n"},{"tags":["algorithm","performance","cpu","cpu-usage","cpu-architecture"],"answer_count":7,"favorite_count":8,"up_vote_count":19,"down_vote_count":0,"view_count":6436,"score":19,"question_id":3748136,"title":"How is CPU usage calculated?","body":"<p>On my desktop, I have a little widget that tells me my current CPU usage. It also shows the usage for each of my two cores. </p>\n\n<p>I always wondered, how does the CPU calculate how much of its processing power is being used? Also, if the CPU is hung up doing some intense calculations, how can it (or whatever handles this activity) examine the usage, without getting hung up as well?</p>\n"},{"tags":["performance","parsing","csv","sse"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":86,"score":2,"question_id":12640421,"title":"SSE 4.2 CSV file parsing","body":"<p>I'm currently investigating how to use the SSE 4.2 String and Text Processing Instructions STTNI (http://software.intel.com/en-us/articles/xml-parsing-accelerator-with-intel-streaming-simd-extensions-4-intel-sse4/) for efficient CSV file parsing.</p>\n\n<p>My question is if this has been tried before for CSV file/in-memory CSV parsing and if examples are available online? So far I was not successful in finding good resources (except the Intel article mentioned above) on how to use SSE 4.2 for text parsing.</p>\n\n<p>The current strategy I'm trying is to, for each 16 bytes, create 4 bitmasks: </p>\n\n<ul>\n<li>one matching each character against the delimiter</li>\n<li>one matching each character against the newline character</li>\n<li>one matching each character against the quotation character (strings); and</li>\n<li>one matching each character against the escape character (escaping delimiter, newlines, quotes)</li>\n</ul>\n\n<p>with the information gained by the bitmasks it is easy to determine the offsets and lengths for each value in the CSV.</p>\n"},{"tags":["mysql","ruby-on-rails-3","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":41,"score":0,"question_id":12753988,"title":"Rails console vs MySQL console - Counting rows in a huge table","body":"<p>I'd like to show a graph on how many records a table has, for the last 30 days. I'm going to do a rake task that I will run each day from a cron job.</p>\n\n<p>I just made a test, and I found a weird \"issue\".\nI ran this code in the Rails console:</p>\n\n<pre><code>SeenEpisode.count\n#=&gt; (57135.2ms)  SELECT COUNT(*) FROM `seen_episodes`\n</code></pre>\n\n<p>I then tested this in the MySQL console:</p>\n\n<pre><code>use my_database;\nSELECT COUNT(*) FROM seen_episodes;\n1 row in set (3.94 sec)\n</code></pre>\n\n<p>This table has 21 million records.</p>\n\n<p>I ran both tests on my production server, to get real performance numbers.</p>\n\n<p>Other tables with around 500k records, take 80-300ms in the Rails console.</p>\n\n<p>Why does counting (using the same query) the 21m big table in Rails take so much longer than in the MySQL console?</p>\n"},{"tags":["java","performance","algorithm","complexity","big-o"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":90,"score":0,"question_id":12750411,"title":"Complexity and Efficiency in Algorithm for: a[j]-a[i] i>=j","body":"<p>I'm looking to make this much quicker. I've contemplated using a tree, but I'm not sure if that would actually help much. </p>\n\n<p>I feel like the problem is for most cases you don't need to calculate all the possible maximums only a hand full, but I'm not sure where to draw the line</p>\n\n<p>Thanks so much for the input, \nJasper</p>\n\n<pre><code>public class SpecialMax {\n\n    //initialized to the lowest possible value of j; \n    public static int jdex = 0; \n    //initialized to the highest possible value of i; \n    public static int idex; \n    //will hold possible maximums \n    public static Stack&lt;Integer&gt; possibleMaxs = new Stack&lt;Integer&gt; (); \n\n    public static int calculate (int[] a){\n        if (isPositive(a)){ \n            int size = a.length; \n            int counterJ; \n            counterJ = size-1;\n\n            //find and return an ordered version of a\n\n            int [] ordered = orderBySize (a);\n\n            while (counterJ&gt;0){\n                /* The first time this function is called, the Jvalue will be \n                 * the largest it can be, similarly, the Ivalue that is found\n                 * is the smallest\n                 */\n                int jVal  = ordered[counterJ];  \n                int iVal  = test (a, jVal);\n                possibleMaxs.push(jVal-iVal);\n                counterJ--; \n            }\n\n            int answer = possibleMaxs.pop(); \n\n            while (!possibleMaxs.empty()){\n                if (answer&lt;possibleMaxs.peek()){\n                    answer = possibleMaxs.pop(); \n                } else { \n                    possibleMaxs.pop(); \n                }\n            }\n\n            System.out.println(\"The maximum of a[j]-a[i] with j&gt;=i is: \");\n            return answer;\n        } else {\n            System.out.println (\"Invalid input, array must be positive\"); \n            return 0; //error\n        }\n    }\n\n    //Check to make sure the array contains positive numbers\n    public static boolean isPositive(int[] a){ \n        boolean positive = true; \n        int size = a.length; \n\n        for (int i=0; i&lt;size; i++){\n            if (a[i]&lt;0){\n                positive = false; \n                break; \n            }\n        }\n\n\n        return positive; \n    }\n\n    public static int[] orderBySize (int[] a){\n         //orders the array into ascending order\n         int [] answer = a.clone(); \n         Arrays.sort(answer);\n         return answer; \n    }\n\n         /*Test returns an Ival to match the input Jval it accounts for \n          * the fact that jdex&lt;idex. \n          */\n    public static int test (int[] a, int jVal){\n        int size = a.length;\n        //initialized to highest possible value\n        int tempMin = jVal; \n        //keeps a running tally \n        Stack&lt;Integer&gt; mIndices = new Stack&lt;Integer&gt; (); \n\n        //finds the index of the jVal being tested\n        for (int i=0; i&lt;size; i++) { \n            if (jVal==a[i]){\n                //finds the highest index for instance\n                if (jdex&lt;i){\n                    jdex = i;\n                }\n            }\n        }\n\n        //look for the optimal minimal below jdex;  \n        for (int i=0; i&lt;jdex; i++){\n            if (a[i]&lt;tempMin){\n                tempMin = a[i]; \n                mIndices.push(i);\n            }\n        }\n\n        //returns the index of the last min\n        if (!mIndices.empty()){\n           idex = mIndices.pop(); \n        }\n\n        return tempMin; \n    }\n\n}\n</code></pre>\n"},{"tags":["java","performance","scala","for-loop","while-loop"],"answer_count":9,"favorite_count":33,"up_vote_count":79,"down_vote_count":0,"view_count":10895,"score":79,"question_id":6146182,"title":"How to optimize for-comprehensions and loops in Scala?","body":"<p>So Scala is supposed to be as fast as Java. I'm revisiting in Scala some Project Euler problems that I originally tackled in Java. Specifically Problem 5 : \"What is the smallest positive number that is evenly divisible by all of the numbers from 1 to 20?\"</p>\n\n<p>Here's my Java solution, which takes 0.7 seconds to complete on my machine:</p>\n\n<pre><code>public class P005_evenly_divisible implements Runnable{\n    final int t = 20;\n\n    public void run() {\n        int i = 10;\n        while(!isEvenlyDivisible(i, t)){\n            i += 2;\n        }\n        System.out.println(i);\n    }\n    boolean isEvenlyDivisible(int a, int b){\n        for (int i = 2; i &lt;= b; i++) {\n            if (a % i != 0) return false;\n        }\n        return true;\n    }\n\n    public static void main(String[] args) {\n        new P005_evenly_divisible().run();\n    }\n}\n</code></pre>\n\n<p>Here's my \"direct translation\" into Scala, which takes 103 seconds (147 times longer!)</p>\n\n<pre><code>object P005_JavaStyle {\n  val t:Int = 20;\n  def run {\n    var i = 10\n    while(!isEvenlyDivisible(i,t)) i += 2\n    println(i)\n  }\n  def isEvenlyDivisible(a:Int, b:Int):Boolean = {\n    for (i &lt;- 2 to b) \n      if (a % i != 0) return false\n    return true\n  }\n  def main(args : Array[String]) {\n    run\n  }\n}\n</code></pre>\n\n<p>Finally here's my attempt at functional programming, which takes 39 seconds (55 times longer)</p>\n\n<pre><code>object P005 extends App{\n  def isDivis(x:Int) = (1 to 20) forall {x % _ == 0}\n  def find(n:Int):Int = if (isDivis(n)) n else find (n+2)\n  println (find (2))\n}\n</code></pre>\n\n<p>Using Scala 2.9.0.1 on Windows 7 64-bit. Any ideas for improving performance? Am I doing something wrong? Or Java just a lot faster?</p>\n"},{"tags":["networking","performance","simulate"],"answer_count":5,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":1614,"score":5,"question_id":2974860,"title":"How can i simulate a slow network connection","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/1094760/network-tools-that-simulate-slow-network-connection\">Network tools that simulate slow network connection</a>  </p>\n</blockquote>\n\n\n\n<p>I have a Bet Server which sends bet data to a game terminal\nI would like to simulate a slow network connection.  i want the network to be really busy, heavily loaded so i can see how the bet server performs and if it times out when there is a heavy load on the network.</p>\n\n<p>Are there any tools i can use to do this?</p>\n\n<p>Thanks for any help.\nRegards, Riaz</p>\n"},{"tags":["sql","sql-server","performance","tsql","join"],"answer_count":6,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":129,"score":2,"question_id":12746927,"title":"Using a join to optimize sql query","body":"<p>I have a table that stores a person's information with close to 10 million rows. </p>\n\n<p>Currently State is a char(2) field on the person table.  This leads to tons of duplication of data as you would expect.  If I normalize State data into it's own table and create an FK to it in the person table would this result in faster query times?</p>\n\n<p>Before:</p>\n\n<pre><code>SELECT Name, City, State FROM Person WHERE State = 'WI'\n</code></pre>\n\n<p>After:</p>\n\n<pre><code>SELECT p.Name, p.City, s.Name as State\nFROM Person p\n    INNER JOIN State s ON p.State == s.Id\nWHERE s.Name = 'WI'\n</code></pre>\n\n<p>It seems to me that this would accomplish an increase in performance but I am far from an expert when it comes to optimizing queries.</p>\n"},{"tags":["sockets","networking","performance","simulate"],"answer_count":5,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":1415,"score":2,"question_id":2142789,"title":"Simulate low speed connection on local network","body":"<p>Sometimes I want to manually test my web applications (either desktop applications calling web services or websites or RIA) to see how they behave with low speed internet connection (56 kbps for example). Is it possible to do it through network by simulating lower speed that the real one? If yes, is it also possible to simulate low quality connection (something as packet loss, connection drop, e.t.c.)?</p>\n"},{"tags":["c","performance","character","occurences"],"answer_count":5,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":164,"score":1,"question_id":12394142,"title":"Most Efficient Way To Count How Many Times A Character Occurs within a String","body":"<p>I am writing a very simple function that counts how many times a certain character occurs within a given string. I have a working function but was wondering if there was a more efficient or preferred method of doing this. </p>\n\n<p>Here is the function:</p>\n\n<pre><code>size_t strchroc(const char *str, const char ch)\n{ \n    int c = 0, i = 0;\n\n    while(str[i]) if(str[i++] == ch) c++;\n    return c;\n}\n</code></pre>\n\n<p>I personally cannot think of any way to get this code more efficient. And was wondering (just for the sake of learning) if anybody knew of a way to make this function more efficient.</p>\n\n<p>(efficient in the sense of speed and using minimal resources).</p>\n"},{"tags":["php","apache","session","cross-browser","performance"],"answer_count":6,"favorite_count":1,"up_vote_count":8,"down_vote_count":0,"view_count":275,"score":8,"question_id":1702739,"title":"Site slows for individual users, but they can switch browsers?","body":"<p>I've tried searching for this but it's pretty difficult to put into words.</p>\n\n<p>Basically, our site will run fine for most users without any issues.  Sometimes though, those of us who use the site pretty heavily all day will suddenly get completely bogged down.  Everything just spins in place.  The site itself is still fine - everyone else can still get to it, but the individual user is stuck.  In fact, restarting the browser entirely doesn't generally fix the issue, even if you explicitly clear your cookies.</p>\n\n<p>You can, however, generally open the site just fine by switching to a different browser.  If you're bogged down in Firefox, you can usually open it up and continue working just fine in IE.  This can happen both ways (you can bog down IE, and switching to firefox works).</p>\n\n<p>Does this make any sense at all?  It's like there's something breaking with the session, but I don't know what would cause this and the session should reset by restarting the browser and clearing cookies and whatnot.</p>\n\n<p>Any ideas?</p>\n\n<p>[Editing to clarify, sorry, should have included this to begin with]\nServer is a very basic LAMP stack on RedHat with Apache 2.2.3, PHP 5.2.11, MySQL 5.0.45 (we've considered upgrading MySQL but I don't think this is the issue here).  It's a standard configuration for Rackspace, so I don't think we're doing anything exotic besides maybe the Zend Optimizer.</p>\n\n<p>We're using a lot of javascript/jquery but it's all pretty standard stuff and I wouldn't expect a memory leak to not affect the other browser, though I may be wrong.</p>\n\n<p>Also, our server's CPU and memory usage have never broken the 25% margin, even in spikes, and the spikes don't seem to correlate with this phenomenon.</p>\n"},{"tags":["mysql","performance","like","indexes"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":79,"score":3,"question_id":12745681,"title":"Slow search by index query LIKE% MYSQL","body":"<p>i have table with 100 000 000 rows so large.\nStructure of table</p>\n\n<pre><code>id         int          INDEX(not primary not unique just index)\nlang_index varchar(5)   INDEX\nname       varchar(255) INDEX\nenam       varchar(255) INDEX\n</code></pre>\n\n<p>Ok. i do query </p>\n\n<p>1 Query</p>\n\n<pre><code>\"SELECT name FROM table WHERE lang_index='en' AND name LIKE 'myname%'\"\n</code></pre>\n\n<p>Speed is ok for this large table. around 0.02 sec.</p>\n\n<p>i try \n2 Query</p>\n\n<pre><code>\"SELECT name FROM table WHERE lang_index='en' AND (name LIKE 'myname%' OR enam LIKE 'myname%')\"\n</code></pre>\n\n<p>Very very slow around 230 sec!!!</p>\n\n<p>then i try this \n3 Query</p>\n\n<pre><code>\"SELECT name FROM table WHERE lang_index='en' AND enam LIKE 'myname%'\"\n</code></pre>\n\n<p>Speed is fantastic. around 0.02 sec.</p>\n\n<p>Then i explode my 2nd query for two queries (1 and 3 query) its faster. around 0.04 sec but it not simply.</p>\n\n<p>Why my query is slow? Two queries much faster than one. \nI need do this <code>\"SELECT name FROM table WHERE lang_index='en' AND (name LIKE 'myname%' OR enam LIKE 'myname%')\"</code>\nHow i can make it faster?</p>\n"},{"tags":["java","performance","design-patterns"],"answer_count":7,"favorite_count":0,"up_vote_count":10,"down_vote_count":5,"view_count":339,"score":5,"question_id":12759650,"title":"Does using design patterns makes java code slow?","body":"<p>Does using design patterns makes java code slow? If I use extra interfaces and syntax constructions (like class wrap) will I get well organized but slow code or that won't make my code significantly slower?</p>\n"},{"tags":["sql-server","performance","sql-server-2008"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":12360558,"title":"SQL Server Intermittent performance on simple select","body":"<p>We are experiencing some intermittent performance issues with queries such as</p>\n\n<pre><code>SELECT * FROM tblUser\n</code></pre>\n\n<p>or  </p>\n\n<pre><code>SELECT * FROM tblUser WITH (NOLOCK)\n</code></pre>\n\n<p>You can run them over and over, returning about 28k records. Most of the time they come back in 3 seconds, and then one in 20 will return in 50 second.</p>\n\n<p>Obviously there are no JOIN or WHERE clause issues, or query plan issues. We have also tried copying that data to another database on a different hard drive, all with the same results.</p>\n\n<p>Running perfmon shows no resource conflicts for memory or CPU.</p>\n\n<p>We have a dedicated SQL Server, running SQL 2008 Enterprise x64 Build 7600 (SP1), on a Windows 2008 R2 Enterprise Build 7600 server. It has 48 CPUs with 256GB of RAM. And lots of disk space. The maximum SQL memory is set to 210GB currently.</p>\n\n<p>And changing MaxDOP has not not helped (we've tried 0, 1, 8, 16, etc.).</p>\n\n<p>The other thing we can see is that when you see the query taking 50 seconds, you can then open up another window in SSMS, run a similar query (say SELECT * FROM otherdb..tblOther) and it will finish at the exact same time as the original query. Both get hung up and then finish together.</p>\n"},{"tags":["php","javascript","performance","smarty"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":72,"score":0,"question_id":12758129,"title":"View section of the site takes too time to load","body":"<p>I am developing a web site using PHP,Smarty and MySQL. It takes a long time to load the view part of the web site . I can get and see the data from the database quickly, problem is with the display section. </p>\n\n<pre><code>http://jobsgang.com/c0b0dfc2379f0379d546b727/Kerala_jobs/Kozhikode_jobs/Other_Places_in_Kozhikode_jobs/Engineering_Projects_jobs/Engineer_jobs/btech_be_jobs/experienced_jobs/permanent_jobs/We_are_looking_for_Fresh_Talented__Project_Management_Tranies.html\n</code></pre>\n\n<p>You can see the data from the php out here . It will load quickly, but the display section of the site will take long time .</p>\n\n<p>Please help me to solve it</p>\n"},{"tags":["android","performance","android-listview"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":49,"score":-1,"question_id":12758092,"title":"how to increase the listview loading speed in android?","body":"<p>I have 2000 list items in my list view. But it takes long time to load. Since my application become very slow. If anybody faced the same issue and got solution, could you please share your idea to fix the same.</p>\n"},{"tags":["performance","spring","hibernate"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":12720569,"title":"Forgot to commit Transaction in hibernate","body":"<p>I am using <strong>hibernate and spring</strong> for my web application.</p>\n\n<p>In this at some places i forgot to commit transaction...like below code</p>\n\n<pre><code> SessionFactory sf = HibernateUtils.getSessionFactory();\n                session = sf.openSession();\n                tx = session.beginTransaction();\n..........................................Some Code.............................\n\nBut forgot to commit transaction.....\n\nfinally\n{\nsession.flush();\nsession.close();\n}\n</code></pre>\n\n<p>Now My question is that :-</p>\n\n<ul>\n<li>Is this creates any problem for me ?? </li>\n<li>Any issue regarding memory leak ??</li>\n<li>Increasing load to database??</li>\n</ul>\n\n<p>Or <strong>what is effect of this on my system ??</strong></p>\n"},{"tags":["performance","excel","ms-access","vba","excel-vba"],"answer_count":7,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":1171,"score":1,"question_id":3499878,"title":"how to Speed Up the VBA Macros","body":"<p>I am Generating a New Sheets using macros. For a New Sheet generation , Data is retrieved from more than 4 MS Access DB. Each DB had minimum 200 field. My Macro code includes</p>\n\n<pre><code>  1. Cell locking\n  2. Alignment and formatting\n  3. One third of the cells in the sheet had a formulas\n  4. Cell reference with other Workbooks\n</code></pre>\n\n<p>My problem is  every sheet generation it takes minimum one hour to complete the hole process. But it seems to me it's taking way too long.</p>\n\n<p>I am already added the   <code>Application.ScreenUpdating = True</code>  to speed up the code but still it takes same time. How to do speed up the code , If you have any idea please guide me.</p>\n\n<pre><code>     `For Ip = 5 To  150\n     resp = Range(\"B\" &amp; Ip).Value\n     With ActiveSheet.QueryTables.Add(Connection:= _\n    \"ODBC;DSN=henkel2;DBQ=C:\\Hl-RF\\RSF-Temp.mdb;DriverId=25;FIL=MS Access;MaxBufferSize=2048;\" _\n    , Destination:=Range(\"IV4\"))\n    .CommandText = \"select Vles from \" &amp; Shtname &amp; \" where cint(PrductID)='\" &amp; resp &amp; \"' and cint(DepotID) = '\" &amp; cnt1 &amp; \"' and Mnth = '\" &amp; mnths &amp; \"' and Type='\" &amp; typs &amp; \"'\"\n    .Name = \"tab product\"\n    .FieldNames = True\n    .RowNumbers = False\n    .FillAdjacentFormulas = False\n    .PreserveFormatting = True\n    .RefreshOnFileOpen = False\n    .BackgroundQuery = True\n    .RefreshStyle = xlInsertDeleteCells\n    .SavePassword = False\n    .SaveData = True\n    .AdjustColumnWidth = True\n    .RefreshPeriod = 0\n    .PreserveColumnInfo = True\n    .SourceConnectionFile = _\n    \"C:\\Hl-RF\\tabct.odc\"\n    .Refresh BackgroundQuery:=False\n    End With`\n\n\n    Is There Is any way to Reduce the loop iteration time\n</code></pre>\n\n<p>Thanks In advance</p>\n"},{"tags":["mysql","performance","sqlite"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":80,"score":0,"question_id":12254326,"title":"Should I use SQLite instead of MySQL?","body":"<p>I need to improve a PHP-MySQL web application, which only uses MySQL for REPL operations (and some search functions). 99% of the applications that I worked with never used advanced MySQL features, like replication, cross-table constraints, locking etc.</p>\n\n<p>To my understanding I should instead use SQLite.</p>\n\n<p>Are there any practical benefits if I do this?\nWill I see a significant (>100ms) speed boost?\nShould I expect problems with tables with more than 1,000,000 rows?</p>\n"},{"tags":["c#","performance","math","factors"],"answer_count":7,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":1469,"score":3,"question_id":4549482,"title":"Getting Factors of a Number","body":"<p>Problem:  I'm trying to refactor this algorithm to make it faster.  What would be the first refactoring here for speed?</p>\n\n<pre><code>public int GetHowManyFactors(int numberToCheck)\n    {\n        // we know 1 is a factor and the numberToCheck\n        int factorCount = 2; \n        // start from 2 as we know 1 is a factor, and less than as numberToCheck is a factor\n        for (int i = 2; i &lt; numberToCheck; i++) \n        {\n            if (numberToCheck % i == 0)\n                factorCount++;\n        }\n        return factorCount;\n    }\n</code></pre>\n"},{"tags":["c++","performance","algorithm"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":162,"score":2,"question_id":9269775,"title":"Numbers between a and b without their permutations","body":"<p>I've written a similar question which was closed I would like to ask <b>not the code</b> but an efficiency tip. I haven't coded but if I can't find any good hint in here I'll go and code straightforward. My question:</p>\n\n<p>Suppose you have a function listNums that take <strong>a</strong> as lower bound and <strong>b</strong> as upper bound.</p>\n\n<p>For example a=120 and b=400</p>\n\n<p>I want to print numbers between these numbers with one rule. 120's permutations are 102,210,201 etc. Since I've got 120 I would like to skip printing 201 or 210.</p>\n\n<p><strong>Reason:</strong> The upper limit can go up to 10<sup>20</sup> and reducing the permutations would help the running time.</p>\n\n<p>Again just asking for efficiency tips.</p>\n"},{"tags":["javascript","performance","html5","canvas","getimagedata"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":1035,"score":2,"question_id":8453712,"title":"Horrible Canvas GetImageData() / PutImageData() performance on mobile","body":"<p>I'm doing a little HTML5 game and, while loading my sprites at the beginning of the map, I do some processing with GetImageData() / looping over all the image / PutImageData().</p>\n\n<p>This works fantastically great on my PC, however, on my cell phones it's horrendously slow.</p>\n\n<pre><code>PC: 5-6 ms\niPhone 4: 300-600 ms\nAndroid HTC Desire S: 2500-3000 ms\n</code></pre>\n\n<p>I've been doing some VERY basic benchmarking, and both GetImageData and PutImageData run very fast, what's taking long is the looping through the contents.</p>\n\n<p>Now, I <em>obviously</em> expect a slowdown on the phone, but 1000x sounds a bit excessive, and the loading takes about 4 minutes on my HTC, so that's not going to work. Also, everything else in the game works at very reasonable speed (mainly because the screen is ridiculously smaller, but still, it works surprisingly fine for JS on a cell phone)</p>\n\n<hr>\n\n<p>What I'm doing in this processing is basically \"darkening\" the sprite to a certain level. I simply loop through all the pixels, and multiply them by a value &lt; 1. That's all.</p>\n\n<p>Since this is too slow... Is there a better way of doing the same thing, using the Canvas functionality, (compositing, opacity, whatever), without looping through all the pixels one by one?</p>\n\n<p>NOTE: This layer has some 100% transparent pixels, and some 100% opaque pixels. Both need to remain either 100% opaque or 100% transparent.</p>\n\n<p>Things I've thought of that wouldn't work:<br>\n  1) Painting the sprites in a new canvas, with lower opacity. This won't work because i need the sprites to remain opaque, just darker.<br>\n  2) Painting the sprites, and painting a semi-transparent black rect on top of them. This will make them darker, but it'll also make my transparent pixels not transparent anymore...  </p>\n\n<p>Any ideas?</p>\n\n<p>This is the code I'm using, just in case you see something terribly idiotic in it:</p>\n\n<pre><code>function DarkenCanvas(baseImage, ratio) {\n    var tmpCanvas = document.createElement(\"canvas\");\n    tmpCanvas.width = baseImage.width;\n    tmpCanvas.height = baseImage.height;\n    var ctx = tmpCanvas.getContext(\"2d\");\n    ctx.drawImage(baseImage, 0, 0);\n\n    var pixelData = ctx.getImageData(0, 0, tmpCanvas.width, tmpCanvas.height);\n    var length = pixelData.data.length;\n    for (var i = 0; i &lt; length; i+= 4) {\n        pixelData.data[i] = pixelData.data[i] * ratio;\n        pixelData.data[i + 1] = pixelData.data[i + 1] * ratio;\n        pixelData.data[i + 2] = pixelData.data[i + 2] * ratio;\n    }\n\n    ctx.putImageData(pixelData, 0, 0);\n    return tmpCanvas\n}\n</code></pre>\n\n<p><strong>EDIT:</strong> This is an example of what i'm trying to do with the image:<br>\nOriginal: <a href=\"http://www.crystalgears.com/isoengine/sprites-ground.png\" rel=\"nofollow\">http://www.crystalgears.com/isoengine/sprites-ground.png</a><br>\nDarkened: <a href=\"http://www.crystalgears.com/isoengine/sprites-ground_darkened.png\" rel=\"nofollow\">http://www.crystalgears.com/isoengine/sprites-ground_darkened.png</a>  </p>\n\n<p>Thanks!<br>\nDaniel</p>\n"},{"tags":["performance","statistics","perfmon"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":1332,"score":3,"question_id":5399603,"title":"Issues remoting to perfmon","body":"<p>Hey all, \nI'm having trouble with PerfMon on one system out of fifteen in a development environment. Accessing it from the local machine is fine but connecting to it remotely throws a \"Cannot connect\" error.</p>\n\n<p>Each machine is running Win 2003, is connected to the same domain and I have admin rights to all. </p>\n\n<p>There were some services set to disabled which are normally enabled by default so I've set these to match the other machines on the network - still have the same problem.</p>\n\n<p>Any ideas?</p>\n\n<p>Cheers</p>\n\n<p><em><strong></em>**Update**</strong></p>\n\n<p>Ok - I found it was the remote registry service not running correctly causing the above error; Once that was enabled Perfmon is now telling me \"No such interface supported\".</p>\n\n<p>If I connect through Computer Management, it fails the first time, but the second attempt is successful. Connecting through perfmon fails everytime.</p>\n"},{"tags":["javascript","performance","algorithm","optimization","autocorrect"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":63,"score":2,"question_id":12662376,"title":"A better way of keeping track of cursor position in a text-field while auto-correcting?","body":"<p>I am working on a jQuery plugin for converting Latin characters to Japanese kana symbols (<a href=\"http://lab.cmikavac.net/autokana/\" rel=\"nofollow\">live example</a>) in real-time (auto-correct) and it works correctly if I'm not typing too fast.. Which is a problem.</p>\n\n<p>For example:</p>\n\n<blockquote>\n  <p>wakarimasu -> わかります</p>\n</blockquote>\n\n<p>However, when I type at my normal speed I get this:</p>\n\n<blockquote>\n  <p>wakarimasu -> わかりあすm (which is actually wakariasum, cursor between \"す\" and \"m\" in the end)</p>\n</blockquote>\n\n<p>If I type it without \"su\", I get this:</p>\n\n<blockquote>\n  <p>wakarima -> わかりあm (which is actually wakariam, cursor between \"あ\" and \"m\" in the end)</p>\n</blockquote>\n\n<p>I figured out why and how this is happening. Basically, \"ri - り\" symbol is still converting from Latin to Japanese kana while I already typed \"m\" in and it finishes just a little bit before I manage to type \"a\" in. Since my function positions the cursor after each conversion, it positions the cursor between \"ri\" and \"m\" and that is where my \"a\" ends up.</p>\n\n<p>This happens for other words as well, but I took this one as an example.</p>\n\n<p>Is there a way I could track the cursor position in a smarter way or is there a way to update/convert/replace only parts of textfield without updating the whole textfield (the way it works now is it fetches what is in the textfield, replaces Latin with kana inside a variable inside the function, updates the textfield with the new string from a variable and then positions the cursor)? Telling visitors not to type too fast wouldn't really be the way to go..</p>\n\n<blockquote>\n  <p>Legend: wa=わ ka=か ri=り ma=ま su=す a=あ</p>\n</blockquote>\n\n<p>Source can be viewed at the link I provided. Thanks.</p>\n\n<p><em>Edit: One more thing to think about is that a user might move cursor left and right to add or remove characters/symbols. This complicates positioning a little bit but it works at the moment.</em></p>\n"},{"tags":["performance","bdd","functional-testing","acceptance-testing"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":69,"score":1,"question_id":12610224,"title":"Functional tests and legacy code in database","body":"<p>The context is the following we have an MVC web application with a <strong>lot of legacy code in the database</strong>. (We are not allowed to migrate this code to the server side)\nFor our persistance store we use the <strong>Repository</strong> pattern.</p>\n\n<p>Our client want to add new features to the application so we obviously added <strong>all the new business logic on the server side</strong>.</p>\n\n<p>The problem we have now is regarding our <strong>functional test suite which is running slower and slower</strong> every days.</p>\n\n<p>The main reason is that we are running tests from the browser using selenium to the database (<strong>End to end</strong>).</p>\n\n<p>I'd like to know if people have been successfull using the following other strategies:</p>\n\n<p><strong>1. Get rid of the UI</strong></p>\n\n<p>Running the tests at the Controller level instead of having to have to go through the browser and web server. To compensate the lost of testing through the UI we would write javascript unit tests or MVC Views unit tests.</p>\n\n<p><strong>2. Get rid of the DB</strong></p>\n\n<p>Write an \"InMemory\" version of our repository so that the application can run completely in memory, that should speed up the Test suite too, since we wont hit the disk and less networking. To compensate we would write integration tests for our Database Repositories. </p>\n\n<p>I think that doing <strong>both 1+2 strategies</strong> would yield maximum speed execution and we will test the stuff that really matter (controllers,business layer, domain entities and various helpers as an integrated whole) (I consider, UI and the DB as \"details\").</p>\n\n<p>Now, the problem is that in fact since the DB has a lot of legacy code, I don't know if it's safe enought to just rely on integration tests for that stuff and keeping the DB out of the functionnal test suite. Should I leave it in the suite anyway ? Or is it fine ?</p>\n\n<p>Any experience or suggestion would be greatly appreciated !</p>\n\n<hr>\n\n<p>Some of my findings are the following:</p>\n\n<ul>\n<li>The Continuous delivery book suggest that these test should always be end to end even if they are slow (althought they also say that not everybody would agree on that)</li>\n<li>Uncle bob would, in my understanding, agree on the 1+2 strategy, but I'm not sure if he would think that with a database with legacy code.</li>\n</ul>\n"},{"tags":["performance","resources","cpu","virtualbox"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":38,"score":0,"question_id":12754696,"title":"Virtual Box CPU resource calculation","body":"<p>I want to build a testing machine with a number of virtual machines running on a virtual network. I need to know how much processor power I need. I figured that I will need about 1GB of ram per one Windows 7 machine, but how do I figure out how much processor power I will need?</p>\n\n<p>Windows will connect and disconnect to a VPN via PPTP, do some automated browser work (clicks etc) and disconnect.</p>\n\n<p>How do I calculate a processor requirement per machine? Let's say I need to have 50 VMs? 100?</p>\n\n<p>Thanks!</p>\n"},{"tags":["performance","assembly","floating-point","x86","sse"],"answer_count":4,"favorite_count":17,"up_vote_count":51,"down_vote_count":0,"view_count":12240,"score":51,"question_id":1528727,"title":"Why is SSE scalar sqrt(x) slower than rsqrt(x) * x?","body":"<p>I've been profiling some of our core math on an Intel Core Duo, and while looking at various approaches to square root I've noticed something odd: using the SSE scalar operations, it is faster to take a reciprocal square root and multiply it to get the sqrt, than it is to use the native sqrt opcode!</p>\n\n<p>I'm testing it with a loop something like:</p>\n\n<pre><code>inline float TestSqrtFunction( float in );\n\nvoid TestFunc()\n{\n  #define ARRAYSIZE 4096\n  #define NUMITERS 16386\n  float flIn[ ARRAYSIZE ]; // filled with random numbers ( 0 .. 2^22 )\n  float flOut [ ARRAYSIZE ]; // filled with 0 to force fetch into L1 cache\n\n  cyclecounter.Start();\n  for ( int i = 0 ; i &lt; NUMITERS ; ++i )\n    for ( int j = 0 ; j &lt; ARRAYSIZE ; ++j )\n    {\n       flOut[j] = TestSqrtFunction( flIn[j] );\n       // unrolling this loop makes no difference -- I tested it.\n    }\n  cyclecounter.Stop();\n  printf( \"%d loops over %d floats took %.3f milliseconds\",\n          NUMITERS, ARRAYSIZE, cyclecounter.Milliseconds() );\n}\n</code></pre>\n\n<p>I've tried this with a few different bodies for the TestSqrtFunction, and I've got some timings that are really scratching my head. The worst of all by far was using the native sqrt() function and letting the \"smart\" compiler \"optimize\". At 24ns/float, using the x87 FPU this was pathetically bad:</p>\n\n<pre><code>inline float TestSqrtFunction( float in )\n{  return sqrt(in); }\n</code></pre>\n\n<p>The next thing I tried was using an intrinsic to force the compiler to use SSE's scalar sqrt opcode:</p>\n\n<pre><code>inline void SSESqrt( float * restrict pOut, float * restrict pIn )\n{\n   _mm_store_ss( pOut, _mm_sqrt_ss( _mm_load_ss( pIn ) ) );\n   // compiles to movss, sqrtss, movss\n}\n</code></pre>\n\n<p>This was better, at 11.9ns/float. I also tried <a href=\"http://www.beyond3d.com/content/articles/8/\">Carmack's wacky Newton-Rhapson approximation technique</a>, which ran even better than the hardware, at 4.3ns/float, although with an error of 1 in 2<sup>10</sup> (which is too much for my purposes). </p>\n\n<p>The doozy was when I tried the SSE op for <em>reciprocal</em> square root, and then used a multiply to get the square root ( x * 1/&radic;x = &radic;x ). Even though this takes two dependent operations, it was the fastest solution by far, at 1.24ns/float and accurate to 2<sup>-14</sup>:</p>\n\n<pre><code>inline void SSESqrt_Recip_Times_X( float * restrict pOut, float * restrict pIn )\n{\n   __m128 in = _mm_load_ss( pIn );\n   _mm_store_ss( pOut, _mm_mul_ss( in, _mm_rsqrt_ss( in ) ) );\n   // compiles to movss, movaps, rsqrtss, mulss, movss\n}\n</code></pre>\n\n<p>My question is basically <em>what gives</em>? <strong>Why is SSE's built-in-to-hardware square root opcode <em>slower</em> than synthesizing it out of two other math operations?</strong></p>\n\n<p>I'm sure that this is really the cost of the op itself, because I've verified:</p>\n\n<ul>\n<li>All data fits in cache, and\naccesses are sequential </li>\n<li>the functions are inlined</li>\n<li>unrolling the loop makes no difference</li>\n<li>compiler flags are set to full optimization (and the assembly is good, I checked)</li>\n</ul>\n\n<p>(<strong>edit</strong>: stephentyrone correctly points out that operations on long strings of numbers should use the vectorizing SIMD packed ops, like <code>rsqrtps</code> &mdash; but the array data structure here is for testing purposes only: what I am really trying to measure is <em>scalar</em> performance for use in code that can't be vectorized.)</p>\n"},{"tags":["java","performance","memory","jvm","memory-allocation"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":2042,"score":2,"question_id":4031242,"title":"How much memory is allocated for one Integer object in Java? How to find out this value for any custom object?","body":"<p>What is the proper way to measure how much memory from the heap should be used to create new object of a certain type (let's talk about Integers to keep it simple)?</p>\n\n<p>Can this value be calculated without experiment? What are the rules in that case? Are these rules strictly specified somewhere or they can vary from jvm to jvm?</p>\n"},{"tags":["python","performance","numpy","matplotlib","interpolation"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":91,"score":0,"question_id":12752221,"title":"Python: improve speed interpolation technique using matplotlib natgrid toolkit with a huge amount of points","body":"<p>i am using <strong>matplotlib natgrid toolkit</strong> to interoplate x,y,z points. My dataset is more than 5 milion of points. I tried my code using a small area (about 900.000,00 points). Using natgrid the time is 44 minute.</p>\n\n<p>Some people know a way to increase the speed or another method more efficent in term of time?  for 2d interpolation there are too many data points to interpolate</p>\n\n<p>thanks in advance for helps and suggestions</p>\n\n<pre><code>import shapefile #\nimport os #\nimport glob #\nimport math #\nimport numpy #\nimport numpy as np #\nimport matplotlib.nxutils as nx #\nimport collections\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as ml\nimport matplotlib.delaunay\nfrom liblas import file as lasfile #\nfrom shapely.geometry import Polygon #\nfrom osgeo import gdal, osr, ogr #\nfrom gdalconst import * #\nfrom matplotlib.mlab import griddata\nfrom collections import OrderedDict\n\n\n    def LAS2DTM(inFile,outFile,gridSize=1,dtype=\"GDT_Float32\",nodata=-9999.00,BBOX=None,EPSG=None):\n        if BBOX == None:\n            X = []\n            Y = []\n            for p in lasfile.File(inFile,None,'r'):\n                X.append(p.x)\n                Y.append(p.y)\n            xmax, xmin = max(X),min(X)\n            ymax, ymin = max(Y), min(Y)\n            del X,Y\n        else:\n            xmax,xmin,ymax,ymin = BBOX[0],BBOX[1],BBOX[2],BBOX[3]\n        # number of row and columns\n        nx = int(math.ceil(abs(xmax - xmin)/gridSize))\n        ny = int(math.ceil(abs(ymax - ymin)/gridSize))\n        # Create an array to hold the number of points in each pixel\n        cnts = np.zeros((ny, nx))\n        # Create an array to hold the values\n        data = np.zeros((ny, nx))\n        # read all points\n        x = []\n        y = []\n        z = []\n        for p in lasfile.File(inFile,None,'r'):\n            x.append(p.x)\n            y.append(p.y)\n            z.append(p.z)\n            # Compute the x and y offsets for where this point would be in the raster\n            dx = int((p.x - xmin)/gridSize)\n            dy = int((ymax - p.y)/gridSize)\n            # Add the z value to the total for that pixel\n            data[dy,dx] += p.z\n            # Add 1 to our count for that pixel\n            cnts[dy,dx] += 1\n        # ingore Error message\n        np.seterr(invalid='ignore')\n        # Compute the averages\n        data = data/cnts\n        del cnts\n        # remove all duplicate points from a X,Y,Z file that have identical x and y coordinates\n        # The first point survives, all subsequent duplicates are removed.\n        tmp = OrderedDict()\n        for point in zip(x, y, z):\n           a = tmp.setdefault(point[:2], point)\n        mypoints = tmp.values()\n        del x,y,z\n        points_zipped = zip(*mypoints)\n        del mypoints\n        xvals = np.array(points_zipped[0])\n        yvals = np.array(points_zipped[1])\n        zvals = np.array(points_zipped[2])\n        del points_zipped\n        # define grid.\n        xi = np.linspace(xmin, xmax, nx)\n        yi = np.linspace(ymin, ymax, ny)\n        # create a meshgrid\n        xi, yi = np.meshgrid(xi, yi)\n        # grid the data.\n        zi = griddata(xvals,yvals,zvals,xi,yi,interp='nn')\n        # convert \"numpy.ma.core.MaskedArray\" in a \"np.array\"\n        zi = np.array(zi)\n        # mask a numpy.ndarray with another numpy.ndarray\n        data[np.isnan(data)] = zi[np.isnan(data)]\n        # Create gtif\n        if dtype == \"GDT_Unknown\": # Unknown or unspecified type\n            target_ds = gdal.GetDriverByName('GTiff').Create(outFile, nx,ny, 1, gdal.GDT_Unknown)\n        elif dtype == \"GDT_Byte\": # Eight bit unsigned integer\n            target_ds = gdal.GetDriverByName('GTiff').Create(outFile, nx,ny, 1, gdal.GDT_Byte)\n        elif dtype == \"GDT_UInt16\": # Sixteen bit unsigned integer\n            target_ds = gdal.GetDriverByName('GTiff').Create(outFile, nx,ny, 1, gdal.GDT_UInt16)\n        elif dtype == \"GDT_Int16\": # Sixteen bit signed integer\n            target_ds = gdal.GetDriverByName('GTiff').Create(outFile, nx,ny, 1, gdal.GDT_Int16)\n        elif dtype == \"GDT_UInt32\": # Thirty two bit unsigned integer\n            target_ds = gdal.GetDriverByName('GTiff').Create(outFile, nx,ny, 1, gdal.GDT_UInt32)\n        elif dtype == \"GDT_Int32\": # Thirty two bit signed integer\n            target_ds = gdal.GetDriverByName('GTiff').Create(outFile, nx,ny, 1, gdal.GDT_Int32)\n        elif dtype == \"GDT_Float32\": # Thirty two bit floating point\n            target_ds = gdal.GetDriverByName('GTiff').Create(outFile, nx,ny, 1, gdal.GDT_Float32)\n        elif dtype == \"GDT_Float64\": # Sixty four bit floating point\n            target_ds = gdal.GetDriverByName('GTiff').Create(outFile, nx,ny, 1, gdal.GDT_Float64)\n        elif dtype == \"GDT_CInt16\": # Complex Int16\n            target_ds = gdal.GetDriverByName('GTiff').Create(outFile, nx,ny, 1, gdal.GDT_CInt16)\n        elif dtype == \"GDT_CInt32\": # Complex Int32\n            target_ds = gdal.GetDriverByName('GTiff').Create(outFile, nx,ny, 1, gdal.GDT_CInt32)\n        elif dtype == \"GDT_CFloat32\": # Complex Float32\n            target_ds = gdal.GetDriverByName('GTiff').Create(outFile, nx,ny, 1, gdal.GDT_CFloat32)\n        elif dtype == \"GDT_CFloat64\": # Complex Float64\n            target_ds = gdal.GetDriverByName('GTiff').Create(outFile, nx,ny, 1, gdal.GDT_CFloat64)\n        # top left x, w-e pixel resolution, rotation, top left y, rotation, n-s pixel resolution\n        target_ds.SetGeoTransform((xmin, gridSize, 0,ymax, 0, -gridSize))\n        # set the reference info\n        if EPSG is None:\n            # Source has no projection (needs GDAL &gt;= 1.7.0 to work)\n            target_ds.SetProjection('LOCAL_CS[\"arbitrary\"]')\n        else:\n            proj = osr.SpatialReference()\n            proj.ImportFromEPSG(EPSG)\n            # Make the target raster have the same projection as the source\n            target_ds.SetProjection(proj.ExportToWkt())\n        # write the band\n        target_ds.GetRasterBand(1).WriteArray(data)\n        target_ds.GetRasterBand(1).SetNoDataValue(nodata)\n        target_ds = None\n</code></pre>\n"},{"tags":["c#","asp.net","performance","c#-4.0","webforms"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":48,"score":0,"question_id":12753148,"title":"Improving nested objects filtering speed","body":"<p>Here's a problem I experience (simplified example):\nLet's say I have several tables:\n<img src=\"http://i.stack.imgur.com/mJfga.png\" alt=\"enter image description here\"></p>\n\n<p>One customer can have mamy products and a product can have multiple features.</p>\n\n<p>On my asp.net front-end I have a grid with customer info:</p>\n\n<p>something like this:   </p>\n\n<pre><code>Name   Address   \nJohn   222 1st st     \nMark   111 2nd st \n</code></pre>\n\n<p>What I need is an ability to filter customers by feature. So, I have a dropdown list of available features that are connected to a customer.  </p>\n\n<p>What I currently do:<br>\n1. I return <code>DataTable</code> of Customers from stored procedure. I store it in viewstate<br>\n2. I return <code>DataTable</code> of features connected to customers from stored procedure. I store it in viewstate\n3. On filter selected, I run stored procedure again with new feature_id filter where I do joins again to only show customers that have selected feature.  </p>\n\n<p><strong>My problem: It is very slow.</strong></p>\n\n<p>I think that possible solutions would be:<br>\n1. On page load return ALL data in one viewstate variable. So basically three lists of nested objects. This will make my page load slow.\n2. Perform async loazing in some smart way. How?</p>\n\n<p>Any better solutions?</p>\n\n<p><strong>Edit:</strong><br>\nthis is a simplified example, so I also need to filter customer by property that is connected through 6 tables to table Customer.</p>\n"},{"tags":["performance","assembly","x86"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":1,"view_count":116,"score":2,"question_id":12749331,"title":"Are shift op codes really 3 times faster than moves? Intel x86","body":"<p>In my Intel x86 Pentium handbook it says that ADD and shifts like SAL/SHR take 1/3 clock compared to things like JMP and MOV that take 1 clock. Is this really true that a bunch of adds and shifts will 3 times faster than a bunch of movs?</p>\n\n<p>I guess I am doubly confused because there is table of \"latencies\" on the web showing \"Pentium M\" and none of timings are 1/3, although a few are 1/2. Is this because my book is old and on newer Pentiums shift is the same speed as JMP?</p>\n"}]}
{"total":25593,"page":15,"pagesize":100,"questions":[{"tags":["c++","algorithm","stl","performance"],"answer_count":5,"favorite_count":1,"up_vote_count":7,"down_vote_count":0,"view_count":635,"score":7,"question_id":6613846,"title":"Random element in STL set/map in log n","body":"<p>Since C++ STL set/map are implemented as red-black trees, it should be possible to not only do <code>insert</code>, <code>delete</code>, and <code>find</code> in <em>O(log n)</em> time, but also <code>getMin</code>, <code>getMax</code>, <code>getRandom</code>. As I understand the former two have their equivalent in <code>begin()</code> and <code>end()</code> (is that correct?). How about the last one? How can I do that?</p>\n\n<p>The only idea I had so far was to use <code>advance</code> with a random argument, which however takes linear time...</p>\n\n<p>EDIT: 'random' should refer to a uniform distribution</p>\n"},{"tags":["java","performance","hibernate","orm"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":198,"score":4,"question_id":12747527,"title":"Moving Hibernate instrumentation to build time","body":"<p>I have an application with about 3000 entities (I know it's a lot but I can't change it).\nWhen the application loads it takes Hibernate a minutes to do all the instrumentation and the SessionFactory setup stuff.<br>\nI was wondering if I can configure Hibernate to do the instrumentation on the original classes during build time.<br>\nThis way I can avoid 3000 additional generated proxy classes and the huge overhead on application start-up.<br>\nI've found some information on Hibernate build time instrumentation (<code>org.hibernate.tool.instrument.javassist.InstrumentTask</code>) but it isn't clear whether this replaces totally the run-time instrumentation or only handles the Hibernate lazy property fetching mechanism.<br>\nAny information on how to move the proxy generation to build time will be appreciated.</p>\n"},{"tags":["ios","performance","thumbnails","tableview","slowness"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":35,"score":0,"question_id":12750803,"title":"Advice: Tableviews loading images from documents","body":"<p>I have implemented 2 methods using Assetslibrary (which worked perfectly well) for loading images from the photoalbum and another approach which would load thumbnails of images from the documents directory into a table view. For the second approach, I have used GCD to load the images but it is slow, choppy and eventually crashes. I have researched online and found lots of resources for images downloaded from the web but this is for images in the documents directory.</p>\n\n<p>I would like to ask from more experienced developers what is the BEST approach to load movies and pictures from the documents directory, create thumbnails and populate my tableview without it becoming choppy, slow and crashing? Note: I have used GCD (dispatch_async) to make the call to create the imageWithContentsOfFile, used another block to call a method that generates the thumbnails. I will appreciate advice (APIs, approach, etc) on how to deal with this problem and eradicate the laggy, slow behaviour.</p>\n"},{"tags":["c#","performance","active-directory"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":69,"score":1,"question_id":12749881,"title":"Active Directory Query Performance in C#","body":"<p>I'm writing some code to work with Active Directory.  This code includes functions to pull back a user given their account name, then get direct reports and get group memberships (these can be run recursively or non-recusively depending if the full hierarchy is required).\nI've seen a few answers on how this can be done.  However all answers seem to rely on the Distinguished Name.</p>\n\n<p>Is the Distinguished Name the foreign key (in database terms) used to relate these objects in active directory?  My intuition suggests that the objectGuid would be the key used to relate items to one another as that will never change.  As a result I'd assume performance would be better if I rewrote the queries to use objectGuid over DN.</p>\n\n<ul>\n<li>is it possible to query for groups containing a user/group by objectGuid?</li>\n<li>is it possible to query for the objectGuids of direct reports for a given manager (objectGuid).</li>\n<li>do objectGuid queries outperform other attributes, or is DN, sAMAccountName, or one of the other key attributes the best key to use from a performance perpective?</li>\n<li>are the above answers true of most(all?) LDAP implementations, or are they specific to MS AD?</li>\n</ul>\n\n<p>Thanks in advance,</p>\n\n<p>JB</p>\n\n<p>ps. as with most of my questions, the performance difference is probably negligible; this is more for academic interest / satisfying my curiosity.</p>\n"},{"tags":["sql","performance","sqlite"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":70,"score":2,"question_id":12749930,"title":"SQLite NOT IN query is slow","body":"<p>I have two tables - <code>Keys</code> and <code>KeysTemp</code>.\n<code>KeysTemp</code> contains temporary data which should be merged with <code>Keys</code> using the <code>Hash</code> field.\nHere is the query:</p>\n\n<pre><code>SELECT\n    r.[Id]\nFROM\n    [KeysTemp] AS k\nWHERE\n    r.[Hash] NOT IN (SELECT [Hash] FROM [Keys] WHERE [SourceId] = 10)\n</code></pre>\n\n<p>I have indexes on both tables for <code>SourceId</code> and <code>Hash</code> fields:</p>\n\n<pre><code>CREATE INDEX [IdxKeysTempSourceIdHash] ON [KeysTemp]\n(\n    [SourceId],\n    [Hash]\n);\n</code></pre>\n\n<p>The same index for <code>Keys</code> table, but query is still very slow.\nThere is 5 rows in temporary table and about 60000 in the main table. Query by hash takes about 27 milliseconds, but querying this 5 rows takes about 3 seconds.</p>\n\n<p>I also tried splitting index, i.e. creating different indexes for <code>SourceId</code> and <code>Hash</code>, but it works the same way. <code>OUTER JOIN</code> works even worse here. How to solve that issue?</p>\n\n<p><strong>UPDATE</strong>\nIf I remove <code>WHERE [SourceId] = 10</code> from the query it completes in 30ms, that's great, but I need this condition :)</p>\n\n<p>Thanks</p>\n"},{"tags":[".net","performance","memory","dll","namespaces"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":53,"score":2,"question_id":12749694,"title":"Is it better to have multiple DLLs or a single large DLL?","body":"<p>Is it better to have multiple DLLs or a single large DLL?  Or does it matter?</p>\n\n<p>The application I'm working on is large with many namespaces.  Currently, each namespace is contained within a separate DLL, however we've been thinking about combining several of them to simplify dependency issues.</p>\n\n<p>When a namespace is referenced, will all the other namespaces be loaded into memory as well?  I'm just concerned about performance.</p>\n"},{"tags":["performance","make","mingw"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":232,"score":1,"question_id":8571657,"title":"MinGW \"make\" starts very slowly","body":"<p>After some pain and suffering, i managed to install everything neccessary for MinGW to work on a computer not on the network.</p>\n\n<p>It worked nicely for a couple days, but now I'm experiencing very long delays before anything starts to happen after i give the \"make\" command to build my project.</p>\n\n<p>I tried disabling the network, as suggested here: <a href=\"http://stackoverflow.com/questions/929495/why-is-mingw-very-slow\">Why is MinGW very slow?</a>\nBut it didn't help.</p>\n\n<p>Note that it's not the actual compilation / linking progress that is slow, but the startup of those processes seems to take forever. 5-10 minutes. Except if i just did it, then it starts in 10-30 seconds.</p>\n\n<p>I know, it used to take a lot longer to load those tapes on Commodore, but over the years I have grown impatient.</p>\n\n<p>Any ideas?</p>\n"},{"tags":["performance","memory","gcc"],"answer_count":2,"favorite_count":0,"up_vote_count":6,"down_vote_count":0,"view_count":403,"score":6,"question_id":10922607,"title":"gcc likely unlikely macro usage","body":"<p>I am writing a critical piece of code with roughly the following logic</p>\n\n<pre><code>if(expression is true){\n   //do something with extremely low latency before the nuke blows up. This branch is entered rarely, but it is the most important case\n}else{\n   //do unimportant thing that doesnt really matter\n}\n</code></pre>\n\n<p>I am thinking to use likely() macro around the expression, so when it hits the important branch, I get minimum latency. My question is that the usage is really opposite of the macro name suggest because I am picking the 'unlikely' branch to be pre-fetch. Is there a clear downside of doing this in terms of performance?</p>\n"},{"tags":["performance","frontend","performance-testing","yslow","web-performance-test"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":44,"score":0,"question_id":12748848,"title":"Front end performance tool to measure page weight and load time through a funnel","body":"<p>I've been doing some front end performance testing on my project using Yslow.   What I would like to do is find out what new things I'm going to be adding to the browser cache as a user progress through a funnel. </p>\n\n<p>I've been going through each page and \"diff'ing\" the files that are downloaded at this point to see what new stuff we downloaded through each page in the funnel and then calculating the amount of extra KB we are getting. I've been doing this diff by getting the list of stuff downloaded from yslow(components tab) and putting in to excel and then comparing.</p>\n\n<p>I was wondering if there was a tool that based on a list of urls to go through would get the page weight and page load time for each page and report back assuming the cache is maintained between each page?  Or is there a config option in yslow that does this?</p>\n\n<p>Any tips on this would be helpful.</p>\n\n<p>Thank you</p>\n"},{"tags":["c#","performance","class","generics"],"answer_count":2,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":279,"score":4,"question_id":5513127,"title":"C# Performance of classes with generics","body":"<p>Consider a set of elements that derive from a base element.</p>\n\n<pre><code>// \"Abstract\" general element class\nclass AElement {\n  public int Key;\n}\n\n// Specific element class implementation\nclass BElement : AElement {\n}\n</code></pre>\n\n<p>That I want to store in a list. The two options:</p>\n\n<pre><code>List&lt;AElement&gt; aData = new List&lt;AElement&gt;();\nList&lt;BElement&gt; bData = new List&lt;BElement&gt;();\n</code></pre>\n\n<p>If adding BElement's to both the aData and bData lists, and doing operations on the two, the bData version is significantly faster than the aData version. For example if using a dump generic BubbleSort, sorting of the \"Key\" of AElement:</p>\n\n<pre><code>static void BubbleSort&lt;TElement&gt;(List&lt;TElement&gt; Data) where TElement : AElement {\n  for (int i = 0; i &lt; Data.Count; i++)  {\n    for (int j = 0; j &lt; Data.Count; j++)  {\n      if (Data[i].Key&lt; Data[j].Key)         {\n        TElement tmp = Data[i];\n        Data[i] = Data[j];\n        Data[j] = tmp;\n      }\n    }\n  }\n}\n</code></pre>\n\n<p>In my case with 5000 data elements I see up to 20% difference in favor of the bData compared to the aData. </p>\n\n<p>Why is the bData faster than the aData here?</p>\n\n<p>Edit: Added complete code:</p>\n\n<pre><code>using System;\nusing System.Collections.Generic;\nusing System.Diagnostics;\n\nnamespace TemplateClassPerformance\n{\n  class Program {\n\n    // \"Abstract\" general element class\n    class AElement {\n      public int Index;\n    }\n\n    // Specific element class implementation\n    class BElement : AElement { }\n\n    static void Main(string[] args)\n    {\n      Random random = new Random();\n      Stopwatch stopwatch = new Stopwatch();\n\n      for (int j = 0; j &lt; 10; j++) {\n        List&lt;AElement&gt; aData = new List&lt;AElement&gt;();\n        List&lt;BElement&gt; bData = new List&lt;BElement&gt;();\n\n        // Put the same elements in both lists\n        for (int i = 0; i &lt; 5000; i++)\n        {\n          BElement element = new BElement();\n          element.Index = random.Next(1000000);\n          aData.Add(element);\n          bData.Add(element);\n        }\n\n        stopwatch.Reset();\n        stopwatch.Start();\n        BubbleSort(bData);\n        stopwatch.Stop();\n        long sbTicks = stopwatch.ElapsedTicks;\n\n        stopwatch.Reset();\n        stopwatch.Start();\n        BubbleSort(aData);\n        stopwatch.Stop();\n        long saTicks = stopwatch.ElapsedTicks;\n\n        Console.Out.WriteLine(\"sb: {0}, sa: {1}\", sbTicks, saTicks);\n      }\n    }\n\n    static void BubbleSort&lt;TElement&gt;(List&lt;TElement&gt; data) where TElement : AElement {\n      for (int i = 0; i &lt; data.Count; i++) {\n        for (int j = 0; j &lt; data.Count; j++) {\n          if (data[i].Index &lt; data[j].Index)   {\n            TElement tmp = data[i];\n            data[i] = data[j];\n            data[j] = tmp;\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>\n"},{"tags":["wcf","performance","tracing"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":681,"score":3,"question_id":3764486,"title":"WCF Trace Log analysis - help","body":"<p>I am having trouble deciphering a WCF trace file, and I hope someone can help me determine where in the pipeline I am incurring latency.\nThe trace for \"Processing Message XX\" is shown below, where there appears to be 997ms delay between the Activity Boundary and the transfer to \"Process Action\" where my service code is executed (which takes approx 50ms).</p>\n\n<p><img src=\"http://i.stack.imgur.com/wI7sY.jpg\" alt=\"Processing Message trace\"></p>\n\n<p>First, I am unsure whether I am right in understanding the \"Time\" column to represent start time for the activity item. I believe this to be the case because, drilling into the \"Processing action\" trace displays a list of activities with the first timestamp equal to the timestamp shown in the above trace for the \"Processing action\" item. </p>\n\n<p>My primary question is this: how do I determine what is happening during this 997ms time span? As I read about the service trace viewer, it seems that this activity type involves \"transport or security processing\", which leads me to believe it is a network issue, but I cannot be sure.</p>\n\n<p>In case it is relevant, below is a snapshot of the drill-down to \"Process action\" trace.</p>\n\n<p><img src=\"http://i.stack.imgur.com/as2ND.jpg\" alt=\"Processing Action trace\"></p>\n\n<p>Does anyone  have some insight on how to drill further into this activity to pinpoint the cause of delay?</p>\n\n<p><em>(I should mention that the response time varies from ~60ms to over a full second, and only seems to do so in a specific environment, which further leads me to the idea of a networking issue)</em></p>\n\n<p>Thank you in advance!</p>\n"},{"tags":["performance","sensor","eventlistener","rate"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":24,"score":0,"question_id":12747072,"title":"G-Sensor Android to slow (SensorEventListener)","body":"<p>At the moment i get 5hz Update rates from the SensorEventListener (5 new Values per Sec). </p>\n\n<p>Form my App i will need 40Hz+.</p>\n\n<p>How can i tune the updatespeed of the G-Sensor? Is there a way?</p>\n\n<p>Thanx :)</p>\n\n<p>PS: Samsung Galaxy SII</p>\n"},{"tags":["visual-studio-2010","performance","build","compilation","asp.net-mvc-4"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":80,"score":0,"question_id":12725814,"title":"How to diagnose visual studio 2010 slow compile times after MVC4 update","body":"<p>I recently upgraded a solution to the latest release of ASP.NET MVC4 4.0.20710.0 which seems to have slowed down the build of the solution a lot.</p>\n\n<p>Before it was taking ~twenty seconds for twenty projects and now that has increased to around two minutes.</p>\n\n<p>So far I've tried</p>\n\n<ul>\n<li>Uninstalling all VS extensions</li>\n<li>Restarting the editor</li>\n<li>Restarting my machine</li>\n<li>Running a repair on visual studio installation</li>\n</ul>\n\n<p>None of these have worked.  My colleague is also experiencing this slowdown.</p>\n\n<p>Has anyone else experienced this?  Does anyone know of anything I can try to diagnose what in the build is slow?</p>\n"},{"tags":["asp.net","performance","browser","count","render"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":18,"score":0,"question_id":12745922,"title":"Count time browser render content","body":"<p>How can I count time browser render (re-render) my aspx page after page load? </p>\n\n<p>(I have a lot of components on the page,so I need to compare perfomance of different approachs)</p>\n"},{"tags":["java","performance","algorithm","min"],"answer_count":9,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":1424,"score":0,"question_id":9576557,"title":"Most efficient way to find smallest of 3 numbers Java?","body":"<p>I have an algorithm written in Java that I would like to make more efficient. A part that I think could be made more efficient is finding the smallest of 3 numbers. Currently I'm using the <code>Math.min</code> method as below:</p>\n\n<pre><code>double smallest = Math.min(a, Math.min(b, c));\n</code></pre>\n\n<p>How efficient is this? Would it be more efficient to replace with if statements like below:</p>\n\n<pre><code>double smallest;\nif(a&lt;b &amp;&amp; a&lt;c){\n    smallest = a;\n}else if(b&lt;c &amp;&amp; b&lt;a){\n    smallest = b;\n}else{\n    smallest = c;\n}\n</code></pre>\n\n<p>Or if any other way is more efficient</p>\n\n<p>I'm wondering if it is worth changing what I'm currently using?</p>\n\n<p>Any speed increase would be greatly helpful</p>\n"},{"tags":["performance","haskell"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":125,"score":2,"question_id":12739604,"title":"Statement for checking only once?Haskell","body":"<p>I have two lists of unequal length. When I add both of them I want the final list to have the length of the longest list.</p>\n\n<pre><code>addtwolists [0,0,221,2121] [0,0,0,99,323,99,32,2332,23,23]\n&gt;[0,0,221,2220,323,99,32,2332,23,23]\naddtwolists [945,45,4,45,22,34,2] [0,34,2,34,2]\n&gt;[945,79,6,79,24,34,2]\n\nzerolist :: Int -&gt; [Integer]\nzerolist x = take x (repeat 0)\n\naddtwolists :: [Integer] -&gt; [Integer] -&gt; [Integer]\naddtwolists x y = zipWith (+) (x ++ (zerolist ((length y)-(length x)))) (y ++ (zerolist ((length x)-(length y))))\n</code></pre>\n\n<p>This code is inefficient. So I tried:</p>\n\n<pre><code>addtwolist :: [Integer] -&gt; [Integer] -&gt; [Integer]\naddtwolist x y = zipWith (+) (x ++ [head (zerolist ((length y)-(length x))) | (length y) &gt; (length x)]) (y ++ [head (zerolist ((length x)-(length y))) | (length x) &gt; (length y)]) \n</code></pre>\n\n<p>Any other way to increase the efficiency?Could you only check once to see which list is bigger?</p>\n"},{"tags":["c#","performance","comparison","appfabric","distributed-caching"],"answer_count":1,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":158,"score":5,"question_id":12647232,"title":"Appfabric Cache is performing 4x slower than SQL Server 2008 ?? ","body":"<p>I am running a test where I am comparing fetch time b/w appfabric and SQL Server 2008 and looks appFabric is performing 4x time slower than SQL Server.</p>\n\n<p>I have a SQL Server 2008 setup which contains only one table with 4 columns (all <code>nvarchar</code>). The table has 6000 rows. I insert the same row (as CLR serializable obj) in the appfabric cache. I am running a loop to fetch data x times.</p>\n\n<p>Here is the code</p>\n\n<pre><code>public class AppFabricCache\n{\nreadonly DataCache myDefaultCache;\n\npublic AppFabricCache()\n{\n//-------------------------\n// Configure Cache Client \n//-------------------------\n\n//Define Array for 1 Cache Host\nvar servers = new List&lt;DataCacheServerEndpoint&gt;(1);\n\n//Specify Cache Host Details \n//  Parameter 1 = host name\n//  Parameter 2 = cache port number\nservers.Add(new DataCacheServerEndpoint(@\"localhost\", 22233));\n\n//Create cache configuration\nvar configuration = new DataCacheFactoryConfiguration();\n\n//Set the cache host(s)\nconfiguration.Servers = servers;\n\n//Set default properties for local cache (local cache disabled)\nconfiguration.LocalCacheProperties = new DataCacheLocalCacheProperties();\n\n//Disable exception messages since this sample works on a cache aside\nDataCacheClientLogManager.ChangeLogLevel(System.Diagnostics.TraceLevel.Off);\n\n//Pass configuration settings to cacheFactory constructor\nDataCacheFactory myCacheFactory = new DataCacheFactory(configuration);\n\n//Get reference to named cache called \"default\"\nmyDefaultCache = myCacheFactory.GetCache(\"default\");\n}\n\npublic bool TryGetCachedObject(string key, out object value)\n{\nvalue = myDefaultCache.Get(key);\nbool result = value != null;\nreturn result;\n}\n\npublic void PutItemIntoCache(string key, object value)\n{\nmyDefaultCache.Put(key, value, TimeSpan.FromDays(365));\n}\n\n}\n</code></pre>\n\n<p>And here is the loop to fetch data from the cache</p>\n\n<pre><code>public double RunReadStressTest(int numberOfIterations, out int recordReadCount)\n{\nrecordReadCount = 0;\nStopwatch sw = Stopwatch.StartNew();\nfor (int i = 0; i &lt; numberOfIterations; i++)\n{\nfor (int j = 1; j &lt;= 6000; j++)\n{\nstring posId = \"PosId-\" + j;\ntry\n{\nobject value;\nif (TryGetCachedObject(posId, out value))\nrecordReadCount++;\n}\ncatch (Exception e)\n{\nTrace.WriteLine(\"AS%% - Exception - \" + e.Message);\n}\n}\n}\nsw.Stop();\nreturn sw.ElapsedMilliseconds;\n}\n}\n</code></pre>\n\n<p>I have exactly the same logic to retrieve data from SQL Server. It creates a </p>\n\n<pre><code>sqlCommand = 'Select * from TableName where posId = 'someId'' \n</code></pre>\n\n<p>Here are the results...</p>\n\n<pre><code>SQL Server 2008 R2  Reading-1(ms)   Reading-2(ms)   Reading-3(ms)   Average Time in Seconds\n Iteration Count = 5    2528              2649            2665                 2.614\n Iteration Count = 10   5280              5445            5343                 5.356\n Iteration Count = 15   7978              8370            7800                 8.049333333\n Iteration Count = 20   9277              9643            10220                9.713333333\n\nAppFabric                 Reading-1         Reading-2   Reading-3   Average Time in Seconds\nIteration Count = 5        10301            10160            10186                10.21566667\nIteration Count = 10       20130            20191            20650                20.32366667\nIteration Count = 15       30747            30571            30647                30.655\nIteration Count = 20       40448            40541            40503                40.49733333\n</code></pre>\n\n<p>Am I missing something here? Why it is so slow?</p>\n"},{"tags":["mysql","performance","optimization","query-performance","phpfox"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":78,"score":-1,"question_id":12744194,"title":"I am trying to speed up following MySQL query","body":"<p>Have got this MySQL query and it's loading very slow , around 23 seconds. Could you please help me to optimise it, the problematic part is in 3rd paragraph below. This query is pulling out feeds from PhpFox, so all the activities users made.</p>\n\n<pre><code>SELECT feed.*, u.user_group_id, f.friend_id AS is_friend, apps.app_title, u.view_id, u.user_id, u.profile_page_id, u.server_id AS user_server_id, u.user_name, u.full_name, u.gender, u.user_image, u.user_group_id, u.is_invisible, u.last_activity \nFROM( \n(SELECT feed.* \nFROM phpfox_feed AS feed \nJOIN phpfox_friend AS f \nON(f.user_id = feed.user_id AND f.friend_user_id = 0) \n\nWHERE feed.privacy IN(1,2) AND feed.time_stamp &gt; '0' AND feed.feed_reference = 0 \n/* OO Query */) UNION (SELECT feed.* \nFROM phpfox_feed AS feed \nJOIN phpfox_friend AS f1 \nON(f1.user_id = feed.user_id) \nJOIN phpfox_friend AS f2 \nON(f2.user_id = 0 AND f2.friend_user_id = f1.friend_user_id) \n\nWHERE feed.privacy IN(2) AND feed.time_stamp &gt; '0' AND feed.feed_reference = 0 \n/* OO Query */) UNION (SELECT feed.* \nFROM phpfox_feed AS feed \nWHERE feed.privacy IN(1,2,3,4) AND feed.user_id = 0 AND feed.time_stamp &gt; '0' AND feed.feed_reference = 0 \n/* OO Query */) UNION (SELECT feed.* \nFROM phpfox_feed AS feed \nWHERE feed.privacy IN(0) AND feed.time_stamp &gt; '0' AND feed.feed_reference = 0 \n/* OO Query */) UNION (SELECT feed.* \nFROM phpfox_feed AS feed \nJOIN phpfox_privacy AS p \nON(p.module_id = feed.type_id AND p.item_id = feed.item_id) \nJOIN phpfox_friend_list_data AS fld \nON(fld.list_id = p.friend_list_id AND fld.friend_user_id = 0) \n\nWHERE feed.privacy IN(4) AND feed.time_stamp &gt; '0' AND feed.feed_reference = 0 \n/* OO Query */)) AS feed \nJOIN phpfox_user AS u \nON(u.user_id = feed.user_id) \nLEFT JOIN phpfox_friend AS f \nON(f.user_id = feed.user_id AND f.friend_user_id = 0) \nLEFT JOIN phpfox_app AS apps \nON(apps.app_id = feed.app_id) \n\nGROUP BY feed.feed_id \nORDER BY feed.time_stamp DESC \nLIMIT 2 \n</code></pre>\n"},{"tags":["java","performance","hibernate"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":101,"score":2,"question_id":12710797,"title":"How to reduce Hibernate SessionFactory memory usage and load time","body":"<p>I have an application with about 3000 entity classes.  </p>\n\n<p>Due to the large number of classes the SessionFactory object consumes about 150 MB of memory and takes almost a minute to setup (process all classes, generate the proxies and building the meta model).<br>\nAfter profiling the process I've discovered that a minute of the time is spent in <code>org.hibernate.ejb.Ejb3Configuration.buildEntityManagerFactory</code> while half is in <code>org.hibernate.tuple.entity.AbstractEntityTuplizer.&lt;init&gt;(EntityMetamodel, PersistentClass)</code>.<br>\nI'm looking for ways to reduce the <code>SessionFactory</code> memory usage and load time.<br>\nOne thought was to move the class enhancement to compile time, but I didn't find much information about it.<br>\nThe number of entities is a fact that I can't change but if someone has better ideas on how to handle their database access (other than Hibernate) I'll be happy to hear about it.</p>\n"},{"tags":["performance","transactions","application-server","message-driven-bean"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":59,"score":1,"question_id":11019408,"title":"Get the most out of high performance MDB","body":"<p>Application server creates a new transaction before calling MDB's onMessage method. Also I am processing database update in onMessage method. Transactions create additional overhead and processing several message in one transaction could increase performance.</p>\n\n<p>Is it possible to make App server to use one transaction for several messages. Or maybe there are other approaches to this problem?</p>\n\n<p>And, by the way, I can't use multiple instances, cause I need to preserve the sequence order. </p>\n"},{"tags":["mysql","sql","performance","magento"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":88,"score":2,"question_id":12596151,"title":"How to make Magento saving product faster?","body":"<p><strong>Everything below this line is out of date. Magento is just slow, nothing less and nothing more.</strong></p>\n\n<hr>\n\n<p>Magento is extremely slow as said in <a href=\"http://stackoverflow.com/questions/12580828/magento-saving-product-is-extremly-slow-but-profiler-shows-it-only-takes-1sec/12583078#12583078\">Magento saving product is extremly slow, but profiler shows it only takes 1sec</a></p>\n\n<p>After some struggling due to lack of root privilege on HostGator, I end up profiling Magento calls myself.</p>\n\n<p>Here's one of those results:</p>\n\n<p><code>Blue: timing 1982878436 Mage_Sales_Model_Mysql4_Quote begin</code> &lt;- this is logged when entering <code>Mage_Sales_Model_Mysql4_Quote</code>'s <code>save</code> method.</p>\n\n<p><code>Blue: timing 1982878436 Mage_Sales_Model_Mysql4_Quote 46</code> &lt;- and this is logged when exiting.</p>\n\n<p>The number <code>1982878436</code> is a random number generated as the id of the call. And number <code>46</code> is the time taken in seconds.</p>\n\n<pre><code>2012-09-26T06:36:16+00:00 DEBUG (7): Blue: timing 1982878436 Mage_Sales_Model_Mysql4_Quote begin\n2012-09-26T06:36:18+00:00 DEBUG (7): Blue: timing 645597828 Mage_Log_Model_Mysql4_Visitor begin\n2012-09-26T06:36:18+00:00 DEBUG (7): Blue: 645597828 Varien_Db_Adapter_Pdo_Mysql\n2012-09-26T06:36:18+00:00 DEBUG (7): Blue: timing 645597828 Mage_Log_Model_Mysql4_Visitor 0\n2012-09-26T06:36:18+00:00 DEBUG (7): Blue: timing 1712949075 Mage_Sales_Model_Mysql4_Quote begin\n2012-09-26T06:36:24+00:00 DEBUG (7): Blue: timing 2103820838 Mage_Sales_Model_Mysql4_Quote begin\n2012-09-26T06:36:56+00:00 DEBUG (7): Blue: timing 1999314779 Mage_Log_Model_Mysql4_Visitor begin\n2012-09-26T06:36:56+00:00 DEBUG (7): Blue: 1999314779 Varien_Db_Adapter_Pdo_Mysql\n2012-09-26T06:36:56+00:00 DEBUG (7): Blue: timing 1999314779 Mage_Log_Model_Mysql4_Visitor 0\n2012-09-26T06:36:56+00:00 DEBUG (7): Blue: timing 504509596 Mage_Sales_Model_Mysql4_Quote begin\n2012-09-26T06:36:56+00:00 DEBUG (7): Blue: timing 1887845167 Mage_Log_Model_Mysql4_Visitor begin\n2012-09-26T06:36:56+00:00 DEBUG (7): Blue: timing 1887845167 Mage_Log_Model_Mysql4_Visitor 0\n2012-09-26T06:37:02+00:00 DEBUG (7): Blue: timing 1887308594 Mage_GoogleOptimizer_Model_Mysql4_Code begin\n2012-09-26T06:37:02+00:00 DEBUG (7): Blue: timing 1887308594 Mage_GoogleOptimizer_Model_Mysql4_Code 0\n2012-09-26T06:37:02+00:00 DEBUG (7): Blue: 504509596 Varien_Db_Adapter_Pdo_Mysql\n2012-09-26T06:37:02+00:00 DEBUG (7): Blue: timing 504509596 Mage_Sales_Model_Mysql4_Quote 6\n2012-09-26T06:37:02+00:00 DEBUG (7): Blue: 1982878436 Varien_Db_Adapter_Pdo_Mysql\n2012-09-26T06:37:02+00:00 DEBUG (7): Blue: timing 1982878436 Mage_Sales_Model_Mysql4_Quote 46\n2012-09-26T06:37:02+00:00 DEBUG (7): Blue: 1712949075 Varien_Db_Adapter_Pdo_Mysql\n2012-09-26T06:37:02+00:00 DEBUG (7): Blue: timing 1712949075 Mage_Sales_Model_Mysql4_Quote 44\n2012-09-26T06:37:02+00:00 DEBUG (7): Blue: 2103820838 Varien_Db_Adapter_Pdo_Mysql\n2012-09-26T06:37:02+00:00 DEBUG (7): Blue: timing 2103820838 Mage_Sales_Model_Mysql4_Quote 38\n</code></pre>\n\n<p>As we can see <code>1982878436</code>, <code>1712949075</code>, <code>2103820838</code> are called in parallel, and each took several tens of seconds to finish. I suspect that there is some lock issue among the three calls make them waiting for each other. Sometimes when I'm saving a product, Magento will even report action being failed because MySQL is failed due to deadlock.</p>\n\n<p>Anyone has any ideas about this?</p>\n"},{"tags":["objective-c","cocoa","performance","osx","bezier"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":541,"score":1,"question_id":4433522,"title":"Bezier path stroking performance issues","body":"<p>My code goes as following -</p>\n\n<pre><code>[[NSColor whiteColor] set];\n// `path' is a bezier path with more than 1000 points in it\n[path setLineWidth:2];\n[path setLineJoinStyle:NSRoundLineJoinStyle];\n[path stroke];\n// some other stuff...\n</code></pre>\n\n<p>Running the time profiling tool in Instruments it tells me my app is spending 93.5% of the time doing the last line <code>[path stroke]</code>, and Quartz Debugger tells me my app is only running at less than 10 fps (another view changing position on top of it is always causing the update).</p>\n\n<p>I'm looking for ways to improve the performance of stroking the bezier path, sometimes paths with more than 1000 points draws very quickly with >60fps, however in some extreme cases even with the same number of points, perhaps if the points are too far from each other (or too dense?) the performance becomes really sluggish.</p>\n\n<p>I'm not sure what I can do about this. I think caching the view as a bitmap rep is helpful, but it can't really help with live resize.</p>\n\n<p>Edit: commenting out the line <code>[path setLineWidth:2];</code> certainly helps, but the path looked really too 'thin'.</p>\n"},{"tags":["iphone","ios","performance","uiscrollview"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":75,"score":0,"question_id":11777709,"title":"UIScrollView or custom drawing","body":"<p>I am facing the following problem in my current project: I want to implement something like the following:</p>\n\n<p><img src=\"http://i.stack.imgur.com/3H1k3.png\" alt=\"enter image description here\"></p>\n\n<p>Source: <a href=\"https://github.com/NOUSguide/NGVaryingGridView\" rel=\"nofollow\">https://github.com/NOUSguide/NGVaryingGridView</a></p>\n\n<p>Since I have to draw an EPG for maybe 40 Stations or more for maybe a week, my question is: Should I fill a UIScrollView with many many subviews (representing the shows) or is it better for the performance if I draw one big UIView using Quartz2D and then add it as a single subview of the UIScrollView?</p>\n\n<p>Hope you could follow me,\nThanks in advance,</p>\n\n<p>Christian </p>\n"},{"tags":["python","performance","powershell","search-and-replace"],"answer_count":4,"favorite_count":2,"up_vote_count":12,"down_vote_count":0,"view_count":903,"score":12,"question_id":9724521,"title":"PowerShell is slow (much slower than Python) in large Search/Replace operation?","body":"<p>I have 265 CSV files with over 4 million total records (lines), and need to do a search and replace in all the CSV files. I have a snippet of my PowerShell code below that does this, but it takes 17 minutes to perform the action:</p>\n\n<pre><code>ForEach ($file in Get-ChildItem C:\\temp\\csv\\*.csv) \n{\n    $content = Get-Content -path $file\n    $content | foreach {$_ -replace $SearchStr, $ReplaceStr} | Set-Content $file\n}\n</code></pre>\n\n<p>Now I have the following Python code that does the same thing but takes less than 1 minute to perform:</p>\n\n<pre><code>import os, fnmatch\n\ndef findReplace(directory, find, replace, filePattern):\n    for path, dirs, files in os.walk(os.path.abspath(directory)):\n        for filename in fnmatch.filter(files, filePattern):\n            filepath = os.path.join(path, filename)\n            with open(filepath) as f:\n                s = f.read()\n            s = s.replace(find, replace)\n            with open(filepath, \"w\") as f:\n                f.write(s)\n\nfindReplace(\"c:/temp/csv\", \"Search String\", \"Replace String\", \"*.csv\")\n</code></pre>\n\n<p>Why is the Python method so much more efficient? Is my PowerShell code in-efficient, or is Python just a more powerful programming language when it comes to text manipulation?</p>\n"},{"tags":["performance","algorithm","big-o"],"answer_count":17,"favorite_count":37,"up_vote_count":47,"down_vote_count":0,"view_count":52350,"score":47,"question_id":251781,"title":"How to find the kth largest element in an unsorted array of length n in O(n)?","body":"<p>I believe there's a way to find the kth largest element in an unsorted array of length n in O(n).  Or perhaps it's \"expected\" O(n) or something.  How can we do this?</p>\n"},{"tags":["jquery","performance","jquery-mobile","browserfield","blackberry-os5"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":59,"score":1,"question_id":12739455,"title":"Blackberry OS 5 web application performance tuning","body":"<p>We have a web application in jquerymobile 1.0.1 and jquery 1.7.1. For BB devices we have created a wrapper application using BrowserField JDE 5 API, which is used to launch the application UI. Using web DOM cache to work in offline mode.</p>\n\n<p>Performance wise the application works good on OS6+ devices, but on OS5+ device it is a real pain. Browsing application is too slow.</p>\n\n<p>We have 3 out of 6 pages due to there dynamic nature; they are scripted in jquery.</p>\n\n<p>Please suggest few good performance tuning steps that would help in improving the application performance.</p>\n"},{"tags":["c++","performance"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":67,"score":1,"question_id":12739014,"title":"Timing difference/increase different when using `%` vs `&`","body":"<p>In trying to determine the cache size for a given CPU, I tried to time the memory access to memory/cache like: </p>\n\n<pre><code>lengthMod = sizes[i]/sizeof(int)  - 1; // where sizes[i] is something like 1024, 2048 ... \nfor (unsigned int k = 0; k &lt; REPS; k++) {\n    data[(k * 16) &amp; lengthMod]++;\n}\n\n1, 0.52 \n4, 0.52 \n8, 0.52 \n16, 0.52 \n32, 0.52 \n64, 1.11 // &lt;&lt; note the jump in timing. L1 cache size is 32K\n128, 1.12 \n256, 1.19 \n</code></pre>\n\n<p>So I think if the lengthMod is not a power of 2, I cant do this. So I tried doing </p>\n\n<pre><code>lengthMod = sizes[i]/sizeof(int);\nfor (unsigned int k = 0; k &lt; REPS; k++) {\n    data[(k * 16) % lengthMod]++;\n}\n\n1, 2.67 \n4, 2.57 \n8, 2.55 \n16, 2.51 \n32, 2.42 \n64, 2.42 // &lt;&lt; no jump anymore ...\n128, 2.42 \n256, 2.42\n</code></pre>\n\n<p>Then I find that the timing increase that I expected is non-existant anymore ... I expected the time to increase but it should apply to all values? So if its <code>x</code> seconds when using <code>&amp;</code>, I'd expect <code>~x+c</code> seconds (where <code>c</code> is approximatly constant), but thats not the case, in fact, it reduces the timing difference to non-existant why is that? </p>\n"},{"tags":["c++","performance","optimization","micro-optimization"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":71,"score":1,"question_id":12739107,"title":"addition vs comparison","body":"<p>Let say I have an array of 1,000,000 elements with about 90% of 0s and 10% of 1s.</p>\n\n<p>In order to count of 1s, I can do</p>\n\n<pre><code>sum=0;\nfor(int i=0;i&lt;size;i++) {\n    sum+=x[i]\n}\n</code></pre>\n\n<p>But I thought maybe comparison is cheaper than addition so this would be better.</p>\n\n<pre><code>sum=0;\nfor(int i=0;i&lt;size;i++) {\n    if(x[i]==1)\n        sum++;\n}\n</code></pre>\n\n<p>But I am not sure. Which one is faster?</p>\n"},{"tags":["performance","3d","marching-cubes"],"answer_count":4,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":1600,"score":2,"question_id":868831,"title":"How to speed up marching cubes?","body":"<p>I'm using <a href=\"http://local.wasp.uwa.edu.au/~pbourke/geometry/polygonise/\" rel=\"nofollow\">this marching cube algorithm</a> to draw 3D isosurfaces (ported into C#, outputting <code>MeshGeomtry3D</code>s, but otherwise the same). The resulting surfaces look great, but are taking a long time to calculate.</p>\n\n<p>Are there any ways to speed up marching cubes? The most obvious one is to simply reduce the spatial sampling rate, but this reduces the quality of the resulting mesh. I'd like to avoid this.</p>\n\n<p>I'm considering a two-pass system, where the first pass samples space much more coarsely, eliminating volumes where the field strength is well below my isolevel. Is this wise? What are the pitfalls?</p>\n\n<p><strong>Edit:</strong> the code has been profiled, and the bulk of CPU time is split between the marching cubes routine itself and the field strength calculation for each grid cell corner. The field calculations are beyond my control, so speeding up the cubes routine is my only option...</p>\n\n<p>I'm still drawn to the idea of trying to eliminate dead space, since this would reduce the number of calls to both systems considerably. </p>\n"},{"tags":["performance","http","httpwebrequest"],"answer_count":3,"favorite_count":0,"up_vote_count":9,"down_vote_count":0,"view_count":602,"score":9,"question_id":5987031,"title":"HTTP request cost vs. page size cost?","body":"<p>I know it's a good practice to minimize the number of requests each page needs.  For example, combining javascript files and using css sprites will greatly reduce the number of requests needed to render your page.</p>\n\n<p>Another approach I've seen is to keep javascript embedded in the page itself, especially for javascript specific to that page and not really shared across other pages.</p>\n\n<p>But my question is this:</p>\n\n<p>At what point does my javascript grow too large that it becomes more efficient to pull the script into a separate file and allow the additional request for the separate js file?</p>\n\n<p>In other words, how do I measure how much bytes equates to the cost of one request?</p>\n\n<p>Since successive requests are cached, the only cost of calling that same js file is the cost of the request.  Whereas keeping the js in the page will always incur the cost of additional page size, but will not incur the cost of an additional request.</p>\n\n<p>Of course, I know several factors go into this: speed of the client, bandwidth speed, latency.  But there has to be a turning point to where it makes more sense to do one over the other.  </p>\n\n<p>Or, is bandwidth so cheap (speed, not money) these days that it requires many more bytes than it used to in order to exceed the cost of a request?  It seems to be the trend that page size is become less of a factor, while the cost of a request has plateaued.</p>\n\n<p>Thoughts?</p>\n"},{"tags":["java","multithreading","performance","executorservice","cyclicbarrier"],"answer_count":6,"favorite_count":4,"up_vote_count":9,"down_vote_count":0,"view_count":1508,"score":9,"question_id":2712232,"title":"What is the fastest cyclic synchronization in Java (ExecutorService vs. CyclicBarrier vs. X)?","body":"<p>Which Java synchronization construct is likely to provide the best \nperformance for a concurrent, iterative processing scenario with a\nfixed number of threads like the one outlined below? After experimenting \non my own for a while (using ExecutorService and CyclicBarrier) and\nbeing somewhat surprised by the results, I would be grateful for some \nexpert advice and maybe some new ideas. Existing questions here do \nnot seem to focus primarily on performance, hence this new one. \nThanks in advance! </p>\n\n<p>The core of the app is a simple iterative data processing algorithm,\nparallelized to the spread the computational load across 8 cores on\na Mac Pro, running OS X 10.6 and Java 1.6.0_07. The data to be processed\nis split into 8 blocks and each block is fed to a Runnable to be executed \nby one of a fixed number of threads. Parallelizing the algorithm was\nfairly straightforward, and it functionally works as desired, but\nits performance is not yet what I think it could be. The app seems\nto spend a lot of time in system calls synchronizing, so after some\nprofiling I wonder whether I selected the most appropriate\nsynchronization mechanism(s).</p>\n\n<p>A key requirement of the algorithm is that it needs to proceed in \nstages, so the threads need to sync up at the end of each stage. \nThe main thread prepares the work (very low overhead), passes it to \nthe threads, lets them work on it, then proceeds when all threads \nare done, rearranges the work (again very low overhead) and repeats \nthe cycle. The machine is dedicated to this task, Garbage Collection\nis minimized by using per-thread pools of pre-allocated items, and\nthe number of threads can be fixed (no incoming requests or the like,\njust one thread per CPU core).</p>\n\n<h2>V1 - ExecutorService</h2>\n\n<p>My first implementation used an ExecutorService with 8 worker\nthreads. The program creates 8 tasks holding the work and then\nlets them work on it, roughly like this:</p>\n\n<pre><code>// create one thread per CPU\nexecutorService = Executors.newFixedThreadPool( 8 );\n...\n// now process data in cycles\nwhile( ...) {\n    // package data into 8 work items\n    ...\n\n    // create one Callable task per work item\n    ...\n\n    // submit the Callables to the worker threads\n    executorService.invokeAll( taskList );\n}\n</code></pre>\n\n<p>This works well functionally (it does what it should), and for\nvery large work items indeed all 8 CPUs become highly loaded, as\nmuch as the processing algorithm would be expected to allow (some\nwork items will finish faster than others, then idle). However,\nas the work items become smaller (and this is not really under\nthe program's control), the user CPU load shrinks dramatically:</p>\n\n<pre><code>blocksize | system | user | cycles/sec\n256k        1.8%    85%     1.30\n64k         2.5%    77%     5.6\n16k         4%      64%     22.5\n4096        8%      56%     86\n1024       13%      38%     227\n256        17%      19%     420\n64         19%      17%     948\n16         19%      13%     1626\n</code></pre>\n\n<p>Legend:\n - block size = size of the work item (= computational steps)\n - system = system load, as shown in OS X Activity Monitor (red bar)\n - user = user load, as shown in OS X Activity Monitor (green bar)\n - cycles/sec = iterations through the main while loop, more is better</p>\n\n<p>The primary area of concern here is the high percentage of time spent\nin the system, which appears to be driven by thread synchronization\ncalls. As expected, for smaller work items, ExecutorService.invokeAll()\nwill require relatively more effort to sync up the threads\nversus the amount of work being performed in each thread. But\nsince ExecutorService is more generic than it would need to be\nfor this use case (it can queue tasks for threads if there are\nmore tasks than cores), I though maybe there would be a leaner\nsynchronization construct.</p>\n\n<h2>V2 - CyclicBarrier</h2>\n\n<p>The next implementation used a CyclicBarrier to sync up\nthe threads before receiving work and after completing it,\nroughly as follows:</p>\n\n<pre><code>main() {\n    // create the barrier\n    barrier = new CyclicBarrier( 8 + 1 );\n\n    // create Runable for thread, tell it about the barrier\n    Runnable task = new WorkerThreadRunnable( barrier );\n\n    // start the threads\n    for( int i = 0; i &lt; 8; i++ )\n    {\n        // create one thread per core\n        new Thread( task ).start();\n    }\n\n    while( ... ) {\n        // tell threads about the work\n        ...\n\n        // N threads + this will call await(), then system proceeds\n        barrier.await();\n\n        // ... now worker threads work on the work...\n\n        // wait for worker threads to finish\n        barrier.await();\n    }\n}\n\nclass WorkerThreadRunnable implements Runnable {\n    CyclicBarrier barrier;\n\n    WorkerThreadRunnable( CyclicBarrier barrier ) { this.barrier = barrier; }\n\n    public void run()\n    {\n        while( true )\n        {\n            // wait for work\n            barrier.await();\n\n            // do the work\n            ...\n\n            // wait for everyone else to finish\n            barrier.await();\n        }\n    }\n}\n</code></pre>\n\n<p>Again, this works well functionally (it does what it should),\nand for very large work items indeed all 8 CPUs become highly\nloaded, as before. However, as the work items become smaller,\nthe load still shrinks dramatically:</p>\n\n<pre><code>blocksize | system | user | cycles/sec\n256k        1.9%     85%    1.30\n64k         2.7%     78%    6.1\n16k         5.5%     52%    25\n4096        9%       29%    64\n1024       11%       15%    117\n256        12%        8%    169\n64         12%        6.5%  285\n16         12%        6%    377\n</code></pre>\n\n<p>For large work items, synchronization is negligible and the\nperformance is identical to V1. But unexpectedly, the results\nof the (highly specialized) CyclicBarrier seem MUCH WORSE than\nthose for the (generic) ExecutorService: throughput (cycles/sec)\nis only about 1/4th of V1. A preliminary conclusion would be \nthat even though this seems to be the advertised ideal use \ncase for CyclicBarrier, it performs much worse than the \ngeneric ExecutorService.</p>\n\n<h2>V3 - Wait/Notify + CyclicBarrier</h2>\n\n<p>It seemed worth a try to replace the first cyclic barrier await()\nwith a simple wait/notify mechanism:</p>\n\n<pre><code>main() {\n    // create the barrier\n    // create Runable for thread, tell it about the barrier\n    // start the threads\n\n    while( ... ) {\n        // tell threads about the work\n        // for each: workerThreadRunnable.setWorkItem( ... );\n\n        // ... now worker threads work on the work...\n\n        // wait for worker threads to finish\n        barrier.await();\n    }\n}\n\nclass WorkerThreadRunnable implements Runnable {\n    CyclicBarrier barrier;\n    @NotNull volatile private Callable&lt;Integer&gt; workItem;\n\n    WorkerThreadRunnable( CyclicBarrier barrier ) { this.barrier = barrier; this.workItem = NO_WORK; }\n\n    final protected void\n    setWorkItem( @NotNull final Callable&lt;Integer&gt; callable )\n    {\n        synchronized( this )\n        {\n            workItem = callable;\n            notify();\n        }\n    }\n\n    public void run()\n    {\n        while( true )\n        {\n            // wait for work\n            while( true )\n            {\n                synchronized( this )\n                {\n                    if( workItem != NO_WORK ) break;\n\n                    try\n                    {\n                        wait();\n                    }\n                    catch( InterruptedException e ) { e.printStackTrace(); }\n                }\n            }\n\n            // do the work\n            ...\n\n            // wait for everyone else to finish\n            barrier.await();\n        }\n    }\n}\n</code></pre>\n\n<p>Again, this works well functionally (it does what it should).</p>\n\n<pre><code>blocksize | system | user | cycles/sec\n256k        1.9%     85%    1.30\n64k         2.4%     80%    6.3\n16k         4.6%     60%    30.1\n4096        8.6%     41%    98.5\n1024       12%       23%    202\n256        14%       11.6%  299\n64         14%       10.0%  518\n16         14.8%      8.7%  679\n</code></pre>\n\n<p>The throughput for small work items is still much worse than that\nof the ExecutorService, but about 2x that of the CyclicBarrier.\nEliminating one CyclicBarrier eliminates half of the gap.</p>\n\n<h2>V4 - Busy wait instead of wait/notify</h2>\n\n<p>Since this app is the primary one running on the system and\nthe cores idle anyway if they're not busy with a work item,\nwhy not try a busy wait for work items in each thread, even if\nthat spins the CPU needlessly. The worker thread code changes\nas follows:</p>\n\n<pre><code>class WorkerThreadRunnable implements Runnable {\n    // as before\n\n    final protected void\n    setWorkItem( @NotNull final Callable&lt;Integer&gt; callable )\n    {\n        workItem = callable;\n    }\n\n    public void run()\n    {\n        while( true )\n        {\n            // busy-wait for work\n            while( true )\n            {\n                if( workItem != NO_WORK ) break;\n            }\n\n            // do the work\n            ...\n\n            // wait for everyone else to finish\n            barrier.await();\n        }\n    }\n}\n</code></pre>\n\n<p>Also works well functionally (it does what it should).</p>\n\n<pre><code>blocksize | system | user | cycles/sec\n256k        1.9%     85%    1.30\n64k         2.2%     81%    6.3\n16k         4.2%     62%     33\n4096        7.5%     40%    107\n1024       10.4%     23%    210\n256        12.0%    12.0%   310\n64         11.9%    10.2%   550\n16         12.2%     8.6%   741\n</code></pre>\n\n<p>For small work items, this increases throughput by a further\n10% over the CyclicBarrier + wait/notify variant, which is not \ninsignificant. But it is still much lower-throughput than V1 \nwith the ExecutorService.</p>\n\n<h2>V5 - ?</h2>\n\n<p>So what is the best synchronization mechanism for such a\n(presumably not uncommon) problem? I am weary of writing my\nown sync mechanism to completely replace ExecutorService\n(assuming that it is too generic and there has to be something\nthat can still be taken out to make it more efficient).\nIt is not my area of expertise and I'm concerned that I'd\nspend a lot of time debugging it (since I'm not even sure\nmy wait/notify and busy wait variants are correct) for\nuncertain gain.</p>\n\n<p>Any advice would be greatly appreciated.</p>\n"},{"tags":["performance","matlab","matrix","time-complexity","matrix-multiplication"],"answer_count":2,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":146,"score":5,"question_id":12721912,"title":"Matlab matrix multiplication speed","body":"<p>I was wondering how can matlab multiply two matrices so fast. When multiplying two NxN matrices, N^3 multiplications are performed.  Even with the <a href=\"http://en.wikipedia.org/wiki/Strassen_algorithm\" rel=\"nofollow\">Strassen Algorithm</a> it takes N^2.8 multiplications, which is still a large number. I was running the following test program:</p>\n\n<pre><code>a = rand(2160);\nb = rand(2160);\ntic;a*b;toc\n</code></pre>\n\n<p>2160 was used because 2160^3=~10^10 ( a*b should be about  10^10 multiplications)</p>\n\n<p>I got:</p>\n\n<pre><code>Elapsed time is 1.164289 seconds.\n</code></pre>\n\n<p>(I'm running on 2.4Ghz notebook and no threading occurs)\nwhich mean my computer made ~10^10 operation in a little more than 1 second. </p>\n\n<p>How this could be??</p>\n"},{"tags":["c++","performance","gmp","rational-numbers"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":75,"score":0,"question_id":12733557,"title":"Fastest C++ Rational Number library","body":"<p>I'm using <a href=\"http://gmplib.org/manual/C_002b_002b-Interface-Rationals.html#C_002b_002b-Interface-Rationals\" rel=\"nofollow\">GMPXX</a> wrapper of <a href=\"http://gmplib.org/\" rel=\"nofollow\">GMP</a> and it is not fast enough. Is it possible to find some comparison of rational number libraries' speed?</p>\n\n<p>During my calculation a very big rational number will appear with 10^100 denominator and same size numerator.</p>\n\n<p>Do you know something faster than GMP?</p>\n"},{"tags":[".net","visual-studio-2010","performance","profiler","performance-testing"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":49,"score":1,"question_id":12732900,"title":"Effects of profiler itself on performance measurement in .Net","body":"<p>What are the effects of running, say, the Visual Studio 2010 profiler on the performance of my program?  That is, in what ways are the results from a profiler actually inaccurate due to how the profiler has to \"interfere\" with the program's normal operation to check the performance?  Or are there any such issues?</p>\n\n<p>This question stems from the fact that I'm seeing property gets/sets as taking up significant amounts of time in my methods in the Visual Studio 2010 profiler (I program in C#).  These properties are the ones that are automatically generated by a .resx Resource Dictionary (avoiding magic strings :-) ), and so I would <em>assume</em> that the JIT compiler and/or the interpreter running behind the scenes <em>generally</em> would inline them if it would be helpful.  I would presume though that such inlining doesn't happen when you are running the profiler...  Or does it?  </p>\n\n<p>I am specifically working with the Visual Studio 2010 profiler, but I would appreciate an answer that includes other profilers.</p>\n\n<p>Sorry if I have missed a question that answers this already- feel free to point the way.</p>\n"},{"tags":["performance","linux-kernel","performancecounter","perf"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":44,"score":0,"question_id":12733402,"title":"'perf top' command doesn't run properly on virtual machine","body":"<p>I got a virtual machine running Ubuntu 12.04 with linux kernel 3.2.0-31. When I try to run the <code>sudo perf top</code> command, which can profile the entire system's processes and gather their information, none of the information shows up. But other commands runs fine, like <code>perf stat ls</code> that collects all performance info running ls command.</p>\n\n<p><img src=\"http://i.stack.imgur.com/o2YZS.png\" alt=\"screenshot\"> </p>\n\n<p>So I am wondering : is that because <code>perf top</code> command does not support virtual machine? Any ideas?</p>\n"},{"tags":["performance","google-chrome"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":588,"score":3,"question_id":10537399,"title":"What does the times mean in Google Chrome's timeline in the network panel?","body":"<p>Often when troubleshooting performance using the Google Chrome's network panel I see different times and often wonder what they mean.  </p>\n\n<p>Can someone validate that I understand these properly:</p>\n\n<ol>\n<li>Blocking: Time blocked by browser's multiple request for the same domain limit(???)</li>\n<li>Waiting: Waiting for a connection from the server (???)</li>\n<li>Sending: Time spent to transfer the file from the server to the browser (???)</li>\n<li>Receiving: Time spent by the browser analyzing and decoding the file (???)</li>\n<li>DNS Lookup: Time spent resolving the hostname.</li>\n<li>Connecting:  Time spent establishing a socket connection.</li>\n</ol>\n\n<p>Now how would someone fix long blocking times?</p>\n\n<p>Now how would someone fix long waiting times?</p>\n"},{"tags":["python","performance","cairo","pycairo"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":58,"score":2,"question_id":12730399,"title":"Drawing tons of circles in Python using cairo","body":"<p>I am currently working on an application, which uses a video projector to create an effect similar to a real laser. A really nice example of what I'm trying to archive can be seen on Youtube <a href=\"http://www.youtube.com/watch?v=UL7ojzXzCew\" rel=\"nofollow\">here</a>.</p>\n\n<p>Basically that application needs to draw simple moving shapes in various colors. I have a pretty complicated setup using pycairo allowing the primitives to pass through a set of modifiers to change position, scale and rotation. This allows for a great deal of flexibility.</p>\n\n<p>Unfortunately pycairo seems to be pretty slow at drawing dashed circles. I tried drawing 30 circles like this:</p>\n\n<pre><code># setup, transforms...\n# Example color-scheme:\nself._colors = [(0.0, 1.0, 0.0)]\n# drawing dashes one after another\nfor count, color in enumerate(self._colors):\n    cr.set_dash(dash_len, self._dash_len * count)\n    cr.set_source_rgb(color[0], color[1], color[2])\n\n    cr.arc(0, 0, self.radius(), 0, 2 * math.pi)\n    cr.stroke()\n</code></pre>\n\n<p>The whole thing looks like <a href=\"http://imgur.com/krfD0\" rel=\"nofollow\">this</a>. This is not able to sustain 25fps with on 800x600 using a Core2Duo.</p>\n\n<p>Is there a faster way to draw circles? Quality is not really an issue.</p>\n\n<p>Thanks for your help!</p>\n"},{"tags":["python","performance","matrix","type-conversion"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":36,"score":1,"question_id":12731891,"title":"Python: speed of conversion of type of 3d matrix element","body":"<p>I believe that this is simple question, but I'm new to Python so any suggestion will be appreciated.\nI have matrix of strings, and I need to convert every element into float type and then to increase it for some number. I did that like this:</p>\n\n<pre><code>for i in range(0,len(matrix)):\n    for j in range(0,len(matrix[i])):\n        for k in range(0,len(matrix[j])):\n            matrix[i][j][k] = float(matrix[i][j][k]) + 5.555\n</code></pre>\n\n<p>Is there any other way of doing this in order to increase speed? Performance is really low when I have matrix[50][50][50] or bigger.\nIs there one-line method that could increase all elements at once?</p>\n"},{"tags":["performance","phonegap","jqmobi"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":91,"score":0,"question_id":12662395,"title":"How can I make input/select focus in PhoneGap faster?","body":"<p>Clicking on input elements (text, texarea, select, etc) has a noticeable delay in PhoneGap...is there any way to overcome this?</p>\n\n<p>I understand that the user might be dragging/scrolling when they touch the screen which is why the delay is there and I have been able to overcome this with anchors by using onTouchStart events instead of onClick, but I don't know how to apply the same thing to other inputs.</p>\n\n<p>I'm using jqMobi (a stripped down jQuery framework which rocks!).</p>\n\n<p>It doesn't appear as though you can call .focus() on input elements or .click() on select elements, which was what I tried by tapping into the onTouchStart event like this:</p>\n\n<pre><code>&lt;input type=\"text\" name=\"email\" ontouchstart=\"$(this).focus();\" /&gt;\n&lt;select name=\"country\" ontouchstart=\"$(this).click();\"&gt;...&lt;/select&gt;\n</code></pre>\n\n<p>Any help would really be appreciated!!</p>\n\n<hr>\n\n<p>The sluggishness is on the device, not in the browser. We're currently targeting iOS and I've experienced the sluggishness on the iPhone 4, 4s, iPad 2 and the new iPad.</p>\n"},{"tags":["performance","oracle","hints"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":37,"score":1,"question_id":12731549,"title":"Oracle query optimization generalquestion","body":"<p>What should be the criteria when the query is being optimized with hints.\nI take it that it is incorrect to use costs in 'explain plan', since Oracle automatically minimize this cost. I other words, it is possible that estimated cost will be higher but actually query is executed faster. Is that correct?</p>\n\n<p>Is the idea above correct? In other words, they point of optimization is that Oracle has imperfect statistic and that leads to not optimal execution plan.</p>\n\n<p>In that case, when tuning a query, I should achieve only minimum execution time, not considering costs. Is that correct?</p>\n\n<p>Thank you</p>\n"},{"tags":["database","performance","map","arraylist","gallery"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":10,"score":0,"question_id":12730938,"title":"map and gallery slowly","body":"<p>My application saves in a database location as well as any pictures. Then visualize on a map the various positions, creating a route and markers that show me where the photos were taken. Below the map visualize a gallery of images. I noticed that when there are more than one photo to handle, the application slows down. I then tried to separate the population of the map from that of the gallery through the use of thread, but the result is the same. Note that for popular map gallery and I access to the ArrayList previously filled from the database, this can be my bottleneck?</p>\n"},{"tags":["haskell","operating-system","performance"],"answer_count":3,"favorite_count":3,"up_vote_count":3,"down_vote_count":0,"view_count":1227,"score":3,"question_id":3862954,"title":"Systems programming in haskell?","body":"<p>Ive been thinking...Is it possible to go very low level in functional languages like haskell?(like making a kernel or device driver).And will functional features (like monads) be fast and efficient there? </p>\n\n<p>Thanks in advance</p>\n"},{"tags":["performance","node.js","http-compression","sdch"],"answer_count":0,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":169,"score":4,"question_id":9424166,"title":"SDCH compression for Node?","body":"<p>Is there a <a href=\"http://en.wikipedia.org/wiki/Shared_Dictionary_Compression_Over_HTTP\" rel=\"nofollow\">SDCH</a> <em>(Shared Dictionary Compression over HTTP)</em> library for Node?  (Or any other implementations for that matter?)</p>\n\n<p>A quick search on npm yielded nothing.</p>\n"},{"tags":["performance","opengl"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":95,"score":0,"question_id":12713594,"title":"OpenGL Performance","body":"<p>First let me explain the application a little bit. This is video security software that can display up to 48 cameras at once. Each video stream gets its own Windows HDC but they all use a shared OpenGL context. I get pretty good performance with OpenGL and it runs on Windows/Linux/Mac. Under the hood the contexts are created using wxWidgets 2.8 wxGLCanvas, but I don't think that has anything to do with the issue.</p>\n\n<p>Now here's the issue. Say I take the same camera and display it in all 48 of my windows. This basically means I'm only decoding 30fps (which is done on a different thread anywa) but displaying up to 1440fps to take decoding out of the picture. I'm using PBOs to transfer the images over, depending on whether pixel shaders and multitexturing are supported I may use those to do YUV->RGB conversion on the GPU. Then I use a quad to position the texture and call SwapBuffers. All the OpenGL calls come from the UI thread. Also I've tried doing YUV->RGB conversion on the CPU and messed with using GL_RGBA and GL_BGRA textures, but all formats still yield roughly the same performance. Now the problem is I'm only getting around 1000fps out of the possible 1440fps (I know I shouldn't be measuring in fps, but its easier in this scenario). The above scenario is using 320x240 (YUV420) video which is roughly only 110MB/sec. If I use a 1280x720 camera then I get roughly the same framerate which is nearly 1.3GB/sec. This tells me that it certainly isn't the texture upload speed. If I do the YUV->RGB conversion and scaling on the CPU and paint using a Windows DC then I can easily get the full 1440fps.</p>\n\n<p>The other thing to mention is that I've disabled vsync both on my video card and through OpenGL using wglSwapIntervalEXT. Also there are no OpenGL errors being reported. However, using very sleepy to profile the application it seems to be spending most of its time in SwapBuffers. I'm assuming the issue is somehow related to my use of multiple HDCs or with SwapBuffers somewhere, however, I'm not sure how else to do what I'm doing.</p>\n\n<p>I'm no expert on OpenGL so if anyone has any suggestions or anything I would love to hear them. If there is anything that I'm doing that sounds wrong or any way I could achieve the same thing more efficiently I'd love to hear it.</p>\n\n<p>Here's some links to glIntercept logs for a better understanding of all the OpenGL calls being made:</p>\n\n<p>Simple RGB: <a href=\"https://docs.google.com/open?id=0BzGMib6CGH4TdUdlcTBYMHNTRnM\" rel=\"nofollow\">https://docs.google.com/open?id=0BzGMib6CGH4TdUdlcTBYMHNTRnM</a></p>\n\n<p>Shaders YUV: <a href=\"https://docs.google.com/open?id=0BzGMib6CGH4TSDJTZGxDanBwS2M\" rel=\"nofollow\">https://docs.google.com/open?id=0BzGMib6CGH4TSDJTZGxDanBwS2M</a></p>\n\n<p>Profiling Information:\nSo after profiling it reported several redundant state changes which I'm not surprised by. I eliminated all of them and saw no noticeable performance difference which I kind of expected. I have 34 state changes per render loop and I am using several deprecated functions. I'll look into using vertex arrays which would solve these. However, I'm just doing one quad per render loop so I don't really expect much performance impact from this. Also keep in mind I don't want to rip everything out and go all VBOs because I still need to support some fairly old Intel chipset drivers that I believe are only OpenGL 1.4.</p>\n\n<p>The thing that really interested me and it hadn't occurred to me before was that each context has its own front and back buffer. Since I'm only using one context the previous HDCs render call must finish writing to the back buffer before the swap can occur and then the next one can start writing to the back buffer again. Would it really be more efficient to use more than one context? Or should I look into rendering to textures (FBOs I think) instead and continue using one context?</p>\n\n<p>EDIT: The original description mentioned using multiple OpenGL contexts, but I was wrong I'm only using one OpenGL context and multiple HDCs.\nEDIT2: Added some information after profiling with gDEBugger.</p>\n"},{"tags":["sql-server","performance","sql-server-2008-r2","database-performance"],"answer_count":1,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":76,"score":3,"question_id":12723252,"title":"Is there a negative impact to using SQL Server with a lower compatibility level","body":"<p>We have SQL Server 2008 R2 which we are using for a database with a compatibility level set as 2005 (90).</p>\n\n<p>We will by upgrading it to 2008 at some point, but in the meantime I would like to know if having the database in a lower compatibility level is this going to have a negative effect on its performance?</p>\n\n<p>I found <a href=\"http://stackoverflow.com/a/178509/95399\">this question</a> that implies there could be a negative effect, but it's for an older version of SQL Server.</p>\n"},{"tags":["performance","sorting","timsort"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":13,"score":0,"question_id":12728406,"title":"Can anyone please explain the pseudocode of TimSort?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/1733073/grokking-timsort\">Grokking Timsort</a>  </p>\n</blockquote>\n\n\n\n<p>I am not able top understand why TimSort performs better? There is no  good explanation available on Google and what are the assumptions made when TimSort was written?</p>\n"},{"tags":["mysql","performance","database-partitioning"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":12727262,"title":"MySQL DB: performance and partitioning","body":"<p>I've migrate an access DB to a MySQL DB. In particular, in this DB I've a table with almost 5 million of rows. The most part of operations on this table are queries (filters to select a data subset).</p>\n\n<p>I'm interested in performance. The partitioning of the MySQL DB could enhance the performance? In this case, what type of partitioning is the better to use? </p>\n\n<p><strong>EDIT</strong></p>\n\n<p>The \"data filter\" most used will be, I think, the date. But normally more criteria will be used.</p>\n"},{"tags":["java","performance","spring","java-ee","outofmemoryerror"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":102,"score":2,"question_id":12713435,"title":"How to debug an OutOfMemoryException in a webapp","body":"<p>I am trying to debug an out of memory error. When I do a heap dump with Jmap and analyze the results with eclipse Mat - I see the following.</p>\n\n<pre><code>7,677 instances of \"java.lang.Class\", loaded by \n\"&lt;system class loader&gt;\" occupy 48,094,720 (23.99%) bytes. \n</code></pre>\n\n<p>Biggest instances:</p>\n\n<pre><code>class blah.BlahService$$EnhancerByCGLIB$$4a0a7d43 @ 0x2aaab06d9668 \n- 2,067,096 (1.03%) bytes. \n</code></pre>\n\n<p>BlahService is a spring service with @Service annotation. All service classes are Singleton in spring - one per IOC per bean - So why would this class be a top suspect.</p>\n\n<p>I also see </p>\n\n<pre><code>One instance of \"org.apache.jasper.servlet.JspServlet\" loaded by \n\"org.apache.catalina.loader.StandardClassLoader @ 0x2aaac17bc260\" occupies 42,724,168 \n(21.31%) bytes. The memory is accumulated in one instance of \n\"java.util.concurrent.ConcurrentHashMap$Segment[]\" loaded by \"&lt;system class loader&gt;\".\n</code></pre>\n\n<p>what does this mean?</p>\n"},{"tags":["c++","performance","for-loop"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":61,"score":-1,"question_id":12726556,"title":"c++ code performance in battery mode and unplugged mode","body":"<p>I have a piece of code written in c++(in Qt creator) in which there is a loop with about 35000 iterations. When I'm measuring time, I get 25ms. The interesting thing is that when I run this code when my laptop's power is unplugged, I get about 10ms.</p>\n\n<p>In addition, sometimes this code gives about 5ms to run and sometimes 10ms. I mean it is not stable.</p>\n\n<p>Can anybody tell me if this time ( 10-20 ms ) is logical or should be significantly less .(I do nothing in the loop scope).</p>\n\n<p>And why this changes when power is unplugged and why it is not stable.?</p>\n"},{"tags":["performance","stored-procedures","reporting-services","ssrs-2008","ssrs-reports"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":54,"score":0,"question_id":12723463,"title":"SSRS Report Timeout - To many records being returned","body":"<p>I am using SSRS 2008.</p>\n\n<p>I have a .RDL that I use that returns around 15 million records with groupings.</p>\n\n<p>The 15 million records get generated based on parameters the user supply and the stored procedure that gets called is huge and it JOINS about 7 tables. <em>(Way to big for me to add here)</em></p>\n\n<p>I do not want to increase the TIMEOUT session for the .RDL.</p>\n\n<p>What are my options here to get best performance and not have the report time out from waiting toooo long?</p>\n\n<p>The report also makes use of DRILL DOWN functionality.</p>\n\n<ul>\n<li>Maybe push the 15 million records into a table and then just select the records from the single table?</li>\n<li>Is is possible to call different stored Procedures based on when the user do a DRILL DOWN?</li>\n</ul>\n\n<p>Any info would be appreciated.</p>\n"},{"tags":["mysql","monitoring","performance"],"answer_count":3,"favorite_count":4,"up_vote_count":6,"down_vote_count":0,"view_count":3011,"score":6,"question_id":3996864,"title":"High Number of MySQL Temporary Disk Tables","body":"<p>We have noticed that MySQL is reporting a very high number of temporary disk tables (over 10,000) this is reported by <a href=\"http://serverdensity.com\" rel=\"nofollow\">Server Density</a>. Were trying to understand a bit more about this.</p>\n\n<ul>\n<li>Why are temporary disk tables created by MySQL? </li>\n<li>What impact do they have on performance?</li>\n<li>Are they ever removed by MySQL or will this number just increase?</li>\n</ul>\n"},{"tags":["java","performance","tomcat"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":65,"score":0,"question_id":12721525,"title":"Server slowdown","body":"<p>I have a GWT application running on Tomcat on a cloud linux(Ubuntu) server, recently I released a new version of the application and suddenly my server response times have gone from 500ms average to 15s average. I have run every monitoring tool I know. </p>\n\n<ul>\n<li>iostat says my disks are 0.03% utilised</li>\n<li>mysqltuner.pl says I am OK other see below</li>\n<li>top says my processor is 99% idle and load average: 0.20, 0.31, 0.33</li>\n<li>memory usage is 50% (-/+ buffers/cache:       3997       3974)</li>\n</ul>\n\n<p>mysqltuner output</p>\n\n<pre><code>[OK] Logged in using credentials from debian maintenance account.\n\n-------- General Statistics --------------------------------------------------\n[--] Skipped version check for MySQLTuner script\n[OK] Currently running supported MySQL version 5.1.63-0ubuntu0.10.04.1-log\n[OK] Operating on 64-bit architecture\n\n-------- Storage Engine Statistics -------------------------------------------\n[--] Status: +Archive -BDB -Federated +InnoDB -ISAM -NDBCluster \n[--] Data in MyISAM tables: 370M (Tables: 52)\n[--] Data in InnoDB tables: 697M (Tables: 1749)\n[!!] Total fragmented tables: 1754\n\n-------- Security Recommendations  -------------------------------------------\n[OK] All database users have passwords assigned\n\n-------- Performance Metrics -------------------------------------------------\n[--] Up for: 19h 25m 41s (1M q [28.122 qps], 1K conn, TX: 2B, RX: 1B)\n[--] Reads / Writes: 98% / 2%\n[--] Total buffers: 1.0G global + 2.7M per thread (500 max threads)\n[OK] Maximum possible memory usage: 2.4G (30% of installed RAM)\n[OK] Slow queries: 0% (1/1M)\n[OK] Highest usage of available connections: 34% (173/500)\n[OK] Key buffer size / total MyISAM indexes: 16.0M/279.0K\n[OK] Key buffer hit rate: 99.9% (50K cached / 40 reads)\n[OK] Query cache efficiency: 61.4% (844K cached / 1M selects)\n[!!] Query cache prunes per day: 553779\n[OK] Sorts requiring temporary tables: 0% (0 temp sorts / 34K sorts)\n[OK] Temporary tables created on disk: 4% (4K on disk / 102K total)\n[OK] Thread cache hit rate: 84% (185 created / 1K connections)\n[!!] Table cache hit rate: 0% (256 open / 27K opened)\n[OK] Open file limit used: 0% (20/2K)\n[OK] Table locks acquired immediately: 100% (692K immediate / 692K locks)\n[OK] InnoDB data size / buffer pool: 697.2M/1.0G\n\n-------- Recommendations -----------------------------------------------------\nGeneral recommendations:\n    Run OPTIMIZE TABLE to defragment tables for better performance\n    MySQL started within last 24 hours - recommendations may be inaccurate\n    Enable the slow query log to troubleshoot bad queries\n    Increase table_cache gradually to avoid file descriptor limits\nVariables to adjust:\n    query_cache_size (&gt; 16M)\n    table_cache (&gt; 256)\n</code></pre>\n"},{"tags":["performance","oracle","entity-framework-4"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":44,"score":0,"question_id":12472549,"title":"Why is the Entity Framework taking 6 seconds to return a query that is executed in 0.2 seconds","body":"<p>I've boiled this down to a very simple example rather than go into the detail of my specific problem but the example replicates my problem exactly.</p>\n\n<p>I have an Oracle database table that contains jobs. When I have 10 records in that table my LINQ query to return all jobs it executes instantly (&lt; 0.2 sec).</p>\n\n<p>When I have 25,000 records in the table it takes 15 seconds to execute. When I profile the Entity Framework to analyse the query that is being executed I take that query and run it against SQL Developer. It executes instantly (&lt;0.2 sec). I understand that SQL Developer only returns 50 records at a time which is why it gets the better performance.</p>\n\n<p>When I put a Take on my LINQ query to only take 10 records it still takes 6 seconds to execute. When I profile that query and run it in SQL Developer it executes instantly (&lt;0.2).</p>\n\n<p>My question is why is this happening. The queries that the Entity Framework generates for \"Select all from a table of 10\" and \"Select 10 from a table of 25,000\" both take roughly the same amount of time to execute in SQL developer. Why is the second query taking so much longer for the Entity Framework to return?</p>\n\n<p>I've looked at varying options, switching off tracking, compiling the query, querying a generated object context vs querying a manually constructed one. Nothing seems to explain the massive difference. Fundamentally if SQL Developer can execute the query in &lt; 0.2 sec, why can't the Entity Framework. I'm only asking for 10 records.</p>\n"},{"tags":["performance","matlab","data"],"answer_count":2,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":72,"score":5,"question_id":12701958,"title":"Matlab - Build fixed interval dataset from random interval dataset using stale data","body":"<p><strong>Update:</strong> I've provided a brief analysis of the three answers at the bottom of the question text and explained my choices. Thanks everyone for your suggestions.</p>\n\n<p><strong>My Question:</strong> What is the most efficient method of building a fixed interval dataset from a random interval dataset using stale data?</p>\n\n<p><strong>Some background:</strong> The above is a common problem in statistics. Frequently, one has a sequence of observations occurring at random times. Call it <code>Input</code>. But one wants a sequence of observations occurring say, every 5 minutes. Call it <code>Output</code>. One of the most common methods to build this dataset is using stale data, ie set each observation in <code>Output</code> equal to the most recently occurring observation in <code>Input</code>.</p>\n\n<p>So, here is some code to build example datasets:</p>\n\n<pre><code>TInput = 100;\nTOutput = 50;\n\nInputTimeStamp = 730486 + cumsum(0.001 * rand(TInput, 1));\nInput = [InputTimeStamp, randn(TInput, 1)];\n\nOutputTimeStamp = 730486.002 + (0:0.001:TOutput * 0.001 - 0.001)';\nOutput = [OutputTimeStamp, NaN(TOutput, 1)];\n</code></pre>\n\n<p>Both datasets start at close to midnight at the turn of the millenium. However, the timestamps in <code>Input</code> occur at random intervals while the timestamps in <code>Output</code> occur at fixed intervals. For simplicity, I have ensured that the first observation in <code>Input</code> always occurs before the first observation in <code>Output</code>. Feel free to make this assumption in any answers.</p>\n\n<p>Currently, I solve the problem like this:</p>\n\n<pre><code>sMax = size(Output, 1);\ntMax = size(Input, 1);\ns = 1;\nt = 2;\n%#Loop over input data\nwhile t &lt;= tMax\n    if Input(t, 1) &gt; Output(s, 1)\n        %#If current obs in Input occurs after current obs in output then set current obs in output equal to previous obs in input\n        Output(s, 2:end) = Input(t-1, 2:end);\n        s = s + 1;\n        %#Check if we've filled out all observations in output\n        if s &gt; sMax\n            break\n        end\n        %#This step is necessary in case we need to use the same input observation twice in a row\n        t = t - 1;\n    end\n    t = t + 1;\n    if t &gt; tMax\n        %#If all remaining observations in output occur after last observation in input, then use last obs in input for all remaining obs in output \n        Output(s:end, 2:end) = Input(end, 2:end);\n        break\n    end\nend\n</code></pre>\n\n<p>Surely there is a more efficient, or at least, more elegant way to solve this problem? As I mentioned, this is a common problem in statistics. Perhaps Matlab has some in-built function I'm not aware of? Any help would be much appreciated as I use this routine a LOT for some large datasets.</p>\n\n<p><strong>THE ANSWERS:</strong> Hi all, I've analyzed the three answers, and as they stand, Angainor's is the best. </p>\n\n<p>ChthonicDaemon's answer, while clearly the easiest to implement, is really slow. This is true even when the conversion to a <code>timeseries</code> object is done outside of the speed test. I'm guessing the <code>resample</code> function has a lot of overhead at the moment. I am running 2011b, so it is possible Mathworks have improved it in the intervening time. Also, this method needs an additional line for the case where <code>Output</code> ends more than one observation after <code>Input</code>.</p>\n\n<p>Rody's answer runs only slightly slower than Angainor's (unsurprising given they both employ the <code>histc</code> approach), however, it seems to have some problems. First, the method of assigning the last observation in <code>Output</code> is not robust to the last observation in <code>Input</code> occurring after the last observation in <code>Output</code>. This is an easy fix. But there is a second problem which I think stems from having <code>InputTimeStamp</code> as the first input to <code>histc</code> instead of the <code>OutputTimeStamp</code> adopted by Angainor. The problem emerges if you change <code>OutputTimeStamp = 730486.002 + (0:0.001:TOutput * 0.001 - 0.001)';</code> to <code>OutputTimeStamp = 730486.002 + (0:0.0001:TOutput * 0.0001 - 0.0001)';</code> when setting up the example inputs.</p>\n\n<p>Angainor's appears robust to everything I threw at it, plus it was the fastest.</p>\n\n<p>I did a lot of speed tests for different input specifications - the following numbers are fairly representative:</p>\n\n<p>My naive loop: <code>Elapsed time is 8.579535 seconds.</code></p>\n\n<p><strong>Angainor</strong>: <code>Elapsed time is 0.661756 seconds.</code></p>\n\n<p>Rody: <code>Elapsed time is 0.913304 seconds.</code></p>\n\n<p>ChthonicDaemon: <code>Elapsed time is 22.916844 seconds.</code></p>\n\n<p>I'm +1-ing Angainor's solution and marking the question solved. Cheers everyone, and thanks for your input.</p>\n"},{"tags":["performance","hyperlink"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":21,"score":0,"question_id":12720973,"title":"Performance issue , takes a long time to load the pages","body":"<p>I am facing a problem in my new project <a href=\"http://jobsgang.com\" rel=\"nofollow\">http://jobsgang.com</a> . It was working well before. Now when I click in any of the link example ' View Job' , it takes a long time to load the page. Is there any way to check the performance of the sql query . I am  not able to find out the problem. Please help me to solve it</p>\n\n<p>Regards Sunil</p>\n"},{"tags":["javascript","performance","web-applications"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":41,"score":1,"question_id":12720504,"title":"Mobile Webapp: render data list in PHP or Javascript?","body":"<p>I am currently developing a webapp aimed for mobile devices which basically consists of a couple of long and complex lists, lots of data, collapsible cascading elements.</p>\n\n<p>I'm getting the data to be displayed as XML, now as I see it I have two options:</p>\n\n<ul>\n<li>build the list on the server and send HTML to the client</li>\n<li>send XML to the client, build the list with Javascript/jQuery</li>\n</ul>\n\n<p>Not sure which is more efficient, less data to transfer is good, less load on (especially older) phones is also good.</p>\n\n<p>Any other pros/cons I'm not seeing? Suggestions?</p>\n"},{"tags":["performance","spring","hibernate","jdbc","persistence.xml"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":97,"score":0,"question_id":12687836,"title":"How to Improve Performance when Persisting Large Collections using Spring EntityManager Hibernate","body":"<p>I am using Spring/Hibernate done the JPA way using <code>org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean</code> and configured using spring xml, persistence.xml and JPA 2 annotations.</p>\n\n<p>Functionally it is fine and persisting correctly. However, I have a requirement to store entity A which has a bidirectional OneToMany with a large collection of B as quickly as possible.</p>\n\n<p>I am using various options in persistence.xml to try to speed up inserts and reduce memory use (the application writes about as much as it reads)</p>\n\n<pre><code>&lt;property name=\"hibernate.id.new_generator_mappings\" value=\"true\" /&gt;\n&lt;property name=\"hibernate.jdbc.batch_size\" value=\"50\" /&gt;\n&lt;property name=\"hibernate.order_inserts\" value=\"true\" /&gt;\n&lt;property name=\"hibernate.order_updates\" value=\"true\" /&gt;\n&lt;property name=\"hibernate.cache.use_query_cache\" value=\"false\" /&gt;\n&lt;property name=\"hibernate.cache.use_second_level_cache\" value=\"false\" /&gt;\n</code></pre>\n\n<p>and the persist is done using </p>\n\n<pre><code>entityManager.persist(instanceOfA)\n</code></pre>\n\n<p>Edit Additional info:</p>\n\n<p>Each entity has a generated id like this:</p>\n\n<pre><code>@Id\n    @Column(name=\"ID\")\n    @GeneratedValue(strategy=GenerationType.AUTO, generator=\"SEQUENCE_GENERATOR\")\n    @SequenceGenerator(name=\"SEQUENCE_GENERATOR\", sequenceName=\"MY_SEQUENCE\", allocationSize=50)\n    private Long id;\n</code></pre>\n\n<p>which relates to an Oracle sequence</p>\n\n<pre><code>CREATE SEQUENCE MY_SEQUENCE MINVALUE 1 MAXVALUE 999999999999999999999999999 START WITH 1 INCREMENT BY 50 NOCYCLE NOCACHE NOORDER;\n</code></pre>\n\n<p>When I run the code with show sql on I can see lots of insert statements taking quite a while.</p>\n\n<p>I have read that I need to call <code>entityManager.flush(); entityManager.clear();</code> every 50 rows inserted.</p>\n\n<p><a href=\"http://abramsm.wordpress.com/2008/04/23/hibernate-batch-processing-why-you-may-not-be-using-it-even-if-you-think-you-are/\" rel=\"nofollow\">http://abramsm.wordpress.com/2008/04/23/hibernate-batch-processing-why-you-may-not-be-using-it-even-if-you-think-you-are/</a></p>\n\n<p>Does this mean that I need to break up the persist into </p>\n\n<pre><code>entityManager.persist(instanceOfA);\ninstanceOfA.addB(instanceOfB);\nentityManager.persist(instanceofB);\n</code></pre>\n\n<p>adding a flush clear every 50 calls to <code>persist()</code>?</p>\n\n<p>Is there a cleaner way of doing it? (my actual object hierarchy has about 7 layers of relations like A and B)</p>\n\n<p>I was thinking about using JDBC for the inserts, but I hate writing row mappers :)</p>\n\n<p>I have heard about <code>org.hibernate.StatelessSession</code> but there is no method of getting that from a JPA entity manager without casting to SessionFactory at some point - again not very clean.</p>\n\n<p>Thanks in advance!</p>\n"},{"tags":["javascript","performance","memory"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":27,"score":0,"question_id":12719469,"title":"Does reassigning the reference to a javascript array to another empty array free memory used by the original array?","body":"<p>So let's say you have an array:</p>\n\n<pre><code>var array = [1, 2, 3, 4, 5, 6]\n</code></pre>\n\n<p>and then you reassign the referrence to that array to a new empty array:</p>\n\n<pre><code>array = [];\n</code></pre>\n\n<p>Is that original array removed from memory now that there are no more references to it?</p>\n"},{"tags":["performance","wsgi","asyncsocket","gevent","eventlet"],"answer_count":1,"favorite_count":12,"up_vote_count":10,"down_vote_count":0,"view_count":226,"score":10,"question_id":12669971,"title":"What does make significant difference of performance between eventlet and gevent?","body":"<p>Those two libraries share the similar philosophy and the similar design decisions as a result.  But <a href=\"http://nichol.as/benchmark-of-python-web-servers\">this popular WSGI benchmark</a> says <code>eventlet</code> is way slower than <code>gevent</code>.  What do make their performance so different?</p>\n\n<p>As I know key differences between them are:</p>\n\n<ul>\n<li><p><code>gevent</code> intentionally depends on and is coupled to <code>libev</code> (<code>libevent</code>, previously) while <code>eventlet</code> defines independent reactor interface and implements particular adapters using <code>select</code>, <code>epoll</code>, and Twisted reactor behind it. <em>Does additional reactor interface make critical performance hits?</em></p></li>\n<li><p><code>gevent</code> is mostly written in Cython while <code>eventlet</code> is written in pure Python. <em>Is natively compiled Cython so faster than pure Python, for not-so-much-computational but IO-bound programs?</em></p></li>\n<li><p>Primitives of <code>gevent</code> emulate standard libraries’ interfaces while <code>eventlet</code>’s primitives differ from standard and provides additional layer to emulate it. <em>Does additional emulation layer makes <code>eventlet</code> slower?</em></p></li>\n<li><p><em>Is the implementation of <code>eventlet.wsgi</code> just worse than <code>gevent.pywsgi</code>?</em></p></li>\n</ul>\n\n<p>I really wonder, because they overall look so similar for me.</p>\n"},{"tags":["objective-c","xcode","performance","nsoperation","unresponsive"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":83,"score":1,"question_id":11165885,"title":"Displaying UIViewController, NSOperationQueue","body":"<p>I have been on an iOS 5 app in which I have an NSOperationQueue which works fine to get some data and create a UIViewController. However, at one point when every operation seems to be complete the app is unresponsive for quite a long time. Here's how it goes:</p>\n\n<ol>\n<li>Get some data from DB</li>\n<li>Queue - For each item Init a custom UIViewController object.</li>\n<li>Hand each UIViewController object over to the MainViewController and display them.</li>\n</ol>\n\n<p>Everything goes fine until the UIVC objects have to appear. \nIt goes past all the [[self view] addSubview:object.view]; and to the end of the function. \nBut then there is a huge lag, from 5 to 30 seconds before the NSLog statements inside the viewDidApear of the UIVC object show up...</p>\n\n<p>So in code:</p>\n\n<pre><code>//MainViewController\n-(void)displayNewView {\n  [[self view] addSubview:object.view];\n  NSLog(@\"Done setup\");\n}\n\n//-- Long unresponsiveness --//\n\n//UIVC object\n-(void)viewDidAppear:(BOOL)animated {\n  NSLog(@\"Start appear\");\n  [super viewDidAppear:animated];\n}\n</code></pre>\n\n<p>What could be causing this? Am I missing something obvious?</p>\n\n<p>Thanks for the help!</p>\n"},{"tags":["c++","performance","optimization"],"answer_count":5,"favorite_count":5,"up_vote_count":14,"down_vote_count":1,"view_count":8955,"score":13,"question_id":4707012,"title":"C++ memcpy() vs std::copy()","body":"<p>Is it better to use memcpy as shown below or is it better to use std::copy() in terms to performance? Why?</p>\n\n<pre><code>char *bits = NULL;\n...\n\nbits = new (std::nothrow) char[((int *) copyMe-&gt;bits)[0]];\nif (bits == NULL)\n{\n    cout &lt;&lt; \"ERROR Not enough memory.\\n\";\n    exit(1);\n}\n\nmemcpy (bits, copyMe-&gt;bits, ((int *) copyMe-&gt;bits)[0]);\n</code></pre>\n"},{"tags":["java","flex","performance","jmeter"],"answer_count":6,"favorite_count":3,"up_vote_count":4,"down_vote_count":0,"view_count":1273,"score":4,"question_id":2429841,"title":"Performance testing Flex applications","body":"<p>What's the best method for performance testing Flex applications with a BlazeDS/Java severs backend. We're looking at JMeter but can it be used with the amf the protocol at a more sophisticated level where values in a request can be manipulated?</p>\n"},{"tags":["sql-server","database","performance","hardware"],"answer_count":11,"favorite_count":7,"up_vote_count":12,"down_vote_count":0,"view_count":23258,"score":12,"question_id":165383,"title":"Optimal RAID setup for SQL server","body":"<p>We have an SQL 2005 database backend for our website, currently about 10GB in size. There are a lot more reads than writes, though I don't have the exact statistics.</p>\n\n<p>We're upgrading our database server and I was thinking of getting 4 disks and setting them up in two RAID 1 arrays - one for the data files and the other for the OS and log files. Would this be the optimal set-up or would RAID 5 be better for the data files? RAID 10 gets a bit pricey and is probably overkill for us.</p>\n\n<p>At this stage SQL Server should keep much of the database in RAM (8GB), but it will grow, so I don't want to entirely rely on that.</p>\n\n<p>Edit: we definitely want redundancy on a production server, so RAID 0 by itself is out. RAID 10 is nice, but might be a bit expensive for us.</p>\n"},{"tags":["performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":48,"score":0,"question_id":11253023,"title":"What is the doubling factor?","body":"<p>Give a formula to predict the running time of a program for a problem of size N\nwhen doubling experiments have shown that the doubling factor is 2^b and the running\ntime for problems of size N0 is T.</p>\n\n<p>My problem is that I don't get the question. What is the doubling factor supposed to mean?! And what is b? What's the difference between N0 and N?! Could someone explain, please?!</p>\n"},{"tags":["sql","performance","oracle"],"answer_count":9,"favorite_count":1,"up_vote_count":7,"down_vote_count":1,"view_count":15903,"score":6,"question_id":636886,"title":"SQL not equals & null","body":"<p>We'd like to write this query:</p>\n\n<pre><code>select * from table \nwhere col1 != 'blah' and col2 = 'something'\n</code></pre>\n\n<p>We want the query to include rows where col1 is null (and col2 = 'something').  Currently the query won't do this for the rows where col1 is null.  Is the below query the best and fastest way?</p>\n\n<pre><code>select * from table \nwhere (col1 != 'blah' or col1 is null) and col2 = 'something'\n</code></pre>\n\n<p>Alternatively, we could if needed update all the col1 null values to empty strings.  Would this be a better approach?  Then our first query would work.</p>\n\n<p><hr /></p>\n\n<p><strong>Update</strong>: Re: using NVL: I've read on another <a href=\"http://stackoverflow.com/questions/191640/get-null-null-in-sql\">post</a> that this is not considered a great option from a performance perspective.</p>\n"},{"tags":["sql-server","performance","tsql"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":52,"score":1,"question_id":12714907,"title":"What is the best way to rewrite this query by removing sub-queries from select clause?","body":"<p>I couldn't figure out how to optimize this query and I didn't find a similar query as example.</p>\n\n<p>What would be the best way to rewrite this query in order not to have so many sub-queries in the select clause achieving the same result grouped by month/brand_id?</p>\n\n<p>The DBMS is SQL Server.</p>\n\n<pre class=\"lang-cs prettyprint-override\"><code>select\n    o.brand_id,\n    count(o.item_id) as \"Total bought items\",\n    count(o.sell_date) as \"Total sold items\",\n     /* Amount of sold items grouped by month per item */\n    (select count(obj.sell_date) from tab_object as obj where month(obj.install_date) = 1 and obj.brand_id = b.brand_id) as \"Jan\",\n    (select count(obj.sell_date) from tab_object as obj where month(obj.install_date) = 2 and obj.brand_id = b.brand_id) as \"Feb\",\n    (select count(obj.sell_date) from tab_object as obj where month(obj.install_date) = 3 and obj.brand_id = b.brand_id) as \"Mar\",\n    (select count(obj.sell_date) from tab_object as obj where month(obj.install_date) = 4 and obj.brand_id = b.brand_id) as \"Apr\",\n    (select count(obj.sell_date) from tab_object as obj where month(obj.install_date) = 5 and obj.brand_id = b.brand_id) as \"May\",\n    (select count(obj.sell_date) from tab_object as obj where month(obj.install_date) = 6 and obj.brand_id = b.brand_id) as \"Jun\",\n    (select count(obj.sell_date) from tab_object as obj where month(obj.install_date) = 7 and obj.brand_id = b.brand_id) as \"Jul\",\n    (select count(obj.sell_date) from tab_object as obj where month(obj.install_date) = 8 and obj.brand_id = b.brand_id) as \"Aug\",\n    (select count(obj.sell_date) from tab_object as obj where month(obj.install_date) = 9 and obj.brand_id = b.brand_id) as \"Sep\",\n    (select count(obj.sell_date) from tab_object as obj where month(obj.install_date) = 10 and obj.brand_id = b.brand_id) as \"Oct\",\n    (select count(obj.sell_date) from tab_object as obj where month(obj.install_date) = 11 and obj.brand_id = b.brand_id) as \"Nov\",\n    (select count(obj.sell_date) from tab_object as obj where month(obj.install_date) = 12 and obj.brand_id = b.brand_id) as \"Dec\"\nfrom  tab_brand as b\nleft join tab_object as o on b.brand_id = o.brand_id\n                            and year(o.sell_date) = 2012\nwhere b.brand_id in (1234, 1324, 1423, 2314)\ngroup by b.brand_id\norder by b.brand_id;\n</code></pre>\n"},{"tags":["java","arrays","performance","search","sorting"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":107,"score":0,"question_id":12257403,"title":"Sorting 2D array or sorting 1D + Linear search.","body":"<p>Just a quick question.</p>\n\n<p>Which option would be more efficient?</p>\n\n<ol>\n<li>Sorting a 2D array (Each value in the 1st dimension of the array is linked to the value in the 2nd so they must be sorted equally [value and ID number] ).</li>\n</ol>\n\n<p>or </p>\n\n<ol>\n<li>Sort a 1D array and then compare(using linear search) values against another set of values to check if they match (in order to find which ID number corresponds to each value). </li>\n</ol>\n\n<p>All values are guaranteed to be different so there is no problem with duplication of numbers. Even if there was it wouldn't matter.</p>\n\n<p>Is there a way of finding out how efficient both methods are in the debugger? </p>\n\n<p>Thank you all for your time. =]</p>\n\n<p>Seb</p>\n"},{"tags":["performance","oracle","query","partitioning"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":73,"score":1,"question_id":12711094,"title":"Performance tuning - Query on partitioned vs non-partitioned table","body":"<p>I have two queries, one of which involves a partitioned table in the query while the other query is the same except that it involves the non-partitioned equivalent table. The original (non-partitioned table) query is performing better than the partitioned counter-part. I am not sure how to isolate the problem for this. Looking at the execution plan, I find that the indexes used are the same b/w the two queries and that the new query shows the PARTITION RANGE clause in its execution plan meaning that partition pruning is taking place. The query is of the following form:-</p>\n\n<pre><code>Select rownum, &lt;some columns&gt; \nfrom partTabA \ninner join tabB on condition1\ninner join tabC on condition2\nwhere partTabA.column1=&lt;value&gt; and &lt;other conditions&gt;\nand partTabA.column2 in (select columns from tabD where conditions) \n</code></pre>\n\n<p>where partTabA is the partitioned table and partTabA.column1 is the partitioning key(range partition). In the original query, this gets replaced by the non-partitioned equivalent of the same table. What parameters should I look at to find out why the new query performs badly. Tool that I have is Oracle SQL Developer.</p>\n"},{"tags":["arrays","performance","matlab","optimization","filtering"],"answer_count":3,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":80,"score":3,"question_id":12381377,"title":"Optimization of timestamp filter in MATLAB - Working with very large datasets","body":"<p>I am writing a program in MATLAB (must use MATLAB and can't really use a MEX) to filter very large amounts of data.</p>\n\n<p>One of the filters I need to implement requires me to compare a timestamp vector versus a list of known \"bad\" times around which other timestamps cannot occur.</p>\n\n<p>A typical timestamp vector has about 2,000,000 entries, and I have a list of about 300,000 \"bad times.\"</p>\n\n<p>Here's an working example, if <code>TIME=[1, 2.3, 5.5, 9.1, 10];</code>, and <code>BAD_TIMES=[5.2, 9.3];</code>, and we have a tolerance <code>tolerance=0.25;</code>, then all timestamps in <code>TIME</code> between <code>4.95 and 5.45</code> and <code>9.05 and 9.55</code> must be erased. This means that the cleaned vector <code>TIME_CLEAN</code> should be equal to <code>TIME_CLEAN=[1, 2.3, 5.5, 10];</code>.</p>\n\n<p>This problem is straightforward to solve, and I have solved it in about 4 or 5 different ways. However, for a 1,000,000 timestamp job, this problem can easily take an hour to compute.</p>\n\n<p>I am looking to solve this type of problem in under 2 minutes on a typical Core-i7 workstation for this filter to be viable with this many time entries.</p>\n\n<p>I have included a working version of this code. I understand code vectorization and functions such as <code>bsxfun()</code> can help, but the improvement is marginal relative to the type of efficiency I require for this filter.</p>\n\n<p>Are there any very clever ways to solve this problem in a very efficient fashion? Any help would be greatly appreciated.</p>\n\n<p>P.S. The code below is complete; it generates all data needed to setup the problem and solves it (although VERY slowly!). Change the <code>NO_OF_TIMESTAMPS</code> variable to something larger (such as 1,000,000) to watch it crawl!</p>\n\n<pre><code>clear all %% CLEAR WORKSPACE\nclose all %% CLOSE FIGURES\nclc %% CLEAR COMMAND WINDOW\n\nNO_OF_TIMESTAMPS=10000; %% NUMBER OF TIMESTAMPS IN ORIGINAL DATA\n\nTOLERANCE=2; %% TOLERANCE AROUND TIMESTAMP\n\nA=sort(randi(NO_OF_TIMESTAMPS/10,NO_OF_TIMESTAMPS,1)); %% GENERATE ARTIFICIAL TIMESTAMPS\n\nB=unique(sort(round(randi([NO_OF_TIMESTAMPS/2,NO_OF_TIMESTAMPS*5],[NO_OF_TIMESTAMPS/10,1])/10))); %% GENERATE ARTIFICIAL LIST OF BAD TIMESTAMPS\n\nB_LB=B-TOLERANCE; %% CREATE A LIST OF LOWERBOUND BAD TIMESTAMPS\nB_UB=B+TOLERANCE; %% CREATE A LIST OF UPPERBPUND BAD TIMESTAMPS\nB_RANGE=[B_LB B_UB]; %% AUGMENTED MATRIX COMPOSED OF VECTORS B_LB and B_UB\n\nA_ROWS=size(A,1); %% SIZE OF A;\n\nB_ROWS=size(B,1); %% SIZE OF B;\n\ntic; %% START TIMER\n\nA_TO_CLEAN=ones(A_ROWS,1); %% BOOLEAN VECTOR TO BE USED IN FILTERING\nfor ii=1:A_ROWS\n\n    for jj=1:B_ROWS\n\n        if A(ii)&gt;=B_RANGE(jj,1) &amp;&amp; A(ii)&lt;=B_RANGE(jj,2) %% CHECK EACH MEMBER OF A VERSUS EACH MEMBER OF B_RANGE\n\n           A_TO_CLEAN(ii)=0; %% SET INDEX VECTOR A_TO_CLEAN = 0 SO THAT WE CAN DELETE LATER\n\n           break; %% A(ii) CAN ONLY BE ERASED ONCE, SO BREAK jj LOOP AND GO TO NEXT ii\n\n        end\n\n    end\n\nend\n\nCLEAN=A(~~A_TO_CLEAN); %% DELETE A VIA LOGICAL INDEXING\n\ntoc; %% END TIMER\n\nclearvars -except A B_RANGE CLEAN %% ONLY SHOW RELEVANT VARIABLES\n</code></pre>\n"},{"tags":["java","performance","hashmap"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":60,"score":3,"question_id":12713260,"title":"How does initial capacity impact the performance of a HashMap","body":"<p>The <a href=\"http://docs.oracle.com/javase/1.4.2/docs/api/java/util/HashMap.html\" rel=\"nofollow\">documentation for HashMap</a> contains the following statement:</p>\n\n<blockquote>\n  <p>Thus, it's very important not to set the initial capacity too high (or\n  the load factor too low) if iteration performance is important.</p>\n</blockquote>\n\n<p>Could someone please explain it. I don't see any method for changing or affecting the load factor of a HashMap </p>\n"},{"tags":["iphone","ios","performance","phonegap","sencha-touch"],"answer_count":5,"favorite_count":7,"up_vote_count":11,"down_vote_count":0,"view_count":10794,"score":11,"question_id":5220377,"title":"How does Sencha Touch + PhoneGap perform compared to native apps in terms of speed?","body":"<p>I'm really worried that when I write iPhone app with Sencha Touch and put it in PhoneGap container the user experience would downgrade.</p>\n\n<p>I particularly see the bottlenecks  in:</p>\n\n<ul>\n<li>fluency of the screen transitions (animations)</li>\n<li>fluency of scrolling</li>\n</ul>\n\n<p>Please have in mind that there are lot of 3G iPhones runnin iOS 4.x that made them very slow. I'm discarding the support for the original iPhone.</p>\n\n<p>I, being a trained UI professional, can spot the ST app just by touching few things in it.</p>\n\n<p>Does the shift from Safari to PhoneGap container increases the performance?</p>\n\n<p>Do you have any experience with it?</p>\n"},{"tags":["php","html","performance","include"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":45,"score":0,"question_id":12713974,"title":"PHP output: file include vs. function (performance)","body":"<p>Here's two methods to display html output: <strong>function</strong> vs <strong>include</strong>.  Is there a performance hit when accessing the file system as opposed to accessing memory?  If each page load has dozens, or even hundreds of includes, at what point does this become a problem?</p>\n\n<p>Option 1:  Html display loop using a <strong>function</strong></p>\n\n<pre><code>foreach ($items as $item){\n    displayItem($item);\n}\n\nfunction displayItem($item){ ?&gt;\n    &lt;html output&gt;\n&lt;?php }\n</code></pre>\n\n<p>Option 2:  Html display loop using <strong>include</strong></p>\n\n<pre><code>foreach ($items as $item){\n    include $path . 'displayItem.php';\n}\n\n//inside displayItem.php:\n&lt;html output&gt;\n</code></pre>\n"},{"tags":["xml","json","performance","variables"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":14,"score":0,"question_id":12712612,"title":"XML tag vs tag var speed?","body":"<p>One was wondering what would perform faster in a sense of parsing(only read).\nOne does not have knowledge to perform such a test, could thee show me the light kind sir-s?</p>\n\n<p>Decided not to use JSON.</p>\n\n<p>1.</p>\n\n<pre><code>&lt;tag&gt;variable&lt;/tag&gt;\n</code></pre>\n\n<p>2.</p>\n\n<pre><code>&lt;tag var=\"variable\"&gt;&lt;/tag&gt;\n</code></pre>\n"},{"tags":["c++","windows","performance","stack"],"answer_count":3,"favorite_count":0,"up_vote_count":5,"down_vote_count":7,"view_count":242,"score":-2,"question_id":12704314,"title":"std stack performance issues","body":"<p>Recently I was trying to do some performance benchmarks, comparing <code>std::stack&lt;int, std::vector&lt;int&gt;&gt;</code> and my own simple implementation of stack (that use pre-allocated memory). Now I’m experiencing some strange behavior.  </p>\n\n<p><strong>First thing I want to ask</strong> is this line in stack benchmark code:</p>\n\n<pre><code>//  std::vector&lt;int&gt; magicVector(10);\n</code></pre>\n\n<p>When I uncomment this line, performance rise about 17% (benchmark time drops from 6.5 to 5.4 seconds). But the line should have no impact on the rest of the program because it's not modifying any others members. Besides, it doesn't matter if it is vector of int or vector of double...  </p>\n\n<p><strong>Second thing I want to ask</strong> is big performance difference between my stack implementation and <code>std::stack</code>. I was told that <code>std::stack</code> should be as fast as my stack but results shows that my \"FastStack\" is twice as fast.  </p>\n\n<p><strong>Results</strong> (with uncommented performance increasing line):<br>\nstack 5.38979<br>\nstack 5.34406<br>\nstack 5.32404<br>\nstack 5.30519<br>\nFastStack 2.59635<br>\nFastStack 2.59204<br>\nFastStack 2.59713<br>\nFastStack 2.64814  </p>\n\n<p>These results come from release build from VS2010 with /O2, /Ot, /Ob2 and others default optimizations. My CPU is Intel i5 3570k with default clock (3.6 GHz for one thread).  </p>\n\n<p>I put all code in one file so anyone can easily test it.  </p>\n\n<pre><code>#define _SECURE_SCL 0\n\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;stack&gt;\n#include &lt;Windows.h&gt;\n\nusing namespace std;\n\n//---------------------------------------------------------------------------------\n//---------------------------------------------------------------------------------\n//  Purpose:    High Resolution Timer\n//---------------------------------------------------------------------------------\n\nclass HRTimer\n{\npublic:\n    HRTimer();\n    double GetFrequency(void);\n    void Start(void) ;\n    double Stop(void);\n    double GetTime();\n\nprivate:\n    LARGE_INTEGER start;\n    LARGE_INTEGER stop;\n    double frequency;\n};\n\nHRTimer::HRTimer()\n{\n    frequency = this-&gt;GetFrequency();\n}\n\ndouble HRTimer::GetFrequency(void)\n{\n    LARGE_INTEGER proc_freq;\n    if (!::QueryPerformanceFrequency(&amp;proc_freq))\n        return -1;\n    return proc_freq.QuadPart;\n}\n\nvoid HRTimer::Start(void)\n{\n    DWORD_PTR oldmask = ::SetThreadAffinityMask(::GetCurrentThread(), 0);\n    ::QueryPerformanceCounter(&amp;start);\n    ::SetThreadAffinityMask(::GetCurrentThread(), oldmask);\n}\n\ndouble HRTimer::Stop(void)\n{\n    DWORD_PTR oldmask = ::SetThreadAffinityMask(::GetCurrentThread(), 0);\n    ::QueryPerformanceCounter(&amp;stop);\n    ::SetThreadAffinityMask(::GetCurrentThread(), oldmask);\n    return ((stop.QuadPart - start.QuadPart) / frequency);\n} \n\ndouble HRTimer::GetTime()\n{\n    LARGE_INTEGER time;\n    ::QueryPerformanceCounter(&amp;time);\n    return time.QuadPart / frequency;\n}\n\n//---------------------------------------------------------------------------------\n//---------------------------------------------------------------------------------\n//  Purpose:    Should be faster than std::stack\n//---------------------------------------------------------------------------------\n\ntemplate &lt;class T&gt;\n\nclass FastStack\n{\npublic:\n    T* st;\n    int allocationSize;\n    int lastIndex;\n\npublic:\n    FastStack(int stackSize);\n    ~FastStack();\n\n    inline void resize(int newSize);\n    inline void push(T x);\n    inline void pop();\n    inline T getAndRemove();\n    inline T getLast();\n    inline void clear();\n};\n\ntemplate &lt;class T&gt;\nFastStack&lt;T&gt;::FastStack( int stackSize )\n{\n    st = NULL;\n    this-&gt;allocationSize = stackSize;\n    st = new T[stackSize];\n    lastIndex = -1;\n}\n\ntemplate &lt;class T&gt;\nFastStack&lt;T&gt;::~FastStack()\n{\n    delete [] st;\n}\n\ntemplate &lt;class T&gt;\nvoid FastStack&lt;T&gt;::clear()\n{\n    lastIndex = -1;\n}\n\ntemplate &lt;class T&gt;\nT FastStack&lt;T&gt;::getLast()\n{\n    return st[lastIndex];\n}\n\ntemplate &lt;class T&gt;\nT FastStack&lt;T&gt;::getAndRemove()\n{\n    return st[lastIndex--];\n}\n\ntemplate &lt;class T&gt;\nvoid FastStack&lt;T&gt;::pop()\n{\n    --lastIndex;\n}\n\ntemplate &lt;class T&gt;\nvoid FastStack&lt;T&gt;::push( T x )\n{\n    st[++lastIndex] = x;\n}\n\ntemplate &lt;class T&gt;\nvoid FastStack&lt;T&gt;::resize( int newSize )\n{\n    if (st != NULL)\n        delete [] st;\n    st = new T[newSize];\n}\n//---------------------------------------------------------------------------------\n//---------------------------------------------------------------------------------\n//---------------------------------------------------------------------------------\n//  Purpose:    Benchmark of std::stack and FastStack\n//---------------------------------------------------------------------------------\n\n\nint main(int argc, char *argv[])\n{\n#if 1\n    for (int it = 0; it &lt; 4; it++)\n    {\n        std::stack&lt;int, std::vector&lt;int&gt;&gt; bStack;\n        int x;\n\n        for (int i = 0; i &lt; 100; i++)   // after this two loops, bStack's capacity will be 141 so there will be no more reallocating\n            bStack.push(i);\n        for (int i = 0; i &lt; 100; i++)\n            bStack.pop();\n    //  std::vector&lt;int&gt; magicVector(10);           // when you uncomment this line, performance will magically rise about 18%\n\n        HRTimer timer;\n        timer.Start();\n\n        for (int i = 0; i &lt; 2000000000; i++)\n        {\n            bStack.push(i);\n            x = bStack.top();\n            if (i % 100 == 0 &amp;&amp; i != 0)\n                for (int j = 0; j &lt; 100; j++)\n                    bStack.pop();\n        }\n\n        double totalTime = timer.Stop();\n        cout &lt;&lt; \"stack \" &lt;&lt; totalTime &lt;&lt; endl;\n    }\n#endif\n\n    //------------------------------------------------------------------------------------\n\n#if 1\n    for (int it = 0; it &lt; 4; it++)\n    {\n        FastStack&lt;int&gt; fstack(200);\n        int x;\n\n        HRTimer timer;\n        timer.Start();\n\n        for (int i = 0; i &lt; 2000000000; i++)\n        {\n            fstack.push(i);\n            x = fstack.getLast();\n            if (i % 100 == 0 &amp;&amp; i != 0)\n                for (int j = 0; j &lt; 100; j++)\n                    fstack.pop();\n        }\n\n        double totalTime = timer.Stop();\n        cout &lt;&lt; \"FastStack \" &lt;&lt; totalTime &lt;&lt; endl;\n    }\n#endif\n\n    cout &lt;&lt; \"Done\";\n    cin.get();\n    return 0;\n}\n</code></pre>\n\n<p>.<br>\n<strong>EDIT:</strong> Since everybody talks about my really bad implementation of my stack I want to set things right. I created that stack in few minutes and I implemented just few features that I currently needed. It was never meant to be a replacement of std::stack :) or save to use in all cases. The only goal was to achieve maximum speed and correct results. I'm sorry about this misunderstanding… I just want to know few answers…</p>\n"},{"tags":["c++","performance","memory-management","gcc"],"answer_count":3,"favorite_count":143,"up_vote_count":363,"down_vote_count":1,"view_count":49842,"score":362,"question_id":12264970,"title":"Why is my program slow when looping over exactly 8192 elements?","body":"<p>Here is the extract from the program in question. The matrix <code>img[][]</code> has the size SIZE×SIZE, and is initialized at:</p>\n\n<p><code>img[j][i] = 2 * j + i</code></p>\n\n<p>Then, you make a matrix <code>res[][]</code>, and each field in here is made to be the average of the 9 fields around it in the img matrix. The border is left at 0 for simplicity.</p>\n\n<pre><code>for(i=1;i&lt;SIZE-1;i++) \n    for(j=1;j&lt;SIZE-1;j++) {\n        res[j][i]=0;\n        for(k=-1;k&lt;2;k++) \n            for(l=-1;l&lt;2;l++) \n                res[j][i] += img[j+l][i+k];\n        res[j][i] /= 9;\n}\n</code></pre>\n\n<p>That's all there's to the program. For completeness' sake, here is what comes before. No code comes after. As you can see, it's just initialization.</p>\n\n<pre><code>#define SIZE 8192\nfloat img[SIZE][SIZE]; // input image\nfloat res[SIZE][SIZE]; //result of mean filter\nint i,j,k,l;\nfor(i=0;i&lt;SIZE;i++) \n    for(j=0;j&lt;SIZE;j++) \n        img[j][i] = (2*j+i)%8196;\n</code></pre>\n\n<p>Basically, this program is slow when SIZE is a multiple of 2048, e.g. the execution times:</p>\n\n<pre><code>SIZE = 8191: 3.44 secs\nSIZE = 8192: 7.20 secs\nSIZE = 8193: 3.18 secs\n</code></pre>\n\n<p>The compiler is GCC.\nFrom what I know, this is because of memory management, but I don't really know too much about that subject, which is why I'm asking here.</p>\n\n<p>Also how to fix this would be nice, but if someone could explain these execution times I'd already be happy enough.</p>\n\n<p>I already know of malloc/free, but the problem is not amount of memory used, it's merely execution time, so I don't know how that would help.</p>\n"},{"tags":["sql","xml","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":665,"score":1,"question_id":7325256,"title":"SQL Server 2008 XML column performance Issue","body":"<p>We have a table with an XML column holding quite a bit of data, this has worked fine in our dev environments but as the table grew in size (close to 10,000 rows) we started seeing performance issues.</p>\n\n<p>Just doing <code>SELECT *</code> takes 12 seconds alone...</p>\n\n<p>Any suggestions to remedy this?</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["sql","mysql","performance","nosql","key-value-store"],"answer_count":4,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":1017,"score":2,"question_id":3077578,"title":"Scalability of Using MySQL as a Key/Value Database","body":"<p>I am interested to know the performance impacts of using MySQL as a key-value database vs. say Redis/MongoDB/CouchDB.  I have used both Redis and CouchDB in the past so I'm very familiar with their use cases, and know that it's better to store key/value pairs in say NoSQL vs. MySQL.</p>\n\n<p>But here's the situation:</p>\n\n<ul>\n<li>the bulk of our applications already have lots of MySQL tables</li>\n<li>We host everything on Heroku (which only has MongoDB and MySQL, and is basically 1-db-type per app)</li>\n<li>we don't want to be using multiple different databases in this case.</li>\n</ul>\n\n<p>So basically, I'm looking for some info on the scalability of having a key/value table in MySQL.  Maybe at three different arbitrary tiers:</p>\n\n<ul>\n<li>1000 writes per day</li>\n<li>1000 writes per hour</li>\n<li>1000 writes per second</li>\n<li>1000 reads per hour</li>\n<li>1000 reads per second</li>\n</ul>\n\n<p>A practical example is in building something like <a href=\"http://mixpanel.com/report/85/events\" rel=\"nofollow\">MixPanel's Real-time Web Analytics Tracker</a>, which would require writing very often depending on traffic.</p>\n\n<p>Wordpress and other popular software use this all the time: Post has \"Meta\" model which is just key/value, so you can add arbitrary properties to an object which can be searched over.</p>\n\n<p>Another option is to store a serializable hash in a blob but that seems worse.</p>\n\n<p>What is your take?</p>\n"},{"tags":["c++","performance","for-loop","stdvector"],"answer_count":4,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":139,"score":3,"question_id":12710138,"title":"c++ fastest loop scan over 3D vector","body":"<p>I want to know what is the fastest way as possible to perform a method on each element of a 3D vector.</p>\n\n<p>Suppose we have:</p>\n\n<pre><code>std::vector&lt;vector&lt;vector&lt;CLS&gt;&gt;&gt; myVec;\n</code></pre>\n\n<p>I want to do the following loops in the fastest way as possible:</p>\n\n<pre><code> for(int cycle=0;cycle&lt;10;cycle++) // do it 10 times\n {\n    for(int i=0;i&lt;myVec.size();i++)\n    {\n       for(int j=0;j&lt;myVec[i][constant].size();j++)\n       {\n           foo(myVec[i][constant][j]);\n       }\n\n    } \n }\n</code></pre>\n\n<p>It's well mentioning that the middle term index is always <strong>constant</strong> in my case.\nIs using std::vector fast enough or you suggest another type container?</p>\n\n<p>Looking forward to your help.Thanks</p>\n"},{"tags":["c#","performance","linq","search","icollection"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":140,"score":3,"question_id":12673830,"title":"For LINQ Performance Gurus- Searching through non-generic ICollection","body":"<p>I need to search through a collection of \"Item\" objects that contain a time property. I have a fast solution in place but its very messy (will post if nothing better comes up). Below are my attempts at performing the search myself using LINQ and whatnot.</p>\n\n<p>In my particular case, I <em>know</em> the items are ordered low to high based on time. When I iterate through them, it's 9/12, 9/13, 9/14. I'd like to find a solution that's fast even if this isn't ordered, but not important right now.</p>\n\n<pre><code>//ICollection c = GetCollection(); //25,000+ items\nDateTime TIME = DateTime.Now.AddDays(-1);\n\nEventLog scan = new EventLog(\"Application\", \"Server\", \"N/A\");\nEventLogCollection c = scan.Entries;\nConsole.WriteLine(logs.Count); // All entries already in list here\n\n// 64 sec - SLOW\nList&lt;Item&gt; l1 = new List&lt;Item&gt;();\nforeach (Item i in c) { \n  if (i.time &gt; TIME) { \n    l1.Add(i); }\n}\n\n// 93 sec - SLOWER\nvar l2 = c.Cast&lt;Item&gt;().AsParallel().Select(n =&gt; n.time &gt; TIME);\nvar i = l2.Count();\n\n// 98 sec - EVEN SLOWER!\nList&lt;Item&gt; l3 = new List&lt;Item&gt;();\nParallel.ForEach(c.Cast&lt;Item&gt;(), n =&gt; {\n    if (n.time &gt; TIME) {\n        l3.add(n);\n    }\n});\n</code></pre>\n\n<p>My current solution is do a BinarySearch for the start and end times and loop through the ICollection based on those indexes. It's very fast (1-2 sec) but very messy. I don't <em>need</em> a different solution but thought I'd toss this out to you performance experts.</p>\n\n<p><strong>Is there a faster, elegant to way to search the ICollection?</strong> BTW I have no control over the collection I'm given and can't change it's structure. .NET 4.0</p>\n\n<p>And no, I can't use System.Diagnostics.Eventing.Reader because I'm stuck with Windows XP</p>\n"},{"tags":["c#","c++","performance","image-processing","dllimport"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":134,"score":1,"question_id":12709153,"title":"Using C++ dll functions to improve performance in C# project","body":"<p>For years a have heard that C++ is fast, so, it is good for image processing, by example.</p>\n\n<p>If i am developing a system in C# that need to do some image processing and i want to improve your performance, whats the best aproach?</p>\n\n<p>1 - Do it all in C# and do the image processing function embraced with \"unmanaged\"</p>\n\n<p>2 - Write the function that process de image in C++ and import this function to my C# code?</p>\n\n<p>Another questions?</p>\n\n<p>Which one improves the performance?</p>\n"},{"tags":["android","performance","swipe"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":25,"score":0,"question_id":12709692,"title":"Android: Swipe to Hide Top Navigation","body":"<p>I want to make that when you open activity, you would see three layers basically: Featured elements, navigation and general content. Then, if you swipe upwards, featured elements would hide and navigation would stay at the top no matter how much you continue swiping (and general content may get quite long).</p>\n\n<p>Is it possible to implement such thing without much performance impact?</p>\n"},{"tags":["c","linux","performance","gcc"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":256,"score":2,"question_id":12670667,"title":"Why instrumented C program runs faster?","body":"<p>I am working on a (quite large) existing monothreaded C application. In this context I modified the application to perform some very few additional work consisting in incrementing a counter each time we call a special function (this function is called ~ 80.000 times). The application is compiled on an Ubuntu 12.04 running a 64 bits Linux kernel 3.2.0-31-generic with -O3 option.</p>\n\n<p>Surprisingly the instrumented version of the code is running faster and I am investigating why.I measure execution time with <code>clock_gettime(CLOCK_PROCESS_CPUTIME_ID)</code> and to get representative results, I am reporting an average execution time value over 100 runs. Moreover, to avoid interference from outside world, I tried as much as possible to launch the application in a system without any other applications running (on a side note, because CLOCK_PROCESS_CPUTIME_ID returns process time and not wall clock time, other applications \"should\" in theory only affect cache and not directly the process execution time)</p>\n\n<p>I was suspecting \"instruction cache effects\", maybe the instrumented code that is a little bit larger (few bytes) fits differently and better in the cache, is this hypothesis conceivable ? I tried to do some cache investigations with valegrind --tool=cachegrind but unfortunately, the instrumented version has (as it seems logical) more cache misses than the initial version.</p>\n\n<p>Any hints on this subject and ideas that may help to find why instrumented code is running faster are welcomes (some GCC optimizations available in one case and not in the other, why ?, ...)</p>\n"},{"tags":["performance","postgresql","database-performance","xa","pgfouine"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":54,"score":0,"question_id":12705412,"title":"PostgreSQL. Slow Prepare Transaction and Commit Prepared","body":"<p>I've just encountered a strange problem. I've made a report in pgfourine, and found out that  my XA transactions started to work really slow. Prepare transaction and commit prepared combined took 12.55s out of 13.2s. But why?</p>\n\n<pre><code>#####  Overall statistics  #####\n\nNumber of unique normalized queries: 175\nNumber of queries:     268,772\nTotal query duration:  13m2s\n\n\n#####  Queries by type  #####\n\nSELECT:    116493     43.3%\nINSERT:     15926      5.9%\nUPDATE:      7935      3.0%\nDELETE:      4923      1.8%\n\n\n#####  Queries that took up the most time (N)  #####\n\n1) 6m32s - 26,338 - COMMIT PREPARED ''\n--\n2) 6m23s - 25,972 - PREPARE TRANSACTION ''\n--\n3) 0.6s - 3,848 - update avatar set lfa_position=NULL where Id=0\n.....\n7) 0.3s - 21,514 - COMMIT\n.....\n</code></pre>\n\n<p>I have a theory but don't have a proof.. I have slow discs and I turned off synchronous_commit. Maybe PostgreSQL has to make an fsync during \"prepare transaction\" even if  synchronous_commit is off?  </p>\n\n<pre><code>fsync = on \nsynchronous_commit = off\n</code></pre>\n\n<p>Any ideas?</p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>Same tests with </p>\n\n<pre><code>fsync = off\nsynchronous_commit = off\n\n\n#####  Overall statistics  #####\n\nNumber of unique normalized queries: 155\nNumber of queries:     186,838\nTotal query duration:  6.6s\n\n\n#####  Queries by type  #####\n\nSELECT:     84367     45.2%\nINSERT:      9197      4.9%\nUPDATE:      5486      2.9%\nDELETE:      2996      1.6%\n\n\n#####  Queries that took up the most time (N)  #####\n\n1) 1.8s - 16,972 - PREPARE TRANSACTION ''\n--\n2) 1.1s - 16,965 - COMMIT PREPARED ''\n--\n3) 0.4s - 2,904 - update avatar set lfa_position=NULL where Id=0\n--\n4) 0.2s - 16,031 - COMMIT\n</code></pre>\n\n<p>Looks like fsync took vast amount of time, but not all the time. 16k commits - 0.2sec, 17k prepare+commit 2.9sec. </p>\n\n<p>Sad story. Looks like XA commit tooks 15 times more time than local commit and doesn't take in account synchronous_commit setting. fsync=off is not safe for production use. So if I want to use XA transactions I have to use it carefully and use a good SSD drive with high IOPS.</p>\n"},{"tags":["php","mysql","performance","query"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":12708796,"title":"How to remove files after LOAD DATA INFILE?","body":"<p>I'm using the <code>LOAD DATA INFILE</code> solution to speed up big inserts, like outlined here: <a href=\"http://www.mysqlperformanceblog.com/2006/07/12/insert-into-select-performance-with-innodb-tables/\" rel=\"nofollow\">http://www.mysqlperformanceblog.com/2006/07/12/insert-into-select-performance-with-innodb-tables/</a>. </p>\n\n<p>It works brilliantly, and very fast. However, when I do the select, I create a file, which is then used by the <code>LOAD DATA INFILE</code> command. But after using this file, I don't need it any more. Can I get rid of it with an additional query? Or can I make it temporary somehow?</p>\n\n<p>I'm using it on my server, so the file stays on my server and doesn't involve the client in any way. I'm using PHP in conjunction with MySQL, so the solution may also be in PHP.</p>\n"},{"tags":["performance","measurement"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":28,"score":1,"question_id":12708465,"title":"How to measure Windows application response times?","body":"<p>I'm looking for a solution/software to mimic a regular desktop user and measure how long different operations on a software take. For example, how long it takes to open the software at different times, how long it takes for a program respond when user clicks button A etc. I don't want to get to API/code level. I'd like to set up different tests like \"1. open sofware 2. click button A 3. click button B from the opening window and log the execution time\".</p>\n\n<p>All questions I found here were related to code-level / server performance measurements.</p>\n"},{"tags":["python","performance","pymongo","tweepy"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":71,"score":1,"question_id":12706607,"title":"See which Python class is using my 100% CPU?","body":"<p>I'm running a python script with a few imported classes (like Tweepy and PyMongo). I'm running into performance issues (aka clogging at 100% of CPU). I'd like to know which of my Python classes are resonsible!</p>\n\n<p>Is that possible?</p>\n\n<p>(Detailed description of my issues <a href=\"http://stackoverflow.com/questions/12698949/efficiency-when-inserting-into-mongodb-pymongo\">here</a>.)</p>\n"},{"tags":["python","database","performance","mongodb","pymongo"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":82,"score":1,"question_id":12698949,"title":"Efficiency when inserting into mongodb (pymongo)","body":"<p><strong>Updated for clarity:</strong> I need advice for performance when inserting/appending to a <code>capped collection</code>. I have two python scripts running: </p>\n\n<p>(1) Tailing the cursor. </p>\n\n<pre><code>while WSHandler.cursor.alive:\n        try:\n            doc = WSHandler.cursor.next()\n            self.render(doc)\n</code></pre>\n\n<p>(2) Inserting like so:</p>\n\n<pre><code>def on_data(self, data):                      #Tweepy\n    if (len(data) &gt; 5):\n        data = json.loads(data)\n        coll.insert(data)                     #insert into mongodb\n        #print(coll.count())\n        #print(data)\n</code></pre>\n\n<p>and it's running fine for a while (at 50 inserts/second). Then, after 20-60secs, it stumbles, hits the cpu roof (though it was running at 20% before), and never recovers. My mongostats take a dive (the dive is shown below).</p>\n\n<p>Mongostat output:\n<img src=\"http://i.stack.imgur.com/TMlA9.png\" alt=\"Mongostat output\"></p>\n\n<p>The CPU is now choked, by the processes doing the insertion (at least according to <code>htop</code>).</p>\n\n<p>When I run the Tweepy lines above with <code>print(data)</code> instead of adding it to db (<code>coll.insert(data)</code>), everything's running along fine at 15% cpu use. </p>\n\n<p>What I see in mongostats:</p>\n\n<ul>\n<li><code>res</code> keeps climbing. (Though clogs may happen at 40m as well as run fine on 100m.)</li>\n<li><code>flushes</code> do not seem to interfere.</li>\n<li><code>locked %</code> is stable at 0.1%. Would this lead to clogging eventually?</li>\n</ul>\n\n<p>(I'm running AWS microinstance; pymongo.)</p>\n"},{"tags":["c#","performance","collections","containers","c5"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":61,"score":3,"question_id":12699252,"title":"Small sized collections from C5 Generic Collection Library are comparatively very slow - can anything be done?","body":"<p>I've recently been testing C5 collections in C# and I'm loving their functionality. For large collections the performance seems on par with the generic counterparts. For small collections however they are significantly slower. I suspect the dramatic deterioration in relative speed comes from constant time operations performed by C5 collections. One operation I know of is firing of events. Could this be the cause of poor performance for small collections? Can this perhaps be remedied by turning some functionality off? Here' the performance test:</p>\n\n<pre><code>//Two containers to be tested. 'Test' is a wrapper over decimal.\nvar arrayList = new C5.ArrayList&lt;Test&gt;();\nvar genericList = new System.Collections.Generic.List&lt;Test&gt;();\n\nvar toBeAdded = new List&lt;Test&gt;();\nvar watch = new Stopwatch();\n\n//Fill both tested containers\nfor (int i = 10; i &gt; 0; i--)\n{\n    var test = new Test(i);\n    genericList.Add(test);\n    arrayList.Add(test);\n}\n\n//Fill the container the range of which will be inserted to the tested containers\nfor (int i = 5; i &gt; 0; i--)\n{\n    toBeAdded.Add(new Test(i+0.5m));\n}\n\n\n//Test the speed of adding a range of values and sorting for both containers\nwatch.Start();\ngenericList.AddRange(toBeAdded);\nConsole.WriteLine(\"Adding values for generic list: {0} ticks\", watch.ElapsedTicks);\nwatch.Restart();\ngenericList.Sort();\nConsole.WriteLine(\"Sorting for generic list: {0} ticks\", watch.ElapsedTicks);\n\nwatch.Restart();\narrayList.AddAll(toBeAdded);\nConsole.WriteLine(\"Adding values for C5 ArrayList: {0} ticks\", watch.ElapsedTicks);\nwatch.Restart();\narrayList.Sort();\nConsole.WriteLine(\"Sorting for C5 ArrayList: {0} ticks\", watch.ElapsedTicks);\n</code></pre>\n\n<p>and the Test class:</p>\n\n<pre><code>class Test : IComparable\n{\n    private decimal _number;\n    internal Test(decimal aNumber)\n    {\n        _number = aNumber;\n    }        \n    public int CompareTo(object obj)\n    {\n        var test = (Test) obj;\n        return _number.CompareTo(test._number);\n    } \n}\n</code></pre>\n\n<p>The output is:</p>\n\n<pre><code>Adding values for generic list: 56 ticks\nSorting for generic list: 770 ticks\nAdding values for C5 ArrayList: 3575 ticks\nSorting for C5 ArrayList: 4815 ticks\n</code></pre>\n\n<p>Both C5 and the test are Release builds. The ratio of speeds of around 60x for inserting and 6x for sorting is consistent between test runs.</p>\n\n<p>EDIT: The above test was ran from within VS. The results for running outside of VS are:</p>\n\n<pre><code>Adding values for generic list: 54 ticks\nSorting for generic list: 2135 ticks\nAdding values for C5 ArrayList: 5765 ticks\nSorting for C5 ArrayList: 5198 ticks\n</code></pre>\n\n<p>Again, the ratio of speeds of around 100x for inserting and 2x for sorting is consistent between test runs.</p>\n\n<p>My project includes a lot of manipulating of small containers and their performance is paramount. The functionality of C5 containers is great and I'd love to use them, but at the moment can't for performance reasons. I'd appreciate any insight on the matter.</p>\n\n<p>EDIT2: As per Iridium answer, I performed the test in a loop (putting the whole logic including the container creation into the the loop so as to exclude any compiler optimization tricks), discarded the first two results and averaged subsequent 1000 results. Here they are:</p>\n\n<pre><code>Adding values for generic list: 1.09 ticks\nSorting for generic list: 14.07 ticks\nAdding values for C5 ArrayList: 1.92 ticks\nSorting for C5 ArrayList: 13.69 ticks\n</code></pre>\n\n<p>Now C5 insertion is 76% slower and sorting is on par with that of List. That's good enough for my purpose. I'm accepting Iridium's answer. Still, if anyone has any insight on the slower insertion, please share it. Thanks all for the help.</p>\n"},{"tags":["python","performance"],"answer_count":1,"favorite_count":2,"up_vote_count":0,"down_vote_count":0,"view_count":76,"score":0,"question_id":12702784,"title":"Efficient way to re-write this code?","body":"<p>I have to measure  the time it takes with varying L values so I want to optimize my code. What I had to do is fill a cubic box (LxLxL) with periodic points (x,y,z) of diameter d that are identical. So far this is what i have:</p>\n\n<pre><code>L=10\nd=2\n\nx,y,z = 0,0,0\ncounter=0\n\nwith open(\"question1.xyz\",\"w\") as f:\n    while x&lt;=L-d:\n        while y&lt;=L-d:\n            while z&lt;=L-d:\n                f.write('H ')\n                f.write('%f ' %x )\n                f.write('%f ' %y )\n                f.write('%f\\n' %z )\n                counter=counter+1\n                z=z+d\n            z=0\n            y=y+d\n        z,y=0,0\n        x=x+d\n</code></pre>\n\n<p>I then have to output the file (the .xyz file) that is in this format:</p>\n\n<pre><code>H 0.000000 0.000000 0.000000\nH 0.000000 0.000000 1.000000\nH 0.000000 0.000000 2.000000\nH 0.000000 0.000000 3.000000\nH 0.000000 0.000000 4.000000\n</code></pre>\n\n<p>Any ideas or suggestions? Thanks in advance!</p>\n"},{"tags":["sql-server","windows","performance","performancecounter"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":23,"score":0,"question_id":12705140,"title":"SQL server 2000 counters missing","body":"<p>When i recompiled *.mof files in winnt\\system32\\wbem, all my sql server counters disappeared in perfmon /wmi. I don't know how to find those counters and re-add them.\nHow can i rebuild sql server counter's wmi classes ? so i can see in perfmon tool or get them via wmi classes;</p>\n\n<p>Thank you in advance , </p>\n"},{"tags":["c#","performance","process"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":2,"view_count":56,"score":0,"question_id":12704947,"title":"How to improve Process performance?","body":"<p>I have a method that has to execute sql scripts for many times. These scripts use for create tables, views, stored procedures, and functions on the database. I came up with this code which works fine with 100 files.</p>\n\n<pre><code>foreach (var folder in ActivityConstant.SourceFolders)\n{\n    var destination = context.GetValue(this.DestinationPath) + folder;\n\n    // TODO: Execute all sql folders instead of each sql file.\n    // this is cmd command for %f in (*.sql) do sqlcmd /S &lt;servername&gt; /d &lt;dbname&gt; /E /i \"%f\"\n\n    foreach (var file in Directory.GetFiles(destination))\n    {\n        var begin = DateTime.Now;\n        context.TrackBuildWarning(string.Format(\"Start exec sql file at {0}.\", \n                                                DateTime.Now));\n\n        Process process = new Process();\n        process.StartInfo.UseShellExecute = false;\n        process.StartInfo.RedirectStandardOutput = true;\n        process.StartInfo.RedirectStandardError = true;\n        process.StartInfo.CreateNoWindow = true;\n        process.StartInfo.FileName = \"sqlcmd.exe\";\n        process.StartInfo.Arguments = string.Format(\"-S {0} -d {1} -i {2} -U {3} -P {4}\", \n                                                    sqlServerName, \n                                                    databaseName, \n                                                    file, \n                                                    sqlUserName, \n                                                    sqlPassword);\n        process.StartInfo.WorkingDirectory = @\"C:\\\";\n        process.Start();\n        //process.WaitForExit();\n\n        context.TrackBuildWarning(\n           string.Format(\n                \"Finished exec sql file at {0} total time {1} milliseconds.\", \n                DateTime.Now, \n                DateTime.Now.Subtract(begin).TotalMilliseconds));\n    }\n}\n</code></pre>\n\n<p>Now, I am moving on to the next step. I am testing it with our database which has around 600 files (tables, views, stored procedures, and functions) and it looks like the current code cannot deal with large amount of query like this.</p>\n\n<p>As far as I have records, it takes between 3 to 5 minutes to run around 100 files. As the time I am writing this question, it has been 40 minutes for those 600 files.</p>\n\n<p>I would like to know how can I improve my code. I also welcome any suggestion if it is better to use difference way to archieve the goal.</p>\n"},{"tags":["javascript","performance","html5","animation"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":81,"score":0,"question_id":12704109,"title":"Best way to load and play an image sequence as an animation in html5?","body":"<p>I just want to hear if anyone has any suggestions on which approach to take when it comes to playing a short animation consisting of an image sequence in hmtl5. The wider the browser support the better, and it needs to work for iOS. Canvas can be assumed.</p>\n\n<p><strong>Requirements:</strong></p>\n\n<ol>\n<li>Be able to start the animation without user interaction</li>\n<li>Have precise control of which frame to show (e.g. go to a specific frame, or start from the first frame and play up to the n:th frame)</li>\n<li>Be able to scale the animation (to fit the browser window)</li>\n<li>Be able to play at least 3 seconds at ~30fps (90 frames)</li>\n<li>Support resolution of at least 1280x720</li>\n</ol>\n\n<p>From a file size/performance point of view having the animation encoded as a video file would be best, but it doesn't fulfill 1) on iOS, and most likely fails to fulfill 2).</p>\n\n<p>The solution I have used in the past is to load the frames individually as a series of jpegs and show the frames either using image elements in the DOM or drawing the frames to a canvas. This fulfills pretty much all requirements, and it works reasonably well except that it's really slow to load. And I have only used this method for animations of about 20-30 frames. You end up with a lot of http requests and quite a lot of data.</p>\n\n<p>For some animations, frames containing lots of motion blur have been saved in a lower resolution which are then stretched when displayed to match the full resolution frames. This saves some file size and isn't really that noticeable.</p>\n\n<p>One way to reduce the amount of http requests would be to combine several images into a sprite sheet, but when the individual frames are 1280x720 you quickly end up with a sprite sheet image with a very high resolution, which I don't think will play nice with iOS.</p>\n\n<p>Are there any other approaches that I'm missing here?</p>\n"},{"tags":["performance","oracle","bulk-load"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":54,"score":0,"question_id":12700883,"title":"ORACLE 10G DELETE USING FORALL","body":"<p>I have a txn table which is having 2 million rows every month i am loading data into that using stored procedure. I need to delete the transactions using the following if it failed during my process and i used to load again .</p>\n\n<pre><code>Table TXN structure\n\nTXN_ID    NUMBER primary key \nTXN_AMT   number\nYEAR    varchar2\nMONTH   varchar2\n</code></pre>\n\n<p>Can i used bulk collect here, but in where clause i need</p>\n\n<pre><code>DELETE FROM TXN WHERE YEAR=cYear and Month=cMonth \n</code></pre>\n\n<p>please advice how i can make it fast</p>\n"},{"tags":["android","performance","android-custom-view"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":12676643,"title":"Custom view being created ultra-slowly","body":"<p>I have a custom view that extends <code>ListView</code>. Depending on the current month, it displays 24 or 36 items.</p>\n\n<p>These items display 6 rows of 7 cells each (it's a scrolling calendar). Each row is a LinearLayout I add to a ScrollView. Each row is a View I add to a LinearLayout. So, each CustomCalendar calls 7x6+6x4 = 42+24 = 66 times to addView(View), which makes for 1584 calls to addView, which is a ultra-slow process. It clogs up the main thread for a lot of time.</p>\n\n<p>Is there any way to make this process faster? I've tried onDraw(canvas), but it's too difficult and anyway I couldn't make it work properly.</p>\n\n<p>EDIT: People misunderstand my question. I have no downloaded images, I have no downloaded resources and I use no images. I only have views. My complain is that ViewGroup.addView works slow as hell, and I'm asking if there's an alternative for that.</p>\n"},{"tags":["performance","r","for-loop","apply","na"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":171,"score":3,"question_id":11197328,"title":"R - Find and Replace first NA in each column without for loops","body":"<p>Trying to do this without For Loop but can't figure it out.</p>\n\n<p>I want to replace the first NA in a column with a default value of 0.0000001.  </p>\n\n<p>I am doing Last Observation Carried Forward (LOCF) imputation but want to give it a default value.  </p>\n\n<p>If I have the following data.frame:</p>\n\n<pre><code>&gt; Col1        Col2        Col3        Col4\n&gt; 1           NA          10          99\n&gt; NA          NA          11          99\n&gt; 1           NA          12          99\n&gt; 1           NA          13          NA\n</code></pre>\n\n<p>I want it to look like this:</p>\n\n<pre><code>&gt; Col1        Col2        Col3        Col4\n&gt; 1           0.0000001   10          99\n&gt; 0.0000001   NA          11          99\n&gt; 1           NA          12          99\n&gt; 1           NA          13          0.0000001 \n</code></pre>\n\n<p>This is the code I haev that works but is very slow...</p>\n\n<pre><code>#Temporary change for missing first observation\nfor (u in 1:ncol(data.frame))\n{\n  for (v in 1:nrow(data.frame)) \n  {\n    #Temporary change the first observations in a row to 0.0000001 until it encounters a value that isn't NA\n    if(is.na(temp_equity_df_merge2[v,u]))\n    {\n        temp_equity_df_merge2[v,u]=0.0000001\n    }\n    else break\n  }\n</code></pre>\n\n<p>I want to use apply or some variant that will be faster.  I am looping over 20 columns and 1 million rows.</p>\n\n<p>Thanks ahead of time for the help.</p>\n"},{"tags":["javascript","performance","inline"],"answer_count":0,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":78,"score":5,"question_id":12702491,"title":"Do modern browsers consistently inline functions? How can inlining be predicted for performance?","body":"<p>My question concerns modern browser optimization through function inlining. I think I really need someone who is familiar with the V8, JavaScriptCore, and/or SpiderMonkey codebase to answer this question; or at least someone knowledgeable in modern browser approaches to optimization.</p>\n\n<p>Basically I want to know if there's any way to predict when function inlining will be used and how to leverage it for optimizing performance with minimal code duplication.</p>\n\n<p>Let's take the following simple function from <a href=\"http://joijs.com/\" rel=\"nofollow\">joi</a> tempo as an example:</p>\n\n<pre><code>function limit(f, max) {\n    var count = 0, isFunction = typeof max == 'function';\n    if(!isFunction) max = max &gt;&gt;&gt; 0 || 1;\n    return function limitedFunction() {\n        if(isFunction ? !max(count) : (count &gt;= max)) return;\n        count++;\n        return f.apply(this, arguments);\n    };\n}\n</code></pre>\n\n<p>This function takes a function <code>f</code> and returns a wrapper function which limits the number of times the original function can be called through the wrapper. Example:</p>\n\n<pre><code>var foo = limit(function() { console.log('foo'); }, 3);\nfoo(); // logs 'foo'\nfoo(); // logs 'foo'\nfoo(); // logs 'foo'\nfoo(); // doesn't log anything\nfoo(); // doesn't log anything\n</code></pre>\n\n<p>It also accepts a function which returns <code>true</code> or <code>false</code> as its second argument (instead of a number):</p>\n\n<pre><code>var bar = limit(\n    function() { console.log('bar'); },\n    function() { return Math.random() &gt; 0.5 ? true : false }\n);\nbar(); // randomly logs 'bar' or doesn't log anything\nbar(); // randomly logs 'bar' or doesn't log anything\nbar(); // randomly logs 'bar' or doesn't log anything\n</code></pre>\n\n<p>This is an overly simplistic, trivial example, so bear with me. I think there are real applications for this in non-trivial cases, but I want to use an easy case to get to the root of how the browser does or can do inlining.</p>\n\n<p>One way to rewrite this function which could (probably) be (very slightly) improved for performance (I imagine) would be to separate the returned <code>function</code> into two cases, one where <code>max</code> is a function and the other where <code>max</code> is a number:</p>\n\n<pre><code>function limit(f, max) {\n    var count = 0, isFunction = typeof max == 'function';\n    if(!isFunction) max = max &gt;&gt;&gt; 0 || 1;\n    return isFunction\n        ? function limitedFunctionA() {\n            if(!max(count)) return;\n            count++;\n            return f.apply(this, arguments);\n        }\n        : function limitedFunctionB() {\n            if(count &gt;= max) return;\n            count++;\n            return f.apply(this, arguments);\n        };\n}\n</code></pre>\n\n<p>This way the check to see if <code>isFunction</code> is <code>true</code> or <code>false</code> doesn't have to occur <em>every time</em> <code>limitedFunction</code> is called but only once when it is determined whether to return <code>limitedFunctionA</code> or <code>limitedFunctionB</code>.  However, this is not ideal because there is some redundant code.</p>\n\n<p>My question concerns the following rewrite:</p>\n\n<pre><code>function limit(f, max) {\n    var count = 0, isFunction = typeof max == 'function',\n        check = isFunction\n            ? function() { return !max(count); }\n            : function() { return count &gt;= max; };\n    if(!isFunction) max = max &gt;&gt;&gt; 0 || 1;\n    return function limitedFunction() {\n        if(check()) return;\n        count++;\n        return f.apply(this, arguments);\n    };\n}\n</code></pre>\n\n<p>So here's my question... <strong>Will modern browsers be smart enough to turn</strong></p>\n\n<pre><code>if(check()) return;\n</code></pre>\n\n<p><strong>into</strong></p>\n\n<pre><code>if(!max(count)) return;\n</code></pre>\n\n<p><strong>when <code>isFunction</code> is true, and</strong></p>\n\n<pre><code>if(count &gt;= max) return;\n</code></pre>\n\n<p><strong>when <code>isFunction</code> is false?</strong> ... preventing the need to call the extra <code>check()</code> function, throwing an extra function onto the stack unnecessarily each time.</p>\n"},{"tags":["c","performance","loops","sum","intel"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":83,"score":2,"question_id":12701326,"title":"Improve the performance of a sum (C version)","body":"<p>I am using a scientific calculation code. And I want to improve it a little bit if possible. I check the code with Amplifier. The most time consuming (heavily used) code is this:</p>\n\n<pre><code>double a = 0.0;\nfor(j = 0; j &lt; n; j++) a += w[j]*fi[((index[j] + i)&lt;&lt;ldf) + k];\n</code></pre>\n\n<p>To me it is just a dot product of w and fi. I am wondering:</p>\n\n<ol>\n<li>Does Intel compiler will do it automatically? (I mean treated the loop as the dot product of two vecterized array.)</li>\n<li>Is there a way to improve the code? (I mean maybe define another array a1 the same size of w. Then all multiplied number can be stored in a1 (unrolled loop?). Do summation in the end. )</li>\n<li>Other suggestions?</li>\n</ol>\n\n<p>I am using parallel composer 2013 with visual studio. Any idea will be appreicated！:)</p>\n"},{"tags":["performance","amazon-web-services","cloud"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":17,"score":1,"question_id":12700924,"title":"AWS High I/O performance for Servers without disk access","body":"<p>Does anyone know about how good is high i/o instances for a application server workload that does not do much disk access?</p>\n\n<p>Only benchmark I found are <a href=\"http://palominodb.com/sites/default/files/PalominoEvaluatesAWSNewSSDInstances%20%281%29.pdf\" rel=\"nofollow\">http://palominodb.com/sites/default/files/PalominoEvaluatesAWSNewSSDInstances%20%281%29.pdf</a> and Netflix one, both focus on SSD disk read write performance. </p>\n\n<p>Any benchmark on network performance? </p>\n"},{"tags":["c++","performance","stream","queue","standard-library"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":67,"score":0,"question_id":12700563,"title":"string producer/consumer in C++: std::deque<char> or std::stringstream?","body":"<p>In our application we have a class that produces characters, and another that consumes them. The current implementation dynamically allocates characters as they are produced (using <code>new</code>) and delete them (with <code>delete</code>) as they are consumed. This is all terribly slow, and I am looking at ways to replace that implementation to improve its performance.</p>\n\n<p>The semantic I need is that of the standard class <code>queue</code>: push at the front, pop at the back. The default implementation uses a <code>deque</code> IIRC. <code>deque</code> is typically implemented using \"blocks\" or \"chunks\" of memory, so I expect far less calls to the OS memory allocator, and a significant speed-up, with little additional memory usage.</p>\n\n<p>However, since the data queued is characters (possibly wide characters), an alternative would be to use the standard input/output stream class, namely the character stream <code>stringstream</code>. AFAIK, their behaviour is queue-like too.</p>\n\n<p>Is their a better choice a priori? Would both classes have similar allocation patterns? I can try and measure performance of both, but perhaps it doesn't really matter and either would be good enough. In that case, which would be easiest/safest to use?</p>\n\n<p>a secondary matter is concurrency between the producer and the consumer. I can restrict access to be sequential (on the same thread), but a thread-safe implementation is likely to be beneficial performance-wise with current multi-core hardware.</p>\n\n<p>Thanks for your wisdom before I dive in and start coding.</p>\n"},{"tags":["python","performance","wurfl"],"answer_count":5,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":509,"score":1,"question_id":3570514,"title":"WURFL on App Engine","body":"<p>I'm trying to implement mobile browser detection in appengine, using <a href=\"http://wurfl.sourceforge.net/\" rel=\"nofollow\">WURFL</a>. However, it appears to be impossible to implement it w/o causing a crazy amount of CPU usage, as well as page loading latency. Any ideas on how I can speed it up?</p>\n"}]}
{"total":25593,"page":16,"pagesize":100,"questions":[{"tags":["performance","mongodb","nosql","couchdb","scalability"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":3,"view_count":76,"score":-3,"question_id":12700217,"title":"Should I really use NoSQL?","body":"<p>I have some reports which combines data from 3 different servers creates json and shows it to users . When there are thousands of records it impossible to process it over web and it times out . I am now queuing users request and generating a json file and sedning a link so when user opens link I just show data instead of any processing .</p>\n\n<p>I came across NoSql recently. In my particular case Will there be any benefits of using NoSql over storing data to a file apart from able to run queries on it ?</p>\n"},{"tags":["python","windows","performance","opencv"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":110,"score":1,"question_id":10887836,"title":"QueryFrame very slow on Windows","body":"<p>I have build a simple webcam recorder on linux which works quite well.\nI get ~25fps video and good audio.</p>\n\n<p>I am porting the recorder on windows (win7) and while it works, it is <em>unusable</em>.\nThe QueryFrame function takes something more than 350ms, i.e 2.5fps. </p>\n\n<p>The code is in python but the problem really seems to be the lib call.</p>\n\n<p>I tested on the same machine with the same webcam (a logitech E2500). \nOn windows, I installed openCV v2.2. I cannot check right now but the version might be a bit higher on Ubuntu. </p>\n\n<p>Any idea what could be the problem ? </p>\n\n<p>edit : I've just installed openCV2.4 and I have the same slow speed.</p>\n"},{"tags":["performance","table","height","set"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":12657766,"title":"Tips to improve performace of javascript","body":"<p>I have a requirement where I need to set the height of each row of a table based on the corresponding row in another table.The table has around 500 rows.\nI have written the below javascript, but the performace is really bad around 8000 ms.\nHow can I make this faster, appreciate any tips .</p>\n\n<pre><code> var start = new Date().getTime();\n\n\n var rows = document.getElementById(\"table1\").rows;\n var dup_rows = document.getElementById(\"table2\").rows;\n var num_rows = rows.length;\n var num_dup = dup_rows.length;\n for (var i = 0; i &lt; num_rows; ++i) {\nvar hg = rows[i].offsetHeight;\n    rows[i].style.height = hg +'px';\ndup_rows[i].style.height = hg +'px';\n   }\n\nvar end = new Date().getTime();\nvar time = end - start;\nalert('Execution time: ' + time);\n</code></pre>\n\n<p>Based on the Suggestion to edit the tables outside of DOM, I tried the below , but the outerHeight / offsetHeight returns 0 when the table is removed from DOM\n          clone_small = $('#table2').clone();\n          clone_main_tab = $('#table1').clone();\n     $(\"#table2\").remove();\n     $(\"#table1\").remove();</p>\n\n<pre><code> $(clone_main_tab).find(\"tr\").each(function(i) {\n var hg = 0;\n hg = $(this).offsetHeight;  // If I hard code the height it works\n  // alert(hg);\n $(this).height(hg);\n clone_small.find(\"tr\").eq(i).height(hg);\n\n\n });   How can I set the height of these rows outside the DOM ?\n</code></pre>\n\n<p>Thanks in advance and appreciate any guidance here</p>\n"},{"tags":["performance",".net-micro-framework","netduino"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":58,"score":0,"question_id":12251232,"title":"What is the overhead for thread creation in the .net micro framework?","body":"<p>I was curious as to what are the impacts of thread creation were on a netduino running the .net micro framework.  It's commonly understood that threads have an <a href=\"http://msdn.microsoft.com/en-us/magazine/cc163552.aspx#S4\" rel=\"nofollow\">inherent overhead</a> to them but I was wondering if anyone knew if there were optimizations or not for .net micro on an embedded environment and if anyone can give me some detail as to what happens under the hood with a thread here (how much memory is allocated, how many cycles it takes to generate, etc).</p>\n"},{"tags":["java","performance","serialization","deserialization","hbase"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":92,"score":1,"question_id":12699040,"title":"double vs long serialization in java","body":"<p>I can store a number as a Long and Double in HBase. Both of them takes 8 bytes in Java.</p>\n\n<p>Advantage of Using Double is that it gives a more wider range for storing Whole Numbers.</p>\n\n<p>However, i think range of Long is also enough for my use. </p>\n\n<p>Does anyone has any idea about the serialization and de-serialization performance of Long vs Dobule? I am interested in comparison between them.</p>\n\n<p>Thanks.</p>\n"},{"tags":["sql","performance","postgresql","indexing","correlated-subquery"],"answer_count":4,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":457,"score":3,"question_id":8029650,"title":"Using a single SQL correlated sub-query to get two columns","body":"<p>My problem is represented by the following query:</p>\n\n<pre><code>SELECT \n  b.row_id, b.x, b.y, b.something,\n  (SELECT a.x FROM my_table a WHERE a.row_id = (b.row_id - 1), a.something != 42 ) AS source_x,\n  (SELECT a.y FROM my_table a WHERE a.row_id = (b.row_id - 1), a.something != 42 ) AS source_y\nFROM \n  my_table b\n</code></pre>\n\n<p>I'm using the same subquery statement twice, for getting both <code>source_x</code> and <code>source_y</code>.\nThat's why I'm wondering if it's possible to do it using one subquery only?</p>\n\n<p>Because once I run this query on my real data (millions of rows) it seems to never finish and take hours, if not days (my connection hang up before the end). </p>\n\n<p>I am using PostgreSQL 8.4</p>\n"},{"tags":["css","performance","browser"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":93,"score":0,"question_id":4338168,"title":"Do browsers interpret/execute css tags that they see as empty? (ie IE hacks)","body":"<p>I am curious about the efficiency of this example piece of CSS:</p>\n\n<pre><code>ul, a, span, p, li { *zoom:1; }\n</code></pre>\n\n<p>Please keep in mind that this is purely theoretical so the merits or pitfalls of CSS hacks are not so much of interest. </p>\n\n<p>My question is — What do browsers other than IE6&amp;7 do:</p>\n\n<ol>\n<li>look to match all the selectors in the page and then realise that it is empty and not act upon it? (horribly inefficient)</li>\n<li>Realise it is empty and not act upon the selectors (fairly efficient)</li>\n<li>No of the above. </li>\n</ol>\n\n<p>Any ideas would be greatly received.</p>\n\n<p>Cheers,\nAd.</p>\n"},{"tags":["mysql","regex","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":35,"score":0,"question_id":12698101,"title":"regex in Mysql select statement","body":"<p>I need to search for some user input patterns in database.<br>\nis it efficient to use Regex in Mysql <code>SELECT</code> statement's where clause ( in a scale of 10000 rows on an indexed column)?<br>\nMy search is for dates and can be in these three forms : </p>\n\n<ul>\n<li><code>2012</code> that should match <code>2012</code> , <code>2012/*</code> &amp; <code>2012/*/*</code> </li>\n<li><code>2012/10</code> that should match <code>2012/10</code> &amp; <code>2012/10/*</code></li>\n<li><code>2012/10/3</code> that should match only <code>2012/10/3</code></li>\n</ul>\n\n<p>Note: I have three columns that should be checked for this type of match separately.</p>\n"},{"tags":["python","performance","algorithm","levenshtein-distance","difflib"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":99,"score":4,"question_id":12659199,"title":"Is it possible that the SequenceMatcher in Python's difflib could provide a more efficient way to calculate Levenshtein distance?","body":"<p>Here's the textbook example of the general algorithm to calculate Levenshtein Distance (I've pulled from <a href=\"http://hetland.org/coding/python/levenshtein.py\" rel=\"nofollow\">Magnus Hetland's webite</a>):</p>\n\n<pre><code>def levenshtein(a,b):\n    \"Calculates the Levenshtein distance between a and b.\"\n    n, m = len(a), len(b)\n    if n &gt; m:\n        # Make sure n &lt;= m, to use O(min(n,m)) space\n        a,b = b,a\n        n,m = m,n\n\n    current = range(n+1)\n    for i in range(1,m+1):\n        previous, current = current, [i]+[0]*n\n        for j in range(1,n+1):\n            add, delete = previous[j]+1, current[j-1]+1\n            change = previous[j-1]\n            if a[j-1] != b[i-1]:\n                change = change + 1\n            current[j] = min(add, delete, change)\n\n    return current[n]\n</code></pre>\n\n<p>I was wondering, however, if there might be a more efficient (and potentially more elegant) pure Python implementation that uses difflib's SequenceManager. After playing around with it, here's what I came up with:</p>\n\n<pre><code>from difflib import SequenceMatcher as sm\n\ndef lev_using_difflib(s1, s2):\n    a = b = size = distance = 0\n    for m in sm(a=s1, b=s2).get_matching_blocks():\n        distance += max(m.a-a, m.b-b) - size\n        a, b, size = m\n    return distance\n</code></pre>\n\n<p>I can't come up with a test case where it fails, and the performance seems to be significantly better than the standard algorithm.</p>\n\n<p>Here are the results with levenshtein algorithm that relies on difflib:</p>\n\n<pre><code>&gt;&gt;&gt; from timeit import Timer\n&gt;&gt;&gt; setup = \"\"\"\n... from difflib import SequenceMatcher as sm\n... \n... def lev_using_difflib(s1, s2):\n...     a = b = size = distance = 0\n...     for m in sm(a=s1, b=s2).get_matching_blocks():\n...         distance += max(m.a-a, m.b-b) - size\n...         a, b, size = m\n...     return distance\n... \n... strings = [('sunday','saturday'),\n...            ('fitting','babysitting'),\n...            ('rosettacode','raisethysword')]\n... \"\"\"\n&gt;&gt;&gt; stmt = \"\"\"\n... for s in strings:\n...     lev_using_difflib(*s)\n... \"\"\"\n&gt;&gt;&gt; Timer(stmt, setup).timeit(100000)\n36.989389181137085\n</code></pre>\n\n<p>And here's the standard pure python implementation:</p>\n\n<pre><code>&gt;&gt;&gt; from timeit import Timer\n&gt;&gt;&gt; setup2 = \"\"\"\n... def levenshtein(a,b):\n...     n, m = len(a), len(b)\n...     if n &gt; m:\n...         a,b = b,a\n...         n,m = m,n\n... \n...     current = range(n+1)\n...     for i in range(1,m+1):\n...         previous, current = current, [i]+[0]*n\n...         for j in range(1,n+1):\n...             add, delete = previous[j]+1, current[j-1]+1\n...             change = previous[j-1]\n...             if a[j-1] != b[i-1]:\n...                 change = change + 1\n...             current[j] = min(add, delete, change)\n... \n...     return current[n]\n... \n... strings = [('sunday','saturday'),\n...            ('fitting','babysitting'),\n...            ('rosettacode','raisethysword')]\n... \"\"\"\n&gt;&gt;&gt; stmt2 = \"\"\"\n... for s in strings:\n...     levenshtein(*s)\n... \"\"\"\n&gt;&gt;&gt; Timer(stmt2, setup2).timeit(100000)\n55.594768047332764\n</code></pre>\n\n<p>Is the performance of the algorithm using difflib's SequenceMatcher really better? Or is it relying on a C library that invalidates the comparison completely? If it is relying on C extensions, how can I tell by looking at the difflib.py implementation?</p>\n\n<p>Using Python 2.7.3 [GCC 4.2.1 (Apple Inc. build 5666)]</p>\n\n<p>Thanks in advance for your help!</p>\n"},{"tags":["performance","oracle","plsql"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":151,"score":0,"question_id":10595337,"title":"Oracle Cursor output takes long time","body":"<p>I have problem in stored procedure. The stored procedure produces Cursor output. The SQL query which produces the output execute in less than 1 Sec.But procedure takes more than 10 Minutes to execute.Stored procedure doesnot have any other quires.</p>\n"},{"tags":["mysql","windows","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":47,"score":0,"question_id":12697271,"title":"Improving MySQL Server performance","body":"<p>I've installed MySQL and using workbench on Win7. In my DB, I have more than 150M rows. To speed up the queries I've used also indexing method. Furthermore, to decrease running time of these queries on the machine (3GB ram (4GB but 32-bit centrino2 double processor) and 70GB HDD space) which parameters should be changed in my.ini configuration file?</p>\n\n<p>my current config file lies below:</p>\n\n<pre><code>query_cache_size=0\nmyisam_max_sort_file_size=100G\nmyisam_sort_buffer_size=27M\nkey_buffer_size=8M\nread_buffer_size=64K\nread_rnd_buffer_size=256K\nsort_buffer_size=256K\ninnodb_additional_mem_pool_size=2M\ninnodb_flush_log_at_trx_commit=1\ninnodb_log_buffer_size=1M\ninnodb_buffer_pool_size=68M\ninnodb_log_file_size=34M\ninnodb_thread_concurrency=8\n</code></pre>\n"},{"tags":["performance","excel","admin","performancecounter","perfmon"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":110,"score":0,"question_id":11775991,"title":"Setting Up Windows 7 Perfmon to Collect Performance Data - which data to collect?","body":"<p>How to collect process memory usage (Excel 2010) ?</p>\n\n<p>Which performance data should I choose to show my boss that my computer is too slow for working with large files (~130 Mb) with Excel 2010 64-bit under Windows 7 ?</p>\n\n<p>(I'm working on Intel 3i-2100 3.1 GHz, 4 GB RAM)</p>\n\n<p>one of the problem is that system freeze and do nothing, so processor is 0% etc.</p>\n\n<p>This looks like soft question, but I think it is important.</p>\n"},{"tags":["mysql","sql","performance","query"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":85,"score":1,"question_id":10030242,"title":"best query to update a big mysql database (rows) against a badword list","body":"<p>I have a table with 8 million rows, which needs to be scanned against a huge list of badwords.</p>\n\n<p>My first idea was to:</p>\n\n<pre><code>UPDATE `master` SET `blacklisted`='1' WHERE MATCH (`content-desc`, `content-title`) AGAINST ('\nbadword1 | badword2 | badword3 | \"and many more\"' IN BOOLEAN MODE)\n</code></pre>\n\n<p>unfortunately this version forgot some words and was not case-insensitive!</p>\n\n<p>next try was to </p>\n\n<pre><code>$badwords = array(\"badword1\",\"badword2\",\"badword3\",\"and-many-more\");\n\nforeach($badwords AS $name)\n   {\n        $sql = \"UPDATE `master` SET `blacklisted`='1' WHERE concat(`content-title(mediumtext)`,`content-desc(mediumtext)`)  LIKE '%\".$name.\"%'\";\n\n        sleep(6);\n\n// Could limit this query by 100.000 and adding another foreach loop help?\n// How would the foreach look like (select count(*) from master?)/100.000\n\n   }\n</code></pre>\n\n<p>a lot of queries which killed my server immediately!\nMaybe the commented idea could help?! (but howto?)</p>\n\n<p>Who has the best idea how to solve this query, without stressing the mysql server too much?\nThank you!</p>\n"},{"tags":["mysql","performance","jdbc","batch"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":49,"score":1,"question_id":12696697,"title":"2273 msec to insert 7679 records is this fast or slow?","body":"<p>I'm planning on inserting a large amount of rows in my mysql database. At the moment i'm inserting about 8000 records in a almost empty table with no indexes (only primary key with autoincrement) using batches and using a mysql server (default install) on localhost (i7 6gb, fast hd)</p>\n\n<p>It currently take about 2273 msec to insert 7679 records.</p>\n\n<p>A single record looks like:</p>\n\n<p>39492, 1.4618, 1.4619, 1.4606, 1.4613, 1199235602000, 0, 133</p>\n\n<p>I was wondering if this is a normal average speed or that i should be worried because it it extremely slow?</p>\n\n<p>I ask this because i have no reference when it comes to speed. And because of this i don't know if my code is good or might be bugged because the speed is slow.</p>\n"},{"tags":["iphone","ios","performance","request","asihttprequest"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":891,"score":3,"question_id":6319365,"title":"how to track a single download speed with asihttprequest","body":"<p>Can someone help me with asihttprequest ?</p>\n\n<p>I want to track the speed download of each file I download and not the average speed of all the files.</p>\n\n<p>For the average spped of all downloads, there is <code>[ASIHTTPRequest averageBandwidthUsedPerSecond]</code>\nbut what can I use to track each downloads?</p>\n\n<p>Thanks </p>\n"},{"tags":["android","performance","throughput"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":22,"score":1,"question_id":12695601,"title":"Android Throughput not mathcing with Access Point traffics .","body":"<p>I am trying to compare Network speed in different way. </p>\n\n<ol>\n<li>Open an input stream to a file server</li>\n<li>Using <a href=\"http://developer.android.com/reference/android/net/TrafficStats.html\" rel=\"nofollow\">TrafficStat</a></li>\n</ol>\n\n<p>Seems like stats i am getting from <a href=\"http://developer.android.com/reference/android/net/TrafficStats.html\" rel=\"nofollow\">TrafficStat</a> is same as when i am calculating using an input stream. </p>\n\n<p>But when i am looking into traffics on the Access Point (device is connected to) i am getting lower value compare to what i am seeing/calculating on Android. </p>\n\n<p>Right now i am very much puzzled and can not figure out whats wrong. </p>\n"},{"tags":["java","android","performance","android-ndk"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":782,"score":1,"question_id":6687855,"title":"Does Android NDK inherit Java problems?","body":"<p>I want to create a game for Android and I need to choose between SDK and NDK. Mobile phones have limited hardware and I want to avoid slowdown, but Java is slow and uses too much memory.</p>\n\n<p>Are NDK apps faster and more efficient than SDK apps?</p>\n\n<p>Java uses a garbage collector, all objects are allocated on heap, I cannot allocate an object inside another object (without a pointer), and a simple class that is used as a struct inherits the Object class.</p>\n\n<p>Will my NDK program be converted into Java bytecode? Will the compiler ignore my delete calls, add garbage collector, add the Object class and transfer all of my objects to heap?</p>\n"},{"tags":["performance","scaling","signalr"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":66,"score":0,"question_id":12684187,"title":"What are the performance characteristics of SignalR at scale?","body":"<p>I'm interested in the performance characteristics of <a href=\"http://signalr.net/\" rel=\"nofollow\">SignalR</a> at scale ... particularly, how it behaves at the fringes of capability. When a server is at capacity, what happens? </p>\n\n<ul>\n<li>Does it drop messages?</li>\n<li>Do some clients not get notified?</li>\n<li>Are messages queued until all are delivered?</li>\n<li>And if so, will the queue eventually overflow and crash the server?</li>\n</ul>\n\n<p>I ask because conducting such a test myself would be impractical, and I'm hoping someone could point me to documentation speaking to this ... or perhaps someone could comment that has seen how SignalR behaves at scale.</p>\n\n<p>Thanks!</p>\n\n<p><em>note: I'm familiar with <a href=\"http://stackoverflow.com/questions/9700944/is-signalr-stable-and-scalable-enough-for-high-traffic-production-environments\">this other stackoverflow question</a> on the stability and scalability of SignalR. But I believe my question is asking a slightly different question in that I'm not concerned with the theoretical scaling limits, I want to know how it behaves when it reaches the limits ... so I know what to be on the lookout for.</em></p>\n"},{"tags":["c++","performance","stack","deque"],"answer_count":2,"favorite_count":1,"up_vote_count":5,"down_vote_count":3,"view_count":258,"score":2,"question_id":12693263,"title":"std deque is surprisingly slow","body":"<p>I just find out that standard std deque is really slow when comparing with my \"home-made\" version of stack that use pre-allocated array.<br>\nThis is code of my stack:  </p>\n\n<pre><code>template &lt;class T&gt;\nclass FastStack\n{    \npublic:\n    T* st;\n    int allocationSize;\n    int lastIndex;\n\npublic:\n    FastStack(int stackSize);\n    FastStack();\n    ~FastStack();\n\n    inline void resize(int newSize);\n    inline void push(T x);\n    inline void pop();\n    inline T getAndRemove();\n    inline T getLast();\n    inline void clear();\n};\n\ntemplate &lt;class T&gt;\nFastStack&lt;T&gt;::FastStack()\n{\n    lastIndex = -1;\n    st = NULL;\n}\n\ntemplate &lt;class T&gt;\nFastStack&lt;T&gt;::FastStack(int stackSize)\n{\n    st = NULL;\n    this-&gt;allocationSize = stackSize;\n    st = new T[stackSize];\n    lastIndex = -1;\n}\n\ntemplate &lt;class T&gt;\nFastStack&lt;T&gt;::~FastStack()\n{\n    delete [] st;\n}\n\ntemplate &lt;class T&gt;\nvoid FastStack&lt;T&gt;::clear()\n{\n    lastIndex = -1;\n}\n\ntemplate &lt;class T&gt;\nT FastStack&lt;T&gt;::getLast()\n{\n    return st[lastIndex];\n}\n\ntemplate &lt;class T&gt;\nT FastStack&lt;T&gt;::getAndRemove()\n{\n    return st[lastIndex--];\n}\n\ntemplate &lt;class T&gt;\nvoid FastStack&lt;T&gt;::pop()\n{\n    --lastIndex;\n}\n\ntemplate &lt;class T&gt;\nvoid FastStack&lt;T&gt;::push(T x)\n{\n    st[++lastIndex] = x;\n}\n\ntemplate &lt;class T&gt;\nvoid FastStack&lt;T&gt;::resize(int newSize)\n{\n    if (st != NULL)\n        delete [] st;\n    st = new T[newSize];\n}\n</code></pre>\n\n<p>.<br>\n<strong>I run this simple benchmark for deque:</strong>  </p>\n\n<pre><code>std::deque&lt;int&gt; aStack;\nint x;\n\nHRTimer timer;\ntimer.Start();\n\nfor (int i = 0; i &lt; 2000000000; i++)\n{\n    aStack.push_back(i);\n    x = aStack.back();\n    if (i % 100 == 0 &amp;&amp; i != 0)\n        for (int j = 0; j &lt; 100; j++)\n            aStack.pop_back();\n}\n\ndouble totalTime = timer.Stop();\nstringstream ss;\nss &lt;&lt; \"std::deque \" &lt;&lt; totalTime;\nlog(ss.str());\n</code></pre>\n\n<p>.<br>\n<strong>Using std stack with vector as container (as 'Michael Kohne' suggested)</strong></p>\n\n<pre><code>std::stack&lt;int, std::vector&lt;int&gt;&gt; bStack;\nint x;\n\nHRTimer timer;\ntimer.Start();\n\nfor (int i = 0; i &lt; 2000000000; i++)\n{\n    bStack.push(i);\n    x = bStack.top();\n    if (i % 100 == 0 &amp;&amp; i != 0)\n        for (int j = 0; j &lt; 100; j++)\n            bStack.pop();\n}\n\ndouble totalTime = timer.Stop();\nstringstream ss;\nss &lt;&lt; \"std::stack \" &lt;&lt; totalTime;\nlog(ss.str());\n</code></pre>\n\n<p>.<br>\n<strong>And the same one for my FastStack:</strong>  </p>\n\n<pre><code>FastStack&lt;int&gt; fstack(200);\nint x;\n\nHRTimer timer;\ntimer.Start();\n\nfor (int i = 0; i &lt; 2000000000; i++)\n{\n    fstack.push(i);\n    x = fstack.getLast();\n    if (i % 100 == 0 &amp;&amp; i != 0)\n        for (int j = 0; j &lt; 100; j++)\n            fstack.pop();\n}\n\ndouble totalTime = timer.Stop();\nstringstream ss;\nss &lt;&lt; \"FastStack \" &lt;&lt; totalTime;\nlog(ss.str());\n</code></pre>\n\n<p>.<br>\n<strong>The results after 4 runs are as follows:</strong><br>\ndeque 15.529<br>\ndeque 15.3756<br>\ndeque 15.429<br>\ndeque 15.4778  </p>\n\n<p>stack 6.19099<br>\nstack 6.1834<br>\nstack 6.19315<br>\nstack 6.19841  </p>\n\n<p>FastStack 3.01085<br>\nFastStack 2.9934<br>\nFastStack 3.02536<br>\nFastStack 3.00937  </p>\n\n<p>The results are in seconds and I was running the code on Intel i5 3570k (on default clock). I used VS2010 compiler with all optimizations that are available.\nI know that my FastStack has a lot of limitations but there are plenty situations, where it could be used and when it can give nice boost! (I used it in one project where I get 2x speed up comparing to std::queue).<br>\nSo now my question is:<br>\n<del>Are there any other \"inhibitors\" in C++ that everybody use but no one knows about them?</del><br>\nEDIT: I don't want to be offensive, I'm just curious if you know some unknown \"speedups\" like this.</p>\n"},{"tags":["ios","facebook","performance","sharekit"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":134,"score":1,"question_id":11973515,"title":"Why is posting an image to Facebook with Sharekit so slow?","body":"<p>I've a universal iOS app that uses ShareKit to share an image and a line of text:</p>\n\n<pre><code>SHKItem *item = [SHKItem image:imageToShare title: shareTitle];\nif (destination == K_FACEBOOK) {\n  [SHKFacebook shareItem: item];\n}\n</code></pre>\n\n<p>The image is a UIImage of not more than 512 x 512, and the line of text is &lt; 200 characters, so the total payload is ~50k.  When I share to email or twitter it's near instantaneous, but when I share to Facebook, it can end up taking somewhere between 20-30 seconds.</p>\n\n<p>In my configuration for the Facebook sharer I've disabled using the Facebook app and Safari, so everything is presented by ShareKit itself.</p>\n\n<p>Is there something that different in the process for uploading to Facebook that could cause this delay?</p>\n"},{"tags":["mysql","performance","index"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":22,"score":1,"question_id":12693284,"title":"prevent query from using index","body":"<p>I have a query that needs to go on EACH AND EVERY record in a table. I do have a condition <code>WHERE LENGTH(f1)&gt;2</code>\nStill, it should do a full table scan, but it seems to be using an index, which makes it actually slower.<br>\nHow do I make a query not to use specific index (namely, the one on <code>f1</code>).</p>\n"},{"tags":["java","performance","hibernate","jpa","java-ee"],"answer_count":4,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":2745,"score":3,"question_id":2761543,"title":"How to handle large dataset with JPA (or at least with Hibernate)?","body":"<p>I need to make my web-app work with really huge datasets. At the moment I get either OutOfMemoryException or output which is being generated 1-2 minutes.</p>\n\n<p>Let's put it simple and suppose that we have 2 tables in DB: <code>Worker</code> and <code>WorkLog</code> with about 1000 rows in the first one and 10 000 000 rows in the second one. Latter table has several fields including 'workerId' and 'hoursWorked' fields among others. What we need is:</p>\n\n<ol>\n<li><p>count total hours worked by each user; </p></li>\n<li><p>list of work periods for each user.</p></li>\n</ol>\n\n<p>The most straightforward approach (IMO) for each task in plain SQL is:</p>\n\n<p>1) </p>\n\n<pre><code>select Worker.name, sum(hoursWorked) from Worker, WorkLog \n   where Worker.id = WorkLog.workerId \n   group by Worker.name;\n\n//results of this query should be transformed to Multimap&lt;Worker, Long&gt;\n</code></pre>\n\n<p>2) </p>\n\n<pre><code>select Worker.name, WorkLog.start, WorkLog.hoursWorked from Worker, WorkLog\n   where Worker.id = WorkLog.workerId;\n\n//results of this query should be transformed to Multimap&lt;Worker, Period&gt;\n//if it was JDBC then it would be vitally \n//to set resultSet.setFetchSize (someSmallNumber), ~100\n</code></pre>\n\n<p>So, I have two questions: </p>\n\n<ol>\n<li>how to implement each of my approaches with JPA (or at least with Hibernate); </li>\n<li>how would you handle this problem (with JPA or Hibernate of course)?</li>\n</ol>\n"},{"tags":["performance","programming-languages","sse","simd"],"answer_count":0,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":46,"score":1,"question_id":12691943,"title":"Which programming languages give access to SIMD instructions?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/1417681/simd-programming-languages\">SIMD programming languages</a>  </p>\n</blockquote>\n\n\n\n<p>It seems that the paramount elements to reach top performance on modern CPUs are:</p>\n\n<ol>\n<li><p>Threads level parallelism</p></li>\n<li><p>SIMD usage inside each thread</p></li>\n</ol>\n\n<p>Most of modern programming languages give support (or even emphasis) on thread level parallelism. However, it seems that SIMD instructions support is much more rare.</p>\n\n<ul>\n<li>I know of C, C++, C#/DotNet/Mono's Mono.SIMD and D's core.simd.</li>\n<li>To my knowledge Java and Go do not (and will not) support explicit SIMD (and hope the best from the compiler).</li>\n<li>Ruby and Python only provide access to SIMD via C wrapper libraries (such as numpy), but no direct access.</li>\n<li>Fortran seems to only support (guided) auto-vectorisation, not explicit code.</li>\n<li>Haskell, Rust seem to not yet support SIMD in their main branches...</li>\n</ul>\n\n<blockquote>\n  <p>Which other programming languages do provide explicit access to SIMD instructions ?</p>\n</blockquote>\n\n<p>Thank you for your suggestions.</p>\n"},{"tags":["performance","website","cloudflare"],"answer_count":6,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":1270,"score":7,"question_id":5237179,"title":"CloudFlare benefits","body":"<p>I came across the service of CloudFlare. It looks all good and does not seem to require a lot of work to be part of. Basic service is also free, which helps of course. However, it would be great to hear about somebody's real experience with that service. So the questions are:</p>\n\n<ol>\n<li>Does it really speeds up and protects your website the way it is described on their website?</li>\n<li>I am in Canada - does it have any limitations for Canadian customers?</li>\n</ol>\n\n<p>I have checked, not much discussion here about CloudFlare.</p>\n\n<p>Please share if you have real experience.</p>\n\n<p>Thanks!</p>\n"},{"tags":["android","performance","sqlite","battery","android-sqlite"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":21,"score":1,"question_id":12692047,"title":"Can size of android database effects battery efficiency of the device?","body":"<p>Irrespective of functionality, can simply the size the database directly effects the efficiency of the device? I read Opening and Closing database is an expensive task, so is this expense depends on the size of the database?</p>\n"},{"tags":["c","performance","optimization","sign-extension"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":70,"score":1,"question_id":12692209,"title":"What is necessity of sign-extension?","body":"<p>Consider following piece of C code -</p>\n\n<pre><code>char sum_char(char a,char b)\n{\n   char c = a+b;\n   return c;\n}\n</code></pre>\n\n<p>It involves -</p>\n\n<ol>\n<li>Convert second parameter to sign extension.</li>\n<li>Push signed extension parameter on stack as b.</li>\n<li>Convert first parameter to sign extension.</li>\n<li>Push signed extension parameter on stack as a.</li>\n<li>Add a &amp; b, result cast to char and store it in c.</li>\n<li>C is again sign extended.</li>\n<li>Sign extended c is copied to return value register and function return to caller.</li>\n<li>To store result caller function again convert int to char.</li>\n</ol>\n\n<p>My questions are -</p>\n\n<ol>\n<li>Who does this ?</li>\n<li>What is necessity of doing so many conversions ? </li>\n<li>Will it reduce/increase the performance of machine/compiler ?</li>\n<li>If it is reducing performance what should we do in order to increase it ?</li>\n</ol>\n"},{"tags":["performance","programming-languages","sse","simd"],"answer_count":0,"favorite_count":1,"up_vote_count":2,"down_vote_count":1,"view_count":84,"score":1,"question_id":12689779,"title":"Which programming language give access to SIMD instructions?","body":"<p>It seems that the paramount elements to reach top performance on modern CPUs are:</p>\n\n<ol>\n<li><p>Threads level parallelism</p></li>\n<li><p>SIMD usage inside each thread</p></li>\n</ol>\n\n<p>Most of modern programming languages give support (or even emphasis) on thread level parallelism. However, it seems that SIMD instructions support is much more rare.</p>\n\n<p>Trying to provide \"great abstractions\" many programming language block access to the details of the underlying CPU.</p>\n\n<p>Because of this C/C++ seems to stay king when speed is the main goal, and many language just provide means to call C level code for the \"numerical hotspots\".</p>\n\n<ul>\n<li>I know of C, C++, C#/DotNet/Mono's Mono.SIMD, D's core.simd.</li>\n<li>To my knowledge Java and Go do not (and will not) support explicit SIMD (and hope the best from the compiler). </li>\n<li>Ruby and Python only provide access to SIMD via C wrapper libraries (such as numpy).</li>\n<li>Haskell, Rust seem to not yet support SIMD in their main branches...</li>\n</ul>\n\n<blockquote>\n  <p>Which other \"performance sensitive\" programming languages do provide access SIMD instructions ?</p>\n</blockquote>\n\n<p>Thank you for your suggestions.</p>\n"},{"tags":["asp.net","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":38,"score":1,"question_id":12690793,"title":"ASP.NET: How can I get the total server AND client page load time?","body":"<p>I am measuring performance of an ASP.NET page.  I need to measurement two parts: 1) The server page load time (to calculate the time it takes to bind controls from data loaded from database) and 2) measure the difference between the millisecond that the server's PageLoad event started and the millisecond that the client has finished loading (because I want to be able to calculate the time it took for the client browser to download and render the HTML).</p>\n\n<p>Measuring the serverside load time is easy with the StopWatch class.  The part I need advice for is capturing the date and time that the client has loaded.</p>\n\n<p>Here's what I am doing in this order:</p>\n\n<ul>\n<li>As soon as server page load event runs, I write the current date and time to a hidden field.</li>\n<li>On the client, I use <code>$(document).ready()</code> to fire off an AJAX call to a WebMethod that returns the server's date and time.</li>\n<li>I then parse the dates and times from the date string stored in the hidden field and the date string returned from the Webmethod call and write their differences to a <code>span</code> element on the page.</li>\n</ul>\n\n<p>I do this so I can keep the client datetime in sync with the server datetime due to the fact that someone's machine time can easily be off by a few seconds.  However, I think that AJAX call adds a little overhead itself which is counterproductive to performance measurement.</p>\n\n<p>Is there an accurate method of using GMT to sync up the client time with the server time without having to report to the server to get it?</p>\n"},{"tags":["java","performance","jetty","default","jetty-8"],"answer_count":1,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":102,"score":4,"question_id":12668616,"title":"Is the default configuration good in Jetty 8 sensible for quite heavily loaded web application?","body":"<p>Its been decided to deploy an application to Jetty 8, previously was using Tomcat 7. Jetty 8 is being used using the defaults, and seems to be working okay, but then occasionally waiting for connections, looking at the jetty.xml I'm confused about the interplay between threads and connectors.</p>\n\n<p>But the general question is should the defaults work for a quite heavily loaded web application or are their obvious changes that need to be made ?</p>\n\n<p><strong>Update</strong>\nLet me try and me more specific about this. </p>\n\n<p>In jetty.xml it says maxThreads=200 so it can in theory deal with 200 requests in parallel (more than we have to deal with) , but is there a rule of thumb about how much memory jetty should be configured with to go with this.</p>\n\n<p>It also defines a SelectChannelConnector connector, I assume this is the thing that receives requests and farms them out to threads. is this connector the best performing one available to us ?</p>\n"},{"tags":["performance","acl","firewall","iptables"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":76,"score":0,"question_id":10985453,"title":"iptables and cisco acl throuhput exponential behaviour","body":"<p>i am writting to ask about iptables and cisco acl performance in TCP and UDP filtering. I was testing it with large number of firewall rules. I get exponential behaviour. </p>\n\n<p><a href=\"http://s18.postimage.org/a0crsee7d/image.png\" rel=\"nofollow\">http://s18.postimage.org/a0crsee7d/image.png</a></p>\n\n<p>Iptables and cisco acl uses linear filter, but throuhput is exponential. I have read that is because of cumulating effect. Can you explain what is this? Or why i get exponiantial behavior?</p>\n"},{"tags":["php","performance","magento"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":76,"score":0,"question_id":12684553,"title":"magento php performance tuning","body":"<p>I am tying to build an ecommerce front-end using magento SOAP apis. Please note that I am directly calling the SOAP apis to fetch data and not using magento theming to build the UI. Few experienced professionals had told me that magento SOAP apis are slow, and that is due to database's EAV structure etc. However, on profiling I had found that most of the time seems to be spent in php layer than database layer. Can someone throw some light on this observation and suggest some solution. </p>\n"},{"tags":["wpf","performance","mvvm","datatemplate","wpf-usercontrols"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":165,"score":1,"question_id":10974437,"title":"How To improve wpf user control performance?","body":"<p>I have a usercontrol, and when I have lots of that in a window, it takes long time to get loaded.\nWill it get better if I change it to a customcontrol or maybe a datatemplate with a class and attached properties?\nany ideas would be greatly appreciated.</p>\n\n<p>Edited:</p>\n\n<p>this is my control:</p>\n\n<pre><code>&lt;UserControl \n    x:Class=\"Pouyansoft.WPF.MVVM.Control.Common.View.DataGridSelectorControl\"\n    xmlns=\"http://schemas.microsoft.com/winfx/2006/xaml/presentation\"\n    xmlns:x=\"http://schemas.microsoft.com/winfx/2006/xaml\"\n    xmlns:mc=\"http://schemas.openxmlformats.org/markup-compatibility/2006\" \n    xmlns:d=\"http://schemas.microsoft.com/expression/blend/2008\" \n    x:Name=\"dataGridSelector\"\n    xmlns:sys=\"clr-namespace:System;assembly=mscorlib\"\n    mc:Ignorable=\"d\" &gt;\n&lt;UserControl.Resources&gt;\n    &lt;CollectionViewSource Source=\"{Binding DataCollection.Source}\" x:Key=\"theSource\"/&gt;\n    &lt;Style x:Key=\"DataGridColumnHeaderStyle1\" TargetType=\"{x:Type DataGridColumnHeader}\"&gt;\n        &lt;Setter Property=\"Template\"&gt;\n            &lt;Setter.Value&gt;\n                &lt;ControlTemplate TargetType=\"{x:Type DataGridColumnHeader}\"&gt;\n                    &lt;Grid VerticalAlignment=\"Center\" HorizontalAlignment=\"Stretch\"&gt;\n                        &lt;Grid.RowDefinitions&gt;\n                            &lt;RowDefinition Height=\"*\"/&gt;\n                            &lt;RowDefinition Height=\"*\"/&gt;\n                        &lt;/Grid.RowDefinitions&gt;\n                        &lt;TextBlock Grid.Row=\"0\" Text=\"{TemplateBinding Content}\" \n                                   HorizontalAlignment=\"Center\" /&gt;\n                        &lt;TextBox x:Name=\"txtSearch\" Grid.Row=\"1\"  HorizontalAlignment=\"Stretch\"  \n                                 BorderThickness=\"1\" TextChanged=\"TextBox_TextChanged\" /&gt;\n                    &lt;/Grid&gt;\n                &lt;/ControlTemplate&gt;\n            &lt;/Setter.Value&gt;\n        &lt;/Setter&gt;\n    &lt;/Style&gt;\n&lt;/UserControl.Resources&gt;\n\n&lt;Grid&gt;       \n   &lt;DataGrid x:Name=\"grd\" \n             ItemsSource=\"{Binding Source={StaticResource theSource}}\" \n             AutoGenerateColumns=\"False\"\n             ColumnHeaderStyle=\"{DynamicResource DataGridColumnHeaderStyle1}\"   \n             PreviewKeyDown=\"grd_PreviewKeyDown\"\n             SelectedIndex=\"{Binding SelectedIndex}\"\n             behavior:MouseDoubleClick.Command=\"{Binding MouseDoubleClickCommand}\" \n             PreviewMouseLeftButtonUp=\"grid1_PreviewMouseLeftButtonUp\"  \n             GridLinesVisibility=\"Vertical\"&gt;\n    &lt;/DataGrid&gt;\n&lt;/Grid&gt;\n</code></pre>\n\n<p></p>\n\n<p>and some code in the code behind.(and actually  all the other control has the same behavior)</p>\n"},{"tags":["c","performance","oprofile"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":105,"score":1,"question_id":11444885,"title":"OPROFILE can't get performance data","body":"<p>I am using OPROFILE to collect some performance data.\nbut I got in troubule.</p>\n\n<p><strong>Here is my shell:</strong></p>\n\n<pre><code>~ # rm -f /root/.oprofile/daemonrc\n~ # opcontrol --setup --no-vmlinux\n~ # opcontrol --init\n~ # opcontrol --reset\n~ # opcontrol --start\n~ # opcontrol --status\n\nDaemon running: pid 14909    \nSeparate options: none\nvmlinux file: none    \nImage filter: none    \nCall-graph depth: 0\n\n~ # opcontrol --shutdown\n\nStopping profiling.\nKilling daemon.\n\n~ # opreport\n\nerror: no sample files found: profile specification too strict?\n\n~ # tree /var/lib/oprofile/\n\n/var/lib/oprofile/\n├── abi\n├── complete_dump\n├── jitdump\n├── opd_pipe\n└── samples\n    ├── current\n    │   └── stats\n    │       ├── bt_lost_no_mapping\n    │       ├── cpu0\n    │       │   ├── backtrace_aborted\n    │       │   ├── sample_invalid_eip\n    │       │   ├── sample_lost_overflow\n    │       │   └── sample_received\n    │       ├── event_lost_overflow\n    │       ├── multiplex_counter\n    │       ├── sample_lost_no_mapping\n    │       └── sample_lost_no_mm\n    └── oprofiled.log\n\n\n5 directories, 13 files\n\n~ # dmesg |grep oprofile\n\noprofile: using NMI interrupt.\n\n\n~ # uname -a\n\nLinux localhost.localdomain 2.6.32-220.4.2.el6.x86_64 #1 SMP Tue Feb 14 04:00:16 GMT 2012 x86_64 x86_64 x86_64 GNU/Linux\n\n~ # cat /proc/cpuinfo \n\nprocessor   : 0    \nvendor_id   : GenuineIntel   \ncpu family  : 6    \nmodel       : 44    \nmodel name  : Intel(R) Xeon(R) CPU           E5620  @ 2.40GHz   \nstepping    : 2    \ncpu MHz     : 2400.085   \ncache size  : 12288 KB   \nfpu     : yes    \nfpu_exception   : yes    \ncpuid level : 11    \nwp      : yes   \nflags       : fpu vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc up arch_perfmon pebs bts xtopology tsc_reliable nonstop_tsc aperfmperf unfair_spinlock pni pclmulqdq ssse3 cx16 sse4_1 sse4_2 popcnt aes hypervisor lahf_lm ida arat epb dts    \nbogomips    : 4800.17    \nclflush size    : 64   \ncache_alignment : 64   \naddress sizes   : 40 bits physical, 48 bits virtual   \npower management:\n</code></pre>\n"},{"tags":["mysql","performance","alter-table","tuning"],"answer_count":7,"favorite_count":8,"up_vote_count":19,"down_vote_count":0,"view_count":10874,"score":19,"question_id":654594,"title":"Optimizing MySQL for ALTER TABLE of InnoDB","body":"<p>Sometime soon we will need to make schema changes to our production database. We need to minimize downtime for this effort, however, the ALTER TABLE statements are going to run for quite a while. Our largest tables have 150 million records, largest table file is 50G.\nAll tables are InnoDB, and it was set up as one big data file (instead of a file-per-table).\nWe're running MySQL 5.0.46 on an 8 core machine, 16G memory and a RAID10 config.</p>\n\n<p>I have some experience with MySQL tuning, but this usually focusses on reads or writes from multiple clients. There is lots of info to be found on the Internet on this subject, however, there seems to be very little information available on best practices for (temporarily) tuning your MySQL server to speed up ALTER TABLE on InnoDB tables, or for INSERT INTO .. SELECT FROM (we will probably use this instead of ALTER TABLE to have some more opportunities to speed things up a bit).</p>\n\n<p>The schema changes we are planning to do is adding a integer column to all tables and make it the primary key, instead of the current primary key. We need to keep the 'old' column as well so overwriting the existing values is not an option.</p>\n\n<p>What would be the ideal settings to get this task done as quick as possible?</p>\n"},{"tags":["php","performance","c#-4.0","phalanger"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":12688274,"title":"Phalanger is slower then PHP?","body":"<p>My goal is the speed up my PHP code. I was googling when i found Phalanger <a href=\"http://phalanger.codeplex.com/\" rel=\"nofollow\">http://phalanger.codeplex.com/</a> .</p>\n\n<blockquote>\n  <p>Phalanger is an open-source PHP implementation introducing the PHP\n  language into the family of compiled .NET languages. It provides PHP\n  applications an execution environment that is fast and extremely\n  compatible with the vast array of existing PHP code.</p>\n</blockquote>\n\n<p>from their other site <a href=\"http://www.php-compiler.net/\" rel=\"nofollow\">http://www.php-compiler.net/</a> </p>\n\n<blockquote>\n  <p>Phalanger improves execution speed, safety and makes integration with\n  existing PHP and .NET code very simple.</p>\n</blockquote>\n\n<p>so i made a test whit the following code:</p>\n\n<pre><code>&lt;?php\n$time_start = microtime(true);\n$j=0;\nfor($i=0;$i&lt;400000;$i++)\n    $j++;\n\necho $j.\"\\n\";\n$time_end = microtime(true);\n$time = $time_end - $time_start;\necho \"Time: \".$time.\"\\n\";\nsleep(5);\n?&gt;\n</code></pre>\n\n<p>the result on my (old) computer are:\nTime: 0.12532997131348 (PHP 5.4.3,Apache)\nTime: 0.2832031 (console application built by Phalanger 3.0)</p>\n\n<p>So it makes more then 2 times slower! Can u confirm this or i just missing something?</p>\n\n<p>ps.: does Phalanger make an actual C# source code from PHP? how can i see it?</p>\n"},{"tags":["performance","google-chrome","browser-cache","stackoverflow.com"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":15,"score":0,"question_id":12688464,"title":"Why is refreshing stackoverflow.com while bypassing the browser cache invariably faster than a regular refresh?","body":"<p>I intentionally didn't put it on meta, because I'm asking the question out of interest in the technical topic of how any webpage could ever be faster without a cache than with.  This has been my consistent experience ever since I've started using Stack. It's quite frustrating, actually.</p>\n\n<p>It's also peculiar and fascinating.</p>\n\n<p>The right answer gets a pony!</p>\n"},{"tags":["wpf","performance","binding"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":42,"score":0,"question_id":12687565,"title":"WPF : Measure data binding time consumption","body":"<p>How to measure the data binding time consumption alone in WPF application.</p>\n"},{"tags":["mysql","sql","performance","stored-procedures"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":52,"score":0,"question_id":12686985,"title":"MySQL Store Procedure with Statement","body":"<p>I would like to know, <strong>in general</strong>, performance comparison executing a stored procedure that execute a statement with a syntax as</p>\n\n<pre><code>BEGIN\n\n    DECLARE query VARCHAR(5000);\n    SET @query = \" &lt;SQL CODE&gt;\";\n\n    PREPARE stmt1 FROM @query;\n    EXECUTE stmt1;\n    DEALLOCATE PREPARE stmt1;\n\nEND\n</code></pre>\n\n<p>or executing directly AS</p>\n\n<pre><code>BEGIN\n\n    &lt;SQL CODE&gt;;\n\nEND\n</code></pre>\n\n<p>in my case i must use first case because i want to set a input parameter to set custom LIMIT. And I wondered if, in general, there could be some performance degradation.</p>\n\n<p>MySQL Information</p>\n\n<ul>\n<li>protocol_version 10</li>\n<li>version  5.0.95 </li>\n<li>version_bdb  Sleepycat Software: Berkeley DB 4.1.24: (December 16, 2011) </li>\n<li>version_comment  Source</li>\n<li>distribution version_compile_machine x86_64</li>\n<li>version_compile_os   redhat-linux-gnu</li>\n</ul>\n"},{"tags":["c#","performance","memory","counter","processor"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":66,"score":0,"question_id":12687060,"title":"C# performance counter missing or renamed","body":"<p>My small C# app uses \"Processor/% Processor Time/0\" counter to determine the busy of the processor core. One day my working app started to not working anymore. Debugging it I found out the the \"Processor\" category renamed to \"Processor Information\". Odd. Maybe after installing some app or applying a win8 update patch?</p>\n\n<p>The same app uses \"Memory/Available MBytes\" counter to check information from the RAM use. After repairing the processor category name, I found out that the Memory category is totally missing (not renamed, just disappeared). Now I spend hours to find out what happened and how to add this counter back (nor the Task Manager window I can see the memory usage statistics). </p>\n\n<p>Two questions I have: \n- is this normal? these kind of apps contains no category names but these come from app config, because it changes so much? How can I work with conuters using fix names, or find out the new names? Or prevent this situation?\n- after searching google I cannot find other possibilities to determine the processor core 0 and core 1 busy percentage, and how much free memory available globally. But it is pretty weird to me that no win32 dll and function exists for these operations. Really, there is no one? </p>\n"},{"tags":["performance","matlab","matrix","comparison"],"answer_count":1,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":53,"score":5,"question_id":12681752,"title":"Pairwise Comparison of rows in a matrix","body":"<p>I have a simulation code that produces a binary matrix in matlab always with 10 rows but a varying number of columns. </p>\n\n<p>For example here:</p>\n\n<pre><code> 1     0     0     0\n 0     0     0     0\n 0     1     0     0\n 1     0     0     0\n 1     0     0     0\n 1     0     1     0\n 0     0     0     1\n 1     0     0     0\n 0     0     0     0\n 0     0     0     0\n</code></pre>\n\n<p>I want to do pairwise comparision between the rows to determine how many elements are different between the two rows, ultimately to create a 10x10 symmetric matrix with the number of differences between the rows. Eg. Row 1 compared with Row 2...and so on. </p>\n\n<p>So the (1,2) element (as well as the 2,1 element) of this matrix would compare row 1 with row 2 and would be 1 in this case as there is only a single difference. </p>\n\n<p>I know this could be done with lots of loop coding, however feel there is likely a simpler way that I do not know of.</p>\n\n<p>How should this be accomplished? </p>\n"},{"tags":["performance","sync","syncframework"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":12676115,"title":"MS Sync Toolkit how to handle huge initial download","body":"<p>We're using Microsoft Sync Framework Toolkit to allow some iOS devices to synchronize their SQLite local storage with a central SQL Server database over an OData service (just like the sample does).</p>\n\n<p>The problem is that the central database is quite big (more than 500 MB) and that the customer want to initially replicate all his content on their devices.</p>\n\n<p>So we configured the service to <strong>sync all tables</strong> but it crashes the first time a device tries to sync (<code>OutOfMemoryException</code>).</p>\n\n<p>How should we design/configure the service to manage such a situation? Setting filters doesn't seem to fit our needs since we need all the data.</p>\n\n<p>I should add that we tried to configure <code>SetDownloadBatchSize</code> with no luck. Maybe there is something to configure on the client as well?</p>\n"},{"tags":["ajax","performance","http"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":189,"score":4,"question_id":11917289,"title":"Max limit on number of concurrent ajax request","body":"<p>I have a website that fetches approximately 20 pages, each page is different. Currently, it takes about 1.2 minutes to load.</p>\n\n<p>Some days back this task only took 11-15 seconds. Now it takes 1.2 minutes. What can be the reason for sudden change?</p>\n\n<p>Is there any solution other than merging some of these to reduce the number of requests? Can the limit of number of requests be somehow altered?</p>\n\n<p><img src=\"http://i.stack.imgur.com/9BCOF.png\" alt=\"enter image description here\"></p>\n\n<p>The above screenshot is from Firebug. the gray portions of the bars represent \"Blocking\".</p>\n"},{"tags":["asp.net","performance","iis","threadpool"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":254,"score":2,"question_id":7365222,"title":"ASP.NET, IIS /CLR Thread & request in relation to synchronous v.s asynchronous programming","body":"<p>I'm just trying to clear up some concepts here. If anyone is willing to share their expertise on this matter, it's greatly appreciated it. </p>\n\n<p>The following is my understanding of how IIS works in relation to threads, please correct me if I'm wrong. </p>\n\n<h2>HTTP.sys</h2>\n\n<p>As I understand it, for IIS 6.0 (I'll leave IIS 7.0 for now), web browser makes a request, gets pick up by HTTP.sys kernel driver, HTTP.sys hands it over to IIS 6.0's threadpool (I/O thread?) and such free up itself. </p>\n\n<h2>IIS 6.0 Thread/ThreadPool</h2>\n\n<p>IIS 6.0's thread in returns hands it over to ASP.NET, which returns a temporary HSE_STATUS_PENDING to IIS 6.0, such frees up the IIS 6.0 thread and then forward it to a CLR Thread. </p>\n\n<h2>CLR Thread/ThreadPool</h2>\n\n<p>When ASP.NET picks up a free thread in the CLR threadpool, it executes the request. If there are no available CLR threads, it gets queued up in the application level queue (which has bad performance)  </p>\n\n<p>So based on the previous understanding, my questions are the following. </p>\n\n<ol>\n<li><p>In synchronous mode, does that mean 1 request per 1 CLR thread? </p>\n\n<p>*) If so, how many CONCURRENT requests can be served on a 1 CPU? Or should I ask the reverse? How may CLR threads are allowed per 1 CPU? Say, 50 CLR threads are allowed, does that mean then it's limited to serve 50 requests at any given time? Confused. </p></li>\n<li><p>If I set the \"requestQueueLimit\" in \"processModle\" configuration to 5000, what does it mean really? You can queue up 5000 requests in the application queue?  Isn't that really bad? Why would you ever set it so high since application queue has bad performance? </p></li>\n<li><p>If you are programming asynchronous page, exactly where it starts to get the benefit in the above process? </p></li>\n<li><p>I researched and see that by default, IIS 6.0's threadpool size is 256. 5000 concurrent requests comes in, handled by 256 IIS 6.0 threads and then each of the 256 threads, hands it off to CLR threads which i'm guessing is even lower by default.  isn't that itself asynchronous? A bit confused. In addition, where and when does the bottleneck start to show up in synchronous mode? and in asynchronous mode?  (Not sure if I'm making any sense, I'm just confused). </p></li>\n<li><p>What happens when IIS threadpool (all 256 of them) are busy? </p></li>\n<li><p>What happens when all CLR threads are busy? (I assume then, all requests are queued up in the application level queue)</p></li>\n<li><p>What happens when application queue is over the requestQueueLimit?</p></li>\n</ol>\n\n<p>Thanks a lot for reading, greatly appreciate your expertise on this matter. </p>\n"},{"tags":["performance","microsoft"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":22,"score":0,"question_id":12685198,"title":"How to generate reports in Fast Search 2010 for Sharepoint","body":"<p>I am using sharepoint search web service to query FS4SP. I want to generate reports which will show number of query, number of successful query, number of failed query and query terms. Is there any way to get this reports?</p>\n\n<p>As i am using web service, so Sharepoint site collection analytics is not showing the query terms and its related things.</p>\n"},{"tags":["c++","performance","visual-studio-2010","compilation","floating-point"],"answer_count":3,"favorite_count":233,"up_vote_count":547,"down_vote_count":4,"view_count":66139,"score":543,"question_id":9314534,"title":"Why does changing 0.1f to 0 slow down performance by 10x?","body":"<p>Why does this bit of code,</p>\n\n<pre><code>const float x[16] = {  1.1,   1.2,   1.3,     1.4,   1.5,   1.6,   1.7,   1.8,\n                       1.9,   2.0,   2.1,     2.2,   2.3,   2.4,   2.5,   2.6};\nconst float z[16] = {1.123, 1.234, 1.345, 156.467, 1.578, 1.689, 1.790, 1.812,\n                     1.923, 2.034, 2.145,   2.256, 2.367, 2.478, 2.589, 2.690};\nfloat y[16];\nfor (int i = 0; i &lt; 16; i++)\n{\n    y[i] = x[i];\n}\n\nfor (int j = 0; j &lt; 9000000; j++)\n{\n    for (int i = 0; i &lt; 16; i++)\n    {\n        y[i] *= x[i];\n        y[i] /= z[i];\n        y[i] = y[i] + 0.1f; // &lt;--\n        y[i] = y[i] - 0.1f; // &lt;--\n    }\n}\n</code></pre>\n\n<p>run more than 10 times faster than the following bit (identical except where noted)?</p>\n\n<pre><code>const float x[16] = {  1.1,   1.2,   1.3,     1.4,   1.5,   1.6,   1.7,   1.8,\n                       1.9,   2.0,   2.1,     2.2,   2.3,   2.4,   2.5,   2.6};\nconst float z[16] = {1.123, 1.234, 1.345, 156.467, 1.578, 1.689, 1.790, 1.812,\n                     1.923, 2.034, 2.145,   2.256, 2.367, 2.478, 2.589, 2.690};\nfloat y[16];\nfor (int i = 0; i &lt; 16; i++)\n{\n    y[i] = x[i];\n}\n\nfor (int j = 0; j &lt; 9000000; j++)\n{\n    for (int i = 0; i &lt; 16; i++)\n    {\n        y[i] *= x[i];\n        y[i] /= z[i];\n        y[i] = y[i] + 0; // &lt;--\n        y[i] = y[i] - 0; // &lt;--\n    }\n}\n</code></pre>\n\n<p>when compiling with Visual Studio 2010 SP1. (I haven't tested with other compilers.)</p>\n"},{"tags":["performance","matlab","math"],"answer_count":5,"favorite_count":0,"up_vote_count":6,"down_vote_count":0,"view_count":324,"score":6,"question_id":5177758,"title":"Winners on a Pentago board","body":"<p>For those who don't know what Pentago is, it's not really that important to the problem, but suffice to say that you have a 6x6 board with four quadrants.  Each player takes turns placing a piece and then rotating a quadrant.  The game is won when one player gets five in a row (either before or after the player's rotate phase).</p>\n\n<p>I'm writing an algorithm to play many different <em>random</em> Pentago games.  However, since it's completely random, I see no good way to get around checking to see if someone wins in between the place and rotate phase of the turn (otherwise, you might accidentally rotate the winning move).  Eventually, I plan on rewriting this to where it has a little bit more strategy involved instead of completely random, but this is for statistical purposes, so randomness does just fine (and in fact is quite useful in some ways).</p>\n\n<p>Anyways, currently I am programming in Matlab, and an empty board looks like this</p>\n\n<pre><code>eeeeee\neeeeee\neeeeee\neeeeee\neeeeee\neeeeee\n</code></pre>\n\n<p>As the game progresses, the board fills with <code>w</code>'s and <code>b</code>'s.  The way that I check for a winning board is quite literally iterating through every column and every row (and every diagonal) to see if there is a winner by performing a regular expression check on the \"string\" that is returned.</p>\n\n<p>In short, my question is this:</p>\n\n<p>Is there a more efficient method for determining the winners of a Pentago board?</p>\n"},{"tags":["php","performance","joomla","prepared-statement"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":54,"score":0,"question_id":12683263,"title":"Joomla JTable $row->store() unbearably slow","body":"<p>I have a wierd issue. I'm running Joomla 3.0 on WAMP for testing, and I'm trying to insert some rows into my menu table using a custom plugin.</p>\n\n<p>The table only has the dummy Joomla data - i.e. less than 100 rows.</p>\n\n<p>My code is very standard (see below).</p>\n\n<p>I have narrowed down the issue to the $row->store(); line - if I allow this to run the system becomes unresponsive for about 60 seconds or more, with just a single insert.</p>\n\n<p>If I comment out the store() line, the script runs very fast, but doesn't save the data (obviously).</p>\n\n<p>Is there any easy way to speed up the store() function? I can't imagine what the problem might be. This is a brand new install of the new Joomla 3.0 &amp; everything else runs fine.</p>\n\n<pre><code>    $array = array(\n        'id' =&gt; $data['id'],\n        'menutype' =&gt; 'mymenu',\n        'title' =&gt; $data['title'],\n        'alias' =&gt; $data['alias'],\n        'link' =&gt; $data['link'],\n        'type' =&gt; 'component',\n        'published' =&gt; $data['published'],\n        'parent_id' =&gt; $data['parent'],\n        'level' =&gt; $data['level'],\n        'component_id' =&gt; 22,\n        'access' =&gt; $data['access'],\n        'language' =&gt; $data['language']\n    );\n    $row = &amp; JTable::getInstance('menu');\n    if($row-&gt;bind($array)) {\n        $row-&gt;store();\n    }\n    $row-&gt;rebuild();\n</code></pre>\n"},{"tags":["linux","performance","cpu-cache","worker-process"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":51,"score":0,"question_id":12676230,"title":"Cache-concious design of Master-Worker processes","body":"<p>I recently started working on a server application designed with the familiar <em>Master-Worker</em> pattern with threads, where one privileged thread manages several worker threads. I have now realized how troublesome threads truly are.</p>\n\n<p>I am now considering the possibility of moving to processes instead of threads, because they solve <em>a lot</em> of the issues I'm experiencing.</p>\n\n<p>However, performance is a major concern, which I fear will decline as the memory usage rises due to duplicated data (lookup tables, context data, etc.) contending for space in the L2/L3 caches. This data needs to be occassionally modified, and may grow quite large.</p>\n\n<pre><code>hash_table files; \n\nfunction serve_file(connection, path)\n    file = hash_table[path]\n    sendfile(connection.fd, file.fd, 0, file.size);\n\nfunction on_file_added_to_server_root(which)\n    files.add(which, ...)\n</code></pre>\n\n<p>Given <code>N</code> worker processes, it'd be a shame if there were <code>N</code> copies of this table. However, some tables would be perfect to have duplicated everywhere. But then there's also numerous <code>malloc(3)</code>-allocated memory that could potentially be shared, but may be scattered all over, causing random pages to be duplicated due to copy-on-write.</p>\n\n<p>Are there any tricks or general strategies to keep memory usage tight in multi-process designs?</p>\n\n<p>Thanks!</p>\n"},{"tags":["mysql","performance","load-data-infile","bulk-load"],"answer_count":3,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":3132,"score":4,"question_id":827445,"title":"What mysql settings affect the speed of LOAD DATA INFILE?","body":"<p>Let me set up the situation.  We are trying to insert a modestly high number of rows (roughly 10-20M a day) into a MyISAM table that is modestly wide:</p>\n\n<pre><code>+--------------+--------------+------+-----+---------+-------+\n| Field        | Type         | Null | Key | Default | Extra |\n+--------------+--------------+------+-----+---------+-------+\n| blah1        | varchar(255) | NO   | PRI |         |       | \n| blah2        | varchar(255) | NO   | PRI |         |       | \n| blah3        | varchar(5)   | NO   | PRI |         |       | \n| blah4        | varchar(5)   | NO   | PRI |         |       | \n| blah5        | varchar(2)   | NO   | PRI |         |       | \n| blah6        | varchar(2)   | NO   | PRI |         |       | \n| blah7        | date         | NO   | PRI |         |       | \n| blah8        | smallint(6)  | NO   | PRI |         |       | \n| blah9        | varchar(255) | NO   | PRI |         |       | \n| blah10       | bigint(20)   | YES  |     | NULL    |       | \n+--------------+--------------+------+-----+---------+-------+\n</code></pre>\n\n<p>The only index besides that whopping primary key is on the blah7, the date field.  We are using LOAD DATA INFILE and seeing what strikes me as pretty awful performance, around 2 hours to load the data.  I was led to believe that LOAD DATA INFILE was orders of magnitude faster than that.</p>\n\n<p>Interestingly, we have some less fat tables (5-6 fields) that we also use LOAD DATA INFILE to batch data into and we see much better performance on those.  The number of records is quite a bit smaller, which leads me to think that we are running up against a buffer size limit when we load the large table, and are having to go to disk (and really, what else but going to disk would explain such slow load times?).</p>\n\n<p>...which brings me to my question.  What my.cnf settings are most important when dealing with LOAD DATA INFILE commands?</p>\n"},{"tags":["mysql","database","performance","query","query-performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":12681907,"title":"MySQL query with limit and offset is running very slow","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/12680738/mysql-query-with-limit-and-large-offset-taking-forever\">MySQL query with limit and large offset taking forever</a>  </p>\n</blockquote>\n\n\n\n<p>My table contains 25,000 rows and im using pagination to display all rows on page.\nIf my page limit is set to 10 rows per page then it displays very fast each page. However if i change the page limit size to 250 rows per page it takes 30 seconds to 1 minute to load the table (display records). The query that im using is this:</p>\n\n<pre><code>SELECT SQL_CALC_FOUND_ROWS Merchants.*, DataSources.DsISOName, PrName\nFROM Merchants \nINNER JOIN DataSources ON MDsID=DsID \nINNER JOIN Processors ON PrID = DsType\nORDER BY MDBA LIMIT 0, 250\n</code></pre>\n\n<p>This is what i get when i load the page first. Limit numbers 0, 250 get changes if i change the page, so if i visit page 2 Limit will change to : </p>\n\n<pre><code>  LIMIT 250, 500 \n</code></pre>\n\n<p>If anyone can see a problem with my query please let me know. I can not understand why does it take 1 minute to load the table when 250 rows are being fetched per page. \nOn mysql table i tried using indexes on various ID's but still same speed.</p>\n"},{"tags":["c#","performance","linq","ienumerable"],"answer_count":4,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":112,"score":3,"question_id":12680933,"title":"LINQ c# efficiency","body":"<p>I need to write a query pulling distinct values from columns defined by a user for any given data set.   There could be millions of rows so the statements must be as efficient as possible.   Below is the code I have. </p>\n\n<p>What is the order of this LINQ query?\nIs there a more efficent way of doing this?</p>\n\n <pre class=\"lang-cs prettyprint-override\"><code>var MyValues = from r in MyDataTable.AsEnumerable()\n                                   orderby r.Field&lt;double&gt;(_varName)\n                                   select r.Field&lt;double&gt;(_varName); \n\nIEnumerable result= MyValues.Distinct();\n</code></pre>\n"},{"tags":["c#","performance","itextsharp"],"answer_count":6,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":3644,"score":0,"question_id":1982028,"title":"Bad performance convert tif to pdf using ITextSharp","body":"<p><strong>Summary:</strong> How can I reduce the amount of time it takes to convert tifs to pdfs using <code>itextsharp</code>?</p>\n\n<p><strong>Background:</strong> I'm converting some fairly large tif's to pdf using C# and <code>itextsharp</code>, and I am getting extremely bad performance.  The tif files are approximately 50kb a piece, and some documents have up to 150 seperate tif files (each representing a page).  For one 132 page document (~6500 kb) it took about 13 minutes to convert.  During the conversion, the single CPU server it was running on was running at 100%, leading me to believe the process was CPU bound.  The output pdf file was 3.5 MB.  I'm ok with the size, but the time taken seem a bit high to me.</p>\n\n<p><strong>Code:</strong></p>\n\n<pre><code>private void CombineAndConvertTif(IList&lt;FileInfo&gt; inputFiles, FileInfo outputFile)\n{\n    using (FileStream fs = new FileStream(outputFile.FullName, FileMode.Create, FileAccess.ReadWrite, FileShare.None))\n    {\n        Document document = new Document(PageSize.A4, 50, 50, 50, 50);\n        PdfWriter writer = PdfWriter.GetInstance(document, fs);\n        document.Open();\n        PdfContentByte cb = writer.DirectContent;\n\n        foreach (FileInfo inputFile in inputFiles)\n        {\n            using (Bitmap bm = new Bitmap(inputFile.FullName))\n            {\n                int total = bm.GetFrameCount(FrameDimension.Page);\n\n                for (int k = 0; k &lt; total; ++k)\n                {\n                    bm.SelectActiveFrame(FrameDimension.Page, k);\n                    //Testing shows that this line takes the lion's share (80%) of the time involved.\n                    iTextSharp.text.Image img =\n                        iTextSharp.text.Image.GetInstance(bm, null, true);\n                    img.ScalePercent(72f / 200f * 100);\n                    img.SetAbsolutePosition(0, 0);\n\n                    cb.AddImage(img);\n                    document.NewPage();\n                }\n            }\n        }\n\n        document.Close();\n        writer.Close();\n    }\n\n}\n</code></pre>\n"},{"tags":["sql-server","asp.net-mvc","performance","iis7","entity-framework-4"],"answer_count":7,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":409,"score":2,"question_id":10700957,"title":"Establishing SQL Connection Taking 10 - 15 Seconds","body":"<p>We are having some strange performance issues and I was hoping somebody may be able to point us in the right direction.  Our scenario is an <code>ASP.NET MVC C#</code> website using <code>EF4 POCO</code> in <code>IIS 7</code> (highly specced servers, dedicated just for this application).  </p>\n\n<p>Obviously it's slow on application_startup which is to be expected, but once that has loaded you can navigate the site and everything is nice and snappy 0.5ms page loads (we are using <code>Mini-Profiler</code>).  Now if you stop using the site for say 5 - 10 minutes (we have the app pool recycle set to 2 hours and we are logging so we know that it hasn't been recycled) then the first page load is ridiculously slow, 10 - 15 seconds, but then you can navigate around again without issue (0.5ms).</p>\n\n<p>This is not <code>SQL queries</code> that are slow as all queries seem to work fine after the first page hit even if they haven't been run yet so not caching anywhere either.</p>\n\n<p>We have done a huge amount of testing and I can't figure this out.  The main thing I have tried so far is to Pre generate EF views but this has not helped.</p>\n\n<p>It seems after looking at <code>Sql Server Profiler</code> after 5 minutes give or take 30 seconds with no activity in Sql Server Profiler and no site interaction a couple of \"Audit Logout\" entries appear for the application and as soon as that happens it then seems to take 10 - 15 seconds to refresh the application. Is there an idle timeout on <code>Sql Server</code>?</p>\n"},{"tags":["c#","performance","linq"],"answer_count":3,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":741,"score":4,"question_id":5060663,"title":"How to find if an element of a list is in another list?","body":"<p>I want to know if at least one element in a first list can be found in a second list.</p>\n\n<p>I can see two ways to do it. Let's say our lists are:</p>\n\n<pre><code>List&lt;string&gt; list1 = new[] { \"A\", \"C\", \"F\", \"H\", \"I\" };\nList&lt;string&gt; list2 = new[] { \"B\", \"D\", \"F\", \"G\", \"I\" };\n</code></pre>\n\n<p>The first approach uses a loop:</p>\n\n<pre><code>bool isFound = false;\nforeach (item1 in list1)\n{\n    if (list2.Contains(item1))\n    {\n        isFound = true;\n        break;\n    }\n}\n</code></pre>\n\n<p>The second one uses Linq directly:</p>\n\n<pre><code>bool isFound = list1.Intersect(list2).Any();\n</code></pre>\n\n<p>The first one is long to write and not very straightforward/easy-to-read. The second one is short and clear, but performances will be low, especially on large lists.</p>\n\n<p>What may be an elegant way to do it?</p>\n"},{"tags":["sql","performance","oracle","optimizer","cbo"],"answer_count":1,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":86,"score":3,"question_id":12681123,"title":"Adding 'distinct' keyword to oracle query obliterates query performance for no reason","body":"<p>I am quite confused by something I'm seeing in an Oracle 10 database.</p>\n\n<p>I have the following query.</p>\n\n<pre><code>select\nt2.duplicate_num\nfrom table1 t1, table2 t2,\n\n( \n   select joincriteria_0 from intable1 it1, intable2 it2 \n   where it2.identifier in (4496486,5911382) \n   and it1.joincriteria_0 = it2.joincriteria_0 \n   and it1.filter_0 = 1 \n) tt\n\nwhere t1.joincriteria_0 = tt.joincriteria_0\nand t2.joincriteria_1 = t1.joincriteria_1\nand t2.filter_0 = 3\nand t2.filter_1 = 1\nand t2.filter_2 not in (48020)\n</code></pre>\n\n<p>It doesn't really seem like anything special to me, here are the baseline performance numbers from autotrace:</p>\n\n<p>CR_GETS: 318</p>\n\n<p>CPU: 3</p>\n\n<p>ROWS: 33173</p>\n\n<p>Now if I add the 'DISTINCT' keyword to the query (e.g. 'select distinct t2.duplicate_num...') this happens</p>\n\n<p>CR_GETS: 152921</p>\n\n<p>CPU: 205</p>\n\n<p>ROWS: 305</p>\n\n<p>The query plan has not changed, but the logical IO  grows by a factor of 500. I was expecting CPU only to go up and logical IO to be largely unchanged.</p>\n\n<p>The net result is a query that runs 10-100x slower with the distinct keyword. I can put code into the applciation which would make the result set distinct in a fraction of the time. How does this make any sense? particularly without the query plan changing?</p>\n"},{"tags":["python","database","django","performance","django-mptt"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":71,"score":1,"question_id":12661488,"title":"How optimize adding new nodes in `django-mptt`?","body":"<p>I am creating a script which will synchronize two databases. There is a data in the database which should be stored as a tree so I use <a href=\"http://django-mptt.github.com/django-mptt/\" rel=\"nofollow\">django-mptt</a> for the new DB. When I syncing DB's I select new data from the old DB and should save it in the new one.  </p>\n\n<p>I want to know if there is a better way to add new nodes into a tree? Now it looks next way:</p>\n\n<pre><code>...\n# Add new data to DB\nfor new_record in new_records:\n    # Find appropriate parent using data in 'new_record'\n    parent = get_parent(new_record)\n\n    # Create object which should be added using data in 'new_record'\n    new_node = MyMPTTModel(...)\n    new_node.insert_at(parent, save = True)\n    # Similar to:\n    # new_node.insert_at(parent, save = False)\n    # new_node.save()\n</code></pre>\n\n<p>But it works very slow. I think it works in a such way because after each call of the <code>insert_at(..., save = True)</code> method <code>django-mptt</code> should write new node to the DB and modify <code>left</code> and <code>right</code> keys for records which are already in the DB.  </p>\n\n<p>Is there any way to make <code>django-mptt</code> modify a query each time I call <code>insert_at</code> and then apply all changes together when I call <code>save</code>? Or do you know any other ways how to reduce execution time?</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["sql","sql-server","performance","tsql"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":58,"score":1,"question_id":12677941,"title":"Matching sub string in a column","body":"<p>First I apologize for the poor formatting here. </p>\n\n<p>Second I should say up front that changing the table schema is not an option.</p>\n\n<p>So I have a table defined as follows:</p>\n\n<p><strong>Pin</strong> varchar<br>\n<strong>OfferCode</strong> varchar</p>\n\n<p>Pin will contain data such as:<br>\nabc,<br>\nabc123</p>\n\n<p>OfferCode will contain data such as:<br>\n123<br>\n123~124~125</p>\n\n<p>I need a query to check for a count of a Pin/OfferCode combination and when I say OfferCode, I mean an individual item delimited by the tilde.</p>\n\n<p>For example if there is one row that looks like <code>abc, 123</code> and another that looks like <code>abc,123~124</code>, and I search for a count of <code>Pin=abc,OfferCode=123</code> I wand to get a count = 2.</p>\n\n<p>Obviously I can do a similar query to this:<br>\n<code>SELECT count(1) from MyTable (nolock) where OfferCode like '%' + @OfferCode + '%' and Pin = @Pin</code></p>\n\n<p>using <code>like</code> here is very expensive and I'm hoping there may be a more efficient way.</p>\n\n<p>I'm also looking into using a split string solution. I have a Table-valued function <code>SplitString(string,delim)</code> that will return table <code>OutParam</code>, but I'm not quite sure how to apply this to a table column vs a string. <em>Would this even be worth wile pursuing?</em> It seems like it would be much more expensive, but I'm unable to get a working solution to compare to the <code>like</code> solution.</p>\n"},{"tags":["c#","multithreading","performance","zip","zipfile"],"answer_count":4,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":97,"score":3,"question_id":12679165,"title":"multithreading for file load in C# 4.0","body":"<p>I need to zip each text file and copy into another server. File size may very from 500MB to 8GB. there is no dependency in each file. I have 35 files Appx.</p>\n\n<p>My regular code taking appx 3-4 hours for this. To reduce the time, I am just thinking to implement Threading for this. Do you thing Threading will reduce the time or is there any other best way to do this.</p>\n"},{"tags":["c#","performance","async-await"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":167,"score":2,"question_id":12673650,"title":"Real advantages of Async-Await?","body":"<p>Earlier i've posted <a href=\"http://stackoverflow.com/questions/12634551/difference-between-applying-async-await-at-client-and-at-service\">this</a> question related to applying Async-Await at client or at service. Do read the question before going ahead with this question as it is tightly coupled with the question.</p>\n\n<p>Based on the answer i've tested the code for C# 4.0 (TPL) and C# 5.0 (Async - Await). I am calling the service using async and sync version of the method provided by the service and comparing the number of threads used in each case.\nFollowing is the code i'm using to test the resources used:</p>\n\n<p><strong>MAIN METHOD</strong></p>\n\n<pre><code>List&lt;Task&lt;string&gt;&gt; tasksList = new List&lt;Task&lt;string&gt;&gt;();\nList&lt;int&gt; asyncThreads = new List&lt;int&gt;();\nList&lt;int&gt; tplThreads = new List&lt;int&gt;();\nStopwatch watch = new Stopwatch();\nwatch.Start();\n\n// Call the Async version of the method\nfor (int i = 0; i &lt; 500; i++)\n{\n    tasksList.Add(GetNameFromServiceAsync(\"Input\" + i.ToString(), asyncThreads));\n}\n\nTask.WaitAll(tasksList.ToArray());\n\nwatch.Stop();\n\nforeach (var item in asyncThreads.Distinct())\n{\n    Console.WriteLine(item);\n}\n\nConsole.WriteLine(\"(C# 5.0)Asynchrony Total Threads = \" + asyncThreads.Distinct().Count());\nConsole.WriteLine(watch.ElapsedMilliseconds.ToString());\n\nwatch.Restart();\n\ntasksList.Clear();\n\n// Call the normal method\nfor (int i = 0; i &lt; 500; i++)\n{\n    tasksList.Add(GetNameFromService(\"Input\" + i.ToString(), tplThreads));\n}\n\nTask.WaitAll(tasksList.ToArray());\n\nwatch.Stop();\n\nforeach (var item in tplThreads.Distinct())\n{\n    Console.WriteLine(item);\n}\n\nConsole.WriteLine(\"(C# 4.0)TPL Total Threads\" + tplThreads.Distinct().Count());\n\nConsole.WriteLine(watch.ElapsedMilliseconds.ToString());\n</code></pre>\n\n<p><strong>Async and Sync CAlls To the service</strong></p>\n\n<pre><code>static async Task&lt;string&gt; GetNameFromServiceAsync(string name, List&lt;int&gt; threads)\n{\n  Console.WriteLine(\" Start Current Thread : \" + System.Threading.Thread.CurrentThread.ManagedThreadId);\n    var task = await client.GetNameAsync(name);\n    threads.Add(System.Threading.Thread.CurrentThread.ManagedThreadId);\n   // Console.WriteLine(\"End GetNameFromServiceAsync Current Thread : \" + System.Threading.Thread.CurrentThread.ManagedThreadId);\n    return task;\n}\n\nstatic Task&lt;string&gt; GetNameFromService(string name, List&lt;int&gt; threads)\n{\n\n    var task = Task&lt;string&gt;.Factory.StartNew(() =&gt;\n        {\n            threads.Add(System.Threading.Thread.CurrentThread.ManagedThreadId);\n         //   Console.WriteLine(\"GetNameFromService Current Thread : \" + System.Threading.Thread.CurrentThread.ManagedThreadId);\n            return client.GetName(name);\n        });\n\n    return task;\n}\n</code></pre>\n\n<p>Now I've worked on the answer and find out following results:</p>\n\n<ul>\n<li>If i make 500 calls to the service it uses only 4-5 threads.</li>\n<li>The TPL calls makes around 44-45 threads.</li>\n<li>Time for Async calls is around 17 - 18 seconds</li>\n<li>Time for TPL calls is around 42 - 45 seconds.</li>\n</ul>\n\n<p>I want to have some feedback on my findings so that it can be useful to the other community members as well. Is it what the answer of my earlier question??</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>Q. My observation concludes that if we use Async-Await instead of TPL's Task.Factory.startNew , then it will consume lesser threads. HOW CORRECT IS THIS? IF NO, THEN WHAT IS THE RIGHT DIRECTION TO COME UP WITH SUCH COMPARISON?</p>\n\n<p>Q. As i am learning async - await, i want to prove its worth by some sort of comparison and solid code.</p>\n"},{"tags":["performance","matlab"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":94,"score":-1,"question_id":8027393,"title":"Can this matlab code be made more efficient/simpler?","body":"<p>I'm new to matlab, and coming from a background of C/C++, i'm not used to matlab's versatile options relating to matrices. The code I have written below uses a lot of loops and breaks, and I think that is why it is so slow. Could please suggest an alternative? I've explained a little bit about it below.\nAlso the numbers i'm dealing with go upto the range of 512*10^5, so is there a way to let matlab know that i'm dealing with such large numbers to speed up processing?</p>\n\n<p>I have a boolean sparse matrix( maxmatrix1, defined as a \"normal\" matrix ) of size 512x15525 and i'm mapping each one to the first ten ones in an anchor region defined by a submatrix of 30x75 and then generating a hash value( fingerprints ) of order 512*10^5, which is the index of another vector( offsets ). \"row\" and \"col\" are the dimensions of the \"maxmatrix1\".</p>\n\n<pre><code>while(x&lt;row)\n    y=1;\n    while(y&lt;col)\n        if(maxmatrix1(x,y)==1)\n            %Define Target Region for the Modelling\n            n=0;\n            for l=(x-15):(x+15)\n                if(l&gt;0 &amp;&amp; l&lt;=row)\n                    for k=(y+1):(y+76)\n                        if(k&lt;=col)\n                            if(maxmatrix1(l,k)==1)\n                                %Define Fingerprint :P\n\n                                if(n&gt;=10)\n                                    break;\n                                end\n\n                                f=f+1;\n                                fingerprint=x*10^5+l*10^2+(k-y);\n                                offsets(fingerprint)=y;\n                                n=n+1;\n                                count=count+1;\n                            end\n                        end\n                    end\n                end\n                if(n&gt;=10)\n                    break;\n                end\n            end\n        end\n        y=y+1;\n    end\n    x=x+1;\nend\n</code></pre>\n"},{"tags":["mysql","performance","group-by","sum"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":51,"score":0,"question_id":12611641,"title":"mysql query performance SUM, group by date","body":"<p>I have this query below in a mysql database.  The query is taking ~6 seconds to run on ~1-2 million rows.  I have indexs on date, uniqueuserid, collectiontype.</p>\n\n<p>How can I increase the performance of this query in mysql?</p>\n\n<pre><code>select \nDATE_FORMAT(MIN(collection.date),'%Y-%m-%d %H:00') as date, \nCOUNT(distinct collection.uniqueuserid) as convertingusers, \nSUM(case when collection.type = 1 then valueofitem end) as valueofitem, \nSUM(case when collection.type = 1 then 1 end) as numofitems, \nSUM(case when collection.type = 1 and collection.network = 1 then collection.valueofitem end) as col1valueofitem, \nSUM(case when collection.type = 1 and collection.network = 1 then 1 end) as col1numofitem, \nSUM(case when collection.type = 1 and collection.network = 2 then collection.valueofitem end) as col2valueofitem, \nSUM(case when collection.type = 1 and collection.network = 2 then 1 end) as col2numofitem, \nSUM(case when collection.type = 1 and collection.network = 3 then collection.valueofitem end) as col3valueofitem, \nSUM(case when collection.type = 1 and collection.network = 3 then 1 end) as col3numofitem, \nSUM(case when collection.type = 2 then collection.valueofitem end) as collectiontypeB, \nSUM(case when collection.type = 3 then collection.valueofitem end) as collectiontypeC,\nCOUNT(distinct collection.uniqueuserid) as convertingusers \n\nfrom collection\nwhere collection.date &gt; date_sub(now(),INTERVAL 3 WEEK) \ngroup by DATE_FORMAT(collection.date,'%Y-%m-%d') order by collection.date DESC limit 500\n</code></pre>\n\n<p>As requested, I've run an analysis with \"EXPLAIN\" before the query</p>\n\n<pre><code>id  select_type table   type    possible_keys   key key_len ref rows    Extra\n1   PRIMARY     ALL                 1196352 Using temporary; Using filesort\n2   DERIVED collection  ALL                 1196352 \n</code></pre>\n"},{"tags":["performance","cassandra","hector"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":67,"score":0,"question_id":12677191,"title":"Hector API SliceQuery versus ColumnQuery performance","body":"<p>I'm writing an application that uses Hector to access a Cassandra database. I have some situations where I only need to query one column, and some where I need to query multiple columns at once. Writing one method that takes an array of column names and returns a list of columns using SliceQuery would be simplest in terms of code, but I'm wondering whether there's a significant drawback to using SliceQuery for one column compared to using ColumnQuery.</p>\n\n<p>In short, are there enough (or any) performance benefits of using ColumnQuery over SliceQuery for one column to make it worth the extra code to deal with a one-column case separately?</p>\n"},{"tags":["performance","actionscript-3","hash","vector","spatial"],"answer_count":0,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":63,"score":2,"question_id":12678088,"title":"Improvement on hash function","body":"<p>It's about spatial hashing for 2d... (Excuse me if it's not spatial hashing I'm talking about)<br>\nWe translate x and y coord to an index in array representing buckets (groups).<br>\nAll papers/articles I've read, and myself use:<br>\n<code>int grid_cell = x/cell_size + y/cell_size*width;</code><br><br>\n<b>In AS3, would it be more efficient</b> to precalculate the indices for every x and y and storing the results in 2 vectors such as <i>xMap:Vector.&lt;int&gt; contains x/cell_size and yMap:Vector.&lt;int&gt; y/cell_size*width</i> then retrieve it using:<br>\n<code>int grid_cell = xMap[x] + yMap[y]; // grid being the array of buckets</code>\n<br><br><b>In short:</b> is retrieving an item from a vector faster than a calculation such as <i>y/cell_size*width</i> in AS3 ??<br>\nMy own bench-marking's results fluctuate a lot for me to decide and this is a performance critical function.</p>\n"},{"tags":["c++","performance","flush","stringstream","ostream"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":121,"score":4,"question_id":12675273,"title":"Why does using std::endl with ostringstream affect output speed?","body":"<p>I'm timing the difference between various ways to print text to standard output. I'm testing cout, printf, and ostringstream using both \\n and std::endl. I expected std::endl to make a difference with cout (and it did), but I didn't expect it to slow down output with ostringstream. I thought using std::endl would just right a \\n to the stream and it would still only get flushed once. What's going on here? Here's all my code:</p>\n\n<pre><code>// cout.cpp\n#include &lt;iostream&gt;\n\nusing namespace std;\n\nint main() {\n  for (int i = 0; i &lt; 10000000; i++) {\n    cout &lt;&lt; \"Hello World!\\n\";\n  }\n  return 0;\n}\n\n// printf.cpp\n#include &lt;stdio.h&gt;\n\nint main() {\n  for (int i = 0; i &lt; 10000000; i++) {\n    printf(\"Hello World!\\n\");\n  }\n  return 0;\n}\n\n// stream.cpp\n#include &lt;iostream&gt;\n#include &lt;sstream&gt;\n\nusing namespace std;\n\nint main () {\n  ostringstream ss;\n  for (int i = 0; i &lt; 10000000; i++) {\n    ss &lt;&lt; \"stream\" &lt;&lt; endl;\n  }\n  cout &lt;&lt; ss.str();\n}\n\n// streamn.cpp\n#include &lt;iostream&gt;\n#include &lt;sstream&gt;\n\nusing namespace std;\n\nint main () {\n  ostringstream ss;\n  for (int i = 0; i &lt; 10000000; i++) {\n    ss &lt;&lt; \"stream\\n\";\n  }\n  cout &lt;&lt; ss.str();\n}\n</code></pre>\n\n<p>And here's my Makefile</p>\n\n<pre><code>SHELL:=/bin/bash\n\nall: cout.cpp printf.cpp\n    g++ cout.cpp -o cout.out\n    g++ printf.cpp -o printf.out\n    g++ stream.cpp -o stream.out\n    g++ streamn.cpp -o streamn.out\ntime:\n    time ./cout.out &gt; output.txt\n    time ./printf.out &gt; output.txt\n    time ./stream.out &gt; output.txt\n    time ./streamn.out &gt; output.txt\n</code></pre>\n\n<p>Here's what I get when I run <code>make</code> followed by <code>make time</code></p>\n\n<pre><code>time ./cout.out &gt; output.txt\n\nreal    0m1.771s\nuser    0m0.616s\nsys 0m0.148s\ntime ./printf.out &gt; output.txt\n\nreal    0m2.411s\nuser    0m0.392s\nsys 0m0.172s\ntime ./stream.out &gt; output.txt\n\nreal    0m2.048s\nuser    0m0.632s\nsys 0m0.220s\ntime ./streamn.out &gt; output.txt\n\nreal    0m1.742s\nuser    0m0.404s\nsys 0m0.200s\n</code></pre>\n\n<p>These results are consistent.</p>\n"},{"tags":["php","performance","coding-style"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":63,"score":1,"question_id":12650702,"title":"Will there be a performance difference between different coding styles in php?","body":"<pre><code>&lt;?php \n\nsuppose 10 lines of php code is here;\n?&gt;\n</code></pre>\n\n<p>and </p>\n\n<pre><code>&lt;?php ?&gt;\n&lt;?php ?&gt;...10 times\n</code></pre>\n\n<p>same above script 10 times in separate tags.</p>\n\n<p>will this create any performance difference between two code if any please describe.   </p>\n"},{"tags":["c++","performance","pointers"],"answer_count":2,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":145,"score":5,"question_id":12655215,"title":"Dereferencing a char pointer gets slower as the string it points to lengthens. Why?","body":"<p>I've come across a strange problem: I have the following code:</p>\n\n<pre><code>int matches = 0;\nfor (int str_id = 0; str_id &lt; STR_COUNT; str_id++) {\n    if (test(strings1[str_id], strings2[str_id]) == 0)\n        matches++;\n}\n</code></pre>\n\n<p>It compares pairs of null-terminated strings using the <code>test()</code> function. <code>strings1</code> and <code>strings2</code> are vectors containing <code>STR_COUNT</code> null-terminated strings of the same length.</p>\n\n<p>Depending on whether <code>test()</code> dereferences its arguments, this snippet executes in constant or in linear time with regards to the length of the strings in <code>strings1</code> and <code>strings2</code>. That is, if i use:</p>\n\n<pre><code>int test(char* a, char* b) {\n    return (a != b)\n}\n</code></pre>\n\n<p>then the running time doesn't depend on the length of the strings stored in strings1 and strings2. On the other hand, if I use</p>\n\n<pre><code>int test(char* a, char* b) {\n    return (*a != *b)\n}\n</code></pre>\n\n<p>then the running time increases linearly with the length of the strings stored in <code>strings1</code> and <code>strings2</code>.</p>\n\n<p>Why could this happen?</p>\n\n<p><strong>EDIT</strong>: Full example of the problem here: <a href=\"http://pastebin.com/QTPAkP1g\" rel=\"nofollow\">http://pastebin.com/QTPAkP1g</a></p>\n"},{"tags":["mysql","performance","query","optimization","query-optimization"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":50,"score":2,"question_id":12677054,"title":"efficiently use OR statement in LEFT JOIN","body":"<p>so I have this query</p>\n\n<pre><code>SELECT * FROM b \n   LEFT JOIN n ON b.id = n.id \n   LEFT JOIN nr ON (b.pid &lt;&gt; 0 AND b.pid = nr.vid) \n        OR (b.pid = 0 AND n.vid = nr.vid)\n</code></pre>\n\n<p>But an EXPLAIN statement on that query would reveal that in order to execute the OR in the join, it needs to search through all the entries in the nr table even though nr.vid is the primary key of that table AND there are indices on b.pid and n.vid..</p>\n\n<p>Any suggestion as to how to make this more efficient? </p>\n\n<p><strong>Explain Result</strong></p>\n\n<pre><code>1, 'SIMPLE', 'b', 'ref', 'PRIMARY,r_rID', 'r_rID', '197', 'const,const', 48, 'Using where; Using temporary; Using filesort'\n1, 'SIMPLE', 'n', 'eq_ref', 'PRIMARY,nst,nnc,ut,uid', 'PRIMARY', '4', 'b.nid', 1, 'Using where'\n1, 'SIMPLE', 'nr', 'ALL', '', '', '', '', 335654, ''\n1, 'SIMPLE', 'tn', 'ref', 'nid', 'nid', '4', 'n.nid', 1, ''\n</code></pre>\n"},{"tags":["php","mysql","performance"],"answer_count":5,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":83,"score":2,"question_id":12675565,"title":"MySQL name and surname in 2 columns vs name in 1 column","body":"<p>One simple question and I couldn't find any answers to id :</p>\n\n<p>Should name be in 2 different DB columns ( name / surname ) or in 1 column ( name + surname ) ? </p>\n\n<p>In all the projects I had they were in 2 different columns, but now I have to start a new project and I was wantering how it better to store it. I mean, the 2 different columns gave me a bit of trouble and sometimes slowed performance down. Please note this very important thing :</p>\n\n<ul>\n<li>A very important part of the public part of the site will be an advanced search and it WILL search for the full name in about 200k records.</li>\n</ul>\n\n<p>So, what do you suggest ? 2 columns or 1 ? I am inclined twords the 1 column solution because I cannot find any advantages in using 2, but maybe I am wrong ?</p>\n\n<p>EDIT :</p>\n\n<p>Thank you for the answers. The only reason for this question was for the performance issue, I need all the extra boost I can get.</p>\n"},{"tags":["performance","windows-phone-7","cpu"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":41,"score":0,"question_id":12669856,"title":"WP7 , High CPU Utilization","body":"<p>I am building a WP7 application.\nI noticed high cpu utilization using the Performance Monitoring tool.</p>\n\n<p>Even a simple hello world application gives High CPU utilization.\nThis happens in the UI Thread.</p>\n\n<p>How do we get the application to use less then 50 % of the CPU ?</p>\n\n<p><img src=\"http://i.stack.imgur.com/AxK7J.png\" alt=\"enter image description here\"></p>\n\n<p>This image is that of the hello world application. We can see the the graph is easily above 50%.\nIs that an area of concern ?</p>\n"},{"tags":["java","performance","duration"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":94,"score":2,"question_id":12658419,"title":"How long will this take to run and how can I speed it up","body":"<p>I'm running some code to test Brownian Motion and divergence, I was curious how long this code will take to run as well as any ways to speed up the process.  I am relatively new to java, so the code at the present time is relatively basic. The arguments that I am running are 1000000 1000000.</p>\n\n<pre><code>public class BrownianMotion {\n\n    public static void main(String[] args) {\n\n    /**starts vars for program*/\n\n    int N = Integer.parseInt(args[0]);\n    int T = Integer.parseInt(args[1]);\n    double sqtotal = 0;\n    double r;\n    double avg;\n\n    /**number of trials loop*/\n\n    for (int count=0;count&lt;T;count++) {\n\n        /**started here so that x &amp; y reset at each trial*/\n\n        int x = 0;\n        int y = 0;\n\n        /**loop for steps*/\n        for (int steps=0;steps&lt;N;steps++) {\n\n        r = Math.random();\n        if      (r &lt; 0.25) x--;\n        else if (r &lt; 0.50) x++;\n        else if (r &lt; 0.75) y--;\n        else if (r &lt; 1.00) y++;\n\n        }\n        /**squared total distance after each trial*/\n        sqtotal = sqtotal + (x*x+y*y);\n\n    }\n\n    /**average of squared total*/\n\n    avg = sqtotal/T;\n    System.out.println(avg);\n\n    }\n}\n</code></pre>\n\n<p>Thanks in advance for the help.</p>\n"},{"tags":["c++","performance","class","struct","unions"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":215,"score":0,"question_id":4741628,"title":"Most performant way to inherit structs in C++?","body":"<p>Let's say I have the following:</p>\n\n<pre><code>#pragma pack(push,1)\n\nstruct HDR {\n   unsigned short msgType;\n   unsigned short msgLen;\n};\n\nstruct Msg1 {\n   unsigned short msgType;\n   unsigned short msgLen;\n   char text[20];\n};\n\nstruct Msg2 {\n   unsigned short msgType;\n   unsigned short msgLen;\n   uint32_t c1;\n   uint32_t c2;\n};\n\n .\n .\n .\n</code></pre>\n\n<p>I want to be able to reuse the HDR struct so I don't have to keep defining the two members: msgType and msgLen.  I don't want to involve vtables for performance reasons but I do want to override operator&lt;&lt; for each of the structs.  Based on this last requirement, I don't see how I could possibly use a union since the sizes are also different.</p>\n\n<p>Any ideas on how this can best be handled for pure performance</p>\n"},{"tags":["performance","opengl","triangle","triangle-count"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":61,"score":0,"question_id":12665922,"title":"Fragment or geometry shaders: Count rendered triangles?","body":"<p><code>gl_PrimitiveID</code> seems to count the number of primitives rendered. This seems to be available via both fragment and geometry shaders.</p>\n\n<p>Would this be an accurate count of rendered triangles in the view frustum (assuming only tris, since the primitive ID also applies to lines and points) as opposed to the whole scene?</p>\n"},{"tags":["c#",".net","performance","caching","asp.net-web-api"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":98,"score":0,"question_id":12646828,"title":"Performance Issues when accessing HttpRequestMessage Properties collection in WebAPI","body":"<p>Using ASP.NET MVC4 I have created a DelegatingHandler in a WebAPI project. I use the Handler to insert some data into the HttpRequestMessage Properties collection so that later portions of the Http pipeline (e.g. other handlers, controllers) can access that data.</p>\n\n<p>Example:</p>\n\n<pre><code>var httpRequest = (HttpRequestMessageProperty) request.Properties[\"httpRequest\"];\nhttpRequest.Headers[\"MY_DATA\"] = \"some data\";\n</code></pre>\n\n<p>I wanted to test the impact of this on the throughput of WebAPI request processing so I created a console app that, within an Action delegate, instantiates an HttpClient, sends a request, and waits for the response before sending the next one. I count the number of successful executions of this Action delegate within a 5 second window using the performance measurement strategy described here:\n<a href=\"http://www.codeproject.com/Articles/61964/Performance-Tests-Precise-Run-Time-Measurements-wi\" rel=\"nofollow\">http://www.codeproject.com/Articles/61964/Performance-Tests-Precise-Run-Time-Measurements-wi</a></p>\n\n<p>What I have observed is that there is over a magnitude loss in performance in executing the single line that retrieves the httpRequest from the Properties collection.  Without this line of code I see >700 completions within 5 seconds.  When I add that line in, it drops to &lt;20 completions within 5 seconds.</p>\n\n<p>Has anyone else experienced this?  Is there a different method I should use to cache data for the length of a single request that will be available throughout the Http pipeline?</p>\n"},{"tags":["performance","joomla","latency"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":111,"score":2,"question_id":12425483,"title":"Joomla html slow page load time (not js, images, css)","body":"<p>The load time for the <strong>base html</strong> of any page on my <strong>Joomla</strong> website (removed) is ~2seconds. </p>\n\n<ul>\n<li>There aren't an excessive number of modules (even the pages with virtually no content take ~2seconds to load: (removed)</li>\n<li>There aren't too many articles (or anything else that would make any db queries overly strenuous)</li>\n<li>Loading a tiny file takes &lt;10ms (robots.txt or images)</li>\n<li>The file sizes of the base html pages are in the 30-40kb range</li>\n</ul>\n\n<p>What is causing the slowdown in loading the initial html page? How can I figure out what is causing the slowdown (and fix it)?</p>\n"},{"tags":["jquery","performance","jquery-selectors","code-efficiency"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":144,"score":0,"question_id":12674591,"title":"Inefficient jQuery usage warnings in PHPStorm IDE","body":"<p>I recently upgraded my version of PHPStorm IDE and it now warns me about inefficient jQuery usage.</p>\n\n<p>For example:</p>\n\n<pre><code>var property_single_location = $(\"#property [data-role='content'] .container\");\n</code></pre>\n\n<p>Prompts this warning:</p>\n\n<blockquote>\n  <p>Checks that jQuery selectors are used in an efficient way. It suggests\n  to split descendant selectors which are prefaced with ID selector and\n  warns about duplicated selectors which could be cached.</p>\n</blockquote>\n\n<p>So my question is:</p>\n\n<blockquote>\n  <p>Why is this inefficient and what is the efficient way to do the above selector?</p>\n</blockquote>\n\n<p>I'd guess at:</p>\n\n<pre><code>var property_single_location = $(\"#property\").find(\"[data-role='content']\").find(\".container\");\n</code></pre>\n\n<p>Is this the right way?</p>\n"},{"tags":[".net","performance","reference","comparison"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":40,"score":1,"question_id":12675096,"title":"ID compare vs. Reference compare","body":"<p>I have some Entity Framework objects which are also identified by their own ID.</p>\n\n<p>I prefer to use reference comparison which is probably more reliable, especially for objects not on the DB yet.</p>\n\n<p>Which is the gap between <em>Int32 ID comparison</em> and <em>ByRef object comparison</em> in terms of performance?</p>\n"},{"tags":["c++","performance","header-files"],"answer_count":6,"favorite_count":8,"up_vote_count":13,"down_vote_count":0,"view_count":20711,"score":13,"question_id":453372,"title":"Writing function definition in header files in C++","body":"<p>I have a class which has many small functons. By small functions, I mean functions that doesn't do any processing but just return a literal value. Something like</p>\n\n<pre><code>string Foo::method() const{\n    return \"A\";\n}\n</code></pre>\n\n<p>I have created a header file \"Foo.h\" and source file \"Foo.cpp\". But since the function is very small, I am thinking about putting it in the header file itself. I have the following questions</p>\n\n<ol>\n<li>Is there any performance or other issues if I put these function defnition in header file? I will have many functions like this.</li>\n<li>My understanding is when the compilation is done, compiler will expand the header file and place it where it is included. Is that correct?</li>\n</ol>\n"},{"tags":["c#","performance","user-profile"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":67,"score":0,"question_id":12671853,"title":"Get user profile size in Windows without calculations using C#?","body":"<p>I need a way how to fetch a user profile size without a long and detailed calculation (sum of file sizes). </p>\n\n<p>Is there a way to get a cached or prefetched value in Windows like in the <em>Control Panel\\User Accounts\\User Accounts\\User Profiles</em> screen?</p>\n\n<p>Thanks for any suggestions, solutions or hints (preferably based on C#, WMI or something like that).</p>\n"},{"tags":["jquery","html","css","performance","infinite-scroll"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":45,"score":1,"question_id":12671641,"title":"perfomance infinite scroll on pc browser","body":"<p>I would use infinite scroll for display in page a potential huge list of results.\nFor example, if a timeline user in facebook has 400 000 posts how infinite scroll in facebook work for display all posts in pages? If it load on scrolling all posts a browser get down performance or crash?</p>\n\n<p>What is the best method to display huge data in infinite scroll without kill browser performance?</p>\n"},{"tags":["asp.net","database","performance","iis"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":45,"score":1,"question_id":12668408,"title":"Website's performance issue","body":"<p>I have an Asp.Net website with one web page whose sole purpose is receiving data in the form of a query string then separate it as required and store this data to the database. </p>\n\n<p>This data comes from several Vehicle Tracking Systems. Each vehicle sends a string of data as query string every 30 seconds.</p>\n\n<p>I have written the code in my webpage in such a way that as the webpage is accessed, in the page load, I read the query string and do the insert operation into the database. Something like this-</p>\n\n<pre><code>protected void Page_Load(object sender, EventArgs e)\n{\n    con.Open();\n    string input = Request.QueryString[\"vinput\"];\n    var m = Regex.Match(input, @\"~(.+)@(.+)@(.+)@(.+)@(.+)@(.+)@(.+)@(.+)@(.+)@(.+)@(.+)@(.+)@(.+)@(.+)@(.+)@(.+)@(.+)@(.+)@(.+)@(.+)@\");\n    if (m.Success)\n    {\n        string[] vals = new string[20];\n        int j = 1;\n        for (int i = 0; i &lt; 20; i++)\n        {\n            vals[i] = m.Groups[j].Value;\n            j++;\n        }\n        cmd.CommandText = \"insert into tracking (vehicle_no,hardware_id,lat,lng,speed,direction,an0,an1,an2,an3,di0,di1,di2,di3,do0,do1,do2,do3,tdate,ttime) values('\" + vals[0] + \"','\" + vals[1] + \"','\" + vals[2] + \"','\" + vals[3] + \"','\" + vals[4] + \"','\" + vals[5] + \"','\" + vals[6] + \"','\" + vals[7] + \"','\" + vals[8] + \"','\" + vals[9] + \"','\" + vals[10] + \"','\" + vals[11] + \"','\" + vals[12] + \"','\" + vals[13] + \"','\" + vals[14] + \"','\" + vals[15] + \"','\" + vals[16] + \"','\" + vals[17] + \"','\" + vals[18] + \"','\" + vals[19] + \"')\";\n        cmd.Connection = con;\n        cmd.ExecuteNonQuery();\n    }\n}\n</code></pre>\n\n<p>I created a simulator app for testing this site in place of vehicle tracking systems. This site works fine when there is data coming from one to three simulator instances, but fails to save data to database when more than 3 simulator instances send data simultaneously(Approximately). </p>\n\n<p>For example, I am sending 5 records, each one at a time from the simulator and 6 such simulator instances sending data to the page. At the end in the database I see only 15 records inserted instead of 30 records. By the way this website runs in <strong>IIS 5.1</strong>.</p>\n\n<p>How do I deal with this issue? Suggestions please.</p>\n\n<hr>\n\n<p><strong>UPDATE</strong>: Finally found the issue. After a lot of googling found this <a href=\"http://forums.iis.net/t/1164347.aspx\" rel=\"nofollow\">link</a> and <a href=\"http://www.carehart.org/blog/client/index.cfm/2011/2/2/iis_request_execution_limits\" rel=\"nofollow\">this</a>. Its with the server, since the request limit is only 3 for basic/starter in windows xp IIS. </p>\n"},{"tags":["performance","flash","actionscript-3","flex","actionscript"],"answer_count":8,"favorite_count":3,"up_vote_count":9,"down_vote_count":0,"view_count":21012,"score":9,"question_id":1010859,"title":"For VS Foreach on Array performance (in AS3/Flex)","body":"<p>Which one is faster? Why?</p>\n\n<pre><code>var messages:Array = [.....]\n\n// 1 - for\nvar len:int = messages.length;\nfor (var i:int = 0; i &lt; len; i++) {\n    var o:Object = messages[i];\n    // ...\n}\n\n// 2 - foreach\nfor each (var o:Object in messages) {\n    // ...\n}\n</code></pre>\n"},{"tags":["performance","internet-explorer-8","primefaces","datatables"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":69,"score":0,"question_id":12670742,"title":"is there any solution for primefaces selectable data table issue on IE 8","body":"<p>currently i'm having big performance issue with prime faces data-table with multiple selection on IE,it has been reported as bug in version 3.4 ,but i tried to downgrade to 3.3 still having the same issue.IE 8 chosen by the customer as main browser,i'm forced find a solution? is there any alternative way it could solve this </p>\n"},{"tags":["java","performance","profiling"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":48,"score":0,"question_id":12669761,"title":"Measure/profile execution time of a method from a third party library","body":"<p>I am interested in this question and have tried a couple of things.</p>\n\n<p>Generally one can use products such as <a href=\"http://www.ej-technologies.com/products/jprofiler/overview.html\" rel=\"nofollow\">JProfiler</a> or <a href=\"http://visualvm.java.net\" rel=\"nofollow\">VisualVM</a> to perform a profiling of CPU and search for the methods that under investigation.</p>\n\n<p>Also, one may use libraries such as <a href=\"http://jetm.void.fm/\" rel=\"nofollow\">JETM</a>, <a href=\"http://metrics.codahale.com/\" rel=\"nofollow\">CodaHale Metrics</a>, or <a href=\"http://github.com/Netflix/servo\" rel=\"nofollow\">Netflix Servo</a> to introduce measuring points in the application. However, the issue with this approach is that for third-party libraries it may not work because the libraries expect you to introduce measuring points inside the application. This is the most interesting approach for me since they are minimal in the amount of code they introduce.</p>\n\n<p>Last but not least, one may also use approached based on AOP such as <a href=\"http://static.springsource.org/spring/docs/current/spring-framework-reference/html/aop.html\" rel=\"nofollow\">Spring AOP</a> to measure specific executions of third party libraries. Since the project is not using AOP at all, I'd rather not introduce new dependencies for the only purpose of this measurement.</p>\n\n<p>In my case, for instance, I would like to measure the execution of logging methods such as the mixture of <a href=\"http://commons.apache.org/logging/commons-logging-1.1.1/apidocs/org/apache/commons/logging/Log.html\" rel=\"nofollow\">JCL Log</a> and <a href=\"http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/Logger.html\" rel=\"nofollow\">log4j Logger</a> implementation. I would like to use a <a href=\"http://junit.sourceforge.net/javadoc/org/junit/runner/notification/RunListener.html\" rel=\"nofollow\">JUnit RunListener</a> with Maven Surefire and measure this execution for all the unit tests that I have. </p>\n\n<p>So, any ideas? I'd appreciate suggestions.</p>\n"},{"tags":["c++","c","performance","assembly","rdtsc"],"answer_count":1,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":227,"score":4,"question_id":12631856,"title":"Difference between rdtscp, rdtsc : memory and cpuid / rdtsc?","body":"<p>Assume we're trying to use the tsc for performance monitoring and we we want to prevent instruction reordering.</p>\n\n<p>These are our options: </p>\n\n<p><strong>1:</strong> <code>rdtscp</code> is a serializing call. It prevents reordering around the call to rdtscp.</p>\n\n<pre><code>__asm__ __volatile__(\"rdtscp; \"         // serializing read of tsc\n                     \"shl $32,%%rdx; \"  // shift higher 32 bits stored in rdx up\n                     \"or %%rdx,%%rax\"   // and or onto rax\n                     : \"=a\"(tsc)        // output to tsc variable\n                     :\n                     : \"%rcx\", \"%rdx\"); // rcx and rdx are clobbered\n</code></pre>\n\n<p>However, <code>rdtscp</code> is only available on newer CPUs. So in this case we have to use <code>rdtsc</code>. But <code>rdtsc</code> is non-serializing, so using it alone will not prevent the CPU from reordering it.</p>\n\n<p>So we can use either of these two options to prevent reordering:</p>\n\n<p><strong>2:</strong> This is a call to <code>cpuid</code> and then <code>rdtsc</code>. <code>cpuid</code> is a serializing call.</p>\n\n<pre><code>volatile int dont_remove __attribute__((unused)); // volatile to stop optimizing\nunsigned tmp;\n__cpuid(0, tmp, tmp, tmp, tmp);                   // cpuid is a serialising call\ndont_remove = tmp;                                // prevent optimizing out cpuid\n\n__asm__ __volatile__(\"rdtsc; \"          // read of tsc\n                     \"shl $32,%%rdx; \"  // shift higher 32 bits stored in rdx up\n                     \"or %%rdx,%%rax\"   // and or onto rax\n                     : \"=a\"(tsc)        // output to tsc\n                     :\n                     : \"%rcx\", \"%rdx\"); // rcx and rdx are clobbered\n</code></pre>\n\n<p><strong>3:</strong> This is a call to <code>rdtsc</code> with <code>memory</code> in the clobber list, which prevents reordering</p>\n\n<pre><code>__asm__ __volatile__(\"rdtsc; \"          // read of tsc\n                     \"shl $32,%%rdx; \"  // shift higher 32 bits stored in rdx up\n                     \"or %%rdx,%%rax\"   // and or onto rax\n                     : \"=a\"(tsc)        // output to tsc\n                     :\n                     : \"%rcx\", \"%rdx\", \"memory\"); // rcx and rdx are clobbered\n                                                  // memory to prevent reordering\n</code></pre>\n\n<p>My understanding for the 3rd option is as follows:</p>\n\n<p>Making the call <code>__volatile__</code> prevents the optimizer from removing the asm or moving it across any instructions that could need the results (or change the inputs) of the asm. However it could still move it with respect to unrelated operations. So <code>__volatile__</code> is not enough.</p>\n\n<p>Tell the compiler memory is being clobbered: <code>: \"memory\")</code>. The <code>\"memory\"</code> clobber means that GCC cannot make any assumptions about memory contents remaining the same across the asm, and thus will not reorder around it.</p>\n\n<p>So my questions are:</p>\n\n<ul>\n<li>1: Is my understanding of <code>__volatile__</code> and <code>\"memory\"</code> correct?</li>\n<li>2: Do the second two calls do the same thing?</li>\n<li>3: Using <code>\"memory\"</code> looks much simpler than using another serializing instruction. Why would anyone use the 3rd option over the 2nd option?</li>\n</ul>\n"},{"tags":["iphone","html","performance","ipad","sticky"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":79,"score":2,"question_id":12637910,"title":"my site is very laggy on ipad/iphone, removed css-effects & javascript","body":"<p>I'm building a site with quite a bit of CSS &amp; some javascript but I am having a lot of problems with the performance on mobile devices, especially on the ipad and the iphone.</p>\n\n<p>I've tried turning off the box-shadow, text-shadow, radius and gradient properties in css as well as turning off all the javascript but it's still lagging severely. Turning off the javascript doesn't improve the performance much, though turning off those css properties did help a bit.</p>\n\n<p>More specifically, when you try scrolling and let go with your finger the scrolling \"sticks\" on the spot where you let go. Zooming in and scrolling around is pretty bad on smaller screens as well, especially on the iphone or ipod.</p>\n\n<p>It does feel like there are things I am overlooking. Any clues?</p>\n\n<p><em>[I removed the site, but my answer shows the underlying problem in more detail. The sluggish behavior comes from the property overflow:scroll]</em></p>\n"},{"tags":["javascript","performance","html5","web-sql"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":38,"score":1,"question_id":12635661,"title":"WebSQLDatabase after inserting rows not responding for a few seconds","body":"<p>The situation here is that I have a web sql database with 8 tables and per table an 2 indexes.\nOnce I start the web application I will load (ajax call) all data into the tables, for about 200 rows in each table.</p>\n\n<p>After inserting, I do a select on one of the tables (select * from cars) but the problem is that the select query takes like 4 seconds in chrome and 30 seconds on my htc desire android phone (did some logging before the execute and in the success callback).</p>\n\n<p>The second time it only takes a few milliseconds to execute the select query, also on the other tables.</p>\n\n<p>What could it be, seems like the database isn't ready after those initial inserts and does some magic stuff to setup or cache or write the database to disk or something? </p>\n\n<p>Any ideas?</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>Found the solution.. just insert all items in one transaction! Beginners mistake I think ;)</p>\n"},{"tags":["mysql","performance","group-by"],"answer_count":5,"favorite_count":2,"up_vote_count":8,"down_vote_count":0,"view_count":226,"score":8,"question_id":12530127,"title":"get many grouped values from mysql","body":"<p>I have a table structure like this:</p>\n\n<pre><code>CREATE TABLE `test` (\n  `a` tinyint(3) unsigned DEFAULT 0,\n  `b` tinyint(3) unsigned DEFAULT 0,\n   `c` tinyint(3) unsigned DEFAULT 0,\n  `d` tinyint(3) unsigned DEFAULT 0,\n  `e` tinyint(3) unsigned DEFAULT 0\n  );\n</code></pre>\n\n<p>This has about 30 columns with some columns that have values from 0-200 (a,b) and some only have 5 values (0,1,2,3,4) (column c-d). There are aprox. 120k rows in the table.</p>\n\n<p>To show the number of items per row I use a query for each column:</p>\n\n<pre><code>select a, count(*) FROM test group by a;\nselect b, count(*) FROM test group by b;\nselect c, count(*) FROM test group by c;\nselect d, count(*) FROM test group by d;\nselect e, count(*) FROM test group by e;\n</code></pre>\n\n<p>The problem with this is that it will fire 30 queries (one per column) and basically goes over the same set of data each time.</p>\n\n<p>Is there a better way to do this?</p>\n\n<p>I have tried with GROUP BY WITH ROLLUP but this results in a massive resultset which is slower to process than each individual query.</p>\n\n<p>You can view a selection of the data on SQLfiddle: <a href=\"http://sqlfiddle.com/#!2/a9fd8/1\">http://sqlfiddle.com/#!2/a9fd8/1</a></p>\n"},{"tags":["c#","performance","unsafe","safe"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":1880,"score":2,"question_id":5374815,"title":"True Unsafe Code Performance","body":"<p>Good morning, afternoon or night,</p>\n\n<p>I understand unsafe code is more appropriate to access things like the Windows API and do unsafe type castings than to write more performant code, but I would like to ask you if you have ever noticed any significant performance improvement in real-world applications by using it when compared to safe c# code.</p>\n\n<p>Thank you very much.</p>\n"},{"tags":["asp.net","sql","performance","linq-to-sql"],"answer_count":4,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":123,"score":4,"question_id":12542587,"title":"Efficient way to load lists of objects from database to instantiate a single object","body":"<p><strong>My situation</strong></p>\n\n<p>I have a c# object which contains some lists. One of these lists are for example a list of tags, which is a list of c# \"SystemTag\"-objects. <strong>I want to instantiate this object the most efficient way</strong>.</p>\n\n<p>In my database structure, I have the following tables:</p>\n\n<ul>\n<li>dbObject - the table which contains some basic information about my c# object</li>\n<li>dbTags - a list of all available tabs</li>\n<li>dbTagConnections - a list which has 2 fields: TagID and ObjectID (to make sure an object can have several tags)</li>\n</ul>\n\n<p><em>(I have several other similar types of data)</em></p>\n\n<p><strong>This is how I do it now...</strong></p>\n\n<ol>\n<li>Retrieve my object from the DB using an ID</li>\n<li>Send the DB object to a \"Object factory\" pattern, which then realise we have to get the tags (and other lists). Then it sends a call to the DAL layer using the ID of our C# object</li>\n<li>The DAL layer retrieves the data from the DB</li>\n<li>These data are send to a \"TagFactory\" pattern which converts to tags</li>\n<li>We are back to the Object Factory</li>\n</ol>\n\n<p>This is really inefficient and we have many calls to the database. This especially gives problems as I have 4+ types of lists. </p>\n\n<p><strong>What have I tried?</strong></p>\n\n<p>I am not really good at SQL, but I've tried the following query:</p>\n\n<pre><code>SELECT * FROM dbObject p\nLEFT JOIN dbTagConnection c on p.Id=  c.PointId\nLEFT JOIN dbTags t on c.TagId = t.dbTagId\nWHERE ....\n</code></pre>\n\n<p>However, this retreives as many objects as there are tagconnections - so I don't see joins as a good way to do this.</p>\n\n<p><strong>Other info...</strong></p>\n\n<ul>\n<li>Using .NET Framework 4.0</li>\n<li>Using LINQ to SQL (BLL and DAL layer with Factory patterns in the BLL to convert from DAL objects)</li>\n</ul>\n\n<p>...</p>\n\n<p><strong>So - how do I solve this as efficient as possible? :-) Thanks!</strong></p>\n"},{"tags":["python","performance","compression","zip","extraction"],"answer_count":2,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":991,"score":4,"question_id":1759736,"title":"What Is The Best Python Zip Module To Handle Large Files?","body":"<p>EDIT: Specifically compression and extraction speeds.</p>\n\n<p>Any Suggestions?</p>\n\n<p>Thanks</p>\n"},{"tags":["android","performance","arraylist","sharedpreferences","android-performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":75,"score":0,"question_id":12661373,"title":"saving, loading expandable listview checkbox states to shared preferences - there must be a better way","body":"<p>basically, i have an expandable listview with checkboxes that i want their states saved to shared preferences. The method that i'm implementing works, but i have a feeling that it is not all that it could be. I wouldn't be surprised to see it crash. Basically with my method, I load and save the states in my activity class with serialization.</p>\n\n<pre><code>SharedPreferences settings3 = getSharedPreferences(\"MYPREFS\", 0);    \n        try {\n            check_states = (ArrayList&lt;ArrayList&lt;Integer&gt;&gt;) ObjectSerializer.deserialize(settings3.getString(\"CBSTATES\", ObjectSerializer.serialize(new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;())));\n        } catch (IOException e) {\n            e.printStackTrace();\n        } catch (ClassNotFoundException e) {\n            e.printStackTrace();\n        }\n</code></pre>\n\n<p>some more code in between.</p>\n\n<pre><code>protected void onStop() {\n            SharedPreferences settings3 = getSharedPreferences(\"MYPREFS\", 0);\n            SharedPreferences.Editor editor3 = settings3.edit();\n            try {\n                editor3.putString(\"CBSTATES\", ObjectSerializer.serialize(check_states));\n            } catch (IOException e) {\n                e.printStackTrace();\n            }\n            editor3.commit();\n        }\n</code></pre>\n\n<p>To get the arraylist from my actual expandable adapter, I have it instantiated as a static variable and transfered. in my espandablelistadpater class --> </p>\n\n<pre><code>//  set checkbox states\n    static ArrayList&lt;ArrayList&lt;Integer&gt;&gt; check_states = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;();\n    public void setChildrenAndValues() {\n        //initialize the states to all 0;\n        for(int i = 0; i &lt; children.length; i++) {\n            ArrayList&lt;Integer&gt; tmp = new ArrayList&lt;Integer&gt;();\n            for(int j = 0; j &lt; children[i].length; j++) {\n                tmp.add(0);\n            }\n            check_states.add(tmp);\n        }\n    }\n</code></pre>\n\n<p>And here is the getChildView </p>\n\n<pre><code>public View getChildView(final int groupPosition, final int childPosition, boolean isLastChild,\n            View convertView, ViewGroup parent) {\n        LayoutInflater childInflate = (LayoutInflater) context.getSystemService(Context.LAYOUT_INFLATER_SERVICE);\n        View childView = childInflate.inflate(R.layout.mtopics_childview, parent, false);\n\n        TextView childtxt = (TextView)childView.findViewById(R.id.mtopicschildtv);\n        childtxt.setText(getChild(groupPosition, childPosition).toString());\n\n        //      Load the checkbox states        \n        setChildrenAndValues();\n\n        final CheckBox childcb = (CheckBox)childView.findViewById(R.id.mtopicchildchkbox);\n\n        if (check_states.get(groupPosition).get(childPosition) == 1) {\n            childcb.setChecked(true);\n        }else{ childcb.setChecked(false);\n        }\n\n        childcb.setOnClickListener(new OnClickListener() {  \n            public void onClick(View v) {\n                if (childcb.isChecked()) {\n                    check_states.get(groupPosition).set(childPosition, 1);\n                }else{ check_states.get(groupPosition).set(childPosition, 0);\n                }\n            }\n        });\n\n        return childView;\n</code></pre>\n\n<p>I just wanted to know if i was doing everything alright, or if i was missing a step cause I can actually feel the slow-down, especially when deserializing. Perhaps i should do this in another thread (whatever that means). Am i using static variables in the correct way cause i'm altering them in both directions. maybe i'm missing an if null statement.</p>\n"},{"tags":["performance","amazon-ec2","scipy","time-complexity","lapack"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":67,"score":0,"question_id":12660052,"title":"Time complexity of scipy.linalg.solve (LAPACK gesv) on large matrix?","body":"<p>If I use <a href=\"http://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve.html\" rel=\"nofollow\"><code>scipy.linalg.solve</code></a> (which I believe calls LAPACK's gesv function) on a ~12000 unknown problem (with a ~12000-square, dense, non-symmetrical matrix) on my workstation, I get a good answer in <strong>10-15 minutes</strong>.</p>\n\n<p>Just to probe the limits of what's possible (note I don't say \"useful\"), I doubled the resolution of my underlying problem, which leads to needing to solve for ~50000 unknowns.  While this would technically run on my workstation once I'd added some more 10s of GBytes of swap, it seemed more prudent to use some HW with adequate RAM, and so I kicked it off on an AWS EC2 High-Memory Quadruple Extra Large... where it has been grinding away for the last <strong>14 hours</strong> (hey, spot instances are cheap) and it's impossible to tell how far through it is.</p>\n\n<p>Unfortunately, I've no idea what the time complexity of the solvers involved is (my google-fu failed me on this one).  If it's O(N^2) then I'd have expected it to be done after around 4 hours; if it's O(N^3) then maybe it'll be done in 16 hours.  Of course that's interpreting N as the number of unknowns - which has quadrupled - the number of elements in the matrix has increased by a factor of 16!</p>\n\n<p>And advice which will help me determine whether this has any chance of completing in my (project's) lifetime or not gratefully received!</p>\n\n<p>Other info:</p>\n\n<p>Sparse matrices aren't of interest here (my matrix is dense, and in any case scipy's don't work with more than <code>2**31</code> non-zero elements even on 64-bit).</p>\n\n<p>I'm using Debian/Squeeze's scipy on the workstation, Ubuntu 12.04 on EC2.  Both 64-bit obviously.</p>\n"},{"tags":["performance","uitableview","uitableviewcell"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":24,"score":0,"question_id":12664206,"title":"Table View loading performance issue","body":"<p>I have a UITableView with seven rows each containing text and a .png image. When I add the code to set the image for each row and run the app in the simulator I experience a long loading time.</p>\n\n<p>I suspect that the cause of the performance issue is the fact that my image sizes are too big and I am in the process of scaling my images to the appropriate pixel dimensions. However, I wanted to ensure there is nothing that I could be doing differently in my code to further optimize performance. Is there anything I should be doing differently in my code? </p>\n\n<pre><code>    -(UITableViewCell *)tableView:(UITableView *)tableView cellForRowAtIndexPath:(NSIndexPath *)indexPath\n{\n    UITableViewCell *c = [tableView dequeueReusableCellWithIdentifier:@\"Cell\"];\n\n    if (!c)\n    {\n        c = [[UITableViewCell alloc] initWithStyle:UITableViewCellStyleDefault\n                                   reuseIdentifier:@\"Cell\"];\n    }\n//adds text to table view\n    [[c textLabel] setText:[categoryNames objectAtIndex:[indexPath row]]];\n    [[c textLabel] setTextColor:[UIColor whiteColor]];\n\n    //add pictures to table view\n\n    NSString *path = [[NSBundle mainBundle] pathForResource:[categoryURLs objectAtIndex:[indexPath row]] ofType:@\"png\"];\n    [[c imageView] setImage:[UIImage imageWithContentsOfFile:path]];\n\n    return c;\n\n\n}\n</code></pre>\n"},{"tags":["performance","scala"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":88,"score":2,"question_id":12654700,"title":"fast copying object content in scala","body":"<p>I have a class with few Int and Double fields. What is the fastes way to copy all data from one object to another?</p>\n\n<pre><code>class IntFields {\n  private val data : Array[Int] = Array(0,0)\n\n  def first : Int = data(0)\n  def first_= (value: Int) = data(0) = value\n  def second : Int = data(1)\n  def second_= (value : Int) = data(1) = value\n\n  def copyFrom(another : IntFields) =\n    Array.copy(another.data,0,data,0,2)\n}\n</code></pre>\n\n<p>This is the way I may suggest. But I doubt it is really effective, since I have no clear understanding scala's internals </p>\n\n<h2>update1:</h2>\n\n<p>In fact I'm searching for scala's equivalent of c++ memcpy. I need just take one simple object and copy it contents byte by byte.</p>\n\n<p>Array copying is just a hack, I've googled for normal scala supported method and find none.</p>\n\n<h2>update2:</h2>\n\n<p>I've tried to microbenchmark two holders: simple case class with 12 variables and one backed up with array. In all benchmarks (simple copying and complex calculations over collection) array-based solution works slower for about 7%.</p>\n\n<p>So, I need other means for simulating memcpy.</p>\n"},{"tags":["javascript","performance","svg","cross-browser","raphael"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":73,"score":0,"question_id":12665007,"title":"Raphael SVG animations choppy in some browsers","body":"<p>I have an SVG object that I am adding to my page using Raphael. With roughly 175 paths in 6 sets I suspect it counts as complex.</p>\n\n<p>I am animating each of the 6 sets using the .animate function</p>\n\n<pre><code>function anim(direction, duration){\n    return Raphael.animation({transform: \"r\"+direction+\"360 250 250\"}, duration).repeat(\"Infinity\");\n}\nIG.sets.white_outer.animate(anim(\"\",100000)); // 2 paths \nIG.sets.grey_outer.animate(anim(\"-\",100000)); // 25 paths\nIG.sets.grey_inner.animate(anim(\"\",100000));  // 25 paths\n\n$(window).load(function(){                      \n    IG.sets.BLUE.animate({transform: \"r0 250 250\"}, 4000, \"&gt;\");  // 32 paths\n    IG.sets.RED.animate({transform: \"r0 250 250\"}, 3000, \"&gt;\");   // 29 paths\n    IG.sets.GREEN.animate({transform: \"r0 250 250\"}, 2000, \"&gt;\"); // 24 paths\n}\n</code></pre>\n\n<p>the problem is that in some browsers, this is very choppy looking.</p>\n\n<p>It's smooth as butter on Mac (tested: FF, Chrome, Safari). It's also lovely on Windows in Chrome, but when I load up windows FF, Safari or IE8 the animations are choppy and a bit stuttering.</p>\n\n<p>It even looks great on a an iPad!</p>\n\n<p>I'd love to be able to make them all look great... so I'm trying to figure out what's costing all the power in FF and Safari (and at the end of the day, hopefully whatever override I manage brings up a fix for IE8 as well).</p>\n\n<p>From the other questions I've seen about choppy animation on Raphael, I see mention of \"getting into the guts\" to add a timeout to the animate function... (as seen in raphael-min.js)</p>\n\n<pre><code>cz= window.requestAnimationFrame||\n    window.webkitRequestAnimationFrame||\n    window.mozRequestAnimationFrame||\n    window.oRequestAnimationFrame||\n    window.msRequestAnimationFrame||\n    function(a){setTimeout(a,16)}, /* there is a function here that animates */\n</code></pre>\n\n<p>Though it looks like in FF for example RequestAnimationFrame gives a smoother performance than setTimeout. (I tested this by deleting the mozRequestAnimationFrame condition and reloading, tried various intervals for the timeout)</p>\n\n<p>Is there something else I might be missing that might help improve the cross browser frame rates of my animations? </p>\n\n<p>Note for example image is at its default 500x500 and the rotation is happening around the image center (point 250x250) however the Image is being displayed on page at 1000x1000 (with viewport doing the scaling at page load.</p>\n\n<p>Is it possible that if I have the designer resize the image to a 1000x1000 canvas and I display at full size from the start that I'll have a boost? Maybe a \"smoother ride\"? I'm not sure what factors effect SVG animation and performance.</p>\n"},{"tags":["php","performance","standards","coding-style","constants"],"answer_count":6,"favorite_count":1,"up_vote_count":6,"down_vote_count":1,"view_count":4793,"score":5,"question_id":247936,"title":"PHP Constants: Advantages/Disadvantages","body":"<p>Lately I've been in the habit of assigning integer values to constants and simply using the constant name as a means of identifying its purpose. However, in some cases this has resulted in the need to write a function like typeToString($const) when a string representation is needed. Obviously this is inefficient and unneccesary, but is only an issue every once and a while.</p>\n\n<p>So my question is, are there any other tradeoffs I should consider? Which case is considered to be cleaner/more standards-compliant? Also, is the performance difference negligable for most cases?</p>\n\n<p><strong>Case 1: (faster when a string version is not needed?)</strong></p>\n\n<pre><code>class Foo {\n    const USER_TYPE_ADMIN = 0;\n    const USER_TYPE_USER = 1;\n    const USER_TYPE_GUEST = 2;\n\n    public $userType = self::USER_TYPE_ADMIN;\n\n    public function __construct($type) {\n    \t$this-&gt;userType = $type;\n    }\n\n    public function typeToString() {\n    \tswitch($this-&gt;userType) {\n    \t\tcase self::USER_TYPE_ADMIN:\n    \t\t\treturn 'admin';\n    \t\t\tbreak;\n\n    \t\tcase self::USER_TYPE_USER:\n    \t\t\treturn 'user';\n    \t\t\tbreak;\n\n    \t\tcase self::USER_TYPE_GUEST:\n    \t\t\treturn 'guest';\n    \t\t\tbreak;\n\n    \t\tdefault:\n    \t\t\treturn 'unknown';\n    \t\t\tbreak;\n    \t}\n    }\n}\n\n$foo = new Foo(Foo::USER_TYPE_GUEST);\necho $foo-&gt;typeToString();\n// Displays \"guest\"\n</code></pre>\n\n<p><strong>Case 2:(faster/easier when a string version is needed)</strong></p>\n\n<pre><code>class Foo {\n    const USER_TYPE_ADMIN = 'admin';\n    const USER_TYPE_USER = 'user';\n    const USER_TYPE_GUEST = 'guest';\n\n    public $userType = self::USER_TYPE_ADMIN;\n\n    public function __construct($type) {\n    \t$this-&gt;userType = $type;\n    }\n}\n\n$foo = new Foo(Foo::USER_TYPE_GUEST);\necho $foo-&gt;userType();\n// Displays \"guest\"\n</code></pre>\n"},{"tags":["java","performance","list","scala"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":645,"score":1,"question_id":6578615,"title":"How to use scala.collection.immutable.List in a Java code","body":"<p>I need to write a code that compares performance of Java's <code>ArrayList</code> with Scala's <code>List</code>. I am having a hard time getting the Scala <code>List</code> working in my Java code. Can some one post a real simple \"hello world\" example of how to create a Scala <code>List</code> in java code (in a <code>.java</code> file) and add say 100 random numbers to it?</p>\n\n<p>PS: I am quite good at Java but have never used Scala.</p>\n"},{"tags":["performance","query","mongodb"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":24,"score":0,"question_id":12616281,"title":"How to Quickly Update Mongo Documents String Fields with Complex Functions","body":"<p>What is the fastest way to update documents in a Mongo database with complex functions, let's say a string search / replace or a <code>sqrt</code> calculation?</p>\n\n<p>Since such operations are missing, e.g. a <code>$replace</code>, it is not possible with <code>update</code> (which would probably be the fastest, since on my test collection it only takes about 50 ms to set a field on some 100k objects).</p>\n\n<p>When I simply iterate over all documents it takes about 45 seconds. It gets a little faster when I limit my query to the fields I'm using during the update.</p>\n\n<p>This time of course grow larger on larger collections, therefore the question whether there is a faster way than iterating over the collection (e.g. via a map reduce job?).</p>\n"},{"tags":["asp.net","sql","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":30,"score":0,"question_id":12663804,"title":"Sync'ing SQL Server with a Web Service (both managed by different vendors)","body":"<p>I had an interesting solution that I did get solved, but I wanted to know if there was a better way to solve it. </p>\n\n<p>I have a piece of software that represents our core business process. This software (Application \"A\") is also managed by company A. I have admin access to the SQL server for the web application, but I don't have access to the compiled source code. </p>\n\n<p>I have a second piece of software is managed by company B. Application B has integration hooks represented as web services so other applications can integrate with it. Other than that I don't have any real access to Application. It may be possible to get SQL Server Admin access to Application B, but at this time I don't have it.  </p>\n\n<p>My question is what is the best way to link these two systems so the data is as \"near real time\" as possible. I developed a CLR Trigger embedded in the SQL Server for Application A, but as all of you know it killed the performance of the application. So I ended up making another table on the SQL server which on a simple trigger wrote the necessary data related to the transaction along with a status column. I then setup a scheduled exe which read the data from the table, and fed it into Application B. It also updated the status column accordingly so I didn't run into threading problems as this executable runs on a high tight frequency. </p>\n\n<p>The gap with my current solution is about 5min when everything is and done. That may not seem bad, but our end users are expecting real time so my support staff is still getting calls as a result of this solution. </p>\n\n<p>Anyone got a better solution? I'm all ears and hoping to learn a bit in the process. </p>\n"},{"tags":["performance","3d","three.js"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":94,"score":1,"question_id":12659461,"title":"Rendering a large number of colored particles using three.js and the canvas renderer","body":"<p>I am trying to use the Three.js library to display a large number of colored points on the screen (about  half a million to million for example). I am trying to use the Canvas renderer rather than the WebGL renderer if possible (The web pages would also be displayed in the Google Earth Client bubbles, which seems to work with Canvas renderer but not the WebGL renderer.)</p>\n\n<p>While I have the problem solved for a small number of points (tens of thousands) by modifying the code from <a href=\"http://mrdoob.github.com/three.js/examples/canvas_particles_random.html\" rel=\"nofollow\"><strong>here</strong></a>, I am having trouble scaling it beyond that. </p>\n\n<p>But in the the following code using WebGL and the Particle System I can render half a million random points, but without colors.</p>\n\n<pre><code>  ...\nvar particles = new THREE.Geometry();\nvar pMaterial = new THREE.ParticleBasicMaterial({\n                    color: 0xFFFFFF,\n                    size: 1,\n                    sizeAttenuation : false\n                    });\n\n// now create the individual particles\nfor (var p = 0; p &lt; particleCount; p++) {\n     // create a particle with randon position values,\n     // -250 -&gt; 250\n     var pX = Math.random() * POSITION_RANGE - (POSITION_RANGE / 2),\n     pY = Math.random() * POSITION_RANGE - (POSITION_RANGE / 2),\n     pZ = Math.random() * POSITION_RANGE - (POSITION_RANGE / 2),\n     particle = new THREE.Vertex(\n                        new THREE.Vector3(pX, pY, pZ)\n                        );\n\n     // add it to the geometry\n     particles.vertices.push(particle);\n    }\n\n    var particleSystem = new THREE.ParticleSystem(\n                            particles, pMaterial);\n    scene.add(particleSystem);\n  ...\n</code></pre>\n\n<p>Is the reason for the better performance of the above code due to the Particle System? From what I have read in the documentation it seems the Particle System can only be used by the WebGL renderer. </p>\n\n<p>So my question(s) are</p>\n\n<p>a) Can I render such large number of particles using the Canvas renderer or is it always going to be slower than the WebGL/ParticleSystem version? If so, how do I go about doing that? What objects and or tricks do I use to improve performance?</p>\n\n<p>b) Is there a compromise I can reach if I give up some features? In other words, can I still use the Canvas renderer for the large dataset if I give up the need to color the individual points?</p>\n\n<p>c) If I have to give up the Canvas and use the WebGL version, is it possible to change the colors of the individual points? It seems the color is set by the material passed to the ParticleSystem and that sets the color for all the points.</p>\n"}]}
{"total":25593,"page":17,"pagesize":100,"questions":[{"tags":["javascript","performance","html5","coffeescript","benchmarking"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":894,"score":1,"question_id":9047276,"title":"is coffeescript faster than javascript?","body":"<p>Javascript is everywhere and to my mind is constantly gaining importance. Most programmers would agree that while Javascript itself is ugly, its \"territory\" sure is impressive. With the capabilities of HTML5 and the speed of modern browsers deploying an application via Javascript is an interesting option: It's probably as cross-platform as you can get.</p>\n\n<p>The natural result are cross compilers. The predominant is probably GWT but there are several other options out there. My favourite is Coffeescript since it adds only a thin layer over Javascript and is much more \"lightweight\" than for example GWT.</p>\n\n<p>There's just one thing that has been bugging me: Although my project is rather small performance has always been an important topic. Here's a quote</p>\n\n<blockquote>\n  <p>The GWT SDK provides a set of core Java APIs and Widgets. These allow\n  you to write AJAX applications in Java and then compile the source to\n  highly optimized JavaScript</p>\n</blockquote>\n\n<p>Is Coffeescript optimized, too? Since Coffeescript seems to make heavy use of non-common Javascript functionality I'm worried how their performance compares.</p>\n\n<p>Have you experience with Coffeescript related speed issues ?\nDo you know a good benchmark comparison ?</p>\n"},{"tags":["performance","sprite","andengine","processing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":39,"score":0,"question_id":12025902,"title":"Sprites move faster on scene on a device with higher processing capacity :- andengine","body":"<p>I am developing a game in which sprites move randomly on the screen. They move on paths defined by PathModifiers and their travel time is 7 seconds. Now the problem is that when i install my game in my phone which has a processing speed of 800MHz, it seems to work fine but when i install it on a device which has processing speed of 1GHz , it seems so much faster. Is this behaviour alright or i have something going wrong.</p>\n\n<p>thanks</p>\n"},{"tags":["css","performance","css3","font-face","webfonts"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":63,"score":1,"question_id":12551049,"title":"CSS Mediaqueries: Defining font-face inside a certain min to max range - is this font loaded outside?","body":"<p>Probably a weird question and after your answers I might be ashamed for asking this. </p>\n\n<p>I have a specific font embedded on my website (via <code>@font-face</code>) this font is used for a section that is only visible on wider resolutions (desktops). On Smartphones for example, this section is not visible (<code>display:none</code>).</p>\n\n<p>The <code>@font-face</code> rule is not defined within a media-query but right at the beginning of my stylesheet.</p>\n\n<p>I wonder now if it would be possible to avoid loading this embedded font-file if I'm viewing the site on a mobile device. </p>\n\n<p>You know. The font-file has a view kb and I want my site to be as fast as possible. Since the font wouldn't even be needed on my mobile version I wonder if </p>\n\n<p>1.) the font is even loaded at the moment? I have no idea how to test this on my iPhone. Since the section where it is used is set to <code>display:none</code> I don't get any feedback.</p>\n\n<p>2.) If it is loaded (and I guess so) would it be possible to set this <code>@font-face</code> declaration inside a <code>media-query</code> with <code>max-width</code> : <code>640px</code> (e.g. iPhone) and the files wouldn't be loaded in this case?</p>\n\n<p>Any ideas on that matter?</p>\n\n<p>thank you in advance.</p>\n"},{"tags":["performance","counters","papi"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":122,"score":0,"question_id":8160004,"title":"How to use PAPI periodically for performance measurements","body":"<p>I want to analyze system's performance for my application using PAPI api in C. The general structure is that<br>\n -- Initialize PAPI<br>\n -- Initialize counters of interest<br>\n -- start counters<br>\n         -- run main logic of the application<br>\n  -- end counters and read values</p>\n\n<p>I want to read the counters periodically say every 1 second instead of reading the final values at the end of the application. does the PAPI output give the aggregate values at end of program execution like the total number of L2 cache misses after the program execution. Another example would be to read number of instructions at every time instance rather than total number of instructions at the end of the program.</p>\n"},{"tags":["performance","html5","video"],"answer_count":2,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":1273,"score":6,"question_id":4511744,"title":"HTML5 video performance","body":"<p>For some weeks now, I have been developing an HTML5 based website and experiencing some major performance problems. These problems surely depend on the demands of the design, but since the coding still is very slim and uses only a few media types, I wonder if I overlooked something that causes these problems.</p>\n\n<p>Before you say it – I know that a Flash website could easily do the same with vastly better performance. I try to get a similar functionality/design with only HTML(5), which should be possible by now.</p>\n\n<p>I'm talking about a website with a full-screen video background (HTML5 ) and the main content on a wide, black-backgrounded middle column. The content can contain other HTML5 videos in YouTube/Vimeo-style size. This is where the CPU panics half way, and I work on a recent Mac Pro…</p>\n\n<p>I set the video (of any size from 480i to 1080i) to scale with the window's width &amp; height. I also set it to 1080i and 100% of size. I also added JavaScript to pause the background whenever one of the smaller project videos was in focus. All variations caused an unacceptable slowness in either browser or site situation (another video played).</p>\n\n<p>I was told <a href=\"http://studio.victorcoulon.fr/javascript/background_video/\">this example</a> would have the best performance.\nBut I duplicated the data rate and resolution without any benefit.</p>\n\n<p>Who is an expert in web video performance?</p>\n"},{"tags":["javascript","performance","arguments"],"answer_count":2,"favorite_count":4,"up_vote_count":10,"down_vote_count":0,"view_count":90,"score":10,"question_id":12662497,"title":"Performance penalty for undefined arguments","body":"<p>I quite often have optional arguments in functions, but some testing is showing a huge performance hit for them in firefox and safari (70-95%). Strangely, if I pass in the literal value <em>undefined</em> then there is no penalty. What could be happening here? I wouldn't have thought that it was a scope chain issue as they are inherently local to the function. Am I to start passing <em>undefined</em> into every optional argument?</p>\n\n<p>jsPerf: <a href=\"http://jsperf.com/function-undefined-args/2\">http://jsperf.com/function-undefined-args/2</a></p>\n"},{"tags":["javascript","performance","caching","scope"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":49,"score":2,"question_id":12662118,"title":"Is \"this\" cached locally?","body":"<p>It's <a href=\"http://www.slideshare.net/madrobby/extreme-javascript-performance\" rel=\"nofollow\">recommended</a> to cache globals locally for better performance like so:</p>\n\n<pre><code>function showWindowSize() {\n    var w = window;\n    var width = w.innerWidth;\n    var height = w.innerHeight;\n    alert(\"width: \" + width + \" height: \" + height);\n}\n</code></pre>\n\n<p>Is the same true when using the \"this\" keyword, or is it cached already?</p>\n\n<p>Example:</p>\n\n<pre><code>Game.prototype.runGameLoop = function() {\n  var self = this;\n  self.update();\n  self.draw();\n};\n</code></pre>\n"},{"tags":["performance","character-encoding","google-pagespeed"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":68,"score":0,"question_id":12661919,"title":"Google Page Speed says avoid meta tag for char set, but I haven't used a meta tag for that","body":"<p><strong>Using Google Page Speed</strong>, I get a message, \"<em>Avoid a character set in the meta tag</em>,\" with a link to a <a href=\"https://developers.google.com/speed/docs/best-practices/rendering#SpecifyCharsetEarly\" rel=\"nofollow\">page</a> which tells me all about why I shouldn't use a meta tag to define the character set.</p>\n\n<p>But my pages (WordPress) don't specify the character set in the meta tag.  Instead I've done it by placing this at the top of the theme's <code>header.php</code>:</p>\n\n<pre><code>&lt;?php\n    header('Content-Type: text/html; charset=utf-8');\n?&gt;\n</code></pre>\n\n<p>My questions are: </p>\n\n<ol>\n<li>is my method of specifying the character set correct ?</li>\n<li>if it is, why is Google's PageSpeed tool complaining about a non-existent meta tag specification ?</li>\n</ol>\n"},{"tags":["mysql","performance","select","connector-net"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":79,"score":0,"question_id":10211550,"title":"Acceptable mysql select speed as per documentation?","body":"<p>Is there an acceptable speed as in this many records per second for MySQL <code>SELECT</code>? I know it depends on how complex the query is and my machine spec. But can I have a vague/approximate speed estimation of a standard <code>SELECT</code> query? May be for something like this:</p>\n\n<pre><code>SELECT a, b, c, d, e, f FROM my_table;\n</code></pre>\n\n<p>I use .NET connector to access MySQL; from my code I do something like this:</p>\n\n<pre><code>MySqlCommand cmd = new MySqlCommand(query, _conn);\nMySqlDataReader r = cmd.ExecuteReader();\n\nList&lt;int&gt; lst = new List&lt;int&gt;();\nwhile (r.Read())\n{\n    lst.Add(.....\n}\n\nr.Close();\n</code></pre>\n\n<p>Currently I can <code>SELECT</code> 25000 records under 150 ms. But when I run it under <code>phpmyadmin</code> it takes about 75 ms. From MySQL console it neeed less than 50 ms. Is there a <code>need not be worried about</code> limit that documentation recommends be it via connector, console or anything? I am running on an Intel Core2 Duo (2 GHz) with 2 Gb RAM. Speed is critical for my need.</p>\n\n<p>I remember reading one such somewhere..</p>\n"},{"tags":["performance","google-apps-script"],"answer_count":5,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":2195,"score":1,"question_id":6882104,"title":"Faster way to find the first empty row - Google Apps Script","body":"<p>I've made a script that every few hours adds a new row to a Google Apps spreadsheet.\nThis is the function I've made to find the first empty row:</p>\n\n<pre><code>function getFirstEmptyRow() {\n  var spr = SpreadsheetApp.getActiveSpreadsheet();\n  var cell = spr.getRange('a1');\n  var ct = 0;\n  while ( cell.offset(ct, 0).getValue() != \"\" ) {\n    ct++;\n  }\n  return (ct);\n}\n</code></pre>\n\n<p>It works fine, but when reaching about 100 rows, it gets really slow, even ten seconds.\nI'm worried that when reaching thousands of rows, it will be too slow, maybe going in timeout or worse.\nIs there a better way?</p>\n"},{"tags":["performance","algorithm","language-agnostic","big-o"],"answer_count":5,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":331,"score":5,"question_id":12165795,"title":"Random integers in array. Find the greatest sum of a continuous subset","body":"<p>I had an interview question a while back that I never got a solution for. Apparently there is a \"very efficient\" algorithm to solve it. </p>\n\n<p>The question: Given an array of random positive and negative numbers, find the continuous subset that has the greatest sum. </p>\n\n<p>Example:</p>\n\n<p><code>[1, -7, 4, 5, -1, 5]</code></p>\n\n<p>The best subset here is <code>{4, 5, -1, 5}</code></p>\n\n<p>I can think of no solution but the brute-force method. What is the efficient method?  </p>\n"},{"tags":["c#","performance","rest"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":68,"score":1,"question_id":12640092,"title":"improve perfomance of a REST Service","body":"<p>I have method which calls a stored procedure 300 times in a for loop and each time the stored procedure returns me 1200 records. How can i improve this ? I cannot eliminate the 300 calls but is there any otherways i can try out. I am using REST service impletemented through ASP.NET and using IBATIS for database connectivity</p>\n"},{"tags":["c#",".net","performance","httpwebrequest","basichttpbinding"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":145,"score":1,"question_id":12329639,"title":"BasicHttpBinding, ServiceReference from WSDL - performance issues","body":"<p>I am currently experiencing some difficulties diagnosing some BasicHttpBinding/ServiceReference performance issues.  In some instances, when using a <code>Stopwatch</code> to time a synchronous method call to the service, it is measured as 2.5seconds.  I have done both Wireshark (pcap) and Fiddler (INET web proxy) analysis of this same call and get back a 400ms response time which is not far from best case scenario.  </p>\n\n<p>So where does this 2 second discrepancy come from?</p>\n\n<p>I am using the following <code>BasicHttpBinding</code>:</p>\n\n<pre><code>BasicHttpBinding httpBinding = new BasicHttpBinding\n    {\n        SendTimeout = TimeSpan.FromSeconds(_settings.SendTimeout),\n        ReceiveTimeout = TimeSpan.FromSeconds(_settings.SendTimeout),\n        MaxReceivedMessageSize = 1024 * 1024 * 100,\n        Security =\n        {\n            Mode = BasicHttpSecurityMode.TransportCredentialOnly,\n            Message = { ClientCredentialType = BasicHttpMessageCredentialType.UserName },\n            Transport = { ClientCredentialType = HttpClientCredentialType.Basic }\n        }\n    };\n</code></pre>\n\n<p>We are using these values:</p>\n\n<pre><code>ServicePointManager.Expect100Continue = false;\nServicePointManager.DefaultConnectionLimit = 20;\nServicePointManager.MaxServicePointIdleTime = 10000;\n</code></pre>\n\n<p>as well as compressed HttpWebRequests:</p>\n\n<pre><code>public class CompressibleHttpRequestCreator : IWebRequestCreate\n{\n    WebRequest IWebRequestCreate.Create(Uri uri)\n    {\n        HttpWebRequest httpWebRequest = Activator.CreateInstance(typeof(HttpWebRequest),\n            BindingFlags.CreateInstance | BindingFlags.Public | BindingFlags.NonPublic | BindingFlags.Instance,\n            null,\n            new object[] { uri, null },\n            null) as HttpWebRequest;\n\n        if (httpWebRequest != null)\n        {\n            httpWebRequest.AutomaticDecompression = DecompressionMethods.GZip |\n                                                    DecompressionMethods.Deflate;\n        }\n\n        return httpWebRequest;\n    }\n}\n</code></pre>\n\n<p>I'm at a loss as to where this addition 2 seconds could come in - I'm am theorising when I suggest the request or response is getting queued somewhere which would delay the request but not sure where this could be?</p>\n\n<p>The requests are UI/User driven and we typically only have 1-5 requests going on concurrently so <code>ServicePointManager.DefaultConnectionLimit</code> of 20 should be plenty.  This is occurring with and without Fiddler so I've eliminated any proxies reusing connections etc - plus it couldn't take 2 seconds to establish a new server connection, could it?</p>\n\n<p><strong>Update:</strong></p>\n\n<p>After some more diagnostics, I have found some interesting timings when outputting ServiceModel tracing times.</p>\n\n<p><img src=\"http://i.stack.imgur.com/MM2zk.jpg\" alt=\"ServiceModel times\"></p>\n\n<p>The timings, from top highlight to bottom are of these TraceIdentifiers:</p>\n\n<ol>\n<li><a href=\"http://msdn.microsoft.com/en-GB/library/System.ServiceModel.MessageWritten.aspx\" rel=\"nofollow\">http://msdn.microsoft.com/en-GB/library/System.ServiceModel.MessageWritten.aspx</a> (A message was written)</li>\n<li><a href=\"http://msdn.microsoft.com/en-GB/library/System.ServiceModel.Channels.MessageSent.aspx\" rel=\"nofollow\">http://msdn.microsoft.com/en-GB/library/System.ServiceModel.Channels.MessageSent.aspx</a> (Sent a message over a channel)</li>\n<li><a href=\"http://msdn.microsoft.com/en-GB/library/System.ServiceModel.Channels.HttpResponseReceived.aspx\" rel=\"nofollow\">http://msdn.microsoft.com/en-GB/library/System.ServiceModel.Channels.HttpResponseReceived.aspx</a> (HTTP response was received)</li>\n</ol>\n\n<p>I cannot explain why the duration between 1 and 2 could take 2+ seconds.  Any ideas?  Is it at all possible that the thread is being descheduled or something?  </p>\n"},{"tags":["c#","performance","concatenation"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":1,"view_count":2186,"score":0,"question_id":6311358,"title":"Efficient way to combine multiple text files","body":"<p>I have multiple files of text that I need to read and combine into one file.\nThe files are of varying size: 1 - 50 MB each.\nWhat's the most efficient way to combine these files without bumping into the dreading System.OutofMemory exception?</p>\n"},{"tags":["ios","performance","rotation","drawing"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":12660556,"title":"Quartz drawing slows on landscape orientation","body":"<p>Quartz drawing is getting slow when you rotate your device. Below, there is a test code that i have used. You can add this view to your auto-rotated application. It generates random points and draws lines between them. After three seconds, it logs FPS and screen refresh count. I tried this for both landscape and portrait orientations, FPS results are not even close. It is extremely slow on landscape mode.</p>\n\n<pre><code>#import \"DrawingView.h\"\n#import &lt;QuartzCore/QuartzCore.h&gt;\n\n@implementation DrawingView\n\n- (id)initWithFrame:(CGRect)frame {\n    self = [super initWithFrame:frame];\n\n    if (self) \n        points = [[NSMutableArray alloc] init];        \n    return self;\n}\n\n- (id) initWithCoder:(NSCoder *)aDecoder {\n    self = [super initWithCoder:aDecoder];\n    if (self) \n        points = [[NSMutableArray alloc] init];\n    return self;\n}\n\n- (void) testDraw {\n    NSTimeInterval start = [NSDate timeIntervalSinceReferenceDate];\n    NSTimeInterval diff = 0.0;\n    int drawCount = 0;\n    while(diff &lt; 3) {\n        long randX = arc4random() % 500;\n        long randY = arc4random() % 500;\n\n        [points addObject:[NSValue valueWithCGPoint:CGPointMake(randX, randY)]];\n\n        [self performSelectorOnMainThread:@selector(setNeedsDisplay) \n                               withObject:nil waitUntilDone:YES];\n        drawCount++;\n\n        diff = [NSDate timeIntervalSinceReferenceDate] - start;\n    }\n\n    NSLog(@\" Screen refresh count is %d \", drawCount);\n    NSLog(@\" FPS is %f\", drawCount / diff);\n    [points removeAllObjects];\n}\n\n- (void) touchesBegan:(NSSet *)touches withEvent:(UIEvent *)event {\n    [self performSelectorInBackground:@selector(testDraw) withObject:nil];\n}\n\n- (CGPoint) centerOfLine:(CGPoint)from to:(CGPoint)to; {\n    return CGPointMake(to.x - ((to.x - from.x) / 2.0), to.y - ((to.y - from.y) / 2.0));\n}\n\n-(void) addPoint:(CGPoint)pPoint {\n    [points addObject:[NSValue valueWithCGPoint:pPoint]];\n}\n\n- (void) drawNormalForContext:(CGContextRef) context {\n    CGContextRef pContext = context;\n\n    if(points.count &gt; 0) {\n        CGPoint firstPoint = [(NSValue*) [points objectAtIndex:0] CGPointValue];\n        CGContextMoveToPoint(pContext, firstPoint.x, firstPoint.y);\n\n        for (int i=1; i &lt; [points count]; i++) {\n            CGPoint to = [(NSValue*) [points objectAtIndex:i] CGPointValue];\n            CGContextAddLineToPoint(pContext, to.x, to.y);\n        }                       \n\n        CGContextStrokePath(pContext);\n    }\n}\n\n\n- (void) drawRect:(CGRect)rect{            \n    [self drawNormalForContext:UIGraphicsGetCurrentContext()];\n}\n@end\n</code></pre>\n"},{"tags":["javascript","html","performance","checkbox","img"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":63,"score":0,"question_id":12659762,"title":"image instead of checkbox work slow","body":"<p>I switched my checkbox with images, but now when I hit one of the checkboxes it takes it 2 second before the checked img appear.</p>\n\n<p>That happened only at the first time when I hit the checkbox, at second etc. it works perfect.</p>\n\n<p>Probably after you hit the checkbox once the img is loaded to the website memory, and then it works smoothly.</p>\n\n<p>Is there is a solution for that?\nmaybe when the page loaded it can load any optional img for its memory or something like this.</p>\n\n<p>Thanks in advance.</p>\n\n<p>This is my <a href=\"http://doarna.com/%D7%93%D7%95%D7%90%D7%A8-%D7%A0%D7%A2/\" rel=\"nofollow\">website</a>, it's in Hebrew but the content doesn't matter.</p>\n\n<p>Im using Gravity Form, but for this case it doesnt matter.\nsample from the code is below and thanks to every body.</p>\n\n<pre><code>.DesignCheckBox label {\n                   background-repeat: no-repeat;\n                   border: 2px solid #DDD;\n                           height: 150px!important;\n                           font-size: 150%;\n                           -moz-border-radius: 20px;\n                           -webkit-border-radius: 20px;\n                border-radius: 20px; \n                -khtml-border-radius: 20px; \n                                                  }\n.DesignCheckBox label:hover {\n                   border: 2px solid #000095;\n                           background-color:  #F0F7F7;\n                           cursor: hand; cursor: pointer;\n                                                  }\n\n.HideCheckBox input[type=\"checkbox\"]{display:none!important;}\n\n         li.gchoice_4_1 label {\n                       background-image: url(http://doarna.com/wp-content/uploads/2012/09/XCellcom.gif);\n                               }\n\n         li.gchoice_4_1 input[type=\"checkbox\"]:checked + label { \n                       background-image: url(http://doarna.com/wp-content/uploads/2012/09/VCellcom.gif);\n                       }\n\n         li.gchoice_4_2 label {\n                       background-image: url(http://doarna.com/wp-content/uploads/2012/09/XOrange.gif);\n                               }\n\n         li.gchoice_4_2 input[type=\"checkbox\"]:checked + label { \n                       background-image: url(http://doarna.com/wp-content/uploads/2012/09/VOrange.gif); \n                       }\n</code></pre>\n"},{"tags":["performance","haskell","numbers","primes"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":128,"score":2,"question_id":12291877,"title":"List of divisors of an integer n (Haskell)","body":"<p>I currently have the following function to get the divisors of an integer:</p>\n\n<pre><code>-- All divisors of a number\ndivisors :: Integer -&gt; [Integer]\ndivisors 1 = [1]\ndivisors n = firstHalf ++ secondHalf\n    where firstHalf = filter (divides n) (candidates n)\n          secondHalf = filter (\\d -&gt; n `div` d /= d) (map (n `div`) (reverse firstHalf))\n          candidates n = takeWhile (\\d -&gt; d * d &lt;= n) [1..n] \n</code></pre>\n\n<p>I ended up adding the <code>filter</code> to <code>secondHalf</code> because a divisor was repeating when <code>n</code> is a square of a prime number. This seems like a very inefficient way to solve this problem.</p>\n\n<p>So I have two questions: How do I measure if this really is a bottle neck in my algorithm? And if it is, how do I go about finding a better way to avoid repetitions when <code>n</code> is a square of a prime?</p>\n"},{"tags":["sql-server","performance","reporting-services","ssrs-2008"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":167,"score":3,"question_id":10562891,"title":"SSRS sql query runs slow","body":"<p>I have a long time issue keep popping up every time.\nI create ssrs report with some select query. when i try to run the report it takes around 20sec to render.\ni've checked the sql profiler and indeed the query run more than 20 sec.\nwhen i copy the query to the management studio, it runs in 0 sec.</p>\n\n<p>as written in earlier posts i've tried the walk around of declaring parameters in the query and setting their value with the ssrs params. sometime it works, currently it doesn't...</p>\n\n<p>any other walk around?</p>\n"},{"tags":["performance","delphi","data-binding","virtualization","delphi-xe3"],"answer_count":0,"favorite_count":5,"up_vote_count":7,"down_vote_count":0,"view_count":172,"score":7,"question_id":12658470,"title":"Live Binding with Automatic updates in XE3 (no need to call Notify() as in XE2)","body":"<p>I am currently using Delphi XE2, and heard about their new Live Binding with Automatic updates in XE3 (no need to call Notify() as in XE2).</p>\n\n<p>in C# or in Delphi XE2 we have to implement <code>INotyfiablePropertyChange</code> (<code>Notify();</code> in Delphi), and this approach really makes sense as we have full control over the contents to be updated and when to update it so we can fine tune the performance and implement virtualization easily.</p>\n\n<p>But I just want to know how does it works, what kind of mechanism they have done in order to implement it, and i have no clue how they have did it, following are the assumptions may have used to implement Automatic updates.</p>\n\n<ol>\n<li><p>Timer : A timer tick frequently and refreshes all the data </p>\n\n<blockquote>\n  <p><em>Very slow performance no virtualisation</em></p>\n</blockquote></li>\n<li><p>Compiler Level Feature : All the <code>notify() events</code> related codes implemented by the compiler auto-magically </p>\n\n<blockquote>\n  <p><em>Lots of over heads</em></p>\n</blockquote></li>\n<li><p>Somme other Approach: </p></li>\n</ol>\n\n<p>Please help me to identified how they have implemented it.</p>\n\n<p>I am currently using XE3 trial so i don't have access to the source code, <strong>your answers will help me decide whether to switch to new features or not</strong>.  I have a class(collection item) with 400 properties to bind(Not all of them all the time), so the performance really plays a major role in the stability of my application.  </p>\n"},{"tags":["performance","bash","loops","script","while-loop"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":2,"view_count":40,"score":-2,"question_id":12658050,"title":"I cannot seem to run this properly... It stucks and does not display an output","body":"<p>Here's my script:</p>\n\n<pre><code>while [[ $startTime -le $endTime ]]\ndo\n\nthisfile=$(find * -type f | xargs grep -l $startDate | xargs grep -l $startTime)\nfordestination=`cut -d$ -f2 $thisfile | xargs cut -d ~ -f4`\n\necho $fordestination\n\nstartTime=$(( $startTime + 1 ))\n\ndone\n</code></pre>\n"},{"tags":["javascript","performance","backbone.js","iteration"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":12654769,"title":"Backbone.js Performance - Adding a class to first and last model of a nested collection","body":"<p>I have a list of ~100 products that are displayed in a long list on one page. Each product belongs to one category, I have about 5 categories. Every product has a category_id.</p>\n\n<p>I want to add a '.first' class to every first product of a category and a '.last' class to every last product of a category.</p>\n\n<p>Performance is key, I came up with a few solutions myself but the were very poorly written and too cumbersome.</p>\n\n<p>Example data:</p>\n\n<pre><code>products = {\n   id: 1, category_id: 1;\n   id: 2, category_id: 1;\n   id: 3, category_id: 1;\n   id: 4, category_id: 2;\n   id: 5, category_id: 2;\n   id: 6, category_id: 2;\n   id: 7, category_id: 3;\n   id: 8, category_id: 3;\n   ...\n}\n</code></pre>\n\n<p>Result I'm looking for:</p>\n\n<pre><code>&lt;ul&gt;\n\n  &lt;li class='first' id='1'&gt;&lt;/li&gt;\n  &lt;li id='2'&gt;&lt;/li&gt;\n  &lt;li class='last' id='3'&gt;&lt;/li&gt;\n\n  &lt;li class='first' id='4'&gt;&lt;/li&gt;\n  &lt;li id='5'&gt;&lt;/li&gt;\n  &lt;li class='last' id='6'&gt;&lt;/li&gt;\n\n  &lt;li class='first' id='7'&gt;&lt;/li&gt;\n  &lt;li id='8'&gt;&lt;/li&gt;\n  ...\n&lt;/ul&gt;\n</code></pre>\n\n<p>EDIT: Thank you for the help @niemand! I used groupBy and this is what I have so far. Is this a good way to go about this, performance wise?</p>\n\n<pre><code>window.firsts = []\nwindow.lasts = []\n@grouped = _.groupBy(@collection.toJSON(), 'category_id')\n_.each @grouped, (group) -&gt;\n  window.firsts.push new App.Models.Piece(_.first(group)).get('id')\n  window.lasts.push new App.Models.Piece(_.last(group)).get('id')\n</code></pre>\n\n<p>This way I can check if window.firsts or window.lasts contains the current model's id when I am rendering out each model view and apply the correct class. If there is a more performant way of going about this I'd love to know!</p>\n"},{"tags":["performance","testing","maven","jmeter","bamboo"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":12646153,"title":"Performance Testing through distributed jmeter instances and bamboo","body":"<p>I´m working on performance test for several services running in an Amazon network. </p>\n\n<p>Our architecture is:</p>\n\n<ul>\n<li>Continuous Integration server running in our facilities (Bamboo);</li>\n<li>A Jmeter server instance in the same network than the services to test;</li>\n<li>A Jmeter client connected to the JMeter server (ssh tunnels) in our facilities.</li>\n</ul>\n\n<p>I want to start the execution of tests from bamboo, and see the different results on it too. </p>\n\n<pre><code>Bamboo with    &lt;---------&gt;   Jmeter server &lt;--------&gt;  WebService\nJmeter client                  on Amazon                on Amazon\n</code></pre>\n\n<p>Has anybody tried something like this?</p>\n"},{"tags":["asp.net","performance","html-rendering","webpage-rendering"],"answer_count":2,"favorite_count":2,"up_vote_count":6,"down_vote_count":0,"view_count":201,"score":6,"question_id":6834177,"title":"Why does the browser wait to end loading the page?","body":"<p>Can anyone explain me why those spaces (marked with ?) are there? They are delaying the page loading. I thought it could be the page/script parsing time, but ~350ms looks too much for a simple page; Okay, there're lots of script, but it still looks to much.</p>\n\n<p>What can it be?</p>\n\n<p><a href=\"http://i.stack.imgur.com/Ju9yq.jpg\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/Ju9yq.jpg\" alt=\"Chrome page speed screenshot\"></a></p>\n"},{"tags":["performance","linq"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":46,"score":0,"question_id":12657241,"title":"Why is the second LINQ query faster?","body":"<p>In this code:</p>\n\n<pre><code>static bool Spin(int WaitTime)\n{\n    Console.WriteLine(\"Running task {0} : thread {1}]\",\n        Task.CurrentId, Thread.CurrentThread.ManagedThreadId);\n    Thread.Sleep(WaitTime);\n    return true;\n}\n\npublic void DemoPLINQLong()\n{\n    var SomeBigNumber = 1000000;\n    var sequence = Enumerable.Range(0, SomeBigNumber);\n    var sw = new Stopwatch();\n    sw.Start();\n    sequence.Where(i =&gt; Spin(SomeBigNumber));\n    sw.Stop();\n    var synchTime = sw.Elapsed;\n    sw.Restart();\n    sequence.Where(i =&gt; Spin(SomeBigNumber));\n    sw.Stop();\n    var asynchTime = sw.Elapsed;\n    Console.WriteLine(\"Synchronous: {0}  Asynchronous: {1}\",\n        synchTime.ToString(), asynchTime.ToString());\n}\n</code></pre>\n\n<p>The results are consistent:\nSynchronous: 00:00:00.0021800  Asynchronous: 00:00:00.0000076</p>\n\n<p>Why is the second LINQ query hundreds of times faster?  Is there some kind of caching going on?  How?</p>\n"},{"tags":["java","performance","metrics","cpu-cache"],"answer_count":3,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":107,"score":3,"question_id":12060519,"title":"Tools to analyse CPU cache performance for Java applications?","body":"<p>I've no preference as regards OS; any tool will be fine so long as it allows me to measure cache performance on Core 2 and i7 architectures.</p>\n"},{"tags":["python","performance","numpy","iteration"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":56,"score":0,"question_id":12656631,"title":"calling a multivariable function iteratively with 2d arrays","body":"<p>Is there a more efficient way to call a function like this:</p>\n\n<pre><code>def func(x,y):\n    # do something here\n</code></pre>\n\n<p>when arguments passed are arrays like this?</p>\n\n<pre><code>X = np.random.rand(5)\nY = np.random.rand(5,25)\n</code></pre>\n\n<p>I use list comprehension in this form:</p>\n\n<pre><code>res = np.array([[func(x,y) for x in X] for y in Y])\n</code></pre>\n\n<p>but was wondering if I could figure out some faster way in numpy, in order to avoid the 'under the hood' loop that happens.</p>\n"},{"tags":["c++","performance"],"answer_count":11,"favorite_count":22,"up_vote_count":80,"down_vote_count":9,"view_count":4309,"score":71,"question_id":12479486,"title":"is x+=a quicker than x=x+a?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/7471891/is-x-1-more-efficient-than-x-x-1\">Is x += 1 more efficient than x = x + 1?</a>  </p>\n</blockquote>\n\n\n\n<p>I was reading Stroustrup's \"The C++ Programming Language\", where he says that\nout of two ways to add something to a variable</p>\n\n<pre><code>x=x+a;\n</code></pre>\n\n<p>and</p>\n\n<pre><code>x+=a;\n</code></pre>\n\n<p>he prefers +=, because it is most likely better implemented. I think he means that it works faster, too.\nBut does it really? If it depends on the compiler and other things, how do I check?  </p>\n"},{"tags":["c#",".net","performance","immutable"],"answer_count":3,"favorite_count":2,"up_vote_count":8,"down_vote_count":1,"view_count":251,"score":7,"question_id":1029611,"title":"Is immutability useful on non parallel applications?","body":"<p>I like the immutability concept but sometimes I wonder, when an application isn't meant to be parallel, should one avoid making things immutable?</p>\n\n<p>When an application isn't multi-threaded, you aren't plagued by shared state problems, right?</p>\n\n<p>Or is immutability a concept like OOP that you either use all the way or not? Excluding the cases when something shouldn't be immutable based on use/performance, etc.</p>\n\n<p>I am faced with this question when writing an application for myself, that is moderately big (maybe like 1-2k lines).</p>\n"},{"tags":["c++","python","performance","cpu-speed"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":1,"view_count":100,"score":1,"question_id":12656098,"title":"C++ vs Python server side performance","body":"<p>I've to develop a server that has to make a lot of connections to receive and send small files. The question is if the increment of performance with C++ worth the time to spend on develop the code or if is better to use Python and debug the code time to time to speed it up. Maybe is a little abstract question without giving a number of connections but I don't really know. At least 10,000 connections/minute to update clients status.</p>\n"},{"tags":["performance","delphi"],"answer_count":0,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":43,"score":0,"question_id":12656104,"title":"Catching the message before InsertControl procedure","body":"<p>I try to create huge number of TWinControl (100 000) at runtime. It goes quickly. But if I change the type to TCustomControl (which has useful property Canvas), it goes pretty slow. The reason for it is, that constructor of TCustomControl creates TControlCanvas too, and it slows down the process significantly. \nMy idea is to make a descendant of TWinControl, which would create TControlCanvas only, when it is inserted on the form. (This huge number is not inserted implicitly)</p>\n\n<p>Is it possible to catch event before (or right after) the descendant is inserted? </p>\n\n<p>Thanx for ideas</p>\n"},{"tags":["java","android","arrays","performance","switch-statement"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":57,"score":0,"question_id":12653748,"title":"Java/Android: virtual dispatch, switch-case, or Array access","body":"<p>I know that \"test it in your app\" is a must. Nonetheless, I would like to ask for an analitical insight as well.</p>\n\n<p>I have an in-app instruction queue (as an <code>Array</code>) which elements are processed sequentially. The details are irrelevant. (Consider \"instruction\" as a virtual instruction, i.e. it is interpreted as such only in terms of my application -- like a small internal Virtual Machine).</p>\n\n<p>The actual question is: which is faster for processing the queue elements -- virtual dispatch provided by Java, or a <code>switch-case</code>?</p>\n\n<ol>\n<li><p>In the first case, the <code>Array</code> consists of descendants of an own <code>Operation</code> object (or simply of <code>Runnable</code> -- you get the point). In this case, instruction execution is merely calling the <code>Operation.run()</code> override of the object. Virtual dispatch will do the rest -- the <code>run()</code> of the concrete instance will be called.</p></li>\n<li><p>The <code>Array</code> is a primitive <code>int</code> array, and each element is an instruction code that comes from a contiguous range (e.g. an <code>int</code> between 0..65535). Instruction processing means that the instruction code is interpreted: (A) either via a <code>switch-case</code> statement, or (B) by using an <code>Array</code> (of <code>Operation</code> objects) that is indexed directly by the instruction codes.</p></li>\n</ol>\n\n<p>In the second case, I suppose <code>switch-case</code> is optimized enough nowadays (especially because I'm using a contiguous range), so we can forget the <code>Array</code> option.</p>\n\n<p>To sum up, which is faster? Adding the interpreting code to the branches of the <code>switch-case</code>, or using the virtual dispatch?</p>\n\n<p>I suppose it breaks down to this: is a <code>switch-case</code> faster or a virtual dispatch? I read that a <code>switch-case</code> can be optimized to a branch table or jump instructions, and I guess in this case it will be possible.</p>\n"},{"tags":["java","performance","big"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":49,"score":0,"question_id":12654834,"title":"How to determine big O of something that looks like this: (x -1) + (x - 2) + (x - 3) ... (x - x)","body":"<p>I'm trying to brush up on my big o calculations. If I have function that shifts all of the items to the right of 'i' 2 spaces I have a formula that looks something like: </p>\n\n<pre><code>(n -1) + (n - 2) + (n - 3) ... (n - n)\n</code></pre>\n\n<p>Where the first iteration I have to move (x-1) items, the second (x-2) items, and so on...\nthe method:</p>\n\n<pre><code>int[] s = {1,2,3,4, , }\n\npublic static char[] moveStringDownTwoSpaces(char[] s){\n    for(int j = 0; j &lt; s.length; j++){\n\n    for(int i = s.length-3; i &gt; j; i--){\n        s[i+2] = s[i];\n    }\n    return s;\n    }\n}\n</code></pre>\n\n<p>I know this is O(n^2), but I don't quite understand the math behind transforming this:</p>\n\n<pre><code>(n -1) + (n - 2) + (n - 3) ... (n - n)\n</code></pre>\n\n<p>into this</p>\n\n<pre><code>O(n^2)\n</code></pre>\n\n<p>In my mind if n = 5 (String is of length 5), I would have...</p>\n\n<pre><code>(5-1) + (5-2) + (5-3) + (5-4) + (5-5) = 5(5 - ???)\n</code></pre>\n\n<p>which is</p>\n\n<pre><code>(n-1) + (n-2) + (n-3) + (n-4) + (n-5) = n(n - ???)\n</code></pre>\n\n<p>so that gives me 5*5 = 25 which is n^2. but what is the ??? I don't know what to put for the variables in the formula. I don't even know if I'm even going by this the correct way. AKA I forgot how to do math :(</p>\n"},{"tags":["c#",".net","performance","struct"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":327,"score":0,"question_id":912514,"title":"Overhead of using this on structs","body":"<p>When you have automatic properties, C# compiler asks you to call the <code>this</code> constructor on any constructor you have, to make sure everything is initialized before you access them.</p>\n\n<p>If you don't use automatic properties, but simply declare the values, you can avoid using the <code>this</code> constructor.</p>\n\n<p>What's the overhead of using <code>this</code> on constructors in structs? Is it the same as double initializing the values?</p>\n\n<p>Would you recommend not using it, if performance was a top concern for this particular type?</p>\n"},{"tags":["jquery","performance","jquery-selectors"],"answer_count":2,"favorite_count":2,"up_vote_count":10,"down_vote_count":0,"view_count":419,"score":10,"question_id":10098367,"title":"Why does jQuery not provide a .firstChild method?","body":"<p>I have seen plenty of discussion regarding the fastest way to select a first child element using jQuery.  As can be expected, the native DOM firstChild property is much faster than using a jQuery selector or combination of selectors -- see <a href=\"http://jsperf.com/jquery-first-child-selection-performance/6\">http://jsperf.com/jquery-first-child-selection-performance/6</a>.  This is not usually a problem -- either it's being used in a place where performance is not a big deal, or it's easy enough to just access the DOM element and use its .firstChild property.  However, there are a couple problems with this:</p>\n\n<ul>\n<li>firstChild could return a text or comment node, rather than an element, as a jQuery selector would return</li>\n<li>If I need to select the first child of multiple elements, I must either use a slow selector, or go to a lot of extra work iterating over DOM elements, adding them to a collection, then putting them back into a jQuery object.</li>\n</ul>\n\n<p>It seems to me that the cost of adding a firstChild method to the core jQuery library would be far smaller than the benefits.  I took my own shot at creating such a method for my own use:</p>\n\n<pre><code>$.fn.firstChild = function() {\n    var ret = [];\n\n    this.each(function(){\n        var el = this.firstChild;\n\n        //the DOM firstChild property could return a text node or comment instead of an element\n        while (el &amp;&amp; el.nodeType != 1)\n            el = el.nextSibling;\n\n            if (el) ret.push(el);\n        });\n\n        //maintain jQuery chaining and end() functionality\n        return this.pushStack(ret);\n    };\n}\n</code></pre>\n\n<p>In the tests i created at <a href=\"http://jsperf.com/jquery-multiple-first-child-selection\">http://jsperf.com/jquery-multiple-first-child-selection</a>, this function performs more than five times faster than any other option.  The tests are based on the tests mentioned above, but are selecting the first children of multiple elements, rather than a single element.</p>\n\n<p>Is there something I am missing?  A technique that I should be using?  Or is this an issue than one should never worry about?  Is there a reason to not include a function like this in jQuery?</p>\n"},{"tags":["jquery","performance","jquery-selectors"],"answer_count":3,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":125,"score":4,"question_id":7897368,"title":"Is there a more efficient way to write $('parent > child')?","body":"<p>Given the following selector $('parent > child'), I believe jQuery will first query for all 'child' elements before filtering down to those that are direct descendants of 'parent'. This can be very inefficient. </p>\n\n<p>My first instinct is to use $('parent').find('child'), but the result is obviously not the same as $('parent > child'). </p>\n\n<p>Is there a better way to write this selector? </p>\n"},{"tags":["performance","web-services","web-applications","benchmarking"],"answer_count":5,"favorite_count":0,"up_vote_count":2,"down_vote_count":2,"view_count":2845,"score":0,"question_id":1526220,"title":"web programming language benchmarks?","body":"<p>I'm thinking that as more and more programming moves to the web with similar and a variety of tools and techniques, has anyone come up with a good set of benchmarks for a programming language?</p>\n\n<p>I'm thinking server-side languages, like Java, PHP, ASP.net, and C# are the big ones.  Though it could be anything like C or perl, I don't want to go there because it's not mainstream or even fast.</p>\n\n<p>When I buy a new CPU, I can benchmark the floating point, integer, and multi-task numbers.  I can bench things like rendering times.</p>\n\n<p>I would think someone could develop a standard bench for simple programming tasks, and practical web serving scenarios.  Preferably an independent organization.  Of course you can always tweak code for performance, but I am talking simple things that can be done identically in every language.  I bet it could be robust as well.</p>\n\n<p>At some point for scalability, performance will be an issue for web languages.  I would like to have a way to tell if a language, or better yet, the latest release of it, stacks up against the competition.</p>\n\n<p>Anybody doing this yet, or should I come up with my own business?</p>\n\n<p>EDIT: yes I know benchmarks are subjective, even more-so for web pages.  I didn't say it would be easy, or even necessary now..  Clearly benchmarking my video card on Crysis versus Far Cry is subjective, but it is a useful real world test.  That's what I'm aiming for, and for the mainstream languages.  Could we see a resurgence in C if performance becomes a focus point down the road?</p>\n"},{"tags":["sql-server","performance","sql-server-2005","stored-procedures"],"answer_count":3,"favorite_count":3,"up_vote_count":4,"down_vote_count":0,"view_count":2586,"score":4,"question_id":3995386,"title":"Query runs fast, but runs slow in stored procedure","body":"<p>I am doing some tests using the SQL 2005 profiler.</p>\n\n<p>I have a stored procedure which simply runs one SQL query.</p>\n\n<p>When I run the stored procedure, it takes a long time and performs 800,000 disk reads.</p>\n\n<p>When I run the same query separate to the stored procedure, it does 14,000 disk reads.</p>\n\n<p>I found that if I run the same query with OPTION(recompile), it takes 800,000 disk reads.</p>\n\n<p>From this, I make the (possibly erroneous) assumption that the stored procedure is recompiling each time, and that's causing the problem.</p>\n\n<p>Can anyone shed some light onto this? </p>\n\n<p>I have set ARITHABORT ON. (This solved a similar problem on stackoverflow, but didn't solve mine)</p>\n\n<p>Here is the entire stored procedure:</p>\n\n<pre><code>CREATE PROCEDURE [dbo].[GET_IF_SETTLEMENT_ADJUSTMENT_REQUIRED]\n @Contract_ID int,\n @dt_From smalldatetime,\n @dt_To smalldatetime,\n @Last_Run_Date datetime\nAS\nBEGIN\n DECLARE @rv int\n\n\n SELECT @rv = (CASE WHEN EXISTS\n (\n  select * from \n  view_contract_version_last_volume_update\n  inner join contract_version\n  on contract_version.contract_version_id = view_contract_version_last_volume_update.contract_version_id\n  where contract_version.contract_id=@Contract_ID\n  and volume_date &gt;= @dt_From\n  and volume_date &lt; @dt_To\n  and last_write_date &gt; @Last_Run_Date\n )\n THEN 1 else 0 end)\n\n -- Note that we are RETURNING a value rather than SELECTING it.\n -- This means we can invoke this function from other stored procedures\n return @rv\nEND\n</code></pre>\n\n<p>Here's a script I run that demonstrates the problem:</p>\n\n<pre><code>DECLARE \n @Contract_ID INT,\n @dt_From smalldatetime,\n @dt_To smalldatetime,\n @Last_Run_Date datetime,\n    @rv int\n\n\nSET @Contract_ID=38\nSET @dt_From='2010-09-01'\nSET @dt_To='2010-10-01'\nSET @Last_Run_Date='2010-10-08 10:59:59:070'\n\n\n-- This takes over fifteen seconds\nexec GET_IF_SETTLEMENT_ADJUSTMENT_REQUIRED @Contract_ID=@Contract_ID,@dt_From=@dt_From,@dt_To=@dt_To,@Last_Run_Date=@Last_Run_Date\n\n-- This takes less than one second!\nSELECT @rv = (CASE WHEN EXISTS\n(\n select * from \n view_contract_version_last_volume_update\n inner join contract_version\n on contract_version.contract_version_id = view_contract_version_last_volume_update.contract_version_id\n where contract_version.contract_id=@Contract_ID\n and volume_date &gt;= @dt_From\n and volume_date &lt; @dt_To\n and last_write_date &gt; @Last_Run_Date\n)\nTHEN 1 else 0 end)\n\n\n-- With recompile option. Takes 15 seconds again!\nSELECT @rv = (CASE WHEN EXISTS\n(\n select * from \n view_contract_version_last_volume_update\n inner join contract_version\n on contract_version.contract_version_id = view_contract_version_last_volume_update.contract_version_id\n where contract_version.contract_id=@Contract_ID\n and volume_date &gt;= @dt_From\n and volume_date &lt; @dt_To\n and last_write_date &gt; @Last_Run_Date\n)\nTHEN 1 else 0 end) OPTION(recompile)\n</code></pre>\n"},{"tags":["mysql","performance","ordering","updating"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":221,"score":0,"question_id":4369952,"title":"Update Multiple Records Efficiently","body":"<p>I have a table on my site where the user has the ability to re-order the rows. This part is easy to do with jQuery UI, and getting the new order back is simple using the <code>$('#sortable').sortable(\"serialize\")</code> method. My question is now, what's the most efficient way to save all these records in the database, since changing the order of one item changes all the records in the table? I know I could create a new <code>UPDATE</code> statement for each row based on their ID, but I feel like there has to be a better way. Is there?</p>\n\n<p>I'm using MySQL and ASP.NET.</p>\n"},{"tags":["performance","matlab","loops","matrix"],"answer_count":1,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":96,"score":4,"question_id":12553064,"title":"Matrix Multiply > for loop > bsxfun - Odd Speed Results","body":"<p>I have an n by n matrix A, a set of n coefficients k (n by 1), and matrix called row (1 by n). My goal is to subtract row weighted by the ith coefficient in k from the ith row of A. I have three questions: Why does my for-loop perform better than the built-in matrix multiply, in general, what explains each method's superiority over the next, and is there a better method than the three with which I came up?</p>\n\n<pre><code>% Define row &amp; coefficients used in each method\nk = (1:1000).';\nrow = 1:1000;\n\n% Method 1 (matrix multiply) ~15 seconds\nA = magic(1e3);\ntic;\nfor z = 1:1e3\n    A = A - k*row;\nend\ntoc;\n% Method 2 (for loop) ~11 seconds\nB = magic(1e3);\ntic;\nfor z = 1:1e3\n    for cr = 1:1000\n        B(cr,:) = B(cr,:) - k(cr)*row;\n    end\nend\ntoc;\n% method 3 (bsxfun) ~ 4 seconds\nC = magic(1e3);\ntic;\nfor z = 1:1e3\n    C = C - bsxfun(@times, k, row);\nend\ntoc\n\nisequal(A,B)\nisequal(A,C)\n</code></pre>\n\n<p>Note, I am doing these row subtractions in an algorithm. I simplified the code a bit, creating this toy test case, but the crux of the computation is still present. Also, to avoid confusion, the for loop with z is used to make the time larger.</p>\n"},{"tags":["android","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":244,"score":0,"question_id":9639734,"title":"Performace Analysis for Android App.","body":"<p>I'm a Computer Science student and I'm working on an Android app. for my final year project.\nThis is a rule in my college to perform some sort of performance analysis in every project that we do. </p>\n\n<p>And this is the point where I got stuck. </p>\n\n<p>Can anyone tell me what sort of performance-analysis I can do in my Project?   </p>\n\n<p>[Please do explain on how that should be performed.]</p>\n"},{"tags":["python","string","performance","time-complexity"],"answer_count":2,"favorite_count":3,"up_vote_count":14,"down_vote_count":0,"view_count":122,"score":14,"question_id":12648002,"title":"What could affect Python string comparison performance for strings over 64 characters?","body":"<p>I'm trying to evaluate if comparing two string get slower as their length increases. My calculations suggest comparing strings should take an amortized constant time, but my Python experiments yield strange results:</p>\n\n<p>Here is a plot of string length (1 to 400) versus time in milliseconds. Automatic garbage collection is disabled, and <code>gc.collect</code> is run between every iteration.</p>\n\n<p><img src=\"http://i.stack.imgur.com/7HZg1.png\" alt=\"Time vs string length\"></p>\n\n<p>I'm comparing 1 million random strings each time, counting matches as follows.The process is repeated 50 times before taking the min of all measured times.</p>\n\n<pre><code>for index in range(COUNT):\n    if v1[index] == v2[index]:\n        matches += 1\n    else:\n        non_matches += 1\n</code></pre>\n\n<p>What might account for the sudden increase around length 64?</p>\n\n<p><strong>Note</strong>: The following snippet can be used to try to reproduce the problem assuming <code>v1</code> and <code>v2</code> are two lists of random strings of length <code>n</code> and COUNT is their length.</p>\n\n<pre><code>timeit.timeit(\"for i in range(COUNT): v1[i] == v2[i]\",\n  \"from __main__ import COUNT, v1, v2\", number=50)\n</code></pre>\n\n<p><strong>Further note</strong>: I've made two extra tests: comparing string with <code>is</code> instead of <code>==</code> suppresses the problem completely, and the performance is about 210ms/1M comparisons.\nSince interning has been mentioned, I made sure to add a white space after each string, which should prevent interning; that doesn't change anything. Is it something else than interning then?</p>\n"},{"tags":["performance","matlab","matrix"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":74,"score":2,"question_id":12620920,"title":"MATLAB: How to efficiently create a matrix, which is the result of an outer product?","body":"<p>I have two vectors a and b and some function f. \nWhat is the best way (in performance) to define a matrix in MATLAB of such a kind:</p>\n\n<pre><code>A(m,n) = f(a(m)*b(n)) / ( (f(a(m)) * f(b(n)) )\n</code></pre>\n\n<p>Is it possible not to use nested loops?</p>\n"},{"tags":["java","performance","time"],"answer_count":4,"favorite_count":1,"up_vote_count":3,"down_vote_count":8,"view_count":123,"score":-5,"question_id":11142891,"title":"Time complexity analysis","body":"<p>Sorry, I don't have the time to write a long contextual spiel here. This is a question from a practice exam I'm doing at the moment, and all my University resources are offline (great Uni, I know). I'm completely stumped on how to even start this. Could someone walk me through it? I'm not the greatest with math.</p>\n\n<p>Consider the following recursive method:</p>\n\n<pre><code>public static int triple(int x) {\n    if (x == 0) return 0;\n    else return add(3, triple(decrement(x)));\n}\n</code></pre>\n\n<p>Assuming that the worst case time performance for the decrement method is constant\nand that the add method is linear in its second parameter (i.e., the time for add(x,y)\ncan be expressed as <code>by+a</code> for some constants <code>b</code> and <code>a</code>), derive the smallest <code>big O</code> that\ndescribes the worst case time performance of the triple method in terms of x.\nTo derive a complexity for this method, determine and expand recurrence relations for\nthe first several method instances (problem sizes) and then generalise your expressions\nto form a closed form equation for the <code>nth case</code>. Show your workings.</p>\n"},{"tags":["xml","vim","performance"],"answer_count":9,"favorite_count":1,"up_vote_count":15,"down_vote_count":0,"view_count":2067,"score":15,"question_id":901313,"title":"Editing xml files with long lines is really slow in vim. What can I do to fix this?","body":"<p>I edit a lot of xml files with vim. The problem is that, because of the long lines, navigation/editing in vim is extremely slow. Is there something I can do (besides turning off syntax highlighting/filetype plugins and filetype indentation) to be able to edit these files without all that lag?</p>\n\n<p>It's really frustrating that a trivial thing such as syntax highlighting is being handled so poorly by vim. I don't remember this being an issue with any other editor. I really like using vim and I hope there is some way to fix this.</p>\n"},{"tags":["android","performance","service","gps","location"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":47,"score":0,"question_id":12285973,"title":"Android Service which keeps track of my current location and/or speed","body":"<p>I want to write a <strong>Service</strong> which keeps track of my current location (and a different Service for my speed).\nI've seen some code examples, but all them are for Activities and not for Services.\nI want the Service to keep track until I stop it.\nHow do I do it?</p>\n"},{"tags":["performance","postgresql","jdbc","batch"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":61,"score":0,"question_id":12639118,"title":"jdbc batch performance","body":"<p>i'm batching updates with jdbc </p>\n\n<pre><code>ps = con.prepareStatement(\"\");\nps.addBatch();\nps.executeBatch();\n</code></pre>\n\n<p>but in the background it seems, that the prostgres driver sends the query bit by bit to the database. </p>\n\n<p>org.postgresql.core.v3.QueryExecutorImpl:398</p>\n\n<pre><code> for (int i = 0; i &lt; queries.length; ++i)\n            {\n                V3Query query = (V3Query)queries[i];\n                V3ParameterList parameters = (V3ParameterList)parameterLists[i];\n                if (parameters == null)\n                    parameters = SimpleQuery.NO_PARAMETERS;\n\n                sendQuery(query, parameters, maxRows, fetchSize, flags, trackingHandler);\n\n                if (trackingHandler.hasErrors())\n                    break;\n            }\n</code></pre>\n\n<p>is there a possibility to let him send 1000 a time to speed it up?</p>\n"},{"tags":["c#","wpf","performance","video","fps"],"answer_count":2,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":148,"score":0,"question_id":11906503,"title":"Fast Video Display WPF","body":"<p>I am working on a WPF application that needs to display several video streams at a fast frame rate (we would like 30 fps). The video streams are 1920x1080 raw (RGB24) frames (they are stored in a System.Drawing.Bitmap). Does anyone have any ideas on how to achieve this? </p>\n\n<p>More details:</p>\n\n<ul>\n<li>Our previous attempts have used a standard WPF Image control, changing its source for each frame. This worked well for a single stream but now that we have to render multiple streams, it is slowing down. </li>\n<li>We have also tried using Direct2D to handle the drawing, using a D3D9 shared surface as the source for an Image control. While this was faster, we are still not able to get a stable 30fps out of it (it jumps between 24-32 fps as things back up). </li>\n<li>The video stream is coming in on a background thread, then being marshaled (using the Dispatcher of the window) to the proper UI thread for drawing. All the drawing is then done on the UI thread. We have also tried giving each window its own thread.</li>\n</ul>\n\n<p>I can provide code samples of what we have tried if anyone wants to see.  </p>\n\n<p>thanks!</p>\n"},{"tags":["performance","grails","grails-2.0"],"answer_count":3,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":168,"score":3,"question_id":12619243,"title":"Grails 2.0 run-app very slow performance","body":"<p>I'm using Grails 2.0. I used to develop under Grails 1.3.7 but when running an application under Grails 2.0 the performance is very slow. A page can take more than 30 seconds to show and it's very embarrassing and unproductive. </p>\n\n<p>I googled and I found that GSPs in 2.0 are in some cases 10 times slower than 1.3.7 ;\nGreame explained that there is a new way of handling GSPs in dev mod, but when executing grails prod run-app I have almost the same problem.</p>\n\n<p>What should I do to speed up development process ? I'm loosing too much time.</p>\n\n<p>PS : My GRAILS_OPTS are \"-‬server‭ -‬Xmx600M‭ -‬Xms600M‭ -‬XX:MaxPermSize=250m‭ -‬Dfile.encoding=UTF-8 -Dserver.port=9090‭\"‬</p>\n"},{"tags":["c#","performance","parallel-processing","task-parallel-library"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":38,"score":1,"question_id":12647305,"title":"Dispose objects read from BlockingCollection","body":"<p>I have to parse a lot of files. So right now I open a file for reading parse the content and write the output on a different location. <strong>That is basically what I need to do but I will like to speed up the process since I am parsing 14000 files.</strong></p>\n\n<p>I improved my algorithm by spliting the work on multiple threads. So I had 1 thread do 25% of the files the next 25 % and so on. </p>\n\n<p>Anyways I believe I will dramatically increase performance and speed if I have:</p>\n\n<ol>\n<li><code>Task 1</code> read files and place the content of the files in  <code>BlockingCollection1</code> (memory)</li>\n<li><code>Task 2</code> will create multiple threads to parse the content of <code>BlockingCollection1</code> and place parsed data in <code>BlockingCollection2</code></li>\n<li><code>Task 3</code> read content from <code>BlockingCollection2</code> and writes it to disk.</li>\n</ol>\n\n<p><strong>The problem that I have now is that I get an out of memory exception.</strong> I will like to let the garbage collector remove items from BlockingCollection1 if they have been used. also I will like to remove items from <code>BlockingCollection2</code> if they have been written to disk. </p>\n\n<p><strong>It is very convenient to use a <code>BlockingCollection&lt;T&gt;</code> since one thread can be adding items to that collection and another thread can be processing those items.</strong> Before I used a linked list but I leaned in this question: <a href=\"http://stackoverflow.com/a/12519345/637142\">http://stackoverflow.com/a/12519345/637142</a> how essay and useful a BlockingCollection can be.  <strong>Anyways how could I solve this problem? Do I have to use a different type of collection such as a stack?</strong></p>\n"},{"tags":["performance","mono","runtime"],"answer_count":7,"favorite_count":4,"up_vote_count":16,"down_vote_count":0,"view_count":10094,"score":16,"question_id":380733,"title":"Mono performance","body":"<p>Are there any performance benchmarks for Mono compared to say Java in GNU/Linux?</p>\n\n<p>Have you ever tested Mono's performance?</p>\n"},{"tags":["performance","image","testing","upload","hardware"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":20,"score":0,"question_id":12631331,"title":"Peformance Test - Images Upload","body":"<p>Normally a server uploads about 20000 images (500kb, PNG format ) per day.</p>\n\n<p>there will be some changes in the amount of images to upload... so i am planning to do a performance test in a virtual machine which doesn't have the same hardware that the original machine... the idea is upload about 60000 images..</p>\n\n<p>I.e  i have  the virtual machine with 2 processors 2GHz each one , and 8 GB RAM...</p>\n\n<p>in the other hand i have the real machine ,  4 processors 2Ghz each one, and 16 GM RAM...</p>\n\n<p>If i can't do the upload of that 60000 images in the real machine, how can i analyze the results in the virtual machine ( Response Times and Alarms in hardware components) and give a idea of how will be the behavior on the real machine ???  </p>\n\n<p>I was thinkg about making a three rule with the hardware but i am not sure... </p>\n\n<p>So  if the virtual machine support the load... how can i get the equivalent of what would happen in the real machine.. </p>\n\n<p>Thanks in advance and excuse me about my english!</p>\n"},{"tags":["java","performance","google-app-engine","datastore"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":312,"score":2,"question_id":6693329,"title":"Google App Engine Performance - Checking Existence of an Object","body":"<p>I am coding a system using Google App Engine and I need to put an object in the datastore only if it doesn't exist yet. I would be fine using the <code>datastore.put()</code> method, except I need to know whether that object already existed to count the number of new objects I have.</p>\n\n<p>As far as I know I have the following options (suppose I have the key both as an attribute and as the entity key):</p>\n\n<pre><code>private Entity getEntity(String key)\n{\n    DatastoreService datastore =\n        DatastoreServiceFactory.getDatastoreService();\n\n    // Build a query to select this entity from the database:\n    Query q = new Query(\"MyEntity\");\n    q.setKeysOnly();\n    // Add a filter for the key attribute:\n    q.addFilter(\"key\", Query.FilterOperator.EQUAL, key);\n    PreparedQuery pq = datastore.prepare(q);\n    // Select a single entity from the database\n    // (there should be no more than one matching row anyway):\n    List&lt;Entity&gt; list = pq.asList(FetchOptions.Builder.withLimit(1));\n\n    if (!list.isEmpty())\n        // Return the found entity:\n        return list.get(0);\n    else\n        return null;\n}\n</code></pre>\n\n<p>or</p>\n\n<pre><code>private Entity getEntity(String key)\n{\nDatastoreService datastore =\n    DatastoreServiceFactory.getDatastoreService();\n\n    // Get a key that matches this entity:\n    Key key = KeyFactory.createKey(\"MyEntity\", key);\n\n    try {\n        return datastore.get(key);\n    } catch (EntityNotFoundException e) {\n        // Entity does not exist in DB:\n        return null;\n    }\n}\n</code></pre>\n\n<p>I'm inclined to use the second one as it seems more straight forward, but I'm worried it might not meant to be used that way since it raises an exception, and it may incur overhead.</p>\n\n<p>Which of the methods are better for checking whether an entity exists in the database?</p>\n\n<p>Is there a better way to do that?</p>\n"},{"tags":["performance","caching","browser-cache","pagespeed"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":31,"score":0,"question_id":12644168,"title":"What is the effect of max-age=0 for load speed optimisation?","body":"<p>In terms of load speed optimisation, is <code>Cache-Control: max-age=0</code> with an \n<code>If-Modified-Since</code> date a problem? My browser reports a 304, but tools like Google's PageSpeed and YSlow flag them as a problem on account of the short (non-existent) \"freshness lifetime\".</p>\n\n<p>I've read through <a href=\"http://stackoverflow.com/questions/1046966/whats-the-difference-between-cache-control-max-age-0-and-no-cache\">this question</a> and some of the linked resources but, while helpful, I'm still not clear on the net effect (if any) for load speed.</p>\n\n<p>Many thanks.</p>\n"},{"tags":["php","performance"],"answer_count":5,"favorite_count":0,"up_vote_count":0,"down_vote_count":2,"view_count":75,"score":-2,"question_id":12643799,"title":"Improving PHP performance with explicit type casting","body":"<p>I know in PHP following variable assignments are valid,</p>\n\n<pre><code>$var = 100;\n$var = 'string';\n$var = 10.24;\n</code></pre>\n\n<p>But I was wondering, If  I use explicit type casting as given below. </p>\n\n<pre><code>$var = (int)100;\n$var = (float)10.24;\n</code></pre>\n\n<p>My script will execute faster, and Web Application as a whole will execute faster, If I use explicit type casting everywhere, AM I RIGHT OR WRONG?</p>\n"},{"tags":["php","performance","oop"],"answer_count":5,"favorite_count":0,"up_vote_count":3,"down_vote_count":2,"view_count":2631,"score":1,"question_id":2396824,"title":"const vs static in PHP","body":"<p>IN PHP5 i can declare a const value to a class:</p>\n\n<pre><code>class config\n{\n     const mailserver = 'mx.google.com';\n}\n</code></pre>\n\n<p>But i can also declare public static:</p>\n\n<pre><code>class config\n{\n     public static $mailserver = 'mx.google.com';\n}\n</code></pre>\n\n<p>In case of a config file which i will later us, like:</p>\n\n<pre><code>imap_connect(config::$mailserver ...\nimap_connect(config::mailserver ...\n</code></pre>\n\n<p>Which of the option do you think is better to be used?? (faster, less memory load, etc ..)</p>\n\n<p>Thanks.</p>\n"},{"tags":["python","performance","go"],"answer_count":7,"favorite_count":1,"up_vote_count":11,"down_vote_count":0,"view_count":685,"score":11,"question_id":12574909,"title":"Can Go really be that much faster than python?","body":"<p>I think I may have implemented this incorrectly because the results do not make sense. I have a go program that counts to 1000000000</p>\n\n<pre><code>    package main\n\n    import (\n        \"fmt\"\n    )\n\n    func main() {\n        for i := 0; i &lt; 1000000000; i++ {}\n        fmt.Println(\"Done\") \n    }\n</code></pre>\n\n<p>It finishes in less than a second. On the other hand I have a python script</p>\n\n<pre><code>    x = 0\n    while x &lt; 1000000000:\n        x+=1\n    print 'Done'\n</code></pre>\n\n<p>It finishes in a few minutes.</p>\n\n<p>Why is the Go version so much faster. Are they both counting up to 1000000000 or am I missing something?</p>\n"},{"tags":["performance","magento"],"answer_count":6,"favorite_count":8,"up_vote_count":7,"down_vote_count":0,"view_count":8956,"score":7,"question_id":1403184,"title":"What are the best steps to improve magento performance?","body":"<p>I have a magento shop (ver 1.2) running under a dedicated LAMP setup.</p>\n\n<p>I have made the following changes to it:</p>\n\n<ul>\n<li>Enabled magento caching</li>\n<li>Enabled APC</li>\n<li>MySQL Query caching</li>\n<li>GZip compression of html,css,js</li>\n</ul>\n\n<p>The shop is still incredibly slow, around 10 secs and over for rendering the homepage. Is there any obvious things I am missing out.</p>\n\n<p>Please share your tips to improve Magento performance</p>\n"},{"tags":["android","xml","string","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":58,"score":3,"question_id":12254683,"title":"Performance at working with strings android","body":"<p>Could somebody tell me what is better in terms of performance?</p>\n\n<p>Is it better <strong>to save 2 strings</strong> at <code>string.xml</code>, like <code>'abc'</code> and <code>'abc:'</code></p>\n\n<p>Or should I <strong>save only the first</strong> one and <strong>concatenate</strong> <code>':'</code> when needed at Java coding ???</p>\n"},{"tags":["javascript","jquery","performance","jquery-ui","singlepage"],"answer_count":1,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":131,"score":4,"question_id":12627065,"title":"Extreme slow down using jquery-ui methods after page has been open for some time","body":"<p>So my webpage uses a lot of jquery everywhere, It is a single page javascript application and I pretty much create all the HTML by hand using jquery. I have a lot of divs in which I use draggable and resizable I also use jquery-ui-effects .hide and .show with slide animation to move some divs around.</p>\n\n<p>When I start the application everything works flawlessly but after 10~15 min that the page has been open everything that uses jquery-ui methods gets so slow that it's unusable. When I try to resize one of my divs there is a major slow down when I first mousedown on the corner and after I let go of the click it takes some seconds for the page to become responsive again. Same is true for drag and drop. Calling $().draggable and $().resizable on the divs also takes a lot of time. Strangely the dragging and resizing itself is not slow, only starting/ending it. The longer the page has been open the slower it gets.</p>\n\n<p>All other functionality in the page works just fine even after one hour of the page being open (I even have some basic canvas drawing in place, other jquery but not jquery-ui functionality also works ok). There is no aparent memory leak since the browser never goes over 150mb of memory used.</p>\n\n<p>Some people might say that the problem are my start/stop resize/drag functions. \nIt's not that because I tried to remove them without any change in speed and also it would not explain the slow down on the animations.</p>\n\n<p>CPU use goes to 100% (I'm using a single core system) when animating, it stays at 0% when not using jquery-ui functions. When profiling the animation function (after the page being open for more than 30 min) I see that there is a method named curCSS (jquery-1.8.1.js line 6672) using 95% of the CPU time. This same function only uses 4.5% if I execute the animation a few seconds after starting the application.</p>\n\n<p>I'm using jquery-1.8.1 and jquery-ui 1.8.22.</p>\n\n<p>I can't post all the code because I don't know what part of it is wrong and the whole code-base is huge. The animation is done this way:</p>\n\n<pre><code>//internal code that prevents updating data on the divs that are part of the animation\nvar hiding= true;\n    var showing= true;\n\n    containerToHide.$div.hide(\"slide\", {direction: \"left\"}, 1000, function() {\n        hiding= false;\n        if (!showing) { //both animations ended\n            //internal code to allow update data on div after animation ends\n        }\n    });\n\n    containerToShow.$div.show(\"slide\", {direction: \"right\"}, 1000, function() {\n        showing= false;\n        if (!hiding) {//both animations ended\n            //internal code to allow update data on div after animation ends\n        }\n    });\n</code></pre>\n\n<p>I don't think this is the problem though, the animations are pretty standard stuff.</p>\n\n<p>Any hints on what to look for would be greatly appreciated.</p>\n"},{"tags":["javascript","html","css","performance"],"answer_count":6,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":257,"score":1,"question_id":11757016,"title":"Performance differences between visibility:hidden and display:none","body":"<p>I want to simplify things in my jQuery Backbone.js web application. One such simplification is the behavior of my menu and dialog widgets.  </p>\n\n<p>Previously I created the div boxes of my menus at start and hid them using <code>display: none; opacity:0;</code>.   When I needed a menu, I changed its style to <code>display:block</code> then used the jQuery ui position utility to position the div box (since elements with <code>display:none</code> cannot be positioned) and when it was done, finally changed its style to <code>opacity:1</code>.  </p>\n\n<p>Now I want to just hide them with <code>visibility:hidden</code>, and when I need one, I use the position utility and then change the style to <code>visibility:visible</code>.  When I begin using this new approach, I will have around 10 div boxes throughout the web application session that are hidden but occupy space, in contrast to the previous div boxes hidden with <code>display:none</code>.  </p>\n\n<p>What are the implications of my new approach? Does it effect browser performance in any regard? </p>\n"},{"tags":["performance","login","user","symfony-1.4","production-environment"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":44,"score":0,"question_id":12641095,"title":"can I use environments as users in symfony?","body":"<p>I am using Symfony 1.4, working having an unique database.</p>\n\n<p>I doesn't matter how much users I have, so i'm wondering if I can use an environment for each new user, then a different configuration file would tell -<em>each time a user logs in</em>- the system which database select.</p>\n\n<p>The result will create a lot of different environments, with a lot of databases. Indeed, it practically works because I've already did this.</p>\n\n<p>But, does this affect the application performance ? Would it cause some problems in the future ?</p>\n"},{"tags":["mysql","performance","magento","replication","cluster-computing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":79,"score":0,"question_id":12638460,"title":"Setup Magento to write on multiple mysql masters","body":"<p>I'm completely newbie in mysql cluster. I was trying long ours to find if, when I setup magento with mysql cluster, it will write to one of the nodes or always to one node?</p>\n\n<p>My second question is, when I setup mysql replication: one master and one slave, and master server die, will magento start to write on slave, or my app stop working?</p>\n\n<p>Thanks a lot!</p>\n"},{"tags":["performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":39,"score":0,"question_id":12489458,"title":"Web services performance testing tools","body":"<p>Can someone please suggest a web services performance testing tool which is free or not too costly. and some amount of scripting capability should be there. </p>\n\n<p>I have already evaluated the below tools and did not suite our needs</p>\n\n<p>1)SoapUI/LoadUI- Could not get data driven load testing for web services in free version.</p>\n\n<p>2)Visual Studio Ultimate - good but too costly.</p>\n\n<p>3)Radview Web Load - the open source seems to be down could not find the forum or the download installer.</p>\n\n<p>4) AppPerfect- looked promising but no support</p>\n\n<p>5) Crosscheck-Soapsonar- Good tool but too costly for us on the longer run</p>\n\n<p>6) Jmeter - could not find help on how to script.</p>\n\n<p>Please suggest any other tool other than the above.</p>\n"},{"tags":["android","c++","performance","build","android-ndk"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":109,"score":2,"question_id":12640377,"title":"How to speed up android ndk builds","body":"<p>We have a quite large C++ project that we build cross platform for Android and iOS. Xcode uses all cores when compiling and is much faster (4-5x depending on machine). Is there any way to improve the abismal performance of the NDK? Any multi CPU options? We have precompiled headers, but it seems to me it is use of resources that is at fault.</p>\n\n<p>So any tips or tricks to speed up android ndk project build times would be much appreciated!</p>\n"},{"tags":["c#","sql","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":85,"score":0,"question_id":12001107,"title":"SQL COALESCE vs. DBNull check in C#","body":"<p>In my project, I have to retrieve a large amount of data from SQL and map the data to object fields; something like this:</p>\n\n<pre><code>cu.UnitName = dr[\"UnitName\"].ToString().Trim();\ncu.LocalId = DbUtil.RemoveNull(dr[\"LocalID\"], \"\");\ncu.DatabaseName = DbUtil.RemoveNull(dr[\"DatabaseName\"], \"\");\ncu.DatabaseServer = DbUtil.RemoveNull(dr[\"DatabaseServer\"], \"\");\ncu.UserName = DbUtil.RemoveNull(dr[\"UserName\"], \"\");\ncu.Password = DbUtil.RemoveNull(dr[\"Password\"], \"\");\ncu.RoleId = DbUtil.RemoveNull(dr[\"RoleId\"], 0);\n</code></pre>\n\n<p>where <code>DbUtil.RemoveNull</code> is following (<code>int</code> version, the <code>string</code> version is similar):</p>\n\n<pre><code>public static int RemoveNull(object data, int defaultValue)\n    {\n        if (data is DBNull || data == null)\n            return defaultValue;\n        return int.Parse(data.ToString());\n    }\n</code></pre>\n\n<p>So I wonder if SQL's <code>COALESCE</code> or <code>ISNULL</code> would do the same job faster. Did someone do similar comparison? Which way would be more efficient?</p>\n"},{"tags":["performance","testing","compact-framework","windows-ce","performance-testing"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":43,"score":0,"question_id":12635451,"title":"What tool can I use to test .NET App Performance on a WinCE device?","body":"<p>I search some tools for testing .net cf 2.0 app which deployed on WinCe device.\nI want something like jetbrains dot trace performance. I've try use visual studio profiler but they show only stats about descriptors and memory and process ugage. I want look what function, run, how many time they work, what work toolong. But all this info I didn't see in VS profiler.</p>\n"},{"tags":["multithreading","performance","threadpool","cpu-cache"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":51,"score":0,"question_id":12639052,"title":"Cache-per-core & Thread pooling","body":"<p>Given (particularly) the following scenario:</p>\n\n<ul>\n<li>One thread per core,</li>\n<li>Each core having its own distinct cache,</li>\n<li>Programs where cache hit/miss ratios are central to good performance (i.e. most today)</li>\n</ul>\n\n<p>I've read frequently about the benefits of thread pooling for scheduling work in multicore systems. Although there are a number of approaches to multithreading, the comparison is often made between a smarter, load-balancing approach like this, and a more naive, \"assign threads by task type\" approach where load-balancing is assumed to have been handled at development time, rather than by the system itself at runtime. An example of this might be dedicated number crunching on one thread and rendering tasks on another.</p>\n\n<p>It seems to me that under the above conditions, the thread-by-task-type approach could lead to far better performance since that core's local cache would be that much more efficient <em>for the specific task to which it has been assigned</em>? (Assuming that waiting is not much of an issue, i.e. both threads are running at or close to full steam.)</p>\n\n<p>I also wonder what performance impact thread-safety mechanisms might have, in load-balanced vs naive approaches.</p>\n"},{"tags":["performance","caching","assembly","masm"],"answer_count":1,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":1377,"score":5,"question_id":524821,"title":"How do I intentionally read from main memory vs cache?","body":"<p>So I am being taught assembly and we have an assignment which is to find the time difference between reading from memory and reading from cache. We have to do this by creating 2 loops and timing them. (one reads from main memory and the other from cache). The thing is, I don't know and can't find anything that tells me how to read from either cache or main memory =/. Could you guys help me? I'm doing this in MASM32. I understand how to make loops and well most of the assembly language but I just can't make it read =/</p>\n\n<p><hr /></p>\n\n<p><strong>Edit:</strong></p>\n\n<p>I have a question, I've done this ...</p>\n\n<pre><code>mov ecx, 100 ;loop 100 times\nxor eax, eax ;set eax to 0\n_label:\nmov eax, eax ;according to me this is read memory is that good?\ndec ecx ;dec loop\njnz _label ;if still not equal to 0 goes again to _label\n</code></pre>\n\n<p>... would that be ok?</p>\n\n<p><hr /></p>\n\n<p><strong>Edit 2:</strong></p>\n\n<p>well then, I don't intend to pry and I appreciate your help, I just have another question, since these are two loops I have to do. I need to compare them somehow, I've been looking for a timer instruction but I haven't found any I've found only: <strong>timeGetTime</strong>, <strong>GetTickCount</strong> and <strong>Performance Counter</strong> but as far as I understand these instructions return the system time not the time it takes for the loop to finish. Is there a way to actually do what I want? or I need to think of another way?</p>\n\n<p>Also, to read from different registers in the second loop (the one not reading from cache) is it ok if I give various \"mov\" instructions? or am I totally off base here?</p>\n\n<p>Sorry for all this questions but again thank you for your help.</p>\n"},{"tags":["java","performance","findbugs"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":54,"score":1,"question_id":12639259,"title":"Please explain this FindBugs warning","body":"<p>Please refer to the following method :</p>\n\n<pre><code>public Set&lt;LIMSGridCell&gt; getCellsInColumn(String columnIndex){\n    Map&lt;String,LIMSGridCell&gt; cellsMap = getCellsMap();\n    Set&lt;LIMSGridCell&gt; cells = new HashSet&lt;LIMSGridCell&gt;();\n    Set&lt;String&gt; keySet = cellsMap.keySet();\n    for(String key: keySet){\n      if(key.startsWith(columnIndex)){\n        cells.add(cellsMap.get(key));\n      }\n    }\n    return cells;\n  }\n</code></pre>\n\n<p>FindBugs give this waring message :</p>\n\n<blockquote>\n  <p>\"<strong>Inefficient use of keySet iterator instead of entrySet iterator</strong>\n  This method accesses the value of a Map entry, using a key that was\n  retrieved from a keySet iterator. It is more efficient to use an\n  iterator on the entrySet of the map, to avoid the Map.get(key)\n  lookup.\"</p>\n</blockquote>\n"},{"tags":["android","performance","canvas"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":30,"score":1,"question_id":12635014,"title":"drawTextOnPath very slow on Android","body":"<p>I draw a lot of shapes and text. I can move the painting with screentouch and all moves so smoothly. Until I paint a text on a path with drawTextOnPath.\n\"drawTextOnPath\" slows down the drawingspeed. </p>\n\n<p>Do anyone have a suggestion howto to speed up my situation ?</p>\n\n<p>kind regards</p>\n"},{"tags":["performance","monitoring","openerp"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":37,"score":1,"question_id":12638851,"title":"Number of active user sessions in an OpenERP server","body":"<p>How can I find the number of \"active sessions\" in my OpenERP server ?</p>\n\n<p>I'm aware that \"active sessions\" is not an exact concept here, but overall what I would like to be aware of is the level of usage stress the server is being subject to, and comparing that to the OS resources being dedicated to the process.</p>\n"},{"tags":["objective-c","ios","performance","iphone-sdk-4.0","uiimageview"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":238,"score":1,"question_id":11130904,"title":"Objective - C, fastest way to show sequence of images in UIImageView","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/2507441/how-to-efficiently-show-many-images-iphone-programming\">How to efficiently show many Images? (iPhone programming)</a>  </p>\n</blockquote>\n\n\n\n<p>I have hundreds of images, which are frame images of one animation <strong>(24 images per second)</strong>. Each image size is <strong>1024x690</strong>.<br><br>\nMy problem is, I need to make <strong>smooth animation iterating each image</strong> frame in <strong>UIImageView</strong>.<br>\nI know I can use animationImages of <strong>UIImageView</strong>. But it crashes, because of memory problem.<br><br>\nAlso, I can use <code>imageView.image = [UIImage imageNamed:@\"\"]</code> that would cache each image, so that the next repeat animation will be <strong>smooth</strong>. But, caching a lot of images crashed app.<br><br>\nNow I use <code>imageView.image = [UIImage imageWithContentsOfFile:@\"\"]</code>, which does not crash app, but doesn't make animation so smooth. <br><br>\nMaybe there is a better way to make good animation of frame images?<br><br>\n<em>Maybe I need to make some preparations, in order to somehow achieve better result. I need your advices. Thank you!</em></p>\n"},{"tags":["javascript","performance","google-maps","google-maps-api-3","crash"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":43,"score":0,"question_id":12627223,"title":"Google Maps JS API intermittently crashes process on custom marker click","body":"<p>I'm afraid the product I'm working on is so inherently full of esoteric client-sensitive stuff that I can't meaningfully reduce the problem scenario to a reproductible example, so this is appealing to domain expertise and insight, based on the following code:</p>\n\n<p><a href=\"http://jsbin.com/barney/1/edit\" rel=\"nofollow\">http://jsbin.com/barney/1/edit</a></p>\n\n<p>(makes use of jQuery, Underscore, Mustache, a few plugins and of course Google Maps v3 JS API)</p>\n\n<p>I am using the Google Maps JS API to produce a relatively simple web app. So far all it does is create a custom content <code>InfoWindow</code>, dynamically changing the content as and when users click on custom <code>MarkerImage</code>s. The map is desaturated by use of stylers.</p>\n\n<p>Intermittently (can't reliably reproduce) the process will completely freeze when I click one of these custom markers. I have a breakpoint on the first line of my code immediately within the <code>event.addListener</code> – the first time my code is hit in the usual stack of things – but the process never gets there: everything is frozen by the time I get there.</p>\n\n<p>4 marker clicks usually does the trick, but other times the application will run through the entire UX without a fuss.</p>\n\n<p>Additional info, off the top of my head:</p>\n\n<ul>\n<li>I have a couple of jQuery click events delegated to the body (not\nthat these are firing in the observable stack); </li>\n<li>Left alone for 5 minutes or so, this app will freeze anyway, so I suspect I am unwittingly silently abusing the Maps API anyway.</li>\n</ul>\n\n<p>Any advice on gotchas for this kind of scenario would be really appreciated.</p>\n"},{"tags":["facebook","performance","facebook-graph-api"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":23,"score":0,"question_id":12637979,"title":"Is there any time/size limit on a Facebook Graph API request","body":"<p>Independently of limits on batch number of request, all paginations, number of requests, is there a <strong>limit in data</strong> (size in Bytes) or in <strong>time to answer</strong> than facebook server can allow to a query ?</p>\n\n<p>This question is motivated by a big query which regularly return an error 500.\nI remember a wrong query will be directly returned as specific error, and lack of authorisation for specific field return nothing.</p>\n"},{"tags":["c#",".net","performance","profiling","performance-testing"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":102,"score":1,"question_id":12629032,"title":"Performance profiling in .NET","body":"<p>I wrote a class which uses <code>Stopwatch</code> to profile methods and <code>for/foreach</code> loops. With <code>for</code> and <code>foreach</code> loops it tests a standard loop against a <code>Parallel.For</code> or <code>Parallel.ForEach</code> implementation.</p>\n\n<p>You would write performance tests like so:</p>\n\n<p><strong>Method:</strong></p>\n\n<pre><code>PerformanceResult result = Profiler.Execute(() =&gt; { FooBar(); });\n</code></pre>\n\n<p><strong>For loop:</strong></p>\n\n<pre><code>SerialParallelPerformanceResult result = Profiler.For(0, 100, x =&gt; { FooBar(x); });\n</code></pre>\n\n<p><strong>ForEach loop:</strong></p>\n\n<pre><code>SerialParallelPerformanceResult result = Profiler.ForEach(list, item =&gt; { FooBar(item); });\n</code></pre>\n\n<p>Whenever I run the tests (one of <code>.Execute</code>, <code>.For</code> or <code>.ForEach</code>) I put them in a loop so I can see how the performance changes over time.</p>\n\n<p>Example of performance might be:</p>\n\n<p>Method execution 1 = 200ms<br/>\nMethod execution 2 = 12ms<br/>\nMethod execution 3 = 0ms</p>\n\n<p>For execution 1 = 300ms (Serial), 100ms (Parallel)<br/>\nFor execution 2 = 20ms (Serial), 75ms (Parallel)<br/>\nFor execution 3 = 2ms (Serial), 50ms (Parallel)</p>\n\n<p>ForEach execution 1 = 350ms (Serial), 300ms (Parallel)<br/>\nForEach execution 2 = 24ms (Serial), 89ms (Parallel)<br/>\nForEach execution 3 = 1ms (Serial), 21ms (Parallel)</p>\n\n<p><strong>My questions are:</strong></p>\n\n<ol>\n<li><p>Why does performance change over time, what is .NET doing in the background to facilitate this?</p></li>\n<li><p>How/why is a serial operation faster than a parallel one? I have made sure that I make the operations complex to see the difference properly...in most cases serial operations seem faster!?</p></li>\n</ol>\n\n<p><strong>NOTE:</strong> For parallel processing I am testing on an 8 core machine.</p>\n"},{"tags":["performance","apache","servicemix","fuseesb"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":58,"score":0,"question_id":12637316,"title":"Performance issue on Apache servicemix and Fuse ESB","body":"<p>I am running the below 'echo' webservice on Fuse ESB 7 and Apache ServiceMix 4.4.2.</p>\n\n<pre><code>&lt;jaxws:endpoint id=\"HTTPEndpoint\" implementor=\"#helloimpl\"\n    address=\"http://0.0.0.0:18080/HelloWorld\" /&gt;\n\n&lt;bean id=\"helloimpl\" class=\"com.tpt.fuse.esb.HelloWorldImpl\"&gt;&lt;/bean&gt;\n\n&lt;jaxws:endpoint id=\"JMSEndpoint\" address=\"jms://\"\n    serviceName=\"HelloWorld\" endpointName=\"HelloWorld\" implementor=\"#helloimpl\"&gt;\n    &lt;jaxws:features&gt;\n        &lt;bean class=\"org.apache.cxf.transport.jms.JMSConfigFeature\"&gt;\n            &lt;property name=\"jmsConfig\" ref=\"jmsConfig1\"&gt;&lt;/property&gt;\n        &lt;/bean&gt;\n    &lt;/jaxws:features&gt;\n&lt;/jaxws:endpoint&gt;\n\n&lt;bean id=\"jndiTemplate\" class=\"org.springframework.jndi.JndiTemplate\"&gt;\n    &lt;property name=\"environment\"&gt;\n        &lt;props&gt;\n            &lt;prop key=\"java.naming.factory.initial\"&gt;weblogic.jndi.WLInitialContextFactory&lt;/prop&gt;\n            &lt;prop key=\"java.naming.provider.url\"&gt;t3://localhost:7001&lt;/prop&gt;\n            &lt;prop key=\"java.naming.security.principal\"&gt;user01&lt;/prop&gt;\n            &lt;prop key=\"java.naming.security.credentials\"&gt;password01&lt;/prop&gt;\n        &lt;/props&gt;\n    &lt;/property&gt;\n&lt;/bean&gt;\n\n&lt;bean id=\"jmsConfig1\" class=\"org.apache.cxf.transport.jms.JMSConfiguration\"&gt;\n    &lt;property name=\"connectionFactory\"&gt;\n        &lt;bean class=\"org.springframework.jms.connection.SingleConnectionFactory\"&gt;\n            &lt;property name=\"targetConnectionFactory\"&gt;\n                &lt;ref local=\"jmsFactory\" /&gt;\n            &lt;/property&gt;\n        &lt;/bean&gt;\n    &lt;/property&gt;\n\n    &lt;property name=\"targetDestination\" value=\"Queue-4\" /&gt;\n    &lt;property name=\"replyDestination\" value=\"Queue-5\" /&gt;    \n    &lt;property name=\"destinationResolver\" ref=\"jmsDestinationResolver\"/&gt;\n&lt;/bean&gt;\n\n&lt;bean id=\"jmsFactory\" class=\"org.springframework.jndi.JndiObjectFactoryBean\"&gt;\n    &lt;property name=\"jndiTemplate\" ref=\"jndiTemplate\" /&gt;\n    &lt;property name=\"jndiName\" value=\"ConnectionFactory-0\" /&gt;\n&lt;/bean&gt;\n</code></pre>\n\n<p>The response time when calling this service 'echo' with different message size is as follows:</p>\n\n<p>MessageSize->ResponseTime</p>\n\n<p>1KB -> 30ms</p>\n\n<p>2KB -> 1049ms</p>\n\n<p>4KB -> 1078ms</p>\n\n<p>8KB -> 1100ms</p>\n\n<p>16KB -> 34ms</p>\n\n<p>32KB -> 36ms</p>\n\n<p>64KB -> 36ms</p>\n\n<p>128KB -> 38ms</p>\n\n<p>Messages with size greater than 8KB or less than 2KB get processed quickly.</p>\n\n<p>Why is there a performance hit only for messages having size 2,4 and 8 KB? \nDo I have to set/reset any buffer size?</p>\n"},{"tags":["c#","performance","reflection"],"answer_count":2,"favorite_count":3,"up_vote_count":5,"down_vote_count":0,"view_count":149,"score":5,"question_id":11243907,"title":"How to read and set private fields without a performance hit in c#?","body":"<p>RavenDB can serialize all public, private and protected properties on any given C# object, even without any serialization attributes.  </p>\n\n<p>I know that this can be done with reflection, but that would have some performance issues. Are there other more efficient ways to achieve this? </p>\n\n<p>To use the built-in serialization in .NET requires annotations or implementing some serialization interfaces, whereas RavenDB can serialize any given object to and from JSON.</p>\n\n<p>I'm more interested in technology it uses and not RavenDB in specific.  </p>\n"},{"tags":["java","performance","file-io","real-time","nio"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":55,"score":2,"question_id":12634163,"title":"Is there a performance advantage in writing a long file sequentially using MappedByteBuffer over a plain FileChannel?","body":"<p>I am trying to understand what would be the difference between calling FileChannel.write in short successions with a 16k buffer AND mapping multiple ByteBuffers with an append size of 16k as described here: <a href=\"http://stackoverflow.com/a/7367952/962872\">http://stackoverflow.com/a/7367952/962872</a></p>\n\n<p>I would think that the mapped bytebuffer approach produces a lot of garbage as you discard the MappedByteBuffers after each append. And I am not sure it is faster either. And you still have to do a bunch of mapping operations... (one per append).</p>\n\n<p>Or perhaps you should map a huge ByteBuffer (as big as possible) and just keep writing to this MappedByteBuffer?</p>\n\n<p>I am using the FileChannel.write approach with a Java-side 16kb buffer as a \"fast\" way to write a file but I want to make sure I am not missing something faster/better.</p>\n\n<p>Can anyone shed a light?</p>\n"},{"tags":["performance","xaml","refresh","designer"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":19,"score":0,"question_id":12635678,"title":"How to stop auto refresh in Visual Studio XAML designer","body":"<p>I want to stop XAML designer from refreshing it self. Because it takes to much time when ever i switch between pages.</p>\n\n<p>Is it possible?\nThanks.</p>\n"},{"tags":["php","performance","apache2"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":36,"score":0,"question_id":12630516,"title":"Apache2 dont run on 100%","body":"<p>I am using apache2 on windows (on localhost) and I have a lot of information to process , but when I run the php file it runs for days.\nTask manages says that processor idle is about 90% and more.</p>\n\n<p>Is there any way to tell apache server to run 100% ?</p>\n"},{"tags":["performance","gcc","c99","aliasing"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":41,"score":2,"question_id":12634844,"title":"Examples of strict aliasing of pointers in GCC C99, no performance differences","body":"<p>I'm trying to understand the impact of strict aliasing on performance in C99. My goal is to optimize a vector dot product, which takes up a large amount of time in my program (profiled it!). I thought that aliasing could be the problem, but the following code doesn't show any substantial difference between the standard approach and the strict aliasing version, even with vectors of size 100 million. I've also tried to use local variables to avoid aliasing, with similar results.</p>\n\n<p>What's happening?</p>\n\n<p>I'm using gcc-4.7 on OSX 10.7.4. Results are in microseconds.</p>\n\n<pre><code>$ /usr/local/bin/gcc-4.7 -fstrict-aliasing -Wall -std=c99 -O3 -o restrict restrict.c\n$ ./restrict\nsum:    100000000   69542\nsum2:   100000000   70432\nsum3:   100000000   70372\nsum4:   100000000   69891\n$ /usr/local/bin/gcc-4.7 -Wall -std=c99 -O0 -fno-strict-aliasing -o restrict restrict.c\n$ ./restrict\nsum:    100000000   258487\nsum2:   100000000   261349\nsum3:   100000000   258829\nsum4:   100000000   258129\n</code></pre>\n\n<p>restrict.c (note this code will need several hundred MB RAM):</p>\n\n<pre><code>#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;time.h&gt;\n#include &lt;sys/time.h&gt;\n#include &lt;unistd.h&gt;\n\n/* original */\nlong sum(int *x, int *y, int n)\n{\n   long i, s = 0;\n\n   for(i = 0 ; i &lt; n ; i++)\n      s += x[i] * y[i];\n\n   return s;\n}\n\n/* restrict */\nlong sum2(int *restrict x, int *restrict y, int n)\n{\n   long i, s = 0;\n\n   for(i = 0 ; i &lt; n ; i++)\n      s += x[i] * y[i];\n\n   return s;\n}\n\n/* local restrict */\nlong sum3(int *x, int *y, int n)\n{\n   int *restrict xr = x;\n   int *restrict yr = y;\n   long i, s = 0;\n\n   for(i = 0 ; i &lt; n ; i++)\n      s += xr[i] * yr[i];\n\n   return s;\n}\n\n/* use local variables */\nlong sum4(int *x, int *y, int n)\n{\n   int xr, yr;\n   long i, s = 0;\n\n   for(i = 0 ; i &lt; n ; i++)\n   {\n      xr = x[i];\n      yr = y[i];\n      s += xr * yr;\n   }\n\n   return s;\n}\n\nint main(void)\n{\n   struct timeval tp1, tp2;\n   struct timezone tzp;\n\n   long i, n = 1e8L, s;\n   int *x = malloc(sizeof(int) * n);\n   int *y = malloc(sizeof(int) * n);\n   long elapsed1;\n\n   for(i = 0 ; i &lt; n ; i++)\n      x[i] = y[i] = 1;\n\n   gettimeofday(&amp;tp1, &amp;tzp);\n   s = sum(x, y, n);\n   gettimeofday(&amp;tp2, &amp;tzp);\n   elapsed1 = (tp2.tv_sec - tp1.tv_sec) * 1e6\n      + (tp2.tv_usec - tp1.tv_usec);\n   printf(\"sum:\\t%ld\\t%ld\\n\", s, elapsed1);\n\n   gettimeofday(&amp;tp1, &amp;tzp);\n   s = sum2(x, y, n);\n   gettimeofday(&amp;tp2, &amp;tzp);\n   elapsed1 = (tp2.tv_sec - tp1.tv_sec) * 1e6\n      + (tp2.tv_usec - tp1.tv_usec);\n   printf(\"sum2:\\t%ld\\t%ld\\n\", s, elapsed1);\n\n   gettimeofday(&amp;tp1, &amp;tzp);\n   s = sum3(x, y, n);\n   gettimeofday(&amp;tp2, &amp;tzp);\n   elapsed1 = (tp2.tv_sec - tp1.tv_sec) * 1e6\n      + (tp2.tv_usec - tp1.tv_usec);\n   printf(\"sum3:\\t%ld\\t%ld\\n\", s, elapsed1);\n\n   gettimeofday(&amp;tp1, &amp;tzp);\n   s = sum3(x, y, n);\n   gettimeofday(&amp;tp2, &amp;tzp);\n   elapsed1 = (tp2.tv_sec - tp1.tv_sec) * 1e6\n      + (tp2.tv_usec - tp1.tv_usec);\n   printf(\"sum4:\\t%ld\\t%ld\\n\", s, elapsed1);\n\n   return EXIT_SUCCESS;\n}\n</code></pre>\n"},{"tags":[".net","windows","performance","profiling","monitoring"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":95,"score":0,"question_id":12629514,"title":"Perfmon, PerfMonitor, and PerfView","body":"<p>A <a href=\"http://msdn.microsoft.com/en-us/magazine/gg490356.aspx\" rel=\"nofollow\">couple</a> <a href=\"http://brokenbitsslowbytes.blogspot.com/2011/12/perfmonitor-primer.html\" rel=\"nofollow\">articles</a> talk about ETW and \"PerfMonitor.exe\".  The articles discuss the ability to launch profiling from the command prompt.  For example:</p>\n\n<pre><code>PerfMonitor.exe runAnalyze MySlowApplication.exe\n</code></pre>\n\n<p>Somehow I had or have the impression that PerfMon.exe is the self-same tool.  But when I try to use command line options with it, it doesn't work.  So perhaps they are indeed two different tools?  Or is one a newer version of the other (with or without command-line capabilities removed)?</p>\n\n<p>Then there is the <a href=\"http://channel9.msdn.com/Series/PerfView-Tutorial\" rel=\"nofollow\">PerfView</a> tool. I'm wondering if it is a newer version of either perfmonitor or perfmon?  Or is it simply a tool unto-its-own?  Finally, is there a breakdown of the features offered by these tools? </p>\n"},{"tags":["database","performance","algorithm","search-engine","relationship"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":57,"score":0,"question_id":12632398,"title":"Algorithm to quickly map out relationships among millions of objects","body":"<p>I have a relational database table with millions of rows, each is linked to hundreds of  rows within this table. It a simple relationship but becomes problematic when there are millions of rows. Each time a new row is added it has to scan the entire range.</p>\n\n<p>Is there a more efficient way to perform this operation?</p>\n\n<p>I'm sure search engines have solved this problem already on an even larger scale. (Is there a term for this kind of problems?)</p>\n"},{"tags":["performance","parallel-processing","cpu","sse"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":28,"score":0,"question_id":12633266,"title":"How to measure how many float operations was made?","body":"<pre><code>float res[size], a[size], b[size]\n//Several initialization is done\nfor(int i=0; i&lt;size; i++) res[i] = a[i]*b[i];\n</code></pre>\n\n<p>I wonder how to measure number of the float operations made. </p>\n\n<p>The machine:\nmodel name  : Intel(R) Core(TM)2 Quad CPU           @ 2.93GH</p>\n\n<p>The Compiler:\n'gcc version 4.6.1 (Ubuntu/Linaro 4.6.1-9ubuntu3)'</p>\n\n<p>The architecture: x86_64.</p>\n\n<p>People say that sse option will be enabled automatically since gcc 4. While, there are comments that multiplication and division are not supported as for sse.</p>\n\n<p>I did not recognize that whether sse is enabled if I just write code like above. And anybody could tell me how to calculate the number of float operations made?</p>\n\n<p>What's more, what if the float is substituted for double?</p>\n\n<p>I wonder whether the equation is:</p>\n\n<p>For float: 2.93 * 4 \nFor Double: 2.93 * 2</p>\n\n<p>Actually, I found there are 8 more new  registers in x86_64. Will this lead to 8 float operations and 4 double operations at one time? </p>\n"},{"tags":["php","performance","comments"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":55,"score":3,"question_id":12633184,"title":"Does PHP parse through code with // comments or line breaks more quickly?","body":"<p>Which does php parse more quickly:</p>\n\n<ol>\n<li><code>//</code></li>\n<li><code>//</code></li>\n<li><code>//</code></li>\n<li><code>//</code></li>\n<li><code>//</code></li>\n</ol>\n\n<p>or:</p>\n\n<ol>\n<li><code></code></li>\n<li><code></code></li>\n<li><code></code></li>\n<li><code></code></li>\n<li><code></code></li>\n</ol>\n\n<p>In other words, is it extremely marginal the speed it takes to parse a <code>//</code> comment, as compared to the time it takes to pass through an empty line?</p>\n"},{"tags":["javascript","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":10,"view_count":154,"score":-9,"question_id":8821163,"title":"How do I time Javacript code?","body":"<p>My processor runs at 2.0 GHz.</p>\n\n<p>I have a new computer with as much software removed as possible except my development tools.  This system is clean with no malware.</p>\n\n<p>If I run the code below I get about 2M loops per second.  That is about 1 MHz.  </p>\n\n<p>Suppose doing and addition, and doing a compare takes 10x the simplest operation, I get about 10 MHz</p>\n\n<p>Why do I not get more utilization of my processor?</p>\n\n<pre><code>var Utility = \n{\n    time: function() \n    {\n        var end_time,\n            start_time,\n            index = 0;\n\n        start_time = new Date().getTime();\n\n        while ( index &lt;= 1000000 )\n        {\n            index++;\n        }\n\n        end_time = new Date().getTime();\n\n    return ( end_time - start_time);\n    }\n};\n</code></pre>\n"},{"tags":["performance","gcc","mingw","gprof"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":28,"score":0,"question_id":12632556,"title":"Something weird in performance analysis","body":"<p>I used gprof to analyze the program's performance, and found something weird.\nThe profiling results said:\ngranularity: each sample hit covers 4 byte(s) for 0.21% of 4.66 seconds\nBut the actually running time was 28 seconds.</p>\n\n<p>So how to understand this huge difference? Could it be possible to be affected by Anti-Virus software? But I don't have the authority to close the AV SW to verify that.</p>\n\n<p>BR, Ruochen</p>\n"},{"tags":["performance","algorithm","runtime"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":27,"score":0,"question_id":12632497,"title":"Running Times of Algorithms","body":"<p>I need to run a segment of code like shown:  </p>\n\n<pre><code> sum = 0;\n for(i = 0; i &lt; n; i++)\n     for(j = 0; j &lt; i; j++)\n         sum++;\n</code></pre>\n\n<p>and find the running times of several values of N and the growth rate associated with those values.</p>\n\n<p>Right now I am using Dev C++ (could use any IDE), but I can't figure out a way to actually display the run times.</p>\n"},{"tags":["django","performance","postgresql","limit","queryset"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":89,"score":0,"question_id":12576115,"title":"Django/Postgres: Performance issues with LIMIT querysets","body":"<p>I have came across a strange query performance issue that I am struggling to understand.</p>\n\n<p>The following is a simplified version of the model structure I have, hopefully it will be enough to illustrate the issue:</p>\n\n<pre><code>class Note(models.Model):\n   ...\n   name = models.CharField(max_length=50)\n   parentNote = models.ForeignKey('self', null=True)\n   form = models.ForeignKey('NoteForm', null=True)\n   ...\n\nclass Event(Note):\n   ...\n   startDate = models.DateField()\n   ...\n\nclass Activity(Event):\n   ...\n</code></pre>\n\n<p>The <code>Activity</code> model is the source of the issue I am facing. It has an extensive inheritance heirarchy, none of which is abstract. I do not know if this contributes to the issue. <code>Activity</code> has ~280000 records and, obviously, its parents have at least that many, if not more.</p>\n\n<p>The NoteForm model is not described above - it is only necessary to know that it is external to the <code>Activity</code> model's hierarchy and contains less than 100 records.</p>\n\n<p>I am using Django version 1.3.</p>\n\n<p>The problem occurs when querying for the latest \"child\" Activity of some parent Activity. The query filters by the <code>parentNote</code> field, orders by the 'startDate' field (descending) and uses Python's index notation to select the first result (which, by my understanding, simply adds <code>LIMIT 1</code> to the generated SQL). See below for the code.</p>\n\n<p>This query runs unexpectedly slowly <em>when no results are found</em> - 10+ seconds. If results are found, it runs as expected - well under 1 second.</p>\n\n<p>Further investigation revealed the following:</p>\n\n<ul>\n<li>It is the limit causing the issue. Just doing the filter, without limiting to the first result, is not slow - whether results are found or not.</li>\n<li>Ordering is partially a culprit. Removing the ordering removes the issue.</li>\n<li>The <code>parentNote</code> filter is partially a culprit. Changing the filter to use the <code>form</code> or <code>name</code> field removes the issue.</li>\n</ul>\n\n<p>In code:</p>\n\n<pre><code># Original - SLOW\ntry:\n    latest = Activity.objects.filter(\n        parentNote=activity.pk\n    ).order_by('-startDate')[0]\nexcept IndexError:\n    latest = None\n\n# FAST\n\n# No limit\nActivity.objects.filter(\n    parentNote=activity.pk\n).order_by('-startDate')\n\n# No ordering\ntry:\n    latest = Activity.objects.filter(\n        parentNote=activity.pk\n    )[0]\nexcept IndexError:\n    latest = None\n\n# Different filter\ntry:\n    latest = Activity.objects.filter(\n        form=activity.pk\n    ).order_by('-startDate')[0]\nexcept IndexError:\n    latest = None\n\n# Different filter\ntry:\n    latest = Activity.objects.filter(\n        name=activity.pk\n    ).order_by('-startDate')[0]\nexcept IndexError:\n    latest = None\n</code></pre>\n\n<p>If the issue is at the database level, I can't see it. I've run the \"Original\" and \"No Limit\" examples from above in the <code>django-debug-toolbar</code>'s <code>debugsqlshell</code>. The \"Original\" took 16 seconds and \"No Limit\" took 59ms. I copied both queries printed by the <code>debugsqlshell</code> and ran them in pgAdmin. \"Original\" took 1375ms and \"No Limit\" took 94ms. So it is slower, but not by the amount I'm seeing using the ORM. <code>EXPLAIN ANALYZE</code> definitely shows the query analyzer taking different paths, which I completely understand. But I cannot reproduce the 16 second query using SQL directly.</p>\n\n<p>So, in summary:</p>\n\n<ul>\n<li>I am seeing LIMIT queries running far slower than identical queries without the LIMIT, but only when no results are found.</li>\n<li>Queries that return results do not run slowly - and they are identical apart from the values of the filters.</li>\n<li>It appears to be a function of which fields are included in the filters, and whether or not the queryset is ordered.</li>\n<li>It does NOT appear to be a database level issue as running the SQL directly does not run slowly.</li>\n</ul>\n\n<p><strong>Update:</strong></p>\n\n<p>While trying suggestions made in the comments, the above examples suddenly ceased suffering from this issue - before I found any evidence as to the cause, let alone implemented a fix. I still have no idea what the problem was, but now I do not have a means to reproduce it in order to further investigate.</p>\n"},{"tags":["java","multithreading","performance","concurrency"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":63,"score":1,"question_id":12631078,"title":"Weak performance of CyclicBarrier with many threads: Would a tree-like synchronization structure be an alternative?","body":"<p>Our application requires all worker threads to synchronize at a defined point. For this we use a <code>CyclicBarrier</code>, but it does not seem to scale well. With more than eight threads, the synchronization overhead seems to outweigh the benefits of multithreading. (However, I cannot support this with measurement data.)</p>\n\n<p><strong>EDIT</strong>: Synchronization happens very frequently, in the order of 100k to 1M times.</p>\n\n<p>If synchronization of many threads is \"hard\", would it help building a synchronization tree? Thread 1 waits for 2 and 3, which in turn wait for 4+5 and 6+7, respectively, etc.; after finishing, threads 2 and 3 wait for thread 1, thread 4 and 5 wait for thread 2, etc..</p>\n\n<pre><code>1\n| \\\n2   3\n|\\  |\\\n4 5 6 7\n</code></pre>\n\n<p>Would such a setup reduce synchronization overhead? I'd appreciate any advice.</p>\n\n<p>See also this featured question: <a href=\"http://stackoverflow.com/q/2712232/946850\">What is the fastest cyclic synchronization in Java (ExecutorService vs. CyclicBarrier vs. X)?</a></p>\n"},{"tags":["regex","performance","oracle","plsql","query-optimization"],"answer_count":3,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":110,"score":3,"question_id":12578269,"title":"I want to optimize a stored procedure that uses IN clause and a regex_str function. I am not sure that how I should optimize it more?","body":"<p>The response time I am getting is around 200ms.\nI want to optimize it more.\nHow can I achieve this?</p>\n\n<pre><code>CREATE OR REPLACE\nPROCEDURE GETSTORES\n(\nLISTOFOFFERIDS IN VARCHAR2,\nREF_OFFERS OUT TYPES.OFFER_RECORD_CURSOR\n)\nAS\nBEGIN\n  OPEN REF_OFFERS FOR\n  SELECT\n  /*+ PARALLEL(STORES 5) PARALLEL(MERCHANTOFFERS 5)*/\n  MOFF.OFFERID,\n  s.STOREID,\n  S.LAT,\n  s.LNG\n  FROM\n  MERCHANTOFFERS MOFF\n  INNER JOIN STORES s ON MOFF.STOREID =S.STOREID\n  WHERE\n  MOFF.OFFERID IN\n  (\n    SELECT\n      REGEXP_SUBSTR(LISTOFOFFERIDS,'[^,]+', 1, LEVEL)\n    FROM\n      DUAL CONNECT BY REGEXP_SUBSTR(LISTOFOFFERIDS, '[^,]+', 1, LEVEL) IS NOT NULL\n  )\n  ;\nEND\nGETSTORES;\n</code></pre>\n\n<p>I am using the regex_substr to get a list of OfferIDs from the comma separated string that comes in LISTOFOFFERIDS.\nI have created the index on STOREID of the Stores table but to no avail.\nA new approach to achieve the same is also fine if its faster.</p>\n\n<p>The types declaration for the same: </p>\n\n<pre><code>create or replace\n\n    PACKAGE TYPES\n    AS\n    TYPE OFFER_RECORD\n    IS\n      RECORD(\n      OFFER_ID MERCHANTOFFERS.OFFERID%TYPE,\n      STORE_ID STORES.STOREID%TYPE,\n      LAT STORES.LAT%TYPE,\n      LNG STORES.LNG%TYPE\n      );\n    TYPE OFFER_RECORD_CURSOR\n    IS\n      REF\n      CURSOR\n        RETURN OFFER_RECORD;\n      END\n      TYPES;\n</code></pre>\n\n<p>The plan for the select reveals following information:</p>\n\n<pre><code>Plan hash value: 1501040938\n\n-------------------------------------------------------------------------------------------------------------\n| Id  | Operation                                  | Name           | Rows  | Bytes | Cost (%CPU)| Time     |\n-------------------------------------------------------------------------------------------------------------\n|   0 | SELECT STATEMENT                           |                |   276 | 67620 |    17  (12)| 00:00:01 |\n|*  1 |  HASH JOIN                                 |                |   276 | 67620 |    17  (12)| 00:00:01 |\n|   2 |   NESTED LOOPS                             |                |       |       |            |          |\n|   3 |    NESTED LOOPS                            |                |   276 | 61272 |     3  (34)| 00:00:01 |\n|   4 |     VIEW                                   | VW_NSO_1       |     1 |   202 |     3  (34)| 00:00:01 |\n|   5 |      HASH UNIQUE                           |                |     1 |       |     3  (34)| 00:00:01 |\n|*  6 |       CONNECT BY WITHOUT FILTERING (UNIQUE)|                |       |       |            |          |\n|   7 |        FAST DUAL                           |                |     1 |       |     2   (0)| 00:00:01 |\n|*  8 |     INDEX RANGE SCAN                       | OFFERID_INDEX  |   276 |       |     0   (0)| 00:00:01 |\n|   9 |    TABLE ACCESS BY INDEX ROWID             | MERCHANTOFFERS |   276 |  5520 |     0   (0)| 00:00:01 |\n|  10 |   TABLE ACCESS FULL                        | STORES         |  9947 |   223K|    13   (0)| 00:00:01 |\n-------------------------------------------------------------------------------------------------------------\n\nPredicate Information (identified by operation id):\n---------------------------------------------------\n\n   1 - access(\"MERCHANTOFFERS\".\"STOREID\"=\"STORES\".\"STOREID\")\n   6 - filter( REGEXP_SUBSTR ('M1-Off2,M2-Off5,M2-Off9,M5-Off4,M10-Off1,M1-Off3,M2-Off4,M3-Off2,M4-Of\n              f6,M5-Off1,M6-Off1,M8-Off1,M7-Off3,M1-Off1,M2-Off1,M3-Off1,M3-Off4,M3-Off5,M3-Off6,M4-Off1,M4-Off7,M2\n              -Off2,M3-Off3,M5-Off2,M7-Off1,M7-Off2,M1-Off7,M2-Off3,M3-Off7,M5-Off5,M4-Off2,M4-Off3,M4-Off5,M8-Off2\n              ,M6-Off2,M1-Off5,M1-Off6,M1-Off9,M1-Off8,M2-Off6,M2-Off7,M4-Off4,M9-Off1,M6-Off4,M1-Off4,M1-Off10,M2-\n              Off8,M3-Off8,M6-Off3,M5-Off3','[^,]+',1,LEVEL) IS NOT NULL)\n   8 - access(\"MERCHANTOFFERS\".\"OFFERID\"=\"$kkqu_col_1\")\n</code></pre>\n"},{"tags":["c#","asp.net","performance","flickr-api"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":40,"score":0,"question_id":12632054,"title":"Flickr.NET performance problems when getting photos","body":"<p>I'm currently working on an ASP.NET site which needs to show photos from Flickr as multiple slideshows/galleries on the frontpage. I am using the Flickr.NET library to interact with Flickr.</p>\n\n<p>My Flickr structure is as follows:</p>\n\n<ul>\n<li>2012 (Collection)\n<ul>\n<li>Artists (Collection)\n<ul>\n<li>Artist1 (Set)\n<ul>\n<li>Photo1</li>\n<li>Photo2</li>\n<li>Photo3</li>\n</ul></li>\n</ul></li>\n</ul></li>\n</ul>\n\n<p>And so on, so forth. There can, of course, be more childcollections and more sets under these collections.</p>\n\n<p>On the page I'm only interested in showing the sets (the collection is basically just for structuring the galleries so it's easier for the photographers to work with in Lightroom) I've written a recursive method to loop through the collections getting their sets. Code:</p>\n\n<pre><code>private static List&lt;CollectionSet&gt; _photoSets;\npublic void GetFlickrPhotosetsRecursive(List&lt;Collection&gt; collections)\n    {\n        // Loop through the collections\n        foreach (var collection in collections)\n        {\n            // Base case: this collection has no childcollections and have got at least one set\n            if (collection.Collections.Count == 0 &amp;&amp; collection.Sets.Count &gt; 0)\n            {\n                _photoSets.AddRange(collection.Sets);\n            }\n            else\n            {\n                // Check if this collection has got any sets\n                if (collection.Sets.Count &gt; 0)\n                    _photoSets.AddRange(collection.Sets);\n\n                // Call method again with child collections\n                GetFlickrPhotosetsRecursive(collection.Collections.ToList());\n            }   \n        }\n    }\n</code></pre>\n\n<p>This runs smooth without any performance problems at all. Reason for this is, that Collections, child collections and sets are loaded in one call to the Flickr API. Now, when I need to get the photos of each of the sets, this causes some serious performanceproblems. It takes almost 10 seconds to load the page since each photo lookup is a call to the API. Code:</p>\n\n<pre><code>GetFlickrPhotosetsRecursive(collections.ToList());\n\nreturn _photoSets.Select(set =&gt; new FlickrGallery()\n                                {\n                                     PhotoCollection = GetPhotosFromPhotoSet(set.SetId),\n                                     SetId = set.SetId, \n                                     Title = set.Title\n                                 }).ToList();\n\npublic PhotosetPhotoCollection GetPhotosFromPhotoSet(string setId)\n{\n    return _flickr.PhotosetsGetPhotos(setId);\n}\n</code></pre>\n\n<p>The FlickrGallery object is an object of mine to store the information I need on the frontend.</p>\n\n<p>So, does anyone know of a better way to do this to prevent huge load times? This is still proof of concept and there's only 30 sets with 1 - 5 photos each. Can't imagine the load time when it goes live with 80+ sets with 40+ photos in each set :-)</p>\n\n<p>Any help/hint on this is greatly appreciated!</p>\n\n<p>Thanks a lot in advance.</p>\n\n<p>All the best,</p>\n\n<p>Bo</p>\n"},{"tags":["ruby-on-rails","ruby","ruby-on-rails-3","performance","optimization"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":73,"score":2,"question_id":12630561,"title":"Improve performance of this server code to look up domains?","body":"<p>We use Ruby to check domain availability, but the code runs painfully slow. We caching the Whois object across requests would accelerate performance, but other posts on SO suggest this would only marginally increase performance, if at all.</p>\n\n<p>The code is so simple. We're not sure if other improvements could be made, or if we're just stuck with slow lookups because of the Whois object.</p>\n\n<p>The <strong>bulk_check</strong> method accepts an array of domains to look up. Everything else is fairly self-explanatory.</p>\n\n<p>We're on Rails 3.</p>\n\n<p>Any suggestions?</p>\n\n<p>Thanks!</p>\n\n<pre><code>def bulk_check\n    domains = params[:domains] || '[]'\n    callback = params[:callback] || ''      \n    results = []\n    threads = []\n\n    # String -&gt; Array\n    domains = ActiveSupport::JSON.decode domains\n\n    # Iterate over each domain and spawn new thread to check domain status\n    domains.each do |d|\n      threads &lt;&lt; Thread.new(d) { |my_domain|\n        Thread.current['domain'] = my_domain\n        Thread.current['status'] = get_domain_status my_domain\n      }\n    end\n\n    # Wait for threads to finish and update results array\n    threads.each do |t| \n        t.join\n        results.push [ t['domain'], t['status'] ]\n    end\n\n    # Render results\n    respond_to do |type|\n        type.json { render :json =&gt; { :results =&gt; results }.to_json, :callback =&gt; callback }\n    end\nend\n\n\ndef get_domain_status domain\n    begin\n        # Create Whois object\n        whois = Whois::Client.new\n\n        # Query Whois for domain data\n        result = whois.query domain\n\n        # Prep JSON response\n        status = result.available? ? 'available' : 'taken'\n    rescue Exception =&gt; e\n        puts \"Exception in parsing '#{domain}' status: #{e.message}\"\n        status = 'error'\n    end\n\n    # Return status\n    return status\nend\n</code></pre>\n"},{"tags":["android","performance","deployment"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":56,"score":1,"question_id":12631380,"title":"Slow Deployment","body":"<p>My current game project is getting large enough that when I deploy for testing I'm waiting a few minutes to actually get this to my phone. Sometimes I have other things to work on, but sometimes I have to sit and wait. This is unfortunately slowing down the development process.</p>\n\n<p>I'm already sure the biggest factor in this is package size. My package is now just under 18 Megs, but I know there are apps out there with much larger packages.</p>\n\n<p>Is there any tricks, phone settings, or eclipse settings that might speed deployment other than reducing the package size?</p>\n\n<p>Regards.</p>\n"},{"tags":["android","performance","onclicklistener"],"answer_count":2,"favorite_count":5,"up_vote_count":11,"down_vote_count":0,"view_count":112,"score":11,"question_id":12600394,"title":"Which One Is Better Performance Wise: setOnClickListener VS android:onclick=\"onClick\"","body":"<p>In Android we have 2 ways to set an onClick event for a buttom (or any other view I think):</p>\n\n<p><strong>Scenario one (programmatically):</strong></p>\n\n<pre><code>Button b = (Button) findViewById(R.id.mybutton);\nb.setOnClickListener(this);\n</code></pre>\n\n<p><strong>Scenario two (in the XML file):</strong></p>\n\n<pre><code>&lt;Button android:onClick=\"handler\" /&gt;\n</code></pre>\n\n<p>Is there any performance penalty for doing this in XML or programmatically or is it the same?</p>\n"},{"tags":["c#","asp.net","performance","loops"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":131,"score":3,"question_id":12629319,"title":"Which is more efficient, looping through DataTable or more database calls?","body":"<p>Basically I have a <code>DataTable</code> with a rows containing part numbers and a couple of columns that contain information on those parts.</p>\n\n<p>In order to compare those infos with the data we have in the database, I have determined I have one of two options.</p>\n\n<p><strong>Option 1</strong> - Loop through each row and SELECT the data</p>\n\n<pre><code>void CompareData(DataTable dt) {\n    foreach (DataRow entry in dt.Rows) {\n        //select that row\n        DataRow dbEntry = ExecuteQuery(\"SELECT * FROM Parts WHERE partno='\" + entry[\"partno\"] + \"'\").Rows[0];\n        if (dbEntry[\"info1\"] == entry[\"info1\"]) {\n            //do something\n        } else {\n            //do something\n        }\n    }\n}\n</code></pre>\n\n<p><strong>Option 2</strong> - SELECT all data at once and compare via loops</p>\n\n<pre><code>void CompareData(DataTable dt, string[] parts) {\n    DataTable dbEntries = ExecuteQuery(\"SELECT * FROM Parts WHERE partno IN('\" + String.Join(parts, \"','\") + \"')\");\n    foreach (DataRow entry in dt.Rows) {\n        foreach (DataRow dbEntry in dt.Rows) {\n            if (dbEntry[\"partno\"] == entry[\"partno\"]) {\n                if (dbEntry[\"info1\"] == entry[\"info1\"]) {\n                    //do something\n                } else {\n                    //do something\n                }\n            }\n        }\n    }\n}\n</code></pre>\n\n<p>They both seem pretty inefficient, so I'm not really sure what to do.  Would LINQ speed this process up? I've never really used it but just browsing around it looks like something that could help.</p>\n"},{"tags":["php","xml","performance","parsing","profiling"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":131,"score":2,"question_id":11983841,"title":"Decrease processing time in parsing large xml file in php","body":"<p>I have this problem in terms of processing time of a large xml files. By large, i mean 600MB on the average.\nCurrently, It takes about 50 - 60 minutes to parse and insert the data into a database.\nI would like to ask for suggestions on how can I improve the processing time? Like goind down to 20 minutes.</p>\n\n<p>Because with the current time it will take me 2.5 months to populate the database with the content from the xml. By the way I have 3000+ xml files with the average of 600mb. And my php script in command line thru cron job.</p>\n\n<p>I have also read other questions like the one below, but I have not found any idea yet.\n<a href=\"http://stackoverflow.com/questions/3048583/what-is-the-fastest-xml-parser-in-php?rq=1\">What is the fastest XML parser in PHP?</a></p>\n\n<p>I see that some have parsed files up to 2GB. I wonder how long are the processing time.</p>\n\n<p>I hope you guys could lend your help.\nIt would be much appreciated.\nThanks.</p>\n\n<p>I have this code:</p>\n\n<pre><code>$handler = $this;\n$parser = xml_parser_create('UTF-8');\nxml_set_object($parser, $handler);\nxml_parser_set_option($parser, XML_OPTION_CASE_FOLDING, false);\nxml_set_element_handler($parser, \"startElement\", \"endElement\");\nxml_set_character_data_handler($parser, \"cdata\");\n\n$fp = fopen($xmlfile, 'r');\n\nwhile (!feof($fp)) {\n    while (($data = fread($fp, 71680))){\n\n    }\n}\n</code></pre>\n\n<p>I put first the parse data in a temporary array.\nMy mysql insert commands are inside the endElement function.\nThere is a specific closing tag to trigger my insert command to the database.</p>\n\n<p>Thanks for the response....</p>\n"},{"tags":["c++","performance","eigen"],"answer_count":5,"favorite_count":3,"up_vote_count":5,"down_vote_count":0,"view_count":923,"score":5,"question_id":10366054,"title":"C++ performance in eigen library","body":"<p>I was looking over the performance benchmarks:</p>\n\n<p><a href=\"http://eigen.tuxfamily.org/index.php?title=Benchmark\" rel=\"nofollow\">http://eigen.tuxfamily.org/index.php?title=Benchmark</a></p>\n\n<p>And I could not help but notice that eigen appears to consistently outperform all the specialized vendor libraries.  The questions is: how is it possible?  One would assume that mkl/goto would use processor specific tuned code, while eigen is rather generic.</p>\n\n<p>Notice this <a href=\"http://download.tuxfamily.org/eigen/btl-results-110323/aat.pdf\" rel=\"nofollow\">http://download.tuxfamily.org/eigen/btl-results-110323/aat.pdf</a>, essentially a dgemm.  For N=1000 Eigen gets roughly 17Gf, MKL only 12Gf</p>\n"},{"tags":["xcode","performance","activity","xcode4","disk"],"answer_count":6,"favorite_count":9,"up_vote_count":8,"down_vote_count":0,"view_count":4840,"score":8,"question_id":6225603,"title":"Xcode 4 became extremely slow and kills my hard drive","body":"<p>My machine has 8 GB or RAM, Core 2 Duo 3,06 GHZ and it seems it is not enough for Xcode 4 (4.0.1).\nFrom some time now it started to behave more and more slow. Auto completion, editing code as well as Xib files became almost impossible to use. </p>\n\n<p>Other applications behave smoothly. </p>\n\n<p>Activity monitor shows a lot of RAM usage (still few GB left of completely free memory, so it is quite OK) and huge disk activity usage. I can see on the graph high peaks of data being saved periodically when I work in Xcode. After 3 hours of work there is 10.5 GB of data written to the disk. Is it normal?</p>\n\n<p>I have tried to disable auto-save but it did not help much.</p>\n\n<p>What can be the other causes of this extremely slow behavior of Xcode 4?</p>\n\n<p><strong>It did not behave like this from the beginning and it is not during Xcode indexing.</strong> To be even more interesting: when Xcode is indexing my project after cleaning it takes less CPU power than when indexing is done (CPU raises up after indexing again). Heh?</p>\n\n<p><strong>UPDATE:</strong> Complete reinstall of Snow Leopard (to the cleaned disk) together with Xcode did not help much. Xcode was working quite well just for a day or so and then slowed down again to the degree it is hardly possible to work with.</p>\n"},{"tags":["performance","stress-testing","qa"],"answer_count":13,"favorite_count":40,"up_vote_count":67,"down_vote_count":1,"view_count":29133,"score":66,"question_id":340564,"title":"Best way to stress test a website","body":"<p>This may be the wrong question to ask but, what's the best way to replicate a large load on an asp.net web application? Is there an easy way to simulate many requests on particular pages? Or is the best thing to use a profiler to track a single request and then work out from that if the performance is ok?</p>\n\n<p>It would be good to know how well a web app works with a server spec. I'd like to be able to simulate heavy traffic on my testing server so that I can work out if the production server is good enough (specifically with iis/asp.net not db performance). </p>\n"}]}
{"total":25593,"page":18,"pagesize":100,"questions":[{"tags":["performance","delphi","exe","pascal","createprocess"],"answer_count":5,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":295,"score":1,"question_id":12622425,"title":"Delphi: Fast alternative to CreateProcess","body":"<p>I'm looking for a fast alternative to CreateProcess in Delphi to execute certain calculations in an exe, including several return-values in XML.\nCurrently, I'm calling an C#-exe with certain parameters. One of these calls takes approx. 0.5s - which is <em>way</em> to expensive, since this exe needs to be called a couple of hundred times (unfortunatley iterative calls, i.e. multithreading won't speed up the job).</p>\n\n<p>My current code looks like this (found the solution to get the console output of the exe somwhere on StackOverflow).</p>\n\n<pre><code>IsExecutable := CreateProcess(\n            nil,\n            PChar(WorkDir + Exe + CommandLine),\n            nil,\n            nil,\n            True,\n            HIGH_PRIORITY_CLASS,\n            nil,\n            nil,\n            StartupInfo,\n            ProcessInformation);\nCloseHandle(StdOutPipeWrite);\n    if IsExecutable then\n      try\n        repeat\n          WasOK := ReadFile(StdOutPipeRead, Buffer, 255, BytesRead, nil);\n          if BytesRead &gt; 0 then\n          begin\n            Buffer[BytesRead] := #0;\n            Result := Result + Buffer;\n          end;\n        until not WasOK or (BytesRead = 0);\n        WaitForSingleObject(ProcessInformation.hProcess, INFINITE);\n      finally\n        CloseHandle(ProcessInformation.hThread);\n        CloseHandle(ProcessInformation.hProcess);\n      end\n</code></pre>\n\n<p>Btw, I'm not very good a Delphi - actually, I kinda feel like the \"I have no idea what I'm doing\" dog-meme-thing...</p>\n"},{"tags":["visual-studio-2008","performance","windows-7"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":432,"score":0,"question_id":5278388,"title":"Visual Studio 2008 SP1 on Windows 7 x86 is unusable","body":"<p>I'm running Visual Studio 2008 (Team System edition) on Windows 7 x86. This is a clean install of the OS, and clean install of VS, including SP1.</p>\n\n<p>I don't know if anyone has experienced this. The editor (not designers or anything else, just the text editor) is extremely unusable. By that I mean that editing text is a sort of herky-jerky experience where it seems there are certain actions that cause lag. Ctrl+Backspace for example, noticeable lag. Scrolling up or down a source file with the arrow keys also seems to lag.</p>\n\n<p>It's difficult to explain this \"lag\". It feels like the editor isn't keeping up with my typing speed.</p>\n\n<p>I've run VS2008 on Windows XP and Server 2003 (as a desktop) without this problem. On those the editor is as snappy as TextPad or any other, simpler editing application.</p>\n\n<p>Also on this same machine VS2010 does not exhibit the behavior. The editor works fine.</p>\n\n<p>In fact, the problem reminds me of the horrible performance of the VS2005 editor, which I had to stop using because it was unbearable. Most of my initial .NET 2.0 experience was obtained by using vim and a NAnt file, actually, until I upgraded to VS2008.</p>\n\n<p>This is what I've tried so far, without success:</p>\n\n<ul>\n<li>Turn off change tracking.</li>\n<li>Turn off delimiter highlighting (1)</li>\n<li>Turn off as much screen candy as possible (line numbers, selection margin, etc)</li>\n<li>Turn off the navigation bar (this was one of the recommended actions to fix VS2005).</li>\n<li>Turn off \"Underline errors in the editor\" and \"Show live semantic errors\"</li>\n<li>Turn off automatic formatting.</li>\n</ul>\n\n<p><em>(1) Is this a bug in VS? I turned it off and it continues to highlight braces!?</em></p>\n\n<p>I even installed a hotfix dated from March 2008 that supposedly fixed a problem with the HTML source editor that I never installed anywhere else before, but that didn't have any effect either.</p>\n\n<p>From the OS perspective I've tried turning off all animations, effects, themes and eye candy. Also to no avail. An interesting thing though, when running Win7 without themes the treeview in the solution explorer tends to flash uncontrollably when it gains or loses the focus. With eye candy turned on it stops flashing.</p>\n\n<p>The problem occurs in any type of project, with any number of files open or closed. It also happens with Intellisense turned off; however once I turn off Intellisense there isn't much point to using VS, so I'd like to keep it. But in any case it has no effect.</p>\n\n<p>Finally my question: <strong>Has anyone run into this problem? And if so, is it fixable? Does anyone know if MS is planning another VS2008 service pack to fix Win7 problems or something like that?</strong></p>\n\n<p>I write text 8-10 hours a day on this thing and I can't deal with these problems. I don't care about fancy designers and thingamajigs but at the very least I need the editor to perform reasonably.</p>\n\n<p><sub>(Note: To head off one line of reasoning here, I can't upgrade this project to VS2010 yet. Maybe in a few months but not now. So I need VS2008 to work <em>now</em>).</sub></p>\n\n<p><strong>Update</strong> This is C#, by the way. I haven't tried C++ or VB. I also tried switching fonts from ProggyClean (my fav) to Consolas to Courier New and back, no effect. Another thing: A symptom seems to be excessive flashing of the caret, especially when hitting Enter. It's like blink-blink-blink--blinkblinkblinkblink-blink-blink. If that makes sense.</p>\n"},{"tags":["mysql","query","indexing","performance","logging"],"answer_count":6,"favorite_count":4,"up_vote_count":9,"down_vote_count":0,"view_count":3272,"score":9,"question_id":1277865,"title":"MySQL indexes - how many are enough?","body":"<p>I'm trying to fine-tune my MySQL server so I check my settings, analyzing slow-query log, and simplify my queries if possible.</p>\n\n<p>Sometimes it is enough if I am indexing correctly, sometimes not. I've read somewhere (please correct me if this is stupidity) that more indexes than I need make the same effect, like if I don't have any of indexes.</p>\n\n<p>How many indexes are enough? You can say it depends on hundreds of factors, but I'm curious about how can I clean up my <code>mysql-slow.log</code> enough to reduce server load.</p>\n\n<p>Furthermore, I saw some \"interesting\" log entries like this:</p>\n\n<pre><code># Query_time: 0  Lock_time: 0  Rows_sent: 22  Rows_examined: 44\nSELECT * FROM `categories` ORDER BY `orderid` ASC;\n</code></pre>\n\n<p>The table in question contains exactly 22 rows, index set in <code>orderid</code>. Why is this query showing up in the log after all? Why examine 44 rows if it only contains 22?</p>\n"},{"tags":["java","performance","collections"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":95,"score":4,"question_id":12628606,"title":"What is the fastest java collection for retrieving large numbers of DTOs?","body":"<p>I'm returning large numbers of collections from a DTO object and was wondering if anyone could point me in right direction.  Any type of collection will do, but I don't know which one is best suited for the task of returning a large number of objects.</p>\n\n<p>I know this can change based on threading and the like, but I'm at least looking for general guidance and benchmarks.  Also, I'm required to stay within standard Java collections (no third-party libraries).</p>\n"},{"tags":["php","performance","curl","http-headers"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":86,"score":2,"question_id":11797680,"title":"CURL - Getting HTTP code","body":"<p>Im using CURL to get the status of a site, if its up/down or redirecting to another site.\nI want to get it as stream line as possible, but its not working so well.</p>\n\n<pre><code>&lt;?php\n$ch = curl_init($url);\ncurl_setopt($ch,CURLOPT_RETURNTRANSFER,1);\ncurl_setopt($ch,CURLOPT_TIMEOUT,10);\n$output = curl_exec($ch);\n$httpcode = curl_getinfo($ch, CURLINFO_HTTP_CODE);\ncurl_close($ch);\n\nreturn $httpcode;\n?&gt;\n</code></pre>\n\n<p>I have that wrapped in a function, it works fine but performance is not the best because it downloads the whole page, thing in if I remove <code>$output = curl_exec($ch);</code> it returns <code>0</code> all the time.</p>\n\n<p>Does anyone know how to make this have good performance? </p>\n"},{"tags":["linux","performance","pipe","named-pipes"],"answer_count":1,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":73,"score":3,"question_id":12626227,"title":"Is there a way to improve performance of linux pipes?","body":"<p>I'm trying to pipe extremely high speed data from one application to another using 64-bit Centos6.  I have done the following benchmarks using dd to discover that the pipes are holding me back and not the algorithm in my program.  My goal is to achieve somewhere around 1.5 GB/s.</p>\n\n<p>First, without pipes:</p>\n\n<pre><code>dd if=/dev/zero of=/dev/null bs=8M count=1000\n1000+0 records in\n1000+0 records out\n8388608000 bytes (8.4 GB) copied, 0.41925 s, 20.0 GB/s\n</code></pre>\n\n<p>Next, a pipe between two dd processes:</p>\n\n<pre><code>dd if=/dev/zero bs=8M count=1000 | dd of=/dev/null bs=8M\n1000+0 records in\n1000+0 records out\n8388608000 bytes (8.4 GB) copied, 9.39205 s, 893 MB/s\n</code></pre>\n\n<p>Are there any tweaks I can make to the kernel or anything else that will improve performance of running data through a pipe?   I have tried named pipes as well, and gotten similar results.</p>\n"},{"tags":["performance","excel","openxml-sdk","openxml"],"answer_count":2,"favorite_count":0,"up_vote_count":6,"down_vote_count":0,"view_count":729,"score":6,"question_id":1878207,"title":"Open XML SDK v2.0 Performance issue when deleting a first row in 20,000+ rows Excel file","body":"<p>Do anyone come across a performance issue when deleting a first row in a 20,000+ rows Excel file using OpenXML SDK v2.0?</p>\n\n<p>I am using the delete row coding suggested in the Open XML SDK document. It takes me several minutes just to delete the first row using Open XML SDK, but it only takes just a second in Excel applicaton.</p>\n\n<p>I eventually found out that the bottle-neck is actually on the bubble-up approach in dealing with row deletion. There are many rows updating after the deleted row. So in my case, there are around 20,000 rows to be updated, shifting up the data row by row.</p>\n\n<p>I wonder if there is any faster way to do the row deletion.</p>\n\n<p>Do anybody have an idea?</p>\n"},{"tags":["java","performance","cryptography","aes","salt"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":114,"score":3,"question_id":12522443,"title":"Can i avoid the cipher reinitialization per encrypt/decrypt call when using random salts per encryption?","body":"<h1>Edit</h1>\n\n<p>Actually reinitializing the cipher is not that slow. Creating the key itself is slow because  of the iteration count.</p>\n\n<p>Also, the iteration count is ignored and never used in the encryption itself, only on the key generation. The JCE api is kind of misleading depending on the chosen algorithm</p>\n\n<h1>Original post</h1>\n\n<p>As cryptography in Java is quite... cryptographic, im struggling to do some optimizations. In the functional aspect, this class works quite well and i hope it serves as an example of AES encryption usage</p>\n\n<p>I have a performance issue when encrypting and decrypting data using AES implementation of BouncyCastle (im not comparing, thats the only one implementation I tested). Actually this problem is generic to any cipher I decide to use.</p>\n\n<p>The main issue is: can i avoid the two ciphers whole re-initialization per encrypt/decrypt call? They are too expensive</p>\n\n<p>For the sake of simplicity, keep in mind that the following code had its exception handling removed and a lot of simplification was made to keep the focus on the problem. The synchronized blocks are there to guarantee thread safety</p>\n\n<p>By the way, feedbacks on any part of the code are welcome</p>\n\n<p>Thx</p>\n\n<pre><code>import java.nio.charset.Charset;\nimport java.security.SecureRandom;\nimport java.security.Security;\nimport java.util.Arrays;\n\nimport javax.crypto.Cipher;\nimport javax.crypto.SecretKey;\nimport javax.crypto.SecretKeyFactory;\nimport javax.crypto.spec.PBEKeySpec;\nimport javax.crypto.spec.PBEParameterSpec;\n\nimport org.bouncycastle.jce.provider.BouncyCastleProvider;\n\npublic class AES {\n\n    private static final int ITERATIONS = 120000;\n    private static final int SALT_SIZE_IN_BYTES = 8;\n    private static final String algorithm = \"PBEWithSHA256And128BitAES-CBC-BC\";\n    private static final byte[] KEY_SALT = \"a fixed key salt\".getBytes(Charset.forName(\"UTF-8\"));\n\n    private Cipher encryptCipher;\n    private Cipher decryptCipher;\n    private SecretKey key;\n    private RandomGenerator randomGenerator = new RandomGenerator();\n\n    static {\n        if (Security.getProvider(BouncyCastleProvider.PROVIDER_NAME) == null)\n            Security.addProvider(new BouncyCastleProvider());\n    }\n\n    public AES(String passphrase) throws Exception {\n        encryptCipher = Cipher.getInstance(algorithm);\n        decryptCipher = Cipher.getInstance(algorithm);\n        PBEKeySpec keySpec = new PBEKeySpec(passphrase.toCharArray(), KEY_SALT, ITERATIONS);\n        SecretKeyFactory keyFactory = SecretKeyFactory.getInstance(algorithm);\n        key = keyFactory.generateSecret(keySpec);\n    }\n\n    public byte[] encrypt(byte[] data) throws Exception {\n        byte[] salt = randomGenerator.generateRandom(SALT_SIZE_IN_BYTES);\n        PBEParameterSpec parameterSpec = new PBEParameterSpec(salt, ITERATIONS);\n        data = DataUtil.append(data, salt);\n\n        byte[] encrypted;\n        synchronized (encryptCipher) {\n            // as a security constrain, it is necessary to use different salts per encryption\n            // core issue: want to avoid this reinitialization to change the salt that will be used. Its quite time consuming\n            encryptCipher.init(javax.crypto.Cipher.ENCRYPT_MODE, key, parameterSpec);\n            encrypted = encryptCipher.doFinal(data);\n        }\n        return DataUtil.append(encrypted, salt);\n    }\n\n    public byte[] decrypt(byte[] data) throws Exception {\n        byte[] salt = extractSaltPart(data);\n        data = extractDataPart(data);\n\n        PBEParameterSpec parameterSpec = new PBEParameterSpec(salt, ITERATIONS);\n\n        byte[] decrypted;\n\n        synchronized (decryptCipher) {\n            // as a security constrain, it is necessary to use different salts per encryption\n            // core issue: want to avoid this reinitialization to change the salt that will be used. Its quite time consuming\n            decryptCipher.init(javax.crypto.Cipher.DECRYPT_MODE, key, parameterSpec); \n            decrypted = decryptCipher.doFinal(data);\n        }\n\n        byte[] decryptedSalt = extractSaltPart(decrypted);\n\n        if (Arrays.equals(salt, decryptedSalt))\n            return extractDataPart(decrypted);\n        else\n            throw new IllegalArgumentException(\"Encrypted data is corrupted: Bad Salt\");\n    }\n\n    protected byte[] extractDataPart(byte[] bytes) {\n        return DataUtil.extract(bytes, 0, bytes.length - SALT_SIZE_IN_BYTES);\n    }\n\n    protected byte[] extractSaltPart(byte[] bytes) {\n        return DataUtil.extract(bytes, bytes.length - SALT_SIZE_IN_BYTES, SALT_SIZE_IN_BYTES);\n    }\n\n    // main method to basic check the code execution\n    public static void main(String[] args) throws Exception {\n        String plainText = \"some plain text, have fun!\";\n        String passphrase = \"this is a secret\";\n\n        byte[] data = plainText.getBytes(Charset.forName(\"UTF-8\"));\n\n        AES cipher = new AES(passphrase);\n        byte[] encrypted = cipher.encrypt(data);\n        byte[] decrypted = cipher.decrypt(encrypted);\n\n        System.out.println(\"expected: true, actual: \" + Arrays.equals(data, decrypted));\n    }\n}\n\n// Utility class\nclass RandomGenerator {\n\n    private SecureRandom random = new SecureRandom();\n\n    public RandomGenerator() {\n        random = new SecureRandom();\n        random.nextBoolean();\n    }\n\n    public synchronized byte[] generateRandom(int length) {\n        byte[] data = new byte[length];\n        random.nextBytes(data);\n        return data;\n    }\n}\n\n// Utility class\nclass DataUtil {\n\n    public static byte[] append(byte[] data, byte[] append) {\n        byte[] merged = new byte[data.length + append.length];\n        System.arraycopy(data, 0, merged, 0, data.length);\n        System.arraycopy(append, 0, merged, data.length, append.length);\n        return merged;\n    }\n\n    public static byte[] extract(byte[] data, int start, int length) {\n        if (start + length &gt; data.length)\n            throw new IllegalArgumentException(\"Cannot extract \" + length + \" bytes starting from index \" + start + \" from data with length \" + data.length);\n\n        byte[] extracted = new byte[length];\n        System.arraycopy(data, start, extracted, 0, length);\n        return extracted;\n    }\n\n}\n</code></pre>\n"},{"tags":["ruby-on-rails-3","performance","exception-handling"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":15,"score":0,"question_id":12626426,"title":"Does try() suffer performance difference between Rails 3.0.x and Rails 3.2.x","body":"<p>Many people use try to handle uncertain chaining.  Let's leave aside whether or not such chaining is a good practice.</p>\n\n<ul>\n<li><a href=\"http://ruby-docs.com/docs/ruby_1.9.3-rails_3.0.12/Rails%203.0.12/classes/Object.html#method-i-try\" rel=\"nofollow\">Rails 3.0.x</a> seems to have no separate handling of the <code>try</code> for the <code>NilClass</code></li>\n<li>Rails 3.2.x handle <a href=\"http://ruby-docs.com/docs/ruby_1.9.3-rails_3.1.4/Rails%203.1.4/classes/NilClass.html#method-i-try\" rel=\"nofollow\"><code>nil.try</code></a> differently than <a href=\"http://ruby-docs.com/docs/ruby_1.9.3-rails_3.1.4/Rails%203.1.4/classes/Object.html#method-i-try\" rel=\"nofollow\"><code>object.try</code></a></li>\n</ul>\n\n<p>Does this difference lead to the <code>nil.try</code> case being faster on Rails > 3.0.x ?</p>\n"},{"tags":["php","optimization","performance"],"answer_count":6,"favorite_count":0,"up_vote_count":3,"down_vote_count":2,"view_count":1164,"score":1,"question_id":2389752,"title":"PHP speed: what is faster? if (isset ($foo)) OR if ($foo==true)","body":"<p>I am just trying to optimize my code.\nI need to prefill a form with data from a database, and I need to check if the variable exist to fill the text box (I don't like the <code>@</code> error hiding).\nThe form is really long, then I need to check multiple times if the variables exist.</p>\n\n<p>What is faster of the following two?</p>\n\n<ul>\n<li><code>if (isset ($item))</code></li>\n<li><code>if ($item_exists==true)</code></li>\n</ul>\n\n<p>Or even</p>\n\n<ul>\n<li><code>if ($item_exists===true)</code></li>\n</ul>\n"},{"tags":["sql","performance","rest","azure","latency"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":201,"score":0,"question_id":12583850,"title":"Timeout Expired on SQL Azure; cannot be reproduced on-premise SQL Server","body":"<p>In our line of business we are hosting a REST based API that is hosted by Windows Azure and with SQL Azure as database storage.</p>\n\n<p>Both the Web Role (Windows 2008R2, IIS 7.5, WCF, Large instance) and SQL Azure is hosted in North Europe region.</p>\n\n<p>The problem is, that when we do intensive SQL work we often get a <em>\"Timeout expired. The timeout period elapsed prior to completion of the operation or the server is not responding.\"</em>.</p>\n\n<p>What troubles me here is, that no matter what we do, we cannot provoke this on our on-premise SQL servers (SQL Server 2008R2).</p>\n\n<p>Any help in clarifying this mystery is appreciated as it seems that the Web Role instance is not directly talking to the SQL Azure instance although both are located in North Europe.</p>\n\n<p>A more detailed exception:</p>\n\n<pre><code>&lt;SqlException&gt;\n    &lt;Message&gt;Timeout expired.  The timeout period elapsed prior to completion of the operation or the server is not responding.&lt;/Message&gt;\n    &lt;StackTrace&gt;\n        &lt;Line&gt;at System.Data.SqlClient.SqlConnection.OnError(SqlException exception, Boolean breakConnection)&lt;/Line&gt;\n        &lt;Line&gt;at System.Data.SqlClient.TdsParser.ThrowExceptionAndWarning()&lt;/Line&gt;\n        &lt;Line&gt;at System.Data.SqlClient.TdsParser.Run(RunBehavior runBehavior, SqlCommand cmdHandler, SqlDataReader dataStream, BulkCopySimpleResultSet bulkCopyHandler, TdsParserStateObject stateObj)&lt;/Line&gt;\n        &lt;Line&gt;at System.Data.SqlClient.SqlDataReader.ConsumeMetaData()&lt;/Line&gt;\n        &lt;Line&gt;at System.Data.SqlClient.SqlDataReader.get_MetaData()&lt;/Line&gt;\n        &lt;Line&gt;at System.Data.SqlClient.SqlCommand.FinishExecuteReader(SqlDataReader ds, RunBehavior runBehavior, String resetOptionsString)&lt;/Line&gt;\n        &lt;Line&gt;at System.Data.SqlClient.SqlCommand.RunExecuteReaderTds(CommandBehavior cmdBehavior, RunBehavior runBehavior, Boolean returnStream, Boolean async)&lt;/Line&gt;\n        &lt;Line&gt;at System.Data.SqlClient.SqlCommand.RunExecuteReader(CommandBehavior cmdBehavior, RunBehavior runBehavior, Boolean returnStream, String method, DbAsyncResult result)&lt;/Line&gt;\n        &lt;Line&gt;at System.Data.SqlClient.SqlCommand.RunExecuteReader(CommandBehavior cmdBehavior, RunBehavior runBehavior, Boolean returnStream, String method)&lt;/Line&gt;\n        &lt;Line&gt;at System.Data.SqlClient.SqlCommand.ExecuteScalar()&lt;/Line&gt;\n        &lt;Line&gt;at SyncInvokeAddCollaboratorFieldInstance(Object , Object[] , Object[] )&lt;/Line&gt;\n        &lt;Line&gt;at System.ServiceModel.Dispatcher.SyncMethodInvoker.Invoke(Object instance, Object[] inputs, Object[]&amp;amp; outputs)&lt;/Line&gt;\n        &lt;Line&gt;at System.ServiceModel.Dispatcher.DispatchOperationRuntime.InvokeBegin(MessageRpc&amp;amp; rpc)&lt;/Line&gt;\n        &lt;Line&gt;at System.ServiceModel.Dispatcher.ImmutableDispatchRuntime.ProcessMessage5(MessageRpc&amp;amp; rpc)&lt;/Line&gt;\n        &lt;Line&gt;at System.ServiceModel.Dispatcher.ImmutableDispatchRuntime.ProcessMessage31(MessageRpc&amp;amp; rpc)&lt;/Line&gt;\n        &lt;Line&gt;at System.ServiceModel.Dispatcher.MessageRpc.Process(Boolean isOperationContextSet)&lt;/Line&gt;\n    &lt;/StackTrace&gt;\n    &lt;UserDefinedInformation&gt;\n        &lt;HelpLink.ProdName&gt;&lt;![CDATA[Microsoft SQL Server]]&gt;&lt;/HelpLink.ProdName&gt;\n        &lt;HelpLink.ProdVer&gt;&lt;![CDATA[11.00.2065]]&gt;&lt;/HelpLink.ProdVer&gt;\n        &lt;HelpLink.EvtSrc&gt;&lt;![CDATA[MSSQLServer]]&gt;&lt;/HelpLink.EvtSrc&gt;\n        &lt;HelpLink.EvtID&gt;&lt;![CDATA[-2]]&gt;&lt;/HelpLink.EvtID&gt;\n        &lt;HelpLink.BaseHelpUrl&gt;&lt;![CDATA[http://go.microsoft.com/fwlink]]&gt;&lt;/HelpLink.BaseHelpUrl&gt;\n        &lt;HelpLink.LinkId&gt;&lt;![CDATA[20476]]&gt;&lt;/HelpLink.LinkId&gt;\n    &lt;/UserDefinedInformation&gt;\n&lt;/SqlException&gt;\n</code></pre>\n"},{"tags":["c++","performance","date","format"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":5,"view_count":108,"score":-5,"question_id":10697128,"title":"Changing date format on c++","body":"<p>I need to make a C++ program that turns a date from <code>12/6/1988</code> to <code>19881206</code> or <code>2/28/2010</code> to <code>20100228</code>. My professor was unfair and gave me this assignment this morning with no warning and I have to turn it in tomorrow no later than 2pm.</p>\n\n<p>Please help!!</p>\n"},{"tags":["android","performance","ormlite","slowness"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":72,"score":0,"question_id":12618003,"title":"Android ORMlite performance problems with queryForAll()","body":"<p>Well, like the title says, i got some performance problems while executing queryForAll on a DAO object with ormLite (with android, if it should matter).</p>\n\n<p>The only thing i am actually doing is to execute this:</p>\n\n<blockquote>\n  <p>this.getHelper().getActivityDao().queryForAll(); (where this.getHelper() returns the database-helper which is extending from the OrmLiteSqliteOpenHelper class)</p>\n</blockquote>\n\n<p>This single line needs actually round about 14 seconds to execute... Well, the \"Activity\" entity got about 80 fields and queryForAll returns a array with a length of 74 objects, but still - this can´t be normal, can it?</p>\n\n<p>(For the Calendar fields i am using the </p>\n\n<blockquote>\n  <p>@DatabaseField(dataType = DataType.SERIALIZABLE)</p>\n</blockquote>\n\n<p>annotation, i am not sure, if this is the right annotation for calendar fields or if it could result in performance issues...)</p>\n"},{"tags":["sql","performance","postgresql"],"answer_count":3,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":79,"score":0,"question_id":12602917,"title":"Does it affect the performance, if a table holds a lot of unused/unselected columns?","body":"<p><strong>Background:</strong>\nI have a table called cars that holds a lot of used cars, aprox 1 million rows. The table has a little more than 170 columns. The table is indexed on individual columns alone. Most of the columns are booleans (e.g. has_automatic_gearbox etc.) and the rest is strings and numbers (e.g. color and price). The cars are shown in a view, where i use around 80 columns out of the total 170.</p>\n\n<p><strong>My question:</strong>\nSo my question is, does it make a difference to the performance, whether I select only the 80 columns out of the table when doing a search, or on the other hand I made a new table ONLY consisting of those 80 columns that I need, instead of the total 170 columns? So in other words, does it make a difference to performance, that a table holds columns, that is not selected?</p>\n"},{"tags":["mysql","sql","database","performance","query"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":67,"score":0,"question_id":12605137,"title":"Optimizing this piece of MYSQL Code for better execution time","body":"<p>This piece of mysql code</p>\n\n<pre><code>SELECT  id, value, LENGTH(stuffing)\nFROM  t_limit ORDER BY id LIMIT 150000, 10\n</code></pre>\n\n<p>can be optimized for better performance by rewriting it like this</p>\n\n<p><strong>Note:Table has Index on Id</strong> </p>\n\n<pre><code>SELECT  l.id, value, LENGTH(stuffing)\nFROM    (\n    SELECT  id\n    FROM    t_limit\n    ORDER BY\n            id\n    LIMIT 150000, 10\n    ) o\nJOIN    t_limit l\nON      l.id = o.id\nORDER BY\n    l.id\n</code></pre>\n\n<p><strong>Ref:http://explainextended.com/2009/10/23/mysql-order-by-limit-performance-late-row-lookups/</strong></p>\n\n<p>Now how to optimize this piece of code in a similar way</p>\n\n<pre><code>SELECT  id, value, LENGTH(stuffing)\nFROM  t_limit where value&gt;100 ORDER BY id LIMIT 150000, 10\n</code></pre>\n"},{"tags":["c++","c","performance","x86"],"answer_count":4,"favorite_count":3,"up_vote_count":15,"down_vote_count":0,"view_count":1488,"score":15,"question_id":2043947,"title":"Speed difference between using int and unsigned int when mixed with doubles","body":"<p>I have an application where part of the inner loop was basically:</p>\n\n<pre><code>double sum = 0;\nfor (int i = 0; i != N; ++i, ++data, ++x) sum += *data * x;\n</code></pre>\n\n<p>If x is an unsigned int, then the code takes 3 times as long as with int!</p>\n\n<p>This was part of a larger code-base, but I got it down to the essentials:</p>\n\n<pre><code>#include &lt;iostream&gt;                                      \n#include &lt;cstdlib&gt;                                       \n#include &lt;vector&gt;\n#include &lt;time.h&gt;\n\ntypedef unsigned char uint8;\n\ntemplate&lt;typename T&gt;\ndouble moments(const uint8* data, int N, T wrap) {\n    T pos = 0;\n    double sum = 0.;\n    for (int i = 0; i != N; ++i, ++data) {\n        sum += *data * pos;\n        ++pos;\n        if (pos == wrap) pos = 0;\n    }\n    return sum;\n}\n\ntemplate&lt;typename T&gt;\nconst char* name() { return \"unknown\"; }\n\ntemplate&lt;&gt;\nconst char* name&lt;int&gt;() { return \"int\"; }\n\ntemplate&lt;&gt;\nconst char* name&lt;unsigned int&gt;() { return \"unsigned int\"; }\n\nconst int Nr_Samples = 10 * 1000;\n\ntemplate&lt;typename T&gt;\nvoid measure(const std::vector&lt;uint8&gt;&amp; data) {\n    const uint8* dataptr = &amp;data[0];\n    double moments_results[Nr_Samples];\n    time_t start, end;\n    time(&amp;start);\n    for (int i = 0; i != Nr_Samples; ++i) {\n        moments_results[i] = moments&lt;T&gt;(dataptr, data.size(), 128);\n    }\n    time(&amp;end);\n    double avg = 0.0;\n    for (int i = 0; i != Nr_Samples; ++i) avg += moments_results[i];\n    avg /= Nr_Samples;\n    std::cout &lt;&lt; \"With \" &lt;&lt; name&lt;T&gt;() &lt;&lt; \": \" &lt;&lt; avg &lt;&lt; \" in \" &lt;&lt; (end - start) &lt;&lt; \"secs\" &lt;&lt; std::endl;\n}\n\n\nint main() {\n    std::vector&lt;uint8&gt; data(128*1024);\n    for (int i = 0; i != data.size(); ++i) data[i] = std::rand();\n    measure&lt;int&gt;(data);\n    measure&lt;unsigned int&gt;(data);\n    measure&lt;int&gt;(data);\n    return 0;\n}\n</code></pre>\n\n<p>Compiling with no optimization:</p>\n\n<pre><code>luispedro@oakeshott:/home/luispedro/tmp/so §g++  test.cpp    \nluispedro@oakeshott:/home/luispedro/tmp/so §./a.out\nWith int: 1.06353e+09 in 9secs\nWith unsigned int: 1.06353e+09 in 14secs\nWith int: 1.06353e+09 in 9secs\n</code></pre>\n\n<p>With optimization:</p>\n\n<pre><code>luispedro@oakeshott:/home/luispedro/tmp/so §g++  -O3  test.cpp\nluispedro@oakeshott:/home/luispedro/tmp/so §./a.out\nWith int: 1.06353e+09 in 3secs\nWith unsigned int: 1.06353e+09 in 12secs\nWith int: 1.06353e+09 in 4secs\n</code></pre>\n\n<p>I don't understand why such a large difference in speed. I tried figuring it out from the generated assembly, but I got nowhere. Anyone have any thoughts?</p>\n\n<p>Is this something to do with the hardware or is it a limitation of gcc's optimisation machinery? I'm betting the second.</p>\n\n<p>My machine is an Intel 32 bit running Ubuntu 9.10.</p>\n\n<p><strong>Edit</strong>: Since Stephen asked, here is the de-compiled source (from a -O3 compilation). I believe I got the main loops:</p>\n\n<p>int version:</p>\n\n<pre><code>40: 0f b6 14 0b             movzbl (%ebx,%ecx,1),%edx\n     sum += *data * pos;\n44: 0f b6 d2                movzbl %dl,%edx\n47: 0f af d0                imul   %eax,%edx\n      ++pos;\n4a: 83 c0 01                add    $0x1,%eax\n      sum += *data * pos;\n4d: 89 95 54 c7 fe ff       mov    %edx,-0x138ac(%ebp)\n      ++pos;\n      if (pos == wrap) pos = 0;\n53: 31 d2                   xor    %edx,%edx\n55: 3d 80 00 00 00          cmp    $0x80,%eax\n5a: 0f 94 c2                sete   %dl\n  T pos = 0;\n  double sum = 0.;\n  for (int i = 0; i != N; ++i, ++data) {\n5d: 83 c1 01                add    $0x1,%ecx\n      sum += *data * pos;\n60: db 85 54 c7 fe ff       fildl  -0x138ac(%ebp)\n      ++pos;\n      if (pos == wrap) pos = 0;\n66: 83 ea 01                sub    $0x1,%edx\n69: 21 d0                   and    %edx,%eax\n  T pos = 0;\n  double sum = 0.;\n  for (int i = 0; i != N; ++i, ++data) {\n6b: 39 f1                   cmp    %esi,%ecx\n      sum += *data * pos;\n6d: de c1                   faddp  %st,%st(1)\n  T pos = 0;\n  double sum = 0.;\n  for (int i = 0; i != N; ++i, ++data) {\n6f: 75 cf                   jne    40\n</code></pre>\n\n<p>unsigned version:</p>\n\n<pre><code>50: 0f b6 34 13             movzbl (%ebx,%edx,1),%esi\n      sum += *data * pos;\n54: 81 e6 ff 00 00 00       and    $0xff,%esi\n5a: 31 ff                   xor    %edi,%edi\n5c: 0f af f0                imul   %eax,%esi\n      ++pos;\n5f: 83 c0 01                add    $0x1,%eax\n      if (pos == wrap) pos = 0;\n62: 3d 80 00 00 00          cmp    $0x80,%eax\n67: 0f 94 c1                sete   %cl\n  T pos = 0;\n  double sum = 0.;\n  for (int i = 0; i != N; ++i, ++data) {\n6a: 83 c2 01                add    $0x1,%edx\n      sum += *data * pos;\n6d: 89 bd 54 c7 fe ff       mov    %edi,-0x138ac(%ebp)\n73: 89 b5 50 c7 fe ff       mov    %esi,-0x138b0(%ebp)\n      ++pos;\n      if (pos == wrap) pos = 0;\n79: 89 ce                   mov    %ecx,%esi\n7b: 81 e6 ff 00 00 00       and    $0xff,%esi\n      sum += *data * pos;\n81: df ad 50 c7 fe ff       fildll -0x138b0(%ebp)\n      ++pos;\n      if (pos == wrap) pos = 0;\n87: 83 ee 01                sub    $0x1,%esi\n8a: 21 f0                   and    %esi,%eax\n  for (int i = 0; i != N; ++i, ++data) {\n8c: 3b 95 34 c7 fe ff       cmp    -0x138cc(%ebp),%edx\n      sum += *data * pos;\n92: de c1                   faddp  %st,%st(1)\n  for (int i = 0; i != N; ++i, ++data) {\n94: 75 ba                   jne    50\n</code></pre>\n\n<p>This is the -O3 version, which is why the source lines jump up and down.\nThank you.</p>\n"},{"tags":["matlab","performance","cuda","gpgpu"],"answer_count":4,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":747,"score":4,"question_id":4403429,"title":"CUDA and MATLAB for loop optimization","body":"<p>I'm going to attempt to optimize some code written in MATLAB, by using CUDA. I recently started programming CUDA, but I've got a general idea of how it works. </p>\n\n<p>So, say I want to add two matrices together. In CUDA, I could write an algorithm that would utilize a thread to calculate the answer for each element in the result matrix. However, isn't this technique probably similar to what MATLAB already does? In that case, wouldn't the efficiency be independent of the technique and attributable only to the hardware level?</p>\n"},{"tags":["jquery","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":21,"score":0,"question_id":12621939,"title":"Will loading jQuery on individual pages be helping with overall website pages load time?","body":"<p>If jQuery is only needed on certain pages, will loading jQuery on these individual pages be helpful with overall website pages load time?</p>\n\n<p>Given browser will cache javascript, it won't make much difference. But overall speaking, will it? Or at least it doesn't hurt to do that. Right?</p>\n"},{"tags":["ruby-on-rails","performance","api","node.js","scalability"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":37,"score":0,"question_id":12621924,"title":"Technology Stack for an iPhone and Android App API","body":"<p>I know this question may cost a lot of points to me but I had to ask because right now even though it's risk I'm at a point where I can wipe all the system I've written and start from the very beginning</p>\n\n<p>I cannot give any information since I'm working in a stealth mode startup but I would like give some the information I'll be dealing as a backend developer.</p>\n\n<ul>\n<li><b>Lots of data</b> that <b>constantly change</b>. Assume it's like twitter, where people constantly change who they follow. So this exact example is quite similar to what I have to deal with. There will be constant mapping in my system, so I have to handle a lot of data changes. </li>\n<li>Also the client side constantly sends me <b>requests related to location updates</b> and there will a lot of request that need to be handled at the same time. So concurrency is highly important.</li>\n<li>In addition to the previous requirement, I also need to be able to <b>multi-thread </b>operations like if I suppose to query 4 tables in my database I would not want to wait them one by one, but instead deliver what I already have.</li>\n</ul>\n\n<p>There reason why I am asking this highly possible to be closed questions is that right now I am developing in a naive manner, but since this is a startup with a lot potential, I don't want to watch the backend I've developed crash into pieces. That's why I wanted to ask <b>which technologies</b> would be the most efficient for me to use.</p>\n\n<p>Currently I'm using <b>Ruby On Rails</b> as my framework(using <b>JSON</b> to handle requests), <b>PostgreSQL</b>(since Heroku only supports this choice) and <b>Heroku</b> as my hosting solution.But I am highly worried about the speeds that we receive and what might possibly it be after being used by 300-400 users at the same.</p>\n\n<p>I'm checking and <b>Node.js</b> seems like a better solution than Ruby On Rails but it will be a massive change since I'll be writing every single thing all over. So what I wanted to ask is what would be the best choices for the API technology<b>(RoR,Node.js or even Django)</b>, database<b>(PostgreSQL, MongoDB, CouchDB and Redis)</b> and an alternative to <b>Heroku</b>?</p>\n\n<p>I would be sincerely grateful if I can receive some constructive responds before the question closes.</p>\n"},{"tags":["asp.net","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":112,"score":0,"question_id":11790697,"title":"ASP Website runs slow when number of users Increases","body":"<p>I need some information from you.I have used session.TimeOut=540 in application.Is that effects on my Application performance after some time.When number of users increases its getting very slow. response time nearly more that 2 minutes for a button click also.This is hosted in server in Application pool .I don't know about Application pool much.If Session Timeout is the problem i will remove it.Please suggest me the way to for more users.\n<img src=\"http://i.stack.imgur.com/p0Scb.png\" alt=\"enter image description here\"></p>\n\n<p>Job Numbers,CustomerID,Tasks will come from one database.when the user click start Button then the data saved in another Database.I need this need to be faster for more Users</p>\n"},{"tags":["php","performance","execution-time"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":42,"score":2,"question_id":12621709,"title":"Strange memory usage in while(1) vs. for(;;)","body":"<p>I have the 2 following codes. </p>\n\n<p>1:</p>\n\n<pre><code>$i = 0;\nwhile(1)\n{\n    $i++;\n\n    echo \"big text for memory usage \";\n    if ( $i == 50000 )\n    break;\n}\n\necho \"&lt;br /&gt;\" . memory_get_usage();\n</code></pre>\n\n<p>It echoes every time : <code>1626464</code></p>\n\n<p>2:</p>\n\n<pre><code>$i = 0;\nfor(;;)\n{\n    $i++;\n\n    echo \"big text for memory usage \";\n    if ( $i == 50000 )\n    break;\n}\n\necho \"&lt;br /&gt;\" . memory_get_usage();\n</code></pre>\n\n<p>It echoes every time : <code>1626656</code></p>\n\n<p>Can anybody exaplain this difference between the 2 different memory usages? Even if they are so small...</p>\n"},{"tags":["loops","performance"],"answer_count":7,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":905,"score":2,"question_id":4218155,"title":"For Loop or While Loop - Efficiency","body":"<p>This may be a stupid question, but how does the efficiency of a while loop compare to that of a for loop?  I've always been taught that if you <em>can</em> use a for loop, then I should.  But what actually is the difference between :</p>\n\n<pre><code>$i = 0;\nwhile($i &lt; 100) {\n     echo $i;\n     $i++;\n} \n</code></pre>\n\n<p>compare to:</p>\n\n<pre><code>for($i = 0; $i &lt; 100; $i++) {\n    echo $i;\n}\n</code></pre>\n\n<p>I know in these specific examples the difference is like .000000001%, but when we are talking about large complex loops, what is the difference?</p>\n"},{"tags":["asp.net","performance","memory-leaks"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":49,"score":0,"question_id":12367230,"title":"ASP.NET App performance & Memory leaks","body":"<p>We have an ASP.NET 4.0 web app, with more than 200 pages. It has a public site and a cms.\nWe have installed this app in almost 30 domains in the same server, so each of those domains has the app installed.</p>\n\n<p>The server memory (private working set) grows slowly but it grows. The server has 8 GB RAM, but at the enb of the day, somedays, we get an OutOfMemory message although we are recycling the AppPool every morning.</p>\n\n<p>It has to have memory leaks, but we don't know how to locate those leaks. We have tried to dispose every element we can...</p>\n\n<p>We need some help on this. \nIs there a company that offers these kind of services, I mean to check apps and fix memory leaks?</p>\n\n<p>Thanks.-</p>\n\n<p>After some researching I have found a lot of lines like this: </p>\n\n<p>Label xLabel = (Label)FormView.FindControl(\"xLabel\")</p>\n\n<p>with no dispose. Don't you think that can generate a huge leak? Wouldn't be better:</p>\n\n<p>using (Label xLabel = (Label)FormView.FindControl(\"xLabel\"))\n{ Actions }</p>\n\n<p>Thanks</p>\n"},{"tags":["html","performance","http"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":16,"score":0,"question_id":12617797,"title":"Display correctly resized image or scale larger on page?","body":"<p>Hi more of a question of opinion or best practice here. I am building a site that has a page with a carousel at the top under which there is an accordion. The accordion includes all of the same images as the carousel but at half the size.</p>\n\n<p>For performance would it be best to use the same images as the carousel as then there will only be one http call for that asset or generate the correct size image variations for all the relevant slots but increase the amount of requests for the same asset?</p>\n"},{"tags":["performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":34,"score":0,"question_id":12286822,"title":"old gen filling up in one node","body":"<p>We observed very high Full GC in one of the node during load test. We used 3 glassfish nodes. IN other 2 nodes GC usage is normal, but in node 1 objects not being able to released and slowly objects reach <code>XMX settings</code>. I've verified all 3 nodes <code>JVM settings</code>. They are same. Verified server logs and the load in 3 nodes are same. Not sure why 1 node have this GC issue and not other nodes.</p>\n\n<p>JVM settings:</p>\n\n<pre><code>-XX:+UnlockDiagnosticVMOptions -XX:ParallelGCThreads=8 -XX:MaxPermSize=512m -XX:+AggressiveOpts -XX:NewRatio=2 -XX:+LogVMOutput -XX:+UseParallelGC -XX:LogFile=/opt/glassfish/domains/xyz/logs/jvm.log -Xmx4096m -Xms4096m \n</code></pre>\n\n<p>O/S: Red Hat Enterprise Linux Server release 5.6 (Tikanga)</p>\n\n<pre><code>JVM Version  \njava version \"1.6.0_24\"  \nJava(TM) SE Runtime Environment (build 1.6.0_24-b07)  \nJava HotSpot(TM) 64-Bit Server VM (build 19.1-b02, mixed mode)  \n</code></pre>\n\n<p>Here is sample of GC from problematic node during FULL GC.</p>\n\n<pre><code> 323233.103: [Full GC [PSYoungGen: 1305536K-&gt;1041065K(1351872K)] [PSOldGen: 2796223K-&gt;2796223K(2796224K)] 4101759K-&gt;3837289K(4148096K) [PSPermGen: 105778K-&gt;105778K(106048K)], 24.3236370 secs] [Times: user=24.32 sys=0.00, real=24.32 secs] \n323264.008: [Full GC [PSYoungGen: 1305536K-&gt;1106487K(1351872K)] [PSOldGen: 2796223K-&gt;2796223K(2796224K)] 4101759K-&gt;3902711K(4148096K) [PSPermGen: 105778K-&gt;105778K(106048K)], 22.2367020 secs] [Times: user=22.22 sys=0.01, real=22.24 secs] \n323291.647: [Full GC [PSYoungGen: 1305536K-&gt;1106550K(1351872K)] [PSOldGen: 2796223K-&gt;2796223K(2796224K)] 4101759K-&gt;3902774K(4148096K) [PSPermGen: 105778K-&gt;105778K(106048K)], 22.0651020 secs] [Times: user=22.06 sys=0.00, real=22.06 secs] \n323318.604: [Full GC [PSYoungGen: 1305536K-&gt;1106756K(1351872K)] [PSOldGen: 2796223K-&gt;2796223K(2796224K)] 4101759K-&gt;3902980K(4148096K) [PSPermGen: 105778K-&gt;105778K(106048K)], 22.4309650 secs] [Times: user=22.42 sys=0.00, real=22.43 secs] \n323345.717: [Full GC [PSYoungGen: 1305536K-&gt;1041019K(1351872K)] [PSOldGen: 2796223K-&gt;2796223K(2796224K)] 4101759K-&gt;3837243K(4148096K) [PSPermGen: 105778K-&gt;105778K(106048K)], 24.6671980 secs] [Times: user=24.65 sys=0.00, real=24.66 secs] \n323377.049: [Full GC [PSYoungGen: 1305536K-&gt;1102486K(1351872K)] [PSOldGen: 2796223K-&gt;2796223K(2796224K)] 4101759K-&gt;3898710K(4148096K) [PSPermGen: 105778K-&gt;105778K(106048K)], 22.5150360 secs] [Times: user=22.50 sys=0.00, real=22.51 secs] \n</code></pre>\n"},{"tags":["php","mysql","performance","caching","webserver"],"answer_count":5,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":102,"score":4,"question_id":12618016,"title":"Does Caching always enhance performance?","body":"<p>I have a number of sites with <code>PHP</code> and <code>MySQL</code>, especially running MediaWiki, and I need to enhance the performance. However, I have only a limited percentage of CPU that I'm allowed to use.</p>\n\n<p>The best thing I can think about to improve performance is to enable caching. However, I'm confused: Does that really enhance performance overall or just enhance speed?</p>\n\n<p>What I can think about is, if caching will use files, then it would take more processing to get the content of these files. If it will use SQL tables, then it will take more processing to query these tables as well, perhaps the time will be shorter, but the CPU usage will be more.</p>\n\n<p>Is that correct or not? does caching consume more CPU to give a speeder results or it improves performance overall?</p>\n"},{"tags":["performance","hdf5"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":22,"score":0,"question_id":12619403,"title":"performance reading nested hierarchy","body":"<p>I have a performance problem while reading the attributes from an hdf file.. it seems it takes almost 1.5 minutes to read attributes(10 each) from about 18000 groups/datasets .. hoping somebody can tell me if this is a reasonable time indeed for such a structure... \nthe hdf file has some 300 groups under root \neach of these 300 groups(T) have about 60 subgroups(V) and each of these 60 subgroups have 1 or 2 datasets(D) </p>\n\n<pre><code>   Root \n\n       ----- T1 \n\n            ----- V1 \n                 ---- D \n\n            -----  V60 \n                  ---- D \n\n         .... \n         .... \n         .... \n\n      -----  T300 \n</code></pre>\n\n<p>at each level I am reading max 10 tiny attributes.. </p>\n\n<p>does reading each group mean mostly a new disk seek? my initial thinking is that since metadata belongs to different groups..they possibly end up on different disk blocks\nI am thinking of creating a dataset of all metatadata</p>\n\n<p>vtune shows large wait times opening groups and data sets\n ..any suggestions for improving performance.. ? </p>\n"},{"tags":["c++","string","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":84,"score":1,"question_id":12616988,"title":"C++ String memory reuse optimization","body":"<p>I was wondering if string memory for <code>tmp</code> can be reuse in below code. Is its memory reallocated in every iteration? Is there any better way to deal with these kind of case?</p>\n\n<pre><code>string s, line;\nmap&lt;string, string&gt; mymap;\nwhile(getline(file, line) {\n  if(a) s = \"_a\";\n  else if(b) s = \"_b\";\n  string tmp = line + s;\n  mymap.insert(tmp, s);\n}\n</code></pre>\n"},{"tags":["mysql","performance","query","join"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":67,"score":0,"question_id":12617750,"title":"MySQL select in join clause scanning too many rows","body":"<p>Oke guys, the following has been bugging me all day:</p>\n\n<p>I use the query below to select an overview of products and prices including the latest result-price based on field StartTime from another table (tresults). To do this I thought I would need a subselect in the join.</p>\n\n<p>The problem is that the EXPLAIN function is telling me that MySQL is scanning ALL result rows (225000 rows) not using any index.</p>\n\n<p>Is there some way I can speed this up? Preferably by adding a WHERE statement to have mysql look only at the rows with the corresponding pID's.</p>\n\n<pre><code>select p.pID, brandname, description, p.EAN, RetailPrice, LowestPrice, min(price), min(price)/lowestprice-1 as afwijking\nfrom tproducts p\n    join ( \n    select Max(tresults.StartTime) AS maxstarttime, tresults.pID\n    from tresults\n    -- maybe adding a where clause here?\n    group by tresults.pID\n    ) p_max on (p_max.pID = p.pID)\njoin tresults res on (res.starttime = p_max.maxstarttime and p.pID = res.pID and res.websiteID = 1)\njoin tsupplierproducts sp on (sp.pID = p.pID AND supplierID = 1)\njoin tbrands b on (b.brandID = p.BrandID)\ngroup by p.pID, brandname, description, p.EAN, RetailPrice, LowestPrice\n</code></pre>\n\n<p>Indexes are on all columns that are part of joins or where clauses.</p>\n\n<p>Any help would be appreciated. Thanks!</p>\n"},{"tags":["android","performance","android-layout"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":40,"score":2,"question_id":12618203,"title":"Does Transparency in Android Layout impact Performance?","body":"<p>I come from an iOS background, where one of the rules for fast views is to avoid transparent backgrounds and pngs if possible. I haven't found any information about this on Android. So my question is, should I use non-transparent views and drawables where possible, or does Android not care about this as much as iOS does?</p>\n"},{"tags":["javascript","performance","google-maps"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":58,"score":1,"question_id":12617468,"title":"how to improve Google map loading time?","body":"<p>My website has a number of pages that show a Google map with a bunch of markers, <a href=\"http://festivals.ie/festival/map?location=EUROPE&amp;types=HEADLINE&amp;types=MUSIC&amp;types=COMEDY&amp;types=SPORT&amp;types=ARTS&amp;types=FILM&amp;types=FOOD_AND_DRINK&amp;types=OTHER\" rel=\"nofollow\">here's an example</a>. </p>\n\n<p>As you can see, the maps take a long time to load and I'm looking for ways to improve this. I was hoping to use <a href=\"http://geowebcache.org/docs/current/introduction/whatis.html\" rel=\"nofollow\">GeoWebCache</a> to cache the map tiles on the server, but I was informed that this would violate the terms of use for Google maps.</p>\n\n<p>The code that I use to display a map and add a marker is appended below. It's a pretty straightforward usage of the Google Maps V3 JavaScript API, so I don't think there's much scope for optimizing it. Are there any obvious steps I could take to reduce the map-loading time?</p>\n\n<pre><code>SF.Map = function(elementId, zoomLevel, center, baseImageDir) {\n\n    this._baseImageDir = baseImageDir;\n    var focalPoint = new google.maps.LatLng(center.latitude, center.longitude);\n\n    var mapOptions = {\n        streetViewControl: false,\n        zoom: zoomLevel,\n        center: focalPoint,\n        mapTypeId: google.maps.MapTypeId.ROADMAP,\n        mapTypeControl: false\n    };\n\n    this._map = new google.maps.Map(document.getElementById(elementId), mapOptions);\n    this._shadow = this._getMarkerImage('shadow.png');\n};\n\nSF.Map.prototype._getMarkerImage = function(imageFile) {\n    return new google.maps.MarkerImage(this._baseImageDir + '/map/' + imageFile);\n};\n\n\nSF.Map.prototype._addField = function(label, value) {\n    return \"&lt;span class='mapField'&gt;&lt;span class='mapLabel'&gt;\" + label + \": &lt;/span&gt;&lt;span class='mapValue'&gt;\" + value + \"&lt;/span&gt;&lt;/span&gt;\";\n};\n\n/**\n * Add a marker to the map\n * @param festivalData Defines where the marker should be placed, the icon that should be used, etc.\n * @param openOnClick\n */\nSF.Map.prototype.addMarker = function(festivalData, openOnClick) {\n\n    var map = this._map;\n    var markerFile = festivalData.markerImage;\n\n    var marker = new google.maps.Marker({\n        position: new google.maps.LatLng(festivalData.latitude, festivalData.longitude),\n        map: map,\n        title: festivalData.name,\n        shadow: this._shadow,\n        animation: google.maps.Animation.DROP,\n        icon: this._getMarkerImage(markerFile)\n    });\n\n    var bubbleContent = \"&lt;a class='festivalName' href='\" + festivalData.url + \"'&gt;\" + festivalData.name + \"&lt;/a&gt;&lt;br/&gt;\";\n    var startDate = festivalData.start;\n    var endDate = festivalData.end;\n\n    if (startDate == endDate) {\n        bubbleContent += this._addField(\"Date\", startDate);\n\n    } else {\n        bubbleContent += this._addField(\"Start Date\", startDate) + \"&lt;br/&gt;\";\n        bubbleContent += this._addField(\"End Date\", endDate);\n    }\n\n    // InfoBubble example page http://google-maps-utility-library-v3.googlecode.com/svn/trunk/infobubble/examples/example.html\n    var infoBubble = new InfoBubble({\n        map: map,\n        content: bubbleContent,\n        shadowStyle: 1,\n        padding: 10,\n        borderRadius: 8,\n        borderWidth: 1,\n        borderColor: '#2c2c2c',\n        disableAutoPan: true,\n        hideCloseButton: false,\n        arrowSize: 0,\n        arrowPosition: 50,\n        arrowStyle: 0\n    });\n\n    var mapEvents = google.maps.event;\n\n     // either open on click or open/close on mouse over/out\n    if (openOnClick) {\n\n        var showPopup = function() {\n            if (!infoBubble.isOpen()) {\n                infoBubble.open(map, marker);\n            }\n        };\n        mapEvents.addListener(marker, 'click', showPopup);\n\n    } else {\n\n        mapEvents.addListener(marker, 'mouseover', function() {\n            infoBubble.open(map, marker);\n        });\n\n        mapEvents.addListener(marker, 'mouseout', function() {\n            infoBubble.close();\n        });\n    }\n};\n</code></pre>\n"},{"tags":["c#","java",".net","c","performance"],"answer_count":26,"favorite_count":10,"up_vote_count":25,"down_vote_count":5,"view_count":9022,"score":20,"question_id":728645,"title":"What is faster- Java or C# (or good old C)?","body":"<p>I'm currently deciding on a platform to build a scientific computational product on, and am deciding on either C#, Java, or plain C with Intel compiler on Core2 Quad CPU's. It's mostly integer arithmetic. </p>\n\n<p>My benchmarks so far show Java and C are about on par with each other, and .NET/C# trails by about 5%- however a number of my coworkers are claiming that .NET with the right optimizations will beat both of these given enough time for the JIT to do its work.</p>\n\n<p>I always assume that the JIT would have done it's job within a few minutes of the app starting (Probably a few seconds in my case, as it's mostly tight loops), so I'm not sure whether to believe them</p>\n\n<p>Can anyone shed any light on the situation? Would .NET beat Java? (Or am I best just sticking with C at this point?).</p>\n\n<p>The code is highly multithreaded and data sets are several terabytes in size. </p>\n\n<p>Haskell/Erlang etc are not options in this case as there is a significant quantity of existing legacy C code that will be ported to the new system, and porting C to Java/C# is a lot simpler than to Haskell or Erlang. (Unless of course these provide a significant speedup). </p>\n\n<p>Edit: We are considering moving to C# or Java because they may, in theory, be faster. Every percent we can shave off our processing time saves us tens of thousands of dollars per year. At this point we are just trying to evaluate whether C, Java, or c# would be faster. </p>\n"},{"tags":["html","css","performance","selectors","css-selectors"],"answer_count":6,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":352,"score":2,"question_id":617446,"title":"Speed/redundancy of selectors in CSS","body":"<p>Does anyone have information on browser selector speeds in CSS? In other words, how different selectors compare to each other (in th same browser).</p>\n\n<p>For example, I often see (and write) code like this:</p>\n\n<pre><code>#content #elem { ...rules... }\n</code></pre>\n\n<p>But since those elements are unique IDs, I should only need <code>#elem</code>, right? This got me thinking about whether maybe it's quicker for browsers to have more complex selectors - my thinking being that a browser might find <code>#content</code> and know to only look in that element, no where else.</p>\n\n<p>Another example might be <code>table tr td .class</code> vs <code>table .class</code></p>\n"},{"tags":["c#","performance","entity-framework","ef-code-first"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":12605647,"title":"EF code first iterates over all properties while creating object","body":"<p>I use EF Code First in my application and have the following class:</p>\n\n<pre><code>   [Table(\"TBL_XYZ\")]\n    public class XYZ\n    {\n        [Required]\n        public string PropA { get; set; }\n\n        [Required]\n        public int PropB  { get; set; }\n\n        public int FormulaA \n        {\n          get \n          {\n            return PropB *  Math.PI / 100;\n          }\n        }\n    }\n</code></pre>\n\n<p>This is how I get the data from the database:</p>\n\n<pre><code>var data = (from e in db.XYZ where e.PropB &lt; 100 select e).ToList();\n</code></pre>\n\n<p>After I added some more fields which do calculations and don't have a set accessor (like FormulaA), I realized a drop in performance when executing the above line.</p>\n\n<p>After some debugging I found out that EF iterates over all Properties.  It calls all get-functions of the properties, while creating the object, even if I don't access them.</p>\n\n<p>What is the purpose of this behaviour and is there a workaround. Does this maybe have something to do with keeping track of changes?\nIt is really convenient for me to have my formulas in the object itself, but right now it severely affects the performance.</p>\n"},{"tags":["database","performance","oracle11g","partitioning","database-performance"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":31,"score":1,"question_id":12598126,"title":"What is the best partitioning method for table where two significant columns are some IDs? The DB is Oracle 11g","body":"<p>The goal is decrease time of deleting from table quering by COLUMN_1. Now this can take up to 15-30 minutes.</p>\n\n<p>Currently data from table is deleted by cascade rule. COLUMN_1 is foreign key.\nI would like to change this this approach to truncate whole partition in BEFORE trigger when parent row is deleted.</p>\n\n<p>Besides, data is queried from application by COLUMN_2.</p>\n\n<p>This two columns are in relationship presented below :</p>\n\n<pre><code>COL_1    COL_2\n 1       1\n         2\n         3\n         4\n 2      \n         5\n         6\n         7\n         8\n         9\n         10\n 3      \n         11\n         12\n         13\n...\n</code></pre>\n\n<p>On both separately there is index.\nTable has millions of records. There is a lot of insert and read actions.</p>\n\n<p>After some research (<a href=\"http://docs.oracle.com/cd/B28359_01/server.111/b32024/part_admin.htm#BAJHFFBE\" rel=\"nofollow\">oracle doc</a>) I would use range-range partitioning<br>\nI need partitions were created dynamically, so maybe Range Interval Partitioning will be appropriate...</p>\n"},{"tags":["c++","c","performance","compiler"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":97,"score":1,"question_id":12612707,"title":"What is the performance hit for the compiler if it initializes variables?","body":"<p>Sutter says this:</p>\n\n<blockquote>\n  <p>\"In the low-level efficiency tradition of C and C++ alike, the\n  compiler is often not required to initialize variables unless you do\n  it explicitly (e.g., local variables, forgotten members omitted from\n  constructor initializer lists)\"</p>\n</blockquote>\n\n<p>I have always wondered why the compiler doesn't initialize primitives like int32 and float to 0. What is the performance hit if the compiler initializes it? It should be better than incorrect code.</p>\n"},{"tags":["java","performance","variables","loops"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":1,"view_count":87,"score":0,"question_id":12616576,"title":"What is the most efficient way of local variables initialization for a loop in Java?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/8803674/declaring-variables-inside-or-outside-of-a-loop\">Declaring variables inside or outside of a loop</a>  </p>\n</blockquote>\n\n\n\n<p>Please consider these 2 samples of Java code:</p>\n\n<pre><code>// 1st sample\nfor (Item item : items) {\n    Foo foo = item.getFoo();\n    int bar = item.getBar();\n    // do smth with foo and bar\n}\n\n// 2nd sample\nFoo foo;\nint bar;\nfor (Item item : items) {\n    foo = item.getFoo();\n    bar = item.getBar();\n    // do smth with foo and bar\n}\n</code></pre>\n\n<p>Is there any difference in performance/memory consumption between the samples? If it is, then does it depend on type of a handle (an Object vs. a primitive)?</p>\n"},{"tags":["performance","nhibernate","fluent-nhibernate"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":45,"score":0,"question_id":12555425,"title":"NHibernate performance and one-to-many relationship","body":"<p>Let's say I have following case. There are two tables <code>Counterparties</code> and <code>Events</code>. One counterparty can have 0, 1 or more related events. </p>\n\n<p>I would like to get counterparties list with lastest event in order to show id DataGrid, so I created:</p>\n\n<p>two entities:</p>\n\n<pre><code>public class Counterparty\n{\n    public virtual int Id { get; set; }\n    public virtual string Name { get; set; }\n\n    public virtual IList&lt;Event&gt; Events { get; set; }\n\n    public Counterparty()\n    {\n        Events = new List&lt;Event&gt;();\n    }\n} \n\npublic class Event\n{\n    public virtual int Id { get; set; }\n    public virtual DateTime EventDate { get; set; }\n    public virtual string Description { get; set; }\n    public virtual Counterparty Counterparty { get; set; }\n}\n</code></pre>\n\n<p>two mappings:</p>\n\n<pre><code>public class CounterpartyMap : ClassMap&lt;Counteparty&gt;\n{\n    public CounterpartyMap()\n    {\n        Id(x =&gt; x.Id);\n        Map(x =&gt; x.Name);\n        HasMany&lt;Event&gt;(x =&gt; x.Events);\n    }\n}\n\npublic class EventMap : ClassMap&lt;Event&gt;\n{\n    public Event()\n    {\n        Id(x =&gt; x.Id);\n        Map(x =&gt; x.EventDate);\n        Map(x =&gt; x.Description);\n        Reference(x =&gt; x.Counterparty);\n    }\n}\n</code></pre>\n\n<p>helper class for containing <code>Counterparty</code> with the lastest <code>Event</code></p>\n\n<pre><code>public class LastestCounterpartyEvent\n{\n    public Counterparty Counterparty { get; set; }\n    public ScoringResult ScoringResult  { get; set; }\n}\n</code></pre>\n\n<p>and finally methods responsible for creating list of all counteparties with lastest events:</p>\n\n<pre><code>    public IList&lt;LastestCounterpartyEvent&gt; All()\n    {\n        // Added line of code responsible for data loading\n        var allCounterparties = DataContext.Session.QueryOver&lt;Counterparty&gt;().List();\n        return allCounterparties.Select(Prepare).ToList();\n    }\n\n    private LastestCounterpartyEvent Prepare(Counterparty counterparty)\n    {\n        var lastestCounterpartyEvent = new LastestCounterpartyEvent {Counterparty = counterparty};\n        if (counterparty.Events.Count &gt; 0)\n            lastestCounterpartyEvent.Event = \n                counterparty.Events.OrderByDescending(x =&gt; x.EventDate).First();\n\n        return lastestCounterpartyEvent;\n    }\n</code></pre>\n\n<p>The prefromance is unacceptable, for 30 counterparties and 10 events data processing takes 5 seconds. </p>\n\n<p>I suppose that NHiberante executes separate database query for every counterparty when checking lastest events.</p>\n\n<p>The question is: what can I do for performance improvement? </p>\n"},{"tags":["javascript","jquery","performance"],"answer_count":5,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":151,"score":7,"question_id":7362782,"title":"In terms of performance, what is the best method to show 1000 images on a page?","body":"<p>I'm trying to show 1000 quite small images on a page (rather a lot indeed but out of my control).</p>\n\n<p>When loading them all at once, the performance obviously suffers drastically rendering 1000 images at once.</p>\n\n<p>I tried implementing applying the image src upon scroll (at numerous amounts - 250px scroll, 25 images load etc.), then tried loading the images on a timer.</p>\n\n<p>These methods did help to increase performance but what would be the most efficient way to do this? They seemed to still have an irritating amount of lag - I understand there is a fundamental problem with rendering that many images on one page, but is there a better solution?</p>\n\n<p><strong>EDIT:</strong></p>\n\n<p>Pagination of course would help but isn't an option here. Also, the images are pulled from an API so it's not convenient to make 1 large image / use sprites.</p>\n"},{"tags":["javascript","performance","dom"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":107,"score":3,"question_id":12613113,"title":"Performance with infinite scroll or a lot of dom elements?","body":"<p>I have a question on a big # of dom elmenets and performance.</p>\n\n<p>Let's say I have 6000 dom elements on a page and the number of the elements can be increased as a user interact with the page (user scrolls to create a new dom element) like twitter.</p>\n\n<p>To improve the performance of the page, I can think of only two things.</p>\n\n<ol>\n<li>set display to none to invisible items to avoid reflow</li>\n<li>remove invisible items from the dom then re-add them as needed.</li>\n</ol>\n\n<p>Are they any other ways of improving a page with a lot of dom elements?</p>\n"},{"tags":["javascript","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":28,"score":0,"question_id":12612353,"title":"what are some drawbacks to using clone & pop to iterate through an array rather than using a for loop?","body":"<p>it looks like this method of iteration is much much faster than a for loop:</p>\n\n<pre><code>var arr = window.arr.slice(0),\n    fruit = arr.pop();\nwhile (fruit) {\n fruit = list.pop();\n}\n</code></pre>\n\n<p>as evidenced in this jsperf test: <a href=\"http://jsperf.com/loop-iteration-length-comparison-variations/7\" rel=\"nofollow\">http://jsperf.com/loop-iteration-length-comparison-variations/7</a></p>\n\n<p>I know I'm taking a memory hit by cloning the array but if i delete the clone right after i loop through it, what else should i be weary of?</p>\n"},{"tags":["performance","swing","gui","swt","benchmarking"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":215,"score":0,"question_id":7470291,"title":"swt versus swing judged from a performance/speed standpoint (yes, this one is a classic :D)","body":"<p>Lately i've been  looking to see which of the above contestants is better performance-wise and i've found a rather interesting article that demolishes SWT's performance rating compared to swing's : <a href=\"http://cosylib.cosylab.com/pub/CSS/DOC-SWT_Vs._Swing_Performance_Comparison.pdf\" rel=\"nofollow\">http://cosylib.cosylab.com/pub/CSS/DOC-SWT_Vs._Swing_Performance_Comparison.pdf</a> . It's worth mentioning that the  benchmark was done in 2005 and since then things might have changed (dunno in who's favor tho ) SO:    </p>\n\n<p>1)does anybody know any performance tests  done recently? (2-3 years at most :D)<br>\n2) your personal experience on this matter is invaluable to me so do tell if you noticed differences between swing and swt when it comes to performance</p>\n"},{"tags":["performance","magento","caching"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":36,"score":0,"question_id":12613051,"title":"Magento Caching Custom Images","body":"<p>I am using the avalanche theme and make use of the homepage slider.</p>\n\n<p>The images are added to the site via a section in the admin panel and they are stored in </p>\n\n<pre><code>media/banners/default/slidex.jpg\n</code></pre>\n\n<p>I notice that these images are not being cached and loaded directly from their location.  Is there any way to include images such as these in the magento image cache in order to reduce load time?</p>\n\n<p>As these are large banner images you would expect them to take longer to load, however in the pingdom tools analysis for my home page it indicates that the server side wait is much longer than the time it actually takes to receive the files.</p>\n"},{"tags":["c#","performance","crystal-reports"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":77,"score":0,"question_id":12542272,"title":"Crystal reports ExportToDisk consumes a lot of time","body":"<p>I have a console application in C# which works with crystal reports. I am loading report (only once) using:</p>\n\n<pre><code>    static ReportDocument cryReportDocument = new ReportDocument();\n\n    static void Main(string[] args)\n    {            \n        cryReportDocument.Load(\"reportLocation\");\n        ....\n        //I am exporting about 20.000 .pdf files\n        while(true)\n        {\n            ....\n            //destianationPath is file location \n            ExportToPdf(destinationPath)\n        }\n    }\n</code></pre>\n\n<p>Then I am exporting this report to pdf file using:</p>\n\n<pre><code>    //Export pdf file\n    static void ExportToPdf(string destianationPath)\n    {                     \n        cryReportDocument.SetDatabaseLogon(\"userName\", \"password\", \"Database\", \"\");\n        //Adding paremeters to report\n        cryReportDocument.SetParameterValue(.., ..);    \n        //This line consumes a lot of time now  \n        cryReportDocument.ExportToDisk(CrystalDecisions.Shared.ExportFormatType.PortableDocFormat, destianationPath);\n    }\n</code></pre>\n\n<p>I used this program before (about two or three days before) to export about 15.000 .pdf files and it worked very well. Exported one .pdf file in approximately less then one second. I changed nothing in my code but that line takes about 5 seconds to export one .pdf file. What can cause to this? Computer is the same, I have changed nothing. But it is not working properly. Can anyone help?</p>\n"},{"tags":["performance","oracle","packages"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":53,"score":2,"question_id":12596126,"title":"Temporary tables in Packages - Oracle","body":"<p>I am kind of new in Oracle.\nI am trying to create a package that has several functions.\nThis is the pseudocode of what I want to do</p>\n\n<pre><code>function FunctionA(UserID, startdate, enddate)\n  /* Select TransactionDate, Amount\n     from TableA\n     where TransactionDate between startdate and enddate\n     and TableA.UserID = UserID */\n  Return TransactionDate, Amount\nend FunctionA\n\nfunction FunctionB(UserID, startdate, enddate)\n  /* Select TransactionDate, Amount\n     from TableB\n     where TransactionDate between startdate and enddate\n     and TableB.UserID = UserID */\n  Return TransactionDate, Amount\nend FunctionA\n\nTYPE TRANSACTION_REC IS RECORD(\n          TransactionDate    DATE,\n          TransactionAmt     NUMBER);\n\nfunction MainFunction(startdate, enddate)\n  return TBL\n  is\n  vTrans TRANSACTION_REC;\nbegin\n  FOR rec IN\n    ( Select UserID, UserName, UserStatus\n      from UserTable\n      where EntryDate between startdate and enddate )\n  LOOP\n    vTrans := FunctionA(rec.UserID, startdate, enddate)\n\n    if vTrans.TransactionDate is null then\n       vTrans := FunctionB(rec.UserID, startdate, enddate)\n\n       if vTrans.TransactionDate is null then\n           rec.UserStatus := 'Inactive'\n       endif;\n    endif;\n  END Loop;\n\n  PIPE ROW(USER_OBJ_TYPE(rec.UserID,\n                       rec.UserName,\n                       rec.UserStatus,\n                       vTrans.TransactionDate,\n                       vTtans.TransactionAmt));\nend MainFunction\n</code></pre>\n\n<p>Running this kind of code takes a long time because TableA and TableB is a very large table, and I am only getting 1 entry per record from the tables.</p>\n\n<p>I would want to create a temporary table (TempTableA, TempTableB) within the package that will temporarily store all records based on the startdate and enddate, so that when I try to retrieve the TransactionDate and Amount for each rec, I will only refer to the TempTables (which is smaller than TableA and TableB).</p>\n\n<p>I also want to take into consideration if the UserID is not found in TableA and TableB. So basically, when there are no records found in TableA and TableB, I also want the entry in the output, but it is indicated that the user is inactive.</p>\n\n<p>Thank you for all your help.</p>\n"},{"tags":["sql","sql-server","performance","tsql"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":98,"score":2,"question_id":12592211,"title":"Overwrite values every time vs query to see if value has changed","body":"<p>I have a column that I would like to update only if the value I pass in to a stored procedure is different than the value in the column. It happens to be an NVARCHAR(255) column, if that matters.</p>\n\n<p>What are the pros and cons of writing this value every time? What are the pros and cons of checking the value first and writing only if what I have passed in is different than what is in the database?</p>\n\n<p>My simplified example where I'm doing a comparison before writing:</p>\n\n<pre><code>-- @URL and @ContentName are parameters of the sproc    \n\nSET @ExistingURL =\n    (SELECT TOP 1 C.URL\n    FROM Content C\n    WHERE ContentName = @ContentName)\n\n-- update only if the parameter and existing value are different\nIF(@ExistingURL != @URL)\nBEGIN\n    UPDATE Content\n    SET URL = @URL\n    WHERE ContentName = @ContentName\nEND\n</code></pre>\n"},{"tags":["performance","postgresql","database-design"],"answer_count":2,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":86,"score":4,"question_id":12604744,"title":"Does the order of columns in a postgres table impact performance?","body":"<p>In Postgres does the order of columns in a <code>CREATE TABLE</code> statement impact performance? Consider the following two cases: </p>\n\n<pre><code>CREATE TABLE foo (\n  a      TEXT, \n  B      VARCHAR(512),\n  pkey   INTEGER PRIMARY KEY,\n  bar_fk INTEGER REFERENCES bar(pkey),\n  C       bytea\n); \n</code></pre>\n\n<p>vs. </p>\n\n<pre><code>   CREATE TABLE foo2 (\n      pkey   INTEGER PRIMARY KEY,\n      bar_fk INTEGER REFERENCES bar(pkey),\n      B      VARCHAR(512),      \n      a      TEXT, \n      C       bytea\n    );\n</code></pre>\n\n<p>will the performance of <code>foo2</code> be better than <code>foo</code> because of better byte alignment for the columns? When Postgres executes a <code>CREATE TABLE</code> does it follow the column order specified or does it re-organize the columns in optimal order for byte alignment or performance? </p>\n"},{"tags":["sql-server","performance","sql-server-2008","indexed-view"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":12611194,"title":"Will using an indexed view improve performance of SELECT COUNT queries?","body":"<p>I have a table with that will grow to several million rows over some years.  As part of my web application, I have to query the count on a subset of this table whenever a user accesses a particular page.  Someone with an architecty hat has said that they have a performance concern with that.  Assuming they are correct, will adding an indexed view address this issue?</p>\n\n<p>Sql that I want to be fast:</p>\n\n<pre><code>SELECT COUNT(*) FROM [dbo].[Txxx] WHERE SomeName = 'ZZZZ'\n</code></pre>\n\n<p>OR </p>\n\n<pre><code>SELECT COUNT_BIG(*) FROM [dbo].[Txxx] WHERE SomeName = 'ZZZZ'\n</code></pre>\n\n<p>Table:</p>\n\n<pre><code>CREATE TABLE [dbo].[Txxx](\n    [Id] [uniqueidentifier] ROWGUIDCOL  NOT NULL,\n    [SomeName] [nvarchar](50) NOT NULL,\n    [SomeGuid] [uniqueidentifier] NOT NULL\n CONSTRAINT [PK_Txxx] PRIMARY KEY CLUSTERED \n(\n    [Id] ASC\n)\n</code></pre>\n\n<p>View:</p>\n\n<pre><code>CREATE view dbo.Vxxx\nWITH SCHEMABINDING\nAS\nSELECT     SomeName, COUNT_BIG(*) AS UsedCount\nFROM         dbo.Txxx\nGROUP BY SomeName\n</code></pre>\n\n<p>Index:</p>\n\n<pre><code>CREATE UNIQUE CLUSTERED INDEX [IV_COUNT] ON [dbo].[Vxxx] \n(\n    [SomeName] ASC\n)\n</code></pre>\n"},{"tags":["python","performance","numpy","order","f2py"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":45,"score":1,"question_id":12606027,"title":"f2py speed with array ordering","body":"<p>I'm writing some code in fortran (<code>f2py</code>) in order to gain some speed because of a large amount of calculations that would be quite bothering to do in pure Python.</p>\n\n<p>I was wondering if setting NumPy arrays in Python as <code>order=Fortran</code> will kind of slow down\nthe main python code with respect to the classical C-style order.</p>\n"},{"tags":[".net","performance","orm","code-first"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":52,"score":0,"question_id":12610426,"title":".NET Code First ORMs","body":"<p>I'm looking for a .NET ORM that supports code first data modeling like Microsoft Entity Framework. I used Entity Framework before but had some performance issues. So also I use BLToolkit as a data access layer and database created directly by T-SQL (a middle size project). This solution has nice performance but it's very annoying to update the code models after updating the database tables.</p>\n\n<p>So it would be nice if somebody shares the experience about using ORMs with code first data modeling approach.</p>\n"},{"tags":["javascript","arrays","performance","object","data-structures"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":12609826,"title":"Which javascript structure has a faster access time for this particular case?","body":"<p>I need to map specific numbers to string values. These numbers are not necessarily consecutive, and so for example I may have something like this:</p>\n\n<pre><code>var obj = {};\nobj[10] = \"string1\";\nobj[126] = \"string2\";\nobj[500] = \"string3\";\n</code></pre>\n\n<p>If I'm doing a search like this <code>obj[126]</code> would it be faster for me to use an object <code>{}</code> or an array <code>[]</code>?</p>\n"},{"tags":["c#","performance","memory","stream","unmanaged"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":1539,"score":3,"question_id":1367350,"title":"How can I in C# stream.Read into unmanaged memory stream?","body":"<p>I can read unmanaged memory in C# using UnmanagedMemoryStream, but how can I do the reverse?</p>\n\n<p>I want to read from a managed stream directly into unmanaged memory, instead of first reading into a byte[] and then copying.  I'm doing async stream reading on a large number of requests, so the added memory is significant (not to mention the additional copy).</p>\n"},{"tags":["java","performance","data-structures","polymorphism","hashtable"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":81,"score":0,"question_id":12610673,"title":"Java Hashtable-like solution for integers","body":"<p>So, here's the deal. I have a list of objects in a static deck. In this context, each object has a static integer key, which can be used against the class to get the static copy of that object. This is fine. However, I then need to assign a variable number of these objects (by cloning them) to other objects; let us call the the assigned objects <code>Properties</code> and the recipient of the assignments <code>Things</code>.</p>\n\n<p>This is necessary because the static deck only provides static information, i.e. members and methods that are always the same no matter the state of the <code>Property</code> assigned to the <code>Thing</code>. These properties may change in an instance, such as being turned on or off, or being ratcheted up along a predefined scale of values.</p>\n\n<p><code>Properties</code> can provide methods, but we do not know that the property exists without asking for it, much the way a hashtable works. If it doesn't exist, ideally we get a null and can't run the method associated with the <code>Property</code>. </p>\n\n<p>This is in essence a very fluid form of polymorphism, but the end result should be something like this:</p>\n\n<ol>\n<li><p>We create a <code>Thing</code>, and it runs a method internally which, based\non certain initial values provided in the constructor assigns <code>Properties</code> to itself accordingly.</p></li>\n<li><p>When certain conditions occur, such as a keystroke event, mouse\nevent, or signal from another object in the program, we look to see\nif the <code>Property</code> associated with that control signal or other signal\nexists, and try to run the method. If not, we handle the failure\naccordingly.</p></li>\n</ol>\n\n<p>My first thought was to simply make a copy of the static deck for each <code>Thing</code>, but if there become a large list of properties some of which may only be used by very few <code>Things</code>, we run into a problem of potentially wasting swaths of space. Or, we run into the situation of wasting <em>some</em> space and leaving certain array indexes pointing to null values. It's a fast solution, but unideal. It places certain fairly strong conceptual limits on the number of <code>Things</code> that can be around in the program at once, and indeed it means that if other users add new Properties it will expand memory usage in an unintuitive way.</p>\n\n<p>In essence, we are able to give unique ID's to each <code>Property</code> without a problem. But how can one create a data structure to hold these <code>Properties</code> that permits easy access (close to <strong>O(1)</strong>) while also not itself using a bunch of space? Does such a structure exist, or can it be created using the structures Java provides? Is it simply a special case of a Hashtable (<code>Hashtable&lt;Integer,Property&gt;</code>) ? In my experience, using a hashtable for integers is not generally a large gain over a sparse array. </p>\n\n<p>For instance, if we assume 12 bytes per reference, a 1000 property static deck would add 12000 bytes to each <code>Thing</code>. Since it's likely to be sparse, does the tradeoff where Things with a large number of properties exceeding the 12000 byte get balanced off by the number of Things which have few? And what about the access penalty (which granted will not be based on the number of <code>Properties</code>, but on the Hash algorithm) I'm not accounting for <code>Property</code> size, which may vary since it is an interface.</p>\n\n<p>Any ideas are appreciated.</p>\n"},{"tags":["java","sql","database","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":92,"score":2,"question_id":12590501,"title":"Performance hit when persisting hundreds of millions of SQL Timestamp objects","body":"<p>During execution of a program that relies on the <code>oracle.sql</code> package  there is a large performance hit for persisting > 200 million Timestamps when compared to persisting the same number of longs.  </p>\n\n<p><a href=\"http://sqlfiddle.com/#!4/62491\" rel=\"nofollow\">Basic Schema</a></p>\n\n<p>Java to persist:  </p>\n\n<pre><code>Collection&lt;ARRAY&gt; longs = new ArrayList&lt;ARRAY&gt;(SIZE);\nCollection&lt;ARRAY&gt; timeStamps = new ArrayList&lt;ARRAY&gt;(SIZE);\nfor(int i = 0; i &lt; SIZE;i++)  \n{  \n    longs.add(new ARRAY(description, connection, i));  \n    timeStamps.add(new ARRAY(description,connection,new Timestamp(new Long(i)));\n}  \n\nStatement timeStatement = conn.createStatement();  \nstatement.setObject(1,timeStamps);  \nstatement.execute();   //5 minutes\n\nStatement longStatement = conn.createStatement();  \nstatement.setObject(1,longs);  \nstatement.execute();  //1 minutes 15 seconds\n</code></pre>\n\n<p>My question is what does Oracle do to Timestamps that make them so awful to insert in a bulk manner?</p>\n\n<p>Configuration:  </p>\n\n<pre><code>64 bit RHEL 5  \njre 6u16  \nojdbc14.jar\n64 GB dedicated to the JVM\n</code></pre>\n\n<p><strong>UPDATE</strong><br>\n  <code>java.sql.Timestamp</code> is being used</p>\n"},{"tags":["linux","performance","apache","benchmarking"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":38,"score":2,"question_id":12609790,"title":"Website Benchmarking using ab","body":"<p>I am trying my hand at various benchmarking tools for the website I am working on and have found <code>Apache Bench</code> (ab) to be an excellent tool for load testing. It is a command line tool and is very easy to use, apparently. However I have a doubt about two of its basic flags. The site I was reading says:</p>\n\n<pre><code>Suppose we want to see how fast Yahoo can handle 100 requests, with a maximum of 10 requests running concurrently:\n\nab -n 100 -c 10 http://www.yahoo.com/\n</code></pre>\n\n<p>and the explanation for the flags states:</p>\n\n<pre><code>Usage: ab [options] [http[s]://]hostname[:port]/path\nOptions are:\n    -n requests     Number of requests to perform\n    -c concurrency  Number of multiple requests to make\n</code></pre>\n\n<p>I guess I am just not able to wrap my head around <code>number of requests to perform</code> and <code>number of multiple requests to make</code>. What happens when I give them both together like in the example above?</p>\n\n<p>Can anyone give me a simpler explanation of what these two flags do together?</p>\n"},{"tags":["performance","sqlbulkcopy","dapper"],"answer_count":1,"favorite_count":0,"up_vote_count":10,"down_vote_count":0,"view_count":397,"score":10,"question_id":10689779,"title":"Bulk inserts taking longer than expected using Dapper","body":"<p>After reading <a href=\"http://www.altdevblogaday.com/2012/05/16/sql-server-high-performance-inserts/\" rel=\"nofollow\">this article</a> I decided to take a closer look at the way I was using Dapper.</p>\n\n<p>I ran this code on an empty database</p>\n\n<pre><code>var members = new List&lt;Member&gt;();\nfor (int i = 0; i &lt; 50000; i++)\n{\n    members.Add(new Member()\n    {\n        Username = i.toString(),\n        IsActive = true\n    });\n}\n\nusing (var scope = new TransactionScope())\n{\n    connection.Execute(@\"\ninsert Member(Username, IsActive)\nvalues(@Username, @IsActive)\", members);\n\n    scope.Complete();\n}\n</code></pre>\n\n<p>it took about 20 seconds. That's 2500 inserts/second. Not bad, but not great either considering the blog was achieving 45k inserts/second. Is there a more efficient way to do this in Dapper?</p>\n\n<p>Also, as a side note, running this code through the Visual Studio debugger took <strong>over 3 minutes!</strong> I figured the debugger would slow it down a little, but I was really surprised to see that much.</p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>So this</p>\n\n<pre><code>using (var scope = new TransactionScope())\n{\n    connection.Execute(@\"\ninsert Member(Username, IsActive)\nvalues(@Username, @IsActive)\", members);\n\n    scope.Complete();\n}\n</code></pre>\n\n<p>and this</p>\n\n<pre><code>    connection.Execute(@\"\ninsert Member(Username, IsActive)\nvalues(@Username, @IsActive)\", members);\n</code></pre>\n\n<p>both took 20 seconds.</p>\n\n<p>But this took 4 seconds!</p>\n\n<pre><code>SqlTransaction trans = connection.BeginTransaction();\n\nconnection.Execute(@\"\ninsert Member(Username, IsActive)\nvalues(@Username, @IsActive)\", members, transaction: trans);\n\ntrans.Commit();\n</code></pre>\n"},{"tags":["sql","sql-server","performance","tsql","query-optimization"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":64,"score":2,"question_id":10744736,"title":"Which is the actual query execution time with Statistic Time enabled in T-SQL?","body":"<p>When Statistics Time is enabled in the SQL Sever options in SQL Server Management Studio, I get the following information.</p>\n\n<blockquote>\n  <p>SQL Server parse and compile time:     CPU time = 15 ms, elapsed time\n  = 47 ms.</p>\n  \n  <p>(1745 row(s) affected)</p>\n  \n  <p>SQL Server Execution Times:    CPU time = 31 ms,  elapsed time = 99\n  ms. SQL Server parse and compile time:     CPU time = 0 ms, elapsed\n  time = 0 ms.</p>\n</blockquote>\n\n<p>There are four times in milliseconds. Now, which one is the actual time it took to run the query? They don't seem to be relative.</p>\n"},{"tags":["performance","delphi","text-rendering"],"answer_count":3,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":159,"score":1,"question_id":12607720,"title":"How do I render text quickly when each character needs separate placement and formatting?","body":"<p>I try to display formatted text on the screen. At first the very simple HTML text is parsed (there are tags like b,u,i) and then each character is rendered using <code>Canvas.TextOut</code> function in appropriate position and font.</p>\n\n<p>The first thing I noticed is, that rendering of every separate character on the canvas is rather slow. The rendering of whole sentence is much faster. It is obvious, when the canvas is forced to repaint, when form is moved around the screen.</p>\n\n<p>One solution would be to cluster the characters with even fonts and render them at once. But it won't help too much, when the formatting is rich. In addition I need the characters to be the discrete entities, which could be rendered in any way. For example, there is no WinAPI to support text alignment taJustify or in block writing...</p>\n\n<p>Another approach is to render on bitmap, or to use wisely <code>ClipRect</code> property of TCanvas (I haven't tried yet). </p>\n\n<p>Anyway, when the same formatted text is displayed in TRichEdit, there is no time penalty by repaint operation. Another quick example are all major browsers, which has no problem to display tons of formated text... do they render each character like I do, but they do it more efficiently ??? I do not know. </p>\n\n<p>So do you know some recipe to speeding up the application (formatted text rendering?).</p>\n\n<p>Thanx for your ideas...</p>\n\n<p>Sample code: (make TForm as big as possible, grab it with mouse and drag it down under screen. When you move it up, you will see \"jumpy\" movement)</p>\n\n<pre><code>procedure TForm1.FormPaint(Sender: TObject);\nvar i, w, h, j:integer;\n    s:string;\n    switch:Boolean;\nbegin\n   w:=0;\n   h:=0;\n   s:='';\n   for j:=0 to 5 do\n       for i:=65 to 90 do s:=s + Char(i);\n\n   switch:=False; // set true to see the difference\n\n   if switch then\n     begin\n     for j:=0 to 70 do begin\n         for i := 1 to Length(s) do\n         begin\n         Form1.Canvas.TextOut(50+ w,h +70 , s[i]);\n         w:=w +  Form1.Canvas.TextWidth(s[i]);\n         end;\n         w:=0;\n         h:=h+15;\n         end;\n     end\n    else\n      begin\n      for j:=0 to 70 do begin\n       Form1.Canvas.TextOut(50+ w,h +70 , s);\n       w:=w +  Form1.Canvas.TextWidth(s);  // not optimalized just for comparison\n       w:=0;                               // not optimalized just for comparison\n       h:=h+15;\n       end;\n      end;\nend;\n</code></pre>\n"},{"tags":["performance","linq","fluent-nhibernate","benchmarking","queryover"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":106,"score":2,"question_id":12578019,"title":"NHibernate LINQ query performance, which code fragment is better?","body":"<p>This question is asking which of the two approaches below is more encouraged (and with what reasons)?</p>\n\n<p>I am working with FluentNHibernate in a ServiceStack REST application using C# 4.0, but this question is general to NHibernate LINQ queries.</p>\n\n<p>Is it more encouraged to:</p>\n\n<p><strong>(Method 1)</strong> quickly run a <strong>simple</strong> query which returns all rows where it matches the user's id:</p>\n\n<pre><code>// Query the User by id\nvar user = session.Get&lt;User&gt;(request.UserId);\n</code></pre>\n\n<p>and then separately use LINQ on the returned List to further narrow down the results:</p>\n\n<pre><code>// 'User' contains a 'List&lt;Location&gt;'\nvar locations = user.Locations.Where(location =&gt;\n                        location.Timestamp &gt;= (request.MinTimestamp.HasValue ? request.MinTimestamp.Value : 0) &amp;&amp;\n                        location.Timestamp &lt;= (request.MaxTimestamp.HasValue ? request.MaxTimestamp.Value : DateTime.Now.ToTimestamp()));\n\nreturn locations;\n</code></pre>\n\n<p><strong>(Method 2) or</strong>, is it more encouraged to run a <strong>more complicated</strong> query which does the above in a single query:</p>\n\n<pre><code>var locationsQuery = session.QueryOver&lt;LocationModel&gt;()\n                        .Where(table =&gt; table.User.Id == request.UserId)\n                        .And(table =&gt; table.Timestamp &gt;= (request.MinTimestamp.HasValue ? request.MinTimestamp.Value : 0))\n                        .And(table =&gt; table.Timestamp &lt;= (request.MaxTimestamp.HasValue ? request.MaxTimestamp.Value : DateTime.Now.ToTimestamp()));\n\nreturn locationsQuery.List();\n</code></pre>\n\n<p><strong>if</strong> my goals are:</p>\n\n<p>a) faster execution time</p>\n\n<hr>\n\n<p><b>Benchmarks (revised)</b></p>\n\n<p>Revised Complete Test Code: <a href=\"http://pastebin.com/0ykKwcxX\" rel=\"nofollow\">http://pastebin.com/0ykKwcxX</a></p>\n\n<p><strong>Benchmarks Output:</strong></p>\n\n<p><strong>Method 1</strong> took <strong>147.291 seconds</strong> over <strong>5000 iterations</strong>.</p>\n\n<p><em>Query results of the last iteration:</em> <br>\n{ Timestamp=1348659703485, Latitude=179.40000, Longitude=209.40000 } <br>\n{ Timestamp=1348659703486, Latitude=179.55000, Longitude=209.55000 } <br>\n{ Timestamp=1348659703487, Latitude=179.70000, Longitude=209.70000 } <br>\n{ Timestamp=1348659703488, Latitude=179.85000, Longitude=209.85000 } <br>\n{ Timestamp=1348659703489, Latitude=180.00000, Longitude=210.00000 } <br></p>\n\n<p><strong>Method 2</strong> took <strong>133.728 seconds</strong> over <strong>5000 iterations</strong>.</p>\n\n<p><em>Query results of the last iteration:</em> <br>\n{ Timestamp=1348659703485, Latitude=179.40000, Longitude=209.40000 } <br>\n{ Timestamp=1348659703486, Latitude=179.55000, Longitude=209.55000 } <br>\n{ Timestamp=1348659703487, Latitude=179.70000, Longitude=209.70000 } <br>\n{ Timestamp=1348659703488, Latitude=179.85000, Longitude=209.85000 } <br>\n{ Timestamp=1348659703489, Latitude=180.00000, Longitude=210.00000 } <br></p>\n\n<p><b>Difference: Method 2 was approximately 13.5 seconds faster.</b></p>\n\n<hr>\n\n<p>b) long-term re-use and stability</p>\n"},{"tags":["performance","application","monitoring"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":547,"score":0,"question_id":2407233,"title":"Monitoring my web application performance from the client browser","body":"<p>I want to be able to monitor the performance(load time of the entire page, load times of individually downloaded js/cs files , amount of memory used by the browser for the page,etc) of my web application from the perspective of the user(i.e the browser client).\nIs there any tool/plugin , that can help me monitor all of these?</p>\n"},{"tags":["c#","winforms","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":71,"score":0,"question_id":12607409,"title":"Why does memory usage increase with each click to the button in Windows Form Application?","body":"<p>Even though there is no event related to the button, every time i clicked the button the memory usage increases slowly. What causes this? Thank you.</p>\n"},{"tags":["c++","performance","opencv","matrix-multiplication","eigen"],"answer_count":2,"favorite_count":4,"up_vote_count":4,"down_vote_count":0,"view_count":221,"score":4,"question_id":12597397,"title":"Fastest way to calculate mininum euclidean distance between two matrices containing high dimensional vectors","body":"<p>I started a similar question on <a href=\"http://stackoverflow.com/questions/12479663/cvmat-cv-8u-product-error-and-slow-cv-32f-product/12527400#12527400\">another thread</a>, but then I was focusing on how to use OpenCV. Having failed to achieve what I originally wanted, I will ask here exactly what I want.</p>\n\n<p>I have two matrices. Matrix a is 2782x128 and Matrix b is 4000x128, both unsigned char values. The values are stored in a single array. For each vector in a, I need the index of the vector in b with the closest euclidean distance.</p>\n\n<p>Ok, now my code to achieve this:</p>\n\n<pre><code>#include &lt;windows.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;cstdio&gt;\n#include &lt;math.h&gt;\n#include &lt;time.h&gt;\n#include &lt;sys/timeb.h&gt;\n#include &lt;iostream&gt;\n#include &lt;fstream&gt;\n#include \"main.h\"\n\nusing namespace std;\n\nvoid main(int argc, char* argv[])\n{\n    int a_size;\n    unsigned char* a = NULL;\n    read_matrix(&amp;a, a_size,\"matrixa\");\n    int b_size;\n    unsigned char* b = NULL;\n    read_matrix(&amp;b, b_size,\"matrixb\");\n\n    LARGE_INTEGER liStart;\n    LARGE_INTEGER liEnd;\n    LARGE_INTEGER liPerfFreq;\n    QueryPerformanceFrequency( &amp;liPerfFreq );\n    QueryPerformanceCounter( &amp;liStart );\n\n    int* indexes = NULL;\n    min_distance_loop(&amp;indexes, b, b_size, a, a_size);\n\n    QueryPerformanceCounter( &amp;liEnd );\n\n    cout &lt;&lt; \"loop time: \" &lt;&lt; (liEnd.QuadPart - liStart.QuadPart) / long double(liPerfFreq.QuadPart) &lt;&lt; \"s.\" &lt;&lt; endl;\n\n    if (a)\n    delete[]a;\nif (b)\n    delete[]b;\nif (indexes)\n    delete[]indexes;\n    return;\n}\n\nvoid read_matrix(unsigned char** matrix, int&amp; matrix_size, char* matrixPath)\n{\n    ofstream myfile;\n    float f;\n    FILE * pFile;\n    pFile = fopen (matrixPath,\"r\");\n    fscanf (pFile, \"%d\", &amp;matrix_size);\n    *matrix = new unsigned char[matrix_size*128];\n\n    for (int i=0; i&lt;matrix_size*128; ++i)\n    {\n        unsigned int matPtr;\n        fscanf (pFile, \"%u\", &amp;matPtr);\n        matrix[i]=(unsigned char)matPtr;\n    }\n    fclose (pFile);\n}\n\nvoid min_distance_loop(int** indexes, unsigned char* b, int b_size, unsigned char* a, int a_size)\n{\n    const int descrSize = 128;\n\n    *indexes = (int*)malloc(a_size*sizeof(int));\n    int dataIndex=0;\n    int vocIndex=0;\n    int min_distance;\n    int distance;\n    int multiply;\n\n    unsigned char* dataPtr;\n    unsigned char* vocPtr;\n    for (int i=0; i&lt;a_size; ++i)\n    {\n        min_distance = LONG_MAX;\n        for (int j=0; j&lt;b_size; ++j)\n        {\n            distance=0;\n            dataPtr = &amp;a[dataIndex];\n            vocPtr = &amp;b[vocIndex];\n\n            for (int k=0; k&lt;descrSize; ++k)\n            {\n                multiply = *dataPtr++-*vocPtr++;\n                distance += multiply*multiply;\n                // If the distance is greater than the previously calculated, exit\n                if (distance&gt;min_distance)\n                    break;\n            }\n\n            // if distance smaller\n            if (distance&lt;min_distance)\n            {\n                min_distance = distance;\n                (*indexes)[i] = j;\n            }\n            vocIndex+=descrSize;\n        }\n        dataIndex+=descrSize;\n        vocIndex=0;\n    }\n}\n</code></pre>\n\n<p>And attached are the files with sample matrices.</p>\n\n<p><a href=\"https://dl.dropbox.com/u/1474325/matrixa\" rel=\"nofollow\">matrixa</a>\n<a href=\"https://dl.dropbox.com/u/1474325/matrixb\" rel=\"nofollow\">matrixb</a></p>\n\n<p>I am using windows.h just to calculate the consuming time, so if you want to test the code in another platform than windows, just change windows.h header and change the way of calculating the consuming time.</p>\n\n<p>This code in my computer is about 0.5 seconds. The problem is that I have another code in Matlab that makes this same thing in 0.05 seconds. In my experiments, I am receiving several matrices like matrix a every second, so 0.5 seconds is too much.</p>\n\n<p>Now the matlab code to calculate this:</p>\n\n<pre><code>aa=sum(a.*a,2); bb=sum(b.*b,2); ab=a*b'; \nd = sqrt(abs(repmat(aa,[1 size(bb,1)]) + repmat(bb',[size(aa,1) 1]) - 2*ab));\n[minz index]=min(d,[],2);\n</code></pre>\n\n<p>Ok. Matlab code is using that (x-a)^2 = x^2 + a^2 - 2ab.</p>\n\n<p>So my next attempt was to do the same thing. I deleted my own code to make the same calculations, but It was 1.2 seconds approx.</p>\n\n<p>Then, I tried to use different external libraries. The first attempt was Eigen:</p>\n\n<pre><code>const int descrSize = 128;\nMatrixXi a(a_size, descrSize);\nMatrixXi b(b_size, descrSize);\nMatrixXi ab(a_size, b_size);\n\nunsigned char* dataPtr = matrixa;\nfor (int i=0; i&lt;nframes; ++i)\n{\n    for (int j=0; j&lt;descrSize; ++j)\n    {\n        a(i,j)=(int)*dataPtr++;\n    }\n}\nunsigned char* vocPtr = matrixb;\nfor (int i=0; i&lt;vocabulary_size; ++i)\n{\n    for (int j=0; j&lt;descrSize; ++j)\n    {\n        b(i,j)=(int)*vocPtr ++;\n    }\n}\nab = a*b.transpose();\na.cwiseProduct(a);\nb.cwiseProduct(b);\nMatrixXi aa = a.rowwise().sum();\nMatrixXi bb = b.rowwise().sum();\nMatrixXi d = (aa.replicate(1,vocabulary_size) + bb.transpose().replicate(nframes,1) - 2*ab).cwiseAbs2();\n\nint* index = NULL;\nindex = (int*)malloc(nframes*sizeof(int));\nfor (int i=0; i&lt;nframes; ++i)\n{\n    d.row(i).minCoeff(&amp;index[i]);\n}\n</code></pre>\n\n<p>This Eigen code costs 1.2 approx for just the line that says: ab = a*b.transpose();</p>\n\n<p>A similar code using opencv was used also, and the cost of the ab = a*b.transpose(); was 0.65 seconds.</p>\n\n<p>So, It is real annoying that matlab is able to do this same thing so quickly and I am not able in C++! Of course being able to run my experiment would be great, but I think the lack of knowledge is what really is annoying me. How can I achieve at least the same performance than in Matlab? Any kind of soluting is welcome. I mean, any external library (free if possible), loop unrolling things, template things, SSE intructions (I know they exist), cache things. As I said, my main purpose is increase my knowledge for being able to code thinks like this with a faster performance.</p>\n\n<p>Thanks in advance</p>\n\n<p>EDIT: more code suggested by David Hammen. I casted the arrays to int before making any calculations. Here is the code:</p>\n\n<pre><code>void min_distance_loop(int** indexes, unsigned char* b, int b_size, unsigned char* a, int a_size)\n{\n    const int descrSize = 128;\n\n    int* a_int;\n    int* b_int;\n\n    LARGE_INTEGER liStart;\n    LARGE_INTEGER liEnd;\n    LARGE_INTEGER liPerfFreq;\n    QueryPerformanceFrequency( &amp;liPerfFreq );\n    QueryPerformanceCounter( &amp;liStart );\n\n    a_int = (int*)malloc(a_size*descrSize*sizeof(int));\n    b_int = (int*)malloc(b_size*descrSize*sizeof(int));\n\n    for(int i=0; i&lt;descrSize*a_size; ++i)\n        a_int[i]=(int)a[i];\n    for(int i=0; i&lt;descrSize*b_size; ++i)\n        b_int[i]=(int)b[i];\n\n    QueryPerformanceCounter( &amp;liEnd );\n\n    cout &lt;&lt; \"Casting time: \" &lt;&lt; (liEnd.QuadPart - liStart.QuadPart) / long double(liPerfFreq.QuadPart) &lt;&lt; \"s.\" &lt;&lt; endl;\n\n    *indexes = (int*)malloc(a_size*sizeof(int));\n    int dataIndex=0;\n    int vocIndex=0;\n    int min_distance;\n    int distance;\n    int multiply;\n\n    /*unsigned char* dataPtr;\n    unsigned char* vocPtr;*/\n    int* dataPtr;\n    int* vocPtr;\n    for (int i=0; i&lt;a_size; ++i)\n    {\n        min_distance = LONG_MAX;\n        for (int j=0; j&lt;b_size; ++j)\n        {\n            distance=0;\n            dataPtr = &amp;a_int[dataIndex];\n            vocPtr = &amp;b_int[vocIndex];\n\n            for (int k=0; k&lt;descrSize; ++k)\n            {\n                multiply = *dataPtr++-*vocPtr++;\n                distance += multiply*multiply;\n                // If the distance is greater than the previously calculated, exit\n                if (distance&gt;min_distance)\n                    break;\n            }\n\n            // if distance smaller\n            if (distance&lt;min_distance)\n            {\n                min_distance = distance;\n                (*indexes)[i] = j;\n            }\n            vocIndex+=descrSize;\n        }\n        dataIndex+=descrSize;\n        vocIndex=0;\n    }\n}\n</code></pre>\n\n<p>The entire process is now 0.6, and the casting loops at the beginning are 0.001 seconds. Maybe I did something wrong?</p>\n\n<p>EDIT2: Anything about Eigen? When I look for external libs they always talk about Eigen and their speed. I made something wrong? Here a simple code using Eigen that shows it is not so fast. Maybe I am missing some config or some flag, or ...</p>\n\n<pre><code>MatrixXd A = MatrixXd::Random(1000, 1000);\nMatrixXd B = MatrixXd::Random(1000, 500);\nMatrixXd X;\n</code></pre>\n\n<p>This code is about 0.9 seconds.</p>\n"},{"tags":["sql","performance","postgresql","database-design","index"],"answer_count":4,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":189,"score":7,"question_id":12544247,"title":"PostgreSQL: How to structure and index time-related data for optimal query performance?","body":"<p><strong>The Problem:</strong></p>\n\n<p>I have time-related data in my database and I am struggling to organize, structure and index that data in a way so that users can retrieve it efficiently; even simple database queries take longer than acceptable.</p>\n\n<p><strong>Project Context:</strong></p>\n\n<p>While this is a pure database question, some context might help to understand the data model:</p>\n\n<p>The project centers around doing research on a big, complex machine. I don't know a lot about the machine itself, but rumour in the lab has it there's a <a href=\"http://en.wikipedia.org/wiki/DeLorean_time_machine#Flux_capacitor\" rel=\"nofollow\">flux capacitor</a> in there somewhere - and I think yesterday, I spotted the tail of <a href=\"http://en.wikipedia.org/wiki/Schr%C3%B6dinger%27s_cat\" rel=\"nofollow\">Schrödinger's cat</a> hanging out of it at the side ;-)</p>\n\n<p>We measure many different <em>parameters</em> while the machine is running using sensors positioned all over the machine at different measurement points (so-called <em>spots</em>) at certain intervals over a period of time. We use not only one <em>device</em> to measure these parameters, but a whole range of them; they differ in the quality of their measurement data (I think this involves sample rates, sensor quality, price and many other aspects that I'm not concerned with); one aim of the project actually is to establish a comparison between these devices. You can visualize these measurement devices as a bunch of lab trolleys, each with a lot of cables connected to the machine, each delivering measurement data. </p>\n\n<p><strong>The Data Model:</strong></p>\n\n<p>There is measurement data from every spot and every device for every parameter, for example once a minute over a period of 6 days. My job is to store that data in a database and to provide efficient access to it.</p>\n\n<p>In a nutshell:</p>\n\n<ul>\n<li>a device has a unique name</li>\n<li>a parameter also has a name; they're not unique though, so it also has an ID</li>\n<li>a spot has an ID</li>\n</ul>\n\n<p>The project database is more complex of course, but these details don't seem relevant to the issue.</p>\n\n<ul>\n<li>a measurement data <em>index</em> has an ID, a time stamp for when the measurement was done and references to the device and the spot on which the measurement was carried out</li>\n<li>a measurement data <em>value</em> has a reference to the parameter and to the value that was actually measured</li>\n</ul>\n\n<p>Initially, I had modeled the measurement data value to have its own ID as primary key; the <code>n:m</code> relationship between measurement data index and value was a separate table that only stored <code>index:value</code> ID pairs, but as that table itself consumed quite a lot of harddrive space, we eliminated it and changed the value ID to be a simple integer that stores the ID of the measurement data index it belongs to; the primary key of the measurement data value is now composed of that ID and the parameter ID.</p>\n\n<p><em>On a side note</em>: When I created the data model, I carefully followed common design guidelines like <a href=\"http://en.wikipedia.org/wiki/Third_normal_form\" rel=\"nofollow\">3NF</a> and appropriate table constraints (such as unique keys); another rule of thumb was to create an index for every foreign key. I have a suspicion that the deviation in the measurement data index / value tables from 'strict' 3NF might be one of the reasons for the performance issues I am looking at now, but changing the data model back has not solved the problem.</p>\n\n<p><strong>The Data Model in DDL:</strong></p>\n\n<p><strong>NOTE:</strong> There is an update to this code further below.</p>\n\n<p>The script below creates the database and all tables involved. Please note that there are no explicit indexes yet. Before you run this, please make sure you don't happen to already have a database called <code>so_test</code> with any valuable data...</p>\n\n<pre><code>\\c postgres\nDROP DATABASE IF EXISTS so_test;\nCREATE DATABASE so_test;\n\\c so_test\n\nCREATE TABLE device\n(\n  name VARCHAR(16) NOT NULL,\n  CONSTRAINT device_pk PRIMARY KEY (name)\n);\n\nCREATE TABLE parameter\n(\n  -- must have ID as names are not unique\n  id SERIAL,\n  name VARCHAR(64) NOT NULL,\n  CONSTRAINT parameter_pk PRIMARY KEY (id)\n);\n\nCREATE TABLE spot\n(\n  id SERIAL,\n  CONSTRAINT spot_pk PRIMARY KEY (id)\n);\n\nCREATE TABLE measurement_data_index\n(\n  id SERIAL,\n  fk_device_name VARCHAR(16) NOT NULL,\n  fk_spot_id INTEGER NOT NULL,\n  t_stamp TIMESTAMP NOT NULL,\n  CONSTRAINT measurement_pk PRIMARY KEY (id),\n  CONSTRAINT measurement_data_index_fk_2_device FOREIGN KEY (fk_device_name)\n    REFERENCES device (name) MATCH FULL\n    ON UPDATE NO ACTION ON DELETE NO ACTION,\n  CONSTRAINT measurement_data_index_fk_2_spot FOREIGN KEY (fk_spot_id)\n    REFERENCES spot (id) MATCH FULL\n    ON UPDATE NO ACTION ON DELETE NO ACTION,\n  CONSTRAINT measurement_data_index_uk_all_cols UNIQUE (fk_device_name, fk_spot_id, t_stamp)\n);\n\nCREATE TABLE measurement_data_value\n(\n  id INTEGER NOT NULL,\n  fk_parameter_id INTEGER NOT NULL,\n  value VARCHAR(16) NOT NULL,\n  CONSTRAINT measurement_data_value_pk PRIMARY KEY (id, fk_parameter_id),\n  CONSTRAINT measurement_data_value_fk_2_parameter FOREIGN KEY (fk_parameter_id)\n    REFERENCES parameter (id) MATCH FULL\n    ON UPDATE NO ACTION ON DELETE NO ACTION\n);\n</code></pre>\n\n<p>I have also created a script to fill the table with some test data:</p>\n\n<pre><code>CREATE OR REPLACE FUNCTION insert_data()\nRETURNS VOID\nLANGUAGE plpgsql\nAS\n$BODY$\n  DECLARE\n    t_stamp  TIMESTAMP := '2012-01-01 00:00:00';\n    index_id INTEGER;\n    param_id INTEGER;\n    dev_name VARCHAR(16);\n    value    VARCHAR(16);\n  BEGIN\n    FOR dev IN 1..5\n    LOOP\n      INSERT INTO device (name) VALUES ('dev_' || to_char(dev, 'FM00'));\n    END LOOP;\n    FOR param IN 1..20\n    LOOP\n      INSERT INTO parameter (name) VALUES ('param_' || to_char(param, 'FM00'));\n    END LOOP;\n    FOR spot IN 1..10\n    LOOP\n      INSERT INTO spot (id) VALUES (spot);\n    END LOOP;\n\n    WHILE t_stamp &lt; '2012-01-07 00:00:00'\n    LOOP\n      FOR dev IN 1..5\n      LOOP\n        dev_name := 'dev_' || to_char(dev, 'FM00');\n        FOR spot IN 1..10\n        LOOP\n          INSERT INTO measurement_data_index\n            (fk_device_name, fk_spot_id, t_stamp)\n            VALUES (dev_name, spot, t_stamp) RETURNING id INTO index_id;\n          FOR param IN 1..20\n          LOOP\n            SELECT id INTO param_id FROM parameter\n              WHERE name = 'param_' || to_char(param, 'FM00');\n            value := 'd'  || to_char(dev,   'FM00')\n                  || '_s' || to_char(spot,  'FM00')\n                  || '_p' || to_char(param, 'FM00');\n            INSERT INTO measurement_data_value (id, fk_parameter_id, value)\n              VALUES (index_id, param_id, value);\n          END LOOP;\n        END LOOP;\n      END LOOP;\n      t_stamp := t_stamp + '1 minute'::INTERVAL;\n    END LOOP;\n\n  END;\n$BODY$;\n\nSELECT insert_data();\n</code></pre>\n\n<p>The PostgreSQL query planner requires up to date statistics, so analyze all tables. Vacuuming might not be required, but do it anyway:</p>\n\n<pre><code>VACUUM ANALYZE device;\nVACUUM ANALYZE measurement_data_index;\nVACUUM ANALYZE measurement_data_value;\nVACUUM ANALYZE parameter;\nVACUUM ANALYZE spot;\n</code></pre>\n\n<p><strong>A Sample Query:</strong></p>\n\n<p>If I now run a really simple query to e.g. obtain all value for a certain parameter, it already takes a couple of seconds, although the database is not very large yet:</p>\n\n<pre><code>EXPLAIN (ANALYZE ON, BUFFERS ON)\nSELECT measurement_data_value.value\n  FROM measurement_data_value, parameter\n WHERE measurement_data_value.fk_parameter_id = parameter.id\n   AND parameter.name = 'param_01';\n</code></pre>\n\n<p>Exemplary result on my development machine (please see below for some details on my environment):</p>\n\n<pre><code>                                                                QUERY PLAN                                                                \n------------------------------------------------------------------------------------------------------------------------------------------\n Hash Join  (cost=1.26..178153.26 rows=432000 width=12) (actual time=0.046..2281.281 rows=432000 loops=1)\n   Hash Cond: (measurement_data_value.fk_parameter_id = parameter.id)\n   Buffers: shared hit=55035\n   -&gt;  Seq Scan on measurement_data_value  (cost=0.00..141432.00 rows=8640000 width=16) (actual time=0.004..963.999 rows=8640000 loops=1)\n         Buffers: shared hit=55032\n   -&gt;  Hash  (cost=1.25..1.25 rows=1 width=4) (actual time=0.010..0.010 rows=1 loops=1)\n         Buckets: 1024  Batches: 1  Memory Usage: 1kB\n         Buffers: shared hit=1\n         -&gt;  Seq Scan on parameter  (cost=0.00..1.25 rows=1 width=4) (actual time=0.004..0.008 rows=1 loops=1)\n               Filter: ((name)::text = 'param_01'::text)\n               Buffers: shared hit=1\n Total runtime: 2313.615 ms\n(12 rows)\n</code></pre>\n\n<p>There are no indexes in the database apart from the implicit ones, so it's not surprising the planner does sequential scans only. If I follow what seems to be a rule of thumb and add btree indexes for every foreign key like</p>\n\n<pre><code>CREATE INDEX measurement_data_index_idx_fk_device_name\n    ON measurement_data_index (fk_device_name);\nCREATE INDEX measurement_data_index_idx_fk_spot_id\n    ON measurement_data_index (fk_spot_id);\nCREATE INDEX measurement_data_value_idx_fk_parameter_id\n    ON measurement_data_value (fk_parameter_id);\n</code></pre>\n\n<p>then do another vacuum analyze (just to be safe) and re-run the query, the planner uses bitmap heap and bitmap index scans and the total query time somewhat improves:</p>\n\n<pre><code>                                                                                   QUERY PLAN                                                                                   \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n Nested Loop  (cost=8089.19..72842.42 rows=431999 width=12) (actual time=66.773..1336.517 rows=432000 loops=1)\n   Buffers: shared hit=55033 read=1184\n   -&gt;  Seq Scan on parameter  (cost=0.00..1.25 rows=1 width=4) (actual time=0.005..0.012 rows=1 loops=1)\n         Filter: ((name)::text = 'param_01'::text)\n         Buffers: shared hit=1\n   -&gt;  Bitmap Heap Scan on measurement_data_value  (cost=8089.19..67441.18 rows=431999 width=16) (actual time=66.762..1237.488 rows=432000 loops=1)\n         Recheck Cond: (fk_parameter_id = parameter.id)\n         Buffers: shared hit=55032 read=1184\n         -&gt;  Bitmap Index Scan on measurement_data_value_idx_fk_parameter_id  (cost=0.00..7981.19 rows=431999 width=0) (actual time=65.222..65.222 rows=432000 loops=1)\n               Index Cond: (fk_parameter_id = parameter.id)\n               Buffers: shared read=1184\n Total runtime: 1371.716 ms\n(12 rows)\n</code></pre>\n\n<p>However, this is still more than a second of execution time for a really simple query.</p>\n\n<p><strong>What I have done so far:</strong></p>\n\n<ul>\n<li>got myself a copy of <a href=\"http://www.packtpub.com/postgresql-90-high-performance/book\" rel=\"nofollow\">PostgreSQL 9.0 High Performance</a> - great book!</li>\n<li>did some basic PostgreSQL server configuration, see environment below</li>\n<li>created a framework to run a series of performance tests using real queries from the project and to display the results graphically; these queries use devices, spots, parameters and a time interval as input parameters and the test series run over e.g. 5, 10 devices, 5, 10 spots, 5, 10, 15, 20 parameters and 1..7 days. The basic result is that they're all too slow, but their query plan was way too complex for me to understand, so I went back to the really simple query used above.</li>\n</ul>\n\n<p>I have looked into <a href=\"http://www.postgresql.org/docs/current/interactive/ddl-partitioning.html\" rel=\"nofollow\">partitioning</a> the value table. The data is time-related and partitioning seems an appropriate means to organize that kind of data; even the <a href=\"http://www.postgresql.org/docs/current/interactive/ddl-partitioning.html#DDL-PARTITIONING-IMPLEMENTATION\" rel=\"nofollow\">examples</a> in the PostgreSQL documentation use something similar. However, I read in the <a href=\"http://www.postgresql.org/docs/current/interactive/ddl-partitioning.html#DDL-PARTITIONING-OVERVIEW\" rel=\"nofollow\">same article</a>:</p>\n\n<blockquote>\n  <p>The benefits will normally be worthwhile only when a table would otherwise be very large. The exact point at which a table will benefit from partitioning depends on the application, although a rule of thumb is that the size of the table should exceed the physical memory of the database server.</p>\n</blockquote>\n\n<p>The entire test database is less than 1GB in size and I am running my tests on a development machine with 8GB of RAM and on a virtual machine with 1GB (see also environment below), so the table is far from being very large or even exceeding the physical memory. I might implement partitioning anyway at some stage, but I have a feeling that approach does not target the performance problem itself.</p>\n\n<p>Furthermore, I am considering to <a href=\"http://www.postgresql.org/docs/current/interactive/sql-cluster.html\" rel=\"nofollow\">cluster</a> the value table. I dislike the fact that clustering must be re-done whenever new data is inserted and that it furthermore requires an exclusive read/write lock, but looking at <a href=\"http://stackoverflow.com/a/9436055/217844\">this</a> SO question, it seems that it anyway has its benefits and might be an option. However, clustering is done on an index and as there are up to 4 selection criteria going into a query (devices, spots, parameters and time), I would have to create clusters for all of them - which in turn gives me the impression that I'm simply not creating the right indexes...</p>\n\n<p><strong>My Environment:</strong></p>\n\n<ul>\n<li>development is taking place on a MacBook Pro (mid-2009) with a dual-core CPU and 8GB of RAM</li>\n<li>I am running database performance tests on a virtual Debian 6.0 machine with 1GB of RAM, hosted on the MBP</li>\n<li>PostgreSQL version is 9.1 as that was the latest version when I installed it, upgrading to 9.2 would be possible</li>\n<li>I have changed <code>shared_buffers</code> from the default 1600kB to 25% of RAM on both machines as recommended in the <a href=\"http://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server#shared_buffers\" rel=\"nofollow\">PostgreSQL docs</a> (which involved enlarging <a href=\"http://www.postgresql.org/docs/current/interactive/kernel-resources.html\" rel=\"nofollow\">kernel settings</a> like SHMALL, SHMMAX, etc.)</li>\n<li>similarly, I have changed <a href=\"http://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server#effective_cache_size\" rel=\"nofollow\">effective_cache_size</a> from the default 128MB to 50% of the RAM available</li>\n<li>I ran performance test with different <a href=\"http://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server#work_mem\" rel=\"nofollow\">work_mem</a> settings, but did not see any major difference in performance</li>\n</ul>\n\n<p><strong>NOTE:</strong> One aspect that I believe is important is that the performance test series with real queries from the project do not differ performance-wise between the MacBook with 8GB and the virtual machine with 1GB; i.e. if a query takes 10s on the MacBook, it also takes 10s on the VM. Also, I ran the same performance tests before and after changing <code>shared_buffers</code>, <code>effective_cache_size</code> and <code>work_mem</code> and the configuration changes did not improve performance by more than 10%; some results in fact even got worse, so it seems any difference is caused rather by test variation than by configuration change. These observations lead me to believe that RAM and <code>postgres.conf</code> settings are not the limiting factors here yet. </p>\n\n<p><strong>My Questions:</strong></p>\n\n<p>I don't know if different or additional indexes would speed up the query and if they did, which ones to create. Looking at the size of the database and how simple my query is, I have the impression there is something fundamentally wrong about my data model or how I have chosen my indexes so far.</p>\n\n<p>Does anyone have some advice for me how to structure and index time-related my to improve query performance ?</p>\n\n<p>Asked more broadly, is tuning query performance</p>\n\n<ul>\n<li>usually done 'on an incident base', i.e. once a query does not perform satisfactorily ? It seems <em>all</em> my queries are too slow...</li>\n<li>mainly a question of looking at (and understanding) query plans, then adding indexes and measuring if things improved, possibly accelerating the process by applying one's experience ?</li>\n</ul>\n\n<p>How do I get this database to fly ?</p>\n\n<hr>\n\n<p><strong>Update 01:</strong></p>\n\n<p>Looking at the responses so far, I think I have not explained the need for measurement data index / values tables properly, so let me try again. <strong>Storage space</strong> is the issue here.</p>\n\n<p><strong>NOTE:</strong></p>\n\n<ul>\n<li>the figures used here are more of illustrative purpose and meant for comparison only, i.e. the numbers themselves are not relevant, what matters is the percental difference in storage requirements between using a single table vs. using an index and a value table</li>\n<li>PostgreSQL data type storage sizes are documented in <a href=\"http://www.postgresql.org/docs/current/interactive/datatype.html\" rel=\"nofollow\">this</a> chapter</li>\n<li>this makes no claim to be scientifically correct, e.g. the units are probably mathematical bogus; the numbers should add up though</li>\n</ul>\n\n<p>Assuming</p>\n\n<ul>\n<li>1 day of measurements</li>\n<li>1 set of measurements per minute</li>\n<li>10 devices</li>\n<li>10 parameters</li>\n<li>10 spots</li>\n</ul>\n\n<p>This adds up to</p>\n\n<pre>\n1 meas/min x 60 min/hour x 24 hour/day = 1440 meas/day\n</pre>\n\n<p>Each measurement has data from every spot and every device for every parameter, so</p>\n\n<pre>\n10 spots x 10 devices x 10 parameters = 1000 data sets/meas\n</pre>\n\n<p>So in total</p>\n\n<pre>\n1440 meas/day x 1000 data sets/meas = 1 440 000 data sets/day\n</pre>\n\n<p>If we store all measurements in a single table as <a href=\"http://stackoverflow.com/a/12545542/217844\">Catcall suggested</a>, e.g.</p>\n\n<pre><code>CREATE TABLE measurement_data\n(\n  device_name character varying(16) NOT NULL,\n  spot_id integer NOT NULL,\n  parameter_id integer NOT NULL,\n  t_stamp timestamp without time zone NOT NULL,\n  value character varying(16) NOT NULL,\n  -- constraints...\n);\n</code></pre>\n\n<p>a single row would add up to</p>\n\n<pre>\n17 + 4 + 4 + 8 + 17 = 50 bytes/row\n</pre>\n\n<p>in the worst case where all varchar fields are fully filled. This amounts to</p>\n\n<pre>\n50 bytes/row x 1 440 000 rows/day = 72 000 000 bytes/day\n</pre>\n\n<p>or ~69 MB per day.</p>\n\n<p>While this does not sound a lot, the storage space requirement in the real database would be prohibitive (again, the numbers used here are only for illustration). We have therefore split measurement data into an index and a value table as explained earlier in the question:</p>\n\n<pre><code>CREATE TABLE measurement_data_index\n(\n  id SERIAL,\n  fk_device_name VARCHAR(16) NOT NULL,\n  fk_spot_id INTEGER NOT NULL,\n  t_stamp TIMESTAMP NOT NULL,\n  -- constraints...\n);\n\nCREATE TABLE measurement_data_value\n(\n  id INTEGER NOT NULL,\n  fk_parameter_id INTEGER NOT NULL,\n  value VARCHAR(16) NOT NULL,\n  -- constraints...\n);\n</code></pre>\n\n<p>where <strong>the ID of a value row is equal to the ID of the index it belongs to</strong>.</p>\n\n<p>The sizes of a row in the index and value tables are</p>\n\n<pre>\nindex: 4 + 17 + 4 + 8 = 33 bytes\nvalue: 4 + 4 + 17     = 25 bytes\n</pre>\n\n<p>(again, worst case scenario). The total amount of rows is</p>\n\n<pre>\nindex: 10 devices x 10 spots x 1440 meas/day =   144 000 rows/day\nvalue: 10 parameters x 144 000 rows/day      = 1 440 000 rows/day\n</pre>\n\n<p>so the total is</p>\n\n<pre>\nindex: 33 bytes/row x   144 000 rows/day =  4 752 000 bytes/day\nvalue: 25 bytes/row x 1 440 000 rows/day = 36 000 000 bytes/day\ntotal:                                   = 40 752 000 bytes/day\n</pre>\n\n<p>or ~39 MB per day - as opposed to ~69 MB for a single table solution.</p>\n\n<hr>\n\n<p><strong>Update 02 (re: <a href=\"http://stackoverflow.com/a/12553459/217844\">wildplassers response</a>):</strong></p>\n\n<p>This question is getting pretty long as it is, so I was considering updating the code in place in the original question above, but I think it might help to have both the first and the improved solutions in here to better see the differences.</p>\n\n<p>Changes compared to the original approach (somewhat in order of importance):</p>\n\n<ul>\n<li>swap timestamp and parameter, i.e. move <code>t_stamp</code> field from <code>measurement_data_index</code> table to <code>measurement_data_value</code> and move <code>fk_parameter_id</code> field from value to index table: With this change, all fields in the index table are constant and new measurement data is written to the value table only. I did not expect any major query performance improvement from this (I was wrong), but I feel it makes the measurement data index concept clearer. While it requires fractionally more storage space (according to some rather crude estimate), having a 'static' index table might also help in deployment when <a href=\"http://www.postgresql.org/docs/current/interactive/manage-ag-tablespaces.html\" rel=\"nofollow\">tablespaces</a> are moved to different harddrives according to their read/write requirements.</li>\n<li>use a surrogate key in device table: From what I understand, a surrogate key is a primary key that is not strictly required from a database design point of view (e.g. device name is already unique, so it could also serve as PK), but might help to improve query performance. I added it because again, I feel it makes the concept clearer if the index table references IDs only (instead of some names and some IDs).</li>\n<li>rewrite <code>insert_data()</code>: Use <code>generate_series()</code> instead of nested <code>FOR</code> loops; makes the code much 'snappier'.</li>\n<li>As a side effect of these changes, inserting test data takes only about 50% of the time required by the first solution.</li>\n<li>I did not add the view as wildplasser suggested; there's no backward compatibility required.</li>\n<li>Additional indexes for the FKs in the index table seem to be ignored by the query planner and have no impact on query plan or performance.</li>\n</ul>\n\n<p>(it seems without this line, the code below is not properly displayed as code on the SO page...)</p>\n\n<pre><code>\\c postgres\nDROP DATABASE IF EXISTS so_test_03;\nCREATE DATABASE so_test_03;\n\\c so_test_03\n\nCREATE TABLE device\n(\n  id SERIAL,\n  name VARCHAR(16) NOT NULL,\n  CONSTRAINT device_pk PRIMARY KEY (id),\n  CONSTRAINT device_uk_name UNIQUE (name)\n);\n\nCREATE TABLE parameter\n(\n  id SERIAL,\n  name VARCHAR(64) NOT NULL,\n  CONSTRAINT parameter_pk PRIMARY KEY (id)\n);\n\nCREATE TABLE spot\n(\n  id SERIAL,\n  name VARCHAR(16) NOT NULL,\n  CONSTRAINT spot_pk PRIMARY KEY (id)\n);\n\nCREATE TABLE measurement_data_index\n(\n  id SERIAL,\n  fk_device_id    INTEGER NOT NULL,\n  fk_parameter_id INTEGER NOT NULL,\n  fk_spot_id      INTEGER NOT NULL,\n  CONSTRAINT measurement_pk PRIMARY KEY (id),\n  CONSTRAINT measurement_data_index_fk_2_device FOREIGN KEY (fk_device_id)\n    REFERENCES device (id) MATCH FULL\n    ON UPDATE NO ACTION ON DELETE NO ACTION,\n  CONSTRAINT measurement_data_index_fk_2_parameter FOREIGN KEY (fk_parameter_id)\n    REFERENCES parameter (id) MATCH FULL\n    ON UPDATE NO ACTION ON DELETE NO ACTION,\n  CONSTRAINT measurement_data_index_fk_2_spot FOREIGN KEY (fk_spot_id)\n    REFERENCES spot (id) MATCH FULL\n    ON UPDATE NO ACTION ON DELETE NO ACTION,\n  CONSTRAINT measurement_data_index_uk_all_cols UNIQUE (fk_device_id, fk_parameter_id, fk_spot_id)\n);\n\nCREATE TABLE measurement_data_value\n(\n  id INTEGER NOT NULL,\n  t_stamp TIMESTAMP NOT NULL,\n  value VARCHAR(16) NOT NULL,\n  -- NOTE: inverse field order compared to wildplassers version\n  CONSTRAINT measurement_data_value_pk PRIMARY KEY (id, t_stamp),\n  CONSTRAINT measurement_data_value_fk_2_index FOREIGN KEY (id)\n    REFERENCES measurement_data_index (id) MATCH FULL\n    ON UPDATE NO ACTION ON DELETE NO ACTION\n);\n\nCREATE OR REPLACE FUNCTION insert_data()\nRETURNS VOID\nLANGUAGE plpgsql\nAS\n$BODY$\n  BEGIN\n    INSERT INTO device (name)\n    SELECT 'dev_' || to_char(item, 'FM00')\n    FROM generate_series(1, 5) item;\n\n    INSERT INTO parameter (name)\n    SELECT 'param_' || to_char(item, 'FM00')\n    FROM generate_series(1, 20) item;\n\n    INSERT INTO spot (name)\n    SELECT 'spot_' || to_char(item, 'FM00')\n    FROM generate_series(1, 10) item;\n\n    INSERT INTO measurement_data_index (fk_device_id, fk_parameter_id, fk_spot_id)\n    SELECT device.id, parameter.id, spot.id\n    FROM device, parameter, spot;\n\n    INSERT INTO measurement_data_value(id, t_stamp, value)\n    SELECT index.id,\n           item,\n           'd'  || to_char(index.fk_device_id,    'FM00') ||\n           '_s' || to_char(index.fk_spot_id,      'FM00') ||\n           '_p' || to_char(index.fk_parameter_id, 'FM00')\n    FROM measurement_data_index index,\n         generate_series('2012-01-01 00:00:00', '2012-01-06 23:59:59', interval '1 min') item;\n  END;\n$BODY$;\n\nSELECT insert_data();\n</code></pre>\n\n<p>At some stage, I will change my own conventions to using inline <code>PRIMARY KEY</code> and <code>REFERENCES</code> statements instead of explicit <code>CONSTRAINT</code>s; for the moment, I think keeping this the way it was makes it easier to compare the two solutions.</p>\n\n<p>Don't forget to update statistics for the query planner:</p>\n\n<pre><code>VACUUM ANALYZE device;\nVACUUM ANALYZE measurement_data_index;\nVACUUM ANALYZE measurement_data_value;\nVACUUM ANALYZE parameter;\nVACUUM ANALYZE spot;\n</code></pre>\n\n<p>Run a query that should produce the same result as the one in the first approach:</p>\n\n<pre><code>EXPLAIN (ANALYZE ON, BUFFERS ON)\nSELECT measurement_data_value.value\n  FROM measurement_data_index,\n       measurement_data_value,\n       parameter\n WHERE measurement_data_index.fk_parameter_id = parameter.id\n   AND measurement_data_index.id = measurement_data_value.id\n   AND parameter.name = 'param_01';\n</code></pre>\n\n<p>Result:</p>\n\n<pre><code>Nested Loop  (cost=0.00..34218.28 rows=431998 width=12) (actual time=0.026..696.349 rows=432000 loops=1)\n  Buffers: shared hit=435332\n  -&gt;  Nested Loop  (cost=0.00..29.75 rows=50 width=4) (actual time=0.012..0.453 rows=50 loops=1)\n        Join Filter: (measurement_data_index.fk_parameter_id = parameter.id)\n        Buffers: shared hit=7\n        -&gt;  Seq Scan on parameter  (cost=0.00..1.25 rows=1 width=4) (actual time=0.005..0.010 rows=1 loops=1)\n              Filter: ((name)::text = 'param_01'::text)\n              Buffers: shared hit=1\n        -&gt;  Seq Scan on measurement_data_index  (cost=0.00..16.00 rows=1000 width=8) (actual time=0.003..0.187 rows=1000 loops=1)\n              Buffers: shared hit=6\n  -&gt;  Index Scan using measurement_data_value_pk on measurement_data_value  (cost=0.00..575.77 rows=8640 width=16) (actual time=0.013..12.157 rows=8640 loops=50)\n        Index Cond: (id = measurement_data_index.id)\n        Buffers: shared hit=435325\nTotal runtime: 726.125 ms\n</code></pre>\n\n<p>This is almost half of the ~1.3s the first approach required; considering I'm loading 432K rows, it is a result I can live with for the moment.</p>\n\n<p><strong>NOTE:</strong> The field order in the value table PK is <code>id, t_stamp</code>; the order in wildplassers response is <code>t_stamp, whw_id</code>. I did this that way because I feel a 'regular' field order is the one in which fields are listed in the table declaration (and 'reverse' is then the other way around), but that's just my own convention that keeps me from getting confused. Either way, as <a href=\"http://stackoverflow.com/a/12545427/217844\">Erwin Brandstetter</a> pointed out, this order is absolutely <strong>critical</strong> for the performance improvement; if it is the wrong way around (and a reverse index as in wildplassers solution is missing), the query plan looks like below and performance is more than 3 times worse:</p>\n\n<pre><code>Hash Join  (cost=22.14..186671.54 rows=431998 width=12) (actual time=0.460..2570.941 rows=432000 loops=1)\n  Hash Cond: (measurement_data_value.id = measurement_data_index.id)\n  Buffers: shared hit=63537\n  -&gt;  Seq Scan on measurement_data_value  (cost=0.00..149929.58 rows=8639958 width=16) (actual time=0.004..1095.606 rows=8640000 loops=1)\n        Buffers: shared hit=63530\n  -&gt;  Hash  (cost=21.51..21.51 rows=50 width=4) (actual time=0.446..0.446 rows=50 loops=1)\n        Buckets: 1024  Batches: 1  Memory Usage: 2kB\n        Buffers: shared hit=7\n        -&gt;  Hash Join  (cost=1.26..21.51 rows=50 width=4) (actual time=0.015..0.359 rows=50 loops=1)\n              Hash Cond: (measurement_data_index.fk_parameter_id = parameter.id)\n              Buffers: shared hit=7\n              -&gt;  Seq Scan on measurement_data_index  (cost=0.00..16.00 rows=1000 width=8) (actual time=0.002..0.135 rows=1000 loops=1)\n                    Buffers: shared hit=6\n              -&gt;  Hash  (cost=1.25..1.25 rows=1 width=4) (actual time=0.008..0.008 rows=1 loops=1)\n                    Buckets: 1024  Batches: 1  Memory Usage: 1kB\n                    Buffers: shared hit=1\n                    -&gt;  Seq Scan on parameter  (cost=0.00..1.25 rows=1 width=4) (actual time=0.004..0.007 rows=1 loops=1)\n                          Filter: ((name)::text = 'param_01'::text)\n                          Buffers: shared hit=1\nTotal runtime: 2605.277 ms\n</code></pre>\n"},{"tags":["c#","performance",".net-4.5","hft"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":2,"view_count":72,"score":-2,"question_id":12132623,"title":"java disruptor library c# analog","body":"<p>I'm writing HFT software.</p>\n\n<p><a href=\"http://code.google.com/p/disruptor/\" rel=\"nofollow\">Disruptor</a> claims to be a \"high performance inter-thread messaging library\", and apparently offers substantial performance improvements.</p>\n\n<p>Is there something with comparable speed for .NET?</p>\n"},{"tags":["mysql","performance","optimization"],"answer_count":9,"favorite_count":4,"up_vote_count":29,"down_vote_count":0,"view_count":11853,"score":29,"question_id":782915,"title":"MYSQL OR vs IN performance","body":"<p>I am wondering if there is any difference in regards to performance between the following</p>\n\n<pre><code>SELECT ... FROM ... WHERE someFIELD IN(1,2,3,4)\n\nSELECT ... FROM ... WHERE someFIELD between  0 AND 5\n\nSELECT ... FROM ... WHERE someFIELD = 1 OR someFIELD = 2 OR someFIELD = 3 ...\n</code></pre>\n\n<p>or will MySQL optimize the SQL in the same way compilers will optimize code ?</p>\n\n<p>EDIT: Changed the AND's to OR's for the reason stated in the comments. </p>\n"},{"tags":["javascript","jquery","performance","chaining"],"answer_count":5,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":52,"score":1,"question_id":12602856,"title":"jQuery dynamic DOM creation performance","body":"<p>When dynamically creating an HTML element in jQuery, is there any difference performance-wise between the following two methods?</p>\n\n<pre><code>// First approach\nvar elem = $('&lt;div/&gt;').attr('id', 'foo').addClass('myClass');\n\n// Second approach\nvar elem = $('&lt;div id=\"foo\" class=\"myClass\" /&gt;');\n</code></pre>\n\n<p>Also, are there any obvious advantages of one approach over the other, or is it just a matter of taste?</p>\n"},{"tags":["performance","lotus-notes","lotus-domino","database-replication"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":69,"score":0,"question_id":12586089,"title":"How can I provide a mini version of a very large application to my users","body":"<p>My users are really frustrated over the bad performance of a large Lotus Notes application we have (not web).  it is currently 10Gb and have approx 500.000 documents and it contains readers fields.</p>\n\n<p>What I would like to do is create a mini version of the application application as most users are only interested in the last years documents. </p>\n\n<p>The big application can't be archived or moved at this time so I'm thinking about providing a new mini replica and do a selective replication to only include this years documents. </p>\n\n<p>The problem I have is that the full version of the application need to be on all servers which means that I will get both the full version and mini version on the same server with the same replica id which seem a bit scary.</p>\n\n<p>Is there a better way to do this?</p>\n"},{"tags":["ruby-on-rails","performance","benchmarking","apachebench"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":57,"score":0,"question_id":12584998,"title":"Apache benchmark multipart/form-data","body":"<p>i'm facing a strange problem with apache benchmark post file. </p>\n\n<p>I need to stress a feature that handles file upload. So, I googled about, and found a post describing how to build a post file properly. Its contents looks like:</p>\n\n<pre><code>--1234567\nContent-Disposition: form-data; name=\"user_id\"\n\n3\n--1234567\nContent-Disposition: form-data; name=\"file\"; filename=\"cf_login.png\"\nContent-Type: image/png\n\n[base64 encoded file content]\n--1234567--\n</code></pre>\n\n<p>The ab line is this:</p>\n\n<pre><code>$ ab -c 1 -n 5 -v 4 -T 'multipart/form-data; boundary=1234567' -p post_file.txt http://localhost:3000/files\n</code></pre>\n\n<p>When ab make the requests the generated header is the following:</p>\n\n<pre><code>INFO: POST header == \n---\nPOST /files.json/ HTTP/1.0\nContent-length: 229\nContent-type: multipart/form-data; boundary=simpleboundary\nHost: localhost:3000\nUser-Agent: ApacheBench/2.3\nAccept: */*\n\n\n---\nLOG: header received:\nHTTP/1.1 500 Internal Server Error \nContent-Type: text/html; charset=utf-8\nContent-Length: 13265\nX-Request-Id: 9ad57d19cd71886a1bb817d00fac651b\nX-Runtime: 0.015504\nServer: WEBrick/1.3.1 (Ruby/1.9.3/2012-04-20)\nDate: Tue, 25 Sep 2012 13:54:29 GMT\nConnection: close\n</code></pre>\n\n<p>The expected return is a raise params.inspect, that let me see if the data is arriving to the other side. If I remove the boundary I can see data received in params, but this is not what I want. I want to get a file upload.</p>\n\n<p>Anyone has a tip? I will really appreciate it.</p>\n"},{"tags":["performance","optimization","traveling-salesman","mappoint","autoroute"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":74,"score":0,"question_id":11580571,"title":"Latest AutoRoute and MapPoint have a far slower \"optimise stops\" tool","body":"<p>I have AutoRoute 11.0, AutoRoute 2010, MapPoint 2010, MapPoint 2011, MapPoint 2013.</p>\n\n<p>I noticed that when I <strong>optimise stops</strong> the first takes a reasonable time, while the other 4 (which are newer...) are far slower.</p>\n\n<p>For instance I tried with the same set of 67 stops, spread in Toscana, Lazio and Campania (in Italia). The older AutoRoute takes half a minute, whistl the others take about 20 minutes. Unfortunately such a slow process is useless for our customers.</p>\n\n<p><strong>How can this be explained?</strong> A change in the alghorithms? Some setting to set up? Thanks!</p>\n"},{"tags":["performance","windows-7","vb6"],"answer_count":3,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":111,"score":2,"question_id":12588496,"title":"Slow VB6 String Operations on Windows 7","body":"<p>We have a VB6 program that does some string processing in a loop (approximately 500 times) and appends info to a textbox. Each iteration on the loop includes basic operations like Trim, Len, Left, Mid, etc, and finally appends the string to a textbox (the whole form is still invisible at this point). Finally, after the loop, the code calls Show on the form.</p>\n\n<p>On Windows XP, these 500 loops take about 4 seconds. On Windows 7, the exact same code runs in about 90 seconds.</p>\n\n<p>Any suggestions on how to fix this?</p>\n\n<p>Thanks.</p>\n"},{"tags":["performance","tomcat","struts2"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":140,"score":2,"question_id":12410951,"title":"Web application very slow in Tomcat 7","body":"<p>I implemented a web application to start the Tomcat service works very quickly, but spending hours and when more users are entering is getting slow (up to 15 users approx.).</p>\n\n<p>Checking RAM usage statistics (20%), CPU (25%)</p>\n\n<p>Server Features:</p>\n\n<ul>\n<li>RAM 8GB </li>\n<li>Processor i7</li>\n<li>Windows Server 2008 64bit </li>\n<li>Tomcat 7 </li>\n<li>MySql 5.0</li>\n<li>Struts2</li>\n<li>-Xms1024m </li>\n<li>-Xmx1024m </li>\n<li>PermGen = 1024</li>\n<li>MaxPernGen = 1024</li>\n</ul>\n\n<p>I do not use Web server, we publish directly on Tomcat.</p>\n\n<p>Entering midnight slowness is still maintained (only 1 user online)</p>\n\n<p>The solution I have is to restart the Tomcat service and response time is again excellent.</p>\n\n<p>Is there anyone who has experienced this issue? Any clue would be appreciated.</p>\n"},{"tags":["performance","yui","yui3"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":70,"score":0,"question_id":12500423,"title":"Difference between YUI 3.3 and YUI 3.7","body":"<p>I have a website that uses YUI 3.3.0 and YUI 2.9.0 (using YUI 2in3 project), right now I'm trying to improve performance in the initial load time of the site, I followed several tips that I found on the web, but I wonder if I have any improvement if I make an upgrade from YUI 3.3.0 to YUI 3.7.1</p>\n"},{"tags":["php","string","performance"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":3,"view_count":66,"score":-1,"question_id":12506621,"title":"Removing first slash character from string PHP","body":"<p>I have a string of this format <code>/abcd-efg-sms-3-topper-2</code></p>\n\n<p>I want to remove the first <code>/</code> character from this.</p>\n\n<p>I know I can remove this using <code>substr()</code> function, but is their any other way I can rmove this without using <code>substr()</code>.</p>\n\n<p>And what will be the faster way using <code>substr()</code> or other way?</p>\n\n<p>Any help will be appreciate.</p>\n\n<p>Cheers....</p>\n"},{"tags":["mysql","performance","query"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":54,"score":2,"question_id":12600301,"title":"MySQL: how to increase speed of a select query with 2 joins and 1 subquery","body":"<p>In a table 'ttraces' I have many records for different tasks (whose value is held in 'taskid' column and is a foreign key of a column 'id' in a table 'ttasks'). Each task inserts a record to 'ttraces' every 8-10 seconds, so caching data to increase performance is not a good idea. What I need is to select only the newest records for each task from 'ttraces', that means the records with the maximum value of the column 'time'. At the moment, I have over 500000 records in the table. The very simplified structure of these two tables looks as follows:</p>\n\n<pre><code>-----------------------\n|       ttasks        |\n-----------------------\n| id | name | blocked |\n-----------------------\n\n---------------------\n|      ttraces       |\n---------------------\n| id | taskid | time |\n---------------------\n</code></pre>\n\n<p>And my query is shown below:</p>\n\n<pre><code>SELECT t.name,tr.time\nFROM \n    ttraces tr \n    JOIN \n    ttasks t ON tr.itask = t.id \n    JOIN (\n        SELECT taskid, MAX(time) AS max_time\n        FROM ttraces \n        GROUP BY itask\n    ) x ON tr.taskid = x.taskid AND tr.time = x.max_time\nWHERE t.blocked\n</code></pre>\n\n<p>All columns used in WHERE and JOIN clauses are indexed. As for now the query runs for ~1,5 seconds. It's extremely crucial to increase its speed. Thanks for all suggestions. BTW: the database is running on a hosted, shared server and I can't move it anywhere else for the moment.</p>\n\n<p>[EDIT]\nEXPLAIN SELECT... results are:</p>\n\n<pre><code>--------------------------------------------------------------------------------------------------------------\nid   select_type   table        type     possible_keys   key       key_len   ref          rows     Extra\n--------------------------------------------------------------------------------------------------------------\n1   PRIMARY        &lt;derived2&gt;   ALL      NULL            NULL      NULL      NULL         74   \n1   PRIMARY        t            eq_ref   PRIMARY         PRIMARY   4         x.taskid     1        Using where\n1   PRIMARY        tr           ref      taskid,time     time      9         x.max_time   1        Using where\n2   DERIVED        ttraces      index    NULL            itask     5         NULL         570853   \n--------------------------------------------------------------------------------------------------------------\n</code></pre>\n\n<p>The engine is InnoDB. Thenk you.</p>\n"},{"tags":["database","performance","algorithm","data-structures"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":56,"score":2,"question_id":12599341,"title":"Finding a suitable data structure for deletion from both lists","body":"<p>This might be deleted, since involves idea sharing which is not quite allowed in stack overflow, but still before that if I could get any ideas from solid programmers, it will be a win situation for me</p>\n\n<p>Assume that you have a class <b>Student</b>, stored in the database, and this class has a list property called <b>favoriteTeachers</b>. This list constantly gets updated by the system and involves the id of teachers.</p>\n\n<p>You also have a class <b>Teacher</b>, also stored in database and likewise has a list property <b>favouriteStudents</b>. It is again updated constantly and involves the id's of students.</p>\n\n<p>In our system, when a student calls a function (let's say <b>notMyFavoriteTeacher</b>), our system has to apply the changes below;</p>\n\n<ol>\n<li>Delete the given teacher's id from favouriteTeacher list</li>\n<li>Delete the student's id from given teacher's favouriteStudent list</li>\n</ol>\n\n<p>I've tried to consider the number of rows updated could exhaust the database so instead of mapping the students with their favorite teachers in a separate table as <b>user_id, teacher_id</b>, instead I created a column and stored a string which contains the teachers id's separated by comma. (Ex: \"1,2,14,4,25\"). Same applied for the <b>teacher</b> as well. </p>\n\n<p>However when we call this function, we also face another problem. In order for this operation to be done, you need to convert the string to list, find the element by linear search and later on delete, and later on convert list to string and push back to db. And you have to do the other operation for the <b>teacher</b> class as well. If we <b>did not</b> apply the string method, deletion would be easier but since we would be handling deletion and addition operations for like 2k times a day, i did not think it would be <b>feasible</b> to use separate tables.</p>\n\n<p>I wanted to ask in order to decrease the number of operations, could a data structure be chosen such that it would increase the <b>efficiency?</b></p>\n"},{"tags":["database","performance","hibernate","postgresql"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":60,"score":1,"question_id":12319676,"title":"WARN (org.hibernate.jdbc.JDBCContext:333) - afterTransactionCompletion() was never called","body":"<p>I am using hibernate as ORM when i checked my logs (catalina.out)</p>\n\n<p>I am getting this warning so many times </p>\n\n<pre><code>WARN   (org.hibernate.jdbc.JDBCContext:333) - afterTransactionCompletion() was never called\n</code></pre>\n\n<p>Can any one please help me to solve this warning...?</p>\n\n<p>what is the meaning of that warning and when it occurs ??</p>\n"},{"tags":["c++","python","perl","performance","lua"],"answer_count":7,"favorite_count":3,"up_vote_count":6,"down_vote_count":0,"view_count":988,"score":6,"question_id":2103728,"title":"Selecting An Embedded Language","body":"<p>I'm making an application that analyses one or more series of data using several different algorithms (agents). I came to the idea that each of these agents could be implemented as separate Python scripts which I run using either the Python C API or Boost.Python in my app.</p>\n\n<p>I'm a little worried about runtime overhead TBH, as I'm doing some pretty heavy duty data processing and I don't want to have to wait several minutes for each simulation. I will typically be making hundreds of thousands, if not millions, of iterations in which I invoke the external \"agents\"; am I better of just hardcoding everything in the app, or will the performance drop be tolerable? </p>\n\n<p>Also, are there any other interpreted languages I can use other than Python?</p>\n"},{"tags":["performance","pagespeed"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":2,"view_count":39,"score":-2,"question_id":12598262,"title":"What should I do to make my website as fast as it possible","body":"<p>My clients said the website is very slow. I've tried to make it as fast as possible by try Google PageSpeed and Yahoo YSlow!. But they still said, it's slow.</p>\n\n<p>So I need some comment to make my website as fast as it can.\nAnd below is the website url.</p>\n\n<p><a href=\"http://www.weltzresidences.com/\" rel=\"nofollow\">http://www.weltzresidences.com/</a></p>\n"},{"tags":["html","performance","dom","javascript-events"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":33,"score":0,"question_id":12598481,"title":"HTML DOM events","body":"<p>I have a browser tree as below. Each node is a div element with individual id. I want to change the style when the mouse hovers over the node. And do something when the user left-clicks or right-clicks the node. \n<img src=\"http://i.stack.imgur.com/nOlL5.png\" alt=\"enter image description here\"></p>\n\n<p>Question1: Some documents say there are performance issues with the :hover selector. So I listen for the mouseover and mouseout events instead. Is this manner recommended?</p>\n\n<p>To implement my requirements, I want to listen for the mouseover, mouseout, click and contextmenu events of each node. I have two solutions. The first is bind the event handler to each node. The second is only bind the event handler to the root node. When the event is bubbled to the root node, the handler is called. </p>\n\n<p>Question 2: Which performance is better about the two solutions?</p>\n"},{"tags":["html","performance","seo"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":46,"score":0,"question_id":12504513,"title":"Language Switcher, Performance Optimization and SEO","body":"<p>I'm developing a multilingual website. In the top of the site there is a language switcher.</p>\n\n<p><img src=\"http://imageshack.us/a/img443/5456/languageswitcher.png\" alt=\"Language Switcher\"></p>\n\n<p>For performance reasons, the flags are wrapped into a single atlas (<img src=\"http://imageshack.us/a/img834/8260/flagsy.png\" alt=\"Atlas\">)and the images are displayed as background of the <code>&lt;a&gt;</code> elements.</p>\n\n<pre><code>&lt;ul class=\"languages\"&gt;\n    &lt;li&gt;&lt;a href=\".../en/\" class=\"lang-en\"&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\na.lang-en { background: url('path-to-atlas.png') -17px 0; }\n</code></pre>\n\n<p>There is no text in the anchor (<code>&lt;a&gt;&lt;/a&gt;</code>).</p>\n\n<p>But what about SEO?</p>\n\n<p>What does a crawler do with this link? How can I tell to the crawler that the link's text is something like <code>'English (UK)'</code> or <code>'Italiano'</code>?</p>\n\n<p>I could do something like:</p>\n\n<pre><code>&lt;a href=\".../en/\" class=\"lang\"&gt;English (UK)&lt;/a&gt;\n\na.lang {\n    line-height: 0; \n    font-size: 0;\n    color: transparent; \n}\n</code></pre>\n\n<p>But this is 'hiding the text' and Google hates it.</p>\n\n<p>What are your suggestions?</p>\n"},{"tags":["java","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":411,"score":2,"question_id":4114796,"title":"Java: Would calling clear() on a List free up memory immediatly?","body":"<p>Which would clear up memory quicker:</p>\n\n<p>Given we have a linked list, in this case ArrayList, but feel free to explain this for other lists:</p>\n\n<pre><code>ArrayList&lt;String&gt; list = ...10000 elements\n</code></pre>\n\n<p><strong>Either A)</strong></p>\n\n<pre><code>list.clear();\nSystem.gc();\n</code></pre>\n\n<p><strong>Or B)</strong></p>\n\n<pre><code>list = null;\nSystem.gc();\n</code></pre>\n"},{"tags":["mysql","performance","amazon-web-services","profiling","amazon-rds"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":56,"score":0,"question_id":12585603,"title":"mysql performance benchmark","body":"<p>I'm thinking about moving our production env from a self hosted solution to amazon aws. I took a look at the different services and thought about using <a href=\"http://aws.amazon.com/rds/\" rel=\"nofollow\">RDS</a> as replacement for our mysql instances. The hardware we're using for our master seems to be better than the best hardware we can get when using rds (Quadruple Extra Large DB Instance). Since I can't simply move our production env to aws and see if the performance is still good enough I'd love to make some tests in advance. </p>\n\n<p>I thought about creating a full query log from our current master, configure the rds instance and start to replay the full query log against it. Actually I don't even know if this kind of testing is a good idea but I guess you'll tell me if there are better ways to make sure the performance of mysql won't drop dramatically when making the move to rds.</p>\n\n<ol>\n<li>Is there a preferred tool to replay the full query log?</li>\n<li>at what metrics should I take a look while running the test\n<ul>\n<li>cpu usage?</li>\n<li>memory usage?</li>\n<li>disk usage?</li>\n<li>query time?</li>\n<li>anything else?</li>\n</ul></li>\n</ol>\n\n<p>Thanks in advance</p>\n"},{"tags":["performance","jboss","startup","esb","mule"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":52,"score":0,"question_id":12597645,"title":"Multiple flow startup very slow after upgrading from 3.1.2 to 3.3.0","body":"<p>We recently upgraded from Mule 3.1.2 to 3.3.0. \nWe are using Mule deployed inside a war in JBoss 5.1. \nOn our backoffice application we have a large number of flows using a quartz inbound endpoint then running various processors. like this one : </p>\n\n<pre><code>&lt;flow name=\"ImportXXX\"&gt;\n    &lt;quartz:inbound-endpoint repeatInterval=\"5000\" startDelay=\"10000\" jobName=\"QuartzImportXXX\" &gt;\n        &lt;quartz:event-generator-job&gt;\n            &lt;quartz:payload&gt;${XXX.depot} | ${archive}&lt;/quartz:payload&gt;\n        &lt;/quartz:event-generator-job&gt;\n    &lt;/quartz:inbound-endpoint&gt;\n    &lt;component&gt;\n       ...\n    &lt;/component&gt;\n    &lt;component&gt;\n    ...\n    &lt;/component&gt;\n&lt;/flow&gt;\n</code></pre>\n\n<p>We have about 40 flows similar to this one (processor does different things depending on the quartz payload) </p>\n\n<p>With mule 3.3.0, each of these flow take about 5s to start, with the exact same configuration on 3.1.2 flows were starting instantly (less than 1/10s per flow). We did not change anything to the underlying processors implementation neither, this was a pure migration. \nI tried to remove all our specific processors, and replace them with a single one which does nothing but log one line. Now the startup of each flow is \"reduced\" to 1.5s, but still a lot slower than with 3.1.2. \nOverall the problem grows with the number of flows and component loaded (If I comment half of the flows, the single flow startup time is improved by 1 or 2s) </p>\n\n<p>This seems to be specific when deployed in a war in JBoss, I tried the exact same mule-config.xml in a standalone Mule app created from a basic maven archetype and all flows starts smoothly and quickly as in 3.1.2. </p>\n\n<p>Is there any change in the way flows are initialized in Mule 3.3 which could explain this ? \nAny idea about what I could check or try ? </p>\n\n<p>Thanks for your help, </p>\n\n<p>Laurent</p>\n"},{"tags":["java","arrays","performance"],"answer_count":5,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":113,"score":1,"question_id":12596866,"title":"What is faster and less in memory in Java: int[] or boolean[]?","body":"<p>In Java, what is faster and less in memory: <code>int[n]</code> or <code>boolean[n]</code> or maybe <code>Bitset(n)</code> ?</p>\n\n<p>The question is applicable for arrays of small (<code>n</code> is up to <code>1000</code>), medium (<code>n</code> is between <code>1000</code> and <code>100000</code>) and huge (<code>n</code> is greater than <code>100000</code>) sizes. Thank you.</p>\n\n<p>I want to achieve flags (1/0) storage.</p>\n"},{"tags":["javascript","ajax","performance","client-side","server-side"],"answer_count":2,"favorite_count":1,"up_vote_count":4,"down_vote_count":0,"view_count":107,"score":4,"question_id":12596347,"title":"Server side processing or client side processing?","body":"<p>I have developed a comparison website for comparing any product sold online in India. Currently, the site is completely client side :-</p>\n\n<blockquote>\n  <ol>\n  <li>Accepts user input.</li>\n  <li>Makes 20-30 AJAX requests and fetches results from all the major online shops.</li>\n  <li>Uses some client-side scripting to sort the results and show it in most appropriate way.</li>\n  </ol>\n</blockquote>\n\n<p>Disadvantages :-</p>\n\n<blockquote>\n  <ol>\n  <li>My client side code is available to everyone. Its javascript.</li>\n  <li>More prone to browser errors.</li>\n  <li>Not robust.</li>\n  </ol>\n</blockquote>\n\n<p>Disadvantages after making it server-side:-</p>\n\n<blockquote>\n  <ol>\n  <li>Considering the traffic of my website, server load will increase as it will be engaged with a client for longer time.</li>\n  <li>Fetching values from various websites can take as much as 10s(at max).Server engaged for that time. Consider the load if I have 500\n  visitors/min at peak time.</li>\n  </ol>\n</blockquote>\n\n<p>Advantages:-</p>\n\n<blockquote>\n  <ol>\n  <li>My codes safe and secure </li>\n  <li>Processing at client side will be minimum. Will work even on mobiles and other devices easily.</li>\n  </ol>\n</blockquote>\n\n<p>I want to analyze about these issues before actually implementing them. Can anyone suggest me about what should I choose for my website ? Which approach will be better for me ?</p>\n\n<p>Please comment if my question is ambiguous.</p>\n"},{"tags":["java","android","performance","testing"],"answer_count":2,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":84,"score":5,"question_id":12567077,"title":"is SharedPreferences access time consuming?","body":"<p>I'm currently trying to test a third party service for my app, and need to identify each test that is being done at every specific run.</p>\n\n<p>Since more than one test can take place every time I run the testApp, I need to Identify every test.</p>\n\n<p>What I thought of, is storing the device name and build (not many devices here), and an index for each test.</p>\n\n<pre><code>private String getTestId(){\n    SharedPreferences settings = getPreferences(0);\n    SharedPreferences.Editor editor = settings.edit();\n    int testNumber = settings.getInt(\"id\", 0);\n    editor.putInt(\"id\", testNumber+1);\n    editor.commit();\n    String id = Build.DEVICE + Build.VERSION.RELEASE+\" - test number: \"+testNumber;\n    return id;\n}\n</code></pre>\n\n<p>Is running this function every time I run a test time consuming, or can I do this without fearing the coast?</p>\n\n<p>if the answer is \"time consuming\", what would you suggest I do every time I run a test in order to differentiate every test?</p>\n"},{"tags":["performance","jsf","java-ee","ejb-3.0","stateless-session-beans"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":717,"score":1,"question_id":3692033,"title":"JSF + Stateless EJB performance","body":"<p>I have a JSF 2.0 application running on GlassFish v3. It has EJBs that serve database data via JPA for the main applications sessions. Even on non-IDE app-server, the EJB calls are very slow. Between some pages, the user has to wait over 10 seconds to get to the next page.</p>\n\n<p>The EJB runs on the same application server, and only <code>Local</code> interface is used. The EJB is injected via <code>@EJB</code> annotation.</p>\n\n<p>Any clues?</p>\n\n<p>Thanks in advance,\nDaniel</p>\n\n<p><strong>EDIT</strong> See my answer for solution.</p>\n"},{"tags":["mysql","performance","query","tags","aggregate"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":85,"score":2,"question_id":12595964,"title":"How to speed up MySQL query: order by count","body":"<p>I have tried everything I could think of to speed up this query, but it still takes about 2.5 seconds.</p>\n\n<p>The table is images_tags (~4 Million Rows):\nHere is the table EXPLAIN:</p>\n\n<pre><code>Field       Type               Null     Key     Default\nimage_ids   int(7) unsigned    NO       PRI     NULL\ntags_id     int(7) unsigned    NO       PRI     NULL\n</code></pre>\n\n<p>Here are the Indexes:</p>\n\n<pre><code>Table         Non_unique  Key_name      Seq_in_index  Column_name  Collation  Cardinality  Sub_part  Packed  Null  Index_type\nimages_tags   0           PRIMARY       1             image_ids    A          NULL         NULL      NULL          BTREE\nimages_tags   0           PRIMARY       2             tags_id      A          4408605      NULL      NULL          BTREE\nimages_tags   1           image_ids     1             image_ids    A          734767       NULL      NULL          BTREE\n</code></pre>\n\n<p>And here is the query:</p>\n\n<pre><code>select image_ids\nfrom images_tags\nwhere tags_id in (1, 2, 21, 846, 3175, 4290, 6591, 9357, 9594, 14289, 43364, 135019, 151295, 208803, 704452)\ngroup by image_ids\norder by count(*) desc\nlimit 10\n</code></pre>\n\n<p>And here is the query EXPLAIN:</p>\n\n<pre><code>select_type  table        type   possible_keys  key                 key_len  ref   rows     Extra\nSIMPLE       vids_x_tags  index  join_tags_id   join_vids_id_unique  8       NULL  4408605  Using where; Using index; Using temporary; Using filesort\n</code></pre>\n\n<p>The goal is to get the 10 images that match those tags the most.\nI have tried messing around with these variables with little to no improvement:</p>\n\n<ul>\n<li>max_heap_table_size</li>\n<li>tmp_table_size</li>\n<li>myisam_sort_buffer_size</li>\n<li>read_buffer_size</li>\n<li>sort_buffer_size</li>\n<li>read_rnd_buffer_size</li>\n<li>net_buffer_length</li>\n<li>preload_buffer_size</li>\n<li>key_buffer_size</li>\n</ul>\n\n<p>Is there any way to speed up this query considerably? There are about 700K images and it's always growing, so I wouldn't want to cache the result for more than a day or 2, and it has to be done for each image, so re-caching that many queries would be impossible.</p>\n"},{"tags":["mysql","performance","optimization","select","query-optimization"],"answer_count":2,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":94,"score":2,"question_id":12583751,"title":"MYSQL query optimization, multiple queries or one large query","body":"<p>I have a query which has some sub queries  (inner selects), i'm trying work out which is better for performance, one larger query or lots of smaller queries, i'm finding it difficult to try and time the differences as it changes all the time on my server.</p>\n\n<p>I use the query below to return 10 results at a time to display on my website, using pagination (offset and limit).</p>\n\n<pre><code>SELECT adverts.*, breed.breed, breed.type, sellers.profile_name, sellers.logo, users.user_level , \nround( sqrt( ( ( (adverts.latitude - '51.558430') * (adverts.latitude - '51.558430') ) * 69.1 * 69.1 ) + ( (adverts.longitude - '-0.0069345') * (adverts.longitude - '-0.0069345') * 53 * 53 ) ), 1 ) as distance, \n( SELECT advert_images.image_name FROM advert_images WHERE advert_images.advert_id = adverts.advert_id AND advert_images.main = 1 LIMIT 1) as imagename, \n( SELECT count(advert_images.advert_id) from advert_images WHERE advert_images.advert_id = adverts.advert_id ) AS num_photos \nFROM adverts \nLEFT JOIN breed ON adverts.breed_id = breed.breed_id \nLEFT JOIN sellers ON (adverts.user_id = sellers.user_id) \nLEFT JOIN users ON (adverts.user_id = users.user_id) \nWHERE (adverts.status = 1) AND (adverts.approved = 1) \nAND (adverts.latitude BETWEEN 51.2692837281 AND 51.8475762719) AND (adverts.longitude BETWEEN -0.472015213613 AND 0.458146213613) \nhaving (distance &lt;= '20') \nORDER BY distance ASC \nLIMIT 0,10\n</code></pre>\n\n<p>Would it be better to remove the 2 inner selects below from the main query, and then in my php loop, call the 2 selects 10 times, once for each record in the loop?</p>\n\n<pre><code>( SELECT advert_images.image_name FROM advert_images WHERE advert_images.advert_id = adverts.advert_id AND advert_images.main = 1 LIMIT 1) as imagename, \n( SELECT count(advert_images.advert_id) from advert_images WHERE advert_images.advert_id = adverts.advert_id ) AS num_photos \n</code></pre>\n"},{"tags":["iphone","objective-c","performance","audio","avaudioplayer"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":21,"score":0,"question_id":12595653,"title":"AVAudioPlayer sounds affecting performance of app","body":"<p>I have recently been adding sounds into a game I'm developing using <code>AVAudioPlayer</code>. I have noticed that playing sounds will often slow the game down in terms of framerate. I have tried to reduce the performance cost by using the method <code>[sound prepareToPlay]</code>, however this seems to make no difference. Does anyone know a way to play sounds without a significant performance cost? Any help would be greatly appreciated.</p>\n"},{"tags":["java","performance","cli","jconsole"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":39,"score":0,"question_id":12595277,"title":"Is there CLI version of jconsole?","body":"<p>I am looking for jvm performance tools which have features: </p>\n\n<ol>\n<li>Running in background.</li>\n<li>Collect memory and thread information.</li>\n<li>Generate performance data in readable text format, which is easily consumed by cmd like grep, awk, sort and so on.</li>\n</ol>\n\n<p>In short words, it's cli version of jconsole.</p>\n\n<p>jstat included in oracle jvm meets 1 and 3, but doesn't collect thread information. I think there should be some more sophisticated tools exist.</p>\n\n<p>Any suggestion? Thanks in advance!</p>\n"},{"tags":["jquery","performance","events"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":54,"score":2,"question_id":12592781,"title":"Is it ok to declare same window events multiple times in jQuery?","body":"<p>I have a few elements listening to window resize and scroll events. In order to make the code clear for reading, I declared same window events multiple times in different blocks, like the following...</p>\n\n<pre><code>$('#foo')...\n$(window).resize(function() {\n    $('#foo')...\n});\n\n// lots of code...\n\n$('#bar')...\n$(window).resize(function() {\n    $('#bar')...\n});\n\n// and so on..\n</code></pre>\n\n<p>where I believe generally it should be written as</p>\n\n<pre><code>$(window).resize(function() {\n    $('#foo')...\n    $('#bar')...\n});\n\n// lots of code\n\n$('#foo')...\n\n// lots of code\n\n$('bar')...\n</code></pre>\n\n<p>My question is, will it make any difference to jQuery performance if I declare window events multiple times?</p>\n"},{"tags":["c","performance","matlab","optimization","mex"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":1,"view_count":139,"score":0,"question_id":12272218,"title":"How to speed up this mex code?","body":"<p>I am reprogramming a piece of MATLAB code in mex (using C). So far my C version of the MATLAB code is about as double as fast as the MATLAB code. Now I have three questions, all related to the code below:</p>\n\n<ol>\n<li>How can I speed up this code more?</li>\n<li>Do you see any problems with this code? I ask this because I don't know mex very well and I am also not a C guru ;-) ... I am aware that there should be some checks in the code (for example if there is still heap space while using <code>realloc</code>, but I left this away for the sake of simplicity for the moment)</li>\n<li>Is it possible, that MATLAB is optimizing so well, that I really can't get much more than twice as fast code in C...?</li>\n</ol>\n\n<p>The code should be more or less platform independent (Win, Linux, Unix, Mac, different Hardware), so I don't want to use assembler or specific linear Algebra Libraries. So that's why I programmed the staff by myself...</p>\n\n<pre class=\"lang-c prettyprint-override\"><code>#include &lt;mex.h&gt;\n#include &lt;math.h&gt;\n#include &lt;matrix.h&gt;\n\nvoid mexFunction(\n    int nlhs, mxArray *plhs[],\n    int nrhs, const mxArray *prhs[])\n{\n    double epsilon = ((double)(mxGetScalar(prhs[0])));\n    int strengthDim = ((int)(mxGetScalar(prhs[1])));\n    int lenPartMat = ((int)(mxGetScalar(prhs[2])));\n    int numParts = ((int)(mxGetScalar(prhs[3])));\n    double *partMat = mxGetPr(prhs[4]);\n    const mxArray* verletListCells = prhs[5];\n    mxArray *verletList;\n\n    double *pseSum = (double *) malloc(numParts * sizeof(double));\n    for(int i = 0; i &lt; numParts; i++) pseSum[i] = 0.0;\n\n    float *tempVar = NULL;\n\n    for(int i = 0; i &lt; numParts; i++)\n    {\n        verletList = mxGetCell(verletListCells,i);\n        int numberVerlet = mxGetM(verletList);\n\n        tempVar = (float *) realloc(tempVar, numberVerlet * sizeof(float) * 2);\n\n\n        for(int a = 0; a &lt; numberVerlet; a++)\n        {\n            tempVar[a*2] = partMat[((int) (*(mxGetPr(verletList) + a))) - 1] - partMat[i];\n            tempVar[a*2 + 1] = partMat[((int) (*(mxGetPr(verletList) + a))) - 1 + lenPartMat] - partMat[i + lenPartMat];\n\n            tempVar[a*2] = pow(tempVar[a*2],2);\n            tempVar[a*2 + 1] = pow(tempVar[a*2 + 1],2);\n\n            tempVar[a*2] = tempVar[a*2] + tempVar[a*2 + 1];\n            tempVar[a*2] = sqrt(tempVar[a*2]);\n\n            tempVar[a*2] = 4.0/(pow(epsilon,2) * M_PI) * exp(-(pow((tempVar[a*2]/epsilon),2)));\n            pseSum[i] = pseSum[i] + ((partMat[((int) (*(mxGetPr(verletList) + a))) - 1 + 2*lenPartMat] - partMat[i + (2 * lenPartMat)]) * tempVar[a*2]);\n        }\n\n    }\n\n    plhs[0] = mxCreateDoubleMatrix(numParts,1,mxREAL);\n    for(int a = 0; a &lt; numParts; a++)\n    {\n        *(mxGetPr(plhs[0]) + a) = pseSum[a];\n    }\n\n    free(tempVar);\n    free(pseSum);\n}\n</code></pre>\n\n<p>So this is the improved version, which is about 12 times faster than MATLAB version. The conversion thing is still eating up much time, but I let this away for now, becaues I have to change something in MATLAB for this. So first focus on the remaining C code. Do you see any more potential in the following code?</p>\n\n<pre class=\"lang-c prettyprint-override\"><code>#include &lt;mex.h&gt;\n#include &lt;math.h&gt;\n#include &lt;matrix.h&gt;\n\nvoid mexFunction(\n    int nlhs, mxArray *plhs[],\n    int nrhs, const mxArray *prhs[])\n{\n    double epsilon = ((double)(mxGetScalar(prhs[0])));\n    int strengthDim = ((int)(mxGetScalar(prhs[1])));\n    int lenPartMat = ((int)(mxGetScalar(prhs[2])));\n    double *partMat = mxGetPr(prhs[3]);\n    const mxArray* verletListCells = prhs[4];\n    int numParts = mxGetM(verletListCells);\n    mxArray *verletList;\n\n    plhs[0] = mxCreateDoubleMatrix(numParts,1,mxREAL);\n    double *pseSum = mxGetPr(plhs[0]);\n\n    double epsilonSquared = epsilon*epsilon;\n\n    double preConst = 4.0/((epsilonSquared) * M_PI);\n\n    int numberVerlet = 0;\n\n    double tempVar[2];\n\n    for(int i = 0; i &lt; numParts; i++)\n    {\n        verletList = mxGetCell(verletListCells,i);\n        double *verletListPtr = mxGetPr(verletList);\n        numberVerlet = mxGetM(verletList);\n\n        for(int a = 0; a &lt; numberVerlet; a++)\n        {\n            int adress = ((int) (*(verletListPtr + a))) - 1;\n\n            tempVar[0] = partMat[adress] - partMat[i];\n            tempVar[1] = partMat[adress + lenPartMat] - partMat[i + lenPartMat];\n\n            tempVar[0] = tempVar[0]*tempVar[0] + tempVar[1]*tempVar[1];\n\n            tempVar[0] = preConst * exp(-(tempVar[0]/epsilonSquared));\n            pseSum[i] += ((partMat[adress + 2*lenPartMat] - partMat[i + (2*lenPartMat)]* tempVar[0]);\n        }\n\n    }\n\n}\n</code></pre>\n"},{"tags":["ruby-on-rails","performance","query","postgresql"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":132,"score":1,"question_id":10031393,"title":"Rails Optimizing Complicated Query","body":"<p>Here's the background, I have an API method thats sending a normalized list of names to my server which then matches up the names to given Persons objects. Each person has many Post objects related to them that I want to return. My database query looks like this:</p>\n\n<pre><code> @articles = Article.joins(:artists).select(\"artists.*\").where({:artists=&gt;{:name=&gt;keys}}).group(\"#{Article.col_list}\").order(\"articles.publish_date DESC\").limit(25)\n</code></pre>\n\n<p>This query can take as long as 2 seconds on my production server when the list of names is large (over 100). How can I optimize this query so its fast?</p>\n\n<p>Here is the EXPLAIN for the query.</p>\n\n<pre><code> Limit  (cost=9144.19..9144.20 rows=25 width=802) (actual time=4341.409..4341.431 rows=25 loops=1)\n   -&gt;  Sort  (cost=9144.19..9150.74 rows=13096 width=802) (actual time=4341.408..4341.416 rows=25 loops=1)\n         Sort Key: articles.publish_date\n         Sort Method:  top-N heapsort  Memory: 47kB\n         -&gt;  HashAggregate  (cost=9030.99..9070.28 rows=13096 width=802) (actual time=4255.854..4315.125 rows=3916 loops=1)\n               -&gt;  Nested Loop  (cost=1581.27..8978.60 rows=13096 width=802) (actual time=7.443..3337.624 rows=39920 loops=1)\n                     -&gt;  Hash Join  (cost=1581.27..7650.04 rows=13096 width=4) (actual time=7.426..2627.426 rows=39920 loops=1)\n                           Hash Cond: (articles_artists.artist_id = artists.id)\n                           -&gt;  Seq Scan on articles_artists  (cost=0.00..5476.38 rows=737461 width=8) (actual time=0.019..1310.806 rows=737515 loops=1)\n                           -&gt;  Hash  (cost=1579.83..1579.83 rows=413 width=4) (actual time=7.383..7.383 rows=300 loops=1)\n                                 Buckets: 1024  Batches: 1  Memory Usage: 8kB\n                                 -&gt;  Bitmap Heap Scan on artists  (cost=477.60..1579.83 rows=413 width=4) (actual time=4.959..7.152 rows=300 loops=1)\n                                       Recheck Cond: ((name)::text = ANY ('{\"A BOY NAMED EARTH\",\"A ROCKET TO MOON\",\"A SKYLIT DRIVE\",\"A VISIBLE BOY\",\"AARON GILLESPIE\",AB,\"ABBIE BARRETT\",ABERFELDY,\"ABRA NIIRE\",\"ABSOLUTE ZERO\",ACDC,\"ADAM GREEN\",ADELE,ADO,\"ADRINA THORPE\",ADVENTURE,AEROSMITH,\"AESOP ROCK\",AFI,AIR,\"AIR TRAFFIC\",AKON,\"AL GREEN\",\"ALANIS MORISSETTE\",ALCENDOR,\"ALEX CHILTON\",\"ALEXI MURDOCH\",\"ALIAS EHREN\",\"ALICE RUSSELL\",\"ALICIA KEYS\",\"AMANDA PALMER\",\"AMERICAN FOOTBALL\",\"AMY MACDONALD\",\"AMY WINEHOUSE\",\"ANALOG REBELLION\",ANBERLIN,\"ANDREW BIRD\",\"ANDREW HILL\",\"ANGELS AIRWAVES\",AQUEDUCT,\"ARCADE FIRE\",\"ARETHA FRANKLIN\",\"ART BRUT\",\"ART GARFUNKEL\",\"ARTIC MONKEYS\",\"ARTIC MONKEYS\",\"AS TALL AS LIONS\",ASHANTI,\"ATLAS SOUND\",\"ATTACK ATTACK!\",AUTOLUX,\"AVENUE D\",\"AVRIL LAVIGNE\",\"B JU\",BACK2SQUARE1,\"BAD ENGLISH\",BALMORHEA,\"BAND OF HORSES\",BARCELONA,\"BAXTER DURY\",\"BEASTIE BOYS\",BEATLES,BECK,\"BELLE AND SEBASTIAN\",\"BEN FOLDS FIVE\",\"BEN MARTIN\",\"BETTE MIDLER\",\"BETTIE SERVEERT\",BEYONCE,\"BIFFY CLYRO\",\"BILLY BRAGG\",\"BILLY BRAGG AND BLOKES\",\"BILLY JOEL\",\"BILLY OCEAN\",BIRDY,\"BLACK EYED PEAS\",\"BLACK KIDS\",BLACKSTONE,\"BLIND PILOT\",\"BLINK 182\",\"BLOC PARTY\",\"BLOODHOUND GANG\",BLUEBOY,BLUR,\"BOB DYLAN\",\"BOB MARLEY\",\"BOB MARLEY WAILERS\",\"BOMBAY BICYCLE CLUB\",\"BON IVER\",\"BONNIE TYLER\",BONOBO,\"BOX CAR RACER\",\"BOYS LIKE GIRLS\",\"BOYZ II MEN\",BREAKBOT,\"BRENDAN BENSON\",\"BRETT ANDERSON\",BRICOLAGE,\"BRITNEY SPEARS\",BRONCO,\"BRUCE SPRINGSTEEN\",\"BRUNO MARS\",BUSH,\"BUSTA RHYMES\",BUSTED,CAESARS,\"CAGE ELEPHANT\",CAKE,CALLA,\"CALVIN HARRIS\",CAMRON,CAMRON,CAMP,CAMPING,\"CANDIE PAYNE\",CASHEW,\"CASSANDRA WILSOM\",CASSIDY,\"CATE LE BON\",\"CHARLIE FEATHERS\",\"CHERRY GHOST\",CHIC,CHICAGO,CHINGY,\"CHRIS BROWN\",\"CHRISTINA AGUILERA\",\"CHRISTINA STURMER\",CHUMBAWAMBA,CIRCLE,\"CLARENCE CARTER\",CLIPSE,\"COBRA STARSHIP\",COLDPLAY,COMMON,CONSPIRATORS,COOLIO,COPELAND,\"COPY HAHO\",CORNELIUS,CREED,CULTS,\"CYNDI LAUPER\",\"CYPRESS HILL\",\"CAT EMPIRE\",CURE,\"DA LATA\",\"DAFT PUNK\",DALMINJO,DAMERO,\"DARREN HANLON\",\"DASHBOARD CONFESSIONAL\",DATAROCK,\"DAVID BOWIE\",\"DAVID E SUGAR\",\"DAVID GUETTA\",\"DAVID KITT\",\"DEATH CAB FOR CUTIE\",\"DIANA KRALL\",DIGITALISM,\"DINOSAUR JR\",\"DISCO INFERNO\",DISCOVERY,\"DIZZEE RASCAL\",DMX,\"DON BLACKMAN\",\"DR DOG\",DRAKE,DRUGSTORE,DIPLOMATS,DREAM,EAGLES,EDITORS,ELBOW,ELECTRELANE,\"ELIZA DOOLITTLE\",\"ELLE MILANO\",\"ELLIE GOULDING\",\"ELTON JOHN\",\"ELVIS COSTELLO WITH BURT BACHARACH\",\"ELVIS PRESLEY\",EMBRACE,EMINEM,\"EMMY GREAT\",\"EMPIRE! EMPIRE!\",\"ERIC CHURCH\",\"ERIC CLAPTON\",\"ERIN MCCARLEY\",ERRORS,\"EUGENE KELLY\",\"EXPLOSIONS IN SKY\",FABOLOUS,FAILURE,\"FAITH EVANS\",\"FAR EAST MOVEMENT\",FAUNTS,FEIST,FELT,FIELDS,\"FLEET FOXES\",\"FLIGHT OF CONCHORDS\",\"FLORENCE MACHINE\",FOALS,\"FOO FIGHTERS\",\"FOSTER PEOPLE\",\"FOUR TET\",\"FRANK TURNER\",\"FRANZ FEDINAND\",FREE,\"FLAMING LIPS\",GORILLAZ,\"GRATEFUL DEAD\",IDLEWILD,\"IMANI COPPOLA\",\"IMOGEN HEAP\",INCUBUS,INCUBUS,\"IRON WINE\",\"J DILLA\",\"J COLE\",JACK,\"JACK JOHNSON\",JACOBITES,\"JAMES BLAKE\",\"JAMES BLUNT\",\"JAMES BONG\",\"JAMES BROWN\",\"JAMIE T\",\"JAN DELAY\",\"JANE CULLUM\",\"JASON DERULO\",\"JASON MORAN\",\"JAY Z\",\"JAY Z\",\"JEFF WAYNE\",\"JENNIFER LOPEZ\",\"JENNY LEWIS\",JEREMIH,\"JIM JONES\",\"JIMMY EAT WORLD\",\"JOHN COLTRANE\",\"JOHN LEGEND\",\"JOHN MAYER\",\"JOY DIVISION\",\"JUELZ SANTANA\",\"JUNIOR BOYS\",\"JUSTIN TIMBERLAKE\",\"KAISER CHEIFS\",KANTE,\"KANYE WEST\",KARMA,\"KATE NASH\",\"KATE NASH\",\"KATY PERRY\",KESHA,KEANE,KEANE,\"KELLY ROWLAND\",\"KENNY G\",\"KERI HILSON\",\"KEVIN AYERS\",\"KID CUDI\",\"KID HARPOON\",\"KID LOCO\",\"KID ROCK\",\"KINGS OF LEON\",KILLERS,KOOKS,\"LADY GAGA\",\"LADY SOVEREIGN\",\"LED ZEPPELIN\",\"LEE DORSEY\",LEMAR,LEMONGRASS,\"LEROY HUTSON\",\"LIL WAYNE\",\"LILY ALLEN\",\"LITTLE COMETS\",\"LITTLE JOY\",\"LL COOL J\",LLOYD,\"LLOYD BANKS\",\"LONDON LOUNGE\",LOST,LOVE,LUDACRIS,LUDIQUE,\"LUPE FIASCO\",\"LUTHER VANDROS\",\"MACHINE HEAD\",\"MACY GRAY\",MADONNA,MAINO,\"MANU CHAO\",\"MAREN MONTAUK\",\"MARIAH CAREY\",\"MARK FRY\",\"MARK MORRISON\",\"MARK RONSON\",\"MAROON 5\",\"MARY J BLIGE\",\"MASSIVE ATTACK\",\"MAT KEARNEY\",\"MATTHEW DEAR\",\"MEAT LOAF\",METALLICA,MGMT,\"MICHAEL GIACCHINO\",\"MICHAEL JACKSON\",\"MILES DAVIS\",\"MISSY ELLIOTT\",\"MOBB DEEP\",\"MODEST MOUSE\",\"MODEST MOUSE\",MONK,MOON,MORRISSEY,MOTZ,\"MOVING HEARTS\",MUSE,MUTEMATH,\"MY WRITES\",\"MORNING OF\",NAS,\"NE YO\",\"NICKI MINAJ\",\"NINE INCH NAILS\",\"P DIDDY\",\"PATRICK WATSON\",\"PAUL MCCARTNEY\",\"PEARL JAM\",\"PEPE ALGILAR\",\"PEPE WHITE\",\"PETER GABRIEL\",PHARRELL,\"PHIL COLLINS\",PHISH,PITBULL,PIXIES,POLICE,PRINCE,\"R KELLY\",RADIOHEAD,RAKIM,RANK,\"RASCAL FLATTS\",\"RAY CHARLES\",\"RED HOT CHILI PEPPERS\",REDNEX,REENO,\"RELIENT K\",\"RICK ROSS\",\"RICKY NELSON\",RIFT,RIHANNA,\"ROBBIE WILLIAMS\",\"ROD STEWART\",\"RODNEY HUNTER\",ROKOKO,\"ROLLING STONES\",\"ROSCOE DASH\",\"ROY DAVIS JR\",\"RYAN LESLIE\",\"RYAN LESLIE\",RZA,\"SARAH MCLACHLAN\",\"SARAH RUSSELL\",SEAL,\"SHANIA TWAIN\",SHERWOOD,\"SMASH MOUTH\",SMITHS,\"SNOOP DOGG\",\"SOMETHING CORPORATE\",SPOON,SPOON,\"ST LUNATICS\",STARS,\"STAT QUO\",\"STEELY DAN\",\"STEVIE WONDER\",\"STYLES P\",\"SUNSET RUBDOWN\",SUPERCHUNK,\"SWIZZ BEATZ\",\"SECRET HANDSHAKE\",SMITHS,\"T PAIN\",TI,\"TALIB KWELI\",\"TAYLOR SWIFT\",\"THROWING MUSES\",TLC,\"TOM JONES\",\"TOM PETTY\",\"TONY YAYO\",TOOL,\"TORI AMOS\",TORTOISE,TRAIN,\"TREY SONGZ\",\"TRIBE CALLED QUEST\",TUBBS,TYCHO,\"WHITE STRIPES\",\"WAKA FLOCKA FLAME\",WALE,WILCO,\"WILL SMITH\",U2,USHER,\"YEAH YEAH YEAHS\",\"YEAR OF RABBIT\",\"YOUNG BUCK\",\"YOUNG JEEZY\",\"ZAC BROWN BAND\",2PAC,\"50 CENT\"}'::text[]))\n                                       -&gt;  Bitmap Index Scan on index_artists_on_name  (cost=0.00..477.58 rows=413 width=0) (actual time=4.897..4.897 rows=311 loops=1)\n                                             Index Cond: ((name)::text = ANY ('{\"A BOY NAMED EARTH\",\"A ROCKET TO MOON\",\"A SKYLIT DRIVE\",\"A VISIBLE BOY\",\"AARON GILLESPIE\",AB,\"ABBIE BARRETT\",ABERFELDY,\"ABRA NIIRE\",\"ABSOLUTE ZERO\",ACDC,\"ADAM GREEN\",ADELE,ADO,\"ADRINA THORPE\",ADVENTURE,AEROSMITH,\"AESOP ROCK\",AFI,AIR,\"AIR TRAFFIC\",AKON,\"AL GREEN\",\"ALANIS MORISSETTE\",ALCENDOR,\"ALEX CHILTON\",\"ALEXI MURDOCH\",\"ALIAS EHREN\",\"ALICE RUSSELL\",\"ALICIA KEYS\",\"AMANDA PALMER\",\"AMERICAN FOOTBALL\",\"AMY MACDONALD\",\"AMY WINEHOUSE\",\"ANALOG REBELLION\",ANBERLIN,\"ANDREW BIRD\",\"ANDREW HILL\",\"ANGELS AIRWAVES\",AQUEDUCT,\"ARCADE FIRE\",\"ARETHA FRANKLIN\",\"ART BRUT\",\"ART GARFUNKEL\",\"ARTIC MONKEYS\",\"ARTIC MONKEYS\",\"AS TALL AS LIONS\",ASHANTI,\"ATLAS SOUND\",\"ATTACK ATTACK!\",AUTOLUX,\"AVENUE D\",\"AVRIL LAVIGNE\",\"B JU\",BACK2SQUARE1,\"BAD ENGLISH\",BALMORHEA,\"BAND OF HORSES\",BARCELONA,\"BAXTER DURY\",\"BEASTIE BOYS\",BEATLES,BECK,\"BELLE AND SEBASTIAN\",\"BEN FOLDS FIVE\",\"BEN MARTIN\",\"BETTE MIDLER\",\"BETTIE SERVEERT\",BEYONCE,\"BIFFY CLYRO\",\"BILLY BRAGG\",\"BILLY BRAGG AND BLOKES\",\"BILLY JOEL\",\"BILLY OCEAN\",BIRDY,\"BLACK EYED PEAS\",\"BLACK KIDS\",BLACKSTONE,\"BLIND PILOT\",\"BLINK 182\",\"BLOC PARTY\",\"BLOODHOUND GANG\",BLUEBOY,BLUR,\"BOB DYLAN\",\"BOB MARLEY\",\"BOB MARLEY WAILERS\",\"BOMBAY BICYCLE CLUB\",\"BON IVER\",\"BONNIE TYLER\",BONOBO,\"BOX CAR RACER\",\"BOYS LIKE GIRLS\",\"BOYZ II MEN\",BREAKBOT,\"BRENDAN BENSON\",\"BRETT ANDERSON\",BRICOLAGE,\"BRITNEY SPEARS\",BRONCO,\"BRUCE SPRINGSTEEN\",\"BRUNO MARS\",BUSH,\"BUSTA RHYMES\",BUSTED,CAESARS,\"CAGE ELEPHANT\",CAKE,CALLA,\"CALVIN HARRIS\",CAMRON,CAMRON,CAMP,CAMPING,\"CANDIE PAYNE\",CASHEW,\"CASSANDRA WILSOM\",CASSIDY,\"CATE LE BON\",\"CHARLIE FEATHERS\",\"CHERRY GHOST\",CHIC,CHICAGO,CHINGY,\"CHRIS BROWN\",\"CHRISTINA AGUILERA\",\"CHRISTINA STURMER\",CHUMBAWAMBA,CIRCLE,\"CLARENCE CARTER\",CLIPSE,\"COBRA STARSHIP\",COLDPLAY,COMMON,CONSPIRATORS,COOLIO,COPELAND,\"COPY HAHO\",CORNELIUS,CREED,CULTS,\"CYNDI LAUPER\",\"CYPRESS HILL\",\"CAT EMPIRE\",CURE,\"DA LATA\",\"DAFT PUNK\",DALMINJO,DAMERO,\"DARREN HANLON\",\"DASHBOARD CONFESSIONAL\",DATAROCK,\"DAVID BOWIE\",\"DAVID E SUGAR\",\"DAVID GUETTA\",\"DAVID KITT\",\"DEATH CAB FOR CUTIE\",\"DIANA KRALL\",DIGITALISM,\"DINOSAUR JR\",\"DISCO INFERNO\",DISCOVERY,\"DIZZEE RASCAL\",DMX,\"DON BLACKMAN\",\"DR DOG\",DRAKE,DRUGSTORE,DIPLOMATS,DREAM,EAGLES,EDITORS,ELBOW,ELECTRELANE,\"ELIZA DOOLITTLE\",\"ELLE MILANO\",\"ELLIE GOULDING\",\"ELTON JOHN\",\"ELVIS COSTELLO WITH BURT BACHARACH\",\"ELVIS PRESLEY\",EMBRACE,EMINEM,\"EMMY GREAT\",\"EMPIRE! EMPIRE!\",\"ERIC CHURCH\",\"ERIC CLAPTON\",\"ERIN MCCARLEY\",ERRORS,\"EUGENE KELLY\",\"EXPLOSIONS IN SKY\",FABOLOUS,FAILURE,\"FAITH EVANS\",\"FAR EAST MOVEMENT\",FAUNTS,FEIST,FELT,FIELDS,\"FLEET FOXES\",\"FLIGHT OF CONCHORDS\",\"FLORENCE MACHINE\",FOALS,\"FOO FIGHTERS\",\"FOSTER PEOPLE\",\"FOUR TET\",\"FRANK TURNER\",\"FRANZ FEDINAND\",FREE,\"FLAMING LIPS\",GORILLAZ,\"GRATEFUL DEAD\",IDLEWILD,\"IMANI COPPOLA\",\"IMOGEN HEAP\",INCUBUS,INCUBUS,\"IRON WINE\",\"J DILLA\",\"J COLE\",JACK,\"JACK JOHNSON\",JACOBITES,\"JAMES BLAKE\",\"JAMES BLUNT\",\"JAMES BONG\",\"JAMES BROWN\",\"JAMIE T\",\"JAN DELAY\",\"JANE CULLUM\",\"JASON DERULO\",\"JASON MORAN\",\"JAY Z\",\"JAY Z\",\"JEFF WAYNE\",\"JENNIFER LOPEZ\",\"JENNY LEWIS\",JEREMIH,\"JIM JONES\",\"JIMMY EAT WORLD\",\"JOHN COLTRANE\",\"JOHN LEGEND\",\"JOHN MAYER\",\"JOY DIVISION\",\"JUELZ SANTANA\",\"JUNIOR BOYS\",\"JUSTIN TIMBERLAKE\",\"KAISER CHEIFS\",KANTE,\"KANYE WEST\",KARMA,\"KATE NASH\",\"KATE NASH\",\"KATY PERRY\",KESHA,KEANE,KEANE,\"KELLY ROWLAND\",\"KENNY G\",\"KERI HILSON\",\"KEVIN AYERS\",\"KID CUDI\",\"KID HARPOON\",\"KID LOCO\",\"KID ROCK\",\"KINGS OF LEON\",KILLERS,KOOKS,\"LADY GAGA\",\"LADY SOVEREIGN\",\"LED ZEPPELIN\",\"LEE DORSEY\",LEMAR,LEMONGRASS,\"LEROY HUTSON\",\"LIL WAYNE\",\"LILY ALLEN\",\"LITTLE COMETS\",\"LITTLE JOY\",\"LL COOL J\",LLOYD,\"LLOYD BANKS\",\"LONDON LOUNGE\",LOST,LOVE,LUDACRIS,LUDIQUE,\"LUPE FIASCO\",\"LUTHER VANDROS\",\"MACHINE HEAD\",\"MACY GRAY\",MADONNA,MAINO,\"MANU CHAO\",\"MAREN MONTAUK\",\"MARIAH CAREY\",\"MARK FRY\",\"MARK MORRISON\",\"MARK RONSON\",\"MAROON 5\",\"MARY J BLIGE\",\"MASSIVE ATTACK\",\"MAT KEARNEY\",\"MATTHEW DEAR\",\"MEAT LOAF\",METALLICA,MGMT,\"MICHAEL GIACCHINO\",\"MICHAEL JACKSON\",\"MILES DAVIS\",\"MISSY ELLIOTT\",\"MOBB DEEP\",\"MODEST MOUSE\",\"MODEST MOUSE\",MONK,MOON,MORRISSEY,MOTZ,\"MOVING HEARTS\",MUSE,MUTEMATH,\"MY WRITES\",\"MORNING OF\",NAS,\"NE YO\",\"NICKI MINAJ\",\"NINE INCH NAILS\",\"P DIDDY\",\"PATRICK WATSON\",\"PAUL MCCARTNEY\",\"PEARL JAM\",\"PEPE ALGILAR\",\"PEPE WHITE\",\"PETER GABRIEL\",PHARRELL,\"PHIL COLLINS\",PHISH,PITBULL,PIXIES,POLICE,PRINCE,\"R KELLY\",RADIOHEAD,RAKIM,RANK,\"RASCAL FLATTS\",\"RAY CHARLES\",\"RED HOT CHILI PEPPERS\",REDNEX,REENO,\"RELIENT K\",\"RICK ROSS\",\"RICKY NELSON\",RIFT,RIHANNA,\"ROBBIE WILLIAMS\",\"ROD STEWART\",\"RODNEY HUNTER\",ROKOKO,\"ROLLING STONES\",\"ROSCOE DASH\",\"ROY DAVIS JR\",\"RYAN LESLIE\",\"RYAN LESLIE\",RZA,\"SARAH MCLACHLAN\",\"SARAH RUSSELL\",SEAL,\"SHANIA TWAIN\",SHERWOOD,\"SMASH MOUTH\",SMITHS,\"SNOOP DOGG\",\"SOMETHING CORPORATE\",SPOON,SPOON,\"ST LUNATICS\",STARS,\"STAT QUO\",\"STEELY DAN\",\"STEVIE WONDER\",\"STYLES P\",\"SUNSET RUBDOWN\",SUPERCHUNK,\"SWIZZ BEATZ\",\"SECRET HANDSHAKE\",SMITHS,\"T PAIN\",TI,\"TALIB KWELI\",\"TAYLOR SWIFT\",\"THROWING MUSES\",TLC,\"TOM JONES\",\"TOM PETTY\",\"TONY YAYO\",TOOL,\"TORI AMOS\",TORTOISE,TRAIN,\"TREY SONGZ\",\"TRIBE CALLED QUEST\",TUBBS,TYCHO,\"WHITE STRIPES\",\"WAKA FLOCKA FLAME\",WALE,WILCO,\"WILL SMITH\",U2,USHER,\"YEAH YEAH YEAHS\",\"YEAR OF RABBIT\",\"YOUNG BUCK\",\"YOUNG JEEZY\",\"ZAC BROWN BAND\",2PAC,\"50 CENT\"}'::text[]))\n                     -&gt;  Index Scan using articles_pkey on articles  (cost=0.00..0.10 rows=1 width=802) (actual time=0.006..0.006 rows=1 loops=39920)\n                           Index Cond: (articles.id = articles_artists.article_id)\n Total runtime: 4343.603 ms\n(18 rows)\n</code></pre>\n"},{"tags":["performance","big-o"],"answer_count":5,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":297,"score":4,"question_id":1106083,"title":"Big O question","body":"<p>If I have the following code:</p>\n\n<pre><code>IterateArray(object[] array)\n{\n    for(int i=0; i&lt;array.length; i++)\n    {\n        Dosomething(array[i]);\n    }\n}\n</code></pre>\n\n<p>and the <code>Dosomething(object)</code> method's time performance is O(log n), is the overall performance of <code>IterateArray</code> O(n) or O(n log n)?</p>\n\n<p>Thanks.</p>\n"},{"tags":["performance","ienumerable","slowness","except"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":12,"score":0,"question_id":12592174,"title":"Why IEnumerable<Contacts> speed degrades rapidly?","body":"<p>Solved:   I just converted to List, performed the same operation on the List, and converted the List back to IEnumerable.  It's fast now.</p>\n\n<hr>\n\n<p>Here are the 4 lines in issue:</p>\n\n<pre><code>        IEnumerable&lt;PersonContactDTO&gt; personToDelete = _personContacts.Where(x =&gt; x.ID == contactID);\n        _personContacts = _personContacts.Except(personToDelete);\n        IEnumerable&lt;BusinuessContactDTO&gt; businessToDelete = _businessContacts.Where(x =&gt; x.ID == contactID);\n        _businessContacts = _businessContacts.Except(businessToDelete);\n</code></pre>\n\n<p>As the first calls are made to delete a contact, it works very fast.   But around the 10th to 15th contact being deleted, it is miserably slow.</p>\n\n<p>Why?</p>\n"},{"tags":["performance","magento"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":94,"score":0,"question_id":12580828,"title":"Magento saving product is extremly slow, but profiler shows it only takes 1sec","body":"<p>After I click \"Save\" or \"Save and Continue Edit\" button when editing a product, the action will take around one minute. But the profiler shows it only takes around one second.</p>\n\n<p>And other operations like listing the products finish rather quickly, even though sometimes it will take 2 second as shown in the profiler result.</p>\n\n<p>What could have caused this?</p>\n\n<hr>\n\n<p>I wonder why the Profiler tells the operation only takes 1 second. </p>\n\n<p>And it is possible that the save action really just takes 1 second and the page redirection caused the delay. \nThe \"Please wait\" indicator only shows for 1 second, and for the remaining time the browser is actually loading the next page. Could it the previous operation left something blocks the redirection somehow?</p>\n"},{"tags":["java","performance","google-app-engine","gae-datastore","google-bigquery"],"answer_count":5,"favorite_count":3,"up_vote_count":9,"down_vote_count":0,"view_count":370,"score":9,"question_id":8743906,"title":"How to retrieve huge (>2000) amount of entities from GAE datastore in under 1 second?","body":"<p>We have some part of our application that need to load a large set of data (>2000 entities) and perform computation on this set. The size of each entity is approximately 5 KB. </p>\n\n<p>On our initial, naïve, implementation, the bottleneck seems to be the time required to load all the entities (<strong>~40 seconds for 2000 entities</strong>), while the time required to perform the computation itself is very small (&lt;1 second).</p>\n\n<p>We had tried several strategies to speed up the entities retrieval:</p>\n\n<blockquote>\n  <ul>\n  <li>Splitting the retrieval request into several parallel instances and then merging the result: <strong>~20 seconds for 2000 entities</strong>.</li>\n  <li>Storing the entities at an in-memory cache placed on a resident backend: <strong>~5 seconds for 2000 entities</strong>.</li>\n  </ul>\n</blockquote>\n\n<p>The computation needs to be dynamically computed, so doing a precomputation at write time and storing the result does not work in our case.</p>\n\n<p>We are hoping to be able to retrieve ~2000 entities in just under one second. Is this within the capability of GAE/J? Any other strategies that we might be able to implement for this kind of retrieval?</p>\n\n<p>UPDATE: Supplying additional information about our use case and parallelization result:</p>\n\n<blockquote>\n  <ul>\n  <li>We have more than 200.000 entities of the same kind in the datastore and the operation is retrieval-only. </li>\n  <li>We experimented with 10 parallel worker instances, and a typical result that we obtained could be seen in <a href=\"http://pastebin.com/Ug2uVGH3\" rel=\"nofollow\">this pastebin</a>. It seems that the serialization and deserialization required when transferring the entities back to the master instance hampers the performance.</li>\n  </ul>\n</blockquote>\n\n<p>UPDATE 2: Giving an example of what we are trying to do:</p>\n\n<blockquote>\n  <ol>\n  <li>Let's say that we have a StockDerivative entity that need to be analyzed to know whether it's a good investment or not. </li>\n  <li>The analysis performed requires complex computations based on many factors both external (e.g. user's preference, market condition) and internal (i.e. from the entity's properties), and would output a single \"investment score\" value. </li>\n  <li>The user could request the derivatives to be sorted based on its investment score and ask to be presented with N-number of highest-scored derivatives.</li>\n  </ol>\n</blockquote>\n"},{"tags":["java","performance","mergesort","recurrence-relation"],"answer_count":1,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":690,"score":5,"question_id":4848387,"title":"Prove running time of optimized mergesort is theta(NK + Nlog(N/K))?","body":"<p>Okay, I know Mergesort has a worst case time of theta(NlogN) but its overhead is high and manifests near the bottom of the recursion tree where the merges are made.  Someone proposed that we stop the recursion once the size reaches K and switch to insertion sort at that point.  I need to prove that the running time of this modified recurrence relation is theta(NK + Nlog(N/k))?  I am blanking as to how to approach this problem..</p>\n"},{"tags":["performance","jsp"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":31,"score":0,"question_id":12557788,"title":"JSP Page taking too long to load","body":"<p>I have a JSP Page (this is a tabbed view) with 2 text boxes, 2 dropdowns, checkbox &amp; 4 list boxes.</p>\n\n<p>One dropdown is populated by making connection to db and query through prepared statement. </p>\n\n<p>The 4 listboxes &amp; 2nd dropdown get populated through AJAX if the checkbox is checked. By default the checkbox is unchecked.</p>\n\n<p>The problem here is, the JSP Page takes too long to load. It is approx 0.11 min to load the page. \nEven before I check the checkbox and populate the list boxes and the 2nd dropdown, the page loads too slow.</p>\n\n<p>Does being page in tab slow the performance?</p>\n\n<p>Any idea why is this happening and what can I do to improve the page performance? </p>\n"}]}
{"total":25593,"page":19,"pagesize":100,"questions":[{"tags":["c++","string","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":66,"score":0,"question_id":12591609,"title":"C++ string construct performance","body":"<p>I was wondering if below code has any difference between #1, #2 and #3 when passing a string to a function (eg. insert into a vector). Especially when dealing with millions of strings in the code. </p>\n\n<pre><code>std::vector&lt;std::string&gt; v;\nstd::string s(\"foo\");\nint i = 1;\nv.push_back( s + \"bar\" + boost::lexical_cast&lt;std::string&gt;(i) );  // #1\nv.push_back( std::string(s + \"bar\" + boost::lexical_cast&lt;std::string&gt;(i)) );  // #2\nstd::string s2 = s + \"bar\" + boost::lexical_cast&lt;std::string&gt;(i);\nv.push_back(s2);  // #3\n</code></pre>\n"},{"tags":["python","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":82,"score":0,"question_id":12590058,"title":"Python - performance with global variables vs local","body":"<p>I am still new to Python, and I have been trying to improve the performance of my Python script, so I tested it with and without global variables. I timed it, and to my surprise, it ran faster with global variables declared rather than passing local vars to functions. What's going on? I thought execution speed was faster with local variables? (I know globals are not safe, I am still curious.)</p>\n"},{"tags":["c#","performance","entity-framework","x86","x64"],"answer_count":1,"favorite_count":5,"up_vote_count":17,"down_vote_count":0,"view_count":182,"score":17,"question_id":12584628,"title":"Entity Framework spinup much slower on x64 vs x86","body":"<p>My coworker posted this question yesterday: <a href=\"http://stackoverflow.com/questions/12572623/7-second-ef-startup-time-even-for-tiny-dbcontext\">7-second EF startup time even for tiny DbContext</a>.</p>\n\n<p>After taking his code and moving it to a separate solution to isolate it as much as possible, I found that the containing project's platform target had a profound affect on the runtime of the EF startup process.</p>\n\n<p>When targeting x64, I saw that the test took ~7 seconds to spin up the first DbContext and &lt;1 second to spin up the second DbContext (consistent with my coworker's findings who is also targeting x64). However when I switched the platform target to x86, the first DbContext spin up time was reduced by about 4 seconds down to 3.34633 seconds while the second DbContext took the same amount of time as the x64 case.</p>\n\n<p>Given this, it appears the Entity Framework is going through a much different initialization process when targeting a 64-bit system vs 32-bit system. Does anyone have any insight into what is going on under the hood to explain this?</p>\n"},{"tags":["performance","networking","distributed","latency"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":235,"score":1,"question_id":335840,"title":"What is the relative difference between in-proc, interprocess, and inter machine calls?","body":"<p>Ignoring payload size, what is relative difference in latency between an in-proc call (say in C++ or Java), a socket call to a process on the same machine, and a socket call to a process on another machine? This can be expressed as a minimum latency in ns/ms or in terms of relative orders of magnitude. </p>\n\n<p>I'm looking for something similar to this:</p>\n\n<p><a href=\"http://duartes.org/gustavo/blog/post/what-your-computer-does-while-you-wait\" rel=\"nofollow\">http://duartes.org/gustavo/blog/post/what-your-computer-does-while-you-wait</a></p>\n\n<p>... but extended to in-proc vs. network calls (assume fast intranet).</p>\n"},{"tags":["performance","mobile","air","runtime","image-scaling"],"answer_count":0,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":25,"score":1,"question_id":12587818,"title":"How expensive is runtime image scaling in mobile AIR environments?","body":"<p>My guess is that this is actually relatively expensive. I haven't profiled it just yet, but I'm looking for some insight.  If I'm scaling a large bitmap (433x396) to a size maybe 1/5 of the size at runtime, how much of a performance hit is it?  Especially in cases where it's repeated 4 or 5 times.  I'm not aware of if it really depends on the underlying system like Java does.  I'm trying to target different pixel densities,  and my goal would be to bundle a different set of assets based on device configuration via ant script to reduce the size of the application bundle.  I'm not looking directly for any workarounds, I just want some insight into understanding the performance cost of runtime scaling so I can make an informed decision.  Thanks to all in advance.</p>\n"},{"tags":["performance","asp-classic","memcached","appfabric"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":359,"score":2,"question_id":3787806,"title":"using microsoft velocity / memcache from classic asp","body":"<p>hey guys can anyone tell me how to read data from cache saved in microsoft velocity from classic asp?</p>\n\n<p>if above is not possible, then what if i use memcache instead. then is it possible and worth it to read memcache from classic asp</p>\n\n<p>thanks in advance</p>\n"},{"tags":["c#","performance","linq","entity-framework","linq-to-entities"],"answer_count":2,"favorite_count":2,"up_vote_count":6,"down_vote_count":0,"view_count":202,"score":6,"question_id":12572623,"title":"7-second EF startup time even for tiny DbContext","body":"<p>I am trying to reduce the startup time of my EF-based application, but I find that I cannot reduce the amount of time taken for an initial read below 7 seconds even for a single-entity context. What's especially strange is that this time is not context-type specific. </p>\n\n<p>Can anyone explain what causes these slow times and/or how I can get things to run faster?</p>\n\n<p>Here's the complete sample code:</p>\n\n<p>In my database, I have a table named se_stores with a primary key column AptId:</p>\n\n<pre><code>    // a sample entity class\npublic class Apartment\n{\n    public int AptId { get; set; }\n}\n\n    // two identical DbContexts        \n\npublic class MyDbContext1 : DbContext\n{\n    public MyDbContext1(string connectionString) : base(connectionString)\n    {           \n    }\n\n    protected override void OnModelCreating(DbModelBuilder modelBuilder)\n    {\n        Database.SetInitializer&lt;MyDbContext1&gt;(null);\n\n        var config = new EntityTypeConfiguration&lt;Apartment&gt;();\n        config.HasKey(a =&gt; a.AptId).ToTable(\"se_stores\");\n        modelBuilder.Configurations.Add(config);\n\n        base.OnModelCreating(modelBuilder);\n    }\n}\n\npublic class MyDbContext2 : DbContext\n{\n    public MyDbContext2(string connectionString)\n        : base(connectionString)\n    {\n    }\n\n    protected override void OnModelCreating(DbModelBuilder modelBuilder)\n    {\n        Database.SetInitializer&lt;MyDbContext2&gt;(null);\n\n        var config = new EntityTypeConfiguration&lt;Apartment&gt;();\n        config.HasKey(a =&gt; a.AptId).ToTable(\"se_stores\");\n        modelBuilder.Configurations.Add(config);\n\n        base.OnModelCreating(modelBuilder);\n    }\n}\n\n    // finally, I run this code using NUnit:\n\nvar start = DateTime.Now;\nvar apt1 = new MyDbContext1(connectionString).Set&lt;Apartment&gt;().FirstOrDefault();\nvar t1 = DateTime.Now - start;\nstart = DateTime.Now;\nvar apt2 = new MyDbContext2(connectionString).Set&lt;Apartment&gt;().FirstOrDefault();\nvar t2 = DateTime.Now - start;\nConsole.WriteLine(t1.TotalSeconds + \", \" + t2.TotalSeconds);\n</code></pre>\n\n<p>It reliably prints something like the following: 7.5277527, 0.060006. When I switch the test to use MyDbContext2 first, I get the same result (so it happens for whichever DbContext gets initialized first). I also tried pre-generating views using EF power tools. This reduced the time for the first context to around 6.8 seconds, and thus was only a small win.</p>\n\n<p>I understand that DateTime.Now is a terrible profiling method, but these results have held up while using dotTrace. I'm also aware that running some code for the first time invokes a JITing cost, but 7 seconds seems far too high to attribute to that. </p>\n\n<p>I am using EF 4.3.1 and .NET 4 with VS 2010.</p>\n\n<p>Thanks in advance for your help!</p>\n\n<p>EDIT: It was suggested that opening the SQL connection might be causing the problem. </p>\n\n<ol>\n<li>I first tried running a random query using a raw SqlConnection and create command with the same connection string. This took 1 second and did not affect the time of DbContext initialization.</li>\n<li>I then tried creating a SqlConnection with the connection string and passing it through to DbContext's constructor that takes a connection. I passed contextOwnsConnection=false. This also made no difference in the DbContext initialization time.</li>\n<li>Finally, I tried connecting through management studio using the same credentials and connection string options. This was nearly instantaneous.</li>\n<li>In the dotTrace profile, it measures SqlConnectionFactory.CreateConnection(connectionString) as taking 0.7 seconds, which is consistent with the raw SQL time.</li>\n</ol>\n\n<p>EDIT: I wanted to know if the delay was per connection or once only. Thus, I tried having MyDbContext1 and MyDbContext2 connect to entirely different databases on different servers. This DID NOT make a difference regardless of which database was connected to first: the use of a first DbContext took ~7 seconds, while the use of a second context is blazingly fast.</p>\n"},{"tags":["php","performance","oop","static"],"answer_count":6,"favorite_count":6,"up_vote_count":20,"down_vote_count":0,"view_count":5062,"score":20,"question_id":1472721,"title":"Performance of static methods vs. functions ","body":"<p>In PHP, (unlike what i originally thought) there an overhead of calling static methods vs simple functions.</p>\n\n<p>On a very simple bench, this overhead is over 30% of the calling time\n(the method just returns the parameter):</p>\n\n<pre><code>// bench static method\n$starttime = microtime(true);\nfor ($i = 0; $i&lt; 10*1000*1000; $i++)\n\tSomeClass::doTest($i);\n\necho \"Static Time:   \" , (microtime(true)-$starttime) , \" ms\\n\";\n\n// bench object method\n$starttime = microtime(true);\n\nfor ($i = 0; $i&lt; 10*1000*1000; $i++)\n\t$someObj-&gt;doTest($i);\n\necho \"Object Time:   \" , (microtime(true)-$starttime) , \" ms\\n\";\n\n// bench function\n$starttime = microtime(true);\n\nfor ($i = 0; $i&lt; 10*1000*1000; $i++)\n\tsomething_doTest($i);\n\necho \"Function Time: \" , (microtime(true)-$starttime) , \" ms\\n\";\n</code></pre>\n\n<p>outputs:</p>\n\n<pre><code>Static Time:   0.640204906464 ms\nObject Time:   0.48961687088 ms\nFunction Time: 0.438289880753 ms\n</code></pre>\n\n<p>I know the actual time is still negligible unless i am actually calling something 1 million times, but the fact is that its there.</p>\n\n<p>Will anyone care to try and explain what is happening behind the scenes?<br ></p>\n\n<p>update:<br />\n- added object method bench</p>\n"},{"tags":["ajax","performance","wordpress"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":55,"score":0,"question_id":12586988,"title":"Load wordpress content with ajax less server demanding and faster than normal page loading?","body":"<p>This is a technical doubt I had after reading some articles about WP ajax process (cannot find them though).</p>\n\n<p>Is it faster and cheaper to load WP content (any content, page, post, widgets etc...) using WP ajax API than making a full canonical page request?</p>\n\n<p>I thought about this because maybe passing through wp-admin/admin-ajax.php is lighter than loading a new page running the whole WP stack altogether.</p>\n\n<p>Am I right?</p>\n\n<p>Thanks</p>\n"},{"tags":["c#","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":2603,"score":5,"question_id":1221270,"title":"How do I use PerformanceCounterType AverageTimer32?","body":"<p>I'm trying to measure the time it takes to execute a piece of code on my production server.  I'd like to monitor this information in real time, so I decided to give Performance Analyser a whizz.  I understand from <a href=\"http://msdn.microsoft.com/en-us/library/system.diagnostics.performancecountertype.aspx\" rel=\"nofollow\">MSDN</a> that I need to create both an AverageTimer32 and an AverageBase performance counter, which I duly have.  I increment the counter in my program, and I can see the CallCount go up and down, but the AverageTime is always zero.  What am I doing wrong?</p>\n\n<p>Here's a snippit of code :</p>\n\n<pre><code>long init_call_time = Environment.TickCount;\n\n// ***\n// Lots and lots of code...\n// ***\n\n// Count number of calls\nPerformanceCounter perf = \n    new PerformanceCounter(\"Cat\", \"CallCount\", \"Instance\", false);\nperf.Increment();\nperf.Close();\n\n// Count execution time\nPerformanceCounter perf2 = \n    new PerformanceCounter(\"Cat\", \"CallTime\", \"Instance\", false);\nperf2.NextValue();\nperf2.IncrementBy(Environment.TickCount - init_call_time);\nperf2.Close();\n\n// Average base for execution time\nPerformanceCounter perf3 = \n    new PerformanceCounter(\"Cat\", \"CallTimeBase\", \"Instance\", false);\nperf3.Increment();\nperf3.Close();\n\nperf2.NextValue();\n</code></pre>\n"},{"tags":["java","performance","collections"],"answer_count":6,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":180,"score":1,"question_id":12585288,"title":"A fast way to find unique values in the list","body":"<p>Given a list of KeyValuePairs, where each pair has a <code>getValue()</code> method, what would be the fastest way to obtain a <code>List</code> (or <code>Set</code>) of unique Values?</p>\n\n<p>All of the below produce acceptable result. <code>u1</code> seems to be fastest over an expected list size (about 1000-2000 KVP)</p>\n\n<p><strong>Can we do better (faster)?</strong></p>\n\n<pre><code>private static Set&lt;String&gt; u1(List&lt;_KVPair&gt; pairs) {\n    Set&lt;String&gt; undefined = new HashSet&lt;String&gt;();\n\n    for (_KVPair pair : pairs) {\n        undefined.add(pair.getValue());\n    }\n\n    if (undefined.size() == 1) {\n        return new HashSet&lt;String&gt;();\n    }\n    return undefined;\n}\n\nprivate static List&lt;String&gt; u2(List&lt;_KVPair&gt; pairs) {\n\n    List&lt;String&gt; undefined = new ArrayList&lt;String&gt;();\n    for (_KVPair pair : pairs) {\n        if (!undefined.contains(pair.getValue())) {\n            undefined.add(pair.getValue());\n        }\n    }\n\n    return undefined;\n}\n\nprivate static List&lt;String&gt; u3(List&lt;_KVPair&gt; pairs) {\n\n    List&lt;String&gt; undefined = new LinkedList&lt;String&gt;();\n\n    Iterator&lt;_KVPair&gt; it = pairs.iterator();\n    while (it.hasNext()) {\n        String value = it.next().getValue();\n        if (!undefined.contains(value)) {\n            undefined.add(value);\n        }\n    }\n    return undefined;\n}\n</code></pre>\n\n<p>At about 3600 pairs, 'u3' wins. At about 1500 pairs, 'u1' wins</p>\n"},{"tags":["performance","hibernate","orm"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":48,"score":1,"question_id":12585228,"title":"Hibernate Mapping Performance","body":"<p>i have an simple object mapped by hibernate4 and fetch them like this</p>\n\n<pre><code>Criteria crit = session.createCriteria(Entity.class);\ncrit.add(Restrictions.in(\"id\", ids));\nSystem.out.println(crit.list().size());\n</code></pre>\n\n<p>it fetches about 2000 Objects. The pure SQL query takes about 100ms. There are no joins etc. But in hibernate about 4 secs elapse. So the or-mapping/caching etc takes a lot of time. Is there a way to speed it up? </p>\n\n<p>What can you suggest?</p>\n\n<p>the entity looks like this. I have enabled sql logging, but there is no select for the ManyToOne eagerly</p>\n\n<pre><code>@Entity\n@Inheritance(strategy = InheritanceType.SINGLE_TABLE)\n@DiscriminatorFormula(\"type\")\npublic abstract class Entity extends ResourceEntity implements Cloneable {\n    @Id\n    @NotNull\n    @GeneratedValue(strategy = GenerationType.AUTO)\n    @JsonProperty(value = \"id\")\n    private Integer id;\n\n    @NotNull\n    private Type type; //Enum\n\n    private Date expirationDate;\n\n    @NotNull\n    private boolean shared;\n\n    private int orderID;\n\n    @ManyToOne\n    private MaappedEntity me;\n\n    private String strinvar;\n</code></pre>\n\n<p>mh, even with a plain sql query hibernate seems to be slow</p>\n\n<pre><code>SQLQuery sql = session.createSQLQuery(\"select * from Entity\");\nSystem.out.println(sql.list().size());\n</code></pre>\n\n<p><img src=\"http://i.stack.imgur.com/A3MGv.png\" alt=\"enter image description here\"></p>\n"},{"tags":["php","performance"],"answer_count":5,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":999,"score":1,"question_id":3532407,"title":"PHP: func_get_args performance?","body":"<p>I'm about to use <code>func_get_args</code> for reading additional arguments of a function call.</p>\n\n<p>How does this affect the performance? Should I rather use an array for passing additional arguments instead of reading them with the function above?</p>\n"},{"tags":["android","performance","animation","android-fragments"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":62,"score":0,"question_id":12583792,"title":"\"Buttery\" fragment animations in \"pre-butter\" Android","body":"<p>I'm using Google's compat lib to enable fragments in my app (Target SDK 16, Min SDK 8) for devices running on 2.2+. From a functional point of view this runs all fine, but the performance of the view animations that are set up via</p>\n\n<pre><code>FragmentTransaction transaction = getSupportFragmentManager().beginTransaction();\ntransaction.setCustomAnimations(inTransition, outTransition, popInTransition, popOutTransition);\n</code></pre>\n\n<p>where <code>inTransition</code> is a simple translation like</p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;set xmlns:android=\"http://schemas.android.com/apk/res/android\" android:interpolator=\"@android:anim/accelerate_interpolator\"&gt;\n    &lt;translate android:fromYDelta=\"100%p\" android:toYDelta=\"0\" android:duration=\"@integer/animation_duration\" /&gt;\n&lt;/set&gt;\n</code></pre>\n\n<p>and <code>outTransition</code> is a fade animation like</p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;set xmlns:android=\"http://schemas.android.com/apk/res/android\"&gt;\n    &lt;alpha android:fromAlpha=\"1.0\" android:toAlpha=\"0.0\" android:duration=\"@integer/animation_duration\" /&gt;\n&lt;/set&gt;\n</code></pre>\n\n<p>is just plain awful on my Galaxy S2 (4.0.3), i.e. for an animation that lasts about 350ms I'd say approx. half of the frames are dropped. I noticed that the performance gets a little (but not much) better if I disable the fade out, but then of course the complete effect of the animation is gone, because the origin fragment turns black instantly. I also tried a release version of the code, but the performance did not improve either.</p>\n\n<p>What am I doing wrong? How can I make fragment animations smoother?</p>\n"},{"tags":["c#",".net","asp.net-mvc-3","performance","asp.net-mvc-views"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":59,"score":1,"question_id":12583893,"title":"MVC FindView slow on application start","body":"<p>Is there anyway to run through the application and pre-populate the MVC ViewCache to eliminate the 2sec time loss that can sometimes occur during the warm-up of a web app?</p>\n\n<p>At current whilst our application is starting, we sometimes are greeted with 2sec performance lag times.. once it's started there are mere-milliseconds.</p>\n\n<p>In case it helps, I am definitely running in release mode, and only use the Razor engine:</p>\n\n<pre><code>    protected void Application_Start()\n    {\n        AreaRegistration.RegisterAllAreas();\n\n        Bootstrapper.Initialise(); //IOC Setup\n        RegisterGlobalFilters(GlobalFilters.Filters);\n        RegisterRoutes(RouteTable.Routes);\n\n        //Only use the RazorEngine. - http://blogs.msdn.com/b/marcinon/archive/2011/08/16/optimizing-mvc-view-lookup-performance.aspx\n        ViewEngines.Engines.Clear();\n\n        IViewEngine razorEngine = new RazorViewEngine() { FileExtensions = new string[] { \"cshtml\" } };\n\n        ViewEngines.Engines.Add(razorEngine);\n    }\n</code></pre>\n\n<p>Any suggestions welcome.</p>\n\n<p>Ta</p>\n"},{"tags":["java","jquery","performance","jsf","richfaces"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":64,"score":0,"question_id":12570635,"title":"How to keep the JSF compnent tree size minimum","body":"<p>I have a complex UI screen built on Rich faces. Assume it to be as rich as <code>facebook</code> with many inline editors and loads of buttons and labels. I am using <code>Mojarra</code> JSF implementation. And its obvious that the performance is taking a beating due to rebuilding of component tree after every server call.<code>Autocompletes</code> are pathetically slow. Having read some books on JSF, I think this is the way it works. How would this scale for a application like the one I am building with complex UI. </p>\n\n<p>Let me explain the problem very clearly.</p>\n\n<p><img src=\"http://i.stack.imgur.com/DLhRG.png\" alt=\"enter image description here\"> </p>\n\n<p>When the page loads top bar, side bar is fully populated. The green portion is empty.  When any tab button is clicked, it fetches a xhtml file from the server and replaces the innerHTML of green box with the fetched data. The process is done using the jquery ajax call.</p>\n\n<pre><code>        jQuery.ajax({\n          url: url,\n          dataType: \"html\",\n          cache: false\n        }).done(function( html ) {  \n\n                 jQuery(\"#green_block\").html(html);\n\n             }      \n        }); \n</code></pre>\n\n<p>Note:</p>\n\n<ol>\n<li><p>Every component on the screen is backed by a <code>request scoped</code> bean\nwhere getter have no business logic.</p></li>\n<li><p>There is a post construct method in every bean to populate bean\n    attributes</p></li>\n</ol>\n\n<p>Observation:</p>\n\n<ol>\n<li>When there is any activity on the top bar, be it clicking a button\nor submitting a data, the logs shows that even the side bar is\nrebuilt internally. i.e. the beans used in the sidebar are setup again. We have verified this many times in all the environments. The green part or the tab content of the page is not reloaded in any way (ie those beans are not rebuilt or setup again)</li>\n<li>When there is any activity within the green block which got loaded\nthrough <code>ajax</code>, we didn't observe any components being rebuilt on top\nbar or side bar. Only the green block was re built during render\nphase</li>\n<li>If any action within green block (command link or command button) tried re-rendering any component in top bar or side bar, the render did not work at all. </li>\n</ol>\n\n<blockquote>\n  <p>This clearly stated that the JSF treated the two blocks independently.\n  We want to know if this is the actual behavior or have we missed\n  something really trivial?</p>\n</blockquote>\n\n<p>Also, every time there is an ajax call in the tab content, all the beans within the tab content (basically displaying different types of data within the tab) are rebuilt (confirmed this with the eclipse debugger). Also all the backing beans for components like rich:autocomplete etc are setup again. This is slowing down operations within the tab (like the autocomplete) and the tab switching as well.</p>\n\n<p>As suggested we will try to hire a JSF architect but until then, any help from JSF experts in SO is much appreciated. </p>\n"},{"tags":["java","performance","jsf-1.2","ajax4jsf"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":88,"score":0,"question_id":11686069,"title":"Performance issue -  LRUMap() in AjaxStateHolder in JSF?","body":"<p>I am using SUN JSF1.2 and Richfaces 3.3.1 in our application. As apart of performance tuning added the below parameters in the web.xml</p>\n\n<pre><code>&lt;context-param&gt;\n   &lt;param-name&gt;com.sun.faces.numberOfViewsInSession&lt;/param-name&gt;\n   &lt;param-value&gt;2&lt;/param-value&gt;\n&lt;/context-param&gt;\n&lt;context-param&gt;\n   &lt;param-name&gt;com.sun.faces.numberOfLogicalViews&lt;/param-name&gt;\n   &lt;param-value&gt;2&lt;/param-value&gt;\n&lt;/context-param&gt;\n</code></pre>\n\n<p>Still when i inspected the session object i saw that org.ajax4jsf.application.AjaxStateHolder was consuming almost 85%( Almost 10 MB) of the session memory. Even i tried to get the AjaxStateHolder object from session but not able to delete all the cached session.</p>\n\n<p>I have searched the web, it was mentioned that is a problem exist in Richfaces. </p>\n\n<p>Is there anyway to clear all those cached views from the LRMap.</p>\n"},{"tags":["mysql","sql","performance","query","join"],"answer_count":2,"favorite_count":2,"up_vote_count":4,"down_vote_count":0,"view_count":67,"score":4,"question_id":12582162,"title":"Mysql: get 2 result from every cat [one to many relation ship]","body":"<p><strong>I'm searching for the best method to get result with database contain more than 100000 Post and more than 100000 Cat</strong> </p>\n\n<p>Here is my tables</p>\n\n<p><strong>Cats</strong></p>\n\n<pre><code>-----------------\n-  id   | name  |\n-----------------\n-  1    | x     |\n-----------------\n-  2    | y     |\n-----------------\n</code></pre>\n\n<p><strong>Post</strong></p>\n\n<pre><code>--------------------------------------\n-  id   | cat_id | title  | content  |\n--------------------------------------\n-  1    | 1      | Post 1 | .. . .  .|\n--------------------------------------\n-  2    | 1      | Post 2 | . . . . .|\n--------------------------------------\n-  3    | 2      | Post 3 | .. . .  .|\n--------------------------------------\n-  4    | 1      | Post 4 | . . . . .|\n--------------------------------------\n-  5    | 1      | Post 5 | .. . .  .|\n--------------------------------------\n-  6    | 2      | Post 6 | . . . . .|\n--------------------------------------\n-  7    | 1      | Post 7 | .. . .  .|\n--------------------------------------\n-  8    | 2      | Post 8 | . . . . .|\n--------------------------------------\n</code></pre>\n\n<p>Here's the Result I want to get</p>\n\n<p><strong>Result</strong></p>\n\n<pre><code>--------------------------------------\n-Postid | cat_id | title  | content  |\n--------------------------------------\n-  1    | 1      | Post 1 | .. . .  .|\n--------------------------------------\n-  2    | 1      | Post 2 | . . . . .|\n--------------------------------------\n-  3    | 2      | Post 3 | .. . .  .|\n--------------------------------------\n-  6    | 2      | Post 4 | . . . . .|\n--------------------------------------\n</code></pre>\n\n<p><strong>Here is Query I Just Write , But i look for Best query</strong></p>\n\n<pre><code>SELECT\n  *\nFrom \n  post\nWHERE posts.cat_id = 1 limit 2\n\nUNION\n\nSELECT\n  * \nFrom \n  post\nWHERE posts.cat_id = 2 limit 2\n</code></pre>\n\n<p><strong>What Happen if i want to get from 10 cats in one query</strong></p>\n"},{"tags":["php","mysql","performance","loops","while-loop"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":1,"view_count":94,"score":2,"question_id":12582398,"title":"Writing a better code / query","body":"<p>In my website, user's can update and other can comment on the updates, as usual like other social networking websites..</p>\n\n<p>What am doing :</p>\n\n<pre><code>query = fetches first name, last name, user id, update id, update post UNION same details for the user who has logged in i.e /* Updates Of Current Logged In User + Updates Of People He Is Following */\n\nloop {\nprints first name last name and update\n}\n</code></pre>\n\n<p>later I introduced commenting feature so now what I do is in the loop I call a query and run a loop again for comments..</p>\n\n<pre><code>loop {\nprints first name last name and update\n\ncommentquery = fetches firstname, last name, user id, comment, of the particulat post\n    again loop {\n         prints first/last name of the person who commented, and his comment\n     }\n}\n</code></pre>\n\n<p>Now according to me I guess this can be accomplished with the SQL query, that when I retrieve posts, I can fetch it's comments along with it, or is this the right way? the problem is when the outer loop runs, the inner loop also runs to fetch the appropriate comments, so can I join tables to retrieve the comments related to posts/updates?</p>\n"},{"tags":["javascript","performance","equality","comparison-operators","equality-operator"],"answer_count":5,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":111,"score":1,"question_id":12374815,"title":"JavaScript - === vs == operators performance","body":"<p>A few weeks ago, I have read this thread <a href=\"http://stackoverflow.com/questions/12135518/is-faster-than\">Is &lt; faster than &lt;=?</a> about comparison operators in <code>C</code>. It was said that there is no difference in the performance between <code>&lt;</code> and <code>&lt;=</code> as they are interpreted as same/similar machine commands.</p>\n\n<p>At the same time, in our company's \"best practices\", it was said that we should always use  \"===\" to compare things instead of \"==\". So, I started to wonder if this is always appropriate as I am used to using the \"==\" and \"typeof ... == \" and do not want to change my way of writing :-] </p>\n\n<p>Note that this is in the context of JavaScript.</p>\n\n<p>So, I have a little research and here <a href=\"http://stackoverflow.com/questions/359494/javascript-vs-does-it-matter-which-equal-operator-i-use\">JavaScript === vs == : Does it matter which &quot;equal&quot; operator I use?</a> it is said that:</p>\n\n<blockquote>\n  <p>This is because the equality operator == does type coercion...meaning\n  that the interpreter implicitly tries to convert the values and then\n  does the comparing.</p>\n  \n  <p>On the other hand, the identity operator === does not do type\n  coercion, and so thus it does not convert the values of the values\n  when comparing</p>\n</blockquote>\n\n<p>And I started to wonder if this means that when I use the \"===\" operator, I will get good performance as no resources will be spent on converting the operands. And after all code is turned into machine commands, does this mean that just as there is no difference in <code>C</code> when you use <code>&lt;</code> and <code>&lt;=</code>, this is the same in JavaScript and other languages?</p>\n"},{"tags":["performance","algorithm","language-agnostic","scheduling"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":56,"score":0,"question_id":12244807,"title":"Algorithm for scheduling optimum record list","body":"<p>Im looking for an algorithm for generating an optimum record list for a recording device given the following items list:</p>\n\n<p><a href=\"http://img692.imageshack.us/img692/7952/recordlist.png\" rel=\"nofollow\"><img src=\"http://img692.imageshack.us/img692/7952/recordlist.png\" alt=\"Image link here\"></a></p>\n\n<p>At the moment the constraints are:  </p>\n\n<ul>\n<li>No overlaps must exist.  </li>\n<li>Compromise between compute speed and solved conflicts.</li>\n</ul>\n\n<p>In the future it's possible more options will be added:  </p>\n\n<ul>\n<li>Possibility for the user to have several recording devices.  </li>\n<li>Possibility for the user to establish recording priorities for his/her favourites programmes (  1 highest -  3 lowest ).</li>\n</ul>\n\n<p>The context is as follows:  </p>\n\n<ul>\n<li>Items list is 1 week as maximum.(current time - current time + 1 week)  </li>\n<li>Average items list size is 100 items and 300 maximum.</li>\n</ul>\n\n<p>Also I would like to know if there is a way to generate the most optimum possible record list ( highest percentage of programmes sent to record)\nin the case we cannot solve the 100% of the conflicts regardless of the processing time.</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["python","django","performance","postgresql"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":72,"score":1,"question_id":12568390,"title":"Django cursor.execute(QUERY) much slower than running the query in the postgres database","body":"<p>I've got a postgresql-query that returns 120 rows <code>{integer, boolean, integer, varchar(255), varchar(255), bigint, text}</code> in about <strong>70ms</strong> when done in the database running <code>psql</code>.</p>\n\n<p>Using python/django with <code>django.db.connection.cursor.execute()</code> it takes <strong>10s</strong> to run, on the same machine.</p>\n\n<p>I've tried putting all the rows into an array, and a single string (18k characters, but returning only the first 500 takes the same time) so there is only one row returned but with no gain.</p>\n\n<p>Any ideas as to why there is such a dramatic slowdown in running a query from within python and in the db?</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>I had to increase the work_mem to get the function running timely in psql. Other functions/queries don't show the same pattern, the difference between psql and python is only a few milliseconds.</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>Cutting down the work_mem to 1MB shows similar numbers in psql and the django shell. Could it be that django is not going by the memory set in work_mem?</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>Ugh. The problem was that the work_mem set in psql is not valid globally, if I set the memory in the function, the call is timely. I suppose setting this in the configuration file would work globally.</p>\n"},{"tags":["performance","graph","storage","relationship"],"answer_count":1,"favorite_count":3,"up_vote_count":4,"down_vote_count":0,"view_count":103,"score":4,"question_id":12580146,"title":"Need solution for objects and relationships storage","body":"<p>I need in solution for the storage objects and relationships between this objects with the best perfomance. I need to store the user object and the relations of friendship between this users. </p>\n\n<p>I need in something likeness of the social graph. I need to store like this:</p>\n\n<pre><code>----------  friend   ----------  friend   ----------\n| user3  | &lt;-------  | user1 | --------&gt;  |  user2 |\n----------           ----------           ----------\n</code></pre>\n\n<p>I try to use orientdb-graphed database for storaging users and relationships between users, but when i started to test it (we use erlang/orientdb http rest api), java/orientdb eventually begins to consume more and more CPU time, and eventually productivity falls.</p>\n\n<p>I use orientdb-graphed-1.0. Service consists from 2 parts: external player service and friend service. Friend service work with orientdb.</p>\n\n<p>Basic operations which we use and planned load:</p>\n\n<ul>\n<li><p>add player1 to friend of player2 -  5-10 requst per minute. Here we send profile - 4kb, and 2 player names by 32 bytes.</p></li>\n<li><p>remove friendship - 1 req per min. Here we send 2 player names by 32 bytes.</p></li>\n<li><p>confirm friendship - 5-10 requst per minute. Here we send 2 player names by 32 bytes.</p></li>\n<li><p>reject friendship - 5-10 requst per minute. Here we send 2 player names by 32 bytes.</p></li>\n<li><p>update player status -  250-500 req per min. Here we send player name 32 bytes and status 128-256 bytes.</p></li>\n<li><p>update player profile - 250-500 req per min. Here we send player name 32 bytes and profile 4kb.</p></li>\n<li><p>get friends list of player - 1000 req per min. Here we send player name 32 bytes and list which consists of: player's name 32 byte, profile 4kb, status - 128-256, and is_friend field - 0 or 1 value.</p></li>\n</ul>\n\n<p>Interaction diagrams: <img src=\"http://i.stack.imgur.com/JpEld.png\" alt=\"enter image description here\"></p>\n\n<p>I expect that will be online from 10 000 to 50 000 users. Total available is to 2.5 million records.</p>\n\n<p>What graph database with the best perfomance are you use in your project? We make friends service for the social game and i need a solution that's i can store users and their relationships in the most convenient and performing case. This does not have to be a graph database. Welcome any easy solutions for this problem</p>\n\n<p>What i need:</p>\n\n<ul>\n<li><p>Database to store users and relationships between users with the best perfomance for add_ friends/ get_friends operations.</p></li>\n<li><p>it's desirable that would be erlang solution or has erlang api or rest api.</p></li>\n</ul>\n\n<p>Thank you.</p>\n"},{"tags":["php","performance","magento"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":1282,"score":1,"question_id":2470772,"title":"Index page in Magento is way too slow, what can I do?","body":"<p>Weirdly, the index page of my Magento commerce is very slow. While you navigate the products, brands, searches etc is very fast, but every time you click on the banner to go to homepage or enter the website, it take ages to load.</p>\n\n<p>I wonder what I can do about this?<br>\nI don't know where to start, since I am new to Magento. I thought I could go on and read the code, but that would take ages too, since Magento is very complex. \nMaybe I can analyze it somehow?</p>\n"},{"tags":["performance","hibernate","jpa","jdbc","batch"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":140,"score":2,"question_id":12276987,"title":"Awful performance of JPA batch processing compared to Hibernate or JDBC","body":"<p>I needed to create a batch recently which reads over a table with millions of rows. The table has about 12 columns and I only need to do a <code>read</code> operation. But I needed all fields therefore I thought about using persistence objects.</p>\n\n<p>I really used only the most basic code only to achieve that and with no tweaks. <code>JPA</code> was quite annoying because it forced me to use custom paging with <code>maxResults</code> and <code>minResults</code>. You can view the approximate code hyperlinks below, if you are interested. There really is nothing else to it, beside the default XML files etc.</p>\n\n<p>The JPA code: <a href=\"http://codeviewer.org/view/code:297e\" rel=\"nofollow\">http://codeviewer.org/view/code:297e</a><br>\nThe Hibernate code: <a href=\"http://codeviewer.org/view/code:297f\" rel=\"nofollow\">http://codeviewer.org/view/code:297f</a><br>\nThe JDBC code: same as above, but with \"d\" on the end (sorry I can only post 2 links)<br></p>\n\n<p>The result in time of finished operations was something like that. I am only talking of read-operations:</p>\n\n<pre><code>JPA:         Per 5 seconds: 1.000||Per Minute: 12.000||Per Hour: 720.000\nHibernate:   Per 5 seconds: 20.000||Per Minute: 240.000||Per Hour: 14.400.000\nJDBC:        Per 5 seconds: 50.000-80.000||Per Minute: 600.000-960.000||Per Hour: 36.000.000-57.600.000\n</code></pre>\n\n<p>I can't explain it, but JPA is ridiculous. It can only be a big bad joke. The funny thing is that it startet with the same speed as the Hibernate code, but after about 30.000 records it became slower and slower until it got stable at 1.000 read operations per 5 seconds. It has reached that point after finishing approximately 100.000 records. But honestly... there is no point in that speed.</p>\n\n<p>Why is that so? Please explain it to me. I really don't know what I'm doing wrong. But I also think it shouldn't be that slow, even with default settings. It can't be and it must not be! In comparison to that Hibernate and <code>JDBC</code> speed is acceptable and stable all the time.</p>\n"},{"tags":["css","performance","css-selectors"],"answer_count":1,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":85,"score":3,"question_id":12228933,"title":"How slow is * really?","body":"<p>Everybody states, that the <code>* { ... }</code> selector is very slow. But how slow is it really?</p>\n\n<p>I always try to avoid it, but sometimes it's very useful. For example: <code>* + h1 { margin-top 1em; }</code></p>\n"},{"tags":["android","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":5,"view_count":100,"score":-5,"question_id":12495078,"title":"The application has stopped unexpectedly. Please try again. This is repeated error","body":"<blockquote>\n  <p>I have written the following code and there comes an error while launching the application. </p>\n  \n  <p><strong>The error is Application has stopped unexpectedly. Please try again.</strong></p>\n</blockquote>\n\n<pre><code>public class MainActivity extends Activity {\n\n    @Override\n    public void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        PowerManager pm= (PowerManager)getSystemService(Context.POWER_SERVICE);\n        PowerManager.WakeLock wl = pm.newWakeLock(PowerManager.SCREEN_DIM_WAKE_LOCK, \"my Tag\");\n        wl.acquire();\n    }\n\n    @Override\n    public boolean onCreateOptionsMenu(Menu menu) {\n        getMenuInflater().inflate(R.menu.activity_main, menu);\n        return true;\n    }\n}\n</code></pre>\n"},{"tags":["c#","performance","autocompleteextender"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":124,"score":1,"question_id":10459046,"title":"AutoCompleteExtender very slow","body":"<p>I asked this in a comment, but I don't think I'm supposed to ask a second question commenting on the first one.  I have AutoCompleteExtender and it is very slow..  The method that gets my list to fill the AutoCompleteExtender has to Get and Query XML from API everytime.  The problem is my method, inside of this method I cannot access SessonState, Cookie, even variables from static methods on the same page, so I see no alternative to GET and Query every time.  This is SLOW though, really not worth having.  There has to be another way (maybe not using the AJAX toolkit) to get this to run fast.</p>\n\n<pre><code>[System.Web.Script.Services.ScriptMethod()]\n[System.Web.Services.WebMethod]\npublic static List&lt;string&gt; GetNames(string prefixText, int count)\n {\n    //Code Here Takes long\n }\n</code></pre>\n"},{"tags":["python","performance","bzip2"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":52,"score":0,"question_id":12575930,"title":"Is Python bz2file slower than bz2?","body":"<p>I have a Python script that parses BZ2 compressed logfiles using regex.</p>\n\n<p>We were getting pretty poor performance, which I initially thought was my regex - however, when I looked further, it seems bz2file was performing poorly.</p>\n\n<p>We are using Python 2.6, and bz2file 0.9.</p>\n\n<p>I've noticed that sequentially reading the file using Bz2file (http://pypi.python.org/pypi/bz2file) appears substantially slower than that using the in-built bz2 implementation.</p>\n\n<p>I wrote two test scripts - one using bz2:</p>\n\n<pre><code>import bz2\nfilename = 'some_bz2file.bz2'\n\nif __name__ == \"__main__\":\n    f = bz2.BZ2File(filename, 'rb')\n    for line in f:\n        print(line)\n</code></pre>\n\n<p>and another using bz2file:</p>\n\n<pre><code>import bz2file\nfilename = 'some_bz2file.bz2'\n\nif __name__ == \"__main__\":\n    f = bz2file.BZ2File(filename, 'rb')\n    for line in f:\n        print(line)\n</code></pre>\n\n<p>I time them both:</p>\n\n<pre><code>$ time python26 testbz.py  &gt; /dev/null\n\nreal    0m0.608s\nuser    0m0.596s\nsys     0m0.011s\n\n$ time python26 testbz2file.py &gt; /dev/null\n\nreal    0m12.035s\nuser    0m11.952s\nsys     0m0.075s\n</code></pre>\n\n<p>For comparison, bzcat on the same file:</p>\n\n<pre><code>$ time bzcat some_bz2file.bz2 &gt; /dev/null\n\nreal    0m0.503s\nuser    0m0.499s\nsys     0m0.004s\n</code></pre>\n\n<p>My understanding was that bz2file was just a wrapper about bz2, but adding handling for multiple-stream BZ2 files (which we do use).</p>\n\n<p>Is there any other reason why bz2file might be so much slower than bz2? (Or is my analysis above flawed?) And if so, is there any way to speed up bz2file?</p>\n\n<p>Cheers,\nVictor</p>\n\n<p>Edit - I did some more testing, including on Python 3.3 - apparently bz2file is a backport of Python 3.3's bz2 module - and also flushing caches as jordanm suggests (I did this in a separate terminal between each run, as root):</p>\n\n<pre><code>[vichoo@dev_desktop_vm Desktop]$ time /opt/python3.3/bin/python3.3 testbz2.py &gt; /dev/null\n\nreal    0m5.170s\nuser    0m5.009s\nsys     0m0.030s\n[vichoo@dev_desktop_vm Desktop]$ time /opt/python3.3/bin/python3.3 testbz2file.py &gt; /dev/null\n\nreal    0m5.245s\nuser    0m4.979s\nsys     0m0.060s\n[vichoo@dev_desktop_vm Desktop]$ time /opt/python2.7/bin/python2.7 testbz2.py &gt; /dev/null\n\nreal    0m0.500s\nuser    0m0.410s\nsys     0m0.030s\n[vichoo@dev_desktop_vm Desktop]$ time /opt/python2.7/bin/python2.7 testbz2file.py &gt; /dev/null\n\nreal    0m5.801s\nuser    0m5.529s\nsys     0m0.050s\n</code></pre>\n\n<p>There does seem to be something funny here, not sure if it's my methodology, or if there is an actual performance regression between Python 2.x's bz2 and Python 3.x's bz2.</p>\n"},{"tags":["java","performance","compiler-optimization"],"answer_count":5,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":2450,"score":1,"question_id":2391219,"title":"java compiler optimization","body":"<p>Is the Java compiler smart enough to optimize loop below, by extracting the</p>\n\n<pre><code>Double average = new Double( totalTime / callCount ); \n</code></pre>\n\n<p>out of the for loop?</p>\n\n<pre><code>public double computeSD( Set values, int callCount, long totalTime ) {\n  double diffs = 0.0d; \n  for( Iterator i=values.iterator(); i.hasNext(); ) {\n    double value = ( ( Double )i.next() ).doubleValue(); \n    Double average = new Double( totalTime / callCount ); \n    diffs += ( value – average.doubleValue() ) * ( value – average.doubleValue() );\n  } \n  double variance = diffs / callCount;\n  return Math.sqrt( variance );\n}\n</code></pre>\n"},{"tags":["javascript","ajax","internet-explorer","performance","html-select"],"answer_count":8,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":19763,"score":5,"question_id":698817,"title":"Faster way to populate <select> with Javascript","body":"<p>I have two &lt;select> boxes on a form.  Selecting an item in the first &lt;select> box will determine what should appear in the second &lt;select> (Using Ajax http_request). </p>\n\n<p>In some cases there can be a large 500 (guess) items in the second select and it takes time 5-10 seconds to update in IE.  Firefox seems to work perfectly.</p>\n\n<p>I wonder if there is a faster way to achieve this.  Currently the server creates a string passes it to the client which is then broken up and add each item to the select by creating an option element and then adding it to the &lt;select>.</p>\n\n<p>I did try to create the whole select item as a string on the server and add that to the form but for some reason it wouldn't work in Firefox (missed something?) </p>\n\n<p>Thanks</p>\n"},{"tags":["performance","image-processing","povray"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":299,"score":3,"question_id":2269460,"title":"Sample configuration to speed up POVRAY image creation","body":"<p>We're using POV-Ray to generate some 80 images per run, which we stitch together to form two  moving GIF files (two 360 degree views of a scene). We're looking for ways to speed-up this image creation (on a headless linux server) as much as possible, as they'll be displayed on a webpage directly following creation.</p>\n\n<p>Now I know setup might be suboptimal, as POV-Ray is mostly designed for high quality images, but unfortunately this process can't be altered as it's an external tool that generates the POV-Ray files.</p>\n\n<p>Given that we're stitching multiple images together into a moving GIF, I suspect there's a lot of performance to be gained in lowering the image quality, colors, lighting and such, but unfortunately I have no prior experience with POV-Ray or any of these settings.</p>\n\n<p>I was wondering if anyone would be able to provide or guide me to a sample configuration that will speed-up this image creation as much as possible, without a much noticeable loss in image quality.</p>\n\n<p>Best regards,\nTim</p>\n"},{"tags":["java","performance","optimization"],"answer_count":5,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":801,"score":5,"question_id":3290233,"title":"Java Loops Optimization","body":"<p>Give the following (straight-forward) code:</p>\n\n<pre><code>public class pr1 {\n\n    public static void f1(){\n        long sx = 0, s;\n        s = System.currentTimeMillis();\n        for(long i = 0; i &lt; Integer.MAX_VALUE; ++i){\n            sx += i;\n        }\n        System.out.println(\"f1(): \" + (System.currentTimeMillis() - s));\n    }\n\n    public static void f2(){\n        long sx = 0, s, i;\n        s = System.currentTimeMillis();\n        i = Integer.MAX_VALUE;\n        while(i--&gt;0){\n            sx+=i;\n        }\n        sx += Integer.MAX_VALUE;\n        System.out.println(\"f2(): \" + (System.currentTimeMillis() - s));\n    }\n\n    public static void f3(){\n        long sx = 0, s, i;\n        s = System.currentTimeMillis();\n        i = Integer.MAX_VALUE;\n        while(--i&gt;0){\n            sx+=i;\n        }\n        sx += Integer.MAX_VALUE;\n        System.out.println(\"f3(): \" + (System.currentTimeMillis() - s));\n    }\n\n    public static void f4(){\n        long sx = 0, s, i;\n        s = System.currentTimeMillis();\n        i = Integer.MAX_VALUE;\n        do{\n            sx+=i;\n        }while(--i&gt;0);\n        System.out.println(\"f4(): \" + (System.currentTimeMillis() - s));\n    }\n\n    public static void main(String args[]){\n        f1();\n        f2();\n        f3();\n        f4();\n    }\n}\n</code></pre>\n\n<p>And the actual results after running the code:</p>\n\n<pre><code>f1(): 5828\nf2(): 8125\nf3(): 3406\nf4(): 3781\n</code></pre>\n\n<p>Can you please explain me the big time differences ? Theoretically the loops are achieving the same functionality, but in practice it seems there is a relevant time difference for each of the four versions.</p>\n\n<p>After repetitive executions the results are very much the same.</p>\n\n<p><strong>LATER EDIT</strong>\nAs another test I've rewritten the main method:</p>\n\n<pre><code>public static void main(String args[]){\n    for(int i = 0; i &lt; 4; ++i){\n        f1(); f2(); f3(); f4();\n    }\n}\n</code></pre>\n\n<p>And the new results are:</p>\n\n<pre><code>f1(): 5906\nf2(): 8266\nf3(): 3406\nf4(): 3844\nf1(): 5843\nf2(): 8125\nf3(): 3438\nf4(): 3859\nf1(): 5891\nf2(): 8156\nf3(): 3406\nf4(): 3813\nf1(): 5859\nf2(): 8172\nf3(): 3438\nf4(): 3828\n</code></pre>\n\n<p>And for 10 repititions:</p>\n\n<pre><code>f1(): 5844\nf2(): 8156\nf3(): 3453\nf4(): 3813\nf1(): 5844\nf2(): 8218\nf3(): 3485\nf4(): 3937\nf1(): 5985\nf2(): 8156\nf3(): 3422\nf4(): 3781\nf1(): 5828\nf2(): 8234\nf3(): 3469\nf4(): 3828\nf1(): 5844\nf2(): 8328\nf3(): 3422\nf4(): 3859\nf1(): 5844\nf2(): 8188\nf3(): 3406\nf4(): 3797\nf1(): 5906\nf2(): 8219\nf3(): 3422\nf4(): 3797\nf1(): 5843\nf2(): 8203\nf3(): 3454\nf4(): 3906\nf1(): 5844\nf2(): 8140\nf3(): 3469\nf4(): 3812\nf1(): 5860\nf2(): 8109\nf3(): 3422\nf4(): 3813\n</code></pre>\n\n<p>After removing the calculus between the loops, the results are still a little different:</p>\n\n<pre><code>public class pr2 {\n\n    public static void f1(){\n        long sx = 0, s;\n        s = System.currentTimeMillis();\n        for(long i = 0; i &lt; Integer.MAX_VALUE; ++i);\n        System.out.println(\"f1(): \" + (System.currentTimeMillis() - s));\n    }\n\n    public static void f2(){\n        long sx = 0, s, i;\n        s = System.currentTimeMillis();\n        i = Integer.MAX_VALUE;\n        while(i--&gt;0);\n        System.out.println(\"f2(): \" + (System.currentTimeMillis() - s));\n    }\n\n    public static void f3(){\n        long sx = 0, s, i;\n        s = System.currentTimeMillis();\n        i = Integer.MAX_VALUE;\n        while(--i&gt;0);\n        System.out.println(\"f3(): \" + (System.currentTimeMillis() - s));\n    }\n\n    public static void f4(){\n        long sx = 0, s, i;\n        s = System.currentTimeMillis();\n        i = Integer.MAX_VALUE;\n        do{\n        }while(--i&gt;0);\n        System.out.println(\"f4(): \" + (System.currentTimeMillis() - s));\n    }\n\n    public static void main(String args[]){\n        for(int i = 0; i &lt; 2; ++i){\n            f1(); f2(); f3(); f4();\n        }\n    }\n}\n</code></pre>\n\n<p>But the time difference still exists:</p>\n\n<pre><code>f1(): 3219\nf2(): 4859\nf3(): 2610\nf4(): 3031\nf1(): 3219\nf2(): 4812\nf3(): 2610\nf4(): 3062\n</code></pre>\n\n<p>JVM:</p>\n\n<pre><code>java version \"1.6.0_20\"\nJava(TM) SE Runtime Environment (build 1.6.0_20-b02)\nJava HotSpot(TM) Client VM (build 16.3-b01, mixed mode, sharing)\n</code></pre>\n\n<p><strong>LATER EDIT:</strong>\nFor the first version, I've used -O parameter for javac. The new results are:</p>\n\n<pre><code>f1(): 3219\nf2(): 4859\nf3(): 2610\nf4(): 3031\n</code></pre>\n\n<p><strong>LATER EDIT</strong></p>\n\n<p>Ok, I've tried the same code at home, using a Linux machine with:</p>\n\n<pre><code>java version \"1.6.0_18\"\nOpenJDK Runtime Environment (IcedTea6 1.8) (6b18-1.8-0ubuntu1)\nOpenJDK Server VM (build 14.0-b16, mixed mode)\n</code></pre>\n\n<p>And the results were \"normal\". No problems now:</p>\n\n<pre><code>f1(): 7495\nf2(): 7418\nf3(): 7457\nf4(): 7384\n</code></pre>\n"},{"tags":["regex","performance","haskell"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":150,"score":1,"question_id":12547446,"title":"Slow file parsing with regex","body":"<p>just tried to rewrite trivial file parser from pythonto haskell but found it painfully slow (about 15 times slower on the same machine). Code compiled with ghc -O2.</p>\n\n<p>The goal is to count number of lines where regular expression matches.\nThe sample text file is huge (several GiBs).</p>\n\n<p>Here is the code:</p>\n\n<pre><code>import Text.Regex.Posix\nimport qualified Data.ByteString as BS\nimport qualified Data.ByteString.Char8 as BC\n\nfilename = \"sample.dat\"\n\nmcount' :: String -&gt; [BS.ByteString] -&gt; Int\nmcount' sample file = foldr (\\e acc -&gt; if e =~ sample then acc+1 else acc) 0 file\n\nmain = do\n    fcnt &lt;- fmap BC.lines $ BS.readFile filename\n    print $ mcount' \"myregex\" fcnt\n</code></pre>\n\n<p>How can I (significantly) improve the performance ?</p>\n"},{"tags":["performance","algorithm","data-structures","maximum"],"answer_count":2,"favorite_count":2,"up_vote_count":5,"down_vote_count":0,"view_count":232,"score":5,"question_id":8905525,"title":"Computing a moving maximum","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/12329073/find-the-min-number-in-all-contiguous-subarrays-of-size-l-of-a-array-of-size-n\">Find the min number in all contiguous subarrays of size l of a array of size n</a>  </p>\n</blockquote>\n\n\n\n<p>I have a (large) array of numeric data (size <code>N</code>) and would like to compute an array of running maximums with a fixed window size <code>w</code>.</p>\n\n<p>More directly, I can define a new array <code>out[k-w+1] = max{data[k-w+1,...,k]}</code> for <code>k &gt;= w-1</code> (this assumes 0-based arrays, as in C++).</p>\n\n<p>Is there a better way to do this than <code>N log(w)</code>?</p>\n\n<p>[I'm hoping there should be a linear one in <code>N</code> without dependence on <code>w</code>, like for moving average, but cannot find it. For <code>N log(w)</code> I think there is a way to manage with a sorted data structure which will do <code>insert()</code>, <code>delete()</code> and <code>extract_max()</code> altogether in <code>log(w)</code> or less on a structure of size <code>w</code> -- like a sorted binary tree, for example].</p>\n\n<p>Thank you very much.</p>\n"},{"tags":["java","performance","benchmarking"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":2,"view_count":89,"score":0,"question_id":12572565,"title":"2D array in Java","body":"<p>Which of the following is faster in Java? Is there any other way that is faster than any of these? </p>\n\n<pre><code>int[][] matrix = new int[50][50];\n\nfor (int k=0;k&lt;10;k++){\n// some calculations here before resetting the array to zero\n      for (int i = 0; i &lt;50; i++) {\n            for (int j = 0; j &lt;50; j++) {\n                matrix[i][j]=0;\n            }              \n        }\n}\n</code></pre>\n\n<p>Or this:</p>\n\n<pre><code>int[][] matrix = new int[50][50];\n\n    for (int k=0;k&lt;10;k++){\n// some calculations here before resetting the array to zero \n          matrix = new int[50][50];\n    }\n</code></pre>\n"},{"tags":["c#","performance","algorithm","ienumerable"],"answer_count":4,"favorite_count":4,"up_vote_count":5,"down_vote_count":0,"view_count":720,"score":5,"question_id":2828203,"title":"Grouping consecutive identical items: IEnumerable<T> to IEnumerable<IEnumerable<T>>","body":"<p>I've got an interresting problem: Given an <code>IEnumerable&lt;string&gt;</code>, is it possible to yield a sequence of <code>IEnumerable&lt;IEnumerable&lt;string&gt;&gt;</code> that groups identical adjacent strings in one pass?</p>\n\n<p>Let me explain.</p>\n\n<p><strong>1. Basic illustrative sample :</strong></p>\n\n<p>Considering the following <code>IEnumerable&lt;string&gt;</code> (pseudo representation): </p>\n\n<pre><code>{\"a\",\"b\",\"b\",\"b\",\"c\",\"c\",\"d\"}\n</code></pre>\n\n<p>How to get an <code>IEnumerable&lt;IEnumerable&lt;string&gt;&gt;</code> that would yield something of the form:</p>\n\n<pre><code>{ // IEnumerable&lt;IEnumerable&lt;string&gt;&gt;\n    {\"a\"},         // IEnumerable&lt;string&gt;\n    {\"b\",\"b\",\"b\"}, // IEnumerable&lt;string&gt;\n    {\"c\",\"c\"},     // IEnumerable&lt;string&gt;\n    {\"d\"}          // IEnumerable&lt;string&gt;\n}\n</code></pre>\n\n<p>The method prototype would be:</p>\n\n<pre><code>public IEnumerable&lt;IEnumerable&lt;string&gt;&gt; Group(IEnumerable&lt;string&gt; items)\n{\n    // todo\n}\n</code></pre>\n\n<p>But it could also be :</p>\n\n<pre><code>public void Group(IEnumerable&lt;string&gt; items, Action&lt;IEnumerable&lt;string&gt;&gt; action)\n{\n    // todo\n}\n</code></pre>\n\n<p>...where <code>action</code> would be called for each subsequence.</p>\n\n<p><strong>2. More complicated sample</strong></p>\n\n<p>Ok, the first sample is very simple, and only aims to make the high level intent clear.</p>\n\n<p>Now imagine we are dealing with <code>IEnumerable&lt;Anything&gt;</code>, where <code>Anything</code> is a type defined like this:</p>\n\n<pre><code>public class Anything\n{\n    public string Key {get;set;}\n    public double Value {get;set;}\n}\n</code></pre>\n\n<p>We now want to generate the subsequences based on the Key, (group every consecutive <code>Anything</code> that have the same key) to later use them in order to calculate the total value by group:</p>\n\n<pre><code>public void Compute(IEnumerable&lt;Anything&gt; items)\n{\n    Console.WriteLine(items.Sum(i=&gt;i.Value));\n}\n\n// then somewhere, assuming the Group method \n// that returns an IEnumerable&lt;IEnumerable&lt;Anything&gt;&gt; actually exists:\nforeach(var subsequence in Group(allItems))\n{\n    Compute(subsequence);\n}\n</code></pre>\n\n<p><strong>3. Important notes</strong></p>\n\n<ul>\n<li>Only <strong>one iteration</strong> over the original sequence</li>\n<li><strong>No intermediary collections</strong> allocations (we can assume millions of items in the original sequence, and millions consecutives items in each group)</li>\n<li>Keeping enumerators and <strong>defered execution</strong> behavior</li>\n<li>We can assume that resulting subsequences will be iterated only once, and will be iterated in order.</li>\n</ul>\n\n<p>Is it possible, and how would you write it?</p>\n"},{"tags":["android","performance","scrollview","imagebutton"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":48,"score":0,"question_id":12541225,"title":"Image button slow load time in android","body":"<p>I am making a scrollView with a bunch of image buttons. Unfortunately this view takes a very long time to load - so much so that the user would be likely to close the app assuming it is an error.</p>\n\n<p>Any ideas on how I can make this process faster?</p>\n\n<pre><code>&lt;ScrollView xmlns:android=\"http://schemas.android.com/apk/res/android\"\nxmlns:tools=\"http://schemas.android.com/tools\"\nandroid:layout_width=\"fill_parent\"\nandroid:layout_height=\"fill_parent\" &gt;\n\n&lt;TableLayout\n    android:layout_height=\"fill_parent\"\n    android:layout_width=\"fill_parent\"\n    &gt;\n    &lt;TableRow&gt; \n        &lt;ImageButton\n            android:id=\"@+id/ImageButtonCone\"\n            android:layout_width=\"160dp\"\n            android:layout_height=\"200dp\"\n            android:scaleType=\"fitCenter\"\n            android:background=\"@drawable/cone\" \n            /&gt;\n        &lt;ImageButton\n            android:id=\"@+id/ImageButtonCube\"\n            android:layout_width=\"160dp\"\n            android:layout_height=\"200dp\"\n            android:scaleType=\"fitCenter\"\n            android:background=\"@drawable/cube\" \n            /&gt;\n    &lt;/TableRow&gt;\n            &lt;TableRow&gt; \n        &lt;ImageButton\n            android:id=\"@+id/ImageButtonCylinder\"\n            android:layout_width=\"160dp\"\n            android:layout_height=\"200dp\"\n            android:scaleType=\"fitCenter\"\n            android:background=\"@drawable/cylinder\" \n            /&gt;\n        &lt;ImageButton\n            android:id=\"@+id/ImageButtonTrapezoidalprism\"\n            android:layout_width=\"160dp\"\n            android:layout_height=\"200dp\"\n            android:scaleType=\"fitCenter\"\n            android:background=\"@drawable/trapezoidal_prism\" \n            /&gt;\n</code></pre>\n\n<p>etc...</p>\n"},{"tags":["c#",".net","wpf","multithreading","performance"],"answer_count":3,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":173,"score":3,"question_id":11394314,"title":"Best way to use Thread in a complex/single window WPF application","body":"<p>I want to know what is the best solution to improve my WPF application.</p>\n\n<p>I know that WPF forces you to do all the UI work on the thread that created the UI. This is a major problem for me, because my UI is very huge and I only have 1 window, so I cannot split 1 thread per window.</p>\n\n<p>When I do pan and zoom, everything needs to be refresh every time, one of my CPU hit 100% usage until all is done.</p>\n\n<p>So I'll try to explain what I have in my application:</p>\n\n<ul>\n<li>1 window (size of the screen)</li>\n<li>Virtual panel, 12000 px X 12000 px (where I do pan and zoom)</li>\n<li>About 2000 controls (button, switch, and some other complex controls)</li>\n<li>Behind my controls, I have a big Bitmap for my background, but I split my background in  something like 100 smaller bitmap that I stack (because 1 bitmap of 12k pixels by 12k pixels crash my app)</li>\n<li>10 virtual displays, connected to a distant computer (who feed the bitmap) and refresh their content every 50 ms</li>\n<li>.NET 4.0, multitouch application</li>\n</ul>\n\n<p>So just my application where I can do pan and zoom (without the 10 displays), there is a lot of lag du to the amount of controls, and when I put the display, it double the lag...</p>\n\n<p>My application takes about 1.5 gb of virtual memory.</p>\n\n<p>I searched to use the dispatcher to makes threads when I can but I don't find what I want... everywhere they talked about 1 thread/window... but I only have 1 Window, don't know what to do.</p>\n\n<ul>\n<li>I cannot split into smaller window.</li>\n<li>I cannot reduce the amount of controls, or the refresh time.</li>\n<li>I cannot change technology (WPF)</li>\n</ul>\n\n<p>So here is the real question: <strong>Where can I create new threads to help my render time? To split the job to different CPU...</strong></p>\n\n<p>I found different website where they talked about this... but not answering my question:</p>\n\n<p><a href=\"http://eprystupa.wordpress.com/2008/07/28/running-wpf-application-with-multiple-ui-threads/\" rel=\"nofollow\">Running WPF Application with Multiple UI Threads</a></p>\n\n<p><a href=\"http://msdn.microsoft.com/en-us/library/ms741870.aspx\" rel=\"nofollow\">MSDN</a></p>\n\n<p><a href=\"http://www.switchonthecode.com/tutorials/working-with-the-wpf-dispatcher\" rel=\"nofollow\">Working With The WPF Dispatcher</a></p>\n"},{"tags":["c#","performance","openxml-sdk"],"answer_count":0,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":447,"score":0,"question_id":8989934,"title":"Simple OOXML PasteDataTable slow performance on large dataset","body":"<p>I'm using Simple OOXML to create a large spreadsheet with approximately 17,000~ rows. I'm using the PasteDataTable method to write the data to the spreadsheet and I am seeing very slow performance. I've used the StopWatch class to time how long the method takes and it's taking just under 2 minutes to complete. Is there anything I can do to speed this up? </p>\n\n<p>Here is the code I'm using, all the time is being chewed up by PasteDataTable.</p>\n\n<pre><code>private byte[] DataTableToExcel(DataTable dt, string templateName)\n{\n  MemoryStream memoryStream = new MemoryStream();\n\n  byte[] buffer = db.ExcelTemplates.FirstOrDefault(x =&gt; x.TemplateName == templateName).FileBytes.ToArray();\n\n  memoryStream.Write(buffer, 0, buffer.Length);\n\n  SpreadsheetDocument doc = SpreadsheetDocument.Open(memoryStream, true);\n  WorksheetPart worksheetPart = SpreadsheetReader.GetWorksheetPartByName(doc, \"Sheet1\");\n  WorksheetWriter writer = new WorksheetWriter(doc, worksheetPart);\n\n  List&lt;string&gt; columnNames = new List&lt;string&gt;();\n\n  foreach (DataColumn c in dt.Columns)\n  {\n    columnNames.Add(c.Caption);\n  }\n\n  writer.PasteValues(\"A1\", columnNames, DocumentFormat.OpenXml.Spreadsheet.CellValues.String);\n\n  Stopwatch watch = Stopwatch.StartNew();\n\n  // This method is taking approximately 00:01:47.8658627~ to complete\n  writer.PasteDataTable(dt, \"A2\");\n\n  watch.Stop();\n\n  Console.WriteLine(\"- PasteDataTable processing time: {0}\", watch.Elapsed);\n\n  SpreadsheetWriter.Save(doc);\n\n  return memoryStream.ToArray();\n}\n</code></pre>\n\n<p>EDIT #1:</p>\n\n<p>I've just tried EEPlus which has similar methods for bulk insertion and it's taking just about the same amount of time. So perhaps this isn't something that can't be easily fixed.</p>\n\n<p>EDIT #2:</p>\n\n<p>I thought I could shave off time by just using PasteValues but this was actually slower. I threw a timer on to time each PasteValues call and it slows down with each thousand rows pasted.  </p>\n\n<p>Here are some numbers:</p>\n\n<pre><code>row 1000  | 00:00:00.0005710\nrow 5000  | 00:00:00.0019151\nrow 10000 | 00:00:00.0098864\nrow 15000 | 00:00:00.0163237\nrow 17000 | 00:00:00.0183432\n</code></pre>\n"},{"tags":["performance","caching","simulation","isa"],"answer_count":0,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":129,"score":3,"question_id":9755060,"title":"L1 and L2 Miss Rates in SimpleScalar Simulation","body":"<p>I have simulated 4 different binaries in the SimpleScalar simulation tool and for every binary the L2 unified miss rate is greater than L1 data miss rate.</p>\n\n<p>In my assignment I am suppose to do some analysis. First thing that come to my mind is L2 miss rate should be smaller since it has higher level in hierarchy and more size than L1 cache. </p>\n\n<p>Besides, as far as I know, L2 is referenced only when there is a miss in L1 cache. From my point of view, L2 should have the data that L1 does not have most of the time so its miss rate should be less.</p>\n\n<p>However, results are not close to what I expected.</p>\n\n<p>For instance,</p>\n\n<ul>\n<li>L1 Data Miss Rate    : 0.0269</li>\n<li>L2 Unified Miss Rate : 0.0566</li>\n</ul>\n\n<p>The miss rate is determined as <code>misses / references</code> to cache.</p>\n\n<p>What is wrong with my approach? Why L2 miss rate is greater than L1? </p>\n"},{"tags":["javascript","performance","firefox"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":69,"score":-1,"question_id":12555591,"title":"A unary operator function expression is ~5261% faster than a brackets function expression in Firefox","body":"<p>In this <a href=\"http://jsperf.com/function-expressions\" rel=\"nofollow\">test</a>, I am looking at two different ways of function expressions; using the exclamation point unary operator and using brackets.</p>\n\n<p>The following are the 2 tests:</p>\n\n<pre><code>var f = !function() {};\n\nvar f = (function() {});\n</code></pre>\n\n<p>Given the results below, I was intrigued when I saw that the unary operator test was ~5261% faster than the brackets test in Firefox.</p>\n\n<p><img src=\"http://i.stack.imgur.com/YFcqh.png\" alt=\"enter image description here\"></p>\n\n<p><img src=\"http://i.stack.imgur.com/62awM.png\" alt=\"enter image description here\"></p>\n\n<p>What sort of optimization is going on that amounts to such a significant increase in performance in Firefox, dwarfing the other browsers?</p>\n"},{"tags":["css","performance"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":72,"score":3,"question_id":8198206,"title":"CSS - which code renders faster and is more efficient?","body":"<p>I would like to know which of below codes renders faster and is more efficient:</p>\n\n<p>This -</p>\n\n<pre><code>body{background:#ddd;font-family:verdana;font-size:12px;color:#808080;}\na{color:#808080;outline:0;text-decoration:none;}\ninput{margin:0;padding:0;font-family:verdana;color:#808080;}\nul{padding:0;margin:0;}\n</code></pre>\n\n<p>Or this -</p>\n\n<pre><code>body,input{font-family:verdana}\nbody,input,a{color:#808080;}\nbody{background:#ddd;;font-size:12px;}\na{outline:0;text-decoration:none;}\ninput,ul{margin:0;padding:0;}\n</code></pre>\n\n<p>Thanks in advanced,</p>\n\n<p>Din.</p>\n"},{"tags":["performance","c#-4.0","task-parallel-library","code-timing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":134,"score":0,"question_id":10522025,"title":"Timing of parallel actions using the Task Parallel Library in C#","body":"<p>I am running some experiments, timing them and comparing the times to find the best \"algorithm\". The question that came up was if running the tasks in parallel would make the relative runningtimes of the experiments wrong and if I would get more representative results by running them sequentially. Here is a (simplified) version of the code:</p>\n\n<pre><code>public static void RunExperient(IEnumerable&lt;Action&gt; experiments)\n    {\n        Parallel.ForEach(experiments, experiment =&gt;\n        {\n            var sw = Stopwatch.StartNew(); //line 1\n            experiment();                  //line 2   \n            sw.Stop();                     //line 3\n            Console.WriteLine(@\"Time was {0}\", sw.ElapsedMilliseconds);\n        });\n    }\n</code></pre>\n\n<p>My questions are about what is happening \"behind the scenes\":</p>\n\n<ol>\n<li><p>When a task has started, is it possible that the OS or the framework can suspend the task during its execution and continue on later making the running time of the experiment all wrong?</p></li>\n<li><p>Would I get more representative results by running the experiments sequentially?</p></li>\n</ol>\n"},{"tags":["asp.net","performance","iis","threadpool"],"answer_count":1,"favorite_count":3,"up_vote_count":3,"down_vote_count":0,"view_count":646,"score":3,"question_id":10455455,"title":"Server 2008 RC2, IIS 7.5, ASP.NET and Requests Queued Poor Performance","body":"<p>I already know the answer to this, but wanted to share with the community since it is NOT documented from Microsoft.</p>\n\n<p>The scenario: A surge of traffic hits your IIS 7.5 ASP.NET website, and you notice that requests start queuing up. The performance of the site slows to a crawl, yet you have plenty of CPU and RAM available.</p>\n\n<p>This is the problem we saw recently with a site that made a bunch of internal web-service calls. An internal health check would start timing-out, which would cause this server to drop out of our cluster. (However, this server is the most powerful hardware of the bunch ...)</p>\n"},{"tags":["asp.net","performance","http-compression"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":92,"score":1,"question_id":12564705,"title":"How to compress images in asp.net?","body":"<p>I am working in asp.net webapi. I have <code>30 to 40 images</code> in my resource folder. Is there anyway I could compress my images programatically instead of manually, resizing the images in resources?</p>\n"},{"tags":["java","performance","memory-efficient"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":26,"score":0,"question_id":12566686,"title":"Space efficient conversion from Wrapper to primitive and primitive to Wrapper","body":"<p>Given the following function:  </p>\n\n<pre><code>public void convertToWrapper(long[] longsToConvert)  \n{    \n\n}  \n</code></pre>\n\n<p>and  </p>\n\n<pre><code>public void convertToPrimitive(Long[] longsToConvert)  \n{  \n}  \n</code></pre>\n\n<p><a href=\"http://commons.apache.org/lang/api-2.4/org/apache/commons/lang/ArrayUtils.html#toObject%28long%5B%5D%29\" rel=\"nofollow\">Apache ArrayUtils</a> exposes the following:  </p>\n\n<pre><code>public Long[] toObject(long[] longs){\n    final Long [] result = new Long [array.length];\n        for (int i = 0; i &lt; array.length; i++) {\n             result[i] = new Long (array[i]);\n         }\n}\n</code></pre>\n\n<p>My question is there a way to do this utilizing only one array?  I have tried the following which does not work:  </p>\n\n<pre><code>for(int i =0;i&lt;array.length;i++)  \n{  \n     array[i] = new Long(array[i]);\n}  \n</code></pre>\n"},{"tags":["java","mysql","performance","jpa","jdbc"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":62,"score":1,"question_id":12557216,"title":"copy data from a mysql database to other mysql database with java","body":"<p>I have developed a small swing desktop application. This app needs data from other database, so for that I've created an small process using java that allows to get the info (using jdbc) from remote db and copy (using jpa) it to the local database, the problem is that this process take a lot of time. is there other  way to do it in order to make faster this task ? </p>\n\n<p>Please let me know if I am not clear, I'm not a native speaker.</p>\n\n<p>Thanks </p>\n\n<p>Diego</p>\n"},{"tags":["javascript","performance","prototype","object-literal"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":21,"score":1,"question_id":12566024,"title":"Javascript performance of accessing object's fields if object was prototyped and if it was created from a literal","body":"<p>I can create object using prototype, and the fields are set in the constructor, or I can create object using JSON. I'd expect that the prototyped version will be as fast as the literal, or faster, but it occurs that it's slower on chrome and ff, while on Opera both seem to be equal.</p>\n\n<p><a href=\"http://jsperf.com/object-literal-vs-object-prototype-field-access-time\" rel=\"nofollow\">http://jsperf.com/object-literal-vs-object-prototype-field-access-time</a></p>\n\n<p>Can someone explain it?</p>\n"},{"tags":["android","performance","graphics","ondraw"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":63,"score":0,"question_id":12565620,"title":"Android graphics: Low performance","body":"<p>I'm making a custom slide control. Is like a Volume wheel, so some values augment or decrease depending on rotate direction.\nI have two issues with this:</p>\n\n<ol>\n<li>The performance is really low</li>\n<li>The garbage collector is trigger, many many times.</li>\n</ol>\n\n<p>Well, I'm sure that i'm doing something wrong, so please give me a Light.</p>\n\n<p>Am working with Android graphics on 2.1 (eclaire) SDK.</p>\n\n<p>This is the code of the view that am calling from my activity:</p>\n\n<pre><code>public class DrawingView extends View {\n\nprivate Paint p;\nBitmap bitmap;\nContext mContext;\nCanvas canvas;\nprivate float sweepAngle;\nprivate int _height;\nprivate int _width;\nprivate float lastAngle;\nprivate int percent;\n\npublic DrawingView(Context context) {\n    super(context);\n    WindowManager wm = (WindowManager) context.getSystemService(Context.WINDOW_SERVICE);\n    Display display = wm.getDefaultDisplay();\n    this.set_Width(display.getWidth());\n    this.set_Height(display.getHeight());\n    this.setSweepAngle(10);\n    mContext = context;\n    p = new Paint();\n    p.setAntiAlias(true);\n\n}\n\nprotected int getAngleFromLocation(Point location){\n    int finalAngle = (int) (Math.atan2(location.y - 200, location.x - 200) * (180 / Math.PI));\n    return finalAngle;\n}\n\n@Override\nprotected void onDraw(Canvas canvas) {\n    super.onDraw(canvas);\n    bitmap = Bitmap.createBitmap(canvas.getWidth(), canvas.getHeight(),Bitmap.Config.ARGB_8888);\n    this.canvas = new Canvas(bitmap);\n    RectF rectF = new RectF();\n    rectF.set(20, 20, this.get_Widtt() - this.get_Widtt()/10, this.get_Widtt() - this.get_Widtt()/10);\n    canvas.drawArc(rectF, 180, this.getSweepAngle(), true, p);\n    //invalidate();\n}\n\n@Override\npublic boolean onTouchEvent(MotionEvent event) {\n    double increment = 3.6;\n    Point touchLocation = new Point();\n    touchLocation.x = (int)event.getX();\n    touchLocation.y = (int)event.getY();\n    canvas.drawBitmap(bitmap = Bitmap.createBitmap(canvas.getWidth(),canvas.getHeight(),Bitmap.Config.ARGB_8888),event.getX(), event.getY(),null );\n    switch (event.getAction())\n    { \n        case MotionEvent.ACTION_DOWN: \n            lastAngle = this.getAngleFromLocation(touchLocation);\n            System.out.println(\"ACTION_DOWN\");\n            break;\n\n        case MotionEvent.ACTION_MOVE:\n            System.out.println(\"ACTION_MOVE\");\n            int currentAngle = this.getAngleFromLocation(touchLocation);\n            System.out.println(\"CURRENT ANGLE: \" + currentAngle); \n            if (currentAngle &gt; lastAngle || (currentAngle == 1 &amp;&amp; lastAngle == 359)) {\n                percent += increment;\n            } else if (currentAngle &lt; lastAngle) {\n                percent -= increment;\n            }\n            if (percent &gt; 360) {\n                percent = 360;\n            } else if (percent &lt; 0) {\n                percent = 0;\n            }\n            lastAngle = currentAngle;\n             this.setSweepAngle(percent);\n             //Write the label\n             //int realPercent = percent*100/360;\n            System.out.println(\"PERCENT: \"+percent); \n            break;\n\n        case MotionEvent.ACTION_UP:\n            break;\n    }\n    return true;\n}\n}\n</code></pre>\n"},{"tags":["c","performance","strcmp"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":161,"score":0,"question_id":12563807,"title":"efficient and portable strcmp for reverse strings","body":"<p>strcmp, at least using g++, has many optimitzations for many architectures. In my pc, a Core2Duo E8400, strcmp is two times faster than use a straigforward implementation.</p>\n\n<p>My question if it exists some library that provides a function that compares two \"reverse strings\". A reverse string <code>char *s1</code> starts in <code>s1</code> and ends at some <code>s1-n</code> such that <code>s1-n == '\\0'</code> (where <code>n >= 0</code> and for all <code>0 &lt;= n' &lt; n, s1-n' != '\\0'</code>).</p>\n\n<p>Of course, the requirements are that this function must be so efficient and portable as strcmp.</p>\n\n<p>edit: I just need know if two strings are equals (so i do not need know which are greater. Then the same optimitzations for strcmp, in principle, will work fine for a reverse strings).</p>\n"},{"tags":["android","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":23,"score":0,"question_id":12565380,"title":"Remeasure all parents","body":"<p>In my newly created application there is a problem with the performance.</p>\n\n<p>When I inflate a quickly moving widget (<code>ViewFlipper</code>) the performance will decrease. (Lower fps)</p>\n\n<p>This is because all the parents are remeasuring thereselfs on each change in the <code>ViewFlipper</code>.\nThe <code>onMeasure</code> and the <code>onLayout</code> are called oftenly and I'm populating my views within the <code>onLayout</code>.</p>\n\n<p>My hierarchy is as followed:</p>\n\n<pre><code> RelativeLayout\n - ViewPager &lt;-- OnLayout takes time.\n - - ViewGroup\n - - - LinearLayout\n - - - - ViewFlipper\n</code></pre>\n\n<p>So my question is, how do I prefent the parents to be remeasured?</p>\n"},{"tags":["c++","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":588,"score":0,"question_id":8572258,"title":"To find Square root to a precision","body":"<p>I was just going through the questions various companies ask in interview. I found one was \"Find square root of a number to a precision. Function definition should be of something like this: <code>double getSquareRoot(int num, int precision)</code>\".</p>\n\n<p>I wrote one small function which gives the square root but doesn't care about precision:</p>\n\n<pre><code>double getSquareRoot(int num){\n int num1=0, num2=0;\n for(int i=1 ;; i++){\n   if(i*i == num){\n    std::cout&lt;&lt;i &lt;&lt;\" is the sq root\"&lt;&lt;std::endl;\n    break;\n   }\n  else if(i*i &gt; num){\n   num2 = i;\n   num1 = --i;\n   break;\n  }\n} \n // in the above for loop, i get the num1 and num2 where my input should lie \n // between them\n // in the 2nd loop below.. now i will do the same process but incrementing \n // by 0.005 each time\nfor(double i =num1;i&lt;(double)num2;i+=0.005)\n  {\n   if(i*i&gt;= num){\n     std::cout&lt;&lt;(double)i &lt;&lt;\" is the sq root\"&lt;&lt;std::endl;\n     break;\n   }\n }\n}\n</code></pre>\n\n<p>Now to reach to precision, i will have to do some tweaks like adding if loops and all. I don't like that. Could you guys help me here? If you are writing code, please explain. I would appreciate it.</p>\n\n<p>Thanks.</p>\n\n<p>This code is very insufficient and this doesn't take care of \"till this precision\" part of the problem. I only wrote it so that you guy's don't think that i tried a bit.\nThis </p>\n"},{"tags":["performance","ipad","retina-display","painting"],"answer_count":3,"favorite_count":3,"up_vote_count":7,"down_vote_count":0,"view_count":461,"score":7,"question_id":9972690,"title":"Webapp on iPad 3 retina display slow screen painting","body":"<p>I have a webapp for iPod touch and iPad. Works like a charm on iPad 1 and iPad 2. Now I have tested on a new iPad (iPad 3, iPad HD whatever name to be used) and I see that the painting of the screen is slow. I can see kind of blocks being painted one by one.</p>\n\n<p>Anyone knows how I can get the painting of the screen faster than iPad 1 and 2 instead of slower ?</p>\n\n<p>EDIT: Bounty expiry\nThe bounty has at least given some answers, although they are not sufficient to award the bounty. The question remains open.</p>\n"},{"tags":["mysql","performance"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":76,"score":1,"question_id":12562149,"title":"Slow mysql search query using exists","body":"<p>We have performance problems with a string search query (MySQL 5.5). Table main contains a few text columns. Table multi contains zero or more multivalue fields per main entry. We want to count how many main items that have one of the fields starting with a specific string.</p>\n\n<p>Here is the query:</p>\n\n<pre><code>select count(main_id) from main ma\nwhere s1 like '888%' or s2 like '888%' or s3 like '888%'\nor exists (select m.multivalue \n           from multi m \n           where ma.main_id=m.main_id and (m.multivalue like '888%'))  \n</code></pre>\n\n<p>Already at about a few hundred thousand records in main, and about the same number of records in multi, the query takes seconds to complete.</p>\n\n<p>Here is the explain:</p>\n\n<pre><code>PRIMARY ma  ALL s1,s2,s3                100407  100.00  Using where\nDEPENDENT SUBQUERY  m   ref PRIMARY,main_id,multivalue  PRIMARY 8   sample.ma.main_id   1   100.00  Using where; Using index\n</code></pre>\n\n<p>And the table definitions:</p>\n\n<pre><code>CREATE TABLE `main` (\n  `main_id` bigint(20) NOT NULL AUTO_INCREMENT,\n  `s1` varchar(50) CHARACTER SET utf8 NOT NULL,\n  `s2` varchar(50) CHARACTER SET utf8 DEFAULT NULL,\n  `s3` varchar(50) CHARACTER SET utf8 DEFAULT NULL,\n  PRIMARY KEY (`main_id`),\n  KEY `s1` (`s1`),\n  KEY `s2` (`s2`),\n  KEY `s3` (`s3`)\n) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COLLATE=utf8_bin;\n\nCREATE TABLE `multi` (\n  `main_id` bigint(20) NOT NULL,\n  `multivalue` varchar(50) CHARACTER SET utf8 NOT NULL,\n  PRIMARY KEY (`main_id`,`multivalue`),\n  KEY `main_id` (`main_id`),\n  KEY `multivalue` (`multivalue`),\n  CONSTRAINT `FK_multi_main` FOREIGN KEY (`main_id`) REFERENCES `main` (`main_id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin;\n</code></pre>\n\n<p>Any ideas how to make this query faster?</p>\n\n<p>innodb settings:</p>\n\n<pre><code>innodb_additional_mem_pool_size=32M\ninnodb_flush_log_at_trx_commit=1\ninnodb_log_buffer_size=64M\ninnodb_buffer_pool_size=1000M\ninnodb_log_file_size=128M\ninnodb_thread_concurrency=16\ninnodb_file_per_table\ninnodb_file_format=Barracuda\n</code></pre>\n"},{"tags":["performance","haskell"],"answer_count":5,"favorite_count":2,"up_vote_count":22,"down_vote_count":1,"view_count":2055,"score":21,"question_id":377082,"title":"How long does it take for you to be comfortable with Haskell?","body":"<p>I'm an OK C/C++ programmer. I find Haskell very intriguing. But it seems to me, that although it's relatively easy to write clean Haskell code, as it mimics math (which I'm very comfortable with) pretty well, it's very hard to write clean code in Haskell that runs fast.</p>\n\n<p>A faster version of quicksort of Haskell is very long and scary, which has no resemblance to the naive but short (two lines), clean and intuitive implementation. The long and scary version of Haskell is actually still much slower than the shorter and simpler C counter part.</p>\n\n<p>Is it because the current Haskell compiler is too dumb or is it just impossible for mortals (other than SJP of course) to write fast Haskell code?</p>\n"},{"tags":["c++","performance","for-loop","stdvector"],"answer_count":4,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":83,"score":3,"question_id":12560796,"title":"Should I iterate a vector by iterator or by access operator?","body":"<p>I have a vector declared as</p>\n\n<pre><code>std::vector&lt;int&gt; MyVector;\nMyVector.push_back(5);\nMyVector.push_back(6);\nMyVector.push_back(7);\n</code></pre>\n\n<p>How do should I use it in a for loop?</p>\n\n<p>By iterating it with an iterator?</p>\n\n<pre><code>for (std::vector&lt;int&gt;::iterator it=MyVector.begin(); it!=MyVector.end(); ++it)\n{\n    std::cout &lt;&lt; \"Vector element (*it): \" &lt;&lt; *it &lt;&lt; std::endl;\n}\n</code></pre>\n\n<p>Or by its access iterator?</p>\n\n<pre><code>for (std::vector&lt;int&gt;::size_type i=0; i&lt;MyVector.size(); i++)\n{\n    std::cout &lt;&lt; \"Vector element  (i) : \" &lt;&lt; MyVector.at(i) &lt;&lt; std::endl;\n}\n</code></pre>\n\n<p>In examples I found on internet both of them are used. Is one of them superior to the other under all conditions? If not, when should I prefer one of them over the other?</p>\n"},{"tags":["php","performance","codeigniter","hmvc","scalable"],"answer_count":2,"favorite_count":2,"up_vote_count":3,"down_vote_count":1,"view_count":1294,"score":2,"question_id":6313864,"title":"Codeigniter HMVC effect on performance","body":"<p>OK, so <a href=\"https://bitbucket.org/wiredesignz/codeigniter-modular-extensions-hmvc/wiki/Home\" rel=\"nofollow\">HMVC in Codeigniter</a> is the way to go for <a href=\"http://web.archive.org/web/20110713001121/http://techportal.ibuildings.com/2010/02/22/scaling-web-applications-with-hmvc/\" rel=\"nofollow\">scalable web applications (with Kohana 3)</a> based off many stackoverflow discussions such as <a href=\"http://stackoverflow.com/questions/2263416/what-is-the-hmvc-pattern/5736164#5736164\">HMVC patterns</a> and <a href=\"http://stackoverflow.com/questions/5454337/mvc-vs-hmvc-for-web-application-development\">MVC vs HMVC</a>.  </p>\n\n<p>But, how will using an HMVC approach affect performance when used in Codeigniter?  From my understanding, HMVC will \"simulate a controller\".  Assuming a one day \"large\" scale project, will Codeigniter's implementation by a future problem?</p>\n"},{"tags":["c++","performance","algorithm"],"answer_count":4,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":410,"score":0,"question_id":10965141,"title":"Removing duplicates from an array using std::map","body":"<p>I'm directly posting my code which I've written on <b>collabedit</b> under 5 minutes (including figuring out the algorithm) thus even though with the risk of completely made of fun in terms of efficiency I wanted to ask my fellow experienced stack overflow algorithm enthusiasts about the problem;</p>\n\n<p>Basically removing duplicate elements from an array. <b>My Approach: </b>Basically using the <b>std::map</b> as my hash table and for each element in duplicated array if the value has not been assigned add it to our new array. If assigned just skip. At the end return the unique array. Here is my code and the only thing I'm asking in terms of an interview question can my solution be more efficient?</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;map&gt;\n\nusing namespace std;\n\nvector&lt;int&gt;uniqueArr(int arr[],int size){\n    std::map&lt;int,int&gt;storedValues;\n    vector&lt;int&gt;uniqueArr;\n    for(int i=0;i&lt;size;i++){\n        if(storedValues[arr[i]]==0){\n            uniqueArr.push_back(arr[i]);\n            storedValues[arr[i]]=1;\n        }\n    }\n    return uniqueArr;  \n}\n\nint main()\n{   \n    const int size=10;\n    int arr[size]={1,2,2,4,2,5,6,5,7,1};\n    vector&lt;int&gt;uniArr=uniqueArr(arr,size);\n    cout&lt;&lt;\"Result: \";\n    for(int i=0;i&lt;uniArr.size();i++) cout&lt;&lt;uniArr[i]&lt;&lt;\" \";\n    cout&lt;&lt;endl;\n    return 0;\n}\n</code></pre>\n"},{"tags":["javascript","jquery","performance","jquery-ui","internet-explorer"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":85,"score":2,"question_id":12560288,"title":"how to detect visible (in view) element using javascript","body":"<p>I have a div (#diagram) larger than the actual visible area in my browser. #diagram contains up to 1500 \"boxes\" (other div elements) and I would like to know which of these 1500 boxes are actually really visible to the user so that I can populate them using ajax when they come into view. I actually do have some working code but that goes through all elements triggered by the onscroll() event. This works so so in Chrome but of course my client HAS to use the evil browser IE8 where the looping of all elements upon scrolling completely forces my application on its knees.</p>\n\n<p>Is there some sort of event that is fired when an element comes into view or any other option to detect \"true\" visibility?</p>\n\n<p>Environment: jQuery/jQueryUI (latest), Internet Explorer 8</p>\n\n<p>Thanks in advance :-)</p>\n\n<p>./cj</p>\n"},{"tags":["performance","graphics","data-structures","distance","point"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":43,"score":0,"question_id":12557747,"title":"Finding closest points to given coordinates - data structure","body":"<p>What is a good data structure to keep a collection of 2d points so that later I can call a method like collection.pointsCloserThenDistance(float d, float[] coordinates) efficiently? - This method would return a list in which every point's distance to the given coordinates is less or equal to d.</p>\n\n<p>(Also how would be the implementation of that method?)</p>\n\n<p>The simplest and probably not so good solution would be a standard array, and then compare every point with the given coordinates .This is O(n), n = number of points. But it might be possible to have O(m), m = number of points whose distance to given coordinates is less or equal to the given value.</p>\n"},{"tags":["c++","performance","opengl","graphics","binding"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":68,"score":1,"question_id":12559561,"title":"OpenGL bind when already bound","body":"<p>I'm writing an engine in OpenGL 3.2+. A renderable object has a mesh (a.k.a VAO) and a couple materials (a.k.a Shaders and Programs). All the renderables are stored in a render queue, which sorts the rendering process on materials and meshes in such a way that the least OpenGL bind calls are needed.</p>\n\n<p>However, sometimes it binds objects which are already bound, my question is: does OpenGL automatically detect when you're trying to bind an object which is already bound and does it skip all the expensive operations, or do you have to implement a system to detect if an object is bound already yourself?</p>\n\n<p>If OpenGL does not detect this, is it likely faster to fetch the object name through glGet* and compare against the object you're trying to bind or to just keep track of the currently bound object yourself?</p>\n"},{"tags":["java","performance","memory"],"answer_count":5,"favorite_count":0,"up_vote_count":0,"down_vote_count":4,"view_count":123,"score":-4,"question_id":12558767,"title":"Reducing memory consumption in Java","body":"<p>I have worked with some projects where achieving better performance and reducing memory consumption has been the primary goal. But believe me it has already been hard to control the memory consumption in Java.</p>\n\n<p>This <a href=\"http://en.wikipedia.org/wiki/Java_performance\" rel=\"nofollow\">wiki page</a>, says on \"<em>Java Program Speed</em>\"\n<em>Java is in some cases equal to C++ on low-level and numeric benchmarks.</em></p>\n\n<p><a href=\"http://shootout.alioth.debian.org/u64q/benchmark.php?test=all&amp;lang=java&amp;lang2=gcc\" rel=\"nofollow\">These statistics</a> do not really show a major difference in time computation Performance benchmark.\nHowever, the memory usage in Java is quite higher than C++, since there is an 8-byte overhead for each object and 12-byte for each array in Java (32-bit; twice as much in 64-bit java) as mentioned in above wiki link.</p>\n\n<p>Now the question is <strong><em>what all measures people take to minimize the memory utilization in Java?</em></strong></p>\n\n<p>Please note that I am much concerned about memory than performance, since I cannot think of any better ways than \"writing better programs\" and \"correctly tuning memory in JDK\"</p>\n"},{"tags":["performance","vba","excel-vba"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":120,"score":0,"question_id":11296434,"title":"Copy Chart & Print Very Slow VBA","body":"<p>I have a performance problem and google doesn't help me.\nThis person has the same problem :\n<a href=\"http://excel.bigresource.com/Copy-Chart-Print-Very-Slow-1eeSa883.html\" rel=\"nofollow\">http://excel.bigresource.com/Copy-Chart-Print-Very-Slow-1eeSa883.html</a> </p>\n\n<p>When I copy an excel chart with VBA : This is so slooooow.</p>\n\n<pre><code>  Dim myChart As ChartObject\n    For Each myChart In consoPDC.ChartObjects\n        myChart.Copy\n        ...\n    Next\n</code></pre>\n\n<p>Any idea that makes it faster ?\nFor information the objective is to paste them in powerpoint.</p>\n\n<p>The weird thing is that making it manually isn't slow at all</p>\n\n<p>Thanks.\nNico.</p>\n"},{"tags":["iphone","performance","ios6","iphone-4"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":156,"score":1,"question_id":12557915,"title":"iOS 6 is slow to paint view when using modalTransitionStyle = UIModalTransitionStyleFlipHorizontal","body":"<p>I recently upgraded my iPhone 4 to iOS 6. Upon doing so I noticed that one of the apps I'm working on is no longer behaving as expected when presenting a modal dialog. </p>\n\n<p>I am using UIModalTransitionStyleFlipHorizontal. </p>\n\n<p>The problem is that during the animation I see what I can only describe as intermediate paint states of the view. I momentarily see the view partially painted, then finally the entire view is drawn. I have never experienced this before iOS 6.</p>\n\n<p>The flow is as follows:</p>\n\n<p>ViewController decides it's time to present a modal dialog to the user:</p>\n\n<pre><code>    self.newController = [[NewViewController alloc] initWithNibName:@\"PaginatedView\" bundle:[NSBundle mainBundle]];\n    self.newController.modalTransitionStyle = UIModalTransitionStyleFlipHorizontal;\n\n    [self presentModalViewController:self.newController animated:YES];\n</code></pre>\n\n<p>NewViewController initializes views for pages: </p>\n\n<p>(called from NewViewController:initWithNibName:bundle:)</p>\n\n<pre><code>NSUInteger pages = 2;\nCGFloat cx = 0;\n\nfor (int i = 0; i &lt; pages ; i++) {\n    // Load the view from nib\n    NSArray *nibObjects = [[NSBundle mainBundle] loadNibNamed:@\"SomeView\" owner:nil options:nil];\n    SomeView*  someView = (SomeView*)[nibObjects objectAtIndex:0];\n    [someView initializeWithData:[_objects objectAtIndex:i]];\n    [someView setParentController:self];\n    [someView setBackButtonTarget:self withSelector:@selector(backButtonClick:)];\n\n    // Set the x origin for the view\n    CGRect viewRect = someView.frame;\n    viewRect.origin.x = ((self.scrollView.frame.size.width - viewRect.size.width) / 2) + cx;\n    viewRect.origin.y = ((self.scrollView.frame.size.height - viewRect.size.height) / 2);\n    someView.frame = viewRect;\n\n    [self.scrollView addSubview:someView];\n    cx += self.scrollView.frame.size.width;\n}\n\nself.pageControl.numberOfPages = pages;\n[self.scrollView setContentSize:CGSizeMake(cx, [scrollView bounds].size.height)];\n</code></pre>\n\n<p>The object SomeView is a view that displays a table to the user. When the animation is playing I momentarily see the table and its header empty, then finally the content comes in.</p>\n\n<p>Switching modalTransitionStyle to any other style resolves the issue, but I'd like to use UIModalTransitionStyleFlipHorizontal.</p>\n\n<p>Any ideas as to what is happening and how I can fix it? </p>\n\n<p>Also, I tried this on an iPhone 4S running iOS 6. No problems... Looks like iOS 6 made the iPhone 4 experience crappy... </p>\n"},{"tags":["asp.net","database","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":32,"score":1,"question_id":12556367,"title":"A fast way to store views on a page and when to save in database","body":"<p>I want to implement a views counter like most forums, Youtube and several others have. So every time a user reads an article, that is stored and remembered. I also want to know who looked at the article.</p>\n\n<p><strong>My queston is: How do you implement this efficiently? What is the best practice?</strong></p>\n\n<p>One way would be to call a stored procedure for every view, but that would result in <strong>a lot of unneeded calls</strong> to the database. </p>\n\n<p>Another way would be to store this to some global application object, and then store in DB every 5 minutes or so (and can you even do that in a good way?)</p>\n\n<p>What's the best way to do this? </p>\n"},{"tags":["mysql","performance","index","database-performance","b-tree-index"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":426,"score":0,"question_id":12555845,"title":"MySQL btree index still results in a full table lookup?","body":"<p>We have a MySQL DB (v 5.5) which powers a large website. Part of the website contains a forum with over 2,000,000 posts.</p>\n\n<p>The site has recently had a large increase in traffic (~700 -> 1,000 simultaneous connections), and this has resulted in some very slow queries on the site. After analysing the slow query log, we see a LOT of queries of this form:</p>\n\n<pre><code>-- Time: 120923 20:00:08\n-- User@Host: muselive_ldu[muselive_ldu] @ localhost []\n-- Query_time: 61.101385  Lock_time: 0.000050 Rows_sent: 2  Rows_examined: 346970\nSET timestamp=1348430408;\nSELECT fp_id FROM ldu_forum_posts WHERE fp_topicid='13731' ORDER BY fp_id ASC LIMIT 2;\n\n-- Time: 120923 20:01:09\n-- User@Host: muselive_ldu[muselive_ldu] @ localhost []\n-- Query_time: 376.077866  Lock_time: 72.060203 Rows_sent: 2  Rows_examined: 2214232\nSET timestamp=1348430469;\nSELECT fp_id FROM ldu_forum_posts WHERE fp_topicid='52526' ORDER BY fp_id ASC LIMIT 2;\n</code></pre>\n\n<p>I'm no MySQL expert, however the Rows_examined part is causing me a headache. We're doing a lookup on a table with ~2,000,000 rows, however we're querying by 'fp_id', which has a btree index applied to it. Despite this index existing, the Rows_examined count is fluctuating from ~300,000 to a full table scan. I should also mention we have an index on fp_topicsid too.</p>\n\n<p>Does anyone know why we might be seeing this? Would really appreciate some help on this :)</p>\n\n<p>Thanks!</p>\n"},{"tags":["java","performance","performance-testing"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":2,"view_count":167,"score":-1,"question_id":12555693,"title":"Testing Performance of a Java method","body":"<p>I have a Java program with a method that accepts 2 strings and checks if they are anagrams of one another.</p>\n\n<p>I believe there are more than one ways to determine if 2 strings are anagrams -</p>\n\n<p>logic - 1 : sort strings and check if they are equal\nlogic - 2 : check frequency of the strings</p>\n\n<p>and there are many other ways.</p>\n\n<p>End of the day, we can have multiple methods [each of them having different logic] taking 2 strings and checking if they are anagrams.</p>\n\n<p>What are different ways to check the performance of these methods and determine the optimistic one?</p>\n\n<p>Any ideas?</p>\n"},{"tags":["c++","c","performance","optimization"],"answer_count":4,"favorite_count":2,"up_vote_count":12,"down_vote_count":11,"view_count":368,"score":1,"question_id":12509298,"title":"Why can ++i ever be different from i+=1 performance-wise","body":"<p>Apparently after reading the old title that was</p>\n\n<blockquote>\n  <p>Why do questions like <code>is ++i fster than i+=1</code> even exist</p>\n</blockquote>\n\n<p>people didn't bother to read the question itself thoroughly. </p>\n\n<p>The question was <strong>not</strong> about people's reasons for asking that! It was about why would a compiler ever make a difference between <code>++i</code> and <code>i+=1</code>, and are there any possible scenarios where that would make sense. While I appreciate all your witty and profound comments, my question was not about it.</p>\n\n<hr>\n\n<p>Well, alright, let me try to put the question it in another way, I hope my English is good enough and I can express myself without being misunderstood this time, so <strong>please read it</strong>. Let's say someone read this in a 10-years-old book:</p>\n\n<blockquote>\n  <p>Using ++i over i=i+1 gives you a performance advantage.</p>\n</blockquote>\n\n<p>I'm not keen on this <em>particular</em> example, rather talking more or less generally.</p>\n\n<p>Obviously, when the author was writing the book, it made sense to him, he didn't just make it up. We know that modern compilers do not care about whether you use <code>++i</code>, <code>i+=1</code> or <code>i = i + 1</code>, the code will be optimized and we will have the same asm output. </p>\n\n<p>This seems quite logical: if two operations do the same thing, and have the same result, there is no reason to compile <code>++i</code> into one thing, and <code>i+=1</code> into another thing. </p>\n\n<p>But <em>since the book author wrote it</em>, he had seen the difference! It means that some compiler would actually produce different output for those two lines. It means that the guys that made the compiler had some reasons for treating <code>++i</code> and <code>i+=1</code> differently. <strong>My question is</strong> why would they ever do so?</p>\n\n<p>Is it just because it was hard/impossible to make compilers advanced enough to perform such optimizations those days? Or maybe on some very specific platforms/hardware/in some special scenario it actually makes sense to make a difference between <code>++i</code> and <code>i+=1</code> and other stuff of that kind? Or maybe it depends on the variable type? Or were the compiler devs just lazy?</p>\n"},{"tags":["javascript","performance","setinterval"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":67,"score":0,"question_id":12554531,"title":"setInterval performance","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/7648557/javascript-setinterval-behaviour-with-0-milliseconds\">Javascript: setInterval() behaviour with 0 milliseconds</a>  </p>\n</blockquote>\n\n\n\n<p>I simply want to hear, if it is a problem having a setInterval with time set to 0 milliseconds? Will there be any performance issues? And will it be better to have e.g. 1000 milliseconds?</p>\n\n<p>I have this script:</p>\n\n<pre><code>var countdown_id = setInterval(function() {\n    var now = (new Date()).getTime()-(2*60*60*1000);\n    ;\n    $('.product').each(function() {\n        var elm = $(this).attr('id');\n        var prod_id = $(\"#\"+elm+\" #hidden-\"+elm).val();\n\n\n        var expires = $('#date-end-' + prod_id).val()*1000;\n\n        var seconds_remaining = Math.round((expires - now)/1000);\n\n        var hours = FormatNumberLength(Math.floor(seconds_remaining/(60*60)), 2);\n        var sec_remaining = seconds_remaining-(hours*60*60);\n        var minutes = FormatNumberLength(Math.floor(sec_remaining/60), 2);\n        var sec_remaining2 = sec_remaining-(minutes*60);\n        var seconds = FormatNumberLength(sec_remaining2, 2);\n\n        var timestr = hours + \":\" + minutes + \":\"+seconds;\n\n        if (expires) {\n            if (seconds_remaining &gt; 0) {\n                $('#'+elm+' .time_left div').text(timestr);\n            }\n            else {\n                $(this).hide();\n            }\n        }\n        $('#'+elm+' .time_left div').text(timestr);\n    });\n}, 0);\n</code></pre>\n\n<p>Thanks in advance!</p>\n"},{"tags":["c#","performance","entity-framework","entity-framework-4"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":89,"score":2,"question_id":12532692,"title":"High lock contention and slow performance on ToList() with EF4","body":"<p>I've got a project that uses EF4 (System.Data.Entities, no nuget package) and hits the context directly for LINQ queries. Whenever I load-test by having multiple users log in (using VS test load testing), I get horrible performance, my CPU shoots to 100%, and VS.NET throws alerts on high .NET lock contention and high garbage collection.</p>\n\n<p>In doing a good deal of profiling and tweaking, everything seems to point to the execution of the LINQ query itself (to be expected to some degree), and an incredible amount of contention on the .ToList() call we make in a bunch of places on the results.</p>\n\n<p>Has anyone experienced this? What's the cause and how do I resolve this? Do I need to thread out the .ToList() call for some reason?</p>\n\n<p>UPDATE:\nA few people have asked for more details.. Here is the code in question (tweaked a bit to remove stuff I can't release).</p>\n\n<pre><code>var query =\n            from f in context.fs\n            where f.usr.Any((u) =&gt; u.id == id)\n            select new\n            {\n                FS = f,\n                f.fList,\n                E = from e in f.fList select new { e.er },\n                L = from l in f.fList select new { l.id }\n            };\n    var res = query.ToList();\n</code></pre>\n\n<p>Under heavy load this same code runs on multiple threads (profiler says 13). The ToList() call is absolute murder, and accounts for nearly all the delays.</p>\n"},{"tags":["mysql","performance","subquery"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":94,"score":1,"question_id":12548842,"title":"MySQL Slow Subquery","body":"<p>I have a support ticketing system. I am in the process of adding in a departments section, where users can be members of multiple departments.</p>\n\n<p>The issue is that I am using a subquery to get tickets with a department_id that is IN the department access table for that user.</p>\n\n<p>This is the subquery:</p>\n\n<pre><code>t.department_id IN (SELECT utd.department_id FROM users_to_departments utd WHERE utd.user_id = :department_or_assigned_or_user_id AND utd.site_id = :site_id)\n</code></pre>\n\n<p>This is slowing the query down. It is taking about 2 seconds for a table of 110,000 tickets. The Subquery is the cause.</p>\n\n<p>I tried converting it to a LEFT JOIN and then using HAVING utd.id IS NOT NULL, but the speed was worse.</p>\n\n<p>I was wondering if I could convert it to an inner join.</p>\n\n<p>The issue is that I always want to get tickets that are created by that user too, even if the ticket is now in a different department.</p>\n\n<p>Currently using the following after the subquery todo this:</p>\n\n<pre><code>OR (t.assigned_user_id = :department_or_assigned_or_user_id OR t.user_id = :department_or_assigned_or_user_id)\n</code></pre>\n\n<p>All the correct columns are indexed, so MySQL isn't doing any filesorts etc.</p>\n\n<p>The users_to_departments table is simply user_id and department_id.</p>\n\n<p>Any help would be awesome.</p>\n\n<p>Here is my complete query.</p>\n\n<pre><code>SELECT \nt.* , \nu.pushover_key AS `owner_pushover_key`, \nu.name AS `owner_name`, \nu.id AS `owner_id`, \nu.email AS `owner_email`, \nu.phone_number AS `owner_phone`, \nu.email_notifications AS `owner_email_notifications`, \nu2.pushover_key AS `assigned_pushover_key`, \nu2.name AS `assigned_name`, \nu2.id AS `assigned_id`, \nu2.email AS `assigned_email`, \nu2.email_notifications AS `assigned_email_notifications`, \nu3.name AS `submitted_name`, \nu3.id AS `submitted_id`, \nu3.email AS `submitted_email`, \nu3.email_notifications AS `submitted_email_notifications`, \ntp.name AS `priority_name`, \ntd.name AS `department_name`, \nts.name AS `status_name`, \nts.colour AS `status_colour`, \nts.active AS `active`, \npa.name AS `pop_account_name` \nFROM \ntickets t \nLEFT JOIN users u ON u.id = t.user_id \nLEFT JOIN users u2 ON u2.id = t.assigned_user_id \nLEFT JOIN users u3 ON u3.id = t.submitted_user_id \nLEFT JOIN ticket_priorities tp ON tp.id = t.priority_id \nLEFT JOIN ticket_departments td ON td.id = t.department_id \nLEFT JOIN ticket_status ts ON ts.id = t.state_id \nLEFT JOIN pop_accounts pa ON pa.id = t.pop_account_id \nWHERE   \n1 = 1 \nAND t.site_id = :site_id \nAND ( \n    t.department_id IN (SELECT utd.department_id FROM users_to_departments utd WHERE utd.user_id = :department_or_assigned_or_user_id AND utd.site_id = :site_id)\nOR \n    (t.assigned_user_id = :department_or_assigned_or_user_id OR t.user_id = :department_or_assigned_or_user_id)\n)\nORDER BY \nt.last_modified DESC \nLIMIT :limit OFFSET :offset\n</code></pre>\n\n<p>Here is a link to the MySQL Explain results:\n<a href=\"http://michaeldale.com.au/images/explain.html\" rel=\"nofollow\">http://michaeldale.com.au/images/explain.html</a></p>\n"},{"tags":["performance","scala","recursion","erlang","jvm"],"answer_count":2,"favorite_count":1,"up_vote_count":6,"down_vote_count":1,"view_count":429,"score":5,"question_id":12552483,"title":"erlang vs jvm (scala) recursion performance","body":"<p>As an exercise of learning scala and functional programming, I implemented the following non tail-recursive def that calculates the <a href=\"http://en.wikipedia.org/wiki/Pascal%27s_triangle\" rel=\"nofollow\">pascal's number</a> at any location. The program itself serves as the definition of pascal's triangle. It looks pictorially as follows</p>\n\n<pre><code>      1\n    1  1\n   1  2  1\n  1 3   3 1\n 1 4  6  4 1\n1 5 10 10 5 1\n...\n\ndef pascal(c: Int, r: Int): Int =\n  if (c == 0 || c == r) 1 else pascal(c - 1, r - 1) + pascal(c, r - 1)\n</code></pre>\n\n<p>However when trying to run for <code>pascal(25,50)</code> on Mac OS X 10.6.8 (2.53 GHz Intel Core 2 Duo) it still hasn't finished running after <strong>20 min</strong>.</p>\n\n<p>Just to compare with erlang, I installed R15B02 and wrote equivalent program as follows:</p>\n\n<pre><code>-module(pascal).\n-export([calc_pascal/2]).\n\ncalc_pascal(0,_) -&gt; 1;\ncalc_pascal(C,R) when C==R -&gt; 1;\ncalc_pascal(C,R) when C&lt;R  -&gt; calc_pascal(C-1,R-1) + calc_pascal(C-1,R).\n</code></pre>\n\n<p><code>pascal:calc_pascal(25,50)</code> finishes in <strong>~4sec</strong>.</p>\n\n<p>Why might be the reason for such a huge performance difference? Is jvm not as advanced as erlang runtime for recursive programs?</p>\n"},{"tags":["c++","performance","design-patterns","visual-c++","bridge-pattern"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":73,"score":0,"question_id":12552999,"title":"Having large abstract class in c++ is good or not?","body":"<p>I want to know creating an abstract class with multiple pure virtual methods in c++ is good or not?</p>\n\n<p>For example in some situations like implementing session initiation protocol's dialogs we found that it can be implemented using bridge pattern. But having abstract base class for common operations lead to have a very large abstract class. Now from performance view what is the effect of using and implementing such class?</p>\n\n<p>Consider we have different dialogs that each one have it's own implementation.\n(ex: InviteDialog,RegisterDialog,InfoDialog,...)</p>\n\n<p>Applying bridge pattern :</p>\n\n<pre><code>class IIDialog{\npublic:\n/*there are multiple pure virtual methodes at least 15*/\nvirtual int32_t SendResponse(ISipMessage* response) = 0;\nprotected:\n/*there are multiple methods that use pure virtual methods*/\nint32_t Send_Response(){retun SendResponse(response);}\n}\n\nclass IInviteDialog : public IIDialog\n{\n/*Implemet virtual methods*/\n}\n\nclass IRegisterationDialog : public IIDialog\n{\n/*implement virtual methode*/\n}\n</code></pre>\n\n<p><strong>other parts of implementation ignored(real implementations and abstracts for implementation that is used by above classes)</strong></p>\n\n<p>Thanks</p>\n"},{"tags":["c#","performance","backgroundworker"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":84,"score":0,"question_id":12373233,"title":"Why are BackgroundWorker's WorkerReportsProgress and WorkerSupportsCancellation's default values false?","body":"<p>Is there a performance penalty for using <code>BackgroundWorker</code>'s <code>WorkerReportsProgress</code> and <code>WorkerSupportsCancellation</code> set to <code>true</code>? Is there some other reason for them to be <code>false</code>?</p>\n\n<p><code>true</code> would be more logical because if you don't want to implement them - just don't. <code>true</code> won't harm you.</p>\n\n<p>I'm thinking of encapsulating <code>BackgroundWorker</code>s in a class (so I won't forget setting these properties to true, and for some other stuff). But since the default values are <code>false</code>, perhaps that indicates some disadvantage to them being <code>true</code>.</p>\n"},{"tags":["mysql","regex","performance","database-performance"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":24,"score":2,"question_id":12552665,"title":"Query to select var REGEX MANY columns","body":"<p>Here is what I was want to Optimize this query</p>\n\n<pre><code>SELECT * \nFROM users \nwhere `username` like \"%abc%\" \n   or `name`     like \"%abc%\" \n   or `email`    like \"%abc%\"\n</code></pre>\n\n<p>This search get result from users where any of <code>username</code> , <code>name</code> , <code>email</code> <code>like %abc%</code></p>\n\n<p>So I tried to use in with like <strong>But</strong></p>\n\n<p>I was looking for mysql Like in and I have found this answer <a href=\"http://stackoverflow.com/questions/1127088/mysql-like-in\"> stackoverflow :Mysql like in </a></p>\n\n<p>So the solution was <strong>regex</strong></p>\n\n<p>But I want to make the regex not use OR</p>\n\n<p>I want to be like this</p>\n\n<pre><code>SELECT * from users where abc REGEXP 'username|name|email';\n</code></pre>\n\n<p>So the answer </p>\n"},{"tags":["java","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":161,"score":1,"question_id":12552394,"title":"Does passing many parameters from constructor slows down the program?","body":"<p>I have been desigining classes in following way:</p>\n\n<pre><code>public class add{\n      private int firstEntry;\n      private int secondEntry;\n\n      public add(int a , int b){\n          this.firstEntry= a;\n          this.secondEntry=b\n      }\n\n      public int makeAddition(){\n          return firstEntry+secondEntry;\n      }\n\n}\n</code></pre>\n\n<p>Does this pattern of program slows down program in java?</p>\n"},{"tags":["mysql","performance","innodb","engine","myisam"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":45,"score":1,"question_id":12516888,"title":"MySQL storage engine for a large log table","body":"<p>(I've seen <a href=\"http://stackoverflow.com/questions/8349752/mysql-storage-engine-for-log-table\">this question</a> but it's not specific enough for what I want to ask.)</p>\n\n<p>I'm setting up a large-ish (a hundred GB or so) log table with an average record size of 100-200 bytes and several indexes (indices?).  Insertion rate will be about 100-200 records per second.  I will run analytical queries on this table, probably not all of them will hit a suitable index so they might run for a long time and look up a lot of data.</p>\n\n<ol>\n<li>What storage engine would you suggest?  (Basically MyISAM vs InnoDB.)</li>\n<li>If using MyISAM, will long queries block inserts?</li>\n<li>Table size is a concern (not a big one, but still).  In this respect, is one engine more efficient than the other?</li>\n<li>Performance-wise, how do they compare?</li>\n<li>Is there anything else I must be aware of in this situation?  I'm not new to databases but definitely not an expert.  Thanks for all the answers.</li>\n</ol>\n"},{"tags":["javascript","html","css","performance"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":4,"view_count":80,"score":-4,"question_id":12552196,"title":"How people write html code faster. Is writing html,css,js one by one is best flow?","body":"<p>I have checked the site <a href=\"http://thecodeplayer.com/\" rel=\"nofollow\">http://thecodeplayer.com/</a> and it's look like they write html then css then javascript.</p>\n\n<p>They are writing faster then I can thing ( how people do).</p>\n\n<p>Is this best for writing fast. Do someone have tried to do first html then stylesheet for them and then javascript.</p>\n"},{"tags":["java","android","performance","instance"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":31,"score":1,"question_id":12552009,"title":"Android caching instances","body":"<p>In performance critical <code>onDraw()</code> method of a <code>View</code>, would it be better to store the instance in a variable and reuse it later or should i just refer my instance as <code>this</code>?</p>\n\n<p>EX:</p>\n\n<pre><code>public class ScheduleListView extends ListView {\n\n    private Paint paint = new Paint();\n\n    public ScheduleListView(Context context, AttributeSet attrs) {\n        super(context, attrs);\n    }\n\n    @Override\n    protected void onDraw(Canvas canvas) {\n        paint.setColor(Color.GRAY);\n        paint.setAlpha(100);\n        paint.setColor(Color.parseColor(\"#47B3EA\"));\n        canvas.drawRect(this.getLeft(), 10, this.getRight(), 10, paint);\n        super.onDraw(canvas);\n        canvas.restore();\n    }\n\n}\n</code></pre>\n\n<p>the <code>this</code> is referred in <code>this.getRight()</code> and <code>this.getLeft()</code>, would i have any performance improvement if I do the following modification:</p>\n\n<pre><code>private ScheduleListView scheduleListView  = this;\n\n... ...\n\n@Override\nprotected void onDraw(Canvas canvas) {\n    ....\n    //refer to my instance using scheduleListView instead of this\n    scheduleListView.getLeft() \n    ....\n}\n</code></pre>\n\n<p>is this good practice? Will this have any performance gain? if not, what is a better way of doing this?</p>\n\n<p><strong>PS: I'm asking this because in jQuery(javascript), if I were to use an object multiple times, I would cache the object in a variable</strong>, don't really know would this be helpful in Java or not....</p>\n\n<p>EX:</p>\n\n<pre><code>var myDiv = $(\".myDiv\");\nmyDiv.html();//do stuff\n</code></pre>\n"},{"tags":["java","performance","hibernate","spring","startup"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":322,"score":1,"question_id":7818884,"title":"Speed up startup of Java enterprise web application","body":"<p>The project I am currently working on involves Tomcat servlet container hosting an application build of:</p>\n\n<ul>\n<li>Hibernate as an ORM framework</li>\n<li>Spring as middle tier</li>\n<li>XLST transformations for web tier</li>\n</ul>\n\n<p>Is there any way to speed mainly building hibernate entinty structers of Spring beans info or are there any common trick to speed up java enterprise web application startup?</p>\n"},{"tags":["android","performance","android-ndk","android-sensors"],"answer_count":1,"favorite_count":1,"up_vote_count":5,"down_vote_count":0,"view_count":421,"score":5,"question_id":9250121,"title":"Android: Improve sensor sampling rate by use of NDK and polling","body":"<p>I want to write an application that reads as many sensor vaulues (per time) as possible from different sensors (GPS, Acc, Gyro, Compass). So I have to investigate if there is an advantage in using NDK.</p>\n\n<p>Here are my questions:</p>\n\n<p>a) What is the bottleneck when reading sensor values from a sensor? Is it the senosr itself or Java? Can I increase the rate by using NDK? (I think for GPS the bottleneck is the sensor itself, but I've read that e.g. the Gyro-Sensor is quite fast)\nI found <a href=\"http://stackoverflow.com/questions/8989686/access-faster-polling-accelerometer-via-nativeactivity-ndk\">this thread</a> and it seems the bottleneck is the sensor. Can someone confirm this?</p>\n\n<p>b) Does polling instead of using EventListener increase the rate? What's the best way to read sensor values fast?</p>\n\n<p>c) Has the use of NDK any influence on the power consumption of the application? I didn't find anything about this.</p>\n\n<p>d) I'm new to Android. Is it much more afford to use NDK instead of normal Java? \nAccording to this <a href=\"http://developer.android.com/reference/android/app/NativeActivity.html\">sample-code</a> it seems to be straightforward to interact with the sensors using an event-queue, but how much afford is it to compile the code and to use it from the application?</p>\n\n<p>Thanks in advance.</p>\n"},{"tags":["c#","networking","performance","bandwidth","detect"],"answer_count":2,"favorite_count":18,"up_vote_count":24,"down_vote_count":0,"view_count":14295,"score":24,"question_id":566139,"title":"Detecting network connection speed and bandwidth usage in C#","body":"<p>Is there a way to detect the network speed and bandwidth usage in C#? Even pointers to open-source components are welcome. Thanks in advance.</p>\n"},{"tags":["database","performance","data","filesystems"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":88,"score":1,"question_id":12320085,"title":"Fastest way to store HTTP session data?","body":"<p>I'm looking for the fastest way to store &amp; read data associated with a HTTP session cookie.</p>\n\n<p>Right now, we have a directory full of files, the filename is the session cookie (random ~150 characters) and the contents is a binary blob, usually only a few bytes - or sometimes as much as 1 or 2 kilobytes. We also use the atime (last read timestamp) to find and delete old session data.</p>\n\n<p>This number of files in the directory can spike to millions, and the server is constantly checking if a file exists, what it's atime is, reading/writing them, and of course deleting/creating them. I suspect ext3 isn't the ideal approach to this usage pattern?</p>\n\n<p>What is the best way to store this kind of data? We tested MySQL but it is orders of magnitude slower than ext3 (I assume we didn't do anything wrong?). Even just establishing a connection takes longer than performing a typical filesystem exists/atime/fread cycle.</p>\n\n<p>Anyone with experience would be appreciated. What is the fastest way to manage a huge database of small unrelated pieces of data?</p>\n\n<p>We are using PHP, with CentOS on high end server hardware (almost the best money can buy). There is no need for clustering/load balancing, we are trying to reduce per-request latency on mid-to-low traffic websites. We are not using PHP's built in session API, because it doesn't work in our situation.</p>\n"},{"tags":["javascript","html","performance","content","render"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":36,"score":1,"question_id":12549953,"title":"Efficient way to render content in same conainter","body":"<p>Suppose, I have an HTML element,</p>\n\n<pre><code>&lt;div class = \"variable-content\"&gt; &lt;/div&gt;\n</code></pre>\n\n<p>I want to place some content inside this div according to some condition. Like there are two buttons, if user clicks on button_a then this div will have some content A and if user clicks on button_b then we display content B.</p>\n\n<p>There are two approaches :-</p>\n\n<ol>\n<li><p>Make two divs, one with content A and other with content B and hide one of them. When user clicks on button_a, hide Div with Content B and show Content A div. And do vice versa.</p></li>\n<li><p>When user clicks on button_a, render the content A in div and when user clicks on button_b then render the content B in same div.</p></li>\n</ol>\n\n<p>Advantage of approach1:- Only one time rendering, rest of the times only showing and hiding of divs are done.</p>\n\n<p>Advantage of apprach2:- Code becomes easier to manage. (i guess)</p>\n\n<p>I want to know which of the two approaches are better and efficient ? Is there any other method of doing this.</p>\n"},{"tags":["sql-server","performance","sql-server-2008","reporting-services","sql-server-2012"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":54,"score":1,"question_id":12512227,"title":"best practice: reportservers and sql servers","body":"<p>we have three SQL servers serving a variety of applications on different webservers.\nEach application is using reporting services functionality.\nThe average load per server per month is about 40.000 reports, taking an average of 3.1 secs to deliver a report.</p>\n\n<p>At this moment the the situation is as follows:</p>\n\n<ul>\n<li>Application A has his database on SQLServer A and is using Reporting\nServices on SQLServer A. (SQL 2008) </li>\n<li>Application B has his database on    SQLServer B and is using\nReporting Services on SQLServer B. (SQL 2008R2)</li>\n<li>Application C has his database on SQLServer C and is using<br>\nReporting Services on SQLServer C.  (SQL 2008R2).</li>\n</ul>\n\n<p>We have just bought a new server, runnning SQL 2012.\nWould it be wise to move all reporting to the Reporting Server 2012?\nMy idea is that there would be a significant performance-gain. Also, There would be only one reporting server to manage. But is that so? Are there penalties when running reports on one server while the database feeding the reports is on another server? Is it a problem if the Reporting Services version is different then that of the database server?</p>\n\n<p>I would like very much to hear your thoughts on this.\nPerformance and manageability are two key-components.</p>\n\n<p>Greetings and thanks for thinking with me,</p>\n\n<p>Henro</p>\n"},{"tags":["c#","performance","double","floating-point-precision"],"answer_count":1,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":82,"score":5,"question_id":12549000,"title":"Any equivalent of \"extended\" for C#?","body":"<p>I'm working on a new version of my <a href=\"http://www.nichesoftware.co.nz/nicheMANDELBROTsaver\" rel=\"nofollow\">Mandelbrot screensaver</a> and I'm running out of floating point accuracy - simple <a href=\"http://msdn.microsoft.com/en-us/library/678hzkk9%28v=vs.110%29.aspx\" rel=\"nofollow\">double</a> values don't have enough significant figures for my needs.</p>\n\n<p>More significant figures = greater levels of zooming into the fractal</p>\n\n<p>Back when I wrote a version of this screensaver in Delphi 7, I used the <a href=\"http://docwiki.embarcadero.com/RADStudio/en/Internal_Data_Formats#The_Extended_type\" rel=\"nofollow\">extended</a> floating point type, 80 bits in size.</p>\n\n<p>In .NET, I could switch to <a href=\"http://msdn.microsoft.com/en-us/library/system.decimal.aspx\" rel=\"nofollow\">decimal</a>, but the performance hit for this is terrible, slowing down fractal generation by a factor of 20 or so.</p>\n\n<p>Is there any equivalent of <strong>extended</strong> for .NET? Or, alternatively, are there any numeric types with higher precision than <strong>double</strong> that still use the FPU for evaluation and therefore don't have the high performance hit of <strong>decimal</strong>?</p>\n\n<p><strong>Update</strong></p>\n\n<p>My screensaver already manages to zoom into the fractal by many (many!) orders of magnitude; currently it resets to the base fractal only when the numeric type in use is unable to separate the ordinates for adjacent pixels. The extra 16 bits of precision from the double-extended improvement would give me close to 16 more doublings of size.</p>\n\n<p>As to performance, my algorithm already manages to eliminate 95-99% of the math required (as compared to a naive implementation that calculates many pixels), while retaining the integrity of the fractal. </p>\n"},{"tags":["c","linux","performance","performance-testing"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":52,"score":0,"question_id":12549288,"title":"What tool do you use to metric performance?","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/26663/whats-your-favorite-profiling-tool-for-c\">What&#39;s your favorite profiling tool (for C++)</a>  </p>\n</blockquote>\n\n\n\n<p>Instead of do it directly inside the C code, I want a some tool to do it for me. e.g, given some C code, it returns how long time it's was executed. Something like LinqPad and most client that given a SQL-query,it returns how time the query was executed in *conds.</p>\n"},{"tags":["css","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":52,"score":2,"question_id":12545929,"title":"Are there any performance differences between color names or hexa values in css?","body":"<p>Color properties in CSS can accept color names ( white, pink, etc) or hexadecimal values ( #FFF , #669966 , etc) or RGB. </p>\n\n<p>But not all the color names are standard for all browsers. There are tests like <a href=\"http://jsperf.com/css-color-names-vs-hex-codes/3\" rel=\"nofollow\">CSS color names vs hex codes</a>, (my results is better hexadecimal) , so is it always better to use hexadecimal than other two options? </p>\n\n<p>Edit: Other duplicate questions is about personal preferences, this is about performance.</p>\n"},{"tags":[".net","performance","stream","bytearray","filestream"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":74,"score":1,"question_id":12545533,"title":"Performance of FileStream's Write vs WriteByte on an IEnumerable<byte>","body":"<p>I need to write bytes of an <code>IEnumerable&lt;byte&gt;</code> to a file.<br>\nI can convert it to an array and use <code>Write(byte[])</code> method:</p>\n\n<pre><code>using (var stream = File.Create(path))\n    stream.Write(bytes.ToArray());\n</code></pre>\n\n<p>But since <code>IEnumerable</code> doesn't provide the collection's item count, using <code>ToArray</code> is not recommended <a href=\"http://stackoverflow.com/a/6750471/704144\" title=\"C#: ToArray performance - Stack Overflow\">unless it's absolutely necessary</a>.</p>\n\n<p>So I can just iterate the <code>IEnumerable</code> and use <code>WriteByte(byte)</code> in each iteration:</p>\n\n<pre><code>using (var stream = File.Create(path))\n    foreach (var b in bytes)\n        stream.WriteByte(b);\n</code></pre>\n\n<p>I wonder which one will be faster when writing lots of data.</p>\n\n<p>I guess using <code>Write(byte[])</code> sets the buffer according to the array size so it would be faster when it comes to arrays.</p>\n\n<p>My question is when I just have an <code>IEnumerable&lt;byte&gt;</code> that has MBs of data, which approach is better? Converting it to an array and call <code>Write(byte[])</code> or iterating it and call <code>WriteByte(byte)</code> for each?</p>\n"},{"tags":["php","iphone","performance","api","geolocation"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":29,"score":0,"question_id":12544509,"title":"live search city?","body":"<p>I'm working on a iPhone-application, and within one section the user should be able to search for cities. I would like to do it like the weather app is working on the iPhone, where user can start type and get a response right away with \"matching\" city names. For each city in the result I need to get; name of the city, county/state, country, longitude, and latitude.</p>\n\n<ol>\n<li><p>Are there any free API for this, that you can use in a commerical iphone-application (that might cost money)?</p></li>\n<li><p>What would be the best way to actually perform the live-search? I means of, to not put to much pressure on the server.. If the user types 2 characters, and the continue to type one more, and so on, then that will be a lot of requests to the server? (im thinking of having a \"proxy\" script on my server that calls the API for geo-info...</p></li>\n</ol>\n\n<p>Thanx</p>\n"},{"tags":["c#","performance","image","algorithm","math"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":66,"score":2,"question_id":12544768,"title":"Algorithm to determine best image saving format","body":"<p>I've got an image that could be a photograph or a screenshot. I want to make sure that the images get saved in an appropriate format. I am talking about jpeg and png at this point.</p>\n\n<p>How can I determine if it would be better to save an image as .jpeg because it is/contains a photo? Also, if the image contains a <strong>used</strong> alpha channel, png is obviously appropriated.</p>\n\n<p>The simplest way would be to save the image two times with different image formats into a memory stream and compare the corresponding data size. Obviously, this is very resource intensive and - in my opinion - not the way to go.</p>\n\n<p>Anybody got suggestions?</p>\n"},{"tags":[".net","performance","benchmarking"],"answer_count":3,"favorite_count":1,"up_vote_count":3,"down_vote_count":0,"view_count":110,"score":3,"question_id":3725070,"title":"tracking metric over multiple time buckets - known algo?","body":"<p>A situation has come up a few times in the last few weeks when I'd like to measure some event which might happen regulary (like the time taken to redraw a frame in a 2D smooth-scrolling UI) or variable freqency (like a message arriving to a webservice endpoint).  I've had and idea of measuring 1) 'normal' frequency, 2) current frequency, 3) min, 4) max.  And I'd like to measure these over multiple buckets of time.</p>\n\n<p>For example, a webservice could get 10 messages in 100 ms, then not get any messages for 5 minutes.  In the UI example it could be running at 60 FPS for 10 seconds straight, then a GC hits and a single frame could be 'frozen' for 1 second, which completely ruins the UI effect.</p>\n\n<p>I think these kinds of measurements could be done using a set of 'buckets' for collecting measurements.  But unlike <a href=\"http://en.wikipedia.org/wiki/Time_series\" rel=\"nofollow\">time series</a> the FPS measurement I care about most is the one that DOESN'T arrive at the normal interval (normal in the UI example is a single frame drawn every 1/60th of a sec, but the one I care about is 60x longer).  So to be useful both in the normal case and the exceptional case, one could use a hierarchy of 'sample buckets'.</p>\n\n<p>1..10 'micro' buckets, each measures 1/10th of a second<br>\nmany 'micro' buckets are needed to keep an accurate sliding window at the 'normal' level<br></p>\n\n<p>1..60 'normal' buckets, each 1 sec<br>\n1..60 'macro' buckets, each 1 min<br>\n... levels could continue: hours, days, months, years</p>\n\n<p>A set of metrics (avg, min, max, count) could be kept per bucket at each level.  When the time period for a bucket expires the bucket could be 'promoted' to the next level and combined into a 'sample queue' at that level.  This would give an accurate sliding-window measurement of each aggregate per bucket at each 'level' in the hierarchy, while using relatively little CPU or memory.</p>\n\n<p>In a development environment I think samples at the 'micro' level could be used to identify real-time problems while debugging.  In production the 'normal' level could be displayed to the end user while the 'macro' level could be stored for long-term trending and analysis (to establish a long-term baseline).  Once patterns are identified it seems like it would be easy to programatically log or react to significant changes in a metric (like the message rate drop-off to flush memory caches) without overreacting to acceptable anamolies (like the GC pause in the UI).</p>\n\n<p>I know this is a bit long, but it seems like a simple idea and I couldn't find any classes or frameworks that do this on the web (at least not in my framework of choice, .NET).  Is this a known pattern for measuring and evaluating the health of applications, systems, or measurements and I just couldn't find it?  Any monitoring library or statistical recipe available via open source or over-the-counter?</p>\n\n<p>P.S.  Because of the possible high rate of sampling I didn't think PerformanceCounters on Windows would be a good fit at the 'micro' level (updating metrics many times per second in some cases, like real-time display of UI FPS).  Also, it would be great if the solution worked on Mono and Silverlight (were PerfCounters aren't available).\nP.P.S.  I spent a couple of hours looking for statistics libraries in .NET, and found a couple, but couldn't find a simple 'hierarchical time-bounded sampling' like I describe above.  Lots of count bounded sampling, which doesn't apply here, because data streams like redraw rate and message arrival rate don't alway occur at regular intervals.</p>\n"},{"tags":["c#","performance","umbraco"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":164,"score":1,"question_id":12420707,"title":"Umbraco: differences between Node, DynamicNode, Content","body":"<p>Then there are many class that represents Umbraco documents:</p>\n\n<pre><code>1) umbraco.cms.businesslogic.Content\n2) umbraco.cms.businesslogic.web.Document\n3) umbraco.MacroEngines.DynamicNode\n4) umbraco.presentation.nodeFactory.Node\n</code></pre>\n\n<p>Are there any others?</p>\n\n<p>Can you explain what they do, and when to use them?</p>\n\n<p>The class number 3 and 4 seem the same. Perhaps it is better to use Node class because it is more faster?</p>\n\n<p>I am a teory:</p>\n\n<p><strong>umbraco.cms.businesslogic.Content</strong> and <strong>umbraco.cms.businesslogic.web.Document</strong> are the representation of <strong>cmsContent</strong> and <strong>cmsDocument</strong> DB tables.</p>\n\n<p><strong>umbraco.presentation.nodeFactory.Node</strong> and <strong>umbraco.MacroEngines.DynamicNode</strong> represents the node cached in XML file, to utilize into website.\nThe first is the simply Node, the second is the same Node with added dynamic properties, one for property defined in nodeType.\nSo, I think that <strong>Node</strong> is more fast of <strong>DynamicNode</strong></p>\n\n<p>There are someone that can confirm it?</p>\n\n<p>Thanks!</p>\n"},{"tags":[".net","performance","architecture","software-engineering"],"answer_count":2,"favorite_count":1,"up_vote_count":3,"down_vote_count":1,"view_count":150,"score":2,"question_id":12544006,"title":"Generating unique ID's ( check or not )?","body":"<p>Considering youtube video url (for example): </p>\n\n<p>e.g. : </p>\n\n<pre><code>http://www.youtube.com/watch?v=-JVkaMqD5mI&amp;feature=related\n</code></pre>\n\n<p>I'm talking about the <code>-JVkaMqD5mI</code> part. ( length=11)</p>\n\n<p>lets calc the options : </p>\n\n<pre><code>a-z = 26     |\nA-Z = 26     |_______ &gt;    26+26+10+2 = 64 optional chars in 11 places  = 64^11 = 73786976294838206464\n0-9 = 10     |\n-_ = 2       |\n</code></pre>\n\n<p>Im still wondering , when they generate a new ID for a new video , do they still check if <em>already exists</em> ?</p>\n\n<p>Im sure they have some list( db or cache) of the \"<em>already generated ID's</em>\" ... ( and if they do , do they aquire the db each time ? or in cache ? or...?)</p>\n\n<p>Or  do they rely on the <code>1.355252...e-20</code> chances which is almost <code>0</code>.( but still !=0)</p>\n\n<p>What is the best practice solutions for this kind of situations ?</p>\n"},{"tags":["performance","curl","web","delay"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":77,"score":0,"question_id":12536567,"title":"performance test using curl - delays","body":"<p>Can somebody help in analysis of the below output received when testing perfomance of certain website? Especially those lines representing progress meter</p>\n\n<p>It appears to me <code>curl</code> re-tries downloading the content of the page a couple of times.. am I right ?</p>\n\n<p>What would be the possible causes - would it be the malformed Content-Lenght response header ?</p>\n\n<hr>\n\n<pre><code>About to connect() to xx.example.com port 80 (#0)\nTrying 12.12.12.12... connected\nConnected to xx.example.com (12.12.12.12) port 80 (#0)\nGET /testing/page HTTP/1.1\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1)\n            Gecko/20100101 Firefox/4.0.1\nHost: mp.example.com\nAccept-Encoding: deflate, gzip\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  HTTP/1.1 200 OK\n\n Age: 26        \n Date: Thu, 21 Sept 2012 15:19:48 GMT\n Cache-Control: max-age=60        \n Xontent-Length:           \n Connection: Close     \n Via: proxy\n ETag: \"KNANXUSNFMBN\"\n Content-Type: application/json;charset=UTF-8\n Vary: Accept-Encoding\n\n[data not shown]\n</code></pre>\n\n<p><strong></p>\n\n<pre><code>100 32074    0 32074    0     0  54987      0 --:--:-- --:--:-- --:--:-- 55300\n100 49400    0 49400    0     0  28372      0 --:--:--  0:00:01 --:--:-- 28423\n100 52121    0 52121    0     0  20174      0 --:--:--  0:00:02 --:--:-- 20201\n100 58912    0 58912    0     0  16923      0 --:--:--  0:00:03 --:--:-- 16938\n</code></pre>\n\n<p></strong></p>\n\n<pre><code>100 58912    0 58912    0     0  13142      0 --:--:--  0:00:04 --:--:-- 13152\n100 58912    0 58912    0     0  10742      0 --:--:--  0:00:05 --:--:--  5476\n100 58912    0 58912    0     0   9083      0 --:--:--  0:00:06 --:--:--  2004\n100 58912    0 58912    0     0   7868      0 --:--:--  0:00:07 --:--:--  1384\n100 58912    0 58912    0     0   6940      0 --:--:--  0:00:08 --:--:--     0\n100 58912    0 58912    0     0   6207      0 --:--:--  0:00:09 --:--:--     0\n100 58912    0 58912    0     0   5615      0 --:--:--  0:00:10 --:--:--     0\n100 58912    0 58912    0     0   5125      0 --:--:--  0:00:11 --:--:--     0\n100 58912    0 58912    0     0   4715      0 --:--:--  0:00:12 --:--:--     0\n100 58912    0 58912    0     0   4365      0 --:--:--  0:00:13 --:--:--     0\n100 58912    0 58912    0     0   4063      0 --:--:--  0:00:14 --:--:--     0\n100 58912    0 58912    0     0   3801      0 --:--:--  0:00:15 --:--:--     0\n100 58912    0 58912    0     0   3570      0 --:--:--  0:00:16 --:--:--     0\n100 58912    0 58912    0     0   3366      0 --:--:--  0:00:17 --:--:--     0\n100 58913    0 58913    0     0   3226      0 --:--:--  0:00:18 --:--:--     0\n100  113k    0  113k    0     0   6067      0 --:--:--  0:00:19 --:--:-- 12387*\nClosing connection #0\n\nEND - total_time: 19.094\n(cumul_times - dns: 0.002 connect: 0.004 pretrans: 0.004 firstbyte: 0.006)\nstatus: 200 size: 115856 hsize: 269 date: 16.08.2012-18:20:33 1345130433\n</code></pre>\n\n<hr>\n\n<p>I would appreciate all input on this.</p>\n\n<p>I am troubleshooting the delays to that specific web page, I am looking for advise on how to interpret those curl progress meter lines.\nin working scenario - where there is no delay - there is 1 progress meter line :</p>\n\n<pre><code>Age: 28        \nDate: Thu, 21 Sep Aug 2012 15:20:46 GMT\nCache-Control: max-age=60        \nContent-Length: 115856    \nConnection: Keep-Alive\nVia: proxy\nETag: \"KXNFGAHSKCUY\"\nContent-Type: application/json;charset=UTF-8\nVary: Accept-Encoding\n[data not shown]\n\n100  113k  100  113k    0     0  6402k      0 --:--:-- --:--:-- --:--:-- 8703k*\nConnection #0 to host xx.example.com left intact\n\nClosing connection #0\nEND - **total_time: 0.018**\n(cumul_times - dns: 0.002 connect: 0.004 pretrans: 0.004 firstbyte: 0.006)\nstatus: 200 size: 115856 hsize: 269 date: 16.08.2012-18:21:14 1345130474\n</code></pre>\n\n<p>My question is what do the individual lines mean? That the curl got only part of the content and has been re-trying and re-trying.\nAnd what could be the cause? Slow server? Drops on the WAN connection .....?</p>\n"},{"tags":["c#",".net","performance","optimization","jit"],"answer_count":2,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":147,"score":5,"question_id":12542802,"title":"Why Math.Pow(x, 2) not optimized to x * x neither compiler nor JIT?","body":"<p>I've encountered with such not optimal code in several open source projects, when programmers do not think what they use.</p>\n\n<p>There is up to 10 times performance difference between two cases, because of Math.Pow use Exp and Ln functions in internal, how it is explained <a href=\"http://stackoverflow.com/questions/8870442/how-is-math-pow-implemented-in-net-framework/8870593#8870593\">in this answer</a>.</p>\n\n<p>The usual multiplication is better than powering in most cases (with small powers), but the best, off course, is <a href=\"http://en.wikipedia.org/wiki/Exponentiation_by_squaring\">Exponentation by squaring algorithm</a>.</p>\n\n<p>Thus, i think that compiler or JIT must permorm such optimization with powers and other functions. Why is it still not introduced? And am i right?</p>\n"},{"tags":["mysql","sql","performance","join","count"],"answer_count":3,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":131,"score":1,"question_id":12446704,"title":"sql count results query with joins perfomance","body":"<p>I have the following tables (example)</p>\n\n<pre><code>t1 (20.000 rows, 60 columns, primary key t1_id)\nt2 (40.000 rows, 8 columns, primary key t2_id)\nt3 (50.000 rows, 3 columns, primary key t3_id)\nt4 (30.000 rows, 4 columns, primary key t4_id)\n</code></pre>\n\n<p>sql query:</p>\n\n<pre><code>SELECT COUNT(*) AS count FROM (t1)\nJOIN t2 ON t1.t2_id = t2.t2_id\nJOIN t3 ON t2.t3_id = t3.t3_id\nJOIN t4 ON t3.t4_id = t4.t4_id\n</code></pre>\n\n<p>I have created indexes on columns that affect the join (e.g on <code>t1.t2_id</code>) and foreign keys where necessary. The query is slow (600 ms) and if I put where clauses (e.g. <code>WHERE t1.column10 = 1</code>, where <code>column10</code> doesn't have index), the query becomes much slower. The queries I do with <code>select (*)</code> and <code>LIMIT</code> are fast, and I can't understand count behaviour. Any solution?</p>\n\n<p><strong>EDIT:</strong> EXPLAIN SQL ADDED</p>\n\n<pre><code>id  select_type     table   type    possible_keys   key     key_len     ref  rows   Extra\n1   SIMPLE          t4      index   PRIMARY     user_id     4           NULL  5259  Using index\n1   SIMPLE          t2      ref     PRIMARY,t4_id   t4_id   4        t4.t4_id   1   Using index\n1   SIMPLE          t1      ref     t2_id         t2_id     4        t2.t2_id   1   Using index\n1   SIMPLE          t3      ref     PRIMARY     PRIMARY     4        t2.t2_id   1   Using index\n</code></pre>\n\n<p>where user_id is a column of t4 table</p>\n\n<p><strong>EDIT:</strong> I changed from innodb to myisam and i had a speed increase, especially if i put where clauses. But i h still have times (100-150 ms) The reason i want count in my application, is to the the user who is processing a search form, the number of results he is expecting with ajax. May be there is a better solution in this, for example creating a temporary table, that is updated every one hour?</p>\n"},{"tags":["performance","sdl","double-buffering"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":121,"score":0,"question_id":8776412,"title":"What is the precise timing and effect of SDL_UpdateRect on a double-buffered screen?","body":"<p>Using a double-buffered screen with the <a href=\"http://www.libsdl.org/\" rel=\"nofollow\">Simple DirectMedia Layer</a>, is it more efficient to call <a href=\"http://www.libsdl.org/docs/html/sdlupdaterect.html\" rel=\"nofollow\"><code>SDL_UpdateRect</code></a> once after blitting multiple images or to call it once after blitting each individual image before calling <a href=\"http://www.libsdl.org/docs/html/sdlflip.html\" rel=\"nofollow\"><code>SDL_Flip</code></a>? In other words, will <a href=\"http://www.libsdl.org/docs/html/sdlupdaterect.html\" rel=\"nofollow\"><code>SDL_UpdateRect</code></a> cause the screen to be updated <em>immediately,</em> or does it simply tell the Simple DirectMedia Layer which areas must be updated when the screen is flipped? How should it typically be used with a double-buffered screen?</p>\n\n<p>For reference, here is the description of <a href=\"http://www.libsdl.org/docs/html/sdlupdaterect.html\" rel=\"nofollow\"><code>SDL_UpdateRect</code></a>.</p>\n\n<blockquote>\n  <p>Makes sure the given area is updated on the given screen. The\n  rectangle must be confined within the screen boundaries (no clipping\n  is done).</p>\n  \n  <p>If '<em>x</em>', '<em>y</em>', '<em>w</em>' and '<em>h</em>' are all 0, <code>SDL_UpdateRect</code> will update the\n  entire screen.</p>\n</blockquote>\n"},{"tags":["java","python","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":153,"score":1,"question_id":12542197,"title":"Why so much runtime difference for same algorithm?","body":"<p>I did the interview street problem <a href=\"https://www.interviewstreet.com/challenges/dashboard/#problem/4edb8abd7cacd\" rel=\"nofollow\">string similarity</a>. initially I did this in python.This gave me Time Limit Exceeded error for last 5 test cases. Then I tried the same one in java and the solution got accepted. The time difference between java and python versions for last 5 test cases were very high buts python beats java for first 5 test cases. Why is that so?</p>\n\n<p>The length of string can go upto 100000.</p>\n\n<p>stringsim.py</p>\n\n<pre><code>N=int(raw_input())\nwhile N!=0:\n    rootstr=[i for i in raw_input()]\n    solution=0\n    for i in xrange(len(rootstr)):\n        for j in xrange(i,len(rootstr)):\n            if(rootstr[j-i]==rootstr[j]):solution+=1\n            else:break\n    print solution\n    N-=1\n</code></pre>\n\n<p>Solution.java:</p>\n\n<pre><code>class Solution{\n    public static void main(String[] args) {\n    java.util.Scanner sc=new java.util.Scanner(System.in);\n    int N=sc.nextInt(),sol;\n    while(N--!=0){\n        sol=0;\n        char[] s=sc.next().toCharArray();\n        for(int i=0;i&lt;s.length;i++){\n            for(int j=i;j&lt;s.length;j++){\n                if(s[j]==s[j-i]) sol++;\n                else break;\n            }\n        }\n        System.out.println(sol);\n    }\n    }\n}\n</code></pre>\n\n<pre>\nRun time for java:\n1               Success 0.172387\n2       Success 0.172177\n3       Success 0.172185\n4       Success 0.172178\n5       Success 0.263904\n6       Success 2.82661\n7       Success 4.66869\n8       Success 4.83201\n9       Success 1.36585\n10      Success 1.02123\n\nFor python:\n1       Success                 0.081229\n2       Success             0.081047\n3       Success             0.081032\n4       Success             0.081015\n5       Success                 0.910672\n6       Time limit exceeded.    16.1818\n7       Time limit exceeded.    16.2357\n8       Time limit exceeded.    16.2001\n9       Time limit exceeded.    16.2408\n10      Time limit exceeded.    16.1831\n</pre>\n"}]}
{"total":25593,"page":20,"pagesize":100,"questions":[{"tags":["c++","performance","floating-point","floating-accuracy","floating-point-precision"],"answer_count":4,"favorite_count":0,"up_vote_count":7,"down_vote_count":1,"view_count":146,"score":6,"question_id":12535740,"title":"Is it always necessary to  use float literals when performing arithmetic on float variables in C++?","body":"<p>I see a lot of C++ code that has lines like:</p>\n\n<pre><code>float a = 2;\nfloat x = a + 1.0f;\nfloat b = 3.0f * a;\nfloat c = 2.0f * (1.0f - a);\n</code></pre>\n\n<p>Are these <code>.0f</code> after these literals really necessary? Would you lose numeric accuracy if you omit these?</p>\n\n<p>I thought you only need them if you have a line like this:</p>\n\n<pre><code>float a = 10;\nfloat x = 1 / a;\n</code></pre>\n\n<p>where you should use <code>1.0f</code>, right?</p>\n"},{"tags":["c++","c","arrays","performance","optimization"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":102,"score":0,"question_id":12541987,"title":"optimize array with only 3 distinct values? How to design?","body":"<p>I am trying a resource insensitive functionality in C++. I am implementing an array which has 10000 records but any record will have only possible 3 values i.e. 0,1,2. So i was wondering  instead of storing memory for 10000 instance all 3 together if some how I can just save one instance of each and manage logically. Not sure how exactly to implemenet.</p>\n\n<p>For example my array would be something like this.</p>\n\n<p>{1, 0, 0, 1, 2, 1, 1, 1, 0, 2, 2, 0, 0, 0, 2,.............}</p>\n\n<p>we might go for more than 10000 records too  </p>\n"},{"tags":["java","string","optimization","memory","performance"],"answer_count":16,"favorite_count":19,"up_vote_count":23,"down_vote_count":0,"view_count":9690,"score":23,"question_id":231051,"title":"Is there a memory-efficient replacement of java.lang.String?","body":"<p>After reading <a href=\"http://www.javaworld.com/javaworld/javatips/jw-javatip130.html?page=2\" rel=\"nofollow\">this old article</a> measuring the memory consumption of several object types, I was amazed to see how much memory <code>String</code>s use in Java:</p>\n\n<pre><code>length: 0, {class java.lang.String} size = 40 bytes\nlength: 7, {class java.lang.String} size = 56 bytes\n</code></pre>\n\n<p>While the article has some tips to minimize this, I did not find them entirely satisfying. It seems to be wasteful to use <code>char[]</code> for storing the data. The obvious improvement for most western languages would be to use <code>byte[]</code> and an encoding like UTF-8 instead, as you only need a single byte to store the most frequent characters then instead of two bytes.</p>\n\n<p>Of course one could use <code>String.getBytes(\"UTF-8\")</code> and <code>new String(bytes, \"UTF-8\")</code>. Even the overhead of the String instance itself would be gone. But then there you lose very handy methods like <code>equals()</code>, <code>hashCode()</code>, <code>length()</code>, ...</p>\n\n<p>Sun has a <a href=\"http://www.freepatentsonline.com/6751790.html\" rel=\"nofollow\">patent</a> on <code>byte[]</code> representation of Strings, as far as I can tell. </p>\n\n<blockquote>\n  <p><strong>Frameworks for efficient representation of string objects in Java programming environments</strong><br>\n  ... The techniques can be implemented to create Java string objects as arrays of one-byte characters when it is appropriate ...</p>\n</blockquote>\n\n<p>But I failed to find an API for that patent.</p>\n\n<p>Why do I care?<br>\nIn most cases I don't. But I worked on applications with huge caches, containing lots of Strings, which would have benefitted from using the memory more efficiently.</p>\n\n<p>Does anybody know of such an API? Or is there another way to keep your memory footprint for Strings small, even at the cost of CPU performance or uglier API?</p>\n\n<p>Please don't repeat the suggestions from the above article:</p>\n\n<ul>\n<li>own variant of <code>String.intern()</code> (possibly with <code>SoftReferences</code>)</li>\n<li>storing a single <code>char[]</code> and exploiting the current <code>String.subString(.)</code> implementation to avoid data copying (nasty ;)</li>\n</ul>\n\n<p><strong>Update</strong></p>\n\n<p>I ran the code from the article on Sun's current JVM (1.6.0_10). It yielded the same results as in 2002. </p>\n"},{"tags":["java","performance","function","pow"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":82,"score":2,"question_id":12541410,"title":"Fast Function resembling a^b","body":"<p>This is kind of obscure, but I need a function that can be computed very quickly and resembles  a^b where a is between 0 and 1 and b is very large. It will be calculated for one a at a time for many b's. Ideally, the result would be within 0.4%. Thanks in advance.</p>\n"},{"tags":["php","performance","xpath","curl"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":53,"score":1,"question_id":12537160,"title":"is there something I can do to HTML pages prior to XPath?","body":"<p>Is there something I can do to cURL acquired HTML to allow for faster or more efficient XPath queries on the DOM? Is it more efficient to reference starting from html/../.. or by [@??]. I've heard of 'scrubbing' the HTML but find no definitive methods for that.</p>\n\n<p>Anyone?</p>\n"},{"tags":["android","performance","view"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":46,"score":1,"question_id":12536892,"title":"Speed up dynamic view creation for android","body":"<p>I have created an android app that works as designed. However, onCreate() uses dynamic(not xml) to create 80% of my layout,which are consist of buttons. It takes about 3-4 second to build the entire layout, which is too slow. Is there any suggestions to speed up this process?</p>\n\n<p>Things I tried:\n1. using fewer view objects, it has not improved the speed and using listview and linearview had very minimal gains\n2. I tried moving some of my to onResume(), no gain\n3. using serializer to keep my view in sharedpreferences did not work</p>\n\n<p>Are there any ways to speed up dynamic view creation?</p>\n"},{"tags":["javascript","jquery","performance","jquery-deferred"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":71,"score":0,"question_id":12538344,"title":"asynchronous keyup events/how to short circuit sequential keyup events for speed boost","body":"<p>On the website I'm building for work, we have a search bar where the user can type in to search various fields stored in data on a set of elements. The problem comes in slower browsers, when they start typing.</p>\n\n<p>What I was trying to do is the following, using a jQuery Deferred object: add a key up handler to the input box. when the keyup is triggered, resolve any previously triggered keyups that are still 'pending'. then continue on using the current keyup event. Inside the keyup event itself where it's doing all the matching based on the input value, it always checks to see if the current event is pending still, and if it's not, it is supposed to skip the remainder of that event.</p>\n\n<p>The problem seems to be that, even with slow browsers like IE7, each keyup event is blocking. So if the user types in \"testing\", it'll try to finish the entire event for the \"t\" character before it even sees the keyup event of \"te\", and so on.</p>\n\n<p>The question then becomes, how can I make the keyup events asynchronous/non-blocking, or is this pretty much impossible?</p>\n\n<p>An example:</p>\n\n<pre><code>var search_values = new Array();\nvar deferred_searches = {};\n\nfunction filterSearchResults(event) {\n    var input_element = jq(event.target);\n    var search_value = input_element.val();\n    console.log('user just typed final letter in :' + search_value);\n    for (var svi = 0; svi &lt; search_values.length-1; svi++) {\n        if (deferred_searches[search_values[svi]] !== undefined &amp;&amp;\n            deferred_searches[search_values[svi]].state() == 'pending') {\n            console.log('resolving ' + search_values[svi])\n            deferred_searches[search_values[svi]].resolve(search_values[svi]);\n        }\n    }\n\n    var result_list = jq('div#header_project_selection_result_list');\n    var all_project_results = result_list.find('div.header_project_result');\n\n    console.log('beginning search for \"'+search_value+'\"');\n    if (search_value == '') {\n        // Blank input value means show all results\n    }\n    else {\n        // Non-blank value means search\n        if (! startSearchFilter(search_value, all_project_results)) {\n            return false; // this should return out of this current event handler\n        }\n    }\n    console.log('ending search for \"'+search_value+'\"');\n},\n\n/**\n * Helper function that actually handles finding and hiding/showing results,\n * and highlighting found text.\n *\n * Uses jQuery for this functionality.\n *\n * @param       search_value                            string              The text the user has input\n * @param       all_project_results                     jQuery              A jQuery extended array of all project results to filter\n *\n * @return      tbr                                     boolean             Flag indicating whether the search was short circuited\n */\nfunction startSearchFilter(search_value, all_project_results) {\n    var tbr = true;\n\n    search_values.push(search_value);\n    deferred_searches[search_value] = jq.Deferred();\n    deferred_searches[search_value].done(function(sv) {\n        search_values = search_values.without(sv);\n        console.log('done deferred search for \"'+sv+'\" - disabling');\n    });\n\n    var number_of_results_found = 0;\n    var pr_count = all_project_results.length - 1;\n    var regexp = new RegExp(search_value, 'im');\n    for (var index = 0; index &lt;= pr_count; index++) {\n        if (deferred_searches[search_value].state() == 'pending') {\n            // KEEP GOING\n            var ext_project_result = all_project_results.eq(index);\n            var result_data = ext_project_result.data();\n\n            //console.log(search_value+\" is in state '\"+deferred_searches[search_value].state()+\"'.....' all_project_results.each() [inside]\");\n            if (result_shown) {\n                number_of_results_found++;\n                // The input text has been found on data for the current project, so make sure we show that result row\n                ext_project_result.addClass('filtered');\n            }\n            else if (ext_project_result.hasClass('filtered')) {\n                // The input text has not been found, or the user is not an admin and can't view it, so hide that row\n                ext_project_result.removeClass('filtered');\n            }\n            continue; // keep going to the next result element\n        }\n        else {\n            tbr = false;\n            break; // break when short circuited\n        }\n    }\n    if (deferred_searches[search_value].state() == 'pending') {\n        deferred_searches[search_value].resolve(search_value); // explicitly resolve this finalized one\n    }\n\n    return tbr;\n}\n</code></pre>\n\n<p>In theory this should work but as I mentioned, it seems to always block.</p>\n"},{"tags":["php","mysql","performance"],"answer_count":5,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":80,"score":5,"question_id":12533332,"title":"MYSQL working slowly as query with subquery than 2 queries (+php)","body":"<p>I have table (about 80'000 rows), looks like</p>\n\n<pre><code>id, parentId, col1, col2, col3...\n 1,     null, 'A', 'B', 'C'\n 2,        1, ...\n 3,        1, ...\n 4,     null, ...\n 5,        4, ...\n</code></pre>\n\n<p>(one level parent - child only)</p>\n\n<p>and I need get all dependent rows - </p>\n\n<pre><code>SELECT ... \nFROM table \nWHERE id = :id OR parentId = :id OR id IN (\n    SELECT parentId \n    FROM table \n    WHERE id = :id\n    )\n</code></pre>\n\n<p>but why this request working slowly instead 2 request - if I get parentId on php first?</p>\n\n<pre><code>$t = executeQuery('SELECT parentId FROM table WHERE id = :Id;', $id);\nif ($t) {\n    $id = $t;\n}\n\n$t = executeQuery('SELECT * FROM table WHERE id = :id OR parentId = :id ORDER BY id;', $id);\n</code></pre>\n\n<p>PS: max depends rows &lt; 70</p>\n\n<p>PPS:</p>\n\n<pre><code>id  select_type table   type    possible_keys   key key_len ref rows    Extra\n1   PRIMARY product ALL PRIMARY,parentId    NULL    NULL    NULL    73415   Using where\n2   DEPENDENT SUBQUERY  product const   PRIMARY,parentId    PRIMARY 4   const   1\n</code></pre>\n"},{"tags":["php","mysql","linux","performance","apache"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":31,"score":0,"question_id":12539379,"title":"SaaS application - slow running queries","body":"<p>I have a SaaS application running on a LAMP stack. In general, everything runs pretty well.</p>\n\n<p>Most tables have a entity_id field that uniquely identifies each of our customers. We have found that as a table gets more and more records for that entity, that things start slowing down. For example, the 'notes' table returns records extremely quickly if the entity has less than 100 records, but once it gets to a few thousand, then things start getting noticeably slower.</p>\n\n<p>In general, things are architecturally pretty sound and queries have been profiled, etc.</p>\n\n<p>What are the kind of things that we should be looking at doing to improve performance? Is there anything we can be doing at the software architecture layer, or should we just be throwing more hardware at the problem?</p>\n"},{"tags":["php","performance","class","memory"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":19,"score":0,"question_id":12538471,"title":"Is good idea to add parent reference or value into child class?","body":"<p>I have class that create a lot of other classes.</p>\n\n<p>Is good idea to use reference instead of variable to create link on parent class?</p>\n\n<p>Can I keep memory use this style or it does not matter?</p>\n\n<p>And is this same (one) class? So if I change value in the parent class, can all other classes see new name? </p>\n\n<pre><code>class item implements Iterator {\n  private $name = 'Awesome name';\n  private $data = Array();\n  function __construct () {\n    for ($i=0; $i&lt;10; $i++) {\n      $this-&gt;data[$i] = new type(&amp;$this);\n    }       \n  }\n  public function rewind() { reset($this-&gt;data); }\n  public function current() { return current($this-&gt;data); }\n  ...\n}\n\nclass type {\n  private $parent = false;\n  function __construct ($a) {\n     $this-&gt;parent =&amp;$a;\n  }\n  function do () {\n     echo $this-&gt;parent-&gt;name . \"\\n\";\n  }\n}\n\n$i = new item ();\n\nforeach ($i as $t) {\n   $t-&gt;do();\n}\n</code></pre>\n"},{"tags":["arrays","performance","matlab"],"answer_count":1,"favorite_count":3,"up_vote_count":16,"down_vote_count":0,"view_count":217,"score":16,"question_id":12522888,"title":"arrayfun can be significantly slower than an explicit loop in matlab. Why?","body":"<p>Consider the following simple speed test for <code>arrayfun</code>:</p>\n\n<pre><code>T = 4000;\nN = 500;\nx = randn(T, N);\nFunc1 = @(a) (3*a^2 + 2*a - 1);\n\ntic\nSoln1 = ones(T, N);\nfor t = 1:T\n    for n = 1:N\n        Soln1(t, n) = Func1(x(t, n));\n    end\nend\ntoc\n\ntic\nSoln2 = arrayfun(Func1, x);\ntoc\n</code></pre>\n\n<p>On my machine (Matlab 2011b on Linux Mint 12), the output of this test is:</p>\n\n<pre><code>Elapsed time is 1.020689 seconds.\nElapsed time is 9.248388 seconds.\n</code></pre>\n\n<p>What the?!? <code>arrayfun</code>, while admittedly a cleaner looking solution, is an order of magnitude slower. What is going on here?</p>\n\n<p>Further, I did a similar style of test for <code>cellfun</code> and found it to be about 3 times slower than an explicit loop. Again, this result is the opposite of what I expected.</p>\n\n<p><strong>My question is:</strong> Why are <code>arrayfun</code> and <code>cellfun</code> so much slower? And given this, are there any good reasons to use them (other than to make the code look good)?</p>\n\n<p><strong>Note:</strong> I'm talking about the standard version of <code>arrayfun</code> here, NOT the GPU version from the parallel processing toolbox.</p>\n\n<p><strong>EDIT:</strong> Just to be clear, I'm aware that <code>Func1</code> above can be vectorized as pointed out by Oli. I only chose it because it yields a simple speed test for the purposes of the actual question.</p>\n\n<p><strong>EDIT:</strong> Following the suggestion of grungetta, I re-did the test with <code>feature accel off</code>. The results are:</p>\n\n<pre><code>Elapsed time is 28.183422 seconds.\nElapsed time is 23.525251 seconds.\n</code></pre>\n\n<p>In other words, it would appear that a big part of the difference is that the JIT accelerator does a much better job of speeding up the explicit <code>for</code> loop than it does <code>arrayfun</code>. This seems odd to me, since <code>arrayfun</code> actually provides more information, ie, its use reveals that the order of the calls to <code>Func1</code> do not matter. Also, I noted that whether the JIT accelerator is switched on or off, my system only ever uses one CPU...</p>\n"},{"tags":["python","performance","optimization"],"answer_count":1,"favorite_count":1,"up_vote_count":14,"down_vote_count":0,"view_count":131,"score":14,"question_id":12537716,"title":"Why is slice assignment faster than `list.insert`?","body":"<p>Inspired by <a href=\"http://stackoverflow.com/a/12537489/748858\">this nice answer</a>,</p>\n\n<p>Here's a benchmark:</p>\n\n<pre><code>import timeit\n\ndef test1():\n    a = [1,2,3]\n    a.insert(0,1)\n\ndef test2():\n    a = [1,2,3]\n    a[0:0]=[1]\n\nprint (timeit.timeit('test1()','from __main__ import test1'))\nprint (timeit.timeit('test2()','from __main__ import test2'))\n</code></pre>\n\n<p>For me, <code>test2</code> is sligtly faster (~10%).  Why is that the case?  I would expect it to be slower since:</p>\n\n<ol>\n<li>slice assignment must be able to accept iterables of any length and therefore must be more general.</li>\n<li>in slice assignment, we need to create a new list on the right hand side just to get it to work.</li>\n</ol>\n\n<p>Can anybody help me understand this?</p>\n\n<p><em>(using python 2.7 on OS-X 10.5.8)</em></p>\n"},{"tags":["c++","performance","boost","cross-platform","ace-tao"],"answer_count":11,"favorite_count":2,"up_vote_count":13,"down_vote_count":0,"view_count":13217,"score":13,"question_id":474840,"title":"boost vs ACE C++ cross platform performance comparison?","body":"<p>I am involved in a venture that will port some communications, parsing, data handling functionality from Win32 to Linux and both will be supported.  The problem domain is very sensitive to throughput and performance.  </p>\n\n<p>I have very little experience with performance characteristics of boost and ACE.  Specifically we want to understand which library provides the best performance for threading.  </p>\n\n<p>Can anyone provide some data -- documented or word-of-mouth or perhaps some links -- about the relative performance between the two?</p>\n\n<p>EDIT</p>\n\n<p>Thanks all.  Confirmed our initial thoughts - we'll most likely choose boost for system level cross-platform stuff.</p>\n"},{"tags":["linux","performance","unix","make"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":46,"score":0,"question_id":12532529,"title":"Make Parallel Jobs Performance","body":"<p>I was using <code>time</code> to profile <code>make</code> builds, and I noticed that having <code>-j 8</code> was several milliseconds slower than <code>-j 4</code>. I am compiling with gcc on a Intel Core2 Quad, so there are only four processor cores. Could this slowdown be due to the resource limitations, and whatever <code>make</code> uses to schedule jobs is adding some overhead?</p>\n"},{"tags":["python","string","performance","algorithm","data-structures"],"answer_count":6,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":171,"score":0,"question_id":12152150,"title":"Efficient replacement of occurrences of a list of words","body":"<p>I need to censor all occurrences of a list of words with *'s. I have about 400 words in the list and it's going to get hit with a lot of traffic, so I want to make it very efficient. What's an efficient algorithm/data structure to do this in? Preferably something already in Python.</p>\n\n<p>Examples:</p>\n\n<ol>\n<li>\"piss off\" => \"**** off\"</li>\n<li>\"hello\" => \"hello\"</li>\n<li>\"go to hell\" => \"go to ****\"</li>\n</ol>\n"},{"tags":["java","multithreading","performance","design","tomcat"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":105,"score":2,"question_id":12516325,"title":"Designing a web-application: where should business logic reside?","body":"<p>There is a design of a web application that I am not sure how to evaluate as good or bad.<br>\nThe idea is the following:<br>\nYou have a web application (non-trivial) deployed in Tomcat that accepts client requests. The load could be as much Tomcat can handle I guess.<br>\nNow the actual processing is not done inside the container but delegated to another process which it connects via a single <code>TCP</code> connection. So what essentially happens here is multiplexing.<br>\nI.e. client requests are send concurrently by clients and these requests are send as messages to the other process to handle (the process itself I guess would be multithreading) but <em>over one connection</em>.<br>\nI have listen from a collegue about similar design (in another platform) that is very good in performance, but from my point of view this has a bottleneck in the usage of that single TCP connection.<br>\nAlthough I am experienced in multithreading, I don't really know or have used asynchronous approaches (nio etc) and I was wondering if this fits into this category.<br>\nHas anyone used multiplexing as part of a big project (this way at least)? Is it a good option?  </p>\n"},{"tags":["performance","visual-studio-2012"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":42,"score":0,"question_id":12008761,"title":"The option to \"Administer Test Controllers\" is not available on the Test menu in Visual Studio 2012","body":"<p>I have installed Visual Studio Ultimate 2012 RC Version 11.0.50522.1 RCREL onto Windows 8 RP on a Virtual Machine running Hyper-V on Server 2012 RC DataCenter.</p>\n\n<p>All the documentation states that if you want to administer the Test Controllers in a Load test rig, you have to click on \"Administer Test Controllers\" in the \"Test\" menu. However, I don't have that option in the Test menu. I've tried adding it manually, but then it simply remains unavailable.<img src=\"http://i.stack.imgur.com/7TUEp.png\" alt=\"enter image description here\"></p>\n\n<p>What am I doing wrong? This is really holding me back.</p>\n\n<p>I am able to administer test controllers by creating a dummy load test and then right-Clicking on the \"Controller Machine\" Note and selecting \"Manage Test Controllers\" from there. The issue I have is that I think there might be some other menu items missing: How do you set the active Controller (I have 4 set-up). It seems to try to run the tests locally, but then the TestResults databases I have created for each of the 4 controllers aren't used, and it can't find it.</p>\n\n<p><strong>Edit:</strong>\nI've submitted a bug with Microsoft at: <a href=\"https://connect.microsoft.com/VisualStudio/feedback/details/758768/can-not-administer-or-manage-test-controllers-from-the-test-menu\" rel=\"nofollow\">https://connect.microsoft.com/VisualStudio/feedback/details/758768/can-not-administer-or-manage-test-controllers-from-the-test-menu</a></p>\n"},{"tags":["performance","oracle","ofbiz"],"answer_count":6,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":7512,"score":1,"question_id":1051593,"title":"How to improve performance in Oracle using SELECT DISTINCT","body":"<p>I'm currently working in the deployment of an OFBiz based ERP\nThe database being used is Oracle 10g Enterprise</p>\n\n<p>One of the biggest issues is some oracle performance problems, analyzing the ofbiz logs, the following query:</p>\n\n<pre><code>SELECT DISTINCT ORDER_ID, ORDER_TYPE_ID, ORDER_NAME, EXTERNAL_ID,\n SALES_CHANNEL_ENUM_ID, ORDER_DATE, ENTRY_DATE, VISIT_ID, STATUS_ID, CREATED_BY, \n FIRST_ATTEMPT_ORDER_ID, CURRENCY_UOM, SYNC_STATUS_ID, BILLING_ACCOUNT_ID, \n ORIGIN_FACILITY_ID, WEB_SITE_ID, PRODUCT_STORE_ID, TERMINAL_ID, TRANSACTION_ID, \n AUTO_ORDER_SHOPPING_LIST_ID, NEEDS_INVENTORY_ISSUANCE, IS_RUSH_ORDER, INTERNAL_CODE, \n REMAINING_SUB_TOTAL, GRAND_TOTAL, LAST_UPDATED_STAMP, LAST_UPDATED_TX_STAMP, CREATED_STAMP, \nCREATED_TX_STAMP, RECIBIR_BODEGAL, RECEPCIONADA_BODEGAL, FECHA_RECEPCION_BODEGAL FROM \nERP.ORDER_HEADER WHERE ((STATUS_ID = :v0 OR STATUS_ID = :v1 OR STATUS_ID = :v2) AND \n(ORDER_TYPE_ID = :v3)) ORDER BY ORDER_DATE DESC\n</code></pre>\n\n<p>is very slow. We've tested executing the query without the DISTINCT and it takes about 30 seconds. There are 4.000.000+ registers in the table.\nThere are index for the PK field orderId and almost every other field</p>\n\n<p>The EXPLAIN PLAN with DISTINCT is:</p>\n\n<pre><code>SELECT STATEMENT () (null)\n SORT (ORDER BY)    (null)\n  HASH (UNIQUE) (null)\n   TABLE ACCESS (FULL)  ORDER_HEADER\n</code></pre>\n\n<p>and without the DISTINCT is:</p>\n\n<pre><code>SELECT STATEMENT () (null)\n SORT (ORDER BY)    (null)\n  TABLE ACCESS (FULL)   ORDER_HEADER\n</code></pre>\n\n<p>any ideas about tuning oracle to improve the performance of this kind of queries?\nIt's very difficult to rewrite the query because is automatically generated by ofbiz\nso I think the solution is about tuning oracle</p>\n\n<p>thanks in advance</p>\n\n<p>EDIT: I analyzed the query using tkprof ,as suggested by Rob van Wijk and haffax,and the result is the following</p>\n\n<pre><code>********************************************************************************\n\nSELECT DISTINCT ORDER_ID, ORDER_TYPE_ID, ORDER_NAME, EXTERNAL_ID,\n SALES_CHANNEL_ENUM_ID, ORDER_DATE, ENTRY_DATE, VISIT_ID, STATUS_ID, CREATED_BY, \n FIRST_ATTEMPT_ORDER_ID, CURRENCY_UOM, SYNC_STATUS_ID, BILLING_ACCOUNT_ID, \n ORIGIN_FACILITY_ID, WEB_SITE_ID, PRODUCT_STORE_ID, TERMINAL_ID, TRANSACTION_ID, \n AUTO_ORDER_SHOPPING_LIST_ID, NEEDS_INVENTORY_ISSUANCE, IS_RUSH_ORDER, INTERNAL_CODE, \n REMAINING_SUB_TOTAL, GRAND_TOTAL, LAST_UPDATED_STAMP, LAST_UPDATED_TX_STAMP, CREATED_STAMP, \nCREATED_TX_STAMP, RECIBIR_BODEGAL, RECEPCIONADA_BODEGAL, FECHA_RECEPCION_BODEGAL FROM \nERP.ORDER_HEADER WHERE STATUS_ID = 'ORDER_COMPLETED' ORDER BY ORDER_DATE DESC\n\ncall     count       cpu    elapsed       disk      query    current        rows\n------- ------  -------- ---------- ---------- ---------- ----------  ----------\nParse        1      0.03       0.01          0          0          0           0\nExecute      1      0.00       0.00          0          0          0           0\nFetch        1      9.10     160.81      66729      65203         37          50\n------- ------  -------- ---------- ---------- ---------- ----------  ----------\ntotal        3      9.14     160.83      66729      65203         37          50\n\nMisses in library cache during parse: 1\nOptimizer mode: ALL_ROWS\nParsing user id: 58  \n\nElapsed times include waiting on following events:\n  Event waited on                             Times   Max. Wait  Total Waited\n  ----------------------------------------   Waited  ----------  ------------\n  SQL*Net message to client                       1        0.00          0.00\n  db file scattered read                       8178        0.28        146.55\n  direct path write temp                       2200        0.04          4.22\n  direct path read temp                          36        0.14          2.01\n  SQL*Net more data to client                     3        0.00          0.00\n  SQL*Net message from client                     1        3.36          3.36\n********************************************************************************\n</code></pre>\n\n<p>So it seems the problem is the 'db file scattered read', any ideas to how to tune oracle in order to reduce the wait in this event?</p>\n\n<p>Follow up with the new tkprof result, this time closing the session:</p>\n\n<pre><code>********************************************************************************\n\nSELECT DISTINCT ORDER_ID, ORDER_TYPE_ID, ORDER_NAME, EXTERNAL_ID,\n SALES_CHANNEL_ENUM_ID, ORDER_DATE, ENTRY_DATE, VISIT_ID, STATUS_ID, CREATED_BY,\n FIRST_ATTEMPT_ORDER_ID, CURRENCY_UOM, SYNC_STATUS_ID, BILLING_ACCOUNT_ID,\n ORIGIN_FACILITY_ID, WEB_SITE_ID, PRODUCT_STORE_ID, TERMINAL_ID, TRANSACTION_ID,\n AUTO_ORDER_SHOPPING_LIST_ID, NEEDS_INVENTORY_ISSUANCE, IS_RUSH_ORDER, INTERNAL_CODE,\n REMAINING_SUB_TOTAL, GRAND_TOTAL, LAST_UPDATED_STAMP, LAST_UPDATED_TX_STAMP, CREATED_STAMP,\nCREATED_TX_STAMP, RECIBIR_BODEGAL, RECEPCIONADA_BODEGAL, FECHA_RECEPCION_BODEGAL FROM\nERP.ORDER_HEADER WHERE STATUS_ID = 'ORDER_COMPLETED' ORDER BY ORDER_DATE DESC\n\ncall     count       cpu    elapsed       disk      query    current        rows\n------- ------  -------- ---------- ---------- ---------- ----------  ----------\nParse        1      0.03       0.01          0          0          0           0\nExecute      2      0.00       0.00          0          0          0           0\nFetch        1      8.23      47.66      66576      65203         31          50\n------- ------  -------- ---------- ---------- ---------- ----------  ----------\ntotal        4      8.26      47.68      66576      65203         31          50\n\nMisses in library cache during parse: 1\nOptimizer mode: ALL_ROWS\nParsing user id: 58 \n\nRows     Row Source Operation\n-------  ---------------------------------------------------\n     50  SORT ORDER BY (cr=65203 pr=66576 pw=75025 time=47666679 us)\n3456659   TABLE ACCESS FULL ORDER_HEADER (cr=65203 pr=65188 pw=0 time=20757300 us)\n\n\nElapsed times include waiting on following events:\n  Event waited on                             Times   Max. Wait  Total Waited\n  ----------------------------------------   Waited  ----------  ------------\n  SQL*Net message to client                       1        0.00          0.00\n  db file scattered read                       8179        0.14         34.96\n  direct path write temp                       2230        0.00          3.91\n  direct path read temp                          52        0.14          0.84\n  SQL*Net more data to client                     3        0.00          0.00\n  SQL*Net message from client                     1     1510.62       1510.62\n********************************************************************************\n</code></pre>\n"},{"tags":["performance","caching","filesystems","smarty","memcached"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":842,"score":1,"question_id":2487015,"title":"THE FASTEST Smarty Cache Handler","body":"<p>Does anyone know if there is an overview of the performance of different cache handlers for smarty?</p>\n\n<p>I compared smarty file cache with a memcache handler, but it seemed memcache has a negative impact on performance.</p>\n\n<p>I figured there would be a faster way to cache than through the filesystem... am I wrong?</p>\n"},{"tags":["sql-server","performance","tsql"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":337,"score":1,"question_id":10722339,"title":"DBCC FREEPROCCACHE and DBCC DROPCLEANBUFFERS alike for specific scope","body":"<p>I would like to check options to improve my queries. </p>\n\n<p>sometimes, i want to do the tests on a production server so i can't use DBCC FREEPROCCACHE\nand DBCC DROPCLEANBUFFERS to clear the entire server cache. </p>\n\n<p>could you please share with me the way to do a kind of \"cache clean\" only for my connection/scope?</p>\n\n<p>Thanks.</p>\n"},{"tags":["performance","optimization","haskell","pragma"],"answer_count":2,"favorite_count":8,"up_vote_count":14,"down_vote_count":0,"view_count":247,"score":14,"question_id":12534145,"title":"When to use various language pragmas and optimisations?","body":"<p>I have a fair bit of understanding of haskell but I am always little unsure about what kind of pragmas and optimizations I should use and where. Like </p>\n\n<ul>\n<li>Like when to use <code>SPECIALIZE</code> pragma and what performance gains it has.</li>\n<li>Where to use <code>RULES</code>. I hear people taking about a particular rule not firing? How do we check that?</li>\n<li>When to make arguments of a function strict and when does that help? I understand that making argument strict will make the arguments to be evaluated to normal form, then  why should I not add strictness to all function arguments? How do I decide? </li>\n<li>How do I see and check I have a space leak in my program? What are the general patterns which constitute to a space leak?</li>\n<li>How do I see if there is a problem with too much lazyness? I can always check the heap profiling but I want to know what are the general cause, examples and patterns where lazyness hurts?</li>\n</ul>\n\n<p>Is there any source which talks about advanced optimizations (both at higher and very low levels) especially particular to haskell?</p>\n"},{"tags":["java","performance","lucene"],"answer_count":2,"favorite_count":0,"up_vote_count":5,"down_vote_count":0,"view_count":138,"score":5,"question_id":4690983,"title":"Committed changes visibility in Lucene. Best practices","body":"<p>I'm using Lucene 3.0.3. I've made a Spring bean aiming to encapsulate all operations on the same index.</p>\n\n<pre><code>public class IndexOperations {\n\n    private IndexWriter writer;\n    private IndexReader reader;\n    private IndexSearcher searcher;\n\n    public void init() {...}\n    public void destroy() {...}\n\n    public void save(Document d) {...}\n    public void delete(Document d) {...}\n    public List&lt;Document&gt; list() {...}\n\n}\n</code></pre>\n\n<p>In order to permit fast changes and searches, I thought leaving writer, reader and searcher open could be a good idea. But the problem is that commited changes on writer can't be seen by readers until reopen. And this operation can be costly, so maybe is not a good idea for fast searches.</p>\n\n<p>What would be the best approach for this typical scenario?</p>\n"},{"tags":["ruby","performance","xpath"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":103,"score":0,"question_id":12497928,"title":"xpath speed comparision","body":"<p>I am using XMLLib for parsing an XML file.</p>\n\n<p>I have the following 2 XPath queries on the document.</p>\n\n<p>First:</p>\n\n<pre><code>xpath = %Q{\n    //premis:event\n        [premis:eventType != 'normalize' and premis:eventType != 'migrate' ]\n        [premis:linkingObjectIdentifier/premis:linkingObjectIdentifierValue = 'info:fda/ED5UQP1LG_4F4AKG/file/197']\n    }\n</code></pre>\n\n<p>Second:</p>\n\n<pre><code>xpath = %Q{\n    /mets:mets/mets:amdSec/mets:digiprovMD//premis:event\n        [premis:eventType != 'normalize' and premis:eventType != 'migrate' ]\n        [premis:linkingObjectIdentifier/premis:linkingObjectIdentifierValue = 'info:fda/ED5UQP1LG_4F4AKG/file/197']\n    }\n</code></pre>\n\n<p>While the second XPath is a more specific / confined one, the result takes around 5816ms. The first XPath takes just 76ms. Any idea why?</p>\n\n<p>Here is the xml file: <a href=\"https://www.dropbox.com/s/ucc9bu4gziml9pv/aip_520\" rel=\"nofollow\">https://www.dropbox.com/s/ucc9bu4gziml9pv/aip_520</a></p>\n\n<p>Here is how I am measuring the time:</p>\n\n<pre><code>parser = LibXML::XML::Parser.file f\ndoc = parser.parse\n\nstart = Time.now\nputs \"Started program at: #{start}\"\nq = time_first_query doc\nputs q.inspect\n\nmilestone = Time.now\nputs \"Time taken by first query: #{time_diff start, milestone} ms\"\nstart = milestone\n\nq = time_first_improved_query doc\nq.inspect\n\nmilestone = Time.now\nputs \"Time taken by first improved query: #{time_diff start, milestone} ms\"\n</code></pre>\n"},{"tags":["c#","performance","optimization"],"answer_count":2,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":116,"score":1,"question_id":11218224,"title":"Do arrays within C# struct defeat the performance benefits?","body":"<p>Supposedly C# structs have some performance benefits over classes for lightweight data objects because they're stored entirely on the stack instead of allocating heap memory.</p>\n\n<p>If one of the members of that struct is an int[] or an array of structs, what effect does that have on allocation and any other performance benefits I might get out of it?</p>\n\n<p>Example:</p>\n\n<pre><code>struct A\n{\n    int foo;\n    int bar;\n}\n\nstruct B\n{\n   int[] foos;\n   A[] bars;\n}\n</code></pre>\n\n<p>How would struct A differ from struct B in performance, memory usage, etc?</p>\n"},{"tags":["mysql","performance","query"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":49,"score":0,"question_id":6902636,"title":"Performant check for existence of values","body":"<p>I have a little performance problem with an MySQL-Query and don't know how to build it right, so it does work with as little as possible effort.</p>\n\n<p>Problem:</p>\n\n<p>Imagine a table with two columns: user(int), item(int)</p>\n\n<p>The users have different items associated with. Maybe user-1 has item-1, item-2, item-3 and user-2 has item-1, item-3, item-4.</p>\n\n<p>What I want to know is for a specific user-X and each item in seperate if at least one other user has this items also.</p>\n\n<p>What I started with was</p>\n\n<pre><code>SELECT item\nFROM table\nWHERE item IN (SELECT item FROM table WHERE user = X) AND user != X\nGROUP BY item\n</code></pre>\n\n<p>... but this was inefficient because in this case the query searchs through the whole table and checks for each item even then if each one was already found.\nI can't LIMIT the query because I don't know how many items user-X actually has.\nAnd sending a search query with LIMTI 1 for each item individually is also not such a good idea.</p>\n\n<p>For abstraction you can also say I want to know if a set of item-X, ..., item-Y (not order or continous numbering) are associated with at least one user.</p>\n\n<p>I don't care for how often the item is around at all, just want to know if at least once.</p>\n\n<p>So, how can this be done right?\nThanks for any suggestions!</p>\n"},{"tags":[".net","asp.net-mvc","performance","mapping","automapper"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":132,"score":0,"question_id":12532129,"title":"Any way to improve Automapper performance?","body":"<p>I am a big fan of Autompper. I am now using it in many projects for mapping entities between differents domains like from wcf service model to business model.</p>\n\n<p>After some load tests (with VS Profiler) in a sample website, I found that Automapper is responsible for a high CPU consumption.</p>\n\n<p>I have done some unit for this behavior :</p>\n\n<pre><code>using Microsoft.VisualStudio.TestTools.UnitTesting;\nusing System;\nusing System.Collections.Generic;\nusing System.Diagnostics;\nusing System.Linq;\n\nnamespace AutoMapper.Tests\n{\n    [TestClass]\n    public class UnitTest\n    {\n        public class ClassSource\n        {\n            public string PropertyA { get; set; }\n            public int PropertyB { get; set; }\n            public NestedClassSource PropertyC { get; set; }\n        }\n\n        public class NestedClassSource\n        {\n            public string PropertyD { get; set; }\n            public DateTime PropertyE { get; set; }\n            public List&lt;int&gt; PropertyF { get; set; }\n        }\n\n        public class ClassDestination\n        {\n            public string PropertyA { get; set; }\n            public int PropertyB { get; set; }\n            public NestedClassDestination PropertyC { get; set; }\n        }\n\n        public class NestedClassDestination\n        {\n            public string PropertyD { get; set; }\n            public DateTime PropertyE { get; set; }\n            public List&lt;int&gt; PropertyF { get; set; }\n        }\n\n        [TestMethod]\n        public void MappingPerfTests()\n        {\n            Mapper.Initialize(a =&gt;\n            {\n                a.CreateMap&lt;ClassSource, ClassDestination&gt;();\n                a.CreateMap&lt;NestedClassSource, NestedClassDestination&gt;();\n\n            });\n            Mapper.AssertConfigurationIsValid();\n\n            IList&lt;ClassSource&gt; items = GenerateRandomSources(nbItems: 500);\n\n            //automapper\n            MicroBench(() =&gt;\n            {\n                var res = Mapper.Map&lt;IList&lt;ClassSource&gt;, IList&lt;ClassDestination&gt;&gt;(items);\n            }, nbIterations: 10);\n            // will take nearly 30 ms per test\n            // total : 300 ms\n\n\n            //manual mapper\n            MicroBench(() =&gt;\n            {\n                var res = new List&lt;ClassDestination&gt;(items.Count);\n                foreach (var source in items)\n                {\n                    res.Add(new ClassDestination()\n                    {\n                        PropertyA = source.PropertyA,\n                        PropertyB = source.PropertyB,\n                        PropertyC = new NestedClassDestination()\n                        {\n                            PropertyD = source.PropertyC.PropertyD,\n                            PropertyE = source.PropertyC.PropertyE,\n                            PropertyF = new List&lt;int&gt;(source.PropertyC.PropertyF)\n                        }\n\n                    });\n                }\n            }, nbIterations: 10);\n            // will take nearly 0 ms per test\n            // total : 1 ms\n\n        }\n\n        private IList&lt;ClassSource&gt; GenerateRandomSources(int nbItems = 1000)\n        {\n            IList&lt;ClassSource&gt; res = new List&lt;ClassSource&gt;(100);\n            foreach (var i in Enumerable.Range(1, nbItems))\n            {\n                ClassSource item = new ClassSource()\n                {\n                    PropertyA = \"PropertyA\",\n                    PropertyB = i,\n                    PropertyC = new NestedClassSource() { PropertyD = \"PropertyD\", PropertyE = DateTime.Now, PropertyF = Enumerable.Range(1, 10).ToList() }\n                };\n                res.Add(item);\n            }\n            return res;\n        }\n\n        private void MicroBench(Action action, int nbIterations = 1000)\n        {\n            long totalElapsed = 0L;\n\n            foreach (var i in Enumerable.Range(1, nbIterations))\n            {\n                Stopwatch watcher = Stopwatch.StartNew();\n\n                action();\n\n                watcher.Stop();\n                Console.WriteLine(\"test : {0} ms\", watcher.ElapsedMilliseconds);\n                totalElapsed += watcher.ElapsedMilliseconds;\n            }\n\n            Console.WriteLine(\"total : {0} ms\", totalElapsed);\n            Console.WriteLine(\"avg : {0} ms\", totalElapsed / nbIterations);\n\n        }\n    }\n}\n</code></pre>\n\n<p>At the end, Automapper seems quite slow : average 30 ms per test whereas manual mapping takes less than one ms. I am \"only\" mapping 500 \"simple\" objects ! Maybe for a MVC ViewModel, this will rarely occurs, but others mapping it will can be frequent.</p>\n\n<p>30 ms seems to be fast, but the real trouble is that this 30 ms (for nothing) is <strong>CPU time on the web server</strong> How does the server will handle heavy load (100 concurrent users) ? Not well in fact, that's why our load test throws a warning.</p>\n\n<p>I founded a way to generate linq expression with Mapper.CreateMapExpression, but unfortunatelly the expression does not contains nested types or options.</p>\n\n<p>So, is there any way to improve Automapper performance ?\nAre there best practices ?</p>\n\n<p>Thanks,</p>\n"},{"tags":["java","android","xml","performance","dynamic"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":1,"view_count":42,"score":0,"question_id":12531948,"title":"Is it more efficient to declare a view invisible or set the background to transparent and change it's background resource when needed?","body":"<p>Trying to get the most of my app means making more efficient code to help the user have a better experience. So I was wandering if anyone could help me and let me know which is a better choice for efficiency.</p>\n\n<p>I have views which come to life at various times in the program through animations but their places are still held in the main layout so the buttons are constantly in existence. I was trying to figure out whether it's more efficient to go ahead and set the background resources in the xml code and just set the visibility to invisible and then change it back to visible dynamically when the views are needed, or is better to set the background resource to transparent and change the background resource when needed dynamically?</p>\n\n<p>I'm aware the difference is minimal but when trying to use an app the most frustrating thing can be speed so knocking off even a quarter of a second in loading time is a step towards more efficient and more complete experience for the user.</p>\n"},{"tags":["php","mysql","performance"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":84,"score":-1,"question_id":12528334,"title":"mySQL update thousands records in while statement","body":"<p>İ must calculate the score of many users once a week in PHP.</p>\n\n<p>My code is something like this:</p>\n\n<pre><code>$result = mysql_query(\"SELECT * FROM mtweekteam where weekID=$week \");\n$week=1001;\nwhile ($row = mysql_fetch_array($result)) { //Repeats 170.000 Times\n\n $total=0;\n\n //SCORE CALCULATION***************\n //You can ignore this score calculation part because system works \n //slowly whether it exsits or not. For example, \n //with this part it takes 20 min. , otherwise it takes 17 min.\n\n $capID=$row['capID'];\n\n $ren = mysql_query(\"SELECT SUM(s.totalScore),SUM(s.yellowCard),SUM(s.redCard),t.auth as tauth FROM mtPlayerStat as s,mtWeekTeam as t WHERE (s.ID=t.pla1_ID OR s.ID=t.pla2_ID  OR s.ID=t.pla3_ID OR s.ID=t.pla4_ID OR s.ID=t.pla5_ID OR s.ID=t.pla6_ID OR s.ID=t.pla7_ID OR s.ID=t.pla8_ID OR s.ID=t.pla9_ID OR s.ID=t.pla10_ID OR s.ID=t.pla11_ID) AND s.weekID=t.weekID AND t.weekID=$week  AND t.teamID=\".$row['teamID']) or die(\"ddd1\") ;\n $rowx = mysql_fetch_array($ren) or die(\"ddd2\");    \n\n $total=$rowx['SUM(s.totalScore)'];\n\n if($rowx['SUM(s.yellowCard)']==0 &amp;&amp; $rowx['SUM(s.redCard)']==0){\n    $total=$total+5;\n }\n\n $rsn = mysql_query(\"SELECT totalScore FROM mtPlayerStat WHERE ID=\".$capID.\" AND weekID=$week\") or die(mysql_error());\n $cap = mysql_fetch_array($rsn);\n\n $total=$total+$cap['totalScore'];\n\n if($rowx['tauth']==\"0\"){\n    $total=0;\n }\n //*************\n\n mysql_query(\"UPDATE mtWeekTeam SET totalScore = '$total' WHERE teamID=\".$row['teamID'].\" AND weekID =$week  LIMIT 1 ;\");\n}\n</code></pre>\n\n<p>This code takes 20 minutes and is constantly growing.\nIs there a better way to do it?</p>\n"},{"tags":["c#","visual-studio-2010","performance","list","sorting"],"answer_count":6,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":57,"score":3,"question_id":12526013,"title":"Efficient way to move list items at the end of the list according to specific criteria","body":"<p>Having a <code>List&lt;Office&gt;</code> where Office is a class, I have to sort its entries by country (where country is a property of class Office).</p>\n\n<p>Some offices have no country set and therefore will be displayed at the top of the list. In this case I have to put them at the bottom of the list since considered \"less relevant\". </p>\n\n<pre><code>switch (sortOptions.SortField)\n\n...  \n\ncase OfficeSortField.Country:\n\nvar noCountryList = officesList.Where(a =&gt; string.IsNullOrEmpty(a.CountryText)).ToList();\nofficesList.RemoveAll(a =&gt; string.IsNullOrEmpty(a.CountryText));\n\nofficesList= sortOptions.SortOrder == SortOrder.Ascending\n                                      ? officesList.OrderBy(o =&gt; o.CountryText).ToList()\n                                      : officesList.OrderByDescending(o =&gt; o.CountryText).ToList();\n\nofficesList.AddRange(noCountryAssoList);\nbreak;\n</code></pre>\n\n<p>Under perfomance perspective, is there a better way to proceed? </p>\n"},{"tags":["visual-studio-2010","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":14,"score":0,"question_id":12531620,"title":"My visual studio 2010 install suffers severe performance problems, what approaches can I take to find out the cause?","body":"<p>I'm using VS2010, and hitting severe performance problems. For example I can load up, create a 'HelloWorld' project, start to debug it, and VS is using 700MB of RAM, and behaving sluggishly whenever I start to type.</p>\n\n<p>Now, given the sparse information I've provided, theres no way I expect the exact cause of the problem I'm having to be given. Instead, I'll ask for some <strong>general ways to find out the cause of VS performance issues</strong>.</p>\n\n<p>For example:</p>\n\n<p>Does VS keep some form of cache from previous sessions which is loaded every startup? Where does this live if so?</p>\n\n<p>Are Extensions commonly a problem? Are there any tools to show separate memory/CPU usage of each?</p>\n"},{"tags":["c","performance","switch-statement"],"answer_count":4,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":154,"score":4,"question_id":12531024,"title":"Switch() { case: } performance in C","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/1028437/why-switch-case-and-not-if-else-if\">Why Switch/Case and not If/Else If?</a>  </p>\n</blockquote>\n\n\n\n<p>I would like to understand how a <code>switch() case:</code> statement in C\nis translated by the compiler into assembler opcodes.</p>\n\n<p>Specifically, i'm interested in understanding the difference\nwith a serie of <code>if then else</code> branches.</p>\n\n<p>Performance comparison is the main topic.</p>\n\n<p>A few words on vocabulary :\ni'm familiar with assembler main concepts, having coded in assembler a long time ago for simpler systems, But certainly do not now anything about x86 assembler semantic.\nSo a direct assembler output will not be useful.\nPseudo-code is much prefered.</p>\n"},{"tags":["java","performance","unsafe"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":210,"score":2,"question_id":12226123,"title":"**BUSTED** How to speed up a byte[] lookup to be faster using sun.misc.Unsafe?","body":"<p>I am experimenting with Unsafe to iterate over memory instead of iterating over the values in a byte[]. A memory block is allocated using unsafe. The memory is sufficient to hold 65536 byte values. </p>\n\n<p>I AM TRYING THIS:</p>\n\n<pre><code>char aChar = some character\n\nif ((byte) 0 == (unsafe.getByte(base_address + aChar) &amp; mask)){\n // do something\n}\n</code></pre>\n\n<p>INSTEAD OF:</p>\n\n<pre><code>char aChar = some character\n\nif ((byte) 0 == ( lookup[aChar] &amp; mask )){\n // do something\n}\n</code></pre>\n\n<hr>\n\n<p>I <em>thought</em> Unsafe could access the memory faster than using a regular array access with the index check it does for each index...</p>\n\n<p>It was only wishful thinking that the jvm would have a special op (unsafe) that would somehow make regular array access and iteration faster. The jvm, it seems to me, works fine with normal byte[] iterations and does them, fast as can be, using normal, unadulterated, vanilla java code.</p>\n\n<p>@millimoose hits the proverbial 'nail on the head' </p>\n\n<p><em><strong>\"Unsafe might be useful for a lot of things, but this level of microoptimisation isn't one of them. – millimoose\"</em></strong> </p>\n\n<p>Using Unsafe is faster in a very strict limited set of circumstances:</p>\n\n<ul>\n<li>(64bit jvm only) faster for a single 65535 byte[] lookup <strong>done exactly once</strong> for each test. In this case UnsafeLookup_8B on 64_bit jvm is 24% faster. If the test repeats itself so that each test is done twice, the normal method is now 30% faster than unsafe. In pure interpreted mode on a cold jvm, the Unsafe is faster by far --- but only the first time and only for a small array size. ON a 32 bit standard Oracle JVM 7.x, the normal is three times faster than using unsafe.</li>\n</ul>\n\n<p>Using Unsafe (in my tests) is slower:</p>\n\n<ul>\n<li>slower on both Oracle java 64 bit and 32 bit virtual machines</li>\n<li>slower regardless of OS and machine architecture (32 and 64 bit)</li>\n<li><p>slower even if <code>server</code>jvm option is invoked</p></li>\n<li><p>Unsafe is slower from 9% or more ( 1_GB array and UnsafeLookup_8B(fastest one) in code below on 32 bit jvm (64bit was even slower??))</p></li>\n<li>Unsafe is slower from 234% or more ( 1_MB array and UnsafeLookup_1B (fastest one) in code below on a 64 bit jvm.</li>\n</ul>\n\n<p>Is there some reason for this?**</p>\n\n<p>When I run the code yellowB posted below (checks a 1GB byte[]), the normal is also still the fastest:</p>\n\n<pre><code>C:\\Users\\wilf&gt;java -Xms1600m -Xprof -jar \"S:\\wilf\\testing\\dist\\testing.jar\"\ninitialize data...\ninitialize data done!\n\nuse normalLookup()...\nNot found '0'\ntime : 1967737 us.\n\nuse unsafeLookup_1B()...\nNot found '0'\ntime : 2923367 us.\n\nuse unsafeLookup_8B()...\nNot found '0'\ntime : 2495663 us.\n\nFlat profile of 26.35 secs (2018 total ticks): main\n\n  Interpreted + native   Method\n  0.0%     1  +     0    test.StackOverflow.main\n  0.0%     1  +     0    Total interpreted\n\n     Compiled + native   Method\n 67.8%  1369  +     0    test.StackOverflow.main\n 11.7%   236  +     0    test.StackOverflow.unsafeLookup_8B\n 11.2%   227  +     0    test.StackOverflow.unsafeLookup_1B\n  9.1%   184  +     0    test.StackOverflow.normalLookup\n 99.9%  2016  +     0    Total compiled\n\n         Stub + native   Method\n  0.0%     0  +     1    sun.misc.Unsafe.getLong\n  0.0%     0  +     1    Total stub\n\n\nFlat profile of 0.00 secs (1 total ticks): DestroyJavaVM\n\n  Thread-local ticks:\n100.0%     1             Blocked (of total)\n\n\nGlobal summary of 26.39 seconds:\n100.0%  2023             Received ticks\n\n\nC:\\Users\\wilf&gt;java -version\njava version \"1.7.0_07\"\nJava(TM) SE Runtime Environment (build 1.7.0_07-b11)\nJava HotSpot(TM) Client VM (build 23.3-b01, mixed mode, sharing)\n</code></pre>\n\n<p>CPU is: Intel Core 2 Duo E4600 @ 2.4GHZ 4.00GB (3.25GB usable)\nOS: Windows 7 (32)</p>\n\n<p>Running the test on an 4 Core AMD64 with Windows 7_64, 32 bit java:</p>\n\n<pre><code>initialize data...\ninitialize data done!\n\nuse normalLookup()...\nNot found '0'\ntime : 1631142 us.\n\nuse unsafeLookup_1B()...\nNot found '0'\ntime : 2365214 us.\n\nuse unsafeLookup_8B()...\nNot found '0'\ntime : 1783320 us.\n</code></pre>\n\n<p>Running the test on an 4 Core AMD64 with Windows 7_64, 64 bit java:</p>\n\n<pre><code>use normalLookup()...\nNot found '0'\ntime : 655146 us.\n\nuse unsafeLookup_1B()...\nNot found '0'\ntime : 904783 us.\n\nuse unsafeLookup_8B()...\nNot found '0'\ntime : 764427 us.\n\nFlat profile of 6.34 secs (13 total ticks): main\n\n  Interpreted + native   Method\n 23.1%     3  +     0    java.io.PrintStream.println\n 23.1%     3  +     0    test.StackOverflow.unsafeLookup_8B\n 15.4%     2  +     0    test.StackOverflow.main\n  7.7%     1  +     0    java.io.DataInputStream.&lt;init&gt;\n 69.2%     9  +     0    Total interpreted\n\n     Compiled + native   Method\n  7.7%     0  +     1    test.StackOverflow.unsafeLookup_1B\n  7.7%     0  +     1    test.StackOverflow.main\n  7.7%     0  +     1    test.StackOverflow.normalLookup\n  7.7%     0  +     1    test.StackOverflow.unsafeLookup_8B\n 30.8%     0  +     4    Total compiled\n\n\nFlat profile of 0.00 secs (1 total ticks): DestroyJavaVM\n\n  Thread-local ticks:\n100.0%     1             Blocked (of total)\n\n\nGlobal summary of 6.35 seconds:\n100.0%    14             Received ticks\n 42.9%     6             Compilation\n</code></pre>\n"},{"tags":["performance","r","memory-management","data.frame","data.table"],"answer_count":2,"favorite_count":10,"up_vote_count":17,"down_vote_count":0,"view_count":667,"score":17,"question_id":5942760,"title":"Most efficient list to data.frame method?","body":"<p>Just had a conversation with coworkers about this, and we thought it'd be worth seeing what people out in SO land had to say.  Suppose I had a list with N elements, where each element was a vector of length X.  Now suppose I wanted to transform that into a data.frame.  As with most things in R, there are multiple ways of skinning the proverbial cat, such as <code>as.dataframe</code>, using the plyr package, comboing <code>do.call</code> with <code>cbind</code>, pre-allocating the DF and filling it in, and others.  </p>\n\n<p>The problem that was presented was what happens when either N or X (in our case it is X) becomes extremely large.  Is there one cat skinning method that's notably superior when efficiency (particularly in terms of memory) is of the essence?</p>\n"},{"tags":["c#","performance","big-o"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":2,"view_count":61,"score":-2,"question_id":12530892,"title":"Performace of Xor","body":"<p>what is the performance of the XOR operator '^' in C#. is it multiple operations in one?</p>\n\n<p>I mean if I have a loop over n elements, will it run with performace O(n)??\ne.g.</p>\n\n<pre><code>  int a=10;\n  for(int i=0; i&lt;coll.Count; i++)\n        int n = a^coll[i]; \n</code></pre>\n"},{"tags":["android","performance","path","draw"],"answer_count":1,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":62,"score":0,"question_id":12530684,"title":"Android draw path","body":"<p>It is not good idea to construct path object every time when call Draw method.\nIs it better to keep path object and clear/set points every time?</p>\n\n<p>Update:\nOne more question - what is difference between 'reset' and 'rewind' path object?</p>\n"},{"tags":["php","performance","parallel-processing","web-scraping"],"answer_count":5,"favorite_count":2,"up_vote_count":6,"down_vote_count":0,"view_count":182,"score":6,"question_id":9233356,"title":"Downloading pages in parallel using PHP","body":"<p>I have to scrap a web site where i need to fetch multiple URLs and then process them one by one. The current process somewhat goes like this.</p>\n\n<p>I fetch a base URL and get all secondary URLs from this page, then for each secondary url I fetch that URL, process found page, download some photos (which takes quite a long time) and store this data to database, then fetch next URL and repeat the process.</p>\n\n<p>In this process, I think I am wasting some time in fetching secondary URL at the start of each iteration. So I am trying to fetch next URLs in parallel while processing first iteration.</p>\n\n<p>The solution in my mind is, from main process call a PHP script, say downloader,  which will download all the URL (with <code>curl_multi</code> or <code>wget</code>) and store them in some database. </p>\n\n<p>My questions are  </p>\n\n<ul>\n<li>How to call such downloder <strong>asynchronously</strong>, I don't want my main script to wait till downloder completes.  </li>\n<li>Any location to store downloaded data, such as shared memory. Of course, other than database.  </li>\n<li>There any chances that data gets corrupt while storing and retrieving, how to avoid this?  </li>\n<li>Also, please guide me know if anyone have a better plan.</li>\n</ul>\n"},{"tags":["performance","entity-framework","entity-framework-4","linq-to-entities"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":17,"score":1,"question_id":12513865,"title":"ToTraceString takes long time to execute","body":"<p>I have a service that executes a query through Entity Framework 4, and it doesn't have any problem, until now I tried to execute it on a Windows Server 2003 with IIS 6 or a Windows 7 with IIS 7.</p>\n\n<p>Now, when I try to run the same service on a Windows Server 2008 R2 and IIS 7.5 the <strong>same query</strong> becomes very slow (From like 200ms to 3 seconds). Analyzing the problem I discovered that takes so long when it tries to generate the sql query to execute, just calling a <strong>ToTraceString() take almost 3 seconds.</strong></p>\n\n<p>I don't know if the OS and IIS version could be the problem, but they are the only differences I see.</p>\n\n<p>Thank you</p>\n\n<p><strong>Update</strong></p>\n\n<p>I have found out that problem is effectively in the time that Entity Framework takes to generate the query, and it depends about the processor, with Intel it works very well, <strong>with AMD it is much slower and it gets worse if the application runs with multi-cores</strong>, forcing the w3wp to use a single core it get better but still slower than the Intel solution</p>\n"},{"tags":["objective-c","performance","automatic-ref-counting"],"answer_count":1,"favorite_count":3,"up_vote_count":4,"down_vote_count":0,"view_count":62,"score":4,"question_id":12527286,"title":"Are there any concrete study of the performance impact of using ARC?","body":"<p>I could not find an objective study regarding ARC performance impact in a real life project.  <a href=\"http://developer.apple.com/library/mac/#releasenotes/ObjectiveC/RN-TransitioningToARC/Introduction/Introduction.html\" rel=\"nofollow\">The official doc</a> says </p>\n\n<blockquote>\n  <p>The compiler efficiently eliminates many extraneous retain/release calls and much effort has been invested in speeding up the Objective-C runtime in general. In particular, the common “return a retain/autoreleased object” pattern is much faster and does not actually put the object into the autorelease pool, when the caller of the method is ARC code.</p>\n</blockquote>\n\n<p>which has been relayed/deformed by tech-fanboys into  \"ARC is faster\".</p>\n\n<p>What I know for sure is what I measured. We recently migrated our iOS project to ARC and I did some performance measurement before/after on some CPU intensive area of our code (production code, compiled with the <code>-Os</code> flag, of course).  </p>\n\n<p>I observed a 70% (yes 70%) performance regression.  Using <em>Instruments</em> to track retain/release events, I realized that the compiler introduces a LOT of retain/release pairs in area where you would not do it (in pre ARC environment). Basically, any temporary becomes strong.  That is, I believe, the source of the performance regression.</p>\n\n<p>The original code, before the migration, was already quite optimized.  There was barely any autorelease done.  Hence, there was little room for improvement by switching to ARC.</p>\n\n<p>Fortunately, <em>Instruments</em> was able to show me the most costly retain/released introduced by ARC and I was able to deactivate them using __unsafe_unretained.  This slightly mitigates the performance regression.</p>\n\n<p>Does anybody have any information on this or other technique to avoid the performance loss ?  (apart from deactivating ARC)</p>\n\n<p>Thanks. </p>\n"},{"tags":["performance","running","sub-query"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":15,"score":0,"question_id":12527231,"title":"sub-query is running slow","body":"<p>I have 2 queries </p>\n\n<p>Query 1</p>\n\n<p><code>select *  from ranktrack_data2 where cron = 2070 limit 1;</code></p>\n\n<p>Query 2</p>\n\n<p><code>select distinct(cron) from ranktrack_thread where status =0 and cron = 2070</code></p>\n\n<p>Both of these Queries take milliseconds to complete </p>\n\n<p>BUT when I try using a sub-query eg:\n Query 3</p>\n\n<pre><code>select *  from ranktrack_data2 where cron \nin (  select distinct(cron) from rankdata_thread where status =0 and cron_id = 2070 )  limit 1;\n</code></pre>\n\n<p>Query 3 will take a hell of lot of time, \nCan any one suggest why?</p>\n"},{"tags":["vim","performance","text-editor"],"answer_count":7,"favorite_count":3,"up_vote_count":30,"down_vote_count":1,"view_count":6833,"score":29,"question_id":237383,"title":"How do I insert a linebreak where the cursor is without entering into insert mode in Vim?","body":"<p>Is possible to insert a linebreak where the cursor is in Vim without entering into insert mode? Here's an example ([x] means cursor is on x):</p>\n\n<pre><code>if (some_condition) {[ ]return; }\n</code></pre>\n\n<p>Occasionally, I might want to enter some more code. So I'd press 'i' to get into insert mode, press enter to insert the linebreak and then delete the extra space. Next, I'd enter normal mode and position the cursor before the closing brace and then do the same thing to get it on its own line.</p>\n\n<p>I've been doing this a while, but there's surely a better way to do it?</p>\n"},{"tags":["performance","ruby-on-rails-3.1"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":16,"score":0,"question_id":12526517,"title":"Why does my Rails page take so long to load? How can I fix it?","body":"<p>Ok, I realize this is not a very specific question and the answer could be any number of things, but I am genuinely curious as to how I can find out why the production site I have takes about 30 seconds to load a page when locally in development it is really fast.</p>\n\n<p>I am using Rails with an SQLite database, so there's barely any overhead there. Other than that not a lot of fancy gems. </p>\n\n<p>I would really like to know what tools I can use to determine the culprit: is it the hosting server? Is it the queries (I really don't think so)? Maybe it's a caching problem?</p>\n\n<p>This is the site: www.hapkidobcn.com</p>\n\n<p>It is using Rails 3.1 with Ruby 1.8.7 because the hosting company has not updated it yet.</p>\n\n<p>I am using a profiling gem locally and it reports very good speeds. I also have another app on the same hosting company and it too is quite slow.</p>\n\n<p>Any help, pointers, etc would be very much appreciated.</p>\n"},{"tags":["java","performance","io","xor"],"answer_count":1,"favorite_count":0,"up_vote_count":4,"down_vote_count":0,"view_count":53,"score":4,"question_id":12525767,"title":"Java IO Performance XOR with 2 Files","body":"<p>I'm having some trouble with my Java IO Performance.</p>\n\n<p>First at all,I've read my tips here about the performance,which i tried to implement.</p>\n\n<p>But here is my Problem:</p>\n\n<p>With small files (up to 400MB), it is pretty fast.\nBut the real files i will be working with are about 30 GB.And with these,it slows down like hell.</p>\n\n<p>What it does: Take 2 Files,do the exclusive or and write a new File.</p>\n\n<p>BTW: Don't worry about the file cut at the end.That is just to fix a little mistake I've not found yet.</p>\n\n<p>I hope someone has a tip for me.Thanks.</p>\n\n<p>Regards Timo</p>\n\n<pre><code>import java.io.File;\nimport java.io.IOException;\nimport java.io.RandomAccessFile;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.FileChannel;\n\npublic class main {\n    public static void main(String[] args) throws IOException {\n\n        final long start = System.currentTimeMillis();\n        // Init. FileChannel1\n        final File file1 = new File(\"/home/tmann/Downloads/test/1.zip\");\n        final RandomAccessFile fis1 = new RandomAccessFile(file1, \"rw\");\n        final FileChannel ch1 = fis1.getChannel();\n\n        // Init. FileChannel2\n        final File file2 = new File(\"/home/tmann/Downloads/test/2.zip\");\n        final RandomAccessFile fis2 = new RandomAccessFile(file2, \"rw\");\n        final FileChannel ch2 = fis2.getChannel();\n        // Init FileChannel3\n        final File file3 = new File(\"/home/tmann/Downloads/test/32.zip\");\n        final RandomAccessFile fis3 = new RandomAccessFile(file3, \"rw\");\n        final FileChannel ch3 = fis3.getChannel();\n        // Init ByteBuffer1\n\n        final ByteBuffer bytebuffer1 = ByteBuffer.allocate(65536);\n        // Init ByteBuffer2\n        final ByteBuffer bytebuffer2 = ByteBuffer.allocate(65536);\n        // Init ByteBuffer3\n        final ByteBuffer bytebuffer3 = ByteBuffer.allocate(65536);\n\n\n        int byte1 = 0;\n        int byte2 = 0;\n\n        final byte[] array1 = bytebuffer1.array();\n        final byte[] array2 = bytebuffer2.array();\n        final byte[] array3 = bytebuffer3.array();\n\n        int count = 0;\n\n        while (byte1 != -1) {\n            byte1=ch1.read(bytebuffer1);\n            byte2=ch2.read(bytebuffer2);\n                while (count &lt; byte1) {\n                        array3[count] = (byte) (array1[count] ^ array2[count]);\n                        count++;\n                                        }\n\n           bytebuffer3.put(array3);\n\n            bytebuffer1.flip();\n            bytebuffer2.flip();\n            bytebuffer3.flip();\n\n                while (bytebuffer3.hasRemaining()) {\n                        ch3.write(bytebuffer3);\n                                                    }\n\n            bytebuffer1.clear();\n            bytebuffer2.clear();\n            bytebuffer3.clear();\n\n        }\n\n        fis3.setLength(fis3.length()-59858);\n        final long endvar = System.currentTimeMillis();\n        System.out.println((500.0 / ((endvar - start) / 1000f)) + \"MB/s\");\n\n    }\n}\n</code></pre>\n"},{"tags":["java","performance","fps","slick2d","rectangles"],"answer_count":5,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":101,"score":1,"question_id":12467785,"title":"Slick is getting very slow but only drawin rectangles","body":"<p>I am using slick for java since a few days and got a serious problem.\nIf i run a completely empty apllication (it just shows the fps) with a solution of 800x600 i get a fps count between 700 and 800.\nIf I now draw an array with 13300 entries as a grid of green and white rectangles, the fps drop to something around 70.</p>\n\n<p>With more entries in the array it becomes really slow.\nFor example in a solution of 1024x768 and an array with 21760 entries the fps drop to 40.</p>\n\n<p>How i draw a single entry:</p>\n\n<pre><code>public void draw(Graphics graphics){\n    graphics.setColor(new Color(getColor().getRed(), getColor().getGreen(), getColor().getBlue(), getColor().getAlpha()));\n\n    graphics.fillRect(getPosition().x, getPosition().y, getSize().x, getSize().y);\n\n    Color_ARGB white = new Color_ARGB(Color_ARGB.ColorNames.WHITE);\n    graphics.setColor(new Color(white.getRed(), white.getGreen(), white.getBlue(), white.getAlpha()));\n}\n</code></pre>\n\n<p>And this is how I draw the complete array:</p>\n\n<pre><code>public void draw(Graphics graphics) {\n    for (int ix = 0; ix &lt; getWidth(); ix++) {\n        for (int iy = 0; iy &lt; getHeight(); iy++) {\n            getGameGridAt(ix, iy).draw(graphics);\n        }\n    }\n}\n</code></pre>\n\n<p>In my opinion 21760 is not that much.\nIs there anything wrong with my code or is slick just too slow to draw so much rectangles?</p>\n"},{"tags":["c","performance","memcpy"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":60,"score":0,"question_id":12524319,"title":"efficiently combining 2 large memory blocks in c","body":"<p>I have 2 buffer of short ints (C language), dynamically allocated\n with size: 9000 (streams)  X 180 (minutes).  Both are the exact same size. They each contain info for 180 minutes, from 9:31 to 12:30:</p>\n\n<pre><code>    Stream  minute\n   ------   ------------------------------------------\n    0       931 932 933 934 ... 1230\n    1       931 932 933 934 ... 1230\n    2       931 932 933 934 ... 1230\n    ...\n    8999    931 932 933 934 ... 1230\n</code></pre>\n\n<p>I need to, as efficiently as possible, combine the last part of the first block with the entirety of the 2nd block. That is, combine the last hour of the first buffer with the entire 3 hours of the second: thus:</p>\n\n<pre><code>   Stream   minute\n   ------   ------------------------------------------\n    0       1131 1132 1133 ...1230 931  932 933 934 ... 1230\n    1       1131 1132 1133 ...1230 931  932 933 934 ... 1230\n    2       1131 1132 1133 ...1230 931  932 933 934 ... 1230\n    ...\n    8999    1131 1132 1133 ...1230 931  932 933 934 ... 1230\n\nSo the new block will be of size 9000 x 240.\n</code></pre>\n\n<p>(I've simplified this, so it doesn't reflect some of the logic that\n   requries things to be laid out in this exact way.)</p>\n\n<p>I can think of a few brute force methods to do this, such as 18,000\n   memcpy's.  Is there an efficient way to do this?\n   Efficiency is important here, because this operation needs to be performed\n   an enormous number of times every minute.  This is thread safe, but even\n   running on a dozen threads it is still slug slow.\n   Any help?</p>\n"},{"tags":["performance","nginx"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":46,"score":0,"question_id":12524587,"title":"nginx slow response after 500 users online","body":"<p>I have a file download site, where I use nginx to deliver the files and apache for web part.\nHowever as soon as I hit around 500 users online and 1500Mbit/s, Nginx responses get delayed with 10 to 60 seconds. Once connection established everything works as expected.</p>\n\n<p>Apache have no issues on browse side, HDDs are only 50% util, IOwait is 3, bandwidth is 70% saturated, CPU is 5%. I really don't understand why is this happening, I get no errors in nginx logs.</p>\n"},{"tags":["database","performance","oracle","view","rownum"],"answer_count":3,"favorite_count":2,"up_vote_count":2,"down_vote_count":0,"view_count":104,"score":2,"question_id":12518494,"title":"Oracle view performance with rownum","body":"<p>I am using Oracle 10g, and I have a view that joins two large tables (millions of records). I am trying to select a limited \"sample\" of the data for the user like this:</p>\n\n<pre><code>select * from VIEW_NAME where ROWNUM &lt; 5; \n</code></pre>\n\n<p>It is very slow, and I think it should not be, because I just need a few rows of the result, so Oracle should not calculate the full join. </p>\n\n<p>The requirement is that the user should be able to specify interactively the number of returned rows (it doesn't matter exactly which rows from the result). Is there any way to achieve this? (with rownum or with another method)</p>\n\n<p>(I can change the view definition or the way the final SQL is built, but as far as I know, I cannot pass information about the desired number of rows dynamically to the view)</p>\n\n<p>EDIT: The view definition is very simple, something like this:</p>\n\n<pre><code>CREATE OR REPLACE VIEW VIEW_NAME AS\n(\n    select\n    e.id as ID,\n    e.somefield as something,\n    ... (some similar selects from e)\n    c.field as anotherthing,\n   ... (lots of other fields from c)\n    from SCHEMA.TABLE1 e\n    inner join SCHEMA.TABLE2 c on e.key = c.key\n)\n</code></pre>\n\n<p>Explain plan mentions a full table access for both tables which is not surprising, because just returning the first few rows should not take a long time. </p>\n\n<p>EDIT2: here's the full plan</p>\n\n<pre><code>SQL&gt; select * from table(dbms_xplan.display);\n\nPLAN_TABLE_OUTPUT\n------------------------------------------------------------------------------------------------------------------------------------------------------\nPlan hash value: 2644394598\n\n----------------------------------------------------------------------------------------------------------------------------------------------\n| Id  | Operation                 | Name        | Rows  | Bytes |TempSpc| Cost (%CPU)| Time     | Pstart| Pstop |    TQ  |IN-OUT| PQ Distrib |\n----------------------------------------------------------------------------------------------------------------------------------------------\n|   0 | SELECT STATEMENT          |             |     4 |  1252 |       | 43546   (1)| 00:08:43 |       |       |        |      |            |\n|*  1 |  COUNT STOPKEY            |             |       |       |       |            |          |       |       |        |      |            |\n|   2 |   PX COORDINATOR          |             |       |       |       |            |          |       |       |        |      |            |\n|   3 |    PX SEND QC (RANDOM)    | :TQ10002    |   696K|   207M|       | 43546   (1)| 00:08:43 |       |       |  Q1,02 | P-&gt;S | QC (RAND)  |\n|*  4 |     COUNT STOPKEY         |             |       |       |       |            |          |       |       |  Q1,02 | PCWC |            |\n|*  5 |      HASH JOIN BUFFERED   |             |   696K|   207M|    49M| 43546   (1)| 00:08:43 |       |       |  Q1,02 | PCWP |            |\n\nPLAN_TABLE_OUTPUT\n------------------------------------------------------------------------------------------------------------------------------------------------------\n|   6 |       BUFFER SORT         |             |       |       |       |            |          |       |       |  Q1,02 | PCWC |            |\n|   7 |        PX RECEIVE         |             |   696K|    90M|       |  5137   (1)| 00:01:02 |       |       |  Q1,02 | PCWP |            |\n|   8 |         PX SEND HASH      | :TQ10000    |   696K|    90M|       |  5137   (1)| 00:01:02 |       |       |        | S-&gt;P | HASH       |\n|   9 |          TABLE ACCESS FULL| TABLE1      |   696K|    90M|       |  5137   (1)| 00:01:02 |       |       |        |      |            |\n|  10 |       PX RECEIVE          |             |   892K|   149M|       |  5260   (1)| 00:01:04 |       |       |  Q1,02 | PCWP |            |\n|  11 |        PX SEND HASH       | :TQ10001    |   892K|   149M|       |  5260   (1)| 00:01:04 |       |       |  Q1,01 | P-&gt;P | HASH       |\n|  12 |         PX BLOCK ITERATOR |             |   892K|   149M|       |  5260   (1)| 00:01:04 |     1 |   140 |  Q1,01 | PCWC |            |\n|  13 |          TABLE ACCESS FULL| TABLE2      |   892K|   149M|       |  5260   (1)| 00:01:04 |     1 |   140 |  Q1,01 | PCWP |            |\n----------------------------------------------------------------------------------------------------------------------------------------------\n\nPredicate Information (identified by operation id):\n\nPLAN_TABLE_OUTPUT\n------------------------------------------------------------------------------------------------------------------------------------------------------\n---------------------------------------------------\n\n   1 - filter(ROWNUM&lt;5)\n   4 - filter(ROWNUM&lt;5)\n   5 - access(\"E\".\"KEY\"=\"C\".\"KEY\")\n\n27 rows selected.\n</code></pre>\n"},{"tags":["performance","optimization","matrix"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":17,"score":0,"question_id":12524276,"title":"Multiplying dense matrices with small values","body":"<p>If multiplying two matrices, <code>A*B = C</code>, either of them can have large number of values which is negligible, ie near zero.  There isn't really any block structure to zeroes.</p>\n\n<p>What solutions do I have to reduce operations?\nI thought of primarily trying to permute matrices to get into block-zero structure but that may by itself be <code>O(3)</code> cost.  CRS or CCS doesn't seem to have many ready-to-use dgemm equivalents.</p>\n"},{"tags":["performance","sql-server-2008"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":5,"view_count":30,"score":-5,"question_id":12524077,"title":"Can anyone suggest a good website for learning SQL Query Performance tuning","body":"<p>Please suggest a good site to learn sql query performance tuning and query hints , for me to start learning..</p>\n"},{"tags":["sql","performance","sql-server-2008","sql-order-by"],"answer_count":6,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":65,"score":1,"question_id":12521902,"title":"Can this ORDER BY on a CASE clause be made faster?","body":"<p>I'm selecting results from a table of ~350 million records, and it's running extremely slowly - around 10 minutes.  The culprit seems to be the <strong>ORDER BY</strong>, as if I remove it the query only takes a moment.  Here's the gist:</p>\n\n<pre><code>SELECT TOP 100\n    (columns snipped)\nFROM (\n    SELECT \n        CASE WHEN (e2.ID IS NULL) THEN\n            CAST(0 AS BIT) ELSE CAST(1 AS BIT) END AS RecordExists,\n        (columns snipped)\n    FROM dbo.Files AS e1\n    LEFT OUTER JOIN dbo.Records AS e2 ON e1.FID = e2.FID\n)  AS p1\nORDER BY p1.RecordExists\n</code></pre>\n\n<p>Basically, I'm ordering the results by whether <em>Files</em> have a corresponding <em>Record</em>, as those without need to be handled first.  I could run two queries with <strong>WHERE</strong> clauses, but I'd rather do it in a single query if possible.</p>\n\n<p>Is there any way to speed this up?</p>\n"},{"tags":["performance","mongodb"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":30,"score":0,"question_id":12522998,"title":"Mongo: Looking for the fastest query for collection of millions of instances","body":"<p>I have a collection of millions of instances of this simple object:</p>\n\n<pre><code>{\n Org: string,\n Value: string\n}\n</code></pre>\n\n<p>There is a secondary index by <code>[Org]</code> field, and there are around thousand of different <code>[Org]</code> values. This collection is not supposed to be updated as it's filled in once at a time.</p>\n\n<p>Now I have a task: request <code>[Value]</code> values for the specific <code>[Org]</code> in the natural order (the order they were inserted into database, i.e. <code>$orderby: _id</code>), and limited by specific number, with maximum performance. My questions are:</p>\n\n<ul>\n<li><p>Does order of operators like <code>orderby</code>, <code>query</code>, <code>limit</code> and <code>sort</code> impact performance and execution plan in the case?</p></li>\n<li><p>Will both indexes (secondary by <code>[Org]</code> and default primary by <code>_id</code>) will be used in the query?</p></li>\n<li><p>Is there any tool for MongoDb to trace indexes usage like Query Analyzer for SQL Server?</p></li>\n</ul>\n"},{"tags":["performance","qt","qgraphicsview","qlabel"],"answer_count":0,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":51,"score":1,"question_id":12522775,"title":"QLabel vs QGraphicsView performance","body":"<p>I am learning QT and I am puzzled by the difference in performance of QLabel and QGraphics view while panning.</p>\n\n<p>I read a huge 36Mpixels (D800) jpeg file into either QLabel or QGraphics objects and try to drag the full scale image with QLabel/Graphics. Surprisingly, the QLabel provides really smooth movement while QGRaphicsView panning is jerky.</p>\n\n<p>The simplified QGraphicsView code is:</p>\n\n<pre><code>QApplication::setGraphicsSystem(\"raster\");    \n...\nQGraphicsView  view();\nview.setDragMode(QGraphicsView::ScrollHandDrag);\nview.setHorizontalScrollBarPolicy(Qt::ScrollBarAlwaysOff);\nview.setVerticalScrollBarPolicy(Qt::ScrollBarAlwaysOff);\nview.setFrameStyle(QFrame::NoFrame);\nview.showFullScreen();\n\nQGraphicsPixmapItem *pmItem = new QGraphicsPixmapItem(pixMap);\nscene.addItem(pmItem); // only one item in the scene\n//view.setRenderHints(QPainter::Antialiasing | QPainter::SmoothPixmapTransform); // no difference\nview.show();\n</code></pre>\n\n<p>The simplified QLabel based code is:</p>\n\n<pre><code>void MyQLabel::mouseMoveEvent(QMouseEvent *event){\n    if(_leftButtonPressed) {\n            // calculate new {_x, _y} position\n            repaint();\n        }\n    } else super::mouseMoveEvent(event);\n}\nvoid MyQLabel::paintEvent(QPaintEvent *aEvent){\n    QPainter paint(this);\n    paint.drawPixmap(_x, _y, pixMap);\n}\n\n... // Somewhere in the code:\nMyQLabel _myLabel(NULL);\n_myLabel.showFullScreen();\n_myLabel.show();\n</code></pre>\n\n<p>It feels like QGraphicsView is skipping the over some positions (with fast dragging), while QLabel paints at all intermediate images.</p>\n\n<p>What do I miss?</p>\n\n<p>Thank you\nAlex</p>\n"},{"tags":["javascript","performance"],"answer_count":2,"favorite_count":2,"up_vote_count":12,"down_vote_count":0,"view_count":407,"score":12,"question_id":8410667,"title":"Object pools in high performance javascript?","body":"<p>I'm writing some javascript code which needs to run fast, and uses a lot of short-lived objects. Am I better off using an object pool, or just creating objects as I need them?</p>\n\n<p>I wrote a <a href=\"http://jsperf.com/object-pool\">JSPerf test</a> which suggests that there is no benefit to using an object pool, however I'm not sure if jsperf benchmarks are run long enough for the browser's garbage collector to kick in.</p>\n\n<p>The code is part of a game, so I don't care about legacy browser support. My graphics engine won't work on old browsers anyway.</p>\n"},{"tags":["mysql","sql","performance"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":38,"score":1,"question_id":12522391,"title":"Should I store multiple ids as varchar and use FIND_IN_SET?","body":"<p>I see platforms using this:</p>\n\n<pre><code>- properties table -\nid | name | ID_CATEGORY (INT)\n\n- categories table -\nid | name\n\n- property_categories -\nid_category | id_property\n</code></pre>\n\n<p>And retrieving properties by category with</p>\n\n<pre><code>SELECT * FROM properties, categories, property_categories\nWHERE 1\n   AND categories.name = \"Some category name here\" # match this category name\n   AND categories.id = property_categories.id_category # match category\n   AND properties.id = property_categories.id_property # and match property\n</code></pre>\n\n<p>But I recently saw in an application this (with 2 tables instead of 3):</p>\n\n<pre><code>- properties table -\nid | name | ID_CATEGORY (VARCHAR(255))\n\n- categories table -\nid | name\n</code></pre>\n\n<p>And retrieving properties by category with this query:</p>\n\n<pre><code>SELECT * FROM properties, categories\nWHERE 1\n   AND categories.name = \"Some category name here\" # match this category name\n   AND FIND_IN_SET(categories.id, properties.ID_CATEGORY)\n</code></pre>\n\n<p>Are there any performance benefits? Should the second method be used in favor of the first one? Is it ok if I leave this unchanged?</p>\n\n<p>Sometimes there are many fields combined (including ID_TYPE, ID_STATUS, etc.) and there's a big JOIN gluing all data together for filters.</p>\n"},{"tags":["javascript","jquery","ajax","performance","caching"],"answer_count":3,"favorite_count":0,"up_vote_count":3,"down_vote_count":0,"view_count":64,"score":3,"question_id":12519716,"title":"Preloading (=Caching) a full Website with Ajax – Possible Problems?","body":"<p>I'm currently building a portfolio-website for an architect that has a whole lot of images on its pages.\nNavigation is done with history.js (=AJAX). In order to save loading time and make the whole thing more \"snappy\", I wrote a script that crawls the page body for links to other pages and fetches these automatically in the background. So far, it works like a charm.</p>\n\n<p>It basically keeps a queue-Array that holds all the links. A setTimeout()-Function works through them and fetches each page using jQuery $.ajax(). The resulting HTML is stored in a Javascript Object.</p>\n\n<p><strong>Now, here's my question:</strong>\nWhat are possible problems that might occur when using this on different machines/browsers/operation systems?</p>\n\n<p>I'm thinking about:</p>\n\n<ul>\n<li>max. javascript object/variable size (The fetched HTML is stored in an javascript object)</li>\n<li>possible performance problems</li>\n<li>max. number of asynchronous requests?</li>\n<li>… anything you can think of?</li>\n</ul>\n\n<p>Thanks a lot in advance,</p>\n\n<p>a hobby programmer</p>\n"},{"tags":["php","performance"],"answer_count":3,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":171,"score":0,"question_id":4518621,"title":"What is fast way of developing project in PHP?","body":"<p>I am Java Developer. But, I am required to develop &amp; submit a sample project of PHP (Online Shopping Management). This is just for learning basic programming in PHP.</p>\n\n<p>I have been using w3schools as well.</p>\n\n<p>Please kindly suggest a very easy &amp; fast way to get confidence over the language for this development.</p>\n\n<p>EDIT : Basically, I need to know the way how do I shift to PHP development from Java ?</p>\n"},{"tags":["python","performance","oop","numpy","numerical"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":89,"score":2,"question_id":12501487,"title":"Mixing numpy and OO in numerical work","body":"<p>First, I'd like to know if it's good practice in general to use an OO approach in numerical work.</p>\n\n<p>Second, a possible use case for OO would be to encapsulate in some object the parameters of some model. Say, I want to study parabolas of the form ax^2 + bx +c. So I would encapsulate a, b, c in some Parabola object. I can plot it and so on. Now, let's say, I want to explore the location of the vertical axis of parabolas. Basically, without OO, I could just plot say a surface of all vertical axis locations w.r.t. a and b (that would be two numpy arrays) for a few given value of c's.</p>\n\n<p>My question is, how do I do such surface plot with the extra OO layer without sacrificing (too much) on numpy performance?</p>\n\n<p><strong>Extra explanation</strong></p>\n\n<p>A way to go with the OO approach would be to create a matrix of Parabola objects for a range of values of parameters a and b. But this way would handle possibly very big objects instead of plain numpy arrays of parameter range.</p>\n"},{"tags":["performance","oracle","entity-framework","entity-framework-4"],"answer_count":2,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":67,"score":0,"question_id":12484605,"title":"Entity Framework SQL Selecting 600+ Columns","body":"<p>I have a query generated by entity framework running against oracle that's too slow. It runs in about 4 seconds. </p>\n\n<p>This is the main portion of my query</p>\n\n<pre><code>var query = from x in db.BUILDINGs\n                    join pro_co in db.PROFILE_COMMUNITY on x.COMMUNITY_ID equals pro_co.COMMUNITY_ID\n                    join co in db.COMMUNITies on x.COMMUNITY_ID equals co.COMMUNITY_ID\n                    join st in db.STATE_PROFILE on co.STATE_CD equals st.STATE_CD\n                    where pro_co.PROFILE_NM == authorizedUser.ProfileName\n\n                    select new\n                    {\n                        COMMUNITY_ID = x.COMMUNITY_ID,\n                        COUNTY_ID = x.COUNTY_ID,\n                        REALTOR_GROUP_NM = x.REALTOR_GROUP_NM,\n                        BUILDING_NAME_TX = x.BUILDING_NAME_TX,\n                        ACTIVE_FL = x.ACTIVE_FL,\n                        CONSTR_SQFT_AVAIL_NB = x.CONSTR_SQFT_AVAIL_NB,\n                        TRANS_RAIL_FL = x.TRANS_RAIL_FL,\n                        LAST_UPDATED_DT = x.LAST_UPDATED_DT,\n                        CREATED_DATE = x.CREATED_DATE,\n                        BUILDING_ADDRESS_TX = x.BUILDING_ADDRESS_TX,\n                        BUILDING_ID = x.BUILDING_ID,\n                        COMMUNITY_NM = co.COMMUNITY_NM,\n                        IMAGECOUNT = x.BUILDING_IMAGE2.Count(),\n                        StateCode = st.STATE_NM,\n                        BuildingTypeItems = x.BUILDING_TYPE_ITEM,\n                        BuildingZoningItems = x.BUILDING_ZONING_ITEM,\n                        BuildingSpecFeatures = x.BUILDING_SPEC_FEATURE_ITEM,\n                        buildingHide = x.BUILDING_HIDE,\n                        buildinghideSort = x.BUILDING_HIDE.Count(y =&gt; y.PROFILE_NM == ProfileName) &gt; 0 ? 1 : 0,\n                        BUILDING_CITY_TX = x.BUILDING_CITY_TX,\n                        BUILDING_ZIP_TX = x.BUILDING_ZIP_TX,\n                        LPF_GENERAL_DS = x.LPF_GENERAL_DS,\n                        CONSTR_SQFT_TOTAL_NB = x.CONSTR_SQFT_TOTAL_NB,\n                        CONSTR_STORIES_NB = x.CONSTR_STORIES_NB,\n                        CONSTR_CEILING_CENTER_NB = x.CONSTR_CEILING_CENTER_NB,\n                        CONSTR_CEILING_EAVES_NB = x.CONSTR_CEILING_EAVES_NB,\n                        DESCR_EXPANDABLE_FL = x.DESCR_EXPANDABLE_FL,\n                        CONSTR_MATERIAL_TYPE_TX = x.CONSTR_MATERIAL_TYPE_TX,\n                        SITE_ACRES_SALE_NB = x.SITE_ACRES_SALE_NB,\n                        DESCR_PREVIOUS_USE_TX = x.DESCR_PREVIOUS_USE_TX,\n                        CONSTR_YEAR_BUILT_TX = x.CONSTR_YEAR_BUILT_TX,\n                        DESCR_SUBDIVIDE_FL = x.DESCR_SUBDIVIDE_FL,\n                        LOCATION_CITY_LIMITS_FL = x.LOCATION_CITY_LIMITS_FL,\n                        TRANS_INTERSTATE_NEAREST_TX = x.TRANS_INTERSTATE_NEAREST_TX,\n                        TRANS_INTERSTATE_MILES_NB = x.TRANS_INTERSTATE_MILES_NB,\n                        TRANS_HIGHWAY_NAME_TX = x.TRANS_HIGHWAY_NAME_TX,\n                        TRANS_HIGHWAY_MILES_NB = x.TRANS_HIGHWAY_MILES_NB,\n                        TRANS_AIRPORT_COM_NAME_TX = x.TRANS_AIRPORT_COM_NAME_TX,\n                        TRANS_AIRPORT_COM_MILES_NB = x.TRANS_AIRPORT_COM_MILES_NB,\n                        UTIL_ELEC_SUPPLIER_TX = x.UTIL_ELEC_SUPPLIER_TX,\n                        UTIL_GAS_SUPPLIER_TX = x.UTIL_GAS_SUPPLIER_TX,\n                        UTIL_WATER_SUPPLIER_TX = x.UTIL_WATER_SUPPLIER_TX,\n                        UTIL_SEWER_SUPPLIER_TX = x.UTIL_SEWER_SUPPLIER_TX,\n                        UTIL_PHONE_SVC_PVD_TX = x.UTIL_PHONE_SVC_PVD_TX,\n                        CONTACT_ORGANIZATION_TX = x.CONTACT_ORGANIZATION_TX,\n                        CONTACT_PHONE_TX = x.CONTACT_PHONE_TX,\n                        CONTACT_EMAIL_TX = x.CONTACT_EMAIL_TX,\n                        TERMS_SALE_PRICE_TX = x.TERMS_SALE_PRICE_TX,\n                        TERMS_LEASE_SQFT_NB = x.TERMS_LEASE_SQFT_NB\n                    };\n</code></pre>\n\n<p>There is a section of code that tacks on dynamic where and sort clauses to the query but I've left those out. The query takes about 4 seconds to run no matter what is in the where and sort. </p>\n\n<p>I dropped the generated SQL in Oracle and an explain plan didn't appear to show anything that screamed fix me. Cost is 1554</p>\n\n<p>If this isn't allowed I apologize but I can't seem to find a good way to share this information. I've uploaded the explain plan generated by Sql Developer here: <a href=\"http://www.123server.org/files/explainPlanzip-e1d291efcd.html\" rel=\"nofollow\">http://www.123server.org/files/explainPlanzip-e1d291efcd.html</a></p>\n\n<p>Table Layout</p>\n\n<pre><code>Building\n--------------------\n- BuildingID\n- CommunityId\n- Lots of other columns\n\nProfile_Community\n-----------------------\n- CommunityId\n- ProfileNM\n- lots of other columns\n\nstate_profile\n---------------------\n- StateCD\n- ProfileNm\n- lots of other columns\n\nProfile\n---------------------\n- Profile-NM\n- a few other columns\n</code></pre>\n\n<p>All of the tables with allot of columns have 120-150 columns each. It seems like entity is generating a select statement that pulls every column from every table instead of just the ones I want. </p>\n\n<p>The thing that's bugging me and I think might be my issue is that in my LINQ I've selected 50 items, but the generated sql is returning 677 columns. I think returning so many columns is the source of my slowness possibly.</p>\n\n<p>Any ideas why I am getting so many columns returned in SQL or how to speed my query?</p>\n"},{"tags":["php","performance","monitoring"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":23,"score":0,"question_id":12519624,"title":"PHP execution time monitoring","body":"<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"http://stackoverflow.com/questions/21133/simplest-way-to-profile-a-php-script\">Simplest way to profile a PHP script</a>  </p>\n</blockquote>\n\n\n\n<p>Now I monitor the execution time of my functions like this:</p>\n\n<pre><code> $time_start = microtime(true);\nmyfunction();\n\n$time_end = microtime(true);\n$time = $time_end - $time_start;\n\necho \"Time: \".$time;\n</code></pre>\n\n<p>Is there any better way / tool to monitor of a whole application's execution time functions by function? So i dont have to mess my code with this lines?\nI want to see how long takes to execute each function / line and how many times a function was called.</p>\n"},{"tags":["c#","performance","caching","execution"],"answer_count":1,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":60,"score":1,"question_id":12519374,"title":"Why does Directory.GetFiles() run faster on subsequent runs?","body":"<p>I'm not really sure what causes this so please forgive me if I couldn't find the information I needed in a search. Here is an example:</p>\n\n<p>Let's say that we have a folder with 1,000,000 files. Running Directory.GetFiles() on that will take a few minutes.  However, running it again right after will take only a few seconds.  Why does this happen?  Are the objects being cached somewhere?  How can I run it with the original time?</p>\n"},{"tags":["performance","memory","vim","slowdown"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":62,"score":0,"question_id":12518705,"title":"does vim read the whole file into memory","body":"<p>When I open a file, does vim read it all into memory? I experienced significant slowdowns when I open large files. Or is it busy computing something (e.g., line number)?</p>\n"},{"tags":["c","performance","while-loop","msp430"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":83,"score":1,"question_id":12518658,"title":"Reducing Clock Cycle Count in While Loop for Granularity","body":"<p>I have a <code>while</code> loop implemented in C for an MSP430 processor that currently looks like this:</p>\n\n<pre class=\"lang-c prettyprint-override\"><code>register unsigned int sw_loop_count = 0U;\n...\n\nwhile (TACCL0 &amp; CCIE)\n{\n    ++sw_loop_count;\n}\n\n...\n#pragma vector=TIMERA0_VECTOR\n__interrupt void Timer_A(void)\n{\n    // Disable the timer interrupt flag and enable.\n    TACCTL0 &amp;= ~CCIFG;\n    TACCTL0 &amp;= ~CCIE;\n}\n</code></pre>\n\n<p>I'm using this loop for calibration purposes, the context of which I don't think matters too much for my question. I've calculated that each iteration of the loop, including the check <code>TACCL0 &amp; CCIE</code> takes 11 clock cycles. For purposes of granularity, I would really like to get this number as low as possible, and programmatically if possible. I might be being a complete moron, but I can't think of a way of reducing the cycle count for the loop, so any advice would be appreciated. I need the <code>sw_loop_count</code> value, one way or another.</p>\n"},{"tags":["c#","performance","list"],"answer_count":5,"favorite_count":0,"up_vote_count":5,"down_vote_count":1,"view_count":58,"score":4,"question_id":12517910,"title":"The (nearly) best way to manage a list with shifting items","body":"<p>Here is the situation:<br>\nI have list which store strings which are actually numbers and can become pretty big (hundreds of millions of items).<br>\nI store the numbers as string because there is a option to display some additional information which is text.</p>\n\n<p>Because this takes a lot of memory to store I decided that I will store only a maximum of 5 million items. (this will only take about 250-300mb).</p>\n\n<p>The list is filled by the output of a calculation. If a number is found it will be added to the list, this number is always bigger than the existing items.</p>\n\n<p>When the list reached 5 mil I want to remove the first item and add the new item to the list.</p>\n\n<p>like:</p>\n\n<pre><code>    // Why is this so freaking slow???\n    if (_result.Count == 5000000)\n        _result.RemoveAt(0);\n    _result.Add(result);\n</code></pre>\n\n<p>As you can read in the comment, this is very, very, very slow. It just cut down my performance by 15 times. Where it took 2 minutes it now takes about 30.</p>\n\n<p>I tried a few things with linq like <code>.Skip(1).ToList</code> but that will recreate the list and is therefore even more slow.</p>\n\n<p>The list must stay in the right order, so overwriting by index is not an option (unless you could explain a nice work around).</p>\n\n<p>My question:<br>\n<strong>Is there any decent way to do this?</strong></p>\n\n<p>I really need the performance here since it may need to check up about 10000000000 numbers. This may take a day ofcourse, but a month is a bit too much :(.</p>\n\n<p>Need additional information, feel free to ask, I'll be happy to supply.</p>\n\n<p><strong>Solution:</strong><br>\nThis performance O(1)</p>\n\n<pre><code>    // Set the _result\n    Queue&lt;object&gt; _result = new Queue&lt;object&gt;(5000000);\n\n    /// Inside the method\n    // If the count has reach it's max, dequeue the first item\n    if (_result.Count == 5000000)\n        _result.Dequeue();\n    _result.Enqueue(result);\n</code></pre>\n"},{"tags":["javascript","performance","variables","attributes"],"answer_count":3,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":52,"score":1,"question_id":12517614,"title":"Javascript performance: Variable vs Large object's attribute","body":"<p>I am building a number of HTML5 games and I am not sure about one thing in javascript.</p>\n\n<p>When dealing with a large object (with a lot of attributes and methods), is it somehow different, if I store the attribute value in a variable?</p>\n\n<p>Say I have to check for some value in <code>application.data.setings.foo.bar</code> multiple times per second. Should I store it in a variable <code>fooBar</code>? If I understand it correctly, the variable would be just a reference, so it shouldn't matter.</p>\n\n<p>so: <strong><em>Should you store values of large objects' attributes in variables?</em></strong></p>\n"},{"tags":[".net","performance","linq"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":57,"score":2,"question_id":12517501,"title":"LINQ First vs Find performance considerations","body":"<p>We have a system that does all kinds of complex and simple select queries.</p>\n\n<p>We did some simple testing and got these results:</p>\n\n<pre><code>Query 3.9 seconds :\nvar result = (from temp in context.model\n               where temp.ID == 1302\n               select temp).First();\n\nStart Transaction time: 17:54:58.7073806\nEnd Transaction time: 17:55:02.6246046\n\nQuery 3.7 seconds :\nModel modelResult = context.Model.Find(1302);\n\nStart Transaction time: 17:53:51.1995194\nEnd Transaction time: 17:53:54.8737295\n</code></pre>\n\n<p>I have been reading trying to figure out what the best options are. There is a lot of conversation of this topic on this site, however, I have not found exactly what I need.</p>\n\n<p>I know that query choices are situational (based off the complexity of the query, etc) but in the case when we need a single entity based off a key that will not be used again (meaning, in the case of Find, caching doesn't matter because that query is very unlikely to be called again) is it better to use direct LINQ or continue to use Find?  </p>\n\n<p>Is the cost of caching the results of Find, when it won't be used, too costly? Are the results of our simple test accurate enough to assume Find will always be faster in a single entity/key situation?</p>\n\n<p>I didn't test the LINQ with turning off tracing, would this be a better approach than the two examples given?</p>\n"},{"tags":["php","performance","large-files","fgets","splfileobject"],"answer_count":2,"favorite_count":0,"up_vote_count":7,"down_vote_count":0,"view_count":216,"score":7,"question_id":9877447,"title":"Delete first X lines from a file PHP","body":"<p>I was wondering if anyone out there knew how this could be done in PHP. I am running a script that involves opening a file, taking the first 1000 lines, doing some stuff with those lines, then the php file opens another instance of itself to take the next thousand lines and so on until it reaches the end of the file. I'm using splfileobject so that I can seek to a certain line, which allows me to break this up into 1000 line chunks quite well. The biggest problem that I'm having is with performance. I'm dealing with files that have upwards of 10,000,000 lines and while it does the first 10,000 lines or so quite fast, there is a huge exponential slowdown after that point that I think is just having to seek to that point.</p>\n\n<p>What I would like to do is read the first thousand lines, then just delete them from the file so that my script is always reading the first thousand lines. Is there a way to do this without reading the rest of the file into memory. Other solutions I have seen involve reading each line into an array then getting rid of the first X entries, but with ten million lines that will eat up too much memory and time.</p>\n\n<p>If anyone has a solution or other suggestions that would speed up the performance, it would be greatly appreciated.</p>\n"},{"tags":["php","mysql","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":44,"score":0,"question_id":12475037,"title":"improve performance of searching in varchar fields","body":"<p>with every new visit from a search engine, I save the keyword (e.g. \"php performance\") the visitor searched for at the search engine. </p>\n\n<p>To count the number how often the keyword is used per day, I need to check whether somebody else already searched for the term that specific day.</p>\n\n<p>After several months the table gets very big and the mysql check for the existence of the keyword that day takes longer and longer. So I switched to an extra table where I only save the keyword and give each keyword a specific ID to only have the checking process once and not each new day. This specific ID which I related to the keyword I insert in the table with diversification for each day.</p>\n\n<p>This already works better but this table also gets bigger and bigger...</p>\n\n<p>Does somebody has a good solution for this to higher performance?</p>\n\n<p>Best,</p>\n\n<p>Freddy</p>\n"},{"tags":["android","eclipse","osx","performance"],"answer_count":4,"favorite_count":17,"up_vote_count":23,"down_vote_count":0,"view_count":7897,"score":23,"question_id":2787055,"title":"Using the Android SDK on a Mac, Eclipse is really slow. How can I speed it up?","body":"<p>I'm using Eclipse + the Android SDK on a Mac running Snow Leopard to develop Android apps.</p>\n\n<p>Thing is, Eclipse is really slow - like, it \"beach balls\" for a few seconds when changing tabs.</p>\n\n<p>Is there anything I can do to improve it's performance?</p>\n"},{"tags":["java","performance","graphics","background"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":39,"score":1,"question_id":12516875,"title":"Java: How do I have a background drawn in a game under the double buffer?","body":"<p>I have a game in java and have all my objects drawn on a double buffer and wanted to draw a background but don't want the background redrawn every frame, how do I only draw it once?</p>\n"},{"tags":["c++","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":50,"score":0,"question_id":12515073,"title":"Is there a more efficient solution for this two consecutive groups of the same loops?","body":"<p>I often find my self writing these pieces of code, specifically when I have to do something in a 2D array.</p>\n\n<p>The loops are the same, except the operations inside are different and, most importantly, the operation in the last group depends on the first.</p>\n\n<p>My main concern is: is there a more efficient code for large values of n,m?</p>\n\n<pre><code>for ( int y = 0 ; y &lt; m ; ++y ) {\n  for ( int x = 0 ; x &lt; n ; ++x ) {\n    if ( v[x][y] == z ) a = true;\n  }\n}\n\nfor ( int y = 0 ; y &lt; m ; ++y ) {\n  for ( int x = 0 ; x &lt; n ; ++x ) {\n    if ( a == true ) do_something( v[x][y] );\n  }\n}\n</code></pre>\n\n<p>Thanks in advance</p>\n"},{"tags":["c#","asp.net","database","performance","datacontext"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":71,"score":0,"question_id":12516637,"title":"In ASP.net is it best to close off datacontext as soon as possible?","body":"<p>Given this code:</p>\n\n<pre><code>/// &lt;summary&gt;\n/// Add to view count of this article\n/// &lt;/summary&gt;\npublic static void IncrementViewCount(int articleID)\n{\n    using (var db = new MainContext())\n    {\n        var q = (from c in db.tblArticles where c.ID == articleID select c).SingleOrDefault();\n        if (q != null)\n        {\n            q.Views ++;\n            db.SubmitChanges();\n\n            if (q.Views == 500)\n            {\n                // Call function\n            }\n        }\n    }\n}\n</code></pre>\n\n<p>Is it better to write it in the follow way:</p>\n\n<pre><code>/// &lt;summary&gt;\n/// Add to view count of this article\n/// &lt;/summary&gt;\npublic static void IncrementViewCount(int articleID)\n{\n    var newViews = 0;\n    using (var db = new MainContext())\n    {\n        var q = (from c in db.tblArticles where c.ID == articleID select c).SingleOrDefault();\n        if (q != null)\n        {\n            newViews = q.Views + 1;\n            q.Views = newViews;\n            db.SubmitChanges();\n        }\n    }\n    if (newViews == 500)\n    {\n        // Call function\n    }\n}\n</code></pre>\n\n<p>Note in example #2, the using block is closed at an earlier point.</p>\n"},{"tags":["sql","database","performance","oracle","explain-plan"],"answer_count":11,"favorite_count":17,"up_vote_count":45,"down_vote_count":0,"view_count":15046,"score":45,"question_id":79266,"title":"How do you interpret a query's explain plan?","body":"<p>When attempting to understand how a SQL statement is executing, it is sometimes recommended to look at the explain plan. What is the process one should go through in interpreting (making sense) of an explain plan? What should stand out as, \"Oh, this is working splendidly?\" versus \"Oh no, that's not right.\"</p>\n"},{"tags":["performance","perl","reference","alias","copy-on-write"],"answer_count":5,"favorite_count":1,"up_vote_count":1,"down_vote_count":0,"view_count":102,"score":1,"question_id":12514234,"title":"alias a hash element in perl","body":"<p>Is it possible to access the same value under different hash keys?  How can I tell Perl not to copy the \"very long text?\"</p>\n\n<pre><code>$hash-&gt;{'key'} = 'very long text';\n$hash-&gt;{'alias'} = $hash-&gt;{'key'};\n</code></pre>\n"},{"tags":["performance","cuda","benchmarking","gpu"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":240,"score":2,"question_id":12508704,"title":"is GTX 690 slower than GTX 480 for computing?","body":"<p>I have a GTX 690 and when I am using one card, I get performance slightly slower than GTX480 on cuda benchmarks. Is this behavior expected? I am getting 16000 graphics score on 3D Mark and 9000 on phsX(using both cards of 690). </p>\n\n<p>However when I run N-body simulation with 8192 particles in gpu computing sdk browser, I get around 600GFlops/s(using 1 card of GTX 690). I also get 600GFlops/s with a GTX 480. Isn't the new kepler card(fully activated GK104 chip) expected to give 2 tera flops single precision performance? I would really appreciate if some body could post the GFlops/s for some of the benchmarks in gpu computing browser for a GTX 690 or GTX 680.</p>\n\n<p>I agree geforce cards are not meant for computing but if it is single precision and if you dont have extra memory requirements, hardly matters if it is tesla or geforce. Correct me if I am wrong.</p>\n"},{"tags":["python","regex","performance","python-2.7"],"answer_count":1,"favorite_count":1,"up_vote_count":2,"down_vote_count":0,"view_count":74,"score":2,"question_id":12514157,"title":"How does Python's regex pattern caching work?","body":"<p>From the Python docs for <a href=\"http://docs.python.org/library/re.html#re.compile\" rel=\"nofollow\"><code>re.compile()</code></a>:</p>\n\n<blockquote>\n  <p>Note The compiled versions of the most recent patterns passed to\n  re.match(), re.search() or re.compile() are cached, so programs that\n  use only a few regular expressions at a time needn’t worry about\n  compiling regular expressions.</p>\n</blockquote>\n\n<p>However, in my testing, this assertion doesn't seem to hold up. When timing the following snippets that use the same pattern repeatedly, the compiled version is still substantially faster than the uncompiled one (which should supposedly be cached).</p>\n\n<p>Is there something I am missing here that explains the time difference?</p>\n\n<pre><code>import timeit\n\nsetup = \"\"\"\nimport re\npattern = \"p.a.t.t.e.r.n\"\ntarget = \"p1a2t3t4e5r6n\"\nr = re.compile(pattern)\n\"\"\"\n\nprint \"compiled:\", \\\n    min(timeit.Timer(\"r.search(target)\", setup).repeat(3, 5000000))\nprint \"uncompiled:\", \\\n    min(timeit.Timer(\"re.search(pattern, target)\", setup).repeat(3, 5000000))\n</code></pre>\n\n<p>Results:</p>\n\n<pre><code>compiled: 2.26673030059\nuncompiled: 6.15612802627\n</code></pre>\n"},{"tags":["c#","wpf","performance","wpf-controls","uielementcollection"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":115,"score":1,"question_id":12513989,"title":"Rapidly removing (and adding) many items to a Canvas in WPF","body":"<p>At a certain point in my WPF application, the UI thread will lock up for ~0 till ~500 ms (depending on the amount) once I remove and/or add items to a Canvas. Several performance tests pointed to Canvas.Children.Remove for the main cause, and .Add as well (.Remove is much more severe).\nI remove about 500 items at the same time, and add about 500 items at the same time as well. This about 10 times per second causes issues.</p>\n\n<p>In the end, I've written a simple benchmarking application, code below.</p>\n\n<pre><code>    public MainWindow() {\n        InitializeComponent();\n        this.Loaded += (object sender, RoutedEventArgs e) =&gt; {\n            List&lt;Image&gt; a = new List&lt;Image&gt;();\n            Stopwatch s = new Stopwatch();\n            s.Start();\n            for (int i = 0; i &lt; 7500; i++) {\n                Image img = new Image();\n                Canvas.SetLeft(img, i * 10);\n                C.Children.Add(img);\n                if (i % 10 == 1)\n                    a.Add(img);\n            }\n            s.Stop();\n            long add = s.ElapsedMilliseconds;\n            s.Reset();\n            s.Start();\n            foreach (Image img in a) {\n                C.Children.Remove(img);\n            }\n            s.Stop();\n            MessageBox.Show(String.Format(\"Add: {0}, Remove: {1}.\", add, s.ElapsedMilliseconds));\n        };\n    }\n</code></pre>\n\n<p>This gives the following result (around this, every time I run it):</p>\n\n<pre><code>Add: 174, Remove: 156.\n</code></pre>\n\n<p>Keep in mind only 750 items get removed, while 7500 get added.</p>\n\n<p>The process is a bit heavy, but I don't want the UI (<code>ScrollViewer</code> mainly) to lock up while it's doing this. Another problem I face is that I can't \"just move\" this to another <code>Thread</code>, since I can't control the UI from that specific <code>Thread</code>.</p>\n\n<p>How to improve this process? Or is there a way to dynamically add/remove items \"over time\" so it does not hang up?</p>\n\n<p>Thanks,</p>\n\n<p>~Tgys</p>\n"},{"tags":["c#","performance","object","reuse"],"answer_count":2,"favorite_count":0,"up_vote_count":3,"down_vote_count":1,"view_count":120,"score":2,"question_id":12512231,"title":"C# best practice for performance: object creation vs reuse","body":"<p>I am currently working on performance tuning of an existing C# website. There is a class say.. MyUtil.cs. This class has been extensively used across all web pages. On some pages around 10/12 instances are created (MyUtil).  I ran the \"Redgate\" performance profiler. The object creation is a costly operation according to RedGate. \nPlease note that each instance sets specific properties and performs specific operation. So I can not reuse the object as it is. I have to reset all the member variables.</p>\n\n<p>I want to optimize this code. I have thought about following options. Kindly help me evaluate which is the better approach here : </p>\n\n<p>(1) Create a \"Reset\" method in \"MyUtil.cs\" class which will reset all the member variables (there are 167 of those :(..) so that I can reuse one single object in an page class.</p>\n\n<p>(2) Continue with the multiple object creation (I do have Dispose() method in \"MyUtil\")</p>\n\n<p>(3) I thought of \"object pooling\" but again I will have to reset the members. I think its better to pool objects at page level and release them instead of keeping them live at the project level.</p>\n\n<p>Any reply on this will be appreciated. Thanks in advance..!</p>\n"},{"tags":["python","performance","optimization"],"answer_count":6,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":1121,"score":6,"question_id":2985797,"title":"Tips for making a fraction calculator code more optimized (faster and using less memory)","body":"<p>Basicly, what I need for the program to do is to act a as simple fraction calculator (for addition, subtraction, multiplication and division) for the a single line of input, for example:<br>\n-input: <code>1/7 + 3/5</code><br>\n-output: <code>26/35</code></p>\n\n<p>My initial code:</p>\n\n<pre><code>import sys\n\ndef euclid(numA, numB):\n    while numB != 0:\n        numRem = numA % numB\n        numA = numB\n        numB = numRem\n    return numA\n\nfor wejscie in sys.stdin:\n    wyjscie = wejscie.split(' ')\n    a, b = [int(x) for x in wyjscie[0].split(\"/\")]\n    c, d = [int(x) for x in wyjscie[2].split(\"/\")]\n    if wyjscie[1] == '+':\n        licz = a * d + b * c\n        mian= b * d\n        nwd = euclid(licz, mian)\n        konA = licz/nwd\n        konB = mian/nwd\n        wynik = str(konA) + '/' + str(konB)\n        print(wynik)\n    elif wyjscie[1] == '-':\n        licz= a * d - b * c\n        mian= b * d\n        nwd = euclid(licz, mian)\n        konA = licz/nwd\n        konB = mian/nwd\n        wynik = str(konA) + '/' + str(konB)\n        print(wynik)\n    elif wyjscie[1] == '*':\n        licz= a * c\n        mian= b * d\n        nwd = euclid(licz, mian)\n        konA = licz/nwd\n        konB = mian/nwd\n        wynik = str(konA) + '/' + str(konB)\n        print(wynik)\n    else:\n        licz= a * d\n        mian= b * c\n        nwd = euclid(licz, mian)\n        konA = licz/nwd\n        konB = mian/nwd\n        wynik = str(konA) + '/' + str(konB)\n        print(wynik)\n</code></pre>\n\n<p>Which I reduced to:</p>\n\n<pre><code>import sys\n\ndef euclid(numA, numB):\n    while numB != 0:\n        numRem = numA % numB\n        numA = numB\n        numB = numRem\n    return numA\n\nfor wejscie in sys.stdin:\n    wyjscie = wejscie.split(' ')\n    a, b = [int(x) for x in wyjscie[0].split(\"/\")]\n    c, d = [int(x) for x in wyjscie[2].split(\"/\")]\n    if wyjscie[1] == '+':\n        print(\"/\".join([str((a * d + b * c)/euclid(a * d + b * c, b * d)),str((b * d)/euclid(a * d + b * c, b * d))]))\n    elif wyjscie[1] == '-':\n        print(\"/\".join([str((a * d - b * c)/euclid(a * d - b * c, b * d)),str((b * d)/euclid(a * d - b * c, b * d))]))\n    elif wyjscie[1] == '*':\n        print(\"/\".join([str((a * c)/euclid(a * c, b * d)),str((b * d)/euclid(a * c, b * d))]))\n    else:\n        print(\"/\".join([str((a * d)/euclid(a * d, b * c)),str((b * c)/euclid(a * d, b * c))]))\n</code></pre>\n\n<p>Any advice on how to improve this futher is welcome.</p>\n\n<p>Edit: one more thing that I forgot to mention - the code can not make use of any libraries apart from sys.</p>\n"},{"tags":["performance","openlayers"],"answer_count":1,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":80,"score":1,"question_id":10845258,"title":"OpenLayers - best way to display a lots of data?","body":"<p>I have a slippy map over which I want to display a overlay with some highways. I have a PHP script that is getting filtered data from my local Overpass API server, send that data to OpenLayers and then visualise it in a Vector layer with a basic styling (red, yellow or green line color, based on the smoothness tag of the highway). It works, but it doesn't scale well when there is a lot of data to be displayed. I'm talking for about 2600 ways composed of 83000 nodes. The SVG render is struggling even on comparatively new computers.</p>\n\n<p>I don't need any fancy interactive features, the goal is just to display that data.</p>\n\n<p>I'm thinking of a way to render transparent tiles with these ways and then just load these tiles as overlay in OpenLayers (maybe with TMS layer?). Maybe Mapnik is going to be useful here?</p>\n\n<p>What will be the best way to display such a load?</p>\n"},{"tags":["erlang","performance","hook","commit","riak"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":219,"score":0,"question_id":5380454,"title":"Implementing EFFICIENTLY a riak pre-commit hook that does get and put","body":"<p>I implemented a pre-commit hook for riak which gets and updates a specific \"meta\" key whenever commits satisfy a specific criteria. I have noticed however, that doing Client:get to retrieve the \"meta\" key takes long, and seriously slows down my commits (about 20 times slower). Are there any advices on how to do this correctly? I currently call riak:local_client in the commit hook and then do riak_client:get.</p>\n"},{"tags":["mysql","performance","merge","index","database-indexes"],"answer_count":2,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":51,"score":2,"question_id":12509568,"title":"adding an additional item in SQL statement causes slow performance","body":"<p>I have a sql statement that looks like this:</p>\n\n<pre><code>SELECT colID\nFROM tableName\nWHERE ColDateStart &lt;='$lowerDate'\nAND ColDateStart&gt;='$upperDate'\nAND ColVcamID='$id1'\nAND ColVlviID='$id2'\nAND ColSomeID='$id3';\n</code></pre>\n\n<p>All the columns in the WHERE statment are indexed columns.</p>\n\n<p>When I run this it takes over a second.  However when I run this without other Id3, the performance is considerably improved (0.03 seconds).</p>\n\n<p>When I run explain, with otherId3, it uses an index merge using otherId1 and otherId3.  However when I remove the otherId3, it uses the single index of otherId2.</p>\n\n<p>Why does adding otherId3 make an impact on the performance?</p>\n\n<p>Table Structure:</p>\n\n<pre><code>+----------------------+-------------+------+-----+---------------------+----------------+\n| Field                | Type        | Null | Key | Default             | Extra          |\n+----------------------+-------------+------+-----+---------------------+----------------+\n| ColID                | int(11)     | NO   | PRI | NULL                | auto_increment |\n| ColCustID            | int(11)     | NO   | MUL | 0                   |                |\n| ColCarrID            | int(11)     | NO   | MUL | NULL                |                |\n| ColTariID            | int(11)     | NO   | MUL | 0                   |                |\n| ColCarrierRef        | varchar(30) | NO   | MUL |                     |                |\n| ColNumbID            | int(11)     | NO   | MUL | 0                   |                |\n| ColVlviID            | int(11)     | NO   | MUL | NULL                |                |\n| ColVcamID            | int(11)     | NO   | MUL | NULL                |                |\n| ColSomeID            | int(11)     | NO   | MUL | NULL                |                |\n| ColVlnsID            | int(11)     | NO   | MUL | NULL                |                |\n| ColNGNumber          | varchar(12) | NO   |     |                     |                |\n| ColOrigNumber        | varchar(16) | NO   | MUL | NULL                |                |\n| ColCLIRestrictedFlag | int(2)      | NO   |     | NULL                |                |\n| ColOrigLocality      | varchar(11) | NO   | MUL |                     |                |\n| ColOrigAreaCode      | varchar(11) | NO   | MUL |                     |                |\n| ColTermNumber        | varchar(16) | NO   | MUL | NULL                |                |\n| ColBatchNumber       | varchar(10) | NO   |     |                     |                |\n| ColDateStart         | date        | NO   | MUL | 0000-00-00          |                |\n| ColDateClear         | date        | NO   |     | 0000-00-00          |                |\n| ColTimeStart         | time        | NO   |     | 00:00:00            |                |\n| ColTimeClear         | time        | NO   |     | 00:00:00            |                |\n| ColCallLength        | time        | NO   |     | 00:00:00            |                |\n| ColRingLength        | time        | NO   |     | 00:00:00            |                |\n| ColEffectiveFlag     | smallint(1) | NO   | MUL | NULL                |                |\n| ColUnansweredFlag    | smallint(1) | NO   | MUL | NULL                |                |\n| ColEngagedFlag       | smallint(1) | NO   |     | NULL                |                |\n| ColRecID             | int(11)     | NO   | MUL | NULL                |                |\n| ColCreatedUserID     | int(11)     | NO   |     | 0                   |                |\n| ColCreatedDatetime   | datetime    | NO   | MUL | 0000-00-00 00:00:00 |                |\n| ColDirection         | int(1)      | NO   | MUL | NULL                |                |\n+----------------------+-------------+------+-----+---------------------+----------------+\n</code></pre>\n\n<p>Indexes</p>\n\n<pre><code>+-------+------------+-------------------------------+--------------+---------------------+-----------+-------------+----------+--------+------+------------+---------+\n| Table | Non_unique | Key_name                      | Seq_in_index | Column_name         | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment |\n+-------+------------+-------------------------------+--------------+---------------------+-----------+-------------+----------+--------+------+------------+---------+\n| tableName |          0 | PRIMARY                       |            1 | ColID              | A         |    18031283 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_ColCustID                |            1 | ColCustID          | A         |        1339 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_ColNumbID                |            1 | ColNumbID          | A         |       24366 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colOrigNumber            |            1 | colOrigNumber      | A         |     4507820 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colOrigLocality          |            1 | colOrigLocality    | A         |       36873 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colOrigAreaCode          |            1 | colOrigAreaCode    | A         |         696 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colTermNumber            |            1 | colTermNumber      | A         |      137643 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colDateStart             |            1 | colDateStart       | A         |        3639 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colEffectiveFlag         |            1 | colEffectiveFlag   | A         |           2 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colUnansweredFlag        |            1 | colUnansweredFlag  | A         |           2 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colEngagedFlag           |            1 | colUnansweredFlag  | A         |           2 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colTariID                |            1 | colTariID          | A         |          91 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_CustID_DateStart          |            1 | colCustID          | A         |        1339 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_CustID_DateStart          |            2 | colDateStart       | A         |      693510 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_NumbID_DateStart          |            1 | colNumbID          | A         |       24366 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_NumbID_DateStart          |            2 | colDateStart       | A         |     4507820 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colRecID                 |            1 | colRecID           | A         |      214658 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colCarrierRef            |            1 | colCarrierRef      | A         |     6010427 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colCustID_colTermNumber |            1 | colCustID          | A         |        1339 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colCustID_colTermNumber |            2 | colTermNumber      | A         |      143105 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colCreatedDatetime       |            1 | colCreatedDatetime | A         |      474507 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colDirection             |            1 | colDirection       | A         |           2 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colVlviID                |            1 | colVlviID          | A         |        4133 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colSomeID                |            1 | colSomeID          | A         |          10 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colVcamID                |            1 | colVcamID          | A         |           7 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colVlnsID                |            1 | colVlnsID          | A         |          18 |     NULL | NULL   |      | BTREE      |         |\n| tableName |          1 | idx_colCarrID                |            1 | colCarrID          | A         |           4 |     NULL | NULL   |      | BTREE      |         |\n+-------+------------+-------------------------------+--------------+---------------------+-----------+-------------+----------+--------+------+------------+---------+\n</code></pre>\n"},{"tags":["c++","performance","map","unordered-map"],"answer_count":6,"favorite_count":17,"up_vote_count":35,"down_vote_count":0,"view_count":11430,"score":35,"question_id":2196995,"title":"Is there any advantage of using map over unordered_map in case of trivial keys?","body":"<p>A recent talk about <code>unordered_map</code> in C++ made me realize, that I should use <code>unordered_map</code> for most cases where I used <code>map</code> before, because of the efficiency of lookup ( <em>amortized O(1)</em> vs. <em>O(log n)</em> ). Most times I use a map I use either <code>int</code>'s or <code>std::strings</code> as keys, hence I've got no problems with the definition of the hash function. The more I thought about it, the more I came to realize that I can't find any reason of using a <code>std::map</code> in case of simple types over a <code>unordered_map</code> -- I took a look at the interfaces, and didn't find any significant differences that would impact my code. </p>\n\n<p>Hence the question - is there any real reason to use <code>std::map</code> over <code>unordered map</code> in case of simple types like <code>int</code> and <code>std::string</code>?</p>\n\n<p>I'm asking from a strictly programming point of view -- I know that it's not fully considered standard, and that it may pose problems with porting. </p>\n\n<p>Also I expect that one of the correct answers might be <em>\"it's more efficient for smaller sets of data\"</em> because of a smaller overhead (is that true?) -- hence I'd like to restrict the question to cases where the amount of keys is non-trivial (>1 024).</p>\n\n<p><strong>Edit:</strong> <em>duh, I forgot the obvious (thanks GMan!) -- yes, map's are ordered of course -- I know that, and am looking for other reasons.</em></p>\n"},{"tags":["php","mysql","performance","global"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":49,"score":0,"question_id":12509308,"title":"global VS function call to mysql inside function","body":"<p>What would you advise me to use performance wise between the following</p>\n\n<p>A)</p>\n\n<pre><code>function db(){ return new mysqli('localhost','user','pass','db'); }\n\n//global scope\n$db = db();\n\nfunction foo(){\n//function scope\n$db = db();\n\n[...]\n\n}\n</code></pre>\n\n<p>B)</p>\n\n<pre><code>//global scope\n$db = new mysqli('localhost','user','pass','db');\n\nfunction bar(){\n//function scope\nglobal $db\n\n[...]\n\n}\n</code></pre>\n\n<p>At the moment I'm using method A but I'm aware there is a overhead in calling a function and db() is called in most functions, so I was wondering.</p>\n"},{"tags":["sql","performance","oracle","sql-update"],"answer_count":5,"favorite_count":0,"up_vote_count":2,"down_vote_count":0,"view_count":109,"score":2,"question_id":12506430,"title":"Horrible Oracle update performance","body":"<p>I am performing an update with a query like this:</p>\n\n<pre><code>UPDATE (SELECT     h.m_id,\n                   m.id\n        FROM       h\n        INNER JOIN m\n        ON         h.foo = m.foo)\nSET    m_id = id\nWHERE  m_id IS NULL\n</code></pre>\n\n<p>Some info:</p>\n\n<ul>\n<li>Table <code>h</code> is roughly ~5 million rows</li>\n<li>All rows in table <code>h</code> have <code>NULL</code> values for <code>m_id</code></li>\n<li>Table <code>m</code> is roughly ~500 thousand rows</li>\n<li><code>m_id</code> on table <code>h</code> is an indexed foreign key pointing to <code>id</code> on table <code>m</code></li>\n<li><code>id</code> on table <code>m</code> is the primary key</li>\n<li>There are indexes on <code>m.foo</code> and <code>h.foo</code></li>\n</ul>\n\n<p>The <code>EXPLAIN PLAN</code> for this query indicated a hash join and full table scans, but I'm no DBA, so I can't really interpret it very well.</p>\n\n<p>The query itself ran for several hours and did not complete.  I would have expected it to complete in no more than a few minutes.  I've also attempted the following query rewrite:</p>\n\n<pre><code>UPDATE h\nSET    m_id = (SELECT id\n               FROM   m\n               WHERE  m.foo = h.foo)\nWHERE  m_id IS NULL\n</code></pre>\n\n<p>The <code>EXPLAIN PLAN</code> for this mentioned ROWID lookups and index usage, but it also went on for several hours without completing.  I've also always been under the impression that queries like this would cause the subquery to be executed for every result from the outer query's predicate, so I would expect very poor performance from this rewrite anyway.</p>\n\n<p><em>Is there anything wrong with my approach, or is my problem related to indexes, tablespace, or some other non-query-related factor?</em></p>\n\n<p><strong>Edit:</strong></p>\n\n<p>I'm also having abysmal performance from simple count queries like this:</p>\n\n<pre><code>SELECT COUNT(*)\nFROM   h\nWHERE  m_id IS NULL\n</code></pre>\n\n<p>These queries are taking anywhere from ~30 seconds to sometimes ~30 minutes(!).</p>\n\n<p>I am noticing no locks, but the tablespace for these tables is sitting at 99.5% usage (only ~6MB free) right now.  I've been told that this shouldn't matter as long as indexes are being used, but I don't know...</p>\n"},{"tags":["mysql","performance","query"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":32,"score":0,"question_id":12507789,"title":"query takes too much time while sorting in MySQL","body":"<p>Query :\nselect field1 as Word,count(*) as Count from TestAdd1214190 group by field1 order by count desc</p>\n\n<p>Btree Index created on field1 column</p>\n\n<p>Data In table 50 lac</p>\n\n<p>Engine : MEMORY</p>\n\n<p>Explain\nTYPE: SIMPLE</p>\n\n<p>USING INDEX;USING TEMPORORY;USING FILESORT</p>\n\n<p>Please suggest how do i make this query fast</p>\n"},{"tags":["javascript","css","performance","html5","css-selectors"],"answer_count":2,"favorite_count":3,"up_vote_count":7,"down_vote_count":1,"view_count":112,"score":6,"question_id":12496884,"title":"Are data attribute css selectors faster than class selectors?","body":"<p>A few months ago <a href=\"http://coding.smashingmagazine.com/2012/06/19/classes-where-were-going-we-dont-need-classes/\" rel=\"nofollow\">this article</a> pointed out that classes could actually be avoided all together from website development. </p>\n\n<p>My question is, how efficient are the data selectors compared to class selectors? </p>\n\n<p>A simple example would be to compare querying for elements with <code>data-component='something'</code> versus elements with <code>class='class1 class2 something anotherClass'</code>.</p>\n\n<p>The <code>[data-&lt;attr&gt;='&lt;value&gt;']</code> selector will check the value as a whole versus the class string that should be split. With this in mind, data atributes should be faster.</p>\n\n<p>So, to refine the question, in the case of CSS, are we better off with class selector or data selector? And from a javascript point of view, will <code>jQuery(\"[data-component='something']\")</code> be more efficient than <code>jQuery(\".something\")</code>?</p>\n"},{"tags":[".net","performance","networking","tcp","network-programming"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":53,"score":0,"question_id":12503865,"title":"Are multiple repeated TcpClient connections a performance bottleneck?","body":"<p>I'm wondering to what degree establishing repeated TCP connections with .NET's TcpClient could affect application performance.</p>\n\n<p>The application I am working on was originally a client/server application designed to have the clients and server on the same LAN network.</p>\n\n<p>We've removed the ODBC database connections on the client and are using a custom server that serves binary data using Protocol Buffers after reading it from the Database. </p>\n\n<p>Our code makes a connection to the server for each SQL statement, and the serialized result is returned in a structure that resembles a DataTable.</p>\n\n<p>On our company network the application is quite fast, however we have begun testing with remote clients and are having latency issues. Our server is on the east coast and our remote clients are on the west coast.</p>\n\n<p>I suspect that the overhead of opening up a connection for each SQL statement is actually a bottleneck for distant clients, however I am not certain about this.</p>\n\n<p><strong>My question: is opening TCP connections repeatedly a performance bottleneck? Would a persistent connection improve performance? And what gotchas would there be if we went to persistent connections?</strong></p>\n"},{"tags":["c#",".net","performance","ado.net","fill"],"answer_count":0,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":92,"score":0,"question_id":12505994,"title":"Fill() Method becomes slow in .net (ADO)","body":"<p>In my .net application am facing a <code>performance problem</code>. One of my Stored procedure dealing with more than <code>two tables having 30,000 records</code>. But the <code>SP itself returns fastly</code>. but in my <code>DataAccesss</code> it will take more time for fill that into <code>DataTable</code></p>\n\n<p>My datataccess is</p>\n\n<pre><code>cmd.CommandType = CommandType.StoredProcedure;//cmd-sqlcommand object\nconn.Open();//conn-sqlconnection object\nadpt = new SqlDataAdapter(cmd);\nDataTable dt = new DataTable();\nadpt.Fill(dt);\nconn.Close();\n\nreturn dt;\n</code></pre>\n\n<p>If any body know the solution for better performance please share.</p>\n"},{"tags":[".net","performance","entity-framework","ef-code-first"],"answer_count":1,"favorite_count":2,"up_vote_count":3,"down_vote_count":0,"view_count":87,"score":3,"question_id":12493965,"title":"EF Code First and Performance With Large Model","body":"<p>Previously, with an EF model, while there was no restriction on the number of entities in a model, it was said that for the best performance, <a href=\"http://thedatafarm.com/blog/data-access/entity-framework-designer-and-large-databases/\" rel=\"nofollow\">a model should be limited to 200 or 400 entitie</a>s.  While you could have an EDMX with over 400 entities, EF may <a href=\"http://blogs.msdn.com/b/adonet/archive/2008/11/24/working-with-large-models-in-entity-framework-part-1.aspx\" rel=\"nofollow\">slow down</a> as a result.</p>\n\n<p>With Code First, is there recommended size limit on a model?  Do we have a cap on the number of entities we can use in a model, before potentially running into performance problems?  I could not find information on this...</p>\n\n<p>Thanks.</p>\n"},{"tags":["mysql","sql","performance"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":43,"score":0,"question_id":12498204,"title":"Limit SQL when I can, or not?","body":"<p>I have many queries I know should return 1 or 2 results. Should I explicitly LIMIT them or just let the DB do his job?\nThe scenario I mean is I am WHEREing on a unique combination of keys, OR when I might have the same result many times, just do LIMIT 1 instead of using DISTINCT...</p>\n"},{"tags":["performance","assembly","x86","padding"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":81,"score":1,"question_id":12501434,"title":"Is there any real benefit to padding instructions","body":"<p>Knowing that Intel and AMD processors fetch instructions in their native word length (64-bit mainly nowadays), I asked my brother about it and he said that to get the processor to run more efficiently, some assembly programmers pad their instructions to 32 bits with <code>nop</code>s if the next instruction will put the byte length at more than 4 or 8 bytes:</p>\n\n<pre><code>xor ax, ax ; 2 bytes\nnop ; 1\nnop ; 1\n</code></pre>\n\n<p>So is there any benefit to doing this?</p>\n"},{"tags":["javascript","performance","operators","post-increment","pre-increment"],"answer_count":2,"favorite_count":0,"up_vote_count":1,"down_vote_count":0,"view_count":82,"score":1,"question_id":12504765,"title":"Is there a performance difference between i++ and ++i in JavaScript?","body":"<p>I read <a href=\"http://stackoverflow.com/questions/24886/is-there-a-performance-difference-between-i-and-i-in-c\">Is there a performance difference between i++ and ++i in C?</a>:</p>\n\n<blockquote>\n  <p>Is there a performance difference between i++ and ++i if the resulting\n  value is not used?</p>\n</blockquote>\n\n<p>What's the answer for JavaScript?</p>\n\n<p>For example, which of the following is better?</p>\n\n<p>1)</p>\n\n<pre><code>for(var i=0;i&lt;max;i++){\n    //code\n}\n</code></pre>\n\n<p>2)</p>\n\n<pre><code>for(var i=0;i&lt;max;++i){\n    //code\n}\n</code></pre>\n"},{"tags":["c","linux","multithreading","performance","pthreads"],"answer_count":1,"favorite_count":0,"up_vote_count":2,"down_vote_count":1,"view_count":76,"score":1,"question_id":12503659,"title":"Thread interference hurting performance on simple for loop","body":"<p>I'm using two different threading libraries on the same benchmark program. One library is pthreads and the other is a research project I'm working on. The \"threads\" in my library are actually processes, in that they don't share heap or global memory. When 2 threads execute the benchmark program, which is a simple for loop, the performance is equivalent. When I bump it up to 3 threads, I see <em>my</em> libraries performance start to suffer. Its about 2x slower. I'm sampling around the for loop like so:</p>\n\n<pre><code>clock_gettime(CLOCK_REALTIME, &amp;t1);\nfor (k=0;k&lt;num;k++);\nclock_gettime(CLOCK_REALTIME, &amp;t2);\n</code></pre>\n\n<p>If I run this for loop inside a mutex, the performance is equivalent to pthreads consistently. However, running them side by side causes occasional slow downs like I mentioned.</p>\n\n<p>Here's what I know. (1) no page faults are happening. (2) I'm setting declaring the loop variables integers as \"int register\".</p>\n\n<p>So I guess the question is, what could cause two seemingly non-conflicting programs to slow down? I guess I should mention that either <em>both</em> slow down or neither do. I'm at my wit's end on this so any guesses or advice on troubleshooting would be greatly appreciated.</p>\n\n<p>EDIT - I have four cores on my system and the slowdown can be seen without the clock_gettime calls. Also, it <em>never</em> happens for pthreads, just when using my library.</p>\n"},{"tags":["php","mysql","query","performance"],"answer_count":3,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":313,"score":0,"question_id":7132258,"title":"Efficient MySQL Query: running various queries through multiple tables, assigning result tables to array","body":"<p>Is there a way of running a single MySQL query, which selects data from various tables and returns each table individually that can be assigned to an array, in PHP, instead of a query for each table and assigning the results manually to the array?</p>\n\n<p>For instance</p>\n\n<pre><code>// Interpretation: \n$multiple       = mysql_query(\"select * from table_a; select * from table_b;\");\nforeach ($multiple as $table =&gt; $results) {\n    $tables[$table] = $results;\n}\n</code></pre>\n\n<p>instead of:</p>\n\n<pre><code>$tables         = array (\n    \"table_a\"   =&gt; mysql_query (\"select * from table_a;\"),\n    \"table_b\"   =&gt; mysql_query (\"select * from table_b;\"),\n);\n</code></pre>\n\n<p>Any guidance will be most appreciated. Thank you.</p>\n"},{"tags":["php","mysql","performance","query"],"answer_count":4,"favorite_count":1,"up_vote_count":0,"down_vote_count":0,"view_count":126,"score":0,"question_id":9596055,"title":"PHP MySQL INSERT 1-3,000 rows as quickly and efficently as possible","body":"<p>I am looking for the fastest way to <code>INSERT</code> 1-3,000 rows into a MySQL database using PHP. My current solution is taking around 42 seconds to insert the rows which I think that could be much faster.</p>\n\n<p>I am using a self-written DB class, the <code>insert()</code> method takes two params <code>(string) $table</code> and <code>(array) $vars</code>. The <code>$items</code> array is an associative array where the key is the column name in the table and the value is the value to insert. This works really well for because I sometimes have 30 columns in a table and already have the data there in an array. The <code>insert()</code> method is below:</p>\n\n<pre><code>    function insert($table,$vars) {\n        if(empty($this-&gt;sql_link)){\n            $this-&gt;connection();\n        }\n        $cols = array();\n        $vals = array();\n        foreach($vars as $key =&gt; $value) {\n            $cols[] = \"`\" . $key . \"`\";\n            $vals[] = \"'\" . $this-&gt;esc($value) . \"'\";\n        }\n        //join the columns and values to insert into sql\n        $fields = join(', ', $cols);\n        $values = join(', ', $vals);\n\n        $insert = mysql_query(\"INSERT INTO `$table` ($fields) VALUES ($values);\", $this-&gt;sql_link);\n        return $insert;\n}\n</code></pre>\n\n<p>It should be self-explanatory but basically I take the keys and values from $vars and create an INSERT statement. It works, I think the problem I am having is sending the queries one at a time.</p>\n\n<p>Should I build a long query string?</p>\n\n<p><code>INSERT INTO table (field, field2, etc) VALUES (1, 2, ect);INSERT INTO table (field, field2, etc) VALUES (1, 2, ect);INSERT INTO table (field, field2, etc) VALUES (1, 2, ect);INSERT INTO table (field, field2, etc) VALUES (1, 2, ect);INSERT INTO table (field, field2, etc) VALUES (1, 2, ect);</code> and send it all at one time? If so can this handle 3,000 insert statements in one call?</p>\n\n<p>Is there another way I am not looking at? Any info is appreciated.</p>\n\n<p>Thanks</p>\n"},{"tags":["performance","big-o"],"answer_count":3,"favorite_count":0,"up_vote_count":6,"down_vote_count":0,"view_count":224,"score":6,"question_id":2980146,"title":"Big O Complexity of a method","body":"<p>I have this method:</p>\n\n<pre><code>public static int what(String str, char start, char end)\n{\n    int count=0;\n    for(int i=0;i&lt;str.length(); i++) {\n        if(str.charAt(i) == start)\n        {\n            for(int j=i+1;j&lt;str.length(); j++)\n            {\n                if(str.charAt(j) == end)\n                    count++;\n            }\n        }\n    }\n    return count;\n}\n</code></pre>\n\n<p>What I need to find is:</p>\n\n<p>1) What is it doing? Answer: counting the total number of <em>end</em> occurrences after <strong>EACH</strong> (or is it? Not specified in the assignment, point 3 depends on this) <em>start</em>.</p>\n\n<p>2) What is its complexity? Answer: the first loops iterates over the string completely, so it's at least <em>O(n)</em>, the second loop executes only if <em>start</em> char is found and even then partially (index at which <em>start</em> was found + 1). Although, big O is all about worst case no? So in the worst case, <em>start</em> is the 1st char &amp; the inner iteration iterates over the string n-1 times, the -1 is a constant so it's <em>n</em>. But, the inner loop won't be executed every outer iteration pass, statistically, but since big O is about worst case, <strong>is it correct to say the complexity of it is O(n^2)</strong>? Ignoring any constants and the fact that in 99.99% of times the inner loop won't execute every outer loop pass.</p>\n\n<p>3) Rewrite it so that complexity is lower.<br />\nWhat I'm not sure of is whether <strong><em>start</em> occurs at most once</strong> or more, if once at most, then method can be rewritten using one loop (having a flag indicating whether <em>start</em> has been encountered and from there on incrementing <em>count</em> at each <em>end</em> occurrence), yielding a complexity of <strong>O(n)</strong>.</p>\n\n<p>In case though, that <em>start</em> can appear multiple times, which <strong>most likely it is</strong>, because assignment is of a Java course and I don't think they would make such ambiguity.<br />\nSolving, in this case, is not possible using one loop... <strong>WAIT</strong>! Yes it is..!<br />\nJust have a variable, say, <em>inc</em> to be incremented each time <em>start</em> is encountered &amp; used to increment <em>count</em> each time <em>end</em> is encountered after the 1st <em>start</em> was found:</p>\n\n<pre><code>inc = 0, count = 0\nif (current char == start) inc++\nif (inc &gt; 0 &amp;&amp; current char == end) count += inc\n</code></pre>\n\n<p>This would also yield a complexity of <strong>O(n)</strong>? Because there is only 1 loop.</p>\n\n<p>Yes I realize I wrote a lot hehe, but what I also realized is that I understand a lot better by forming my thoughts into words... </p>\n"},{"tags":["performance","buffer","zero-copy"],"answer_count":1,"favorite_count":0,"up_vote_count":0,"down_vote_count":0,"view_count":25,"score":0,"question_id":12502858,"title":"Vectored Referencing buffer implementation","body":"<p>I was reading code from one of the projects from github. I came across something called a Vectored Referencing buffer implementation. Can have someone come across this ? What are the practical applications of this. I did a quick google search and wasn't able to find any simple sample implementation for this.</p>\n\n<p>Some insight would be helpful.</p>\n"},{"tags":["c#","performance","quicksort"],"answer_count":4,"favorite_count":0,"up_vote_count":0,"down_vote_count":1,"view_count":69,"score":-1,"question_id":12463598,"title":"Does List<T>.Sort suffer worst case performance on sorted lists?","body":"<p>According to <a href=\"http://msdn.microsoft.com/en-us/library/b0zbh7b6.aspx\" rel=\"nofollow\">the docs</a> <code>List&lt;T&gt;.Sort</code> uses the QuickSort algorithm. I've heard that this can exibit worst case performance when called on a pre-sorted list if the pivot is not chosen wisely.</p>\n\n<p>Does the .NET implementation of QuickSort experience worst case behaviour on pre-sorted lists?</p>\n\n<p>In my case I'm writing a method that's going to do some processing on a list. The list needs to be sorted in order for the method to work. In most usage cases the list will be passed already sorted, but it's not impossible that there will be some small changes to the order. I'm wondering whether it's a good idea to re-sort the list on every method call. Clearly though, I am falling into the <a href=\"http://c2.com/cgi/wiki?PrematureOptimization\" rel=\"nofollow\">premature optimization</a> trap.</p>\n"},{"tags":["c++","performance","optimization","if-statement","code-size"],"answer_count":4,"favorite_count":1,"up_vote_count":3,"down_vote_count":1,"view_count":145,"score":2,"question_id":12501436,"title":"Should I encapsulate the last `return` statement with `else { return ... }` if it is logically optional?","body":"<p>Which format is better in terms of speed, performance and machine code size?</p>\n\n<p>Last <code>return</code> is encapsulated:</p>\n\n<pre><code>static bool MyClass::IsEqual(int A, int B)\n{\n    if (A == B)\n    {\n        return true;\n    } \n    else\n    {\n        return false;\n    }\n}\n</code></pre>\n\n<p>Last <code>return</code> is not encapsulated:</p>\n\n<pre><code>static bool MyClass::IsEqual(int A, int B)\n{\n    if (A == B)\n    {\n        return true;\n    }\n    return false;\n}\n</code></pre>\n"},{"tags":["c#","performance","linq-to-sql","architecture","repository-pattern"],"answer_count":4,"favorite_count":4,"up_vote_count":2,"down_vote_count":0,"view_count":841,"score":2,"question_id":1669607,"title":"Is this Repository pattern efficient with LINQ-to-SQL?","body":"<p>I'm currently reading the book Pro Asp.Net MVC Framework. In the book, the author suggests using a repository pattern similar to the following. </p>\n\n<pre><code>[Table(Name = \"Products\")]\npublic class Product\n{\n    [Column(IsPrimaryKey = true, \n            IsDbGenerated = true, \n            AutoSync = AutoSync.OnInsert)]\n    public int ProductId { get; set; }\n    [Column] public string Name { get; set; }\n    [Column] public string Description { get; set; }\n    [Column] public decimal Price { get; set; }\n    [Column] public string Category { get; set; }\n}\n\npublic interface IProductsRepository\n{\n    IQueryable&lt;Product&gt; Products { get; }\n}\n\npublic class SqlProductsRepository : IProductsRepository\n{\n    private Table&lt;Product&gt; productsTable;\n\n    public SqlProductsRepository(string connectionString)\n    {\n        productsTable = new DataContext(connectionString).GetTable&lt;Product&gt;();\n    }\n\n    public IQueryable&lt;Product&gt; Products\n    {\n        get { return productsTable; }\n    }\n}\n</code></pre>\n\n<p>Data is then accessed in the following manner:</p>\n\n<pre><code>public ViewResult List(string category)\n{\n    var productsInCategory =  (category == null) ? productsRepository.Products : productsRepository.Products.Where(p =&gt; p.Category == category);\n\n    return View(productsInCategory);\n}\n</code></pre>\n\n<p>Is this an efficient means of accessing data? Is the entire table going to be retrieved from the database and filtered in memory or is the chained Where() method going to cause some LINQ magic to create an optimized query based on the lambda?</p>\n\n<p>Finally, what other implementations of the Repository pattern in C# might provide better performance when hooked up via LINQ-to-SQL?</p>\n"},{"tags":["sql","performance","tsql"],"answer_count":3,"favorite_count":1,"up_vote_count":6,"down_vote_count":0,"view_count":108,"score":6,"question_id":12500646,"title":"Which is faster - NOT IN or NOT EXISTS?","body":"<p>I have an insert-select statement that needs to only insert rows where a particular identifier of the row does not exist in either of two other tables. Which of the following would be faster?</p>\n\n<pre><code>INSERT INTO Table1 (...)\nSELECT (...) FROM Table2 t2\nWHERE ...\n   AND NOT EXISTS (SELECT 'Y' from Table3 t3 where t2.SomeFK = t3.RefToSameFK)\n   AND NOT EXISTS (SELECT 'Y' from Table4 t4 where t2.SomeFK = t4.RefToSameFK AND ...)\n</code></pre>\n\n<p>... or...</p>\n\n<pre><code>INSERT INTO Table1 (...)\nSELECT (...) FROM Table2 t2\nWHERE ...\n   AND t2.SomeFK NOT IN (SELECT RefToSameFK from Table3)\n   AND t2.SomeFK NOT IN (SELECT RefToSameFK from Table4 WHERE ...)\n</code></pre>\n\n<p>... or do they perform about the same? Additionally, is there any other way to structure this query that would be preferable? I generally dislike subqueries as they add another \"dimension\" to the query that increases runtime by polynomial factors.</p>\n"}]}
